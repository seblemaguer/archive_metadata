{
  "title": "Interspeech 2017",
  "location": "Stockholm, Sweden",
  "startDate": "20/8/2017",
  "endDate": "24/8/2017",
  "URL": "http://www.interspeech2017.org/",
  "chair": "Chair: Francisco Lacerda",
  "conf": "Interspeech",
  "year": "2017",
  "name": "interspeech_2017",
  "series": "Interspeech",
  "SIG": "",
  "title1": "Interspeech 2017",
  "date": "20-24 August 2017",
  "booklet": "interspeech_2017.pdf",
  "papers": {
    "itakura17_interspeech": {
      "authors": [
        [
          "Fumitada",
          "Itakura"
        ]
      ],
      "title": "ISCA Medal for Scientific Achievement",
      "original": "3001",
      "page_count": 1,
      "order": 1,
      "p1": "1",
      "pn": "1",
      "abstract": [
        "The ISCA Medal for Scientific Achievement 2017 will be awarded to Professor\nFumitada Itakura by the President of ISCA during the opening ceremony.\n"
      ]
    },
    "kinnunen17_interspeech": {
      "authors": [
        [
          "Tomi",
          "Kinnunen"
        ],
        [
          "Md.",
          "Sahidullah"
        ],
        [
          "H\u00e9ctor",
          "Delgado"
        ],
        [
          "Massimiliano",
          "Todisco"
        ],
        [
          "Nicholas",
          "Evans"
        ],
        [
          "Junichi",
          "Yamagishi"
        ],
        [
          "Kong Aik",
          "Lee"
        ]
      ],
      "title": "The ASVspoof 2017 Challenge: Assessing the Limits of Replay Spoofing Attack Detection",
      "original": "1111",
      "page_count": 5,
      "order": 2,
      "p1": "2",
      "pn": "6",
      "abstract": [
        "The ASVspoof initiative was created to promote the development of countermeasures\nwhich aim to protect automatic speaker verification (ASV) from spoofing\nattacks. The first community-led, common evaluation held in 2015 focused\non countermeasures for speech synthesis and voice conversion spoofing\nattacks. Arguably, however, it is replay attacks which pose the greatest\nthreat. Such attacks involve the replay of recordings collected from\nenrolled speakers in order to provoke false alarms and can be mounted\nwith greater ease using everyday consumer devices. ASVspoof 2017, the\nsecond in the series, hence focused on the development of replay attack\ncountermeasures. This paper describes the database, protocols and initial\nfindings. The evaluation entailed highly heterogeneous acoustic recording\nand replay conditions which increased the equal error rate (EER) of\na baseline ASV system from 1.76% to 31.46%. Submissions were received\nfrom 49 research teams, 20 of which improved upon a baseline replay\nspoofing detector EER of 24.77%, in terms of replay/non-replay discrimination.\nWhile largely successful, the evaluation indicates that the quest for\ncountermeasures which are resilient in the face of variable replay\nattacks remains very much alive.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1111"
    },
    "font17_interspeech": {
      "authors": [
        [
          "Roberto",
          "Font"
        ],
        [
          "Juan M.",
          "Esp\u00edn"
        ],
        [
          "Mar\u00eda Jos\u00e9",
          "Cano"
        ]
      ],
      "title": "Experimental Analysis of Features for Replay Attack Detection &#8212; Results on the ASVspoof 2017 Challenge",
      "original": "0450",
      "page_count": 5,
      "order": 3,
      "p1": "7",
      "pn": "11",
      "abstract": [
        "This paper presents an experimental comparison of different features\nfor the detection of replay spoofing attacks in Automatic Speaker Verification\nsystems. We evaluate the proposed countermeasures using two recently\nintroduced databases, including the dataset provided for the ASVspoof\n2017 challenge. This challenge provides researchers with a common framework\nfor the evaluation of replay attack detection systems, with a particular\nfocus on the generalization to new, unknown conditions (for instance,\nreplay devices different from those used during system training). Our\ncross-database experiments show that, although achieving this level\nof generalization is indeed a challenging task, it is possible to train\nclassifiers that exhibit stable and consistent results across different\nexperiments. The proposed approach for the ASVspoof 2017 challenge\nconsists in the score-level fusion of several base classifiers using\nlogistic regression. These base classifiers are 2-class Gaussian Mixture\nModels (GMMs) representing  genuine and  spoofed speech respectively.\nOur best system achieves an Equal Error Rate of 10.52% on the challenge\nevaluation set. As a result of this set of experiments, we provide\nsome general conclusions regarding feature extraction for replay attack\ndetection and identify which features show the most promising results.\n"
      ],
      "doi": "10.21437/Interspeech.2017-450"
    },
    "patil17_interspeech": {
      "authors": [
        [
          "Hemant A.",
          "Patil"
        ],
        [
          "Madhu R.",
          "Kamble"
        ],
        [
          "Tanvina B.",
          "Patel"
        ],
        [
          "Meet H.",
          "Soni"
        ]
      ],
      "title": "Novel Variable Length Teager Energy Separation Based Instantaneous Frequency Features for Replay Detection",
      "original": "1362",
      "page_count": 5,
      "order": 4,
      "p1": "12",
      "pn": "16",
      "abstract": [
        "Replay attacks presents a great risk for Automatic Speaker Verification\n(ASV) system. In this paper, we propose a novel replay detector based\non Variable length Teager Energy Operator-Energy Separation Algorithm-Instantaneous\nFrequency Cosine Coefficients (VESA-IFCC) for the ASV spoof 2017 challenge.\nThe key idea here is to exploit the contribution of IF in each subband\nenergy via ESA to capture possible changes in spectral envelope (due\nto transmission and channel characteristics of replay device) of replayed\nspeech. The IF is computed from narrowband components of speech signal,\nand DCT is applied in IF to get proposed feature set. We compare the\nperformance of the proposed VESA-IFCC feature set with the features\ndeveloped for detecting synthetic and voice converted speech. This\nincludes the CQCC, CFCCIF and prosody-based features. On the development\nset, the proposed VESA-IFCC features when fused at score-level with\na variant of CFCCIF and prosody-based features gave the least EER of\n0.12%. On the evaluation set, this combination gave an EER of 18.33%.\nHowever, post-evaluation results of challenge indicate that VESA-IFCC\nfeatures alone gave the relatively least EER of 14.06% (i.e., relatively\n16.11% less compared to baseline CQCC) and hence, is a very useful\ncountermeasure to detect replay attacks.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1362"
    },
    "cai17_interspeech": {
      "authors": [
        [
          "Weicheng",
          "Cai"
        ],
        [
          "Danwei",
          "Cai"
        ],
        [
          "Wenbo",
          "Liu"
        ],
        [
          "Gang",
          "Li"
        ],
        [
          "Ming",
          "Li"
        ]
      ],
      "title": "Countermeasures for Automatic Speaker Verification Replay Spoofing Attack : On Data Augmentation, Feature Representation, Classification and Fusion",
      "original": "0906",
      "page_count": 5,
      "order": 5,
      "p1": "17",
      "pn": "21",
      "abstract": [
        "The ongoing ASVspoof 2017 challenge aims to detect replay attacks for\ntext dependent speaker verification. In this paper, we propose multiple\nreplay spoofing countermeasure systems, with some of them boosting\nthe CQCC-GMM baseline system after score level fusion. We investigate\ndifferent steps in the system building pipeline, including data augmentation,\nfeature representation, classification and fusion. First, in order\nto augment training data and simulate the unseen replay conditions,\nwe converted the raw genuine training data into replay spoofing data\nwith parametric sound reverberator and phase shifter. Second, we employed\nthe original spectrogram rather than CQCC as input to explore the end-to-end\nfeature representation learning methods. The spectrogram is randomly\ncropped into fixed size segments, and then fed into a deep residual\nnetwork (ResNet). Third, upon the CQCC features, we replaced the subsequent\nGMM classifier with deep neural networks including fully-connected\ndeep neural network (FDNN) and Bidirectional Long Short Term Memory\nneural network (BLSTM). Experiments showed that data augmentation strategy\ncan significantly improve the system performance. The final fused system\nachieves to 16.39% EER on the test set of ASVspoof 2017 for the common\ntask.\n"
      ],
      "doi": "10.21437/Interspeech.2017-906"
    },
    "jelil17_interspeech": {
      "authors": [
        [
          "Sarfaraz",
          "Jelil"
        ],
        [
          "Rohan Kumar",
          "Das"
        ],
        [
          "S.R. Mahadeva",
          "Prasanna"
        ],
        [
          "Rohit",
          "Sinha"
        ]
      ],
      "title": "Spoof Detection Using Source, Instantaneous Frequency and Cepstral Features",
      "original": "0930",
      "page_count": 5,
      "order": 6,
      "p1": "22",
      "pn": "26",
      "abstract": [
        "This work describes the techniques used for spoofed speech detection\nfor the ASVspoof 2017 challenge. The main focus of this work is on\nexploiting the differences in the speech-specific nature of genuine\nspeech signals and spoofed speech signals generated by replay attacks.\nThis is achieved using glottal closure instants, epoch strength, and\nthe peak to side lobe ratio of the Hilbert envelope of linear prediction\nresidual. Apart from these source features, the instantaneous frequency\ncosine coefficient feature, and two cepstral features namely, constant\nQ cepstral coefficients and mel frequency cepstral coefficients are\nused. A combination of all these features is performed to obtain a\nhigh degree of accuracy for spoof detection. Initially, efficacy of\nthese features are tested on the development set of the ASVspoof 2017\ndatabase with Gaussian mixture model based systems. The systems are\nthen fused at score level which acts as the final combined system for\nthe challenge. The combined system is able to outperform the individual\nsystems by a significant margin. Finally, the experiments are repeated\non the evaluation set of the database and the combined system results\nin an equal error rate of 13.95%.\n"
      ],
      "doi": "10.21437/Interspeech.2017-930"
    },
    "witkowski17_interspeech": {
      "authors": [
        [
          "Marcin",
          "Witkowski"
        ],
        [
          "Stanis\u0142aw",
          "Kacprzak"
        ],
        [
          "Piotr",
          "\u017belasko"
        ],
        [
          "Konrad",
          "Kowalczyk"
        ],
        [
          "Jakub",
          "Ga\u0142ka"
        ]
      ],
      "title": "Audio Replay Attack Detection Using High-Frequency Features",
      "original": "0776",
      "page_count": 5,
      "order": 7,
      "p1": "27",
      "pn": "31",
      "abstract": [
        "This paper presents our contribution to the ASVspoof 2017 Challenge.\nIt addresses a replay spoofing attack against a speaker recognition\nsystem by detecting that the analysed signal has passed through multiple\nanalogue-to-digital (AD) conversions. Specifically, we show that most\nof the cues that enable to detect the replay attacks can be found in\nthe high-frequency band of the replayed recordings. The described anti-spoofing\ncountermeasures are based on (1) modelling the subband spectrum and\n(2) using the proposed features derived from the linear prediction\n(LP) analysis. The results of the investigated methods show a significant\nimprovement in comparison to the baseline system of the ASVspoof 2017\nChallenge. A relative equal error rate (EER) reduction by 70% was achieved\nfor the development set and a reduction by 30% was obtained for the\nevaluation set.\n"
      ],
      "doi": "10.21437/Interspeech.2017-776"
    },
    "wang17_interspeech": {
      "authors": [
        [
          "Xianliang",
          "Wang"
        ],
        [
          "Yanhong",
          "Xiao"
        ],
        [
          "Xuan",
          "Zhu"
        ]
      ],
      "title": "Feature Selection Based on CQCCs for Automatic Speaker Verification Spoofing",
      "original": "0304",
      "page_count": 5,
      "order": 8,
      "p1": "32",
      "pn": "36",
      "abstract": [
        "The ASVspoof 2017 challenge aims to assess spoofing and countermeasures\nattack detection accuracy for automatic speaker verification. It has\nbeen proven that constant Q cepstral coefficients (CQCCs) processes\nspeech in different frequencies with variable resolution and performs\nmuch better than traditional features. When coupled with a Gaussian\nmixture model (GMM), it is an excellently effective spoofing countermeasure.\nThe baseline CQCC+GMM system considers short-term impacts while ignoring\nthe whole influence of channel. In the meanwhile, dimension of the\nfeature is relatively higher than the traditional feature and usually\nwith a higher variance. This paper explores different features for\nASVspoof 2017 challenge. The mean and variance of the CQCC features\nof an utterance is used as the representation of the whole utterance.\nFeature selection method is introduced to avoid high variance and overfitting\nfor spoofing detection. Experimental results on ASVspoof 2017 dataset\nshow that feature selection followed by Support Vector Machine (SVM)\ngets an improvement compared to the baseline. It is also shown that\npitch feature contributes to the performance improvement, and it obtains\na relative improvement of 37.39% over the baseline CQCC+GMM system.\n"
      ],
      "doi": "10.21437/Interspeech.2017-304"
    },
    "ylmaz17_interspeech": {
      "authors": [
        [
          "Emre",
          "Y\u0131lmaz"
        ],
        [
          "Jelske",
          "Dijkstra"
        ],
        [
          "Hans Van de",
          "Velde"
        ],
        [
          "Frederik",
          "Kampstra"
        ],
        [
          "Jouke",
          "Algra"
        ],
        [
          "Henk van den",
          "Heuvel"
        ],
        [
          "David Van",
          "Leeuwen"
        ]
      ],
      "title": "Longitudinal Speaker Clustering and Verification Corpus with Code-Switching Frisian-Dutch Speech",
      "original": "0301",
      "page_count": 5,
      "order": 9,
      "p1": "37",
      "pn": "41",
      "abstract": [
        "In this paper, we present a new longitudinal and bilingual broadcast\ndatabase designed for speaker clustering and text-independent verification\nresearch. The broadcast data is extracted from the archives of Omrop\nFrysl&#226;n which is the regional broadcaster in the province of Frysl&#226;n,\nlocated in the north of the Netherlands. Two speaker verification tasks\nare provided in a standard enrollment-test setting with language consistent\ntrials. The first task contains target trials from all speakers available\nappearing in at least two different programs, while the second task\ncontains target trials from a subgroup of speakers appearing in programs\nrecorded in multiple years. The second task is designed to investigate\nthe effects of ageing on the accuracy of speaker verification systems.\nThis database also contains unlabeled spoken segments from different\nradio programs for speaker clustering research. We provide the output\nof an existing speaker diarization system for baseline verification\nexperiments. Finally, we present the baseline speaker verification\nresults using the Kaldi GMM- and DNN-UBM speaker verification system.\nThis database will be an extension to the recently presented open source\nFrisian data collection and it is publicly available for research purposes.\n"
      ],
      "doi": "10.21437/Interspeech.2017-301"
    },
    "ylmaz17b_interspeech": {
      "authors": [
        [
          "Emre",
          "Y\u0131lmaz"
        ],
        [
          "Henk van den",
          "Heuvel"
        ],
        [
          "David Van",
          "Leeuwen"
        ]
      ],
      "title": "Exploiting Untranscribed Broadcast Data for Improved Code-Switching Detection",
      "original": "0391",
      "page_count": 5,
      "order": 10,
      "p1": "42",
      "pn": "46",
      "abstract": [
        "We have recently presented an automatic speech recognition (ASR) system\noperating on Frisian-Dutch code-switched speech. This type of speech\nrequires careful handling of unexpected language switches that may\noccur in a single utterance. In this paper, we extend this work by\nusing some raw broadcast data to improve multilingually trained deep\nneural networks (DNN) that have been trained on 11.5 hours of manually\nannotated bilingual speech. For this purpose, we apply the initial\nASR to the untranscribed broadcast data and automatically create transcriptions\nbased on the recognizer output using different language models for\nrescoring. Then, we train new acoustic models on the combined data,\ni.e., the manually and automatically transcribed bilingual broadcast\ndata, and investigate the automatic transcription quality based on\nthe recognition accuracies on a separate set of development and test\ndata. Finally, we report code-switching detection performance elaborating\non the correlation between the ASR and the code-switching detection\nperformance.\n"
      ],
      "doi": "10.21437/Interspeech.2017-391"
    },
    "ramanarayanan17_interspeech": {
      "authors": [
        [
          "Vikram",
          "Ramanarayanan"
        ],
        [
          "David",
          "Suendermann-Oeft"
        ]
      ],
      "title": " Jee haan, I&#8217;d like both, por favor: Elicitation of a Code-Switched Corpus of Hindi&#8211;English and Spanish&#8211;English Human&#8211;Machine Dialog",
      "original": "1198",
      "page_count": 5,
      "order": 11,
      "p1": "47",
      "pn": "51",
      "abstract": [
        "We present a database of code-switched conversational human&#8211;machine\ndialog in English&#8211;Hindi and English&#8211;Spanish. We leveraged\nHALEF, an open-source standards-compliant cloud-based dialog system\nto capture audio and video of bilingual crowd workers as they interacted\nwith the system. We designed conversational items with  intra-sentential\ncode-switched machine prompts, and examine its efficacy in eliciting\ncode-switched speech in a total of over 700 dialogs. We analyze various\ncharacteristics of the code-switched corpus and discuss some considerations\nthat should be taken into account while collecting and processing such\ndata. Such a database can be leveraged for a wide range of potential\napplications, including automated processing, recognition and understanding\nof code-switched speech and language learning applications for new\nlanguage learners.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1198"
    },
    "rallabandi17_interspeech": {
      "authors": [
        [
          "SaiKrishna",
          "Rallabandi"
        ],
        [
          "Alan W.",
          "Black"
        ]
      ],
      "title": "On Building Mixed Lingual Speech Synthesis Systems",
      "original": "1244",
      "page_count": 5,
      "order": 12,
      "p1": "52",
      "pn": "56",
      "abstract": [
        "Codemixing &#8212; phenomenon where lexical items from one language\nare embedded in the utterance of another &#8212; is relatively frequent\nin multilingual communities. However, TTS systems today are not fully\ncapable of effectively handling such mixed content despite achieving\nhigh quality in the monolingual case. In this paper, we investigate\nvarious mechanisms for building mixed lingual systems which are built\nusing a mixture of monolingual corpora and are capable of synthesizing\nsuch content. First, we explore the possibility of manipulating the\nphoneme representation: using target word to source phone mapping with\nthe aim of emulating the native speaker intuition. We then present\nexperiments at the acoustic stage investigating training techniques\nat both spectral and prosodic levels. Subjective evaluation shows that\nour systems are capable of generating high quality synthesis in codemixed\nscenarios.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1244"
    },
    "chandu17_interspeech": {
      "authors": [
        [
          "Khyathi Raghavi",
          "Chandu"
        ],
        [
          "SaiKrishna",
          "Rallabandi"
        ],
        [
          "Sunayana",
          "Sitaram"
        ],
        [
          "Alan W.",
          "Black"
        ]
      ],
      "title": "Speech Synthesis for Mixed-Language Navigation Instructions",
      "original": "1259",
      "page_count": 5,
      "order": 13,
      "p1": "57",
      "pn": "61",
      "abstract": [
        "Text-to-Speech (TTS) systems that can read navigation instructions\nare one of the most widely used speech interfaces today. Text in the\nnavigation domain may contain named entities such as location names\nthat are not in the language that the TTS database is recorded in.\nMoreover, named entities can be compound words where individual lexical\nitems belong to different languages. These named entities may be transliterated\ninto the script that the TTS system is trained on. This may result\nin incorrect pronunciation rules being used for such words. We describe\nexperiments to extend our previous work in generating code-mixed speech\nto synthesize navigation instructions, with a mixed-lingual TTS system.\nWe conduct subjective listening tests with two sets of users, one being\nstudents who are native speakers of an Indian language and very proficient\nin English, and the other being drivers with low English literacy,\nbut familiarity with location names. We find that in both sets of users,\nthere is a significant preference for our proposed system over a baseline\nsystem that synthesizes instructions in English.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1259"
    },
    "amazouz17_interspeech": {
      "authors": [
        [
          "Djegdjiga",
          "Amazouz"
        ],
        [
          "Martine",
          "Adda-Decker"
        ],
        [
          "Lori",
          "Lamel"
        ]
      ],
      "title": "Addressing Code-Switching in French/Algerian Arabic Speech",
      "original": "1373",
      "page_count": 5,
      "order": 14,
      "p1": "62",
      "pn": "66",
      "abstract": [
        "This study focuses on code-switching (CS) in French/Algerian Arabic\nbilingual communities and investigates how speech technologies, such\nas automatic data partitioning, language identification and automatic\nspeech recognition (ASR) can serve to analyze and classify this type\nof bilingual speech. A preliminary study carried out using a corpus\nof Maghrebian broadcast data revealed a relatively high presence of\nCS Algerian Arabic as compared to the neighboring countries Morocco\nand Tunisia. Therefore this study focuses on code switching produced\nby bilingual Algerian speakers who can be considered native speakers\nof both Algerian Arabic and French. A specific corpus of four hours\nof speech from 8 bilingual French Algerian speakers was collected.\nThis corpus contains read speech and conversational speech in both\nlanguages and includes stretches of code-switching. We provide a linguistic\ndescription of the code-switching stretches in terms of intra-sentential\nand inter-sentential switches, the speech duration in each language.\nWe report on some initial studies to locate French, Arabic and the\ncode-switched stretches, using ASR system word posteriors for this\npair of languages.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1373"
    },
    "guzman17_interspeech": {
      "authors": [
        [
          "Gualberto",
          "Guzm\u00e1n"
        ],
        [
          "Joseph",
          "Ricard"
        ],
        [
          "Jacqueline",
          "Serigos"
        ],
        [
          "Barbara E.",
          "Bullock"
        ],
        [
          "Almeida Jacqueline",
          "Toribio"
        ]
      ],
      "title": "Metrics for Modeling Code-Switching Across Corpora",
      "original": "1429",
      "page_count": 5,
      "order": 15,
      "p1": "67",
      "pn": "71",
      "abstract": [
        "In developing technologies for code-switched speech, it would be desirable\nto be able to predict how much language mixing might be expected in\nthe signal and the regularity with which it might occur. In this work,\nwe offer various metrics that allow for the classification and visualization\nof multilingual corpora according to the ratio of languages represented,\nthe probability of switching between them, and the time-course of switching.\nApplying these metrics to corpora of different languages and genres,\nwe find that they display distinct probabilities and periodicities\nof switching, information useful for speech processing of mixed-language\ndata.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1429"
    },
    "westhuizen17_interspeech": {
      "authors": [
        [
          "Ewald van der",
          "Westhuizen"
        ],
        [
          "Thomas",
          "Niesler"
        ]
      ],
      "title": "Synthesising isiZulu-English Code-Switch Bigrams Using Word Embeddings",
      "original": "1437",
      "page_count": 5,
      "order": 16,
      "p1": "72",
      "pn": "76",
      "abstract": [
        "Code-switching is prevalent among South African speakers, and presents\na challenge to automatic speech recognition systems. It is predominantly\na spoken phenomenon, and generally does not occur in textual form.\nTherefore a particularly serious challenge is the extreme lack of training\nmaterial for language modelling. We investigate the use of word embeddings\nto synthesise isiZulu-to-English code-switch bigrams with which to\naugment such sparse language model training data. A variety of word\nembeddings are trained on a monolingual English web text corpus, and\nsubsequently queried to synthesise code-switch bigrams. Our evaluation\nis performed on language models trained on a new, although small, English-isiZulu\ncode-switch corpus compiled from South African soap operas. This data\nis characterised by fast, spontaneously spoken speech containing frequent\ncode-switching. We show that the augmentation of the training data\nwith code-switched bigrams synthesised in this way leads to a reduction\nin perplexity.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1437"
    },
    "soto17_interspeech": {
      "authors": [
        [
          "Victor",
          "Soto"
        ],
        [
          "Julia",
          "Hirschberg"
        ]
      ],
      "title": "Crowdsourcing Universal Part-of-Speech Tags for Code-Switching",
      "original": "1663",
      "page_count": 5,
      "order": 17,
      "p1": "77",
      "pn": "81",
      "abstract": [
        "Code-switching is the phenomenon by which bilingual speakers switch\nbetween multiple languages during communication. The importance of\ndeveloping language technologies for code-switching data is immense,\ngiven the large populations that routinely code-switch. High-quality\nlinguistic annotations are extremely valuable for any NLP task, and\nperformance is often limited by the amount of high-quality labeled\ndata. However, little such data exists for code-switching. In this\npaper, we describe crowd-sourcing universal part-of-speech tags for\nthe Miami Bangor Corpus of Spanish-English code-switched speech. We\nsplit the annotation task into three subtasks: one in which a subset\nof tokens are labeled automatically, one in which questions are specifically\ndesigned to disambiguate a subset of high frequency words, and a more\ngeneral cascaded approach for the remaining data in which questions\nare displayed to the worker following a decision tree structure. Each\nsubtask is extended and adapted for a multilingual setting and the\nuniversal tagset. The quality of the annotation process is measured\nusing hidden check questions annotated with gold labels. The overall\nagreement between gold standard labels and the majority vote is between\n0.95 and 0.96 for just three labels and the average recall across part-of-speech\ntags is between 0.87 and 0.99, depending on the task.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1663"
    },
    "lavrentyeva17_interspeech": {
      "authors": [
        [
          "Galina",
          "Lavrentyeva"
        ],
        [
          "Sergey",
          "Novoselov"
        ],
        [
          "Egor",
          "Malykh"
        ],
        [
          "Alexander",
          "Kozlov"
        ],
        [
          "Oleg",
          "Kudashev"
        ],
        [
          "Vadim",
          "Shchemelinin"
        ]
      ],
      "title": "Audio Replay Attack Detection with Deep Learning Frameworks",
      "original": "0360",
      "page_count": 5,
      "order": 18,
      "p1": "82",
      "pn": "86",
      "abstract": [
        "Nowadays spoofing detection is one of the priority research areas in\nthe field of automatic speaker verification. The success of Automatic\nSpeaker Verification Spoofing and Countermeasures (ASVspoof) Challenge\n2015 confirmed the impressive perspective in detection of unforeseen\nspoofing trials based on speech synthesis and voice conversion techniques.\nHowever, there is a small number of researches addressed to replay\nspoofing attacks which are more likely to be used by non-professional\nimpersonators. This paper describes the Speech Technology Center (STC)\nanti-spoofing system submitted for ASVspoof 2017 which is focused on\nreplay attacks detection. Here we investigate the efficiency of a deep\nlearning approach for solution of the mentioned-above task. Experimental\nresults obtained on the Challenge corpora demonstrate that the selected\napproach outperforms current state-of-the-art baseline systems in terms\nof spoofing detection quality. Our primary system produced an EER of\n6.73% on the evaluation part of the corpora which is 72% relative improvement\nover the ASVspoof 2017 baseline system.\n"
      ],
      "doi": "10.21437/Interspeech.2017-360"
    },
    "ji17_interspeech": {
      "authors": [
        [
          "Zhe",
          "Ji"
        ],
        [
          "Zhi-Yi",
          "Li"
        ],
        [
          "Peng",
          "Li"
        ],
        [
          "Maobo",
          "An"
        ],
        [
          "Shengxiang",
          "Gao"
        ],
        [
          "Dan",
          "Wu"
        ],
        [
          "Faru",
          "Zhao"
        ]
      ],
      "title": "Ensemble Learning for Countermeasure of Audio Replay Spoofing Attack in ASVspoof2017",
      "original": "1246",
      "page_count": 5,
      "order": 19,
      "p1": "87",
      "pn": "91",
      "abstract": [
        "To enhance the security and reliability of automatic speaker verification\n(ASV) systems, ASVspoof 2017 challenge focuses on the detection problem\nof known and unknown audio replay attacks. We proposed an ensemble\nlearning classifier for CNCB team&#8217;s submitted system scores,\nwhich across uses a variety of acoustic features and classifiers. An\neffective post-processing method is studied to improve the performance\nof Constant Q cepstral coefficients (CQCC) and to form a base feature\nset with some other classical acoustic features. We also proposed using\nan ensemble classifier set, which includes multiple Gaussian Mixture\nModel (GMM) based classifiers and two novel GMM mean supervector-Gradient\nBoosting Decision Tree (GSV-GBDT) and GSV-Random Forest (GSV-RF) classifiers.\nExperimental results have shown that the proposed ensemble learning\nsystem can provide substantially better performance than baseline.\nOn common training condition of the challenge, Equal Error Rate (EER)\nof primary system on development set is 1.5%, compared to baseline\n10.4%. EER of primary system (S02 in ASVspoof 2017 board) on evaluation\ndata set are 12.3% (with only train dataset) and 10.8% (with train+dev\ndataset), which are also much better than baseline 30.6% and 24.8%,\ngiven by ASVSpoof 2017 organizer, with 59.7% and 56.4% relative performance\nimprovement.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1246"
    },
    "li17b_interspeech": {
      "authors": [
        [
          "Lantian",
          "Li"
        ],
        [
          "Yixiang",
          "Chen"
        ],
        [
          "Dong",
          "Wang"
        ],
        [
          "Thomas Fang",
          "Zheng"
        ]
      ],
      "title": "A Study on Replay Attack and Anti-Spoofing for Automatic Speaker Verification",
      "original": "0456",
      "page_count": 5,
      "order": 20,
      "p1": "92",
      "pn": "96",
      "abstract": [
        "For practical automatic speaker verification (ASV) systems, replay\nattack poses a true risk. By replaying a pre-recorded speech signal\nof the genuine speaker, ASV systems tend to be easily fooled. An effective\nreplay detection method is therefore highly desirable. In this study,\nwe investigate a major difficulty in replay detection: the over-fitting\nproblem caused by variability factors in speech signal. An F-ratio\nprobing tool is proposed and three variability factors are investigated\nusing this tool: speaker identity, speech content and playback &amp;\nrecording device. The analysis shows that device is the most influential\nfactor that contributes the highest over-fitting risk. A frequency\nwarping approach is studied to alleviate the over-fitting problem,\nas verified on the ASV-spoof 2017 database.\n"
      ],
      "doi": "10.21437/Interspeech.2017-456"
    },
    "nagarsheth17_interspeech": {
      "authors": [
        [
          "Parav",
          "Nagarsheth"
        ],
        [
          "Elie",
          "Khoury"
        ],
        [
          "Kailash",
          "Patil"
        ],
        [
          "Matt",
          "Garland"
        ]
      ],
      "title": "Replay Attack Detection Using DNN for Channel Discrimination",
      "original": "1377",
      "page_count": 5,
      "order": 21,
      "p1": "97",
      "pn": "101",
      "abstract": [
        "Voice is projected to be the next input interface for portable devices.\nThe increased use of audio interfaces can be mainly attributed to the\nsuccess of speech and speaker recognition technologies. With these\nadvances comes the risk of criminal threats where attackers are reportedly\ntrying to access sensitive information using diverse voice spoofing\ntechniques. Among them, replay attacks pose a real challenge to voice\nbiometrics. This paper addresses the problem by proposing a deep learning\narchitecture in tandem with low-level cepstral features. We investigate\nthe use of a deep neural network (DNN) to discriminate between the\ndifferent channel conditions available in the ASVSpoof 2017 dataset,\nnamely recording, playback and session conditions. The high-level feature\nvectors derived from this network are used to discriminate between\ngenuine and spoofed audio. Two kinds of low-level features are utilized:\nstate-of-the-art constant-Q cepstral coefficients (CQCC), and our proposed\nhigh-frequency cepstral coefficients (HFCC) that derive from the high-frequency\nspectrum of the audio. The fusion of both features proved to be effective\nin generalizing well across diverse replay attacks seen in the evaluation\nof the ASVSpoof 2017 challenge, with an equal error rate of 11.5%,\nthat is 53% better than the baseline Gaussian Mixture Model (GMM) applied\non CQCC.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1377"
    },
    "chen17_interspeech": {
      "authors": [
        [
          "Zhuxin",
          "Chen"
        ],
        [
          "Zhifeng",
          "Xie"
        ],
        [
          "Weibin",
          "Zhang"
        ],
        [
          "Xiangmin",
          "Xu"
        ]
      ],
      "title": "ResNet and Model Fusion for Automatic Spoofing Detection",
      "original": "1085",
      "page_count": 5,
      "order": 22,
      "p1": "102",
      "pn": "106",
      "abstract": [
        "Speaker verification systems have achieved great progress in recent\nyears. Unfortunately, they are still highly prone to different kinds\nof spoofing attacks such as speech synthesis, voice conversion, and\nfake audio recordings etc. Inspired by the success of ResNet in image\nrecognition, we investigated the effectiveness of using ResNet for\nautomatic spoofing detection. Experimental results on the ASVspoof2017\ndata set show that ResNet performs the best among all the single-model\nsystems. Model fusion is a good way to further improve the system performance.\nNevertheless, we found that if the same feature is used for different\nfused models, the resulting system can hardly be improved. By using\ndifferent features and models, our best fused model further reduced\nthe Equal Error Rate (EER) by 18% relatively, compared with the best\nsingle-model system.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1085"
    },
    "alluri17_interspeech": {
      "authors": [
        [
          "K.N.R.K. Raju",
          "Alluri"
        ],
        [
          "Sivanand",
          "Achanta"
        ],
        [
          "Sudarsana Reddy",
          "Kadiri"
        ],
        [
          "Suryakanth V.",
          "Gangashetty"
        ],
        [
          "Anil Kumar",
          "Vuppala"
        ]
      ],
      "title": "SFF Anti-Spoofer: IIIT-H Submission for Automatic Speaker Verification Spoofing and Countermeasures Challenge 2017",
      "original": "0676",
      "page_count": 5,
      "order": 23,
      "p1": "107",
      "pn": "111",
      "abstract": [
        "The ASVspoof 2017 challenge is about the detection of replayed speech\nfrom human speech. The proposed system makes use of the fact that when\nthe speech signals are replayed, they pass through multiple channels\nas opposed to original recordings. This channel information is typically\nembedded in low signal to noise ratio regions. A speech signal processing\nmethod with high spectro-temporal resolution is required to extract\nrobust features from such regions. The single frequency filtering (SFF)\nis one such technique, which we propose to use for replay attack detection.\nWhile SFF based feature representation was used at front-end, Gaussian\nmixture model and bi-directional long short-term memory models are\ninvestigated at the backend as classifiers. The experimental results\non ASVspoof 2017 dataset reveal that, SFF based representation is very\neffective in detecting replay attacks. The score level fusion of back\nend classifiers further improved the performance of the system which\nindicates that both classifiers capture complimentary information.\n"
      ],
      "doi": "10.21437/Interspeech.2017-676"
    },
    "hartmann17_interspeech": {
      "authors": [
        [
          "William",
          "Hartmann"
        ],
        [
          "Roger",
          "Hsiao"
        ],
        [
          "Tim",
          "Ng"
        ],
        [
          "Jeff",
          "Ma"
        ],
        [
          "Francis",
          "Keith"
        ],
        [
          "Man-Hung",
          "Siu"
        ]
      ],
      "title": "Improved Single System Conversational Telephone Speech Recognition with VGG Bottleneck Features",
      "original": "1513",
      "page_count": 5,
      "order": 24,
      "p1": "112",
      "pn": "116",
      "abstract": [
        "On small datasets, discriminatively trained bottleneck features from\ndeep networks commonly outperform more traditional spectral or cepstral\nfeatures. While these features are typically trained with small, fully-connected\nnetworks, recent studies have used more sophisticated networks with\ngreat success. We use the recent deep CNN (VGG) network for bottleneck\nfeature extraction &#8212; previously used only for low-resource tasks\n&#8212; and apply it to the Switchboard English conversational telephone\nspeech task. Unlike features derived from traditional MLP networks,\nthe VGG features outperform cepstral features even when used with BLSTM\nacoustic models trained on large amounts of data. We achieve the best\nBBN single system performance when combining the VGG features with\na BLSTM acoustic model. When decoding with an n-gram language model,\nwhich are used for deployable systems, we have a realistic production\nsystem with a WER of 7.4%. This result is competitive with the current\nstate-of-the-art in the literature. While our focus is on realistic\nsingle system performance, we further reduce the WER to 6.1% through\nsystem combination and using expensive neural network language model\nrescoring.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1513"
    },
    "wong17_interspeech": {
      "authors": [
        [
          "Jeremy H.M.",
          "Wong"
        ],
        [
          "Mark J.F.",
          "Gales"
        ]
      ],
      "title": "Student-Teacher Training with Diverse Decision Tree Ensembles",
      "original": "0145",
      "page_count": 5,
      "order": 25,
      "p1": "117",
      "pn": "121",
      "abstract": [
        "Student-teacher training allows a large teacher model or ensemble of\nteachers to be compressed into a single student model, for the purpose\nof efficient decoding. However, current approaches in automatic speech\nrecognition assume that the state clusters, often defined by Phonetic\nDecision Trees (PDT), are the same across all models. This limits the\ndiversity that can be captured within the ensemble, and also the flexibility\nwhen selecting the complexity of the student model output. This paper\nexamines an extension to student-teacher training that allows for the\npossibility of having different PDTs between teachers, and also for\nthe student to have a different PDT from the teacher. The proposal\nis to train the student to emulate the logical context dependent state\nposteriors of the teacher, instead of the frame posteriors. This leads\nto a method of mapping frame posteriors from one PDT to another. This\napproach is evaluated on three speech recognition tasks: the Tok Pisin\nand Javanese low resource conversational telephone speech tasks from\nthe IARPA Babel programme, and the HUB4 English broadcast news task.\n"
      ],
      "doi": "10.21437/Interspeech.2017-145"
    },
    "cui17_interspeech": {
      "authors": [
        [
          "Xiaodong",
          "Cui"
        ],
        [
          "Vaibhava",
          "Goel"
        ],
        [
          "George",
          "Saon"
        ]
      ],
      "title": "Embedding-Based Speaker Adaptive Training of Deep Neural Networks",
      "original": "0460",
      "page_count": 5,
      "order": 26,
      "p1": "122",
      "pn": "126",
      "abstract": [
        "An embedding-based speaker adaptive training (SAT) approach is proposed\nand investigated in this paper for deep neural network acoustic modeling.\nIn this approach, speaker embedding vectors, which are a constant given\na particular speaker, are mapped through a control network to layer-dependent\nelement-wise affine transformations to canonicalize the internal feature\nrepresentations at the output of hidden layers of a main network. The\ncontrol network for generating the speaker-dependent mappings are jointly\nestimated with the main network for the overall speaker adaptive acoustic\nmodeling. Experiments on large vocabulary continuous speech recognition\n(LVCSR) tasks show that the proposed SAT scheme can yield superior\nperformance over the widely-used speaker-aware training using i-vectors\nwith speaker-adapted input features.\n"
      ],
      "doi": "10.21437/Interspeech.2017-460"
    },
    "ma17_interspeech": {
      "authors": [
        [
          "Jeff",
          "Ma"
        ],
        [
          "Francis",
          "Keith"
        ],
        [
          "Tim",
          "Ng"
        ],
        [
          "Man-Hung",
          "Siu"
        ],
        [
          "Owen",
          "Kimball"
        ]
      ],
      "title": "Improving Deliverable Speech-to-Text Systems with Multilingual Knowledge Transfer",
      "original": "1058",
      "page_count": 5,
      "order": 27,
      "p1": "127",
      "pn": "131",
      "abstract": [
        "This paper reports our recent progress on using multilingual data for\nimproving speech-to-text (STT) systems that can be easily delivered.\nWe continued the work BBN conducted on the use of multilingual data\nfor improving Babel evaluation systems, but focused on training time-delay\nneural network (TDNN) based chain models. As done for the Babel evaluations,\nwe used multilingual data in two ways: first, to train multilingual\ndeep neural networks (DNN) for extracting bottle-neck (BN) features,\nand second, for initializing training on target languages.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  Our results show that\nTDNN chain models trained on multilingual DNN bottleneck features yield\nsignificant gains over their counterparts trained on MFCC plus i-vector\nfeatures. By initializing from models trained on multilingual data,\nTDNN chain models can achieve great improvements over random initializations\nof the network weights on target languages. Two other important findings\nare: 1) initialization with multilingual TDNN chain models produces\nlarger gains on target languages that have less training data; 2) inclusion\nof target languages in multilingual training for either BN feature\nextraction or initialization have limited impact on performance measured\non the target languages. Our results also reveal that for TDNN chain\nmodels, the combination of multilingual BN features and multilingual\ninitialization achieves the best performance on all target languages.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1058"
    },
    "saon17_interspeech": {
      "authors": [
        [
          "George",
          "Saon"
        ],
        [
          "Gakuto",
          "Kurata"
        ],
        [
          "Tom",
          "Sercu"
        ],
        [
          "Kartik",
          "Audhkhasi"
        ],
        [
          "Samuel",
          "Thomas"
        ],
        [
          "Dimitrios",
          "Dimitriadis"
        ],
        [
          "Xiaodong",
          "Cui"
        ],
        [
          "Bhuvana",
          "Ramabhadran"
        ],
        [
          "Michael",
          "Picheny"
        ],
        [
          "Lynn-Li",
          "Lim"
        ],
        [
          "Bergul",
          "Roomi"
        ],
        [
          "Phil",
          "Hall"
        ]
      ],
      "title": "English Conversational Telephone Speech Recognition by Humans and Machines",
      "original": "0405",
      "page_count": 5,
      "order": 28,
      "p1": "132",
      "pn": "136",
      "abstract": [
        "Word error rates on the Switchboard conversational corpus that just\na few years ago were 14% have dropped to 8.0%, then 6.6% and most recently\n5.8%, and are now believed to be within striking range of human performance.\nThis then raises two issues: what is human performance, and how far\ndown can we still drive speech recognition error rates? In trying to\nassess human performance, we performed an independent set of measurements\non the Switchboard and CallHome subsets of the Hub5 2000 evaluation\nand found that human accuracy may be considerably better than what\nwas earlier reported, giving the community a significantly harder goal\nto achieve. We also report on our own efforts in this area, presenting\na set of acoustic and language modeling techniques that lowered the\nWER of our system to 5.5%/10.3% on these subsets, which is a new performance\nmilestone (albeit not at what we measure to be human performance).\nOn the acoustic side, we use a score fusion of one LSTM with multiple\nfeature inputs, a second LSTM trained with speaker-adversarial multi-task\nlearning and a third convolutional residual net (ResNet). On the language\nmodeling side, we use word and character LSTMs and convolutional WaveNet-style\nlanguage models.\n"
      ],
      "doi": "10.21437/Interspeech.2017-405"
    },
    "stolcke17_interspeech": {
      "authors": [
        [
          "Andreas",
          "Stolcke"
        ],
        [
          "Jasha",
          "Droppo"
        ]
      ],
      "title": "Comparing Human and Machine Errors in Conversational Speech Transcription",
      "original": "1544",
      "page_count": 5,
      "order": 29,
      "p1": "137",
      "pn": "141",
      "abstract": [
        "Recent work in automatic recognition of conversational telephone speech\n(CTS) has achieved accuracy levels comparable to human transcribers,\nalthough there is some debate how to precisely quantify human performance\non this task, using the NIST 2000 CTS evaluation set. This raises the\nquestion what systematic differences, if any, may be found differentiating\nhuman from machine transcription errors. In this paper we approach\nthis question by comparing the output of our most accurate CTS recognition\nsystem to that of a standard speech transcription vendor pipeline.\nWe find that the most frequent substitution, deletion and insertion\nerror types of both outputs show a high degree of overlap. The only\nnotable exception is that the automatic recognizer tends to confuse\nfilled pauses (&#8220;uh&#8221;) and backchannel acknowledgments (&#8220;uhhuh&#8221;).\nHuman tend not to make this error, presumably due to the distinctive\nand opposing pragmatic functions attached to these words. Furthermore,\nwe quantify the correlation between human and machine errors at the\nspeaker level, and investigate the effect of speaker overlap between\ntraining and test data. Finally, we report on an informal &#8220;Turing\ntest&#8221; asking humans to discriminate between automatic and human\ntranscription error cases.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1544"
    },
    "petukhova17_interspeech": {
      "authors": [
        [
          "Volha",
          "Petukhova"
        ],
        [
          "Manoj",
          "Raju"
        ],
        [
          "Harry",
          "Bunt"
        ]
      ],
      "title": "Multimodal Markers of Persuasive Speech: Designing a Virtual Debate Coach",
      "original": "0098",
      "page_count": 5,
      "order": 30,
      "p1": "142",
      "pn": "146",
      "abstract": [
        "The study presented in this paper is carried out to support debate\nperformance assessment in the context of debate skills training. The\nperception of good performance as a debater is influenced by how believable\nand convincing the debater&#8217;s argumentation is. We identified\na number of features that are useful for explaining perceived properties\nof persuasive speech and for defining rules and strategies to produce\nand assess debate performance. We collected and analysed multimodal\nand multisensory data of the trainees debate behaviour, and contrasted\nit with those of skilled professional debaters. Observational, correlation\nand machine learning studies were performed to identify multimodal\nmarkers of persuasive speech and link them to experts&#8217; assessments.\nA combination of multimodal in- and out-of-domain debate data, and\nvarious non-verbal, prosodic, lexical, linguistic and structural features\nhas been computed based on our analysis and assessed used to , and\nseveral classification procedures has been applied achieving an accuracy\nof 0.79 on spoken debate data.\n"
      ],
      "doi": "10.21437/Interspeech.2017-98"
    },
    "bone17_interspeech": {
      "authors": [
        [
          "Daniel",
          "Bone"
        ],
        [
          "Julia",
          "Mertens"
        ],
        [
          "Emily",
          "Zane"
        ],
        [
          "Sungbok",
          "Lee"
        ],
        [
          "Shrikanth S.",
          "Narayanan"
        ],
        [
          "Ruth",
          "Grossman"
        ]
      ],
      "title": "Acoustic-Prosodic and Physiological Response to Stressful Interactions in Children with Autism Spectrum Disorder",
      "original": "0179",
      "page_count": 5,
      "order": 31,
      "p1": "147",
      "pn": "151",
      "abstract": [
        "Social anxiety is a prevalent condition affecting individuals to varying\ndegrees. Research on autism spectrum disorder (ASD), a group of neurodevelopmental\ndisorders marked by impairments in social communication, has found\nthat social anxiety occurs more frequently in this population. Our\nstudy aims to further understand the multimodal manifestation of social\nstress for adolescents with ASD versus neurotypically developing (TD)\npeers. We investigate this through objective measures of speech behavior\nand physiology (mean heart rate) acquired during three tasks: a low-stress\nconversation, a medium-stress interview, and a high-stress presentation.\nMeasurable differences are found to exist for speech behavior and heart\nrate in relation to task-induced stress. Additionally, we find the\nacoustic measures are particularly effective for distinguishing between\ndiagnostic groups. Individuals with ASD produced higher prosodic variability,\nagreeing with previous reports. Moreover, the most informative features\ncaptured an individual&#8217;s vocal changes between low and high social-stress,\nsuggesting an interaction between vocal production and social stressors\nin ASD.\n"
      ],
      "doi": "10.21437/Interspeech.2017-179"
    },
    "burmania17_interspeech": {
      "authors": [
        [
          "Alec",
          "Burmania"
        ],
        [
          "Carlos",
          "Busso"
        ]
      ],
      "title": "A Stepwise Analysis of Aggregated Crowdsourced Labels Describing Multimodal Emotional Behaviors",
      "original": "1278",
      "page_count": 5,
      "order": 32,
      "p1": "152",
      "pn": "156",
      "abstract": [
        "Affect recognition is a difficult problem that most often relies on\nhuman annotated data to train automated systems. As humans perceive\nemotion differently based on personality, cognitive state and past\nexperiences, it is important to collect rankings from multiple individuals\nto assess the emotional content in corpora, which are later aggregated\nwith rules such as majority vote. With the increased use of crowdsourcing\nservices for perceptual evaluations, collecting large amount of data\nis now feasible. It becomes important to question the amount of data\nneeded to create well-trained classifiers. How different are the aggregated\nlabels collected from five raters compared to the ones obtained from\ntwenty evaluators? Is it worthwhile to spend resources to increase\nthe number of evaluators beyond those used in conventional/laboratory\nstudies? This study evaluates the consensus labels obtained by incrementally\nadding new evaluators during perceptual evaluations. Using majority\nvote over categorical emotional labels, we compare the changes in the\naggregated labels starting with one rater, and finishing with 20 raters.\nThe large number of evaluators in a subset of the MSP-IMPROV database\nand the ability to filter annotators by quality allows us to better\nunderstand label aggregation as a function of the number of annotators.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1278"
    },
    "fotedar17_interspeech": {
      "authors": [
        [
          "Gaurav",
          "Fotedar"
        ],
        [
          "Prasanta Kumar",
          "Ghosh"
        ]
      ],
      "title": "An Information Theoretic Analysis of the Temporal Synchrony Between Head Gestures and Prosodic Patterns in Spontaneous Speech",
      "original": "0999",
      "page_count": 5,
      "order": 33,
      "p1": "157",
      "pn": "161",
      "abstract": [
        "We analyze the temporal co-ordination between head gestures and prosodic\npatterns in spontaneous speech in a data-driven manner. For this study,\nwe consider head motion and speech data from 24 subjects while they\ntell a fixed set of five stories. The head motion, captured using a\nmotion capture system, is converted to Euler angles and translations\nin X, Y and Z-directions to represent head gestures. Pitch and short-time\nenergy in voiced segments are used to represent the prosodic patterns.\nTo capture the statistical relationship between head gestures and prosodic\npatterns, mutual information (MI) is computed at various delays between\nthe two using data from 24 subjects in six native languages. The estimated\nMI, averaged across all subjects, is found to be maximum when the head\ngestures lag the prosodic patterns by 30msec. This is found to be true\nwhen subjects tell stories in English as well as in their native language.\nWe observe a similar pattern in the root mean squared error of predicting\nhead gestures from prosodic patterns using Gaussian mixture model.\nThese results indicate that there could be an asynchrony between head\ngestures and prosody during spontaneous speech where head gestures\nfollow the corresponding prosodic patterns.\n"
      ],
      "doi": "10.21437/Interspeech.2017-999"
    },
    "huang17_interspeech": {
      "authors": [
        [
          "D.-Y.",
          "Huang"
        ],
        [
          "Wan",
          "Ding"
        ],
        [
          "Mingyu",
          "Xu"
        ],
        [
          "Huaiping",
          "Ming"
        ],
        [
          "Minghui",
          "Dong"
        ],
        [
          "Xinguo",
          "Yu"
        ],
        [
          "Haizhou",
          "Li"
        ]
      ],
      "title": "Multimodal Prediction of Affective Dimensions via Fusing Multiple Regression Techniques",
      "original": "1088",
      "page_count": 4,
      "order": 34,
      "p1": "162",
      "pn": "165",
      "abstract": [
        "This paper presents a multimodal approach to predict affective dimensions,\nthat makes full use of features from audio, video, Electrodermal Activity\n(EDA) and Electrocardiogram (ECG) using three regression techniques\nsuch as support vector regression (SVR), partial least squares regression\n(PLS), and a deep bidirectional long short-term memory recurrent neural\nnetwork (DBLSTM-RNN) regression. Each of the three regression techniques\nperforms multimodal affective dimension prediction followed by a fusion\nof different models on features of four modalities using a support\nvector regression. A support vector regression is also applied for\na final fusion of the three regression systems. Experiments show that\nour proposed approach obtains promising results on the AVEC 2015 benchmark\ndataset for prediction of multimodal affective dimensions. For the\ndevelopment set, the concordance correlation coefficient (CCC) achieves\nresults of 0.856 for arousal and 0.720 for valence, which increases\n3.88% and 4.66% of the top-performer of AVEC 2015 in arousal and valence,\nrespectively.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1088"
    },
    "dohen17_interspeech": {
      "authors": [
        [
          "Marion",
          "Dohen"
        ],
        [
          "Benjamin",
          "Roustan"
        ]
      ],
      "title": "Co-Production of Speech and Pointing Gestures in Clear and Perturbed Interactive Tasks: Multimodal Designation Strategies",
      "original": "1329",
      "page_count": 5,
      "order": 35,
      "p1": "166",
      "pn": "170",
      "abstract": [
        "Designation consists in attracting an interlocutor&#8217;s attention\non a specific object and/or location. It is most often achieved using\nboth speech (e.g., demonstratives) and gestures (e.g., manual pointing).\nThis study aims at analyzing how speech and pointing gestures are co-produced\nin a semi-directed interactive task involving designation. 20 native\nspeakers of French were involved in a cooperative task in which they\nprovided instructions to a partner for her to reproduce a model she\ncould not see on a grid both of them saw. They had to use only sentences\nof the form &#8216;The [target word] goes there.&#8217;. They did this\nin two conditions: silence and noise. Their speech and articulatory/hand\nmovements (motion capture) were recorded. The analyses show that the\nparticipants&#8217; speech features were modified in noise (Lombard\neffect). They also spoke slower and made more pauses and errors. Their\npointing gestures lasted longer and started later showing an adaptation\nof gesture production to speech. The condition did not influence speech/gesture\ncoordination. The apex (part of the gesture that shows) mainly occurred\nat the same time as the target word and not as the demonstrative showing\nthat speakers group speech and gesture carrying complementary rather\nthan redundant information.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1329"
    },
    "guzewich17_interspeech": {
      "authors": [
        [
          "Peter",
          "Guzewich"
        ],
        [
          "Stephen A.",
          "Zahorian"
        ]
      ],
      "title": "Improving Speaker Verification for Reverberant Conditions with Deep Neural Network Dereverberation Processing",
      "original": "0461",
      "page_count": 5,
      "order": 36,
      "p1": "171",
      "pn": "175",
      "abstract": [
        "We present an improved method for training Deep Neural Networks for\ndereverberation and show that it can improve performance for the speech\nprocessing tasks of speaker verification and speech enhancement. We\nreplicate recently proposed methods for dereverberation using Deep\nNeural Networks and present our improved method, highlighting important\naspects that influence performance. We then experimentally evaluate\nthe capabilities and limitations of the method with respect to speech\nquality and speaker verification to show that ours achieves better\nperformance than other proposed methods.\n"
      ],
      "doi": "10.21437/Interspeech.2017-461"
    },
    "bulling17_interspeech": {
      "authors": [
        [
          "Philipp",
          "Bulling"
        ],
        [
          "Klaus",
          "Linhard"
        ],
        [
          "Arthur",
          "Wolf"
        ],
        [
          "Gerhard",
          "Schmidt"
        ]
      ],
      "title": "Stepsize Control for Acoustic Feedback Cancellation Based on the Detection of Reverberant Signal Periods and the Estimated System Distance",
      "original": "0046",
      "page_count": 5,
      "order": 37,
      "p1": "176",
      "pn": "180",
      "abstract": [
        "A new approach for acoustic feedback cancellation is presented. The\nchallenge in acoustic feedback cancellation is a strong correlation\nbetween the local speech and the loudspeaker signal. Due to this correlation,\nthe convergence rate of adaptive algorithms is limited. Therefore,\na novel stepsize control of the adaptive filter is presented. The stepsize\ncontrol exploits reverberant signal periods to update the adaptive\nfilter. As soon as local speech stops, the reverberation energy of\nthe system decays exponentially. This means that during reverberation\nthere is only excitation of the filter but no local speech. Thus, signals\nare not correlated and the filter can converge without correlation\nproblems. Consequently, the stepsize control accelerates the adaption\nprocess during reverberation and slows it down at the beginning of\nspeech activity. It is shown, that with a particular gain control,\nthe reverb-based stepsize control can be interpreted as the theoretical\noptimum stepsize. However, for this purpose a precise estimation of\nthe system distance is required. One estimation method is presented.\nThe proposed estimator has a rescue mechanism to detect enclosure dislocations.\nBoth, simulations and real world testing show that the acoustic feedback\ncanceler is capable of improving stability and convergence rate, even\nat high system gains.\n"
      ],
      "doi": "10.21437/Interspeech.2017-46"
    },
    "franzen17_interspeech": {
      "authors": [
        [
          "Jan",
          "Franzen"
        ],
        [
          "Tim",
          "Fingscheidt"
        ]
      ],
      "title": "A Delay-Flexible Stereo Acoustic Echo Cancellation for DFT-Based In-Car Communication (ICC) Systems",
      "original": "1084",
      "page_count": 5,
      "order": 38,
      "p1": "181",
      "pn": "185",
      "abstract": [
        "In-car communication (ICC) systems supporting speech communication\nin noise by reproducing amplified speech from the car cabin in the\ncar cabin ask for low-delay acoustic echo cancellation (AEC). In this\npaper we propose a delay-flexible DFT-based stereo AEC capable of cancelling\nalso the echoes stemming from the audio player or FM radio. For the\nprice of a somewhat higher complexity we are able to reduce the 32\nms delay of the baseline down to 4 ms, loosing only 1 dB in ERLE while\neven preserving system distance properties.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1084"
    },
    "wang17b_interspeech": {
      "authors": [
        [
          "Dongmei",
          "Wang"
        ],
        [
          "John H.L.",
          "Hansen"
        ]
      ],
      "title": "Speech Enhancement Based on Harmonic Estimation Combined with MMSE to Improve Speech Intelligibility for Cochlear Implant Recipients",
      "original": "0078",
      "page_count": 5,
      "order": 39,
      "p1": "186",
      "pn": "190",
      "abstract": [
        "In this paper, a speech enhancement algorithm is proposed to improve\nthe speech intelligibility for cochlear implant recipients. Our method\nis based on combination of harmonic estimation and traditional statistical\nmethod. Traditional statistical based speech enhancement method is\neffective only for stationary noise suppression, but not non-stationary\nnoise. To address more complex noise scenarios, we explore the harmonic\nstructure of target speech to obtain a more accurate noise estimation.\nThe estimated noise is then employed in the MMSE framework to obtain\nthe gain function for recovering the target speech. Listening test\nexperiments show a substantial speech intelligibility improvement for\ncochlear implant recipients in noisy environments.\n"
      ],
      "doi": "10.21437/Interspeech.2017-78"
    },
    "ayllon17_interspeech": {
      "authors": [
        [
          "David",
          "Ayll\u00f3n"
        ],
        [
          "Roberto",
          "Gil-Pita"
        ],
        [
          "Manuel",
          "Rosa-Zurera"
        ]
      ],
      "title": "Improving Speech Intelligibility in Binaural Hearing Aids by Estimating a Time-Frequency Mask with a Weighted Least Squares Classifier",
      "original": "0771",
      "page_count": 5,
      "order": 40,
      "p1": "191",
      "pn": "195",
      "abstract": [
        "An efficient algorithm for speech enhancement in binaural hearing aids\nis proposed. The algorithm is based on the estimation of a time-frequency\nmask using supervised machine learning. The standard least-squares\nlinear classifier is reformulated to optimize a metric related to speech/noise\nseparation. The method is energy-efficient in two ways: the computational\ncomplexity is limited and the wireless data transmission optimized.\nThe ability of the algorithm to enhance speech contaminated with different\ntypes of noise and low SNR has been evaluated. Objective measures of\nspeech intelligibility and speech quality demonstrate that the algorithm\nincrements both the hearing comfort and speech understanding of the\nuser. These results are supported by subjective listening tests.\n"
      ],
      "doi": "10.21437/Interspeech.2017-771"
    },
    "wu17_interspeech": {
      "authors": [
        [
          "Tsung-Chen",
          "Wu"
        ],
        [
          "Tai-Shih",
          "Chi"
        ],
        [
          "Chia-Fone",
          "Lee"
        ]
      ],
      "title": "Simulations of High-Frequency Vocoder on Mandarin Speech Recognition for Acoustic Hearing Preserved Cochlear Implant",
      "original": "0858",
      "page_count": 5,
      "order": 41,
      "p1": "196",
      "pn": "200",
      "abstract": [
        "Vocoder simulations are generally adopted to simulate the electrical\nhearing induced by the cochlear implant (CI). Our research group is\ndeveloping a new four-electrode CI microsystem which induces high-frequency\nelectrical hearing while preserving low-frequency acoustic hearing.\nTo simulate the functionality of this CI, a previously developed hearing-impaired\n(HI) hearing model is combined with a 4-channel vocoder in this paper\nto respectively mimic the perceived acoustic hearing and electrical\nhearing. Psychoacoustic experiments are conducted on Mandarin speech\nrecognition for determining parameters of electrodes for this CI. Simulation\nresults show that initial consonants of Mandarin are more difficult\nto recognize than final vowels of Mandarin via acoustic hearing of\nHI patients. After electrical hearing being induced through logarithmic-frequency\ndistributed electrodes, speech intelligibility of HI patients is boosted\nfor all Mandarin phonemes, especially for initial consonants. Similar\nresults are consistently observed in clean and noisy test conditions.\n"
      ],
      "doi": "10.21437/Interspeech.2017-858"
    },
    "hermes17_interspeech": {
      "authors": [
        [
          "Zainab",
          "Hermes"
        ],
        [
          "Marissa",
          "Barlaz"
        ],
        [
          "Ryan",
          "Shosted"
        ],
        [
          "Zhi-Pei",
          "Liang"
        ],
        [
          "Brad",
          "Sutton"
        ]
      ],
      "title": "Phonetic Correlates of Pharyngeal and Pharyngealized Consonants in Saudi, Lebanese, and Jordanian Arabic: An rt-MRI Study",
      "original": "1601",
      "page_count": 5,
      "order": 42,
      "p1": "201",
      "pn": "205",
      "abstract": [
        "The phonemic inventory of Arabic includes sounds that involve a pharyngeal\nconstriction. Sounds referred to as &#8216;pharyngeal&#8217; (/&#x295;/\nand /&#x127;/) are reported to have a primary constriction in the pharynx,\nwhile sounds referred to as &#8216;pharyngealized&#8217; (/s<sup>&#x295;</sup>/,\n/t<sup>&#x295;</sup>/, /d<sup>&#x295;</sup>/, and /&#240;<sup>&#x295;</sup>/\nor /z<sup>&#x295;</sup>/) are reported to have a secondary constriction\nin the pharynx. Some studies propose grouping both types of sounds\ntogether, citing phonetic and phonological evidence. Phonetically,\npharyngeal consonants are argued to have a primary constriction below\nthe pharynx, and are thus posited to be pharyngealized laryngeals.\nUnder this view, the pharyngeal constriction is secondary, not primary.\nPhonologically, it has been established that pharyngealized sounds\ntrigger pharyngealization spread, and proposals for grouping pharyngeal\nand pharyngealized consonants together cite similar, but not identical,\nspread patterns triggered by pharyngeals. In this study, Real-time\nMagnetic Resonance Imaging is employed to investigate the phonetic\ncorrelates of the pharyngeal constriction in both pharyngeal and pharyngealized\nsounds in Saudi, Lebanese, and Jordanian Arabic as exemplified by one\nspeaker from each dialect. Our findings demonstrate a difference in\nthe location of constriction among both types of sounds. These distinctions\nin place possibly account for the differences in the spread patterns\ntriggered by each type of sound.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1601"
    },
    "elie17_interspeech": {
      "authors": [
        [
          "Benjamin",
          "Elie"
        ],
        [
          "Yves",
          "Laprie"
        ]
      ],
      "title": "Glottal Opening and Strategies of Production of Fricatives",
      "original": "1039",
      "page_count": 4,
      "order": 43,
      "p1": "206",
      "pn": "209",
      "abstract": [
        "This work investigates the influence of the gradual opening of the\nglottis along its length during the production of fricatives in intervocalic\ncontexts. Acoustic simulations reveal the existence of a transient\nzone in the articulatory space where the frication noise level is very\nsensitive to small perturbations of the glottal opening. This corresponds\nto the configurations where both frication noise and voiced contributions\nare present in the speech signal. To avoid this unstability, speakers\nmay adopt different strategies to ensure the voiced/voiceless contrast\nof fricatives. This is evidenced by experimental data of simultaneous\nglottal opening measurements, performed with ePGG, and audio recordings\nof vowel-fricative-vowel pseudowords. Voiceless fricatives are usually\nlonger, in order to maximize the number of voiceless time frames over\nvoiced frames due to the crossing of the transient regime. For voiced\nfricatives, the speaker may avoid the unstable regime by keeping low\nfrication noise level, and thus by favoring the voicing characteristic,\nor by doing very short crossings into the unstable regime. It is also\nshown that when speakers are asked to sustain voiced fricatives longer\nthan in natural speech, they adopt the strategy of keeping low frication\nnoise level to avoid the unstable regime.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1039"
    },
    "frej17_interspeech": {
      "authors": [
        [
          "Mohamed Yassine",
          "Frej"
        ],
        [
          "Christopher",
          "Carignan"
        ],
        [
          "Catherine T.",
          "Best"
        ]
      ],
      "title": "Acoustics and Articulation of Medial versus Final Coronal Stop Gemination Contrasts in Moroccan Arabic",
      "original": "1292",
      "page_count": 5,
      "order": 44,
      "p1": "210",
      "pn": "214",
      "abstract": [
        "This paper presents results of a simultaneous acoustic and articulatory\ninvestigation of word-medial and word-final geminate/singleton coronal\nstop contrasts in Moroccan Arabic (MA). The acoustic analysis revealed\nthat, only for the word-medial contrast, the two MA speakers adopted\ncomparable strategies in contrasting geminates with singletons, mainly\nby significantly lengthening closure duration in geminates, relative\nto singletons. In word-final position, two speaker-specific contrasting\npatterns emerged. While one speaker also lengthened the closure duration\nfor final geminates, the other speaker instead lengthened only the\nrelease duration for final geminates, relative to singletons. Consonant\nclosure and preceding vowel were significantly longer for the geminate\nonly in medial position, not in final position. These temporal differences\nwere even more clearly delineated in the articulatory signal, captured\nvia ultrasound, to which we applied the novel approach of using TRACTUS\n[Temporally Resolved Articulatory Configuration Tracking of UltraSound:\n15] to index temporal properties of closure gestures for these geminate/singleton\ncontrasts.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1292"
    },
    "turco17_interspeech": {
      "authors": [
        [
          "Giuseppina",
          "Turco"
        ],
        [
          "Karim",
          "Shoul"
        ],
        [
          "Rachid",
          "Ridouane"
        ]
      ],
      "title": "How are Four-Level Length Distinctions Produced? Evidence from Moroccan Arabic",
      "original": "1553",
      "page_count": 4,
      "order": 45,
      "p1": "215",
      "pn": "218",
      "abstract": [
        "We investigate the durational properties of Moroccan Arabic identical\nconsonant sequences contrasting singleton (S) and geminate (G) dental\nfricatives, in six combinations of four-level length contrasts across\nword boundaries (#) (one timing slot for #S, two for #G and S#S, three\nfor S#G and G#S, and four for G#G). The aim is to determine the nature\nof the mapping between discrete phonological timing units and phonetic\ndurations. Acoustic results show that the largest and most systematic\njump in duration is displayed between the singleton fricative on the\none hand and the other sequences on the other hand. Looking at these\nsequences, S#S is shown to have the same duration as #G. When a geminate\nis within the sequence, a temporal reorganization is observed: G#S\nis not significantly longer than S#S and #G; and G#G is only slightly\nlonger than S#G. Instead of a four-way hierarchy, our data point towards\na possible upper limit of three-way length contrasts for consonants:\nS &#60; G=S#S=G#S &#60; S#G=G#G. The interplay of a number of factors\nresulting in this mismatch between phonological length and phonetic\nduration are discussed, and a working hypothesis is provided for why\nduration contrasts are rarely ternary, and almost never quaternary.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1553"
    },
    "jones17_interspeech": {
      "authors": [
        [
          "Caroline",
          "Jones"
        ],
        [
          "Katherine",
          "Demuth"
        ],
        [
          "Weicong",
          "Li"
        ],
        [
          "Andre",
          "Almeida"
        ]
      ],
      "title": "Vowels in the Barunga Variety of North Australian Kriol",
      "original": "1552",
      "page_count": 5,
      "order": 46,
      "p1": "219",
      "pn": "223",
      "abstract": [
        "North Australian Kriol is an English based creole spoken widely by\nIndigenous people in northern Australia in areas where the traditional\nlanguages are endangered or no longer spoken. This paper offers the\nfirst acoustic description of the vowel phonology of Roper Kriol, within\na variety spoken at Barunga Community, east of the town of Katherine\nin the Northern Territory.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  Drawing on a new corpus\nfor Barunga Kriol, the paper presents analyses of the short and long\nmonophthongs, as well as the diphthongs in the spontaneous speech of\nyoung adults. The results show the durations and spectral characteristics\nof the vowels, including major patterns of allophony (i.e. coarticulation\nand context effects). This updates the phonology over the previous\ndescription from the 1970s, showing that there is an additional front\nlow vowel phoneme in the speech of young people today, as well as a\nvowel length contrast. Interestingly there are points of similarity\nwith the vowel acoustics for traditional Aboriginal languages of the\nregion, for example in a relatively compact vowel space and in the\nmodest trajectories of diphthongs.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1552"
    },
    "dutta17_interspeech": {
      "authors": [
        [
          "Indranil",
          "Dutta"
        ],
        [
          "Irfan",
          "S."
        ],
        [
          "Pamir",
          "Gogoi"
        ],
        [
          "Priyankoo",
          "Sarmah"
        ]
      ],
      "title": "Nature of Contrast and Coarticulation: Evidence from Mizo Tones and Assamese Vowel Harmony",
      "original": "1304",
      "page_count": 5,
      "order": 47,
      "p1": "224",
      "pn": "228",
      "abstract": [
        "Tonal coarticulation is universally found to be greater in extent in\nthe carryover direction compared to the anticipatory direction ([1],\n[2], [3], [4], [5]) leading to assimilatory processes. In general,\ncarryover coarticulation has been understood to be due to intertio-mechanical\nforces, and, anticipatory effects are seen to be a consequence of parallel\nactivation of articulatory plans ([6]). In this paper, we report on\nresults from a set of Artificial Neural Networks (ANN) trained to predict\nadjacent tones in disyllabic sequences. Our results confirm the universal\npattern of greater carryover effects in Mizo leading to tonal assimilation.\nIn addition, we report on results from single-layered ANN models and\nSupport Vector Machines (SVM) that predict the identity of V<SUB>2</SUB>\nfrom V<SUB>1</SUB> (anticipatory) consistently better than V<SUB>1</SUB>\nfrom V<SUB>2</SUB> (carryover) in Assamese non-harmonic #&#8230;V<SUB>1</SUB>CV<SUB>2</SUB>&#8230;#\nsequences. The directionality in the performance of the V<SUB>1</SUB>\nand V<SUB>2</SUB> models, help us conclude that the directionality\neffect of coarticulation in Assamese non-harmonic sequences is greater\nin the anticipatory direction, which is the same direction as in the\nharmonic sequences. We argue that coarticulatory propensity exhibits\na great deal of sensitivity to the nature of contrast in a language.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1304"
    },
    "cabral17_interspeech": {
      "authors": [
        [
          "Jo\u00e3o Paulo",
          "Cabral"
        ],
        [
          "Benjamin R.",
          "Cowan"
        ],
        [
          "Katja",
          "Zibrek"
        ],
        [
          "Rachel",
          "McDonnell"
        ]
      ],
      "title": "The Influence of Synthetic Voice on the Evaluation of a Virtual Character",
      "original": "0325",
      "page_count": 5,
      "order": 48,
      "p1": "229",
      "pn": "233",
      "abstract": [
        "Graphical realism and the naturalness of the voice used are important\naspects to consider when designing a virtual agent or character. In\nthis work, we evaluate how synthetic speech impacts people&#8217;s\nperceptions of a rendered virtual character. Using a controlled experiment,\nwe focus on the role that speech, in particular voice expressiveness\nin the form of personality, has on the assessment of voice level and\ncharacter level perceptions. We found that people rated a real human\nvoice as more expressive, understandable and likeable than the expressive\nsynthetic voice we developed. Contrary to our expectations, we found\nthat the voices did not have a significant impact on the character\nlevel judgments; people in the voice conditions did not significantly\nvary on their ratings of appeal, credibility, human-likeness and voice\nmatching the character. The implications this has for character design\nand how this compares with previous work are discussed.\n"
      ],
      "doi": "10.21437/Interspeech.2017-325"
    },
    "gully17_interspeech": {
      "authors": [
        [
          "Amelia J.",
          "Gully"
        ],
        [
          "Takenori",
          "Yoshimura"
        ],
        [
          "Damian T.",
          "Murphy"
        ],
        [
          "Kei",
          "Hashimoto"
        ],
        [
          "Yoshihiko",
          "Nankaku"
        ],
        [
          "Keiichi",
          "Tokuda"
        ]
      ],
      "title": "Articulatory Text-to-Speech Synthesis Using the Digital Waveguide Mesh Driven by a Deep Neural Network",
      "original": "0900",
      "page_count": 5,
      "order": 49,
      "p1": "234",
      "pn": "238",
      "abstract": [
        "Following recent advances in direct modeling of the speech waveform\nusing a deep neural network, we propose a novel method that directly\nestimates a physical model of the vocal tract from the speech waveform,\nrather than magnetic resonance imaging data. This provides a clear\nrelationship between the model and the size and shape of the vocal\ntract, offering considerable flexibility in terms of speech characteristics\nsuch as age and gender. Initial tests indicate that despite a highly\nsimplified physical model, intelligible synthesized speech is obtained.\nThis illustrates the potential of the combined technique for the control\nof physical models in general, and hence the generation of more natural-sounding\nsynthetic speech.\n"
      ],
      "doi": "10.21437/Interspeech.2017-900"
    },
    "maguer17_interspeech": {
      "authors": [
        [
          "S\u00e9bastien Le",
          "Maguer"
        ],
        [
          "Ingmar",
          "Steiner"
        ],
        [
          "Alexander",
          "Hewer"
        ]
      ],
      "title": "An HMM/DNN Comparison for Synchronized Text-to-Speech and Tongue Motion Synthesis",
      "original": "0936",
      "page_count": 5,
      "order": 50,
      "p1": "239",
      "pn": "243",
      "abstract": [
        "We present an end-to-end text-to-speech (TTS) synthesis system that\ngenerates audio and synchronized tongue motion directly from text.\nThis is achieved by adapting a statistical shape space model of the\ntongue surface to an articulatory speech corpus and training a speech\nsynthesis system directly on the tongue model parameter weights. We\nfocus our analysis on the application of two standard methodologies,\nbased on Hidden Markov Models (HMMs) and Deep Neural Networks (DNNs),\nrespectively, to train both acoustic models and the tongue model parameter\nweights. We evaluate both methodologies at every step by comparing\nthe predicted articulatory movements against the reference data. The\nresults show that even with less than 2h of data, DNNs already outperform\nHMMs.\n"
      ],
      "doi": "10.21437/Interspeech.2017-936"
    },
    "alexander17_interspeech": {
      "authors": [
        [
          "Rachel",
          "Alexander"
        ],
        [
          "Tanner",
          "Sorensen"
        ],
        [
          "Asterios",
          "Toutios"
        ],
        [
          "Shrikanth S.",
          "Narayanan"
        ]
      ],
      "title": "VCV Synthesis Using Task Dynamics to Animate a Factor-Based Articulatory Model",
      "original": "1410",
      "page_count": 5,
      "order": 51,
      "p1": "244",
      "pn": "248",
      "abstract": [
        "This paper presents an initial architecture for articulatory synthesis\nwhich combines a dynamical system for the control of vocal tract shaping\nwith a novel MATLAB implementation of an articulatory synthesizer.\nThe dynamical system controls a speaker-specific vocal tract model\nderived by factor analysis of mid-sagittal real-time MRI data and provides\ninput to the articulatory synthesizer, which simulates the propagation\nof sound waves in the vocal tract. First, parameters of the dynamical\nsystem are estimated from real-time MRI data of human speech production.\nSecond, vocal-tract dynamics is simulated for vowel-consonant-vowel\nutterances using a sequence of two dynamical systems: the first one\nstarts from a vowel vocal-tract configuration and achieves a vocal-tract\nclosure; the second one starts from the closure and achieves the target\nconfiguration of the second vowel. Third, vocal-tract dynamics is converted\nto area function dynamics and is input to the synthesizer to generate\nthe acoustic signal. Synthesized vowel-consonant-vowel examples demonstrate\nthe feasibility of the method.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1410"
    },
    "mendelson17_interspeech": {
      "authors": [
        [
          "Joseph",
          "Mendelson"
        ],
        [
          "Matthew P.",
          "Aylett"
        ]
      ],
      "title": "Beyond the Listening Test: An Interactive Approach to TTS Evaluation",
      "original": "1438",
      "page_count": 5,
      "order": 52,
      "p1": "249",
      "pn": "253",
      "abstract": [
        "Traditionally, subjective text-to-speech (TTS) evaluation is performed\nthrough audio-only listening tests, where participants evaluate unrelated,\ncontext-free utterances. The ecological validity of these tests is\nquestionable, as they do not represent real-world end-use scenarios.\nIn this paper, we examine a novel approach to TTS evaluation in an\nimagined end-use, via a complex interaction with an avatar. 6 different\nvoice conditions were tested: Natural speech, Unit Selection and Parametric\nSynthesis, in neutral and expressive realizations. Results were compared\nto a traditional audio-only evaluation baseline. Participants in both\nstudies rated the voices for naturalness and expressivity. The baseline\nstudy showed canonical results for naturalness: Natural speech scored\nhighest, followed by Unit Selection, then Parametric synthesis. Expressivity\nwas clearly distinguishable in all conditions. In the avatar interaction\nstudy, participants rated naturalness in the same order as the baseline,\nthough with smaller effect size; expressivity was not distinguishable.\nFurther, no significant correlations were found between cognitive or\naffective responses and any voice conditions. This highlights 2 primary\nchallenges in designing more valid TTS evaluations: in real-world use-cases\ninvolving interaction, listeners generally interact with a single voice,\nmaking comparative analysis unfeasible, and in complex interactions,\nthe context and content may confound perception of voice quality.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1438"
    },
    "cao17_interspeech": {
      "authors": [
        [
          "Beiming",
          "Cao"
        ],
        [
          "Myungjong",
          "Kim"
        ],
        [
          "Jan van",
          "Santen"
        ],
        [
          "Ted",
          "Mau"
        ],
        [
          "Jun",
          "Wang"
        ]
      ],
      "title": "Integrating Articulatory Information in Deep Learning-Based Text-to-Speech Synthesis",
      "original": "1762",
      "page_count": 5,
      "order": 53,
      "p1": "254",
      "pn": "258",
      "abstract": [
        "Articulatory information has been shown to be effective in improving\nthe performance of hidden Markov model (HMM)-based text-to-speech (TTS)\nsynthesis. Recently, deep learning-based TTS has outperformed HMM-based\napproaches. However, articulatory information has rarely been integrated\nin deep learning-based TTS. This paper investigated the effectiveness\nof integrating articulatory movement data to deep learning-based TTS.\nThe integration of articulatory information was achieved in two ways:\n(1) direct integration, where articulatory and acoustic features were\nthe output of a deep neural network (DNN), and (2) direct integration\nplus forward-mapping, where the output articulatory features were mapped\nto acoustic features by an additional DNN; These forward-mapped acoustic\nfeatures were then combined with the output acoustic features to produce\nthe final acoustic features. Articulatory (tongue and lip) and acoustic\ndata collected from male and female speakers were used in the experiment.\nBoth objective measures and subjective judgment by human listeners\nshowed the approaches integrated articulatory information outperformed\nthe baseline approach (without using articulatory information) in terms\nof naturalness and speaker voice identity (voice similarity).\n"
      ],
      "doi": "10.21437/Interspeech.2017-1762"
    },
    "ma17b_interspeech": {
      "authors": [
        [
          "Min",
          "Ma"
        ],
        [
          "Michael",
          "Nirschl"
        ],
        [
          "Fadi",
          "Biadsy"
        ],
        [
          "Shankar",
          "Kumar"
        ]
      ],
      "title": "Approaches for Neural-Network Language Model Adaptation",
      "original": "1310",
      "page_count": 5,
      "order": 54,
      "p1": "259",
      "pn": "263",
      "abstract": [
        "Language Models (LMs) for Automatic Speech Recognition (ASR) are typically\ntrained on large text corpora from news articles, books and web documents.\nThese types of corpora, however, are unlikely to match the test distribution\nof ASR systems, which expect spoken utterances. Therefore, the LM is\ntypically adapted to a smaller held-out in-domain dataset that is drawn\nfrom the test distribution. We propose three LM adaptation approaches\nfor Deep NN and Long Short-Term Memory (LSTM): (1) Adapting the softmax\nlayer in the Neural Network (NN); (2) Adding a non-linear adaptation\nlayer before the softmax layer that is trained only in the adaptation\nphase; (3) Training the extra non-linear adaptation layer in pre-training\nand adaptation phases. Aiming to improve upon a hierarchical Maximum\nEntropy (MaxEnt) second-pass LM baseline, which factors the model into\nword-cluster and word models, we build an NN LM that predicts only\nword clusters. Adapting the LSTM LM by training the adaptation layer\nin both training and adaptation phases (Approach 3), we reduce the\ncluster perplexity by 30% on a held-out dataset compared to an unadapted\nLSTM LM. Initial experiments using a state-of-the-art ASR system show\na 2.3% relative reduction in WER on top of an adapted MaxEnt LM.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1310"
    },
    "oualil17_interspeech": {
      "authors": [
        [
          "Youssef",
          "Oualil"
        ],
        [
          "Dietrich",
          "Klakow"
        ]
      ],
      "title": "A Batch Noise Contrastive Estimation Approach for Training Large Vocabulary Language Models",
      "original": "0818",
      "page_count": 5,
      "order": 55,
      "p1": "264",
      "pn": "268",
      "abstract": [
        "Training large vocabulary Neural Network Language Models (NNLMs) is\na difficult task due to the explicit requirement of the output layer\nnormalization, which typically involves the evaluation of the full\nsoftmax function over the complete vocabulary. This paper proposes\na Batch Noise Contrastive Estimation (B-NCE) approach to alleviate\nthis problem. This is achieved by reducing the vocabulary, at each\ntime step, to the target words in the batch and then replacing the\nsoftmax by the noise contrastive estimation approach, where these words\nplay the role of targets and noise samples at the same time. In doing\nso, the proposed approach can be fully formulated and implemented using\noptimal dense matrix operations. Applying B-NCE to train different\nNNLMs on the Large Text Compression Benchmark (LTCB) and the One Billion\nWord Benchmark (OBWB) shows a significant reduction of the training\ntime with no noticeable degradation of the models performance. This\npaper also presents a new baseline comparative study of different standard\nNNLMs on the large OBWB on a single Titan-X GPU.\n"
      ],
      "doi": "10.21437/Interspeech.2017-818"
    },
    "chen17b_interspeech": {
      "authors": [
        [
          "X.",
          "Chen"
        ],
        [
          "A.",
          "Ragni"
        ],
        [
          "X.",
          "Liu"
        ],
        [
          "Mark J.F.",
          "Gales"
        ]
      ],
      "title": "Investigating Bidirectional Recurrent Neural Network Language Models for Speech Recognition",
      "original": "0513",
      "page_count": 5,
      "order": 56,
      "p1": "269",
      "pn": "273",
      "abstract": [
        "Recurrent neural network language models (RNNLMs) are powerful language\nmodeling techniques. Significant performance improvements have been\nreported in a range of tasks including speech recognition compared\nto n-gram language models. Conventional n-gram and neural network language\nmodels are trained to predict the probability of the next word given\nits preceding context history. In contrast, bidirectional recurrent\nneural network based language models consider the context from future\nwords as well. This complicates the inference process, but has theoretical\nbenefits for tasks such as speech recognition as additional context\ninformation can be used. However to date, very limited or no gains\nin speech recognition performance have been reported with this form\nof model. This paper examines the issues of training bidirectional\nrecurrent neural network language models (bi-RNNLMs) for speech recognition.\nA bi-RNNLM probability smoothing technique is proposed, that addresses\nthe very sharp posteriors that are often observed in these models.\nThe performance of the bi-RNNLMs is evaluated on three speech recognition\ntasks: broadcast news; meeting transcription (AMI); and low-resource\nsystems (Babel data). On all tasks gains are observed by applying the\nsmoothing technique to the bi-RNNLM. In addition consistent performance\ngains can be obtained by combining bi-RNNLMs with n-gram and uni-directional\nRNNLMs.\n"
      ],
      "doi": "10.21437/Interspeech.2017-513"
    },
    "huang17b_interspeech": {
      "authors": [
        [
          "Yinghui",
          "Huang"
        ],
        [
          "Abhinav",
          "Sethy"
        ],
        [
          "Bhuvana",
          "Ramabhadran"
        ]
      ],
      "title": "Fast Neural Network Language Model Lookups at N-Gram Speeds",
      "original": "0564",
      "page_count": 5,
      "order": 57,
      "p1": "274",
      "pn": "278",
      "abstract": [
        "Feed forward Neural Network Language Models (NNLM) have shown consistent\ngains over backoff word n-gram models in a variety of tasks. However,\nbackoff n-gram models still remain dominant in applications with real\ntime decoding requirements as word probabilities can be computed orders\nof magnitude faster than the NNLM. In this paper, we present a combination\nof techniques that allows us to speed up the probability computation\nfrom a neural net language model to make it comparable to the word\nn-gram model without any approximations. We present results on state\nof the art systems for Broadcast news transcription and conversational\nspeech which demonstrate the speed improvements in real time factor\nand probability computation while retaining the WER gains from NNLM.\n"
      ],
      "doi": "10.21437/Interspeech.2017-564"
    },
    "kurata17_interspeech": {
      "authors": [
        [
          "Gakuto",
          "Kurata"
        ],
        [
          "Abhinav",
          "Sethy"
        ],
        [
          "Bhuvana",
          "Ramabhadran"
        ],
        [
          "George",
          "Saon"
        ]
      ],
      "title": "Empirical Exploration of Novel Architectures and Objectives for Language Models",
      "original": "0723",
      "page_count": 5,
      "order": 58,
      "p1": "279",
      "pn": "283",
      "abstract": [
        "While recurrent neural network language models based on Long Short\nTerm Memory (LSTM) have shown good gains in many automatic speech recognition\ntasks, Convolutional Neural Network (CNN) language models are relatively\nnew and have not been studied in-depth. In this paper we present an\nempirical comparison of LSTM and CNN language models on English broadcast\nnews and various conversational telephone speech transcription tasks.\nWe also present a new type of CNN language model that leverages dilated\ncausal convolution to efficiently exploit long range history. We propose\na novel criterion for training language models that combines word and\nclass prediction in a multi-task learning framework. We apply this\ncriterion to train word and character based LSTM language models and\nCNN language models and show that it improves performance. Our results\nalso show that CNN and LSTM language models are complementary and can\nbe combined to obtain further gains.\n"
      ],
      "doi": "10.21437/Interspeech.2017-723"
    },
    "benes17_interspeech": {
      "authors": [
        [
          "Karel",
          "Bene\u0161"
        ],
        [
          "Murali Karthick",
          "Baskar"
        ],
        [
          "Luk\u00e1\u0161",
          "Burget"
        ]
      ],
      "title": "Residual Memory Networks in Language Modeling: Improving the Reputation of Feed-Forward Networks",
      "original": "1442",
      "page_count": 5,
      "order": 59,
      "p1": "284",
      "pn": "288",
      "abstract": [
        "We introduce the Residual Memory Network (RMN) architecture to language\nmodeling. RMN is an architecture of feed-forward neural networks that\nincorporates residual connections and time-delay connections that allow\nus to naturally incorporate information from a substantial time context.\nAs this is the first time RMNs are applied for language modeling, we\nthoroughly investigate their behaviour on the well studied Penn Treebank\ncorpus. We change the model slightly for the needs of language modeling,\nreducing both its time and memory consumption. Our results show that\nRMN is a suitable choice for small-sized neural language models: With\ntest perplexity 112.7 and as few as 2.3M parameters, they out-perform\nboth a much larger vanilla RNN (PPL 124, 8M parameters) and a similarly\nsized LSTM (PPL 115, 2.08M parameters), while being only by less than\n3 perplexity points worse than twice as big LSTM.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1442"
    },
    "poorjam17_interspeech": {
      "authors": [
        [
          "Amir Hossein",
          "Poorjam"
        ],
        [
          "Jesper Rindom",
          "Jensen"
        ],
        [
          "Max A.",
          "Little"
        ],
        [
          "Mads Gr\u00e6sb\u00f8ll",
          "Christensen"
        ]
      ],
      "title": "Dominant Distortion Classification for Pre-Processing of Vowels in Remote Biomedical Voice Analysis",
      "original": "0378",
      "page_count": 5,
      "order": 60,
      "p1": "289",
      "pn": "293",
      "abstract": [
        "Advances in speech signal analysis facilitate the development of techniques\nfor remote biomedical voice assessment. However, the performance of\nthese techniques is affected by noise and distortion in signals. In\nthis paper, we focus on the vowel /a/ as the most widely-used voice\nsignal for pathological voice assessments and investigate the impact\nof four major types of distortion that are commonly present during\nrecording or transmission in voice analysis, namely: background noise,\nreverberation, clipping and compression, on Mel-frequency cepstral\ncoefficients (MFCCs) &#8212; the most widely-used features in biomedical\nvoice analysis. Then, we propose a new distortion classification approach\nto detect the most dominant distortion in such voice signals. The proposed\nmethod involves MFCCs as frame-level features and a support vector\nmachine as classifier to detect the presence and type of distortion\nin frames of a given voice signal. Experimental results obtained from\nthe healthy and Parkinson&#8217;s voices show the effectiveness of\nthe proposed approach in distortion detection and classification.\n"
      ],
      "doi": "10.21437/Interspeech.2017-378"
    },
    "le17_interspeech": {
      "authors": [
        [
          "Duc",
          "Le"
        ],
        [
          "Keli",
          "Licata"
        ],
        [
          "Emily Mower",
          "Provost"
        ]
      ],
      "title": "Automatic Paraphasia Detection from Aphasic Speech: A Preliminary Study",
      "original": "0626",
      "page_count": 5,
      "order": 61,
      "p1": "294",
      "pn": "298",
      "abstract": [
        "Aphasia is an acquired language disorder resulting from brain damage\nthat can cause significant communication difficulties. Aphasic speech\nis often characterized by errors known as paraphasias, the analysis\nof which can be used to determine an appropriate course of treatment\nand to track an individual&#8217;s recovery progress. Being able to\ndetect paraphasias automatically has many potential clinical benefits;\nhowever, this problem has not previously been investigated in the literature.\nIn this paper, we perform the first study on detecting phonemic and\nneologistic paraphasias from scripted speech samples in AphasiaBank.\nWe propose a speech recognition system with task-specific language\nmodels to transcribe aphasic speech automatically. We investigate features\nbased on speech duration, Goodness of Pronunciation, phone edit distance,\nand Dynamic Time Warping on phoneme posteriorgrams. Our results demonstrate\nthe feasibility of automatic paraphasia detection and outline the path\ntoward enabling this system in real-world clinical applications.\n"
      ],
      "doi": "10.21437/Interspeech.2017-626"
    },
    "garcia17_interspeech": {
      "authors": [
        [
          "N.",
          "Garcia"
        ],
        [
          "Juan Rafael",
          "Orozco-Arroyave"
        ],
        [
          "L.F.",
          "D\u2019Haro"
        ],
        [
          "Najim",
          "Dehak"
        ],
        [
          "Elmar",
          "N\u00f6th"
        ]
      ],
      "title": "Evaluation of the Neurological State of People with Parkinson&#8217;s Disease Using i-Vectors",
      "original": "0819",
      "page_count": 5,
      "order": 62,
      "p1": "299",
      "pn": "303",
      "abstract": [
        "The i-vector approach is used to model the speech of PD patients with\nthe aim of assessing their condition. Features related to the articulation,\nphonation, and prosody dimensions of speech were used to train different\ni-vector extractors. Each i-vector extractor is trained using utterances\nfrom both PD patients and healthy controls. The i-vectors of the healthy\ncontrol (HC) speakers are averaged to form a single i-vector that represents\nthe HC group, i.e., the reference i-vector. A similar process is done\nto create a reference of the group with PD patients. Then the i-vectors\nof test speakers are compared to these reference i-vectors using the\ncosine distance. Three analyses are performed using this distance:\nclassification between PD patients and HC, prediction of the neurological\nstate of PD patients according to the MDS-UPDRS-III scale, and prediction\nof a modified version of the Frenchay Dysarthria Assessment. The Spearman&#8217;s\ncorrelation between this cosine distance and the MDS-UPDRS-III scale\nwas 0.63. These results show the suitability of this approach to monitor\nthe neurological state of people with Parkinson&#8217;s Disease.\n"
      ],
      "doi": "10.21437/Interspeech.2017-819"
    },
    "chien17_interspeech": {
      "authors": [
        [
          "Yu-Ren",
          "Chien"
        ],
        [
          "Michal",
          "Borsk\u00fd"
        ],
        [
          "J\u00f3n",
          "Gu\u00f0nason"
        ]
      ],
      "title": "Objective Severity Assessment from Disordered Voice Using Estimated Glottal Airflow",
      "original": "0138",
      "page_count": 5,
      "order": 63,
      "p1": "304",
      "pn": "308",
      "abstract": [
        "In clinical practice, the severity of disordered voice is typically\nrated by a professional with auditory-perceptual judgment. The present\nstudy aims to automate this assessment procedure, in an attempt to\nmake the assessment objective and less labor-intensive. In the automated\nanalysis, glottal airflow is estimated from the analyzed voice signal\nwith an inverse filtering algorithm. Automatic assessment is realized\nby a regressor that predicts from temporal and spectral features of\nthe glottal airflow. A regressor trained on overtone amplitudes and\nharmonic richness factors extracted from a set of continuous-speech\nutterances was applied to a set of sustained-vowel utterances, giving\nseverity predictions (on a scale of ratings from 0 to 100) with an\naverage error magnitude of 14.\n"
      ],
      "doi": "10.21437/Interspeech.2017-138"
    },
    "pokorny17_interspeech": {
      "authors": [
        [
          "Florian B.",
          "Pokorny"
        ],
        [
          "Bj\u00f6rn",
          "Schuller"
        ],
        [
          "Peter B.",
          "Marschik"
        ],
        [
          "Raymond",
          "Brueckner"
        ],
        [
          "P\u00e4r",
          "Nystr\u00f6m"
        ],
        [
          "Nicholas",
          "Cummins"
        ],
        [
          "Sven",
          "B\u00f6lte"
        ],
        [
          "Christa",
          "Einspieler"
        ],
        [
          "Terje",
          "Falck-Ytter"
        ]
      ],
      "title": "Earlier Identification of Children with Autism Spectrum Disorder: An Automatic Vocalisation-Based Approach",
      "original": "1007",
      "page_count": 5,
      "order": 64,
      "p1": "309",
      "pn": "313",
      "abstract": [
        "Autism spectrum disorder (ASD) is a neurodevelopmental disorder usually\ndiagnosed in or beyond toddlerhood. ASD is defined by repetitive and\nrestricted behaviours, and deficits in social communication. The early\nspeech-language development of individuals with ASD has been characterised\nas delayed. However, little is known about ASD-related characteristics\nof pre-linguistic vocalisations at the feature level. In this study,\nwe examined pre-linguistic vocalisations of 10-month-old individuals\nlater diagnosed with ASD and a matched control group of typically developing\nindividuals (N = 20). We segmented 684 vocalisations from parent-child\ninteraction recordings. All vocalisations were annotated and signal-analytically\ndecomposed. We analysed ASD-related vocalisation specificities on the\nbasis of a standardised set (eGeMAPS) of 88 acoustic features selected\nfor clinical speech analysis applications. 54 features showed evidence\nfor a differentiation between vocalisations of individuals later diagnosed\nwith ASD and controls. In addition, we evaluated the feasibility of\nautomated, vocalisation-based identification of individuals later diagnosed\nwith ASD. We compared linear kernel support vector machines and a 1-layer\nbidirectional long short-term memory neural network. Both classification\napproaches achieved an accuracy of 75% for subject-wise identification\nin a subject-independent 3-fold cross-validation scheme. Our promising\nresults may be an important contribution en-route to facilitate earlier\nidentification of ASD.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1007"
    },
    "vasquezcorrea17_interspeech": {
      "authors": [
        [
          "J.C.",
          "V\u00e1squez-Correa"
        ],
        [
          "Juan Rafael",
          "Orozco-Arroyave"
        ],
        [
          "Elmar",
          "N\u00f6th"
        ]
      ],
      "title": "Convolutional Neural Network to Model Articulation Impairments in Patients with Parkinson&#8217;s Disease",
      "original": "1078",
      "page_count": 5,
      "order": 65,
      "p1": "314",
      "pn": "318",
      "abstract": [
        "Speech impairments are one of the earliest manifestations in patients\nwith Parkinson&#8217;s disease. Particularly, articulation deficits\nrelated to the capability of the speaker to start/stop the vibration\nof the vocal folds have been observed in the patients. Those difficulties\ncan be assessed by modeling the transitions between voiced and unvoiced\nsegments from speech. A robust strategy to model the articulatory deficits\nrelated to the starting or stopping vibration of the vocal folds is\nproposed in this study. The transitions between voiced and unvoiced\nsegments are modeled by a convolutional neural network that extracts\nsuitable information from two time-frequency representations: the short\ntime Fourier transform and the continuous wavelet transform. The proposed\napproach improves the results previously reported in the literature.\nAccuracies of up to 89% are obtained for the classification of Parkinson&#8217;s\npatients vs. healthy speakers. This study is a step towards the robust\nmodeling of the speech impairments in patients with neuro-degenerative\ndisorders.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1078"
    },
    "bai17_interspeech": {
      "authors": [
        [
          "Linxue",
          "Bai"
        ],
        [
          "Peter",
          "Jan\u010dovi\u010d"
        ],
        [
          "Martin",
          "Russell"
        ],
        [
          "Philip",
          "Weber"
        ],
        [
          "Steve",
          "Houghton"
        ]
      ],
      "title": "Phone Classification Using a Non-Linear Manifold with Broad Phone Class Dependent DNNs",
      "original": "1179",
      "page_count": 5,
      "order": 66,
      "p1": "319",
      "pn": "323",
      "abstract": [
        "Most state-of-the-art automatic speech recognition (ASR) systems use\na single deep neural network (DNN) to map the acoustic space to the\ndecision space. However, different phonetic classes employ different\nproduction mechanisms and are best described by different types of\nfeatures. Hence it may be advantageous to replace this single DNN with\nseveral phone class dependent DNNs. The appropriate mathematical formalism\nfor this is a manifold. This paper assesses the use of a non-linear\nmanifold structure with multiple DNNs for phone classification. The\nsystem has two levels. The first comprises a set of broad phone class\n(BPC) dependent DNN-based mappings and the second level is a fusion\nnetwork. Various ways of designing and training the networks in both\nlevels are assessed, including varying the size of hidden layers, the\nuse of the bottleneck or softmax outputs as input to the fusion network,\nand the use of different broad class definitions. Phone classification\nexperiments are performed on TIMIT. The results show that using the\nBPC-dependent DNNs provides small but significant improvements in phone\nclassification accuracy relative to a single global DNN. The paper\nconcludes with visualisations of the structures learned by the local\nand global DNNs and discussion of their interpretations.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1179"
    },
    "chen17c_interspeech": {
      "authors": [
        [
          "Siyuan",
          "Chen"
        ],
        [
          "Julien",
          "Epps"
        ],
        [
          "Eliathamby",
          "Ambikairajah"
        ],
        [
          "Phu Ngoc",
          "Le"
        ]
      ],
      "title": "An Investigation of Crowd Speech for Room Occupancy Estimation",
      "original": "0070",
      "page_count": 5,
      "order": 67,
      "p1": "324",
      "pn": "328",
      "abstract": [
        "Room occupancy estimation technology has been shown to reduce building\nenergy cost significantly. However speech-based occupancy estimation\nhas not been well explored. In this paper, we investigate energy mode\nand babble speaker count methods for estimating both small and large\ncrowds in a party-mode room setting. We also examine how distance between\nspeakers and microphone affects their estimation accuracies. Then we\npropose a novel entropy-based method, which is invariant to different\nspeakers and their different positions in a room. Evaluations on synthetic\ncrowd speech generated using the TIMIT corpus show that acoustic volume\nfeatures are less affected by distance, and our proposed method outperforms\nexisting methods across a range of different conditions.\n"
      ],
      "doi": "10.21437/Interspeech.2017-70"
    },
    "vijayan17_interspeech": {
      "authors": [
        [
          "Karthika",
          "Vijayan"
        ],
        [
          "Jitendra Kumar",
          "Dhiman"
        ],
        [
          "Chandra Sekhar",
          "Seelamantula"
        ]
      ],
      "title": "Time-Frequency Coherence for Periodic-Aperiodic Decomposition of Speech Signals",
      "original": "0726",
      "page_count": 5,
      "order": 68,
      "p1": "329",
      "pn": "333",
      "abstract": [
        "Decomposing speech signals into periodic and aperiodic components is\nan important task, finding applications in speech synthesis, coding,\ndenoising, etc. In this paper, we construct a time-frequency coherence\nfunction to analyze spectro-temporal signatures of speech signals for\ndistinguishing between deterministic and stochastic components of speech.\nThe narrowband speech spectrogram is segmented into patches, which\nare represented as 2-D cosine carriers modulated in amplitude and frequency.\nSeparation of carrier and amplitude/frequency modulations is achieved\nby 2-D demodulation using Riesz transform, which is the 2-D extension\nof Hilbert transform. The demodulated AM component reflects contributions\nof the vocal tract to spectrogram. The frequency modulated carrier\n(FM-carrier) signal exhibits properties of the excitation. The time-frequency\ncoherence is defined with respect to FM-carrier and a coherence map\nis constructed, in which highly coherent regions represent nearly periodic\nand deterministic components of speech, whereas the incoherent regions\ncorrespond to unstructured components. The coherence map shows a clear\ndistinction between deterministic and stochastic components in speech\ncharacterized by jitter, shimmer, lip radiation, type of excitation,\netc. Binary masks prepared from the time-frequency coherence function\nare used for periodic-aperiodic decomposition of speech. Experimental\nresults are presented to validate the efficiency of the proposed method.\n"
      ],
      "doi": "10.21437/Interspeech.2017-726"
    },
    "meireles17_interspeech": {
      "authors": [
        [
          "Alexsandro R.",
          "Meireles"
        ],
        [
          "Ant\u00f4nio R.M.",
          "Sim\u00f5es"
        ],
        [
          "Antonio Celso",
          "Ribeiro"
        ],
        [
          "Beatriz Raposo de",
          "Medeiros"
        ]
      ],
      "title": "Musical Speech: A New Methodology for Transcribing Speech Prosody",
      "original": "0316",
      "page_count": 5,
      "order": 69,
      "p1": "334",
      "pn": "338",
      "abstract": [
        "Musical Speech is a new methodology for transcribing speech prosody\nusing musical notation. The methodology presented in this paper is\nan updated version of our work [12]. Our work is situated in a historical\ncontext with a brief survey of the literature of speech melodies, in\nwhich we highlight the pioneering works of John Steele, Leo&#353; Jan&#225;vcek,\nEngelbert Humperdinck, and Arnold Schoenberg, followed by a linguistic\nview of musical notation in the analysis of speech. Finally, we present\nthe current state-of-the-art of our innovative methodology that uses\na quarter-tone scale for transcribing speech, and shows some initial\nresults of the application of this methodology to prosodic transcription.\n"
      ],
      "doi": "10.21437/Interspeech.2017-316"
    },
    "nataraj17_interspeech": {
      "authors": [
        [
          "K.S.",
          "Nataraj"
        ],
        [
          "Prem C.",
          "Pandey"
        ],
        [
          "Hirak",
          "Dasgupta"
        ]
      ],
      "title": "Estimation of Place of Articulation of Fricatives from Spectral Characteristics for Speech Training",
      "original": "1074",
      "page_count": 5,
      "order": 70,
      "p1": "339",
      "pn": "343",
      "abstract": [
        "A visual feedback of the place of articulation is considered to be\nuseful for speech training aids for hearing-impaired children and for\nlearners of second languages in helping them in improving pronunciation.\nFor such applications, the relation between place of articulation of\nfricatives and their spectral characteristics is investigated using\nEnglish fricatives available in the XRMB database, which provides simultaneously\nacquired speech signal and articulogram. Place of articulation is estimated\nfrom the articulogram as the position of maximum constriction in the\noral cavity, using an automated graphical technique. The magnitude\nspectrum is smoothed by critical band based median and mean filters\nfor improving the consistency of the spectral parameters. Out of several\nspectral parameters investigated, spectral moments and spectral slope\nappear to be related to the place of articulation of the fricative\nsegment of the utterances as measured from articulogram. The data are\nused to train and test a Gaussian mixture model to estimate the place\nof articulation with spectral parameters as the inputs. The estimated\nvalues showed a good match with those obtained from the articulograms.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1074"
    },
    "backstrom17_interspeech": {
      "authors": [
        [
          "Tom",
          "B\u00e4ckstr\u00f6m"
        ]
      ],
      "title": "Estimation of the Probability Distribution of Spectral Fine Structure in the Speech Source",
      "original": "0389",
      "page_count": 5,
      "order": 71,
      "p1": "344",
      "pn": "348",
      "abstract": [
        "The efficiency of many speech processing methods rely on accurate modeling\nof the distribution of the signal spectrum and a majority of prior\nworks suggest that the spectral components follow the Laplace distribution.\nTo improve the probability distribution models based on our knowledge\nof speech source modeling, we argue that the model should in fact be\na multiplicative mixture model, including terms for voiced and unvoiced\nutterances. While prior works have applied Gaussian mixture models,\nwe demonstrate that a mixture of generalized Gaussian models more accurately\nfollows the observations. The proposed estimation method is based on\nmeasuring the ratio of L<SUB>p</SUB>-norms between spectral bands.\nSuch ratios follow the Beta-distribution when the input signal is generalized\nGaussian, whereby the estimated parameters can be used to determine\nthe underlying parameters of the mixture of generalized Gaussian distributions.\n"
      ],
      "doi": "10.21437/Interspeech.2017-389"
    },
    "ghosh17_interspeech": {
      "authors": [
        [
          "Sucheta",
          "Ghosh"
        ],
        [
          "Camille",
          "Fauth"
        ],
        [
          "Yves",
          "Laprie"
        ],
        [
          "Aghilas",
          "Sini"
        ]
      ],
      "title": "End-to-End Acoustic Feedback in Language Learning for Correcting Devoiced French Final-Fricatives",
      "original": "1031",
      "page_count": 5,
      "order": 72,
      "p1": "349",
      "pn": "353",
      "abstract": [
        "This work aims at providing an end-to-end acoustic feedback framework\nto help learners of French to pronounce voiced fricatives. A classifier\nensemble detects voiced/unvoiced utterances, then a correction method\nis proposed to improve the perception and production of voiced fricatives\nin a word-final position. Realizations of voiced fricatives contained\nin French sentences uttered by French and German speakers were analyzed\nto find out the deviations between the acoustic cues realized by the\ntwo groups of speakers. The correction method consists in substituting\nthe erroneous devoiced fricative by TD-PSOLA concatenative synthesis\nthat uses exemplars of voiced fricatives chosen from a French speaker\ncorpus. To achieve a seamless concatenation the energy of the replacement\nfricative was adjusted with respect to the energy levels of the learner&#8217;s\nand French speaker&#8217;s preceding vowels. Finally, a perception\nexperiment with the corrected stimuli has been carried out with French\nnative speakers to check the appropriateness of the fricative revoicing.\nThe results showed that the proposed revoicing strategy proved to be\nvery efficient and can be used as an acoustic feedback.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1031"
    },
    "jacewicz17_interspeech": {
      "authors": [
        [
          "Ewa",
          "Jacewicz"
        ],
        [
          "Robert A.",
          "Fox"
        ]
      ],
      "title": "Dialect Perception by Older Children",
      "original": "0018",
      "page_count": 5,
      "order": 73,
      "p1": "354",
      "pn": "358",
      "abstract": [
        "The acquisition of regional dialect variation is an inherent part of\nthe language learning process that takes place in the specific environments\nin which the child participates. This study examined dialect perception\nby 9&#8211;12-year-olds who grew up in two very diverse dialect regions\nin the United States, Western North Carolina (NC) and Southeastern\nWisconsin (WI). In a dialect identification task, each group of children\nresponded to 120 talkers from the same dialects representing three\ngenerations, ranging in age from old adults to children. There was\na robust discrepancy in the children&#8217;s dialect identification\nperformance: WI children were able to identify talker dialect quite\nwell (although still not as well as the adults) whereas NC children\nwere at chance level. WI children were also more sensitive to cross-generational\nchanges in both dialects as a function of diachronic sound change.\nIt is concluded that both groups of children demonstrated their sociolinguistic\nawareness in very different ways, corresponding to relatively stable\n(WI) and changing (NC) socio-cultural environments in their respective\nspeech communities.\n"
      ],
      "doi": "10.21437/Interspeech.2017-18"
    },
    "yoneyama17_interspeech": {
      "authors": [
        [
          "Kiyoko",
          "Yoneyama"
        ],
        [
          "Mafuyu",
          "Kitahara"
        ],
        [
          "Keiichi",
          "Tajima"
        ]
      ],
      "title": "Perception of Non-Contrastive Variations in American English by Japanese Learners: Flaps are Less Favored Than Stops",
      "original": "0207",
      "page_count": 5,
      "order": 74,
      "p1": "359",
      "pn": "363",
      "abstract": [
        "Alveolar flaps are non-contrastive allophonic variants of alveolar\nstops in American English. A lexical decision experiment was conducted\nwith Japanese learners of English (JE) to investigate whether second-language\n(L2) learners are sensitive to such allophonic variations when recognizing\nwords in L2. The stimuli consisted of 36 isolated bisyllabic English\nwords containing word-medial /t/, half of which were flap-favored words,\ne.g.  city, and the other half were [t]-favored words, e.g.  faster.\nAll stimuli were recorded with two surface forms: /t/ as a flap, e.g.\n city with a flap, or as [t], e.g.  city with [t]. The stimuli were\ncounterbalanced so that participants only heard one of the two surface\nforms of each word. The accuracy data indicated that flap-favored words\npronounced with a flap, e.g.  city with a flap, were recognized significantly\nless accurately than flap-favored words with [t], e.g.  city with [t],\nand [t]-favored words with [t], e.g.  faster with [t]. These results\nsuggest that JE learners prefer canonical forms over frequent forms\nproduced with context-dependent allophonic variations. These results\nare inconsistent with previous studies that found native speakers&#8217;\npreference for frequent forms, and highlight differences in the effect\nof allophonic variations on the perception of native-language and L2\nspeech.\n"
      ],
      "doi": "10.21437/Interspeech.2017-207"
    },
    "maastricht17_interspeech": {
      "authors": [
        [
          "Lieke van",
          "Maastricht"
        ],
        [
          "Tim",
          "Zee"
        ],
        [
          "Emiel",
          "Krahmer"
        ],
        [
          "Marc",
          "Swerts"
        ]
      ],
      "title": "L1 Perceptions of L2 Prosody: The Interplay Between Intonation, Rhythm, and Speech Rate and Their Contribution to Accentedness and Comprehensibility",
      "original": "1150",
      "page_count": 5,
      "order": 75,
      "p1": "364",
      "pn": "368",
      "abstract": [
        "This study investigates the cumulative effect of (non-)native intonation,\nrhythm, and speech rate in utterances produced by Spanish learners\nof Dutch on Dutch native listeners&#8217; perceptions. In order to\nassess the relative contribution of these language-specific properties\nto perceived accentedness and comprehensibility, speech produced by\nSpanish learners of Dutch was manipulated using transplantation and\nresynthesis techniques. Thus, eight manipulation conditions reflecting\nall possible combinations of L1 and L2 intonation, rhythm, and speech\nrate were created, resulting in 320 utterances that were rated by 50\nDutch natives on their degree of foreign accent and ease of comprehensibility.<br\nstyle=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  Our analyses show that all manipulations result in lower accentedness\nand higher comprehensibility ratings. Moreover, both measures are not\naffected in the same way by different combinations of prosodic features:\nFor accentedness, Dutch listeners appear most influenced by intonation,\nand intonation combined with speech rate. This holds for comprehensibility\nratings as well, but here the combination of all three properties,\nincluding rhythm, also significantly affects ratings by native speakers.\nThus, our study reaffirms the importance of differentiating between\ndifferent aspects of perception and provides insight into those features\nthat are most likely to affect how native speakers perceive second\nlanguage learners.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1150"
    },
    "takiguchi17_interspeech": {
      "authors": [
        [
          "Izumi",
          "Takiguchi"
        ]
      ],
      "title": "Effects of Pitch Fall and L1 on Vowel Length Identification in L2 Japanese",
      "original": "0763",
      "page_count": 5,
      "order": 76,
      "p1": "369",
      "pn": "373",
      "abstract": [
        "This study investigated whether and how the role of pitch fall in the\nfirst language (L1) interacts with its use as a cue for Japanese phonological\nvowel length in the second language (L2). Native listeners of Japanese\n(NJ) and L2 learners of Japanese with L1 backgrounds in Mandarin Chinese\n(NC), Seoul Korean (NK), American English (NE), and French (NFr) participated\nin a perception experiment. The results showed that the proportion\nof &#8220;long&#8221; responses increased as a function of vowel duration\nfor all groups, giving s-shaped curves. Meanwhile, the presence or\nabsence of a pitch fall within a syllable affected only NJ and NC&#8217;s\nperception. Their category boundary occurred at a shorter duration\nfor vowels with a pitch fall than without a pitch fall. Among the four\ngroups of L2 learners, only NC use pitch fall to distinguish words\nin the L1. Thus, it is possible to think that the role of pitch fall\nas an L1 cue relates to its use as a cue for L2 length identification.\nL2 learners tend to attend to an important phonetic feature as a cue\nfor perceiving an L1 category differentiating L1 words even in the\nL2 as implied by the Feature Hypothesis.\n"
      ],
      "doi": "10.21437/Interspeech.2017-763"
    },
    "zhang17_interspeech": {
      "authors": [
        [
          "Yuanyuan",
          "Zhang"
        ],
        [
          "Hongwei",
          "Ding"
        ]
      ],
      "title": "A Preliminary Study of Prosodic Disambiguation by Chinese EFL Learners",
      "original": "1210",
      "page_count": 5,
      "order": 77,
      "p1": "374",
      "pn": "378",
      "abstract": [
        "This study investigated whether Chinese learners of English as a foreign\nlanguage (EFL learners hereafter) could use prosodic cues to resolve\nsyntactically ambiguous sentences in English. 8 sentences with 3 types\nof syntactic ambiguity were adopted. They were far/near PP attachment,\nleft/right word attachment and wide/narrow scope. In the production\nexperiment, 15 Chinese college students who passed the annual national\nexamination CET (College English Test) Band 4 and 5 native English\nspeakers from America were recruited. They were asked to read the 8\ntarget sentences after hearing the contexts spoken by a Native American\nspeaker, which clarified the intended meaning of the ambiguous sentences.\nThe preliminary results showed that, as the native speakers did, Chinese\nEFL learners employed different durational patterns to express the\nalternative meanings of the ambiguous sentences by altering prosodic\nphrasing. That is, the duration of the pre-boundary items were lengthened\nand pause were inserted at the boundary. But the perception experiment\nshowed that the utterances produced by Chinese EFL learners couldn&#8217;t\nbe effectively perceived by the native speakers due to their different\nuse of pre-boundary lengthening and pause. The conclusion is that Chinese\nEFL learners find prosodic disambiguation difficult.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1210"
    },
    "kim17_interspeech": {
      "authors": [
        [
          "Chanwoo",
          "Kim"
        ],
        [
          "Ananya",
          "Misra"
        ],
        [
          "Kean",
          "Chin"
        ],
        [
          "Thad",
          "Hughes"
        ],
        [
          "Arun",
          "Narayanan"
        ],
        [
          "Tara N.",
          "Sainath"
        ],
        [
          "Michiel",
          "Bacchiani"
        ]
      ],
      "title": "Generation of Large-Scale Simulated Utterances in Virtual Rooms to Train Deep-Neural Networks for Far-Field Speech Recognition in Google Home",
      "original": "1510",
      "page_count": 5,
      "order": 78,
      "p1": "379",
      "pn": "383",
      "abstract": [
        "We describe the structure and application of an acoustic room simulator\nto generate large-scale simulated data for training deep neural networks\nfor far-field speech recognition. The system simulates millions of\ndifferent room dimensions, a wide distribution of reverberation time\nand signal-to-noise ratios, and a range of microphone and sound source\nlocations. We start with a relatively clean training set as the source\nand artificially create simulated data by randomly sampling a noise\nconfiguration for every new training example. As a result, the acoustic\nmodel is trained using examples that are virtually never repeated.\nWe evaluate performance of this approach based on room simulation using\na factored complex Fast Fourier Transform (CFFT) acoustic model introduced\nin our earlier work, which uses CFFT layers and LSTM AMs for joint\nmultichannel processing and acoustic modeling. Results show that the\nsimulator-driven approach is quite effective in obtaining large improvements\nnot only in simulated test conditions, but also in real / rerecorded\nconditions. This room simulation system has been employed in training\nacoustic models including the ones for the recently released Google\nHome.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1510"
    },
    "kinoshita17_interspeech": {
      "authors": [
        [
          "Keisuke",
          "Kinoshita"
        ],
        [
          "Marc",
          "Delcroix"
        ],
        [
          "Haeyong",
          "Kwon"
        ],
        [
          "Takuma",
          "Mori"
        ],
        [
          "Tomohiro",
          "Nakatani"
        ]
      ],
      "title": "Neural Network-Based Spectrum Estimation for Online WPE Dereverberation",
      "original": "0733",
      "page_count": 5,
      "order": 79,
      "p1": "384",
      "pn": "388",
      "abstract": [
        "In this paper, we propose a novel speech dereverberation framework\nthat utilizes deep neural network (DNN)-based spectrum estimation to\nconstruct linear inverse filters. The proposed dereverberation framework\nis based on the state-of-the-art inverse filter estimation algorithm\ncalled weighted prediction error (WPE) algorithm, which is known to\neffectively reduce reverberation and greatly boost the ASR performance\nin various conditions. In WPE, the accuracy of the inverse filter estimation,\nand thus the dereverberation performance, is largely dependent on the\nestimation of the power spectral density (PSD) of the target signal.\nTherefore, the conventional WPE iteratively performs the inverse filter\nestimation, actual dereverberation and the PSD estimation to gradually\nimprove the PSD estimate. However, while such iterative procedure works\nwell when sufficiently long acoustically-stationary observed signals\nare available, WPE&#8217;s performance degrades when the duration of\nobserved/accessible data is short, which typically is the case for\nreal-time applications using online block-batch processing with small\nbatches. To solve this problem, we incorporate the DNN-based spectrum\nestimator into the framework of WPE, because a DNN can estimate the\nPSD robustly even from very short observed data. We experimentally\nshow that the proposed framework outperforms the conventional WPE,\nand improves the ASR performance in real noisy reverberant environments\nin both single-channel and multichannel cases.\n"
      ],
      "doi": "10.21437/Interspeech.2017-733"
    },
    "ichikawa17_interspeech": {
      "authors": [
        [
          "Osamu",
          "Ichikawa"
        ],
        [
          "Takashi",
          "Fukuda"
        ],
        [
          "Gakuto",
          "Kurata"
        ],
        [
          "Steven J.",
          "Rennie"
        ]
      ],
      "title": "Factorial Modeling for Effective Suppression of Directional Noise",
      "original": "0852",
      "page_count": 5,
      "order": 80,
      "p1": "389",
      "pn": "393",
      "abstract": [
        "The assumed scenario is transcription of a face-to-face conversation,\nsuch as in the financial industry when an agent and a customer talk\nover a desk with microphones placed between the speakers and then it\nis transcribed. From the automatic speech recognition (ASR) perspective,\none of the speakers is the target speaker, and the other speaker is\na directional noise source. When the number of microphones is small,\nwe often accept microphone intervals that are larger than the spatial\naliasing limit because the performance of the beamformer is better.\nUnfortunately, such a configuration results in significant leakage\nof directional noise in certain frequency bands because the spatial\naliasing makes the beamformer and post-filter inaccurate there. Thus,\nwe introduce a factorial model to compensate only the degraded bands\nwith information from the reliable bands in a probabilistic framework\nintegrating our proposed metrics and speech model. In our experiments,\nthe proposed method reduced the errors from 29.8% to 24.9%.\n"
      ],
      "doi": "10.21437/Interspeech.2017-852"
    },
    "tu17_interspeech": {
      "authors": [
        [
          "Yan-Hui",
          "Tu"
        ],
        [
          "Jun",
          "Du"
        ],
        [
          "Lei",
          "Sun"
        ],
        [
          "Feng",
          "Ma"
        ],
        [
          "Chin-Hui",
          "Lee"
        ]
      ],
      "title": "On Design of Robust Deep Models for CHiME-4 Multi-Channel Speech Recognition with Multiple Configurations of Array Microphones",
      "original": "0853",
      "page_count": 5,
      "order": 81,
      "p1": "394",
      "pn": "398",
      "abstract": [
        "We design a novel deep learning framework for multi-channel speech\nrecognition in two aspects. First, for the front-end, an iterative\nmask estimation (IME) approach based on deep learning is presented\nto improve the beamforming approach based on the conventional complex\nGaussian mixture model (CGMM). Second, for the back-end, deep convolutional\nneural networks (DCNNs), with augmentation of both noisy and beamformed\ntraining data, are adopted for acoustic modeling while the forward\nand backward long short-term memory recurrent neural networks (LSTM-RNNs)\nare used for language modeling. The proposed framework can be quite\neffective to multi-channel speech recognition with random combinations\nof fixed microphones. Testing on the CHiME-4 Challenge speech recognition\ntask with a single set of acoustic and language models, our approach\nachieves the best performance of all three tracks (1-channel, 2-channel,\nand 6-channel) among submitted systems.\n"
      ],
      "doi": "10.21437/Interspeech.2017-853"
    },
    "li17c_interspeech": {
      "authors": [
        [
          "Bo",
          "Li"
        ],
        [
          "Tara N.",
          "Sainath"
        ],
        [
          "Arun",
          "Narayanan"
        ],
        [
          "Joe",
          "Caroselli"
        ],
        [
          "Michiel",
          "Bacchiani"
        ],
        [
          "Ananya",
          "Misra"
        ],
        [
          "Izhak",
          "Shafran"
        ],
        [
          "Ha\u015fim",
          "Sak"
        ],
        [
          "Golan",
          "Pundak"
        ],
        [
          "Kean",
          "Chin"
        ],
        [
          "Khe Chai",
          "Sim"
        ],
        [
          "Ron J.",
          "Weiss"
        ],
        [
          "Kevin W.",
          "Wilson"
        ],
        [
          "Ehsan",
          "Variani"
        ],
        [
          "Chanwoo",
          "Kim"
        ],
        [
          "Olivier",
          "Siohan"
        ],
        [
          "Mitchel",
          "Weintraub"
        ],
        [
          "Erik",
          "McDermott"
        ],
        [
          "Richard",
          "Rose"
        ],
        [
          "Matt",
          "Shannon"
        ]
      ],
      "title": "Acoustic Modeling for Google Home",
      "original": "0234",
      "page_count": 5,
      "order": 82,
      "p1": "399",
      "pn": "403",
      "abstract": [
        "This paper describes the technical and system building advances made\nto the Google Home multichannel speech recognition system, which was\nlaunched in November 2016. Technical advances include an adaptive dereverberation\nfrontend, the use of neural network models that do multichannel processing\njointly with acoustic modeling, and Grid-LSTMs to model frequency variations.\nOn the system level, improvements include adapting the model using\nGoogle Home specific data. We present results on a variety of multichannel\nsets. The combination of technical and system advances result in a\nreduction of WER of 8&#8211;28% relative compared to the current production\nsystem.\n"
      ],
      "doi": "10.21437/Interspeech.2017-234"
    },
    "mirsamadi17_interspeech": {
      "authors": [
        [
          "Seyedmahdad",
          "Mirsamadi"
        ],
        [
          "John H.L.",
          "Hansen"
        ]
      ],
      "title": "On Multi-Domain Training and Adaptation of End-to-End RNN Acoustic Models for Distant Speech Recognition",
      "original": "0398",
      "page_count": 5,
      "order": 83,
      "p1": "404",
      "pn": "408",
      "abstract": [
        "Recognition of distant (far-field) speech is a challenge for ASR due\nto mismatch in recording conditions resulting from room reverberation\nand environment noise. Given the remarkable learning capacity of deep\nneural networks, there is increasing interest to address this problem\nby using a large corpus of reverberant far-field speech to train robust\nmodels. In this study, we explore how an end-to-end RNN acoustic model\ntrained on speech from different rooms and acoustic conditions (different\ndomains) achieves robustness to environmental variations. It is shown\nthat the first hidden layer acts as a domain separator, projecting\nthe data from different domains into different subspaces. The subsequent\nlayers then use this encoded domain knowledge to map these features\nto final representations that are invariant to domain change. This\nmechanism is closely related to noise-aware or room-aware approaches\nwhich append manually-extracted domain signatures to the input features.\nAdditionally, we demonstrate how this understanding of the learning\nprocedure provides useful guidance for model adaptation to new acoustic\nconditions. We present results based on AMI corpus to demonstrate the\npropagation of domain information in a deep RNN, and perform recognition\nexperiments which indicate the role of encoded domain knowledge on\ntraining and adaptation of RNN acoustic models.\n"
      ],
      "doi": "10.21437/Interspeech.2017-398"
    },
    "morise17_interspeech": {
      "authors": [
        [
          "Masanori",
          "Morise"
        ],
        [
          "Genta",
          "Miyashita"
        ],
        [
          "Kenji",
          "Ozawa"
        ]
      ],
      "title": "Low-Dimensional Representation of Spectral Envelope Without Deterioration for Full-Band Speech Analysis/Synthesis System",
      "original": "0067",
      "page_count": 5,
      "order": 84,
      "p1": "409",
      "pn": "413",
      "abstract": [
        "A speech coding for a full-band speech analysis/synthesis system is\ndescribed. In this work, full-band speech is defined as speech with\na sampling frequency above 40 kHz, whose Nyquist frequency covers the\naudible frequency range. In prior works, speech coding has generally\nfocused on the narrow-band speech with a sampling frequency below 16\nkHz. On the other hand, statistical parametric speech synthesis currently\nuses the full-band speech, and low-dimensional representation of speech\nparameters is being used. The purpose of this study is to achieve speech\ncoding without deterioration for full-band speech. We focus on a high-quality\nspeech analysis/synthesis system and mel-cepstral analysis using frequency\nwarping. In the frequency warping function, we directly use three auditory\nscales. We carried out a subjective evaluation using the WORLD vocoder\nand found that the optimum number of dimensions was around 50. The\nkind of frequency warping did not significantly affect the sound quality\nin the dimensions.\n"
      ],
      "doi": "10.21437/Interspeech.2017-67"
    },
    "loweimi17_interspeech": {
      "authors": [
        [
          "Erfan",
          "Loweimi"
        ],
        [
          "Jon",
          "Barker"
        ],
        [
          "Oscar Saz",
          "Torralba"
        ],
        [
          "Thomas",
          "Hain"
        ]
      ],
      "title": "Robust Source-Filter Separation of Speech Signal in the Phase Domain",
      "original": "0210",
      "page_count": 5,
      "order": 85,
      "p1": "414",
      "pn": "418",
      "abstract": [
        "In earlier work we proposed a framework for speech source-filter separation\nthat employs phase-based signal processing. This paper presents a further\ntheoretical investigation of the model and optimisations that make\nthe filter and source representations less sensitive to the effects\nof noise and better matched to downstream processing. To this end,\nfirst, in computing the Hilbert transform, the log function is replaced\nby the generalised logarithmic function. This introduces a tuning parameter\nthat adjusts both the dynamic range and distribution of the phase-based\nrepresentation. Second, when computing the group delay, a more robust\nestimate for the derivative is formed by applying a regression filter\ninstead of using sample differences. The effectiveness of these modifications\nis evaluated in clean and noisy conditions by considering the accuracy\nof the fundamental frequency extracted from the estimated source, and\nthe performance of speech recognition features extracted from the estimated\nfilter. In particular, the proposed filter-based front-end reduces\nAurora-2 WERs by 6.3% (average 0&#8211;20 dB) compared with previously\nreported results. Furthermore, when tested in a LVCSR task (Aurora-4)\nthe new features resulted in 5.8% absolute WER reduction compared to\nMFCCs without performance loss in the clean/matched condition.\n"
      ],
      "doi": "10.21437/Interspeech.2017-210"
    },
    "stone17_interspeech": {
      "authors": [
        [
          "Simon",
          "Stone"
        ],
        [
          "Peter",
          "Steiner"
        ],
        [
          "Peter",
          "Birkholz"
        ]
      ],
      "title": "A Time-Warping Pitch Tracking Algorithm Considering Fast f<SUB>0</SUB> Changes",
      "original": "0382",
      "page_count": 5,
      "order": 86,
      "p1": "419",
      "pn": "423",
      "abstract": [
        "Accurately tracking the fundamental frequency (f<SUB>0</SUB>) or pitch\nin speech data is of great interest in numerous contexts. All currently\navailable pitch tracking algorithms perform a short-term analysis of\na speech signal to extract the f<SUB>0</SUB> under the assumption that\nthe pitch does not change within a single analysis frame, a simplification\nthat introduces errors when the f<SUB>0</SUB> changes rather quickly\nover time. This paper proposes a new algorithm that warps the time\naxis of an analysis frame to counteract intra-frame f<SUB>0</SUB> changes\nand thus to improve the total tracking results. The algorithm was evaluated\non a set of 4718 sentences from 20 speakers (10 male, 10 female) and\nwith added white and babble noise. It was comparative in performance\nto the state-of-the-art algorithms RAPT and PRAAT to Pitch (ac) under\nclean conditions and outperformed both of them under noisy conditions.\n"
      ],
      "doi": "10.21437/Interspeech.2017-382"
    },
    "kawahara17_interspeech": {
      "authors": [
        [
          "Hideki",
          "Kawahara"
        ],
        [
          "Ken-Ichi",
          "Sakakibara"
        ],
        [
          "Masanori",
          "Morise"
        ],
        [
          "Hideki",
          "Banno"
        ],
        [
          "Tomoki",
          "Toda"
        ]
      ],
      "title": "A Modulation Property of Time-Frequency Derivatives of Filtered Phase and its Application to Aperiodicity and f<SUB>o</SUB> Estimation",
      "original": "0436",
      "page_count": 5,
      "order": 87,
      "p1": "424",
      "pn": "428",
      "abstract": [
        "We introduce a simple and linear SNR (strictly speaking, periodic to\nrandom power ratio) estimator (0 dB to 80 dB without additional calibration/linearization)\nfor providing reliable descriptions of aperiodicity in speech corpus.\nThe main idea of this method is to estimate the background random noise\nlevel without directly extracting the background noise. The proposed\nmethod is applicable to a wide variety of time windowing functions\nwith very low sidelobe levels. The estimate combines the frequency\nderivative and the time-frequency derivative of the mapping from filter\ncenter frequency to the output instantaneous frequency. This procedure\ncan replace the periodicity detection and aperiodicity estimation subsystems\nof recently introduced open source vocoder, YANG vocoder. Source code\nof MATLAB implementation of this method will also be open sourced.\n"
      ],
      "doi": "10.21437/Interspeech.2017-436"
    },
    "kumar17_interspeech": {
      "authors": [
        [
          "Avinash",
          "Kumar"
        ],
        [
          "S.",
          "Shahnawazuddin"
        ],
        [
          "Gayadhar",
          "Pradhan"
        ]
      ],
      "title": "Non-Local Estimation of Speech Signal for Vowel Onset Point Detection in Varied Environments",
      "original": "0624",
      "page_count": 5,
      "order": 88,
      "p1": "429",
      "pn": "433",
      "abstract": [
        "Vowel onset point (VOP) is an important information extensively employed\nin speech analysis and synthesis. Detecting the VOPs in a given speech\nsequence, independent of the text contexts and recording environments,\nis a challenging area of research. Performance of existing VOP detection\nmethods have not yet been extensively studied in varied environmental\nconditions. In this paper, we have exploited the non-local means estimation\nto detect those regions in the speech sequence which are of high signal-to-noise\nratio and exhibit periodicity. Mostly, those regions happen to be the\nvowel regions. This helps in overcoming the ill-effects of environmental\ndegradations. Next, for each short-time frame of estimated speech sequence,\nwe cumulatively sum the magnitude of the corresponding Fourier transform\nspectrum. The cumulative sum is then used as the feature to detect\nthe VOPs. The experiments conducted on TIMIT database show that the\nproposed approach provides better results in terms of detection and\nspurious rate when compared to a few existing methods under clean and\nnoisy test conditions.\n"
      ],
      "doi": "10.21437/Interspeech.2017-624"
    },
    "alradhi17_interspeech": {
      "authors": [
        [
          "Mohammed Salah",
          "Al-Radhi"
        ],
        [
          "Tam\u00e1s G\u00e1bor",
          "Csap\u00f3"
        ],
        [
          "G\u00e9za",
          "N\u00e9meth"
        ]
      ],
      "title": "Time-Domain Envelope Modulating the Noise Component of Excitation in a Continuous Residual-Based Vocoder for Statistical Parametric Speech Synthesis",
      "original": "0678",
      "page_count": 5,
      "order": 89,
      "p1": "434",
      "pn": "438",
      "abstract": [
        "In this paper, we present an extension of a novel continuous residual-based\nvocoder for statistical parametric speech synthesis. Previous work\nhas shown the advantages of adding envelope modulated noise to the\nvoiced excitation, but this has not been investigated yet in the context\nof continuous vocoders, i.e. of which all parameters are continuous.\nThe noise component is often not accurately modeled in modern vocoders\n(e.g. STRAIGHT). For more natural sounding speech synthesis, four time-domain\nenvelopes (Amplitude, Hilbert, Triangular and True) are investigated\nand enhanced, and then applied to the noise component of the excitation\nin our continuous vocoder. The performance evaluation is based on the\nstudy of time envelopes. In an objective experiment, we investigated\nthe Phase Distortion Deviation of vocoded samples. A MUSHRA type subjective\nlistening test was also conducted comparing natural and vocoded speech\nsamples. Both experiments have shown that the proposed framework using\nHilbert and True envelopes provides high-quality vocoding while outperforming\nthe two other envelopes.\n"
      ],
      "doi": "10.21437/Interspeech.2017-678"
    },
    "wu17b_interspeech": {
      "authors": [
        [
          "Chia-Lung",
          "Wu"
        ],
        [
          "Hsiang-Ping",
          "Hsu"
        ],
        [
          "Syu-Siang",
          "Wang"
        ],
        [
          "Jeih-Weih",
          "Hung"
        ],
        [
          "Ying-Hui",
          "Lai"
        ],
        [
          "Hsin-Min",
          "Wang"
        ],
        [
          "Yu",
          "Tsao"
        ]
      ],
      "title": "Wavelet Speech Enhancement Based on Robust Principal Component Analysis",
      "original": "0781",
      "page_count": 5,
      "order": 90,
      "p1": "439",
      "pn": "443",
      "abstract": [
        "Most state-of-the-art speech enhancement (SE) techniques prefer to\nenhance utterances in the frequency domain rather than in the time\ndomain. However, the overlap-add (OLA) operation in the short-time\nFourier transform (STFT) for speech signal processing possibly distorts\nthe signal and limits the performance of the SE techniques. In this\nstudy, a novel SE method that integrates the discrete wavelet packet\ntransform (DWPT) and a novel subspace-based method, robust principal\ncomponent analysis (RPCA), is proposed to enhance noise-corrupted signals\ndirectly in the time domain. We evaluate the proposed SE method on\nthe Mandarin hearing in noise test (MHINT) sentences. The experimental\nresults show that the new method reduces the signal distortions dramatically,\nthereby improving speech quality and intelligibility significantly.\nIn addition, the newly proposed method outperforms the STFT-RPCA-based\nspeech enhancement system.\n"
      ],
      "doi": "10.21437/Interspeech.2017-781"
    },
    "sharma17_interspeech": {
      "authors": [
        [
          "Bidisha",
          "Sharma"
        ],
        [
          "S.R. Mahadeva",
          "Prasanna"
        ]
      ],
      "title": "Vowel Onset Point Detection Using Sonority Information",
      "original": "0790",
      "page_count": 5,
      "order": 91,
      "p1": "444",
      "pn": "448",
      "abstract": [
        "Vowel onset point (VOP) refers to the starting event of a vowel, that\nmay be reflected in different aspects of the speech signal. The major\nissue in VOP detection using existing methods is the confusion among\nthe vowels and other categories of sounds preceding them. This work\nexplores the usefulness of sonority information to reduce this confusion\nand improve VOP detection. Vowels are the most sonorant sounds followed\nby semivowels, nasals, voiced fricatives, voiced stops. The sonority\nfeature is derived from the vocal-tract system, excitation source and\nsuprasegmental aspects. As this feature has the capability to discriminate\namong different sonorant sound units, it reduces the confusion among\nonset of vowels with that of other sonorant sounds. This results in\nimproved detection and resolution of VOP detection for continuous speech.\nThe performance of proposed sonority information based VOP detection\nis found to be 92.4%, compared to 85.2% by the existing method. Also\nthe resolution of localizing VOP within 10 ms is significantly enhanced\nand a performance of 73.0% is achieved as opposed to 60.2% by the existing\nmethod.\n"
      ],
      "doi": "10.21437/Interspeech.2017-790"
    },
    "laine17_interspeech": {
      "authors": [
        [
          "Unto K.",
          "Laine"
        ]
      ],
      "title": "Analytic Filter Bank for Speech Analysis, Feature Extraction and Perceptual Studies",
      "original": "1232",
      "page_count": 5,
      "order": 92,
      "p1": "449",
      "pn": "453",
      "abstract": [
        "Speech signal consists of events in time and frequency, and therefore\nits analysis with high-resolution time-frequency tools is often of\nimportance. Analytic filter bank provides a simple, fast, and flexible\nmethod to construct time-frequency representations of signals. Its\nparameters can be easily adapted to different situations from uniform\nto any auditory frequency scale, or even to a focused resolution. Since\nthe Hilbert magnitude values of the channels are obtained at every\nsample, it provides a practical tool for a high-resolution time-frequency\nanalysis.<br style=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  The present study describes the basic theory of analytic filters\nand tests their main properties. Applications of analytic filter bank\nto different speech analysis tasks including pitch period estimation\nand pitch synchronous analysis of formant frequencies and bandwidths\nare demonstrated. In addition, a new feature vector called group delay\nvector is introduced. It is shown that this representation provides\ncomparable, or even better results, than those obtained by spectral\nmagnitude feature vectors in the analysis and classification of vowels.\nThe implications of this observation are discussed also from the speech\nperception point of view.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1232"
    },
    "kroos17_interspeech": {
      "authors": [
        [
          "Christian",
          "Kroos"
        ],
        [
          "Mark D.",
          "Plumbley"
        ]
      ],
      "title": "Learning the Mapping Function from Voltage Amplitudes to Sensor Positions in 3D-EMA Using Deep Neural Networks",
      "original": "1681",
      "page_count": 5,
      "order": 93,
      "p1": "454",
      "pn": "458",
      "abstract": [
        "The first generation of three-dimensional Electromagnetic Articulography\ndevices (Carstens AG500) suffered from occasional critical tracking\nfailures. Although now superseded by new devices, the AG500 is still\nin use in many speech labs and many valuable data sets exist. In this\nstudy we investigate whether deep neural networks (DNNs) can learn\nthe mapping function from raw voltage amplitudes to sensor positions\nbased on a comprehensive movement data set. This is compared to arriving\nsample by sample at individual position values via direct optimisation\nas used in previous methods. We found that with appropriate hyperparameter\nsettings a DNN was able to approximate the mapping function with good\naccuracy, leading to a smaller error than the previous methods, but\nthat the DNN-based approach was not able to solve the tracking problem\ncompletely.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1681"
    },
    "dai17_interspeech": {
      "authors": [
        [
          "Jia",
          "Dai"
        ],
        [
          "Wei",
          "Xue"
        ],
        [
          "Wenju",
          "Liu"
        ]
      ],
      "title": "Multilingual i-Vector Based Statistical Modeling for Music Genre Classification",
      "original": "0074",
      "page_count": 5,
      "order": 94,
      "p1": "459",
      "pn": "463",
      "abstract": [
        "For music signal processing, compared with the strategy which models\neach short-time frame independently, when the long-time features are\nconsidered, the time-series characteristics of the music signal can\nbe better presented. As a typical kind of long-time modeling strategy,\nthe identification vector (i-vector) uses statistical modeling to model\nthe audio signal in the segment level. It can better capture the important\nelements of the music signal, and these important elements may benefit\nto the classification of music signal. In this paper, the i-vector\nbased statistical feature for music genre classification is explored.\nIn addition to learn enough important elements for music signal, a\nnew multilingual i-vector feature is proposed based on the multilingual\nmodel. The experimental results show that the multilingual i-vector\nbased models can achieve better classification performances than conventional\nshort-time modeling based methods.\n"
      ],
      "doi": "10.21437/Interspeech.2017-74"
    },
    "khonglah17_interspeech": {
      "authors": [
        [
          "Banriskhem K.",
          "Khonglah"
        ],
        [
          "K.T.",
          "Deepak"
        ],
        [
          "S.R. Mahadeva",
          "Prasanna"
        ]
      ],
      "title": "Indoor/Outdoor Audio Classification Using Foreground Speech Segmentation",
      "original": "0309",
      "page_count": 5,
      "order": 95,
      "p1": "464",
      "pn": "468",
      "abstract": [
        "The task of indoor/ outdoor audio classification using foreground speech\nsegmentation is attempted in this work. Foreground speech segmentation\nis the use of features to segment between foreground speech and background\ninterfering sources like noise. Initially, the foreground and background\nsegments are obtained from foreground speech segmentation by using\nthe normalized autocorrelation peak strength (NAPS) of the zero frequency\nfiltered signal (ZFFS) as a feature. The background segments are then\nconsidered for determining whether a particular segment is an indoor\nor outdoor audio sample. The mel frequency cepstral coefficients are\nobtained from the background segments of both the indoor and outdoor\naudio samples and are used to train the Support Vector Machine (SVM)\nclassifier. The use of foreground speech segmentation gives a promising\nperformance for the indoor/ outdoor audio classification task.\n"
      ],
      "doi": "10.21437/Interspeech.2017-309"
    },
    "guo17_interspeech": {
      "authors": [
        [
          "Jinxi",
          "Guo"
        ],
        [
          "Ning",
          "Xu"
        ],
        [
          "Li-Jia",
          "Li"
        ],
        [
          "Abeer",
          "Alwan"
        ]
      ],
      "title": "Attention Based CLDNNs for Short-Duration Acoustic Scene Classification",
      "original": "0440",
      "page_count": 5,
      "order": 96,
      "p1": "469",
      "pn": "473",
      "abstract": [
        "Recently, neural networks with deep architecture have been widely applied\nto acoustic scene classification. Both Convolutional Neural Networks\n(CNNs) and Long Short-Term Memory Networks (LSTMs) have shown improvements\nover fully connected Deep Neural Networks (DNNs). Motivated by the\nfact that CNNs, LSTMs and DNNs are complimentary in their modeling\ncapability, we apply the CLDNNs (Convolutional, Long Short-Term Memory,\nDeep Neural Networks) framework to short-duration acoustic scene classification\nin a unified architecture. The CLDNNs take advantage of frequency modeling\nwith CNNs, temporal modeling with LSTM, and discriminative training\nwith DNNs. Based on the CLDNN architecture, several novel attention-based\nmechanisms are proposed and applied on the LSTM layer to predict the\nimportance of each time step. We evaluate the proposed method on the\ntruncated version of the 2016 TUT acoustic scenes dataset which consists\nof recordings from 15 different scenes. By using CLDNNs with bidirectional\nLSTM, we achieve higher performance compared to the conventional neural\nnetwork architectures. Moreover, by combining the attention-weighted\noutput with LSTM final time step output, significant improvement can\nbe further achieved.\n"
      ],
      "doi": "10.21437/Interspeech.2017-440"
    },
    "xia17_interspeech": {
      "authors": [
        [
          "Xianjun",
          "Xia"
        ],
        [
          "Roberto",
          "Togneri"
        ],
        [
          "Ferdous",
          "Sohel"
        ],
        [
          "David",
          "Huang"
        ]
      ],
      "title": "Frame-Wise Dynamic Threshold Based Polyphonic Acoustic Event Detection",
      "original": "0746",
      "page_count": 5,
      "order": 97,
      "p1": "474",
      "pn": "478",
      "abstract": [
        "Acoustic event detection, the determination of the acoustic event type\nand the localisation of the event, has been widely applied in many\nreal-world applications. Many works adopt multi-label classification\ntechniques to perform the polyphonic acoustic event detection with\na global threshold to detect the active acoustic events. However, the\nglobal threshold has to be set manually and is highly dependent on\nthe database being tested. To deal with this, we replaced the fixed\nthreshold method with a frame-wise dynamic threshold approach in this\npaper. Two novel approaches, namely contour and regressor based dynamic\nthreshold approaches are proposed in this work. Experimental results\non the popular TUT Acoustic Scenes 2016 database of polyphonic events\ndemonstrated the superior performance of the proposed approaches.\n"
      ],
      "doi": "10.21437/Interspeech.2017-746"
    },
    "jang17_interspeech": {
      "authors": [
        [
          "Inseon",
          "Jang"
        ],
        [
          "ChungHyun",
          "Ahn"
        ],
        [
          "Jeongil",
          "Seo"
        ],
        [
          "Younseon",
          "Jang"
        ]
      ],
      "title": "Enhanced Feature Extraction for Speech Detection in Media Audio",
      "original": "0792",
      "page_count": 5,
      "order": 98,
      "p1": "479",
      "pn": "483",
      "abstract": [
        "Speech detection is an important first step for audio analysis on media\ncontents, whose goal is to discriminate the presence of speech from\nnon-speech. It remains a challenge owing to various sound sources included\nin media audio. In this work, we present a novel audio feature extraction\nmethod to reflect the acoustic characteristic of the media audio in\nthe time-frequency domain. Since the degree of combination of harmonic\nand percussive components varies depending on the type of sound source,\nthe audio features which further distinguish between speech and non-speech\ncan be obtained by decomposing the signal into both components. For\nthe evaluation, we use over 20 hours of drama which manually annotated\nfor speech detection as well as 4 full-length movies with annotations\nreleased for a research community, whose total length is over 8 hours.\nExperimental results with deep neural network show superior performance\nof the proposed in media audio condition.\n"
      ],
      "doi": "10.21437/Interspeech.2017-792"
    },
    "sonowal17_interspeech": {
      "authors": [
        [
          "Sukanya",
          "Sonowal"
        ],
        [
          "Tushar",
          "Sandhan"
        ],
        [
          "Inkyu",
          "Choi"
        ],
        [
          "Nam Soo",
          "Kim"
        ]
      ],
      "title": "Audio Classification Using Class-Specific Learned Descriptors",
      "original": "0982",
      "page_count": 4,
      "order": 99,
      "p1": "484",
      "pn": "487",
      "abstract": [
        "This paper presents a classification scheme for audio signals using\nhigh-level feature descriptors. The descriptor is designed to capture\nthe relevance of each acoustic feature group (or feature set like mel-frequency\ncepstral coefficients, perceptual features etc.) in recognizing an\naudio class. For this, a bank of RVM classifiers are modeled for each\n&#8216;audio class&#8217;&#8211;&#8216;feature group&#8217; pair. The\nresponse of an input signal to this bank of RVM classifiers forms the\nentries of the descriptor. Each entry of the descriptor thus measures\nthe proximity of the input signal to an audio class based on a single\nfeature group. This form of signal representation offers two-fold advantages.\nFirst, it helps to determine the effectiveness of each feature group\nin classifying a specific audio class. Second, the descriptor offers\nhigher discriminability than the low-level feature groups and a simple\nSVM classifier trained on the descriptor produces better performance\nthan several state-of-the-art methods. \n"
      ],
      "doi": "10.21437/Interspeech.2017-982"
    },
    "ebbers17_interspeech": {
      "authors": [
        [
          "Janek",
          "Ebbers"
        ],
        [
          "Jahn",
          "Heymann"
        ],
        [
          "Lukas",
          "Drude"
        ],
        [
          "Thomas",
          "Glarner"
        ],
        [
          "Reinhold",
          "Haeb-Umbach"
        ],
        [
          "Bhiksha",
          "Raj"
        ]
      ],
      "title": "Hidden Markov Model Variational Autoencoder for Acoustic Unit Discovery",
      "original": "1160",
      "page_count": 5,
      "order": 100,
      "p1": "488",
      "pn": "492",
      "abstract": [
        "Variational Autoencoders (VAEs) have been shown to provide efficient\nneural-network-based approximate Bayesian inference for observation\nmodels for which exact inference is intractable. Its extension, the\nso-called Structured VAE (SVAE) allows inference in the presence of\nboth discrete and continuous latent variables. Inspired by this extension,\nwe developed a VAE with Hidden Markov Models (HMMs) as latent models.\nWe applied the resulting HMM-VAE to the task of acoustic unit discovery\nin a zero resource scenario. Starting from an initial model based on\nvariational inference in an HMM with Gaussian Mixture Model (GMM) emission\nprobabilities, the accuracy of the acoustic unit discovery could be\nsignificantly improved by the HMM-VAE. In doing so we were able to\ndemonstrate for an unsupervised learning task what is well-known in\nthe supervised learning case: Neural networks provide superior modeling\npower compared to GMMs.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1160"
    },
    "zohrer17_interspeech": {
      "authors": [
        [
          "Matthias",
          "Z\u00f6hrer"
        ],
        [
          "Franz",
          "Pernkopf"
        ]
      ],
      "title": "Virtual Adversarial Training and Data Augmentation for Acoustic Event Detection with Gated Recurrent Neural Networks",
      "original": "1238",
      "page_count": 5,
      "order": 101,
      "p1": "493",
      "pn": "497",
      "abstract": [
        "In this paper, we use gated recurrent neural networks (GRNNs) for efficiently\ndetecting environmental events of the IEEE Detection and Classification\nof Acoustic Scenes and Events challenge (DCASE2016). For this acoustic\nevent detection task data is limited. Therefore, we propose data augmentation\nsuch as  on-the-fly shuffling and virtual adversarial training for\nregularization of the GRNNs. Both improve the performance using GRNNs.\nWe obtain a segment-based error rate of 0.59 and an F-score of 58.6%.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1238"
    },
    "mcauliffe17_interspeech": {
      "authors": [
        [
          "Michael",
          "McAuliffe"
        ],
        [
          "Michaela",
          "Socolof"
        ],
        [
          "Sarah",
          "Mihuc"
        ],
        [
          "Michael",
          "Wagner"
        ],
        [
          "Morgan",
          "Sonderegger"
        ]
      ],
      "title": "Montreal Forced Aligner: Trainable Text-Speech Alignment Using Kaldi",
      "original": "1386",
      "page_count": 5,
      "order": 102,
      "p1": "498",
      "pn": "502",
      "abstract": [
        "We present the Montreal Forced Aligner (MFA), a new open-source system\nfor speech-text alignment. MFA is an update to the Prosodylab-Aligner,\nand maintains its key functionality of trainability on new data, as\nwell as incorporating improved architecture (triphone acoustic models\nand speaker adaptation), and other features. MFA uses Kaldi instead\nof HTK, allowing MFA to be distributed as a stand-alone package, and\nto exploit parallel processing for computationally-intensive training\nand scaling to larger datasets. We evaluate MFA&#8217;s performance\non aligning word and phone boundaries in English conversational and\nlaboratory speech, relative to human-annotated boundaries, focusing\non the effects of aligner architecture and training on the data to\nbe aligned. MFA performs well relative to two existing open-source\naligners with simpler architecture (Prosodylab-Aligner and FAVE), and\nboth its improved architecture and training on data to be aligned generally\nresult in more accurate boundaries.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1386"
    },
    "meenakshi17_interspeech": {
      "authors": [
        [
          "G. Nisha",
          "Meenakshi"
        ],
        [
          "Prasanta Kumar",
          "Ghosh"
        ]
      ],
      "title": "A Robust Voiced/Unvoiced Phoneme Classification from Whispered Speech Using the &#8216;Color&#8217; of Whispered Phonemes and Deep Neural Network",
      "original": "1388",
      "page_count": 5,
      "order": 103,
      "p1": "503",
      "pn": "507",
      "abstract": [
        "In this work, we propose a robust method to perform frame-level classification\nof voiced (V) and unvoiced (UV) phonemes from whispered speech, a challenging\ntask due to its voiceless and noise-like nature. We hypothesize that\na whispered speech spectrum can be represented as a linear combination\nof a set of colored noise spectra. A five-dimensional (5D) feature\nis computed by employing non-negative matrix factorization with a fixed\nbasis dictionary, constructed using spectra of five colored noises.\nDeep Neural Network (DNN) is used as the classifier. We consider two\nbaseline features-1) Mel Frequency Cepstral Coefficients (MFCC), 2)\nfeatures computed from a data driven dictionary. Experiments reveal\nthat the features from the colored noise dictionary perform better\n(on average) than that using the data driven dictionary, with a relative\nimprovement in the average V/UV accuracy of 10.30%, within, and 10.41%,\nacross, data from seven subjects. We also find that the MFCCs and 5D\nfeatures carry complementary information regarding the nature of voicing\ndecisions in whispered speech. Hence, across all subjects, we obtain\na balanced frame-level V/UV classification performance, when MFCC and\n5D features are combined, compared to a skewed performance when they\nare considered separately.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1388"
    },
    "williams17_interspeech": {
      "authors": [
        [
          "Ian",
          "Williams"
        ],
        [
          "Petar",
          "Aleksic"
        ]
      ],
      "title": "Rescoring-Aware Beam Search for Reduced Search Errors in Contextual Automatic Speech Recognition",
      "original": "1671",
      "page_count": 5,
      "order": 104,
      "p1": "508",
      "pn": "512",
      "abstract": [
        "Using context in automatic speech recognition allows the recognition\nsystem to dynamically task-adapt and bring gains to a broad variety\nof use-cases. An important mechanism of context-inclusion is on-the-fly\nrescoring of hypotheses with contextual language model content available\nonly in real-time.<br style=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  In systems where rescoring occurs on the lattice during its construction\nas part of beam search decoding, hypotheses eligible for rescoring\nmay be missed due to pruning. This can happen for many reasons: the\nlanguage model and rescoring model may assign significantly different\nscores, there may be a lot of noise in the utterance, or word prefixes\nwith a high out-degree may necessitate aggressive pruning to keep the\nsearch tractable. This results in misrecognitions when contextually-relevant\nhypotheses are pruned before rescoring, even if a contextual rescoring\nmodel favors those hypotheses by a large margin.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  We present a technique\nto adapt the beam search algorithm to preserve hypotheses when they\nmay benefit from rescoring. We show that this technique significantly\nreduces the number of search pruning errors on rescorable hypotheses,\nwithout a significant increase in the search space size. This technique\nmakes it feasible to use one base language model, but still achieve\nhigh-accuracy speech recognition results in all contexts.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1671"
    },
    "zenkel17_interspeech": {
      "authors": [
        [
          "Thomas",
          "Zenkel"
        ],
        [
          "Ramon",
          "Sanabria"
        ],
        [
          "Florian",
          "Metze"
        ],
        [
          "Jan",
          "Niehues"
        ],
        [
          "Matthias",
          "Sperber"
        ],
        [
          "Sebastian",
          "St\u00fcker"
        ],
        [
          "Alex",
          "Waibel"
        ]
      ],
      "title": "Comparison of Decoding Strategies for CTC Acoustic Models",
      "original": "1683",
      "page_count": 5,
      "order": 105,
      "p1": "513",
      "pn": "517",
      "abstract": [
        "Connectionist Temporal Classification has recently attracted a lot\nof interest as it offers an elegant approach to building acoustic models\n(AMs) for speech recognition. The CTC loss function maps an input sequence\nof observable feature vectors to an output sequence of symbols. Output\nsymbols are conditionally independent of each other under CTC loss,\nso a language model (LM) can be incorporated conveniently during decoding,\nretaining the traditional separation of acoustic and linguistic components\nin ASR.<br style=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  For fixed vocabularies, Weighted Finite State Transducers provide\na strong baseline for efficient integration of CTC AMs with n-gram\nLMs. Character-based neural LMs provide a straight forward solution\nfor open vocabulary speech recognition and all-neural models, and can\nbe decoded with beam search. Finally, sequence-to-sequence models can\nbe used to translate a sequence of individual sounds into a word string.<br\nstyle=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  We compare the performance of these three approaches, and analyze\ntheir error patterns, which provides insightful guidance for future\nresearch and development in this important area.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1683"
    },
    "hadian17_interspeech": {
      "authors": [
        [
          "Hossein",
          "Hadian"
        ],
        [
          "Daniel",
          "Povey"
        ],
        [
          "Hossein",
          "Sameti"
        ],
        [
          "Sanjeev",
          "Khudanpur"
        ]
      ],
      "title": "Phone Duration Modeling for LVCSR Using Neural Networks",
      "original": "1680",
      "page_count": 5,
      "order": 106,
      "p1": "518",
      "pn": "522",
      "abstract": [
        "We describe our work on incorporating probabilities of phone durations,\nlearned by a neural net, into an ASR system. Phone durations are incorporated\nvia lattice rescoring. The input features are derived from the phone\nidentities of a context window of phones, plus the durations of preceding\nphones within that window. Unlike some previous work, our network outputs\nthe probability of different durations (in frames) directly, up to\na fixed limit. We evaluate this method on several large vocabulary\ntasks, and while we consistently see improvements inWord Error Rates,\nthe improvements are smaller when the lattices are generated with neural\nnet based acoustic models.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1680"
    },
    "chorowski17_interspeech": {
      "authors": [
        [
          "Jan",
          "Chorowski"
        ],
        [
          "Navdeep",
          "Jaitly"
        ]
      ],
      "title": "Towards Better Decoding and Language Model Integration in Sequence to Sequence Models",
      "original": "0343",
      "page_count": 5,
      "order": 107,
      "p1": "523",
      "pn": "527",
      "abstract": [
        "The recently proposed Sequence-to-Sequence (seq2seq) framework advocates\nreplacing complex data processing pipelines, such as an entire automatic\nspeech recognition system, with a single neural network trained in\nan end-to-end fashion. In this contribution, we analyse an attention-based\nseq2seq speech recognition system that directly transcribes recordings\ninto characters. We observe two shortcomings: overconfidence in its\npredictions and a tendency to produce incomplete transcriptions when\nlanguage models are used. We propose practical solutions to both problems\nachieving competitive speaker independent word error rates on the Wall\nStreet Journal dataset: without separate language models we reach 10.6%\nWER, while together with a trigram language model, we reach 6.7% WER,\na state-of-the-art result for HMM-free methods.\n"
      ],
      "doi": "10.21437/Interspeech.2017-343"
    },
    "li17d_interspeech": {
      "authors": [
        [
          "Wenpeng",
          "Li"
        ],
        [
          "Binbin",
          "Zhang"
        ],
        [
          "Lei",
          "Xie"
        ],
        [
          "Dong",
          "Yu"
        ]
      ],
      "title": "Empirical Evaluation of Parallel Training Algorithms on Acoustic Modeling",
      "original": "0547",
      "page_count": 5,
      "order": 108,
      "p1": "528",
      "pn": "532",
      "abstract": [
        "Deep learning models (DLMs) are state-of-the-art techniques in speech\nrecognition. However, training good DLMs can be time consuming especially\nfor production-size models and corpora. Although several parallel training\nalgorithms have been proposed to improve training efficiency, there\nis no clear guidance on which one to choose for the task in hand due\nto lack of systematic and fair comparison among them. In this paper\nwe aim at filling this gap by comparing four popular parallel training\nalgorithms in speech recognition, namely asynchronous stochastic gradient\ndescent (ASGD), blockwise model-update filtering (BMUF), bulk synchronous\nparallel (BSP) and elastic averaging stochastic gradient descent (EASGD),\non 1000-hour LibriSpeech corpora using feed-forward deep neural networks\n(DNNs) and convolutional, long short-term memory, DNNs (CLDNNs). Based\non our experiments, we recommend using BMUF as the top choice to train\nacoustic models since it is most stable, scales well with number of\nGPUs, can achieve reproducible results, and in many cases even outperforms\nsingle-GPU SGD. ASGD can be used as a substitute in some cases.\n"
      ],
      "doi": "10.21437/Interspeech.2017-547"
    },
    "xiang17_interspeech": {
      "authors": [
        [
          "Xu",
          "Xiang"
        ],
        [
          "Yanmin",
          "Qian"
        ],
        [
          "Kai",
          "Yu"
        ]
      ],
      "title": "Binary Deep Neural Networks for Speech Recognition",
      "original": "1343",
      "page_count": 5,
      "order": 109,
      "p1": "533",
      "pn": "537",
      "abstract": [
        "Deep neural networks (DNNs) are widely used in most current automatic\nspeech recognition (ASR) systems. To guarantee good recognition performance,\nDNNs usually require significant computational resources, which limits\ntheir application to low-power devices. Thus, it is appealing to reduce\nthe computational cost while keeping the accuracy. In this work, in\nlight of the success in image recognition, binary DNNs are utilized\nin speech recognition, which can achieve competitive performance and\nsubstantial speed up. To our knowledge, this is the first time that\nbinary DNNs have been used in speech recognition. For binary DNNs,\nnetwork weights and activations are constrained to be binary values,\nwhich enables faster matrix multiplication based on bit operations.\nBy exploiting the hardware population count instructions, the proposed\nbinary matrix multiplication can achieve 5&#126;7 times speed up compared\nwith highly optimized floating-point matrix multiplication. This results\nin much faster DNN inference since matrix multiplication is the most\ncomputationally expensive operation. Experiments on both TIMIT phone\nrecognition and a 50-hour Switchboard speech recognition show that,\nbinary DNNs can run about 4 times faster than standard DNNs during\ninference, with roughly 10.0% relative accuracy reduction.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1343"
    },
    "chandrashekaran17_interspeech": {
      "authors": [
        [
          "Akshay",
          "Chandrashekaran"
        ],
        [
          "Ian",
          "Lane"
        ]
      ],
      "title": "Hierarchical Constrained Bayesian Optimization for Feature, Acoustic Model and Decoder Parameter Optimization",
      "original": "1583",
      "page_count": 5,
      "order": 110,
      "p1": "538",
      "pn": "542",
      "abstract": [
        "We describe the implementation of a hierarchical constrained Bayesian\nOptimization algorithm and it&#8217;s application to joint optimization\nof features, acoustic model structure and decoding parameters for deep\nneural network (DNN)-based large vocabulary continuous speech recognition\n(LVCSR) systems. Within our hierarchical optimization method we perform\nconstrained Bayesian optimization jointly of feature hyper-parameters\nand acoustic model structure in the first-level, and then perform an\niteration of constrained Bayesian optimization for the decoder hyper-parameters\nin the second. We show the the proposed hierarchical optimization method\ncan generate a model with higher performance than a manually optimized\nsystem on a server platform. Furthermore, we demonstrate that the proposed\nframework can be used to automatically build real-time speech recognition\nsystems for graphics processing unit (GPU)-enabled embedded platforms\nthat retain similar accuracy to a server platform, while running with\nconstrained computing resources.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1583"
    },
    "toyama17_interspeech": {
      "authors": [
        [
          "Shohei",
          "Toyama"
        ],
        [
          "Daisuke",
          "Saito"
        ],
        [
          "Nobuaki",
          "Minematsu"
        ]
      ],
      "title": "Use of Global and Acoustic Features Associated with Contextual Factors to Adapt Language Models for Spontaneous Speech Recognition",
      "original": "0717",
      "page_count": 5,
      "order": 111,
      "p1": "543",
      "pn": "547",
      "abstract": [
        "In this study, we propose a new method of adapting language models\nfor speech recognition using para-linguistic and extra-linguistic features\nin speech. When we talk with others, we often change the way of lexical\nchoice and speaking style according to various contextual factors.\nThis fact indicates that the performance of automatic speech recognition\ncan be improved by taking the contextual factors into account, which\ncan be estimated from speech acoustics. In this study, we attempt to\nfind global and acoustic features that are associated with those contextual\nfactors, then integrate those features into Recurrent Neural Network\n(RNN) language models for speech recognition. In experiments, using\nJapanese spontaneous speech corpora, we examine how i-vector and openSMILE\nare associated with contextual factors. Then, we use those features\nin the reranking process of RNN-based language models. Results show\nthat perplexity is reduced by 16% relative and word error rate is reduced\nby 2.1% relative for highly emotional speech.\n"
      ],
      "doi": "10.21437/Interspeech.2017-717"
    },
    "pahuja17_interspeech": {
      "authors": [
        [
          "Vardaan",
          "Pahuja"
        ],
        [
          "Anirban",
          "Laha"
        ],
        [
          "Shachar",
          "Mirkin"
        ],
        [
          "Vikas",
          "Raykar"
        ],
        [
          "Lili",
          "Kotlerman"
        ],
        [
          "Guy",
          "Lev"
        ]
      ],
      "title": "Joint Learning of Correlated Sequence Labeling Tasks Using Bidirectional Recurrent Neural Networks",
      "original": "1247",
      "page_count": 5,
      "order": 112,
      "p1": "548",
      "pn": "552",
      "abstract": [
        "The stream of words produced by Automatic Speech Recognition (ASR)\nsystems is typically devoid of punctuations and formatting. Most natural\nlanguage processing applications expect segmented and well-formatted\ntexts as input, which is not available in ASR output. This paper proposes\na novel technique of jointly modeling multiple correlated tasks such\nas punctuation and capitalization using bidirectional recurrent neural\nnetworks, which leads to improved performance for each of these tasks.\nThis method could be extended for joint modeling of any other correlated\nsequence labeling tasks.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1247"
    },
    "shen17_interspeech": {
      "authors": [
        [
          "Xiaoyu",
          "Shen"
        ],
        [
          "Youssef",
          "Oualil"
        ],
        [
          "Clayton",
          "Greenberg"
        ],
        [
          "Mittul",
          "Singh"
        ],
        [
          "Dietrich",
          "Klakow"
        ]
      ],
      "title": "Estimation of Gap Between Current Language Models and Human Performance",
      "original": "0729",
      "page_count": 5,
      "order": 113,
      "p1": "553",
      "pn": "557",
      "abstract": [
        "Language models (LMs) have gained dramatic improvement in the past\nyears due to the wide application of neural networks. This raises the\nquestion of how far we are away from the perfect language model and\nhow much more research is needed in language modelling. As for perplexity\ngiving a value for human perplexity (as an upper bound of what is reasonably\nexpected from an LM) is difficult. Word error rate (WER) has the disadvantage\nthat it also measures the quality of other components of a speech recognizer\nlike the acoustic model and the feature extraction. We therefore suggest\nevaluating LMs in a generative setting (which has been done before\non selected hand-picked examples) and running a human evaluation on\nthe generated sentences. The results imply that LMs need about 10 to\n20 more years of research before human performance is reached. Moreover,\nwe show that the human judgement scores on the generated sentences\nand perplexity are closely correlated. This leads to an estimated perplexity\nof 12 for an LM that would be able to pass the human judgement test\nin the setting we suggested.\n"
      ],
      "doi": "10.21437/Interspeech.2017-729"
    },
    "moro17_interspeech": {
      "authors": [
        [
          "Anna",
          "Mor\u00f3"
        ],
        [
          "Gy\u00f6rgy",
          "Szasz\u00e1k"
        ]
      ],
      "title": "A Phonological Phrase Sequence Modelling Approach for Resource Efficient and Robust Real-Time Punctuation Recovery",
      "original": "0204",
      "page_count": 5,
      "order": 114,
      "p1": "558",
      "pn": "562",
      "abstract": [
        "For the automatic punctuation of Automatic Speech Recognition (ASR)\noutput, both prosodic and text based features are used, often in combination.\nPure prosody based approaches usually have low computation needs, introduce\nlittle latency (delay) and they are also more robust to ASR errors.\nText based approaches usually yield better performance, they are however\nresource demanding (both regarding their training and computational\nneeds), often introduce high time latency and are more sensitive to\nASR errors. The present paper proposes a lightweight prosody based\npunctuation approach following a new paradigm: we argue in favour of\nan all-inclusive modelling of speech prosody instead of just relying\non distinct acoustic markers: first, the entire phonological phrase\nstructure is reconstructed, then its close correlation with punctuations\nis exploited in a sequence modelling approach with recurrent neural\nnetworks. With this tiny and easy to implement model we reach performance\nin Hungarian punctuation comparable to large, text based models for\nother languages by keeping resource requirements minimal and suitable\nfor real-time operation with low latency.\n"
      ],
      "doi": "10.21437/Interspeech.2017-204"
    },
    "wang17c_interspeech": {
      "authors": [
        [
          "Lei",
          "Wang"
        ],
        [
          "Fei",
          "Chen"
        ]
      ],
      "title": "Factors Affecting the Intelligibility of Low-Pass Filtered Speech",
      "original": "0002",
      "page_count": 4,
      "order": 115,
      "p1": "563",
      "pn": "566",
      "abstract": [
        "Frequency compression is an effective alternative to conventional hearing\naids amplification for patients with severe-to-profound middle- and\nhigh-frequency hearing loss and with some low-frequency residual hearing.\nIn order to develop novel frequency compression strategy, it is important\nto first understand the mechanism for recognizing low-pass filtered\nspeech, which simulates high-frequency hearing loss. The present work\ninvestigated three factors affecting the intelligibility of low-pass\nfiltered speech, i.e., vowels, temporal fine-structure, and fundamental\nfrequency (F0) contour. Mandarin sentences were processed to generate\nthree types (i.e., vowel-only, fine-structure-only, and F0-contour-flattened)\nof low-pass filtered stimuli. Listening experiments with normal-hearing\nlisteners showed that among the three factors assessed, the vowel-only\nlow-pass filtered speech was the most intelligible, which was followed\nby the fine-structure-based low-pass filtered speech. Flattening F0-contour\nsignificantly deteriorated the intelligibility of low-pass filtered\nspeech.\n"
      ],
      "doi": "10.21437/Interspeech.2017-2"
    },
    "wang17d_interspeech": {
      "authors": [
        [
          "Shi-yu",
          "Wang"
        ],
        [
          "Fei",
          "Chen"
        ]
      ],
      "title": "Phonetic Restoration of Temporally Reversed Speech",
      "original": "0004",
      "page_count": 4,
      "order": 116,
      "p1": "567",
      "pn": "570",
      "abstract": [
        "Early study showed that temporally reversed speech may still be very\nintelligible. The present work further assessed the role of acoustic\ncues accounting for the intelligibility of temporally reversed speech.\nMandarin sentences were edited to be temporally reversed. Experiment\n1 preserved the original consonant segments, and experiment 2 only\npreserved the temporally reversed fine-structure waveform. Experimental\nresults with normal-hearing listeners showed that for Mandarin speech,\nlisteners could still perfectly understand the temporally reversed\nspeech with a reversion duration up to 50 ms. Preserving original consonant\nsegments did not significantly improve the intelligibility of the temporally\nreversed speech, suggesting that the reversion processing applied to\nvowels largely affected the intelligibility of temporally reversed\nspeech. When the local short-time envelope waveform was removed, listeners\ncould still understand stimuli with primarily temporally reversed fine-structure\nwaveform, suggesting the perceptual role of temporally reversed fine-structure\nto the intelligibility of temporally reversed speech.\n"
      ],
      "doi": "10.21437/Interspeech.2017-4"
    },
    "ishida17_interspeech": {
      "authors": [
        [
          "Mako",
          "Ishida"
        ]
      ],
      "title": "Simultaneous Articulatory and Acoustic Distortion in L1 and L2 Listening: Locally Time-Reversed &#8220;Fast&#8221; Speech",
      "original": "0083",
      "page_count": 5,
      "order": 117,
      "p1": "571",
      "pn": "575",
      "abstract": [
        "The current study explores how native and non-native speakers cope\nwith simultaneous articulatory and acoustic distortion in speech perception.\nThe articulatory distortion was generated by asking a speaker to articulate\ntarget speech as fast as possible (fast speech). The acoustic distortion\nwas created by dividing speech signals into small segments with equal\ntime duration (e.g., 50 ms) from the onset of speech, and flipping\nevery segment on a temporal axis, and putting them back together (locally\ntime-reversed speech). This study explored how &#8220;locally time-reversed\nfast speech&#8221; was intelligible as compared to &#8220;locally time-reversed\nnormal speech&#8221; measured in Ishida, Samuel, and Arai (2016). Participants\nwere native English speakers and native Japanese speakers who spoke\nEnglish as a second language. They listened to English words and pseudowords\nthat contained a lot of stop consonants. These items were spoken fast\nand locally time-reversed at every 10, 20, 30, 40, 50, or 60 ms. In\ngeneral, &#8220;locally time-reversed fast speech&#8221; became gradually\nunintelligible as the length of reversed segments increased. Native\nspeakers generally understood locally time-reversed fast spoken words\nwell but not pseudowords, while non-native speakers hardly understood\nboth words and pseudowords. Language proficiency strongly supported\nthe perceptual restoration of locally time-reversed fast speech.\n"
      ],
      "doi": "10.21437/Interspeech.2017-83"
    },
    "burchfield17_interspeech": {
      "authors": [
        [
          "L. Ann",
          "Burchfield"
        ],
        [
          "San-hei Kenny",
          "Luk"
        ],
        [
          "Mark",
          "Antoniou"
        ],
        [
          "Anne",
          "Cutler"
        ]
      ],
      "title": "Lexically Guided Perceptual Learning in Mandarin Chinese",
      "original": "0618",
      "page_count": 5,
      "order": 118,
      "p1": "576",
      "pn": "580",
      "abstract": [
        "Lexically guided perceptual learning refers to the use of lexical knowledge\nto retune speech categories and thereby adapt to a novel talker&#8217;s\npronunciation. This adaptation has been extensively documented, but\nprimarily for segmental-based learning in English and Dutch. In languages\nwith lexical tone, such as Mandarin Chinese, tonal categories can also\nbe retuned in this way, but segmental category retuning had not been\nstudied. We report two experiments in which Mandarin Chinese listeners\nwere exposed to an ambiguous mixture of [f] and [s] in lexical contexts\nfavoring an interpretation as either [f] or [s]. Listeners were subsequently\nmore likely to identify sounds along a continuum between [f] and [s],\nand to interpret minimal word pairs, in a manner consistent with this\nexposure. Thus lexically guided perceptual learning of segmental categories\nhad indeed taken place, consistent with suggestions that such learning\nmay be a universally available adaptation process.\n"
      ],
      "doi": "10.21437/Interspeech.2017-618"
    },
    "davis17_interspeech": {
      "authors": [
        [
          "Chris",
          "Davis"
        ],
        [
          "Chee Seng",
          "Chong"
        ],
        [
          "Jeesun",
          "Kim"
        ]
      ],
      "title": "The Effect of Spectral Profile on the Intelligibility of Emotional Speech in Noise",
      "original": "0948",
      "page_count": 5,
      "order": 119,
      "p1": "581",
      "pn": "585",
      "abstract": [
        "The current study investigated why the intelligibility of expressive\nspeech in noise varies as a function of the emotion expressed (e.g.,\nhappiness being more intelligible than sadness), even though the signal-to-noise\nratio is the same. We tested the straightforward proposal that the\nexpression of some emotions affect speech intelligibility by shifting\nspectral energy above the energy profile of the noise masker. This\nwas done by determining how the spectral profile of speech is affected\nby different emotional expressions using three different expressive\nspeech databases. We then examined if these changes were correlated\nwith scores produced by an objective intelligibility metric. We found\na relatively consistent shift in spectral energy for different emotions\nacross the databases and a high correlation between the extent of these\nchanges and the objective intelligibility scores. Moreover, the pattern\nof intelligibility scores is consistent with human perception studies\n(although there was considerable individual variation). We suggest\nthat the intelligibility of emotion speech in noise is simply related\nto its audibility as conditioned by the effect that the expression\nof emotion has on its spectral profile.\n"
      ],
      "doi": "10.21437/Interspeech.2017-948"
    },
    "maslowski17_interspeech": {
      "authors": [
        [
          "Merel",
          "Maslowski"
        ],
        [
          "Antje S.",
          "Meyer"
        ],
        [
          "Hans Rutger",
          "Bosker"
        ]
      ],
      "title": "Whether Long-Term Tracking of Speech Rate Affects Perception Depends on Who is Talking",
      "original": "1517",
      "page_count": 5,
      "order": 120,
      "p1": "586",
      "pn": "590",
      "abstract": [
        "Speech rate is known to modulate perception of temporally ambiguous\nspeech sounds. For instance, a vowel may be perceived as short when\nthe immediate speech context is slow, but as long when the context\nis fast. Yet, effects of long-term tracking of speech rate are largely\nunexplored. Two experiments tested whether long-term tracking of rate\ninfluences perception of the temporal Dutch vowel contrast /&#593;/-/a:/.\nIn Experiment 1, one low-rate group listened to &#8216;neutral&#8217;\nrate speech from talker A and to slow speech from talker B. Another\nhigh-rate group was exposed to the same neutral speech from A, but\nto fast speech from B. Between-group comparison of the &#8216;neutral&#8217;\ntrials revealed that the low-rate group reported a higher proportion\nof /a:/ in A&#8217;s &#8216;neutral&#8217; speech, indicating that\nA sounded faster when B was slow. Experiment 2 tested whether one&#8217;s\nown speech rate also contributes to effects of long-term tracking of\nrate. Here, talker B&#8217;s speech was replaced by playback of participants&#8217;\nown fast or slow speech. No evidence was found that one&#8217;s own\nvoice affected perception of talker A in larger speech contexts. These\nresults carry implications for our understanding of the mechanisms\ninvolved in rate-dependent speech perception and of dialogue.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1517"
    },
    "peres17_interspeech": {
      "authors": [
        [
          "Daniel Oliveira",
          "Peres"
        ],
        [
          "Dominic",
          "Watt"
        ],
        [
          "Waldemar Ferreira",
          "Netto"
        ]
      ],
      "title": "Emotional Thin-Slicing: A Proposal for a Short- and Long-Term Division of Emotional Speech",
      "original": "1719",
      "page_count": 5,
      "order": 121,
      "p1": "591",
      "pn": "595",
      "abstract": [
        "Human listeners are adept at successfully recovering linguistically-\nand socially-relevant information from very brief utterances. Studies\nusing the &#8216;thin-slicing&#8217; approach show that accurate judgments\nof the speaker&#8217;s emotional state can be made from minimal quantities\nof speech. The present experiment tested the performance of listeners\nexposed to thin-sliced samples of spoken Brazilian Portuguese selected\nto exemplify four emotions ( anger, fear, sadness, happiness). Rather\nthan attaching verbal labels to the audio samples, participants were\nasked to pair the excerpts with averaged facial images illustrating\nthe four emotion categories. Half of the listeners were native speakers\nof Brazilian Portuguese, while the others were native English speakers\nwho knew no Portuguese. Both groups of participants were found to be\naccurate and consistent in assigning the audio samples to the expected\nemotion category, but some emotions were more reliably identified than\nothers.  Fear was misidentified most frequently. We conclude that the\nphonetic cues to speakers&#8217; emotional states are sufficiently\nsalient and differentiated that listeners need only a few syllables\nupon which to base judgments, and that as a species we owe our perceptual\nsensitivity in this area to the survival value of being able to make\nrapid decisions concerning the psychological states of others.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1719"
    },
    "guevararukoz17_interspeech": {
      "authors": [
        [
          "Adriana",
          "Guevara-Rukoz"
        ],
        [
          "Erika",
          "Parlato-Oliveira"
        ],
        [
          "Shi",
          "Yu"
        ],
        [
          "Yuki",
          "Hirose"
        ],
        [
          "Sharon",
          "Peperkamp"
        ],
        [
          "Emmanuel",
          "Dupoux"
        ]
      ],
      "title": "Predicting Epenthetic Vowel Quality from Acoustics",
      "original": "1735",
      "page_count": 5,
      "order": 122,
      "p1": "596",
      "pn": "600",
      "abstract": [
        "Past research has shown that sound sequences not permitted in our native\nlanguage may be distorted by our perceptual system. A well-documented\nexample is vowel epenthesis, a phenomenon by which listeners hallucinate\nnon-existent vowels within illegal consonantal sequences. As reported\nin previous work, this occurs for instance in Japanese (JP) and Brazilian\nPortuguese (BP), languages for which the &#8216;default&#8217; epenthetic\nvowels are /u/ and /i/, respectively. In a perceptual experiment, we\ncorroborate the finding that the quality of this illusory vowel is\nlanguage-dependent, but also that this default choice can be overridden\nby coarticulatory information present on the consonant cluster. In\na second step, we analyse recordings of JP and BP speakers producing\n&#8216;epenthesized&#8217; versions of stimuli from the perceptual\ntask. Results reveal that the default vowel corresponds to the vowel\nwith the most reduced acoustic characteristics and whose formants are\nacoustically closest to formant transitions present in consonantal\nclusters. Lastly, we model behavioural responses from the perceptual\nexperiment with an exemplar model using dynamic time warping (DTW)-based\nsimilarity measures on MFCCs.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1735"
    },
    "matsui17_interspeech": {
      "authors": [
        [
          "Toshie",
          "Matsui"
        ],
        [
          "Toshio",
          "Irino"
        ],
        [
          "Kodai",
          "Yamamoto"
        ],
        [
          "Hideki",
          "Kawahara"
        ],
        [
          "Roy D.",
          "Patterson"
        ]
      ],
      "title": "The Effect of Spectral Tilt on Size Discrimination of Voiced Speech Sounds",
      "original": "0282",
      "page_count": 5,
      "order": 123,
      "p1": "601",
      "pn": "605",
      "abstract": [
        "A number of studies, with either voiced or unvoiced speech, have demonstrated\nthat a speaker&#8217;s geometric mean formant frequency (MFF) has a\nlarge effect on the perception of the speaker&#8217;s size, as would\nbe expected. One study with unvoiced speech showed that lifting the\nslope of the speech spectrum by 6 dB/octave also led to a reduction\nin the perceived size of the speaker. This paper reports an analogous\nexperiment to determine whether lifting the slope of the speech spectrum\nby 6 dB/octave affects the perception of speaker size with voiced speech\n(words). The results showed that voiced speech with high-frequency\nenhancement was perceived to arise from smaller speakers. On average,\nthe point of subjective equality in MFF discrimination was reduced\nby about 5%. However, there were large individual differences; some\nlisteners were effectively insensitive to spectral enhancement of 6\ndB/octave; others showed a consistent effect of the same enhancement.\nThe results suggest that models of speaker size perception will need\nto include a listener specific parameter for the effect of spectral\nslope.\n"
      ],
      "doi": "10.21437/Interspeech.2017-282"
    },
    "lorenzotrueba17_interspeech": {
      "authors": [
        [
          "Jaime",
          "Lorenzo-Trueba"
        ],
        [
          "Cassia Valentini",
          "Botinhao"
        ],
        [
          "Gustav Eje",
          "Henter"
        ],
        [
          "Junichi",
          "Yamagishi"
        ]
      ],
      "title": "Misperceptions of the Emotional Content of Natural and Vocoded Speech in a Car",
      "original": "0532",
      "page_count": 5,
      "order": 124,
      "p1": "606",
      "pn": "610",
      "abstract": [
        "This paper analyzes a) how often listeners interpret the emotional\ncontent of an utterance incorrectly when listening to vocoded or natural\nspeech in adverse conditions; b) which noise conditions cause the most\nmisperceptions; and c) which group of listeners misinterpret emotions\nthe most. The long-term goal is to construct new emotional speech synthesizers\nthat adapt to the environment and to the listener. We performed a large-scale\nlistening test where over 400 listeners between the ages of 21 and\n72 assessed natural and vocoded acted emotional speech stimuli. The\nstimuli had been artificially degraded using a room impulse response\nrecorded in a car and various in-car noise types recorded in a real\ncar. Experimental results show that the recognition rates for emotions\nand perceived emotional strength degrade as signal-to-noise ratio decreases.\nInterestingly, misperceptions seem to be more pronounced for negative\nand low-arousal emotions such as calmness or anger, while positive\nemotions such as happiness appear to be more robust to noise. An ANOVA\nanalysis of listener meta-data further revealed that gender and age\nalso influenced results, with elderly male listeners most likely to\nincorrectly identify emotions.\n"
      ],
      "doi": "10.21437/Interspeech.2017-532"
    },
    "niebuhr17_interspeech": {
      "authors": [
        [
          "Oliver",
          "Niebuhr"
        ],
        [
          "Jana",
          "Winkler"
        ]
      ],
      "title": "The Relative Cueing Power of F0 and Duration in German Prominence Perception",
      "original": "0375",
      "page_count": 5,
      "order": 125,
      "p1": "611",
      "pn": "615",
      "abstract": [
        "Previous studies showed for German and other (West) Germanic language,\nincluding English, that perceived syllable prominence is primarily\ncontrolled by changes in duration and F0, with the latter cue being\nmore powerful than the former. Our study is an initial approach to\ndevelop this prominence hierarchy further by putting numbers on the\ninterplay of duration and F0. German listeners indirectly judged through\nlexical identification the relative prominence levels of two neighboring\nsyllables. Results show that an increase in F0 of between 0.49 and\n0.76 st is required to outweigh the prominence effect of a 30% increase\nin duration of a neighboring syllable. These numbers are fairly stable\nacross a large range of absolute F0 and duration levels and hence useful\nin speech technology.\n"
      ],
      "doi": "10.21437/Interspeech.2017-375"
    },
    "marques17_interspeech": {
      "authors": [
        [
          "Luciana",
          "Marques"
        ],
        [
          "Rebecca",
          "Scarborough"
        ]
      ],
      "title": "Perception and Acoustics of Vowel Nasality in Brazilian Portuguese",
      "original": "0570",
      "page_count": 5,
      "order": 126,
      "p1": "616",
      "pn": "620",
      "abstract": [
        "This study explores the relationship between identification, degree\nof nasality and vowel quality in oral, nasal and nasalized vowels in\nBrazilian Portuguese. Despite common belief that the language possesses\ncontrastive nasal vowels, literature examination shows that nasal vowels\nmay be followed by a nasal resonance, while nasalized vowels must be\nfollowed by a nasal consonant. It is argued that the nasal resonance\nmay be the remains of a consonant that nasalizes the vowel, making\nnasal vowels simply coarticulatorily nasalized (e.g. [1]). If so, vowel\nnasality should not be more informative for the perception of a word\ncontaining a nasal vowel than for a word containing a nasalized vowel,\nas nasality is attributed to coarticulation. To test this hypothesis,\nrandomized stimuli containing the first syllable of words with oral,\nnasal and nasalized vowels were presented to BP listeners who had to\nidentify the stimuli original word. Preliminary results demonstrate\nthat accuracy decreased for nasal and nasalized stimuli. A comparison\nbetween patterns of response to measured degrees of vowel acoustic\nnasality and formant values demonstrate that vowel quality differences\nmay play a more relevant role in word identification than type of nasality\nin a vowel.\n"
      ],
      "doi": "10.21437/Interspeech.2017-570"
    },
    "kim17b_interspeech": {
      "authors": [
        [
          "Jonny",
          "Kim"
        ],
        [
          "Katie",
          "Drager"
        ]
      ],
      "title": "Sociophonetic Realizations Guide Subsequent Lexical Access",
      "original": "1742",
      "page_count": 5,
      "order": 127,
      "p1": "621",
      "pn": "625",
      "abstract": [
        "Previous studies on spoken word recognition suggest that lexical access\nis facilitated when social information attributed to the voice is congruent\nwith the social characteristics associated with the word. This paper\nbuilds on this work, presenting results from a lexical decision task\nin which target words associated with different age groups were preceded\nby sociophonetic primes. No age-related phonetic cues were provided\nwithin the target words; instead, the non-related prime words contained\na sociophonetic variable involved in ongoing change. We found that\nage-associated words are recognized faster when preceded by an age-congruent\nphonetic variant in the prime word. The results demonstrate that lexical\naccess is influenced by sociophonetic variation, a result which we\nargue arises from experience-based probabilities of covariation between\nsounds and words.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1742"
    },
    "silva17_interspeech": {
      "authors": [
        [
          "Samuel",
          "Silva"
        ],
        [
          "Ant\u00f3nio",
          "Teixeira"
        ]
      ],
      "title": "Critical Articulators Identification from RT-MRI of the Vocal Tract",
      "original": "0742",
      "page_count": 5,
      "order": 128,
      "p1": "626",
      "pn": "630",
      "abstract": [
        "Several technologies, such as electromagnetic midsagittal articulography\n(EMA) or real-time magnetic resonance (RT-MRI), enable studying the\nstatic and dynamic aspects of speech production. The resulting knowledge\ncan, in turn, inform the improvement of speech production models, e.g.,\nfor articulatory speech synthesis, by enabling the identification of\nwhich articulators and gestures are involved in producing specific\nsounds.<br style=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  The amount of data available from these technologies, and the\nneed for a systematic quantitative assessment, advise tackling these\nmatters through data-driven approaches, preferably unsupervised, since\nannotated data is scarce. In this context, a method for statistical\nidentification of critical articulators has been proposed, in the literature,\nand successfully applied to EMA data. However, the many differences\nregarding the data available from other technologies, such as RT-MRI,\nand language-specific aspects create a challenging setting for its\ndirect and wider applicability.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  In this article, we\naddress the steps needed to extend the applicability of the proposed\nstatistical analyses, initially applied to EMA, to an existing RT-MRI\ncorpus and test it for a different language, European Portuguese. The\nobtained results, for three speakers, and considering 33 phonemes,\nprovide phonologically meaningful critical articulator outcomes and\nshow evidence of the applicability of the method to RT-MRI.\n"
      ],
      "doi": "10.21437/Interspeech.2017-742"
    },
    "somandepalli17_interspeech": {
      "authors": [
        [
          "Krishna",
          "Somandepalli"
        ],
        [
          "Asterios",
          "Toutios"
        ],
        [
          "Shrikanth S.",
          "Narayanan"
        ]
      ],
      "title": "Semantic Edge Detection for Tracking Vocal Tract Air-Tissue Boundaries in Real-Time Magnetic Resonance Images",
      "original": "1580",
      "page_count": 5,
      "order": 129,
      "p1": "631",
      "pn": "635",
      "abstract": [
        "Recent developments in real-time magnetic resonance imaging (rtMRI)\nhave enabled the study of vocal tract dynamics during production of\nrunning speech at high frame rates (e.g., 83 frames per second). Such\nlarge amounts of acquired data require scalable automated methods to\nidentify different articulators (e.g., tongue, velum) for further analysis.\nIn this paper, we propose a convolutional neural network with an encoder-decoder\narchitecture to jointly detect the relevant air-tissue boundaries as\nwell as to label them, which we refer to as &#8216;semantic edge detection&#8217;.\nWe pose this as a pixel labeling problem, with the outline contour\nof each articulator of interest as positive class and the remaining\ntissue and airway as negative classes. We introduce a loss function\nmodified with additional penalty for misclassification at air-tissue\nboundaries to account for class imbalance and improve edge localization.\nWe then use a greedy search algorithm to draw contours from the probability\nmaps of the positive classes predicted by the network. The articulator\ncontours obtained by our method are comparable to the true labels generated\nby iteratively fitting a manually created subject-specific template.\nOur results generalize well across subjects and different vocal tract\npostures, demonstrating a significant improvement over the structured\nregression baseline.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1580"
    },
    "asadiabadi17_interspeech": {
      "authors": [
        [
          "Sasan",
          "Asadiabadi"
        ],
        [
          "Engin",
          "Erzin"
        ]
      ],
      "title": "Vocal Tract Airway Tissue Boundary Tracking for rtMRI Using Shape and Appearance Priors",
      "original": "1016",
      "page_count": 5,
      "order": 130,
      "p1": "636",
      "pn": "640",
      "abstract": [
        "Knowledge about the dynamic shape of the vocal tract is the basis of\nmany speech production applications such as, articulatory analysis,\nmodeling and synthesis. Vocal tract airway tissue boundary segmentation\nin the mid-sagittal plane is necessary as an initial step for extraction\nof the cross-sectional area function. This segmentation problem is\nhowever challenging due to poor resolution of real-time speech MRI,\ngrainy noise and the rapidly varying vocal tract shape. We present\na novel approach to vocal tract airway tissue boundary tracking by\ntraining a statistical shape and appearance model for human vocal tract.\nWe manually segment a set of vocal tract profiles and utilize a statistical\napproach to train a shape and appearance model for the tract. An active\ncontour approach is employed to segment the airway tissue boundaries\nof the vocal tract while restricting the curve movement to the trained\nshape and appearance model. Then the contours in subsequent frames\nare tracked using dense motion estimation methods. Experimental evaluations\nover the mean square error metric indicate significant improvements\ncompared to the state-of-the-art.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1016"
    },
    "ananthapadmanabha17_interspeech": {
      "authors": [
        [
          "T.V.",
          "Ananthapadmanabha"
        ],
        [
          "A.G.",
          "Ramakrishnan"
        ],
        [
          "Shubham",
          "Sharma"
        ]
      ],
      "title": "An Objective Critical Distance Measure Based on the Relative Level of Spectral Valley",
      "original": "0201",
      "page_count": 4,
      "order": 131,
      "p1": "641",
      "pn": "644",
      "abstract": [
        "Spectral integration is a subjective phenomenon in which a vowel with\ntwo formants, spaced below a critical distance, is perceived to be\nof the same phonetic quality as that of a vowel with a single formant.\nIt is tedious to conduct perceptual tests to determine the critical\ndistance for various experimental conditions. To alleviate this difficulty,\nwe propose an objective critical distance (OCD) that can be determined\nfrom the spectral envelope of a speech signal. OCD is defined as that\nspacing between the adjacent formants when the level of the spectral\nvalley between them reaches the mean spectral value. The measured OCD\nlies in the same range of 3 to 3.5 Bark as the subjective critical\ndistance for similar experimental conditions giving credibility to\nthe definition. However, it is noted that OCD for front vowels is significantly\ndifferent from that for the back vowels.\n"
      ],
      "doi": "10.21437/Interspeech.2017-201"
    },
    "sorensen17_interspeech": {
      "authors": [
        [
          "Tanner",
          "Sorensen"
        ],
        [
          "Zisis",
          "Skordilis"
        ],
        [
          "Asterios",
          "Toutios"
        ],
        [
          "Yoon-Chul",
          "Kim"
        ],
        [
          "Yinghua",
          "Zhu"
        ],
        [
          "Jangwon",
          "Kim"
        ],
        [
          "Adam",
          "Lammert"
        ],
        [
          "Vikram",
          "Ramanarayanan"
        ],
        [
          "Louis",
          "Goldstein"
        ],
        [
          "Dani",
          "Byrd"
        ],
        [
          "Krishna",
          "Nayak"
        ],
        [
          "Shrikanth S.",
          "Narayanan"
        ]
      ],
      "title": "Database of Volumetric and Real-Time Vocal Tract MRI for Speech Science",
      "original": "0608",
      "page_count": 5,
      "order": 132,
      "p1": "645",
      "pn": "649",
      "abstract": [
        "We present the USC Speech and Vocal Tract Morphology MRI Database,\na 17-speaker magnetic resonance imaging database for speech research.\nThe database consists of real-time magnetic resonance images (rtMRI)\nof dynamic vocal tract shaping, denoised audio recorded simultaneously\nwith rtMRI, and 3D volumetric MRI of vocal tract shapes during sustained\nspeech sounds. We acquired 2D real-time MRI of vocal tract shaping\nduring consonant-vowel-consonant sequences, vowel-consonant-vowel sequences,\nread passages, and spontaneous speech. We acquired 3D volumetric MRI\nof the full set of vowels and continuant consonants of American English.\nEach 3D volumetric MRI was acquired in one 7-second scan in which the\nparticipant sustained the sound. This is the first database to combine\nrtMRI of dynamic vocal tract shaping and 3D volumetric MRI of the entire\nvocal tract. The database provides a unique resource with which to\nexamine the relationship between vocal tract morphology and vocal tract\nfunction. The USC Speech and Vocal Tract Morphology MRI Database is\nprovided free for research use at  http://sail.usc.edu/span/morphdb.\n"
      ],
      "doi": "10.21437/Interspeech.2017-608"
    },
    "cao17b_interspeech": {
      "authors": [
        [
          "Chong",
          "Cao"
        ],
        [
          "Yanlu",
          "Xie"
        ],
        [
          "Qi",
          "Zhang"
        ],
        [
          "Jinsong",
          "Zhang"
        ]
      ],
      "title": "The Influence on Realization and Perception of Lexical Tones from Affricate&#8217;s Aspiration",
      "original": "1267",
      "page_count": 5,
      "order": 133,
      "p1": "650",
      "pn": "654",
      "abstract": [
        "Consonants in /CV/ syllables usually have potential influence on onset\nfundamental frequency (i.e., onset f0) of succeeding vowels. Previous\nstudies showed such effect with respect to the aspiration of stops\nwith evidence from Mandarin, a tonal language. While few studies investigated\nthe effect on onset f0 from the aspiration of affricates. The differences\nbetween stops and affricates in aspiration leave space for further\ninvestigations. We examined the effect of affricate&#8217;s aspiration\non the realization of onset f0 of following vowels in the form of isolated\nsyllables and continuous speech by reference to a minimal pair of syllables\nwhich differ only in aspiration. Besides, we conducted tone identification\ntests using two sets of tone continua based on the same minimal pair\nof syllables. Experimental results showed that the aspirated syllables\nincreased the onset f0 of following vowels compared with unaspirated\ncounterparts in both kinds of contexts. While the magnitude of differences\nvaried with tones. And the perception results showed that aspirated\nsyllables tended to be perceived as tones that have relative lower\nonset f0, which in turn supported the production result. The present\nstudy may have applications for speech identification and speech synthesis.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1267"
    },
    "franken17_interspeech": {
      "authors": [
        [
          "Matthias K.",
          "Franken"
        ],
        [
          "Frank",
          "Eisner"
        ],
        [
          "Jan-Mathijs",
          "Schoffelen"
        ],
        [
          "Daniel J.",
          "Acheson"
        ],
        [
          "Peter",
          "Hagoort"
        ],
        [
          "James M.",
          "McQueen"
        ]
      ],
      "title": "Audiovisual Recalibration of Vowel Categories",
      "original": "0122",
      "page_count": 4,
      "order": 134,
      "p1": "655",
      "pn": "658",
      "abstract": [
        "One of the most daunting tasks of a listener is to map a continuous\nauditory stream onto known speech sound categories and lexical items.\nA major issue with this mapping problem is the variability in the acoustic\nrealizations of sound categories, both within and across speakers.\nPast research has suggested listeners may use visual information (e.g.,\nlip-reading) to calibrate these speech categories to the current speaker.\nPrevious studies have focused on audiovisual recalibration of consonant\ncategories. The present study explores whether vowel categorization,\nwhich is known to show less sharply defined category boundaries, also\nbenefit from visual cues.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  Participants were\nexposed to videos of a speaker pronouncing one out of two vowels, paired\nwith audio that was ambiguous between the two vowels. After exposure,\nit was found that participants had recalibrated their vowel categories.\nIn addition, individual variability in audiovisual recalibration is\ndiscussed. It is suggested that listeners&#8217; category sharpness\nmay be related to the weight they assign to visual information in audiovisual\nspeech perception. Specifically, listeners with less sharp categories\nassign more weight to visual information during audiovisual speech\nrecognition.\n"
      ],
      "doi": "10.21437/Interspeech.2017-122"
    },
    "peters17_interspeech": {
      "authors": [
        [
          "Judith",
          "Peters"
        ],
        [
          "Marieke",
          "Hoetjes"
        ]
      ],
      "title": "The Effect of Gesture on Persuasive Speech",
      "original": "0194",
      "page_count": 5,
      "order": 135,
      "p1": "659",
      "pn": "663",
      "abstract": [
        "Speech perception is multimodal, with not only speech, but also gesture\npresumably playing a role in how a message is perceived. However, there\nhave not been many studies on the effect that hand gestures may have\non speech perception in general, and on persuasive speech in particular.\nMoreover, we do not yet know whether an effect of gestures may be larger\nwhen addressees are not involved in the topic of the discourse, and\nare therefore more focused on peripheral cues, rather than the content\nof the message. In the current study participants were shown a speech\nwith or without gestures. Some participants were involved in the topic\nof the speech, others were not. We studied five measures of persuasiveness.\nResults showed that for all but one measure, viewing the video with\naccompanying gestures made the speech more persuasive. In addition,\nthere were several interactions, showing that the performance of the\nspeaker and the factual accuracy of the speech scored high especially\nfor those participants who not only saw gestures but were also not\ninvolved in the topic of the speech.\n"
      ],
      "doi": "10.21437/Interspeech.2017-194"
    },
    "lai17_interspeech": {
      "authors": [
        [
          "Wei",
          "Lai"
        ]
      ],
      "title": "Auditory-Visual Integration of Talker Gender in Cantonese Tone Perception",
      "original": "1069",
      "page_count": 5,
      "order": 136,
      "p1": "664",
      "pn": "668",
      "abstract": [
        "This study investigated the auditory-visual integration of talker gender\nin the perception of tone variances. Two experiments were conducted\nto evaluate how listeners use the information of talker gender to adjust\ntheir expectation towards speakers&#8217; pitch range and uncover intended\ntonal targets in Cantonese tone perception. Results from an audio-only\ntone identification task showed that tone categorization along the\nsame pitch continuum shifted under different conditions of voice gender.\nListeners generally heard a tone of lower pitch when the word was produced\nby a female voice, while they heard a tone of higher pitch when the\nword was produced at the same pitch level by a male voice. Results\nfrom an audio-visual tone identification task showed that tone categorization\nalong the same pitch continuum shifted under different conditions of\nface gender, despite the fact that the photos of different genders\nwere disguised for the same set of stimuli in identical voices with\nidentical pitch heights. These findings show that gender normalization\nplays a role in uncovering linguistic pitch targets, and lend support\nto a hypothesis according to which listeners make use of socially constructed\nstereotypes to facilitate their basic phonological categorization in\nspeech perception and processing.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1069"
    },
    "ito17_interspeech": {
      "authors": [
        [
          "Takayuki",
          "Ito"
        ],
        [
          "Hiroki",
          "Ohashi"
        ],
        [
          "Eva",
          "Montas"
        ],
        [
          "Vincent L.",
          "Gracco"
        ]
      ],
      "title": "Event-Related Potentials Associated with Somatosensory Effect in Audio-Visual Speech Perception",
      "original": "0139",
      "page_count": 5,
      "order": 137,
      "p1": "669",
      "pn": "673",
      "abstract": [
        "Speech perception often involves multisensory processing. Although\nprevious studies have demonstrated visual [1, 2] and somatosensory\ninteractions [3, 4] with auditory processing, it is not clear whether\nsomatosensory information can contribute to the processing of audio-visual\nspeech perception. This study explored the neural consequence of somatosensory\ninteractions in audio-visual speech processing. We assessed whether\nsomatosensory orofacial stimulation influenced event-related potentials\n(ERPs) in response to an audio-visual speech illusion (the McGurk Effect\n[1]). 64 scalp sites of ERPs were recorded in response to audio-visual\nspeech stimulation and somatosensory stimulation. In the audio-visual\ncondition, an auditory stimulus /ba/ was synchronized with the video\nof congruent facial motion (the production of /ba/) or incongruent\nfacial motion (the production of the /da/: McGurk condition). These\ntwo audio-visual stimulations were randomly presented with and without\nsomatosensory stimulation associated with facial skin deformation.\nWe found ERPs differences associated with the McGurk effect in the\npresence of the somatosensory conditions. ERPs for the McGurk effect\nreliably diverge around 280 ms after auditory onset. The results demonstrate\na change of cortical potential of audio-visual processing due to somatosensory\ninputs and suggest that somatosensory information encoding facial motion\nalso influences speech processing.\n"
      ],
      "doi": "10.21437/Interspeech.2017-139"
    },
    "renner17_interspeech": {
      "authors": [
        [
          "Lena F.",
          "Renner"
        ],
        [
          "Marcin",
          "W\u0142odarczak"
        ]
      ],
      "title": "When a Dog is a Cat and How it Changes Your Pupil Size: Pupil Dilation in Response to Information Mismatch",
      "original": "0353",
      "page_count": 5,
      "order": 138,
      "p1": "674",
      "pn": "678",
      "abstract": [
        "In the present study, we investigate pupil dilation as a measure of\nlexical retrieval. We captured pupil size changes in reaction to a\nmatch or a mismatch between a picture and an auditorily presented word\nin 120 trials presented to ten native speakers of Swedish. In each\ntrial a picture was displayed for six seconds, and 2.5 seconds into\nthe trial the word was played through loudspeakers. The picture and\nthe word were matching in half of the trials, and all stimuli were\ncommon high-frequency monosyllabic Swedish words. The difference in\npupil diameter trajectories across the two conditions was analyzed\nwith Functional Data Analysis. In line with the expectations, the results\nindicate greater dilation in the mismatch condition starting from around\n800 ms after the stimulus onset. Given that similar processes were\nobserved in brain imaging studies, pupil dilation measurements seem\nto provide an appropriate tool to reveal lexical retrieval. The results\nsuggest that pupillometry could be a viable alternative to existing\nmethods in the field of speech and language processing, for instance\nacross different ages and clinical groups.\n"
      ],
      "doi": "10.21437/Interspeech.2017-353"
    },
    "kyaw17_interspeech": {
      "authors": [
        [
          "Win Thuzar",
          "Kyaw"
        ],
        [
          "Yoshinori",
          "Sagisaka"
        ]
      ],
      "title": "Cross-Modal Analysis Between Phonation Differences and Texture Images Based on Sentiment Correlations",
      "original": "1236",
      "page_count": 5,
      "order": 139,
      "p1": "679",
      "pn": "683",
      "abstract": [
        "Motivated by the success of speech characteristics representation by\ncolor attributes, we analyzed the cross-modal sentiment correlations\nbetween voice source characteristics and textural image characteristics.\nFor the analysis, we employed vowel sounds with representative three\nphonation differences (modal, creaky and breathy) and 36 texture images\nwith 36 semantic attributes (e.g., banded, cracked and scaly) annotated\none semantic attribute for each texture. By asking 40 subjects to select\nthe most fitted textures from 36 figures with different textures after\nlistening 30 speech samples with different phonations, we measured\nthe correlations between acoustic parameters showing voice source variations\nand the parameters of selected textural image differences showing coarseness,\ncontrast, directionality, busyness, complexity and strength. From the\ntexture classifications, voice characteristics can be roughly characterized\nby textural differences: modal &#8212; gauzy, banded and smeared, creaky\n&#8212; porous, crystalline, cracked and scaly, breathy &#8212; smeared,\nfreckled and stained. We have also found significant correlations between\nvoice source acoustic parameters and textural parameters. These correlations\nsuggest the possibility of cross-modal mapping between voice source\ncharacteristics and textural parameters, which enables visualization\nof speech information with source variations reflecting human sentiment\nperception.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1236"
    },
    "mehta17_interspeech": {
      "authors": [
        [
          "Daryush D.",
          "Mehta"
        ],
        [
          "Patrick C.",
          "Chwalek"
        ],
        [
          "Thomas F.",
          "Quatieri"
        ],
        [
          "Laura J.",
          "Brattain"
        ]
      ],
      "title": "Wireless Neck-Surface Accelerometer and Microphone on Flex Circuit with Application to Noise-Robust Monitoring of Lombard Speech",
      "original": "0048",
      "page_count": 5,
      "order": 140,
      "p1": "684",
      "pn": "688",
      "abstract": [
        "Ambulatory monitoring of real-world voice characteristics and behavior\nhas the potential to provide important assessment of voice and speech\ndisorders and psychological and emotional state. In this paper, we\nreport on the novel development of a lightweight, wireless voice monitor\nthat synchronously records dual-channel data from an acoustic microphone\nand a neck-surface accelerometer embedded on a flex circuit. In this\npaper, Lombard speech effects were investigated in pilot data from\nfour adult speakers with normal vocal function who read a phonetically\nbalanced paragraph in the presence of different ambient acoustic noise\nlevels. Whereas the signal-to-noise ratio (SNR) of the microphone signal\ndecreased in the presence of increasing ambient noise level, the SNR\nof the accelerometer sensor remained high. Lombard speech properties\nwere thus robustly computed from the accelerometer signal and observed\nin all four speakers who exhibited increases in average estimates of\nsound pressure level (+2.3 dB), fundamental frequency (+21.4 Hz), and\ncepstral peak prominence (+1.3 dB) from quiet to loud ambient conditions.\nFuture work calls for ambulatory data collection in naturalistic environments,\nwhere the microphone acts as a sound level meter and the accelerometer\nfunctions as a noise-robust voicing sensor to assess voice disorders,\nneurological conditions, and cognitive load.\n"
      ],
      "doi": "10.21437/Interspeech.2017-48"
    },
    "bandini17_interspeech": {
      "authors": [
        [
          "Andrea",
          "Bandini"
        ],
        [
          "Aravind",
          "Namasivayam"
        ],
        [
          "Yana",
          "Yunusova"
        ]
      ],
      "title": "Video-Based Tracking of Jaw Movements During Speech: Preliminary Results and Future Directions",
      "original": "1371",
      "page_count": 5,
      "order": 141,
      "p1": "689",
      "pn": "693",
      "abstract": [
        "Facial (e.g., lips and jaw) movements can provide important information\nfor the assessment, diagnosis and treatment of motor speech disorders.\nHowever, due to the high costs of the instrumentation used to record\nspeech movements, such information is typically limited to research\nstudies. With the recent development of depth sensors and efficient\nalgorithms for facial tracking, clinical applications of this technology\nmay be possible. Although lip tracking methods have been validated\nin the past, jaw tracking remains a challenge. In this study, we assessed\nthe accuracy of tracking jaw movements with a video-based system composed\nof a face tracker and a depth sensor, specifically developed for short\nrange applications (Intel RealSense SR300). The assessment was performed\non healthy subjects during speech and non-speech tasks. Preliminary\nresults showed that jaw movements can be tracked with reasonable accuracy\n(RMSE&#8776;2mm), with better performance for slow movements. Further\ntests are needed in order to improve the performance of these systems\nand develop accurate methodologies that can reveal subtle changes in\njaw movements for the assessment and treatment of motor speech disorders.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1371"
    },
    "sb17_interspeech": {
      "authors": [
        [
          "Sunil Kumar",
          "S.B."
        ],
        [
          "K. Sreenivasa",
          "Rao"
        ],
        [
          "Tanumay",
          "Mandal"
        ]
      ],
      "title": "Accurate Synchronization of Speech and EGG Signal Using Phase Information",
      "original": "1374",
      "page_count": 5,
      "order": 142,
      "p1": "694",
      "pn": "698",
      "abstract": [
        "Synchronization of speech and corresponding Electroglottographic (EGG)\nsignal is very helpful for speech processing research and development.\nDuring simultaneous recording of speech and EGG signals, the speech\nsignal will be delayed by the duration corresponding to the speech\nwave propagation from the glottis to the microphone relative to the\nEGG signal. Even in same session of recording, the delay between the\nspeech and the EGG signals is varying due to the natural movement of\nspeaker&#8217;s head and movement of microphone in case MIC is held\nby hand. To study and model the information within glottal cycles,\nprecise synchronization of speech and EGG signals is of utmost necessity.\nIn this work, we propose a method for synchronization of speech and\nEGG signals based on the glottal activity information present in the\nsignals. The performance of the proposed method is demonstrated by\nestimation of delay between the two signals (speech signals and corresponding\nEGG signals) and synchronizing these signals by compensating the estimated\ndelay. The CMU-Arctic database consist of simultaneous recording of\nthe speech and the EGG signals is used for the evaluation of the proposed\nmethod.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1374"
    },
    "romren17_interspeech": {
      "authors": [
        [
          "Anna Sara H.",
          "Rom\u00f8ren"
        ],
        [
          "Aoju",
          "Chen"
        ]
      ],
      "title": "The Acquisition of Focal Lengthening in Stockholm Swedish",
      "original": "1065",
      "page_count": 5,
      "order": 143,
      "p1": "699",
      "pn": "703",
      "abstract": [
        "In order to be efficient communicators, children need to adapt their\nutterances to the common ground shared between themselves and their\nconversational partners. One way of doing this is by prosodically highlighting\nfocal information. In this paper we look at one specific prosodic manipulation,\nnamely word duration, asking whether Swedish-speaking children lengthen\nwords to mark focus, as compared to adult controls. To the best of\nour knowledge, this is the first study on the relationship between\nfocus and word duration in Swedish-speaking children.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1065"
    },
    "zhou17_interspeech": {
      "authors": [
        [
          "Shiyu",
          "Zhou"
        ],
        [
          "Yuanyuan",
          "Zhao"
        ],
        [
          "Shuang",
          "Xu"
        ],
        [
          "Bo",
          "Xu"
        ]
      ],
      "title": "Multilingual Recurrent Neural Networks with Residual Learning for Low-Resource Speech Recognition",
      "original": "0111",
      "page_count": 5,
      "order": 144,
      "p1": "704",
      "pn": "708",
      "abstract": [
        "The shared-hidden-layer multilingual deep neural network (SHL-MDNN),\nin which the hidden layers of feed-forward deep neural network (DNN)\nare shared across multiple languages while the softmax layers are language\ndependent, has been shown to be effective on acoustic modeling of multilingual\nlow-resource speech recognition. In this paper, we propose that the\nshared-hidden-layer with Long Short-Term Memory (LSTM) recurrent neural\nnetworks can achieve further performance improvement considering LSTM\nhas outperformed DNN as the acoustic model of automatic speech recognition\n(ASR). Moreover, we reveal that shared-hidden-layer multilingual LSTM\n(SHL-MLSTM) with residual learning can yield additional moderate but\nconsistent gain from multilingual tasks given the fact that residual\nlearning can alleviate the degradation problem of deep LSTMs. Experimental\nresults demonstrate that SHL-MLSTM can relatively reduce word error\nrate (WER) by 2.1&#8211;6.8% over SHL-MDNN trained using six languages\nand 2.6&#8211;7.3% over monolingual LSTM trained using the language\nspecific data on CALLHOME datasets. Additional WER reduction, about\nrelatively 2% over SHL-MLSTM, can be obtained through residual learning\non CALLHOME datasets, which demonstrates residual learning is useful\nfor SHL-MLSTM on multilingual low-resource ASR.\n"
      ],
      "doi": "10.21437/Interspeech.2017-111"
    },
    "siohan17_interspeech": {
      "authors": [
        [
          "Olivier",
          "Siohan"
        ]
      ],
      "title": "CTC Training of Multi-Phone Acoustic Models for Speech Recognition",
      "original": "0505",
      "page_count": 5,
      "order": 145,
      "p1": "709",
      "pn": "713",
      "abstract": [
        "Phone-sized acoustic units such as triphones cannot properly capture\nthe long-term co-articulation effects that occur in spontaneous speech.\nFor that reason, it is interesting to construct acoustic units covering\na longer time-span such as syllables or words. Unfortunately, the frequency\ndistribution of those units is such that a few high frequency units\naccount for most of the tokens, while many units rarely occur. As a\nresult, those units suffer from data sparsity and can be difficult\nto train. In this paper we propose a scalable data-driven approach\nto construct a set of salient units made of sequences of phones called\nM-phones. We illustrate that since the decomposition of a word sequence\ninto a sequence of M-phones is ambiguous, those units are well suited\nto be used with a connectionist temporal classification (CTC) approach\nwhich does not rely on an explicit frame-level segmentation of the\nword sequence into a sequence of acoustic units. Experiments are presented\non a Voice Search task using 12,500 hours of training data.\n"
      ],
      "doi": "10.21437/Interspeech.2017-505"
    },
    "tong17_interspeech": {
      "authors": [
        [
          "Sibo",
          "Tong"
        ],
        [
          "Philip N.",
          "Garner"
        ],
        [
          "Herv\u00e9",
          "Bourlard"
        ]
      ],
      "title": "An Investigation of Deep Neural Networks for Multilingual Speech Recognition Training and Adaptation",
      "original": "1242",
      "page_count": 5,
      "order": 146,
      "p1": "714",
      "pn": "718",
      "abstract": [
        "Different training and adaptation techniques for multilingual Automatic\nSpeech Recognition (ASR) are explored in the context of hybrid systems,\nexploiting Deep Neural Networks (DNN) and Hidden Markov Models (HMM).\nIn multilingual DNN training, the hidden layers (possibly extracting\nbottleneck features) are usually shared across languages, and the output\nlayer can either model multiple sets of language-specific senones or\none single universal IPA-based multilingual senone set. Both architectures\nare investigated, exploiting and comparing different language adaptive\ntraining (LAT) techniques originating from successful DNN-based speaker-adaptation.\nMore specifically, speaker adaptive training methods such as Cluster\nAdaptive Training (CAT) and Learning Hidden Unit Contribution (LHUC)\nare considered. In addition, a language adaptive output architecture\nfor IPA-based universal DNN is also studied and tested.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  Experiments show that\nLAT improves the performance and adaptation on the top layer further\nimproves the accuracy. By combining state-level minimum Bayes risk\n(sMBR) sequence training with LAT, we show that a language adaptively\ntrained IPA-based universal DNN outperforms a monolingually sequence\ntrained model.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1242"
    },
    "karafiat17_interspeech": {
      "authors": [
        [
          "Martin",
          "Karafi\u00e1t"
        ],
        [
          "Murali Karthick",
          "Baskar"
        ],
        [
          "Pavel",
          "Mat\u011bjka"
        ],
        [
          "Karel",
          "Vesel\u00fd"
        ],
        [
          "Franti\u0161ek",
          "Gr\u00e9zl"
        ],
        [
          "Luk\u00e1\u0161",
          "Burget"
        ],
        [
          "Jan",
          "\u010cernock\u00fd"
        ]
      ],
      "title": "2016 BUT Babel System: Multilingual BLSTM Acoustic Model with i-Vector Based Adaptation",
      "original": "1775",
      "page_count": 5,
      "order": 147,
      "p1": "719",
      "pn": "723",
      "abstract": [
        "The paper provides an analysis of BUT automatic speech recognition\nsystems (ASR) built for the 2016 IARPA Babel evaluation. The IARPA\nBabel program concentrates on building ASR system for many low resource\nlanguages, where only a limited amount of transcribed speech is available\nfor each language. In such scenario, we found essential to train the\nASR systems in a multilingual fashion. In this work, we report superior\nresults obtained with pre-trained multilingual BLSTM acoustic models,\nwhere we used multi-task training with separate classification layer\nfor each language. The results reported on three Babel Year 4 languages\nshow over 3% absolute WER reductions obtained from such multilingual\npre-training. Experiments with different input features show that the\nmultilingual BLSTM performs the best with simple log-Mel-filter-bank\noutputs, which makes our previously successful multilingual stack bottleneck\nfeatures with CMLLR adaptation obsolete. Finally, we experiment with\ndifferent configurations of i-vector based speaker adaptation in the\nmono- and multi-lingual BLSTM architectures. This results in additional\nWER reductions over 1% absolute.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1775"
    },
    "matassoni17_interspeech": {
      "authors": [
        [
          "Marco",
          "Matassoni"
        ],
        [
          "Alessio",
          "Brutti"
        ],
        [
          "Daniele",
          "Falavigna"
        ]
      ],
      "title": "Optimizing DNN Adaptation for Recognition of Enhanced Speech",
      "original": "0755",
      "page_count": 5,
      "order": 148,
      "p1": "724",
      "pn": "728",
      "abstract": [
        "Speech enhancement directly using deep neural network (DNN) is of major\ninterest due to the capability of DNN to tangibly reduce the impact\nof noisy conditions in speech recognition tasks. Similarly, DNN based\nacoustic model adaptation to new environmental conditions is another\nchallenging topic. In this paper we present an analysis of acoustic\nmodel adaptation in presence of a disjoint speech enhancement component,\nidentifying an optimal setting for improving the speech recognition\nperformance. Adaptation is derived from a consolidated technique that\nintroduces in the training process a regularization term to prevent\noverfitting. We propose to optimize the adaptation of the clean acoustic\nmodels towards the enhanced speech by tuning the regularization term\nbased on the degree of enhancement. Experiments on a popular noisy\ndataset (e.g., AURORA-4) demonstrate the validity of the proposed approach.\n"
      ],
      "doi": "10.21437/Interspeech.2017-755"
    },
    "kim17c_interspeech": {
      "authors": [
        [
          "Younggwan",
          "Kim"
        ],
        [
          "Hyungjun",
          "Lim"
        ],
        [
          "Jahyun",
          "Goo"
        ],
        [
          "Hoirin",
          "Kim"
        ]
      ],
      "title": "Deep Least Squares Regression for Speaker Adaptation",
      "original": "0783",
      "page_count": 5,
      "order": 149,
      "p1": "729",
      "pn": "733",
      "abstract": [
        "Recently, speaker adaptation methods in deep neural networks (DNNs)\nhave been widely studied for automatic speech recognition. However,\nalmost all adaptation methods for DNNs have to consider various heuristic\nconditions such as mini-batch sizes, learning rate scheduling, stopping\ncriteria, and initialization conditions because of the inherent property\nof a stochastic gradient descent (SGD)-based training process. Unfortunately,\nthose heuristic conditions are hard to be properly tuned. To alleviate\nthose difficulties, in this paper, we propose a least squares regression-based\nspeaker adaptation method in a DNN framework utilizing posterior mean\nof each class. Also, we show how the proposed method can provide a\nunique solution which is quite easy and fast to calculate without SGD.\nThe proposed method was evaluated in the TED-LIUM corpus. Experimental\nresults showed that the proposed method achieved up to a 4.6% relative\nimprovement against a speaker independent DNN. In addition, we report\nfurther performance improvement of the proposed method with speaker-adapted\nfeatures.\n"
      ],
      "doi": "10.21437/Interspeech.2017-783"
    },
    "do17_interspeech": {
      "authors": [
        [
          "Van Hai",
          "Do"
        ],
        [
          "Nancy F.",
          "Chen"
        ],
        [
          "Boon Pang",
          "Lim"
        ],
        [
          "Mark",
          "Hasegawa-Johnson"
        ]
      ],
      "title": "Multi-Task Learning Using Mismatched Transcription for Under-Resourced Speech Recognition",
      "original": "0788",
      "page_count": 5,
      "order": 150,
      "p1": "734",
      "pn": "738",
      "abstract": [
        "It is challenging to obtain large amounts of native (matched) labels\nfor audio in under-resourced languages. This could be due to a lack\nof literate speakers of the language or a lack of universally acknowledged\northography. One solution is to increase the amount of labeled data\nby using mismatched transcription, which employs transcribers who do\nnot speak the language (in place of native speakers), to transcribe\nwhat they hear as nonsense speech in their own language (e.g., Mandarin).\nThis paper presents a multi-task learning framework where the DNN acoustic\nmodel is simultaneously trained using both a limited amount of native\n(matched) transcription and a larger set of mismatched transcription.\nWe find that by using a multi-task learning framework, we achieve improvements\nover monolingual baselines and previously proposed mismatched transcription\nadaptation techniques. In addition, we show that using alignments provided\nby a GMM adapted by mismatched transcription further improves acoustic\nmodeling performance. Our experiments on Georgian data from the IARPA\nBabel program show the effectiveness of the proposed method.\n"
      ],
      "doi": "10.21437/Interspeech.2017-788"
    },
    "joy17_interspeech": {
      "authors": [
        [
          "Neethu Mariam",
          "Joy"
        ],
        [
          "Sandeep Reddy",
          "Kothinti"
        ],
        [
          "S.",
          "Umesh"
        ],
        [
          "Basil",
          "Abraham"
        ]
      ],
      "title": "Generalized Distillation Framework for Speaker Normalization",
      "original": "0874",
      "page_count": 5,
      "order": 151,
      "p1": "739",
      "pn": "743",
      "abstract": [
        "Generalized distillation framework has been shown to be effective in\nspeech enhancement in the past. We extend this idea to speaker normalization\nwithout any explicit adaptation data in this paper. In the generalized\ndistillation framework, we assume the presence of some &#8220;privileged&#8221;\ninformation to guide the training process in addition to the training\ndata. In the proposed approach, the privileged information is obtained\nfrom a &#8220;teacher&#8221; model, trained on speaker-normalized FMLLR\nfeatures. The &#8220;student&#8221; model is trained on un-normalized\nfilterbank features and uses teacher&#8217;s supervision for cross-entropy\ntraining. The proposed distillation method does not need first pass\ndecode information during testing and imposes no constraints on the\nduration of the test data for computing speaker-specific transforms\nunlike in FMLLR or  i-vector. Experiments done on Switchboard and AMI\ncorpus show that the generalized distillation framework shows improvement\nover un-normalized features with or without  i-vectors.\n"
      ],
      "doi": "10.21437/Interspeech.2017-874"
    },
    "samarakoon17_interspeech": {
      "authors": [
        [
          "Lahiru",
          "Samarakoon"
        ],
        [
          "Brian",
          "Mak"
        ],
        [
          "Khe Chai",
          "Sim"
        ]
      ],
      "title": "Learning Factorized Transforms for Unsupervised Adaptation of LSTM-RNN Acoustic Models",
      "original": "1136",
      "page_count": 5,
      "order": 152,
      "p1": "744",
      "pn": "748",
      "abstract": [
        "Factorized Hidden Layer (FHL) adaptation has been proposed for speaker\nadaptation of deep neural network (DNN) based acoustic models. In FHL\nadaptation, a speaker-dependent (SD) transformation matrix and an SD\nbias are included in addition to the standard affine transformation.\nThe SD transformation is a linear combination of rank-1 matrices whereas\nthe SD bias is a linear combination of vectors. Recently, the Long\nShort-Term Memory (LSTM) Recurrent Neural Networks (RNNs) have shown\nto outperform DNN acoustic models in many Automatic Speech Recognition\n(ASR) tasks. In this work, we investigate the effectiveness of SD transformations\nfor LSTM-RNN acoustic models. Experimental results show that when combined\nwith scaling of LSTM cell states&#8217; outputs, SD transformations\nachieve 2.3% and 2.1% absolute improvements over the baseline LSTM\nsystems for the AMI IHM and AMI SDM tasks respectively.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1136"
    },
    "fainberg17_interspeech": {
      "authors": [
        [
          "Joachim",
          "Fainberg"
        ],
        [
          "Steve",
          "Renals"
        ],
        [
          "Peter",
          "Bell"
        ]
      ],
      "title": "Factorised Representations for Neural Network Adaptation to Diverse Acoustic Environments",
      "original": "1365",
      "page_count": 5,
      "order": 153,
      "p1": "749",
      "pn": "753",
      "abstract": [
        "Adapting acoustic models jointly to both speaker and environment has\nbeen shown to be effective. In many realistic scenarios, however, either\nthe speaker or environment at test time might be unknown, or there\nmay be insufficient data to learn a joint transform. Generating independent\nspeaker and environment transforms improves the match of an acoustic\nmodel to unseen combinations. Using i-vectors, we demonstrate that\nit is possible to factorise speaker or environment information using\nmulti-condition training with neural networks. Specifically, we extract\nbottleneck features from networks trained to classify either speakers\nor environments. We perform experiments on the Wall Street Journal\ncorpus combined with environment noise from the Diverse Environments\nMultichannel Acoustic Noise Database. Using the factorised i-vectors\nwe show improvements in word error rates on perturbed versions of the\neval92 and dev93 test sets, both when one factor is missing and when\nthe factors are seen but not in the desired combination.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1365"
    },
    "sproat17_interspeech": {
      "authors": [
        [
          "Richard",
          "Sproat"
        ],
        [
          "Navdeep",
          "Jaitly"
        ]
      ],
      "title": "An RNN Model of Text Normalization",
      "original": "0035",
      "page_count": 5,
      "order": 154,
      "p1": "754",
      "pn": "758",
      "abstract": [
        "We present a recurrent neural net (RNN) model of text normalization\n&#8212; defined as the mapping of  written text to its  spoken form,\nand a description of the open-source dataset that we used in our experiments.\nWe show that while the RNN model achieves very high overall accuracies,\nthere remain errors that would be unacceptable in a speech application\nlike TTS. We then show that a simple FST-based filter can help mitigate\nthose errors. Even with that mitigation challenges remain, and we end\nthe paper outlining some possible solutions. In releasing our data\nwe are thereby inviting others to help solve this problem.\n"
      ],
      "doi": "10.21437/Interspeech.2017-35"
    },
    "rendel17_interspeech": {
      "authors": [
        [
          "Asaf",
          "Rendel"
        ],
        [
          "Raul",
          "Fernandez"
        ],
        [
          "Zvi",
          "Kons"
        ],
        [
          "Andrew",
          "Rosenberg"
        ],
        [
          "Ron",
          "Hoory"
        ],
        [
          "Bhuvana",
          "Ramabhadran"
        ]
      ],
      "title": "Weakly-Supervised Phrase Assignment from Text in a Speech-Synthesis System Using Noisy Labels",
      "original": "0487",
      "page_count": 5,
      "order": 155,
      "p1": "759",
      "pn": "763",
      "abstract": [
        "The proper segmentation of an input text string into meaningful intonational\nphrase units is a fundamental task in the text-processing component\nof a text-to-speech (TTS) system that generates intelligible and natural\nsynthesis. In this work we look at the creation of a symbolic, phrase-assignment\nmodel within the front end (FE) of a North American English TTS system\nwhen high-quality labels for supervised learning are unavailable and/or\npotentially mismatched to the target corpus and domain. We explore\na labeling scheme that merges heuristics derived from (i) automatic\nhigh-quality phonetic alignments, (ii) linguistic rules, and (iii)\na legacy acoustic phrase-labeling system to arrive at a ground truth\nthat can be used to train a bidirectional recurrent neural network\nmodel. We evaluate the performance of this model in terms of objective\nmetrics describing categorical phrase assignment within the FE proper,\nas well as on the effect that these intermediate labels carry onto\nthe TTS back end for the task of continuous prosody prediction (i.e.,\nintonation and duration contours, and pausing). For this second task,\nwe rely on subjective listening tests and demonstrate that the proposed\nsystem significantly outperforms a linguistic rules-based baseline\nfor two different synthetic voices.\n"
      ],
      "doi": "10.21437/Interspeech.2017-487"
    },
    "ijima17_interspeech": {
      "authors": [
        [
          "Yusuke",
          "Ijima"
        ],
        [
          "Nobukatsu",
          "Hojo"
        ],
        [
          "Ryo",
          "Masumura"
        ],
        [
          "Taichi",
          "Asami"
        ]
      ],
      "title": "Prosody Aware Word-Level Encoder Based on BLSTM-RNNs for DNN-Based Speech Synthesis",
      "original": "0521",
      "page_count": 5,
      "order": 156,
      "p1": "764",
      "pn": "768",
      "abstract": [
        "Recent studies have shown the effectiveness of the use of word vectors\nin DNN-based speech synthesis. However, these word vectors trained\nfrom a large amount of text generally carry not prosodic information,\nwhich is important information for speech synthesis, but semantic information.\nTherefore, if word vectors that take prosodic information into account\ncan be obtained, it would be expected to improve the quality of synthesized\nspeech. In this paper, to obtain word-level vectors that take prosodic\ninformation into account, we propose a novel prosody aware word-level\nencoder. A novel point of the proposed technique is to train a word-level\nencoder by using a large speech corpus constructed for automatic speech\nrecognition. A word-level encoder that estimates the F0 contour for\neach word from the input word sequence is trained. The outputs of the\nbottleneck layer in the trained encoder are used as the word-level\nvector. By training the relationship between words and their prosodic\ninformation by using large speech corpus, the outputs of the bottleneck\nlayer would be expected to contain prosodic information. The results\nof objective and subjective experiments indicate the proposed technique\ncan synthesize speech with improved naturalness.\n"
      ],
      "doi": "10.21437/Interspeech.2017-521"
    },
    "ni17_interspeech": {
      "authors": [
        [
          "Jinfu",
          "Ni"
        ],
        [
          "Yoshinori",
          "Shiga"
        ],
        [
          "Hisashi",
          "Kawai"
        ]
      ],
      "title": "Global Syllable Vectors for Building TTS Front-End with Deep Learning",
      "original": "0669",
      "page_count": 5,
      "order": 157,
      "p1": "769",
      "pn": "773",
      "abstract": [
        "Recent vector space representations of words have succeeded in capturing\nsyntactic and semantic regularities. In the context of text-to-speech\n(TTS) synthesis, a front-end is a key component for extracting multi-level\nlinguistic features from text, where syllable acts as a link between\nlow- and high-level features. This paper describes the use of global\nsyllable vectors as features to build a front-end, particularly evaluated\nin Chinese. The global syllable vectors directly capture global statistics\nof syllable-syllable co-occurrences in a large-scale text corpus. They\nare learned by a global log-bilinear regression model in an unsupervised\nmanner, whilst the front-end is built using deep bidirectional recurrent\nneural networks in a supervised fashion. Experiments are conducted\non large-scale Chinese speech and treebank text corpora, evaluating\ngrapheme to phoneme (G2P) conversion, word segmentation, part of speech\n(POS) tagging, phrasal chunking, and pause break prediction. Results\nshow that the proposed method is efficient for building a compact and\nrobust front-end with high performance. The global syllable vectors\ncan be acquired relatively cheaply from plain text resources, therefore,\nthey are vital to develop multilingual speech synthesis, especially\nfor under-resourced language modeling.\n"
      ],
      "doi": "10.21437/Interspeech.2017-669"
    },
    "fukuoka17_interspeech": {
      "authors": [
        [
          "Ishin",
          "Fukuoka"
        ],
        [
          "Kazuhiko",
          "Iwata"
        ],
        [
          "Tetsunori",
          "Kobayashi"
        ]
      ],
      "title": "Prosody Control of Utterance Sequence for Information Delivering",
      "original": "0708",
      "page_count": 5,
      "order": 158,
      "p1": "774",
      "pn": "778",
      "abstract": [
        "We propose a conversational speech synthesis system in which the prosodic\nfeatures of each utterance are controlled throughout the entire input\ntext. We have developed a &#8220;news-telling system,&#8221; which\ndelivered news articles through spoken language. The speech synthesis\nsystem for the news-telling should be able to highlight utterances\ncontaining noteworthy information in the article with a particular\nway of speaking so as to impress them on the users. To achieve this,\nwe introduced role and position features of the individual utterances\nin the article into the control parameters for prosody generation throughout\nthe text. We defined three categories for the role feature: a nucleus\n(which is assigned to the utterance including the noteworthy information),\na front satellite (which precedes the nucleus) and a rear satellite\n(which follows the nucleus). We investigated how the prosodic features\ndiffered depending on the role and position features through an analysis\nof news-telling speech data uttered by a voice actress. We designed\nthe speech synthesis system on the basis of a deep neural network having\nthe role and position features added to its input layer. Objective\nand subjective evaluation results showed that introducing those features\nwas effective in the speech synthesis for the information delivering.\n"
      ],
      "doi": "10.21437/Interspeech.2017-708"
    },
    "huang17c_interspeech": {
      "authors": [
        [
          "Yuchen",
          "Huang"
        ],
        [
          "Zhiyong",
          "Wu"
        ],
        [
          "Runnan",
          "Li"
        ],
        [
          "Helen",
          "Meng"
        ],
        [
          "Lianhong",
          "Cai"
        ]
      ],
      "title": "Multi-Task Learning for Prosodic Structure Generation Using BLSTM RNN with Structured Output Layer",
      "original": "0949",
      "page_count": 5,
      "order": 159,
      "p1": "779",
      "pn": "783",
      "abstract": [
        "Prosodic structure generation from text plays an important role in\nChinese text-to-speech (TTS) synthesis, which greatly influences the\nnaturalness and intelligibility of the synthesized speech. This paper\nproposes a multi-task learning method for prosodic structure generation\nusing bidirectional long short-term memory (BLSTM) recurrent neural\nnetwork (RNN) and structured output layer (SOL). Unlike traditional\nmethods where prerequisites such as lexicon word or even syntactic\ntree are usually required as the input, the proposed method predicts\nprosodic boundary labels directly from Chinese characters. BLSTM RNN\nis used to capture the bidirectional contextual dependencies of prosodic\nboundary labels. SOL further models correlations between prosodic structures,\nlexicon words as well as part-of-speech (POS), where the prediction\nof prosodic boundary labels are conditioned upon word tokenization\nand POS tagging results. Experimental results demonstrate the effectiveness\nof the proposed method.\n"
      ],
      "doi": "10.21437/Interspeech.2017-949"
    },
    "zheng17_interspeech": {
      "authors": [
        [
          "Yibin",
          "Zheng"
        ],
        [
          "Jianhua",
          "Tao"
        ],
        [
          "Zhengqi",
          "Wen"
        ],
        [
          "Ya",
          "Li"
        ],
        [
          "Bin",
          "Liu"
        ]
      ],
      "title": "Investigating Efficient Feature Representation Methods and Training Objective for BLSTM-Based Phone Duration Prediction",
      "original": "1086",
      "page_count": 5,
      "order": 160,
      "p1": "784",
      "pn": "788",
      "abstract": [
        "Accurate modeling and prediction of speech-sound durations are important\nin generating natural synthetic speech. This paper focuses on both\nfeature and training objective aspects to improve the performance of\nthe phone duration model for speech synthesis system. In feature aspect,\nwe combine the feature representation from gradient boosting decision\ntree (GBDT) and phoneme identity embedding model (which is realized\nby the jointly training of phoneme embedded vector (PEV) and word embedded\nvector (WEV)) for BLSTM to predict the phone duration. The PEV is used\nto replace the one-hot phoneme identity, and GBDT is utilized to transform\nthe traditional contextual features. In the training objective aspect,\na new training objective function which taking into account of the\ncorrelation and consistency between the predicted utterance and the\nnatural utterance is proposed. Perceptual tests indicate the proposed\nmethods could improve the naturalness of the synthetic speech, which\nbenefits from the proposed feature representation methods could capture\nmore precise contextual features, and the proposed training objective\nfunction could tackle the over-averaged problem for the generated phone\ndurations.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1086"
    },
    "chen17d_interspeech": {
      "authors": [
        [
          "Bo",
          "Chen"
        ],
        [
          "Tianling",
          "Bian"
        ],
        [
          "Kai",
          "Yu"
        ]
      ],
      "title": "Discrete Duration Model for Speech Synthesis",
      "original": "1144",
      "page_count": 5,
      "order": 161,
      "p1": "789",
      "pn": "793",
      "abstract": [
        "The acoustic model and the duration model are the two major components\nin statistical parametric speech synthesis (SPSS) systems. The neural\nnetwork based acoustic model makes it possible to model phoneme duration\nat phone-level instead of state-level in conventional hidden Markov\nmodel (HMM) based SPSS systems. Since the duration of phonemes is countable\nvalue, the distribution of the phone-level duration is discrete given\nthe linguistic features, which means the Gaussian hypothesis is no\nlonger necessary. This paper provides an investigation on the performance\nof LSTM-RNN duration model that directly models the probability of\nthe countable duration values given linguistic features using cross\nentropy as criteria. The multi-task learning is also experimented at\nthe same time, with a comparison to the standard LSTM-RNN duration\nmodel in objective and subjective measures. The result shows that directly\nmodeling the discrete distribution has its benefit and multi-task model\nachieves better performance in phone-level duration modeling.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1144"
    },
    "chen17e_interspeech": {
      "authors": [
        [
          "Bo",
          "Chen"
        ],
        [
          "Jiahao",
          "Lai"
        ],
        [
          "Kai",
          "Yu"
        ]
      ],
      "title": "Comparison of Modeling Target in LSTM-RNN Duration Model",
      "original": "1152",
      "page_count": 5,
      "order": 162,
      "p1": "794",
      "pn": "798",
      "abstract": [
        "Speech duration is an important component in statistical parameter\nspeech synthesis(SPSS). In LSTM-RNN based SPSS system, the speech duration\naffects the quality of synthesized speech in two aspects, the prosody\nof speech and the position features in acoustic model. This paper investigated\nthe effects of duration in LSTM-RNN based SPSS system. The performance\nof the acoustic models with position features at different levels are\ncompared. Also, duration models with different network architectures\nare presented. A method to utilize the priori knowledge that the sum\nof state duration of a phoneme should be equal to the phone duration\nis proposed and proved to have better performance in both state duration\nand phone duration modeling. The result shows that acoustic model with\nstate-level position features has better performance in acoustic modeling\n(especially in voice/unvoice classification), which means state-level\nduration model still has its advantage and the duration models with\nthe priori knowledge can result in better speech quality.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1152"
    },
    "ribeiro17_interspeech": {
      "authors": [
        [
          "M. Sam",
          "Ribeiro"
        ],
        [
          "Oliver",
          "Watts"
        ],
        [
          "Junichi",
          "Yamagishi"
        ]
      ],
      "title": "Learning Word Vector Representations Based on Acoustic Counts",
      "original": "1340",
      "page_count": 5,
      "order": 163,
      "p1": "799",
      "pn": "803",
      "abstract": [
        "This paper presents a simple count-based approach to learning word\nvector representations by leveraging statistics of co-occurrences between\ntext and speech. This type of representation requires two discrete\nsequences of units defined across modalities. Two possible methods\nfor the discretization of an acoustic signal are presented, which are\nthen applied to fundamental frequency and energy contours of a transcribed\ncorpus of speech, yielding a sequence of textual objects (e.g. words,\nsyllables) aligned with a sequence of discrete acoustic events. Constructing\na matrix recording the co-occurrence of textual objects with acoustic\nevents and reducing its dimensionality with matrix decomposition results\nin a set of context-independent representations of word types. These\nare applied to the task of acoustic modelling for speech synthesis;\nobjective and subjective results indicate that these representations\nare useful for the generation of acoustic parameters in a text-to-speech\n(TTS) system. In general, we observe that the more discretization approaches,\nacoustic signals, and levels of linguistic analysis are incorporated\ninto a TTS system via these count-based representations, the better\nthat TTS system performs.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1340"
    },
    "szekely17_interspeech": {
      "authors": [
        [
          "\u00c9va",
          "Sz\u00e9kely"
        ],
        [
          "Joseph",
          "Mendelson"
        ],
        [
          "Joakim",
          "Gustafson"
        ]
      ],
      "title": "Synthesising Uncertainty: The Interplay of Vocal Effort and Hesitation Disfluencies",
      "original": "1507",
      "page_count": 5,
      "order": 164,
      "p1": "804",
      "pn": "808",
      "abstract": [
        "As synthetic voices become more flexible, and conversational systems\ngain more potential to adapt to the environmental and social situation,\nthe question needs to be examined, how different modifications to the\nsynthetic speech interact with each other and how their specific combinations\ninfluence perception. This work investigates how the vocal effort of\nthe synthetic speech together with added disfluencies affect listeners&#8217;\nperception of the degree of uncertainty in an utterance. We introduce\na DNN voice built entirely from spontaneous conversational speech data\nand capable of producing a continuum of vocal efforts, prolongations\nand filled pauses with a corpus-based method. Results of a listener\nevaluation indicate that decreased vocal effort, filled pauses and\nprolongation of function words increase the degree of perceived uncertainty\nof conversational utterances expressing the speaker&#8217;s beliefs.\nWe demonstrate that the effect of these three cues are not merely additive,\nbut that interaction effects, in particular between the two types of\ndisfluencies and between vocal effort and prolongations need to be\nconsidered when aiming to communicate a specific level of uncertainty.\nThe implications of these findings are relevant for adaptive and incremental\nconversational systems using expressive speech synthesis and aspiring\nto communicate the attitude of uncertainty.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1507"
    },
    "oktem17_interspeech": {
      "authors": [
        [
          "Alp",
          "\u00d6ktem"
        ],
        [
          "Mireia",
          "Farr\u00fas"
        ],
        [
          "Leo",
          "Wanner"
        ]
      ],
      "title": "Prosograph: A Tool for Prosody Visualisation of Large Speech Corpora",
      "original": "2034",
      "page_count": 2,
      "order": 165,
      "p1": "809",
      "pn": "810",
      "abstract": [
        "This paper presents an open-source tool that has been developed to\nvisualize a speech corpus with its transcript and prosodic features\naligned at word level. In particular, the tool is aimed at providing\na simple and clear way to visualize prosodic patterns along large segments\nof speech corpora, and can be applied in any research that involves\nprosody analysis.\n"
      ]
    },
    "vetchinnikova17_interspeech": {
      "authors": [
        [
          "Svetlana",
          "Vetchinnikova"
        ],
        [
          "Anna",
          "Mauranen"
        ],
        [
          "Nina",
          "Miku\u0161ov\u00e1"
        ]
      ],
      "title": "ChunkitApp: Investigating the Relevant Units of Online Speech Processing",
      "original": "2048",
      "page_count": 2,
      "order": 166,
      "p1": "811",
      "pn": "812",
      "abstract": [
        "This paper presents a web-based application for tablets &#8216;ChunkitApp&#8217;\ndeveloped to investigate chunking in online speech processing. The\ndesign of the app is based on recent theoretical developments in linguistics\nand cognitive science, and in particular on the suggestions of Linear\nUnit Grammar [1]. The data collected using the app provides evidence\nfor the reality of online chunking in language processing and the validity\nof the construct. In addition to experimental uses, the app has potential\napplications in language education and speech recognition.\n"
      ]
    },
    "jochim17_interspeech": {
      "authors": [
        [
          "Markus",
          "Jochim"
        ]
      ],
      "title": "Extending the EMU Speech Database Management System: Cloud Hosting, Team Collaboration, Automatic Revision Control",
      "original": "2049",
      "page_count": 2,
      "order": 167,
      "p1": "813",
      "pn": "814",
      "abstract": [
        "In this paper, we introduce a new component of the EMU Speech Database\nManagement System [1, 2] to improve the team workflow of handling production\ndata (both acoustic and physiological) in phonetics and the speech\nsciences. It is named  emuDB Manager, and it facilitates the coordination\nof team efforts, possibly distributed over several nations, by introducing\nautomatic revision control (based on Git), cloud hosting (in private\nclouds provided by the researchers themselves or a third party), by\nkeeping track of which parts of the database have already been edited\n(and by whom), and by centrally collecting and making searchable the\nnotes made during the edit process.\n"
      ]
    },
    "warlaumont17_interspeech": {
      "authors": [
        [
          "Anne S.",
          "Warlaumont"
        ],
        [
          "Mark",
          "VanDam"
        ],
        [
          "Elika",
          "Bergelson"
        ],
        [
          "Alejandrina",
          "Cristia"
        ]
      ],
      "title": "HomeBank: A Repository for Long-Form Real-World Audio Recordings of Children",
      "original": "2051",
      "page_count": 2,
      "order": 168,
      "p1": "815",
      "pn": "816",
      "abstract": [
        "HomeBank is a new component of the TalkBank system, focused on long-form\n(i.e., multi-hour, typically daylong) real-world recordings of children&#8217;s\nlanguage experiences, and it is linked to a GitHub repository in which\ntools for analyzing those recordings can be shared. HomeBank constitutes\nnot only a rich resource for researchers interested in early language\nacquisition specifically, but also for those seeking to study spontaneous\nspeech, media exposure, and audio environments more generally. This\nShow and Tell describes the procedures for accessing and contributing\nHomeBank data and code. It also overviews the current contents of the\nrepositories, and provides some examples of audio recordings, available\ntranscriptions, and currently available analysis tools.\n"
      ]
    },
    "bell17_interspeech": {
      "authors": [
        [
          "Peter",
          "Bell"
        ],
        [
          "Joachim",
          "Fainberg"
        ],
        [
          "Catherine",
          "Lai"
        ],
        [
          "Mark",
          "Sinclair"
        ]
      ],
      "title": "A System for Real Time Collaborative Transcription Correction",
      "original": "2052",
      "page_count": 2,
      "order": 169,
      "p1": "817",
      "pn": "818",
      "abstract": [
        "We present a system to enable efficient, collaborative human correction\nof ASR transcripts, designed to operate in real-time situations, for\nexample, when post-editing live captions generated for news broadcasts.\nIn the system, confusion networks derived from ASR lattices are used\nto highlight low-confident words and present alternatives to the user\nfor quick correction. The system uses a client-server architecture,\nwhereby information about each manual edit is posted to the server.\nSuch information can be used to dynamically update the one-best ASR\noutput for all utterances currently in the editing pipeline. We propose\nto make updates in three different ways; by finding a new one-best\npath through an existing ASR lattice consistent with the correction\nreceived; by identifying further instances of out-of-vocabulary terms\nentered by the user; and by adapting the language model on the fly.\nUpdates are received asynchronously by the client.\n"
      ]
    },
    "bhat17_interspeech": {
      "authors": [
        [
          "Chitralekha",
          "Bhat"
        ],
        [
          "Anjali",
          "Kant"
        ],
        [
          "Bhavik",
          "Vachhani"
        ],
        [
          "Sarita",
          "Rautara"
        ],
        [
          "Ashok Kumar",
          "Sinha"
        ],
        [
          "Sunil Kumar",
          "Kopparapu"
        ]
      ],
      "title": "MoPAReST &#8212; Mobile Phone Assisted Remote Speech Therapy Platform",
      "original": "2058",
      "page_count": 2,
      "order": 170,
      "p1": "819",
      "pn": "820",
      "abstract": [
        "Through this paper, we present the Mobile Phone Assisted Remote Speech\nTherapy Platform for individuals with speech disabilities to avail\nthe benefits of therapy remotely with minimal  face-to-face sessions\nwith the Speech Language Pathologist (SLP). The objective is to address\nthe skewed ratio of SLP to patients as well increase the efficacy of\nthe therapy by keeping the patient engaged more frequently albeit asynchronously\nand remotely. The platform comprises (1) A web-interface to be used\nby the SLP to monitor the progress of their patients at a time convenient\nto them and (2) A mobile application along with speech processing algorithms\nto provide instant feedback to the patient. We envision this platform\nto cut down the therapy time, especially for rural Indian patients.\nEvaluation of this platform is being done for five patients with mis-articulation\nin Marathi language.\n"
      ]
    },
    "jaumardhakoun17_interspeech": {
      "authors": [
        [
          "Aurore",
          "Jaumard-Hakoun"
        ],
        [
          "Samy",
          "Chikhi"
        ],
        [
          "Takfarinas",
          "Medani"
        ],
        [
          "Angelika",
          "Nair"
        ],
        [
          "G\u00e9rard",
          "Dreyfus"
        ],
        [
          "Fran\u00e7ois-Beno\u00eet",
          "Vialatte"
        ]
      ],
      "title": "An Apparatus to Investigate Western Opera Singing Skill Learning Using Performance and Result Biofeedback, and Measuring its Neural Correlates",
      "original": "2035",
      "page_count": 2,
      "order": 171,
      "p1": "821",
      "pn": "822",
      "abstract": [
        "We present our preliminary developments on a biofeedback interface\nfor Western operatic style training, combining performance and result\nbiofeedback. Electromyographic performance feedbacks, as well as formant-tuning\nresult feedbacks are displayed visually, using continuously scrolling\ndisplays, or discrete post-trial evaluations. Our final aim is to investigate\nelectroencephalographic (EEG) measurements in order to identify neural\ncorrelates of feedback-based skill learning. \n"
      ]
    },
    "draxler17_interspeech": {
      "authors": [
        [
          "Christoph",
          "Draxler"
        ]
      ],
      "title": "PercyConfigurator &#8212; Perception Experiments as a Service",
      "original": "2044",
      "page_count": 2,
      "order": 172,
      "p1": "823",
      "pn": "824",
      "abstract": [
        "PercyConfigurator is an experiment editor that eliminates the need\nfor programming; the experiment definition and content are simply dropped\nonto the PercyConfigurator web page for interactive editing and testing.\nWhen the editing is done, the experiment definition and content are\nuploaded to the server. The server returns a link to the experiment\nwhich is then distributed to potential participants.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  The Bavarian Archive\nfor Speech Signals (BAS) hosts PercyConfigurator as a free service\nto the academic community.\n"
      ]
    },
    "salimbajevs17_interspeech": {
      "authors": [
        [
          "Askars",
          "Salimbajevs"
        ],
        [
          "Indra",
          "Ikauniece"
        ]
      ],
      "title": "System for Speech Transcription and Post-Editing in Microsoft Word",
      "original": "2045",
      "page_count": 2,
      "order": 173,
      "p1": "825",
      "pn": "826",
      "abstract": [
        "In this demonstration paper, we introduce a transcription service that\ncan be used for transcription of different meetings, sessions etc.\nThe service performs speaker diarization, automatic speech recognition,\npunctuation restoration and produces human-readable transcripts as\nspecial Microsoft Word documents that have audio and word alignments\nembedded. Thereby, a widely-used word processor is transformed into\na transcription post-editing tool. Currently, Latvian and Lithuanian\nlanguages are supported, but other languages can be easily added.\n"
      ]
    },
    "park17_interspeech": {
      "authors": [
        [
          "Ji Ho",
          "Park"
        ],
        [
          "Nayeon",
          "Lee"
        ],
        [
          "Dario",
          "Bertero"
        ],
        [
          "Anik",
          "Dey"
        ],
        [
          "Pascale",
          "Fung"
        ]
      ],
      "title": "Emojive! Collecting Emotion Data from Speech and Facial Expression Using Mobile Game App",
      "original": "2047",
      "page_count": 2,
      "order": 174,
      "p1": "827",
      "pn": "828",
      "abstract": [
        "We developed Emojive!, a mobile game app to make emotion recognition\nfrom audio and image interactive and fun, motivating the users to play\nwith the app. The game is to act out a specific emotion, among six\nemotion labels (happy, sad, anger, anxiety, loneliness, criticism),\ngiven by the system. Double player mode lets two people to compete\ntheir acting skills. The more users play the game, the more emotion-labelled\ndata will be acquired. We are using deep Convolutional Neural Network\n(CNN) models to recognize emotion from audio and facial image in real-time\nwith a mobile front-end client including intuitive user interface and\nsimple data visualization.\n"
      ]
    },
    "lennes17_interspeech": {
      "authors": [
        [
          "Mietta",
          "Lennes"
        ],
        [
          "Jussi",
          "Piitulainen"
        ],
        [
          "Martin",
          "Matthiesen"
        ]
      ],
      "title": "Mylly &#8212; The Mill: A New Platform for Processing Speech and Text Corpora Easily and Efficiently",
      "original": "2059",
      "page_count": 2,
      "order": 175,
      "p1": "829",
      "pn": "830",
      "abstract": [
        "Speech and language researchers need to manage and analyze increasing\nquantities of material. Various tools are available for various stages\nof the work, but they often require the researcher to use different\ninterfaces and to convert the output from each tool into suitable input\nfor the next one.<br style=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  The Language Bank of Finland (Kielipankki) is developing an on-line\nplatform called Mylly for processing speech and language data in a\ngraphical user interface that integrates different tools into a single\nworkflow. Mylly provides tools and computational resources for processing\nmaterial and for the inspecting the results. The tools plugged into\nMylly include a parser, morphological analyzers, generic finite-state\ntechnology, and a speech recognizer. Users can upload data and download\nany intermediate results in the tool chain. Mylly runs on CSC&#8217;s\nTaito cluster and is an instance of the Chipster platform. Access rights\nto Mylly are given for academic use.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  The Language Bank\nof Finland is a collection of corpora, tools and other services maintained\nby FIN-CLARIN, a consortium of Finnish universities and research organizations\ncoordinated by the University of Helsinki. The technological infrastructure\nfor the Language Bank of Finland is provided by CSC &#8211; IT Center\nfor Science.\n"
      ]
    },
    "suzuki17_interspeech": {
      "authors": [
        [
          "Kyori",
          "Suzuki"
        ],
        [
          "Ian",
          "Wilson"
        ],
        [
          "Hayato",
          "Watanabe"
        ]
      ],
      "title": "Visual Learning 2: Pronunciation App Using Ultrasound, Video, and MRI",
      "original": "2040",
      "page_count": 2,
      "order": 176,
      "p1": "831",
      "pn": "832",
      "abstract": [
        "We demonstrate  Visual Learning 2, an English pronunciation app for\nsecond-language (L2) learners and phonetics students. This iOS app\nlinks together audio, front and side video, MRI and ultrasound movies\nof a native speaker reading a phonetically balanced text. Users can\nwatch and shadow front and side video overlaid with an ultrasound tongue\nmovie. They are able to play the video at three speeds and start the\nvideo from any word by tapping on it, with a choice of display in either\nEnglish or IPA. Users can record their own audio/video and play it\nback in sync with the model for comparison.\n"
      ]
    },
    "allen17_interspeech": {
      "authors": [
        [
          "James",
          "Allen"
        ]
      ],
      "title": "Dialogue as Collaborative Problem Solving",
      "original": "3002",
      "page_count": 1,
      "order": 177,
      "p1": "833",
      "pn": "833",
      "abstract": [
        "I will describe the current status of a long-term effort at developing\ndialogue systems that go beyond simple task execution models to systems\nthat involve collaborative problem solving. Such systems involve open-ended\ndiscussion and the tasks cannot be accomplished without extensive interaction\n(e.g., 10 turns or more). The key idea is that dialogue itself arises\nfrom an agent&#8217;s ability for collaborative problem solving (CPS).\nIn such dialogues, agents may introduce, modify and negotiate goals;\npropose and discuss the merits possible paths to solutions; explicitly\ndiscuss progress as the two agents work towards the goals; and evaluate\nhow well a goal was accomplished. To complicate matters, user utterances\nin such settings are much more complex than seen in simple task execution\ndialogues and requires full semantic parsing. A key question we have\nbeen exploring in the past few years is how much of dialogue can be\naccounted for by domain-independent mechanisms. I will discuss these\nissues and draw examples from a dialogue system we have built that,\nexcept for the specialized domain reasoning required in each case,\nuses the same architecture to perform three different tasks: collaborative\nblocks world planning, when the system and user build structures and\nmay have differing goals; biocuration, in which a biologist and the\nsystem interact in order to build executable causal models of biological\npathways; and collaborative composition, where the user and system\ncollaborate to compose simple pieces of music.\n"
      ]
    },
    "stasak17_interspeech": {
      "authors": [
        [
          "Brian",
          "Stasak"
        ],
        [
          "Julien",
          "Epps"
        ],
        [
          "Roland",
          "Goecke"
        ]
      ],
      "title": "Elicitation Design for Acoustic Depression Classification: An Investigation of Articulation Effort, Linguistic Complexity, and Word Affect",
      "original": "1223",
      "page_count": 5,
      "order": 178,
      "p1": "834",
      "pn": "838",
      "abstract": [
        "Assessment of neurological and psychiatric disorders like depression\nare unusual from a speech processing perspective, in that speakers\ncan be prompted or instructed in what they should say (e.g. as part\nof a clinical assessment). Despite prior speech-based depression studies\nthat have used a variety of speech elicitation methods, there has been\nlittle evaluation of the best elicitation mode. One approach to understand\nthis better is to analyze an existing database from the perspective\nof articulation effort, word affect, and linguistic complexity measures\nas proxies for depression sub-symptoms (e.g. psychomotor retardation,\nnegative stimulus suppression, cognitive impairment). Here a novel\nmeasure for quantifying articulation effort is introduced, and when\napplied experimentally to the DAIC corpus shows promise for identifying\nspeech data that are more discriminative of depression. Interestingly,\nexperiment results demonstrate that by selecting speech with higher\narticulation effort, linguistic complexity, or word-based arousal/valence,\nimprovements in acoustic speech-based feature depression classification\nperformance can be achieved, serving as a guide for future elicitation\ndesign.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1223"
    },
    "novoa17_interspeech": {
      "authors": [
        [
          "Jos\u00e9",
          "Novoa"
        ],
        [
          "Jorge",
          "Wuth"
        ],
        [
          "Juan Pablo",
          "Escudero"
        ],
        [
          "Josu\u00e9",
          "Fredes"
        ],
        [
          "Rodrigo",
          "Mahu"
        ],
        [
          "Richard M.",
          "Stern"
        ],
        [
          "Nestor Becerra",
          "Yoma"
        ]
      ],
      "title": "Robustness Over Time-Varying Channels in DNN-HMM ASR Based Human-Robot Interaction",
      "original": "1308",
      "page_count": 5,
      "order": 179,
      "p1": "839",
      "pn": "843",
      "abstract": [
        "This paper addresses the problem of time-varying channels in speech-recognition-based\nhuman-robot interaction using Locally-Normalized Filter-Bank features\n(LNFB), and training strategies that compensate for microphone response\nand room acoustics. Testing utterances were generated by re-recording\nthe Aurora-4 testing database using a PR2 mobile robot, equipped with\na Kinect audio interface while performing head rotations and movements\ntoward and away from a fixed source. Three training conditions were\nevaluated called Clean, 1-IR and 33-IR. With Clean training, the DNN-HMM\nsystem was trained using the Aurora-4 clean training database. With\n1-IR training, the same training data were convolved with an impulse\nresponse estimated at one meter from the source with no rotation of\nthe robot head. With 33-IR training, the Aurora-4 training data were\nconvolved with impulse responses estimated at one, two and three meters\nfrom the source and 11 angular positions of the robot head. The 33-IR\ntraining method produced reductions in WER greater than 50% when compared\nwith Clean training using both LNFB and conventional Mel filterbank\nfeatures. Nevertheless, LNFB features provided a WER 23% lower than\nMelFB using 33-IR training. The use of 33-IR training and LNFB features\nreduced WER by 64% compared to Clean training and MelFB features.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1308"
    },
    "turker17_interspeech": {
      "authors": [
        [
          "Bekir Berker",
          "T\u00fcrker"
        ],
        [
          "Zana",
          "Bu\u00e7inca"
        ],
        [
          "Engin",
          "Erzin"
        ],
        [
          "Y\u00fccel",
          "Yemez"
        ],
        [
          "Metin",
          "Sezgin"
        ]
      ],
      "title": "Analysis of Engagement and User Experience with a Laughter Responsive Social Robot",
      "original": "1395",
      "page_count": 5,
      "order": 180,
      "p1": "844",
      "pn": "848",
      "abstract": [
        "We explore the effect of laughter perception and response in terms\nof engagement in human-robot interaction. We designed two distinct\nexperiments in which the robot has two modes: laughter responsive and\nlaughter non-responsive. In responsive mode, the robot detects laughter\nusing a multimodal real-time laughter detection module and invokes\nlaughter as a backchannel to users accordingly. In non-responsive mode,\nrobot has no utilization of detection, thus provides no feedback. In\nthe experimental design, we use a straightforward question-answer based\ninteraction scenario using a back-projected robot head. We evaluate\nthe interactions with objective and subjective measurements of engagement\nand user experience.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1395"
    },
    "baird17_interspeech": {
      "authors": [
        [
          "Alice",
          "Baird"
        ],
        [
          "Shahin",
          "Amiriparian"
        ],
        [
          "Nicholas",
          "Cummins"
        ],
        [
          "Alyssa M.",
          "Alcorn"
        ],
        [
          "Anton",
          "Batliner"
        ],
        [
          "Sergey",
          "Pugachevskiy"
        ],
        [
          "Michael",
          "Freitag"
        ],
        [
          "Maurice",
          "Gerczuk"
        ],
        [
          "Bj\u00f6rn",
          "Schuller"
        ]
      ],
      "title": "Automatic Classification of Autistic Child Vocalisations: A Novel Database and Results",
      "original": "0730",
      "page_count": 5,
      "order": 181,
      "p1": "849",
      "pn": "853",
      "abstract": [
        "Humanoid robots have in recent years shown great promise for supporting\nthe educational needs of children on the autism spectrum. To further\nimprove the efficacy of such interactions, user-adaptation strategies\nbased on the individual needs of a child are required. In this regard,\nthe proposed study assesses the suitability of a range of speech-based\nclassification approaches for automatic detection of autism severity\naccording to the commonly used Social Responsiveness Scale second edition\n(SRS-2). Autism is characterised by socialisation limitations including\nchild language and communication ability. When compared to neurotypical\nchildren of the same age these can be a strong indication of severity.\nThis study introduces a novel dataset of 803 utterances recorded from\n14 autistic children aged between 4&#8211;10 years, during Wizard-of-Oz\ninteractions with a humanoid robot. Our results demonstrate the suitability\nof support vector machines (SVMs) which use acoustic feature sets from\nmultiple Interspeech COMPARE challenges. We also evaluate deep spectrum\nfeatures, extracted via an image classification convolutional neural\nnetwork (CNN) from the spectrogram of autistic speech instances. At\nbest, by using SVMs on the acoustic feature sets, we achieved a UAR\nof 73.7% for the proposed 3-class task. \n"
      ],
      "doi": "10.21437/Interspeech.2017-730"
    },
    "oertel17_interspeech": {
      "authors": [
        [
          "Catharine",
          "Oertel"
        ],
        [
          "Patrik",
          "Jonell"
        ],
        [
          "Dimosthenis",
          "Kontogiorgos"
        ],
        [
          "Joseph",
          "Mendelson"
        ],
        [
          "Jonas",
          "Beskow"
        ],
        [
          "Joakim",
          "Gustafson"
        ]
      ],
      "title": "Crowd-Sourced Design of Artificial Attentive Listeners",
      "original": "0926",
      "page_count": 5,
      "order": 182,
      "p1": "854",
      "pn": "858",
      "abstract": [
        "Feedback generation is an important component of human-human communication.\nHumans can choose to signal support, understanding, agreement or also\nscepticism by means of feedback tokens. Many studies have focused on\nthe timing of feedback behaviours. In the current study, however, we\nkeep the timing constant and instead focus on the lexical form and\nprosody of feedback tokens as well as their sequential patterns.<br\nstyle=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  For this we crowdsourced participant&#8217;s feedback behaviour\nin identical interactional contexts in order to model a virtual agent\nthat is able to provide feedback as an attentive/supportive as well\nas attentive/sceptical listener. The resulting models were realised\nin a robot which was evaluated by third-party observers.\n"
      ],
      "doi": "10.21437/Interspeech.2017-926"
    },
    "lancia17_interspeech": {
      "authors": [
        [
          "Leonardo",
          "Lancia"
        ],
        [
          "Thierry",
          "Chaminade"
        ],
        [
          "No\u00ebl",
          "Nguyen"
        ],
        [
          "Laurent",
          "Pr\u00e9vot"
        ]
      ],
      "title": "Studying the Link Between Inter-Speaker Coordination and Speech Imitation Through Human-Machine Interactions",
      "original": "1431",
      "page_count": 5,
      "order": 183,
      "p1": "859",
      "pn": "863",
      "abstract": [
        "According to accounts of inter-speaker coordination based on internal\npredictive models, speakers tend to imitate each other each time they\nneed to coordinate their behavior. According to accounts based on the\nnotion of dynamical coupling, imitation should be observed only if\nit helps stabilizing the specific coordinative pattern produced by\nthe interlocutors or if it is a direct consequence of inter-speaker\ncoordination. To compare these accounts, we implemented an artificial\nagent designed to repeat a speech utterance while coordinating its\nbehavior with that of a human speaker performing the same task. We\nasked 10 Italian speakers to repeat the utterance /topkop/ simultaneously\nwith the agent during short time intervals. In some interactions, the\nagent was parameterized to cooperate with the speakers (by producing\nits syllables simultaneously with those of the human) while in others\nit was parameterized to compete with them (by producing its syllables\nin-between those of the human). A positive correlation between the\nstability of inter-speaker coordination and the degree of f0 imitation\nwas observed only in cooperative interactions. However, in line with\naccounts based on prediction, speakers imitate the f0 of the agent\nregardless of whether this is parameterized to cooperate or to compete\nwith them.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1431"
    },
    "delalez17_interspeech": {
      "authors": [
        [
          "Samuel",
          "Delalez"
        ],
        [
          "Christophe",
          "d\u2019Alessandro"
        ]
      ],
      "title": "Adjusting the Frame: Biphasic Performative Control of Speech Rhythm",
      "original": "0396",
      "page_count": 5,
      "order": 184,
      "p1": "864",
      "pn": "868",
      "abstract": [
        "Performative time and pitch scaling is a new research paradigm for\nprosodic analysis by synthesis. In this paper, a system for real-time\nrecorded speech time and pitch scaling by the means of hands or feet\ngestures is designed and evaluated. Pitch is controlled with the preferred\nhand, using a stylus on a graphic tablet. Time is controlled using\nrhythmic frames, or constriction gestures, defined by pairs of control\npoints. The &#8220;Arsis&#8221; corresponds to the constriction (weak\nbeat of the syllable) and the &#8220;Thesis&#8221; corresponds to the\nvocalic nucleus (strong beat of the syllable). This biphasic control\nof rhythmic units is performed by the non-preferred hand using a button.\nPitch and time scales are modified according to these gestural controls\nwith the help of a real-time pitch synchronous overlap-add technique\n(RT-PSOLA). Rhythm and pitch control accuracy are assessed in a prosodic\nimitation experiment: the task is to reproduce intonation and rhythm\nof various sentences. The results show that inter-vocalic durations\ndiffer on average of only 20 ms. The system appears as a new and effective\ntool for performative speech and singing synthesis. Consequences and\napplications in speech prosody research are discussed.\n"
      ],
      "doi": "10.21437/Interspeech.2017-396"
    },
    "saryazdi17_interspeech": {
      "authors": [
        [
          "Raheleh",
          "Saryazdi"
        ],
        [
          "Craig G.",
          "Chambers"
        ]
      ],
      "title": "Attentional Factors in Listeners&#8217; Uptake of Gesture Cues During Speech Processing",
      "original": "1676",
      "page_count": 5,
      "order": 185,
      "p1": "869",
      "pn": "873",
      "abstract": [
        "In conversation, speakers spontaneously produce manual gestures that\ncan facilitate listeners&#8217; comprehension of speech. However, various\nfactors may affect listeners&#8217; ability to use gesture cues. Here\nwe examine a situation where a speaker is referring to physical objects\nin the contextual here-and-now. In this situation, objects for potential\nreference will compete with gestures for visual attention. In two experiments,\na speaker provided instructions to pick up objects in the visual environment\n(&#8220; Pick up the candy&#8221;). On some trials, the speaker produced\na &#8220;pick up&#8221; gesture that reflected the size/shape of the\ntarget object. Gaze position was recorded to evaluate how listeners\nallocated attention to scene elements. Experiment 1 showed that, although\niconic gestures (when present) were rarely fixated directly, peripheral\nuptake of these cues speeded listeners&#8217; visual identification\nof intended referents as the instruction unfolded. However, the benefit\nwas mild and occurred primarily for small/hard-to-identify objects.\nIn Experiment 2, background noise was added to reveal whether challenging\nauditory environments lead listeners to allocate additional visual\nattention to gesture cues in a compensatory manner. Interestingly,\nbackground noise actually  reduced listeners&#8217; use of gesture\ncues. Together the findings highlight how situational factors govern\nthe use of visual cues during multimodal communication.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1676"
    },
    "ishi17_interspeech": {
      "authors": [
        [
          "Carlos",
          "Ishi"
        ],
        [
          "Takashi",
          "Minato"
        ],
        [
          "Hiroshi",
          "Ishiguro"
        ]
      ],
      "title": "Motion Analysis in Vocalized Surprise Expressions",
      "original": "0631",
      "page_count": 5,
      "order": 186,
      "p1": "874",
      "pn": "878",
      "abstract": [
        "The background of our research is the generation of natural human-like\nmotions during speech in android robots that have a highly human-like\nappearance. Mismatches in speech and motion are sources of unnaturalness,\nespecially when emotion expressions are involved. Surprise expressions\noften occur in dialogue interactions, and they are often accompanied\nby verbal interjectional utterances. In this study, we analyze facial,\nhead and body motions during several types of vocalized surprise expressions\nappearing in human-human dialogue interactions. The analysis results\nindicate an inter-dependence between motion types and different types\nof surprise expression (such as emotional, social or quoted) as well\nas different degrees of surprise expression. The synchronization between\nmotion and surprise utterances is also analyzed.\n"
      ],
      "doi": "10.21437/Interspeech.2017-631"
    },
    "ruede17_interspeech": {
      "authors": [
        [
          "Robin",
          "Ruede"
        ],
        [
          "Markus",
          "M\u00fcller"
        ],
        [
          "Sebastian",
          "St\u00fcker"
        ],
        [
          "Alex",
          "Waibel"
        ]
      ],
      "title": "Enhancing Backchannel Prediction Using Word Embeddings",
      "original": "1606",
      "page_count": 5,
      "order": 187,
      "p1": "879",
      "pn": "883",
      "abstract": [
        "Backchannel responses like &#8220;uh-huh&#8221;, &#8220;yeah&#8221;,\n&#8220;right&#8221; are used by the listener in a social dialog as\na way to provide feedback to the speaker. In the context of human-computer\ninteraction, these responses can be used by an artificial agent to\nbuild rapport in conversations with users. In the past, multiple approaches\nhave been proposed to detect backchannel cues and to predict the most\nnatural timing to place those backchannel utterances. Most of these\nare based on manually optimized fixed rules, which may fail to generalize.\nMany systems rely on the location and duration of pauses and pitch\nslopes of specific lengths. In the past, we proposed an approach by\ntraining artificial neural networks on acoustic features such as pitch\nand power and also attempted to add word embeddings via word2vec. In\nthis work, we refined this approach by evaluating different methods\nto add timed word embeddings via word2vec. Comparing the performance\nusing various feature combinations, we could show that adding linguistic\nfeatures improves the performance over a prediction system that only\nuses acoustic features.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1606"
    },
    "raveh17_interspeech": {
      "authors": [
        [
          "Eran",
          "Raveh"
        ],
        [
          "Ingmar",
          "Steiner"
        ],
        [
          "Bernd",
          "M\u00f6bius"
        ]
      ],
      "title": "A Computational Model for Phonetically Responsive Spoken Dialogue Systems",
      "original": "1042",
      "page_count": 5,
      "order": 188,
      "p1": "884",
      "pn": "888",
      "abstract": [
        "This paper introduces a model for segment-level phonetic responsiveness.\nIt is based on behavior observed in human-human interaction, and is\ndesigned to be integrated into spoken dialogue systems to capture potential\nphonetic variation and simulate convergence capabilities. Each step\nin the process is responsible for an aspect of the interaction, including\nmonitoring the input speech and appropriately analyzing it. Various\nparameters can be tuned to configure the speech handling and adjust\nthe response style. Evaluation was performed by simulating simple end-to-end\ndialogue scenarios, including analyzing the synthesized output of the\nmodel. The results show promising ground for further extensions.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1042"
    },
    "ebhotemhen17_interspeech": {
      "authors": [
        [
          "Eustace",
          "Ebhotemhen"
        ],
        [
          "Volha",
          "Petukhova"
        ],
        [
          "Dietrich",
          "Klakow"
        ]
      ],
      "title": "Incremental Dialogue Act Recognition: Token- vs Chunk-Based Classification",
      "original": "0738",
      "page_count": 5,
      "order": 189,
      "p1": "889",
      "pn": "893",
      "abstract": [
        "This paper presents a machine learning based approach to incremental\ndialogue act classification with a focus on the recognition of communicative\nfunctions associated with dialogue segments in a multidimensional space,\nas defined in the ISO 24617-2 dialogue act annotation standard. The\nmain goal is to establish the nature of an  increment whose processing\nwill result in a reliable overall system performance. We explore scenarios\nwhere increments are tokens or syntactically, semantically or prosodically\nmotivated chunks. Combing local classification with meta-classifiers\nat a late fusion decision level we obtained state-of-the-art classification\nperformance. Experiments were carried out on manually corrected transcriptions\nand on potentially erroneous ASR output. Chunk-based classification\nyields better results on the manual transcriptions, whereas token-based\nclassification shows a more robust performance on the ASR output. It\nis also demonstrated that layered hierarchical and cascade training\nprocedures result in better classification performance than the single-layered\napproach based on a joint classification predicting complex class labels.\n"
      ],
      "doi": "10.21437/Interspeech.2017-738"
    },
    "niebuhr17b_interspeech": {
      "authors": [
        [
          "Oliver",
          "Niebuhr"
        ]
      ],
      "title": "Clear Speech &#8212; Mere Speech? How Segmental and Prosodic Speech Reduction Shape the Impression That Speakers Create on Listeners",
      "original": "0028",
      "page_count": 5,
      "order": 190,
      "p1": "894",
      "pn": "898",
      "abstract": [
        "Research on speech reduction is primarily concerned with analyzing,\nmodeling, explaining, and, ultimately, predicting phonetic variation.\nThat is, the focus is on the speech signal itself. The present paper\nadds a little side note to this fundamental line of research by addressing\nthe question whether variation in the degree of reduction also has\na systematic effect on the attributes we ascribe to the speaker who\nproduces the speech signal. A perception experiment was carried out\nfor German in which 46 listeners judged whether or not speakers showing\n3 different combinations of segmental and prosodic reduction levels\n(unreduced, moderately reduced, strongly reduced) are appropriately\ndescribed by 13 physical, social, and cognitive attributes. The experiment\nshows that clear speech is not mere speech, and less clear speech is\nnot just reduced either. Rather, results revealed a complex interplay\nof reduction levels and perceived speaker attributes in which moderate\nreduction can make a better impression on listeners than no reduction.\nIn addition to its relevance in reduction models and theories, this\ninterplay is instructive for various fields of speech application from\nsocial robotics to charisma coaching.\n"
      ],
      "doi": "10.21437/Interspeech.2017-28"
    },
    "kouklia17_interspeech": {
      "authors": [
        [
          "Charlotte",
          "Kouklia"
        ],
        [
          "Nicolas",
          "Audibert"
        ]
      ],
      "title": "Relationships Between Speech Timing and Perceived Hostility in a French Corpus of Political Debates",
      "original": "0293",
      "page_count": 5,
      "order": 191,
      "p1": "899",
      "pn": "903",
      "abstract": [
        "This study investigates the relationship between perceived hostility\nand speech timing features within extracts from Montreuil&#8217;s City\nCouncil sessions in 2013, marked by a tense political context at this\ntime. A dataset of 118 speech extracts from the mayor (Dominique Voynet)\nand four of her political opponents during the City Council has been\nanalyzed through the combination of perception tests and speech timing\nphenomena, estimated from classical timing-related measurements and\ncustom metrics. We also develop a methodological framework for the\nphonetic analysis of nonscripted speech: a double perceptive evaluation\nof the original dataset (22 participants) allowed us to measure the\ndifference of hostility perceived (dHost) between the original audio\nextracts and their read transcriptions, and the five speakers produced\nthe same utterances in a controlled reading task to make the direct\ncomparison with original extracts possible. Correlations between dHost\nand speech timing features differences between each original utterance\nand its control counterpart show that perceived hostility is mainly\ninfluenced by local deviations to the expected accentuation pattern\nin French combined with the insertion of silent pauses. Moreover, a\nfiner-grained analysis of rhythmic features reveals different strategies\namongst speakers, especially regarding the realization of interpausal\nspeech rate variation and final syllables lengthening.\n"
      ],
      "doi": "10.21437/Interspeech.2017-293"
    },
    "gallardo17_interspeech": {
      "authors": [
        [
          "Laura Fern\u00e1ndez",
          "Gallardo"
        ],
        [
          "Benjamin",
          "Weiss"
        ]
      ],
      "title": "Towards Speaker Characterization: Identifying and Predicting Dimensions of Person Attribution",
      "original": "0328",
      "page_count": 5,
      "order": 192,
      "p1": "904",
      "pn": "908",
      "abstract": [
        "A great number of investigations on person characterization rely on\nthe assessment of the Big-Five personality traits, a prevalent and\nwidely accepted model with strong psychological foundation. However,\nin the context on characterizing unfamiliar individuals from their\nvoices only, it may be hard for assessors to determine the Big-Five\ntraits based on their first impression. In this study, a 28-item semantic\ndifferential rating scale has been completed by a total of 33 listeners\nwho were presented with 15 male voice stimuli. A factor analysis on\ntheir responses enabled us to identify five perceptual factors of person\nattribution: (social and physical) attractiveness, confidence, apathy,\nserenity, and incompetence. A discussion on the relations of these\ndimensions of speaker attribution to the Big-Five factors is provided\nand speech features relevant to the automatic prediction of our dimensions\nare analyzed, together with SVM regression performance. Although more\ndata are needed to validate our findings, we believe that our approach\ncan lead to establish a space of person attributions with dimensions\nthat can easily be detected from utterances in zero-acquaintance scenarios.\n"
      ],
      "doi": "10.21437/Interspeech.2017-328"
    },
    "ishi17b_interspeech": {
      "authors": [
        [
          "Carlos",
          "Ishi"
        ],
        [
          "Jun",
          "Arai"
        ],
        [
          "Norihiro",
          "Hagita"
        ]
      ],
      "title": "Prosodic Analysis of Attention-Drawing Speech",
      "original": "0623",
      "page_count": 5,
      "order": 193,
      "p1": "909",
      "pn": "913",
      "abstract": [
        "The term &#8220;attention drawing&#8221; refers to the action of sellers\nwho call out to get the attention of people passing by in front of\ntheir stores or shops to invite them inside to buy or sample products.\nSince the speaking styles exhibited in such attention-drawing speech\nare clearly different from conversational speech, in this study, we\nfocused on prosodic analyses of attention-drawing speech and collected\nthe speech data of multiple people with previous attention-drawing\nexperience by simulating several situations. We then investigated the\neffects of several factors, including background noise, interaction\nphases, and shop categories on the prosodic features of attention-drawing\nutterances. Analysis results indicate that compared to dialogue interaction\nutterances, attention-drawing utterances usually have higher power,\nhigher mean F0s, smaller F0 ranges, and do not drop at the end of sentences,\nregardless of the presence or absence of background noise. Analysis\nof sentence-final syllable intonation indicates the presence of lengthened\nflat or rising tones in attention-drawing utterances.\n"
      ],
      "doi": "10.21437/Interspeech.2017-623"
    },
    "simpson17_interspeech": {
      "authors": [
        [
          "Adrian P.",
          "Simpson"
        ],
        [
          "Riccarda",
          "Funk"
        ],
        [
          "Frederik",
          "Palmer"
        ]
      ],
      "title": "Perceptual and Acoustic CorreLates of Gender in the Prepubertal Voice",
      "original": "1055",
      "page_count": 5,
      "order": 194,
      "p1": "914",
      "pn": "918",
      "abstract": [
        "This study investigates the perceptual and acoustic correlates of gender\nin the prepubertal voice. 23 German-speaking primary school pupils\n(13 female, 10 male) aged 8&#8211;9 years were recorded producing 10\nsentences each. Two sentences from each speaker were presented in random\norder to a group of listeners who were asked to assign a gender to\neach stimulus. Single utterances from each of the three male and three\nfemale speakers whose gender was identified most reliably were played\nin a second experiment to two further groups of listeners who judged\neach stimulus against seven perceptual attribute pairs. Acoustic analysis\nof those parameters corresponding most directly to the perceptual attributes\nrevealed a number of highly significant correlations, indicating some\naspects of the voice and speech (f0, harmonics-to-noise ratio, tempo)\nthat children use to construct and adults use to identify gender in\nthe prepubertal voice.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1055"
    },
    "schweitzer17_interspeech": {
      "authors": [
        [
          "Katrin",
          "Schweitzer"
        ],
        [
          "Michael",
          "Walsh"
        ],
        [
          "Antje",
          "Schweitzer"
        ]
      ],
      "title": "To See or not to See: Interlocutor Visibility and Likeability Influence Convergence in Intonation",
      "original": "1248",
      "page_count": 5,
      "order": 195,
      "p1": "919",
      "pn": "923",
      "abstract": [
        "In this paper we look at convergence and divergence in intonation in\nthe context of social qualities. Specifically we examine pitch accent\nrealisations in the GECO corpus of German conversations. Pitch accents\nare represented as 6-dimensional vectors where each dimension corresponds\nto a characteristic of the accent&#8217;s shape. Convergence/divergence\nis then measured by calculating the distance between pitch accent realisations\nof conversational partners. A decrease of distance values over time\nindicates convergence, an increase divergence. The corpus comprises\ndialogue sessions in two modalities: partners either saw each other\nduring the conversation or not. Linear mixed model analyses show convergence\nas well as divergence effects in the realisations of H*L accents. This\nconvergence/divergence is strongly related to the modality and to how\nmuch speakers like their partners: generally, seeing the partner comes\nwith divergence, whereas when the dialogue partners cannot see each\nother, there is convergence. The effect varies, however, depending\non the extent to which a speaker likes their partner. Less liking entails\na greater change in the realisations over time &#8212; stronger divergence\nwhen partners could see each other, and stronger convergence when they\ncould not.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1248"
    },
    "weirich17_interspeech": {
      "authors": [
        [
          "Melanie",
          "Weirich"
        ],
        [
          "Adrian P.",
          "Simpson"
        ]
      ],
      "title": "Acoustic Correlates of Parental Role and Gender Identity in the Speech of Expecting Parents",
      "original": "1394",
      "page_count": 5,
      "order": 196,
      "p1": "924",
      "pn": "928",
      "abstract": [
        "Differences between male and female speakers have been explained in\nterms of biological inevitabilities but also in terms of behavioral\nand socially motivated factors. The aim of this study is to investigate\nthe latter by examining gender-specific variability within the same\ngender.<br style=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  The speech of 29 German men and women &#8212; all of them expecting\ntheir first child but varying in the time they plan to stay at home\nduring their child&#8217;s first year (parental role) &#8212; is analyzed.\nAcoustic analyses comprise the vowel space size and the realization\nof the inter-sibilant contrast.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  While the data is\npart of a larger longitudinal project investigating adult- and infant-directed\nspeech during the infant&#8217;s first year of life, this study concentrates\non the recordings made before the birth of the child. Inter-speaker\nvariability is investigated in relation to 1) the chosen parental role\nand 2) self-ascribed ratings on positive feminine attributes (gender\nidentity).<br style=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  Results show that both factors (planned duration of parental leave\nand the femininity ratings) contribute to the variability found between,\nbut also within the same gender. In particular, the vowel space size\nwas found to be positively correlated with self-ascribed femininity\nratings in male speakers.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1394"
    },
    "soleraurena17_interspeech": {
      "authors": [
        [
          "Rub\u00e9n",
          "Solera-Ure\u00f1a"
        ],
        [
          "Helena",
          "Moniz"
        ],
        [
          "Fernando",
          "Batista"
        ],
        [
          "Vera",
          "Cabarr\u00e3o"
        ],
        [
          "Anna",
          "Pompili"
        ],
        [
          "Ramon Fernandez",
          "Astudillo"
        ],
        [
          "Joana",
          "Campos"
        ],
        [
          "Ana",
          "Paiva"
        ],
        [
          "Isabel",
          "Trancoso"
        ]
      ],
      "title": "A Semi-Supervised Learning Approach for Acoustic-Prosodic Personality Perception in Under-Resourced Domains",
      "original": "1732",
      "page_count": 5,
      "order": 197,
      "p1": "929",
      "pn": "933",
      "abstract": [
        "Automatic personality analysis has gained attention in the last years\nas a fundamental dimension in human-to-human and human-to-machine interaction.\nHowever, it still suffers from limited number and size of speech corpora\nfor specific domains, such as the assessment of children&#8217;s personality.\nThis paper investigates a semi-supervised training approach to tackle\nthis scenario. We devise an experimental setup with age and language\nmismatch and two training sets: a small labeled training set from the\nInterspeech 2012 Personality Sub-challenge, containing French adult\nspeech labeled with personality OCEAN traits, and a large unlabeled\ntraining set of Portuguese children&#8217;s speech. As test set, a\ncorpus of Portuguese children&#8217;s speech labeled with OCEAN traits\nis used. Based on this setting, we investigate a weak supervision approach\nthat iteratively refines an initial model trained with the labeled\ndata-set using the unlabeled data-set. We also investigate knowledge-based\nfeatures, which leverage expert knowledge in acoustic-prosodic cues\nand thus need no extra data. Results show that, despite the large mismatch\nimposed by language and age differences, it is possible to attain improvements\nwith these techniques, pointing both to the benefits of using a weak\nsupervision and expert-based acoustic-prosodic features across age\nand language.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1732"
    },
    "tatman17_interspeech": {
      "authors": [
        [
          "Rachael",
          "Tatman"
        ],
        [
          "Conner",
          "Kasten"
        ]
      ],
      "title": "Effects of Talker Dialect, Gender &amp; Race on Accuracy of Bing Speech and YouTube Automatic Captions",
      "original": "1746",
      "page_count": 5,
      "order": 198,
      "p1": "934",
      "pn": "938",
      "abstract": [
        "This project compares the accuracy of two automatic speech recognition\n(ASR) systems &#8212; Bing Speech and YouTube&#8217;s automatic captions\n&#8212; across gender, race and four dialects of American English.\nThe dialects included were chosen for their acoustic dissimilarity.\nBing Speech had differences in word error rate (WER) between dialects\nand ethnicities, but they were not statistically reliable. YouTube&#8217;s\nautomatic captions, however, did have statistically different WERs\nbetween dialects and races. The lowest average error rates were for\nGeneral American and white talkers, respectively. Neither system had\na reliably different WER between genders, which had been previously\nreported for YouTube&#8217;s automatic captions [1]. However, the higher\nerror rate non-white talkers is worrying, as it may reduce the utility\nof these systems for talkers of color.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1746"
    },
    "prabhavalkar17_interspeech": {
      "authors": [
        [
          "Rohit",
          "Prabhavalkar"
        ],
        [
          "Kanishka",
          "Rao"
        ],
        [
          "Tara N.",
          "Sainath"
        ],
        [
          "Bo",
          "Li"
        ],
        [
          "Leif",
          "Johnson"
        ],
        [
          "Navdeep",
          "Jaitly"
        ]
      ],
      "title": "A Comparison of Sequence-to-Sequence Models for Speech Recognition",
      "original": "0233",
      "page_count": 5,
      "order": 199,
      "p1": "939",
      "pn": "943",
      "abstract": [
        "In this work, we conduct a detailed evaluation of various all-neural,\nend-to-end trained, sequence-to-sequence models applied to the task\nof speech recognition. Notably, each of these systems directly predicts\ngraphemes in the written domain, without using an external pronunciation\nlexicon, or a separate language model. We examine several sequence-to-sequence\nmodels including connectionist temporal classification (CTC), the recurrent\nneural network (RNN) transducer, an attention-based model, and a model\nwhich augments the RNN transducer with an attention mechanism.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  We find that the sequence-to-sequence\nmodels are competitive with traditional state-of-the-art approaches\non dictation test sets, although the baseline, which uses a separate\npronunciation and language model, outperforms these models on voice-search\ntest sets.\n"
      ],
      "doi": "10.21437/Interspeech.2017-233"
    },
    "zeyer17_interspeech": {
      "authors": [
        [
          "Albert",
          "Zeyer"
        ],
        [
          "Eugen",
          "Beck"
        ],
        [
          "Ralf",
          "Schl\u00fcter"
        ],
        [
          "Hermann",
          "Ney"
        ]
      ],
      "title": "CTC in the Context of Generalized Full-Sum HMM Training",
      "original": "1073",
      "page_count": 5,
      "order": 200,
      "p1": "944",
      "pn": "948",
      "abstract": [
        "We formulate a generalized hybrid HMM-NN training procedure using the\nfull-sum over the hidden state-sequence and identify CTC as a special\ncase of it. We present an analysis of the alignment behavior of such\na training procedure and explain the strong localization of label output\nbehavior of full-sum training (also referred to as peaky or spiky behavior).\nWe show how to avoid that behavior by using a state prior. We discuss\nthe temporal decoupling between output label position/time-frame, and\nthe corresponding evidence in the input observations when this is trained\nwith BLSTM models. We also show a way how to overcome this by jointly\ntraining a FFNN. We implemented the Baum-Welch alignment algorithm\nin CUDA to be able to do fast soft realignments on GPU. We have published\nthis code along with some of our experiments as part of RETURNN, RWTH&#8217;s\nextensible training framework for universal recurrent neural networks.\nWe finish with experimental validation of our study on WSJ and Switchboard.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1073"
    },
    "hori17_interspeech": {
      "authors": [
        [
          "Takaaki",
          "Hori"
        ],
        [
          "Shinji",
          "Watanabe"
        ],
        [
          "Yu",
          "Zhang"
        ],
        [
          "William",
          "Chan"
        ]
      ],
      "title": "Advances in Joint CTC-Attention Based End-to-End Speech Recognition with a Deep CNN Encoder and RNN-LM",
      "original": "1296",
      "page_count": 5,
      "order": 201,
      "p1": "949",
      "pn": "953",
      "abstract": [
        "We present a state-of-the-art end-to-end Automatic Speech Recognition\n(ASR) model. We learn to listen and write characters with a joint Connectionist\nTemporal Classification (CTC) and attention-based encoder-decoder network.\nThe encoder is a deep Convolutional Neural Network (CNN) based on the\nVGG network. The CTC network sits on top of the encoder and is jointly\ntrained with the attention-based decoder. During the beam search process,\nwe combine the CTC predictions, the attention-based decoder predictions\nand a separately trained LSTM language model. We achieve a 5&#8211;10%\nerror reduction compared to prior systems on spontaneous Japanese and\nChinese speech, and our end-to-end model beats out traditional hybrid\nASR systems.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1296"
    },
    "lu17_interspeech": {
      "authors": [
        [
          "Liang",
          "Lu"
        ],
        [
          "Lingpeng",
          "Kong"
        ],
        [
          "Chris",
          "Dyer"
        ],
        [
          "Noah A.",
          "Smith"
        ]
      ],
      "title": "Multitask Learning with CTC and Segmental CRF for Speech Recognition",
      "original": "0071",
      "page_count": 5,
      "order": 202,
      "p1": "954",
      "pn": "958",
      "abstract": [
        "Segmental conditional random fields (SCRFs) and connectionist temporal\nclassification (CTC) are two sequence labeling methods used for end-to-end\ntraining of speech recognition models. Both models define a transcription\nprobability by marginalizing decisions about latent segmentation alternatives\nto derive a sequence probability: the former uses a globally normalized\njoint model of segment labels and durations, and the latter classifies\neach frame as either an output symbol or a &#8220;continuation&#8221;\nof the previous label. In this paper, we train a recognition model\nby optimizing an interpolation between the SCRF and CTC losses, where\nthe same recurrent neural network (RNN) encoder is used for feature\nextraction for both outputs. We find that this multitask objective\nimproves recognition accuracy when decoding with either the SCRF or\nCTC models. Additionally, we show that CTC can also be used to pretrain\nthe RNN encoder, which improves the convergence rate when learning\nthe joint model.\n"
      ],
      "doi": "10.21437/Interspeech.2017-71"
    },
    "audhkhasi17_interspeech": {
      "authors": [
        [
          "Kartik",
          "Audhkhasi"
        ],
        [
          "Bhuvana",
          "Ramabhadran"
        ],
        [
          "George",
          "Saon"
        ],
        [
          "Michael",
          "Picheny"
        ],
        [
          "David",
          "Nahamoo"
        ]
      ],
      "title": "Direct Acoustics-to-Word Models for English Conversational Speech Recognition",
      "original": "0546",
      "page_count": 5,
      "order": 203,
      "p1": "959",
      "pn": "963",
      "abstract": [
        "Recent work on end-to-end automatic speech recognition (ASR) has shown\nthat the connectionist temporal classification (CTC) loss can be used\nto convert acoustics to phone or character sequences. Such systems\nare used with a dictionary and separately-trained Language Model (LM)\nto produce word sequences. However, they are not truly end-to-end in\nthe sense of mapping acoustics directly to words without an intermediate\nphone representation. In this paper, we present the first results employing\ndirect acoustics-to-word CTC models on two well-known public benchmark\ntasks: Switchboard and CallHome. These models do not require an LM\nor even a decoder at run-time and hence recognize speech with minimal\ncomplexity. However, due to the large number of word output units,\nCTC word models require orders of magnitude more data to train reliably\ncompared to traditional systems. We present some techniques to mitigate\nthis issue. Our CTC word model achieves a word error rate of 13.0%/18.8%\non the Hub5-2000 Switchboard/CallHome test sets without any LM or decoder\ncompared with 9.6%/16.0% for phone-based CTC with a 4-gram LM. We also\npresent rescoring results on CTC word model lattices to quantify the\nperformance benefits of a LM, and contrast the performance of word\nand phone CTC models.\n"
      ],
      "doi": "10.21437/Interspeech.2017-546"
    },
    "li17e_interspeech": {
      "authors": [
        [
          "Bo",
          "Li"
        ],
        [
          "Tara N.",
          "Sainath"
        ]
      ],
      "title": "Reducing the Computational Complexity of Two-Dimensional LSTMs",
      "original": "1164",
      "page_count": 5,
      "order": 204,
      "p1": "964",
      "pn": "968",
      "abstract": [
        "Long Short-Term Memory Recurrent Neural Networks (LSTMs) are good at\nmodeling temporal variations in speech recognition tasks, and have\nbecome an integral component of many state-of-the-art ASR systems.\nMore recently, LSTMs have been extended to model variations in the\nspeech signal in two dimensions, namely time and frequency [1, 2].\nHowever, one of the problems with two-dimensional LSTMs, such as Grid-LSTMs,\nis that the processing in both time and frequency occurs sequentially,\nthus increasing computational complexity. In this work, we look at\nminimizing the dependence of the Grid-LSTM with respect to previous\ntime and frequency points in the sequence, thus reducing computational\ncomplexity. Specifically, we compare reducing computation using a bidirectional\nGrid-LSTM (biGrid-LSTM) with non-overlapping frequency sub-band processing,\na PyraMiD-LSTM [3] and a frequency-block Grid-LSTM (fbGrid-LSTM) for\nparallel time-frequency processing. We find that the fbGrid-LSTM can\nreduce computation costs by a factor of four with no loss in accuracy,\non a 12,500 hour Voice Search task.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1164"
    },
    "lucero17_interspeech": {
      "authors": [
        [
          "Jorge C.",
          "Lucero"
        ]
      ],
      "title": "Functional Principal Component Analysis of Vocal Tract Area Functions",
      "original": "0181",
      "page_count": 5,
      "order": 205,
      "p1": "969",
      "pn": "973",
      "abstract": [
        "This paper shows the application of a functional version of principal\ncomponent analysis to build a parametrization of vocal tract area functions\nfor vowel production. Sets of measured area values for ten vowels are\nexpressed as smooth functional data and next decomposed into a mean\narea function and a basis of orthogonal eigenfunctions. Interpretations\nof the first four eigenfunctions are provided in terms of tongue movements\nand vocal tract length variations. Also, an alternative set of eigenfunctions\nwith closer association to specific regions of the vocal tract is obtained\nvia a varimax rotation. The general intention of the paper is to show\nthe benefits of a functional approach to analyze vocal tract shapes\nand motivate further applications.\n"
      ],
      "doi": "10.21437/Interspeech.2017-181"
    },
    "sivaraman17_interspeech": {
      "authors": [
        [
          "Ganesh",
          "Sivaraman"
        ],
        [
          "Carol",
          "Espy-Wilson"
        ],
        [
          "Martijn",
          "Wieling"
        ]
      ],
      "title": "Analysis of Acoustic-to-Articulatory Speech Inversion Across Different Accents and Languages",
      "original": "0260",
      "page_count": 5,
      "order": 206,
      "p1": "974",
      "pn": "978",
      "abstract": [
        "The focus of this paper is estimating articulatory movements of the\ntongue and lips from acoustic speech data. While there are several\npotential applications of such a method in speech therapy and pronunciation\ntraining, performance of such acoustic-to-articulatory inversion systems\nis not very high due to limited availability of simultaneous acoustic\nand articulatory data, substantial speaker variability, and variable\nmethods of data collection. This paper therefore evaluates the impact\nof speaker, language and accent variability on the performance of an\nacoustic-to-articulatory speech inversion system. The articulatory\ndataset used in this study consists of 21 Dutch speakers reading Dutch\nand English words and sentences, and 22 UK English speakers reading\nEnglish words and sentences. We trained several acoustic-to-articulatory\nspeech inversion systems both based on deep and shallow neural network\narchitectures in order to estimate electromagnetic articulography (EMA)\nsensor positions, as well as vocal tract variables (TVs). Our results\nshow that with appropriate feature and target normalization, a speaker-independent\nspeech inversion system trained on data from one language is able to\nestimate sensor positions (or TVs) for the same language correlating\nat about r = 0.53 with the actual sensor positions (or TVs). Cross-language\nresults show a reduced performance of r = 0.47.\n"
      ],
      "doi": "10.21437/Interspeech.2017-260"
    },
    "arai17_interspeech": {
      "authors": [
        [
          "Takayuki",
          "Arai"
        ]
      ],
      "title": "Integrated Mechanical Model of [r]-[l] and [b]-[m]-[w] Producing Consonant Cluster [br]",
      "original": "0617",
      "page_count": 5,
      "order": 207,
      "p1": "979",
      "pn": "983",
      "abstract": [
        "We have developed two types of mechanical models of the human vocal\ntract. The first model was designed for the retroflex approximant [r]\nand the alveolar lateral approximant [l]. It consisted of the main\nvocal tract and a flapping tongue, where the front half of the tongue\ncan be rotated against the palate. When the tongue is short and rotated\napproximately 90 degrees, the retroflex approximant [r] is produced.\nThe second model was designed for [b], [m], and [w]. Besides the main\nvocal tract, this model contains a movable lower lip for lip closure\nand a nasal cavity with a controllable velopharyngeal port. In the\npresent study, we joined these two mechanical models to form a new\nmodel containing the main vocal tract, the flapping tongue, the movable\nlower lip, and the nasal cavity with the controllable velopharyngeal\nport. This integrated model now makes it possible to produce consonant\nsequences. Therefore, we examined the sequence [br], in particular,\nadjusting the timing of the lip and lingual gestures to produce the\nbest sound. Because the gestures are visually observable from the outside\nof this model, the timing of the gestures were examined with the use\nof a high-speed video camera.\n"
      ],
      "doi": "10.21437/Interspeech.2017-617"
    },
    "badino17_interspeech": {
      "authors": [
        [
          "Leonardo",
          "Badino"
        ],
        [
          "Luca",
          "Franceschi"
        ],
        [
          "Raman",
          "Arora"
        ],
        [
          "Michele",
          "Donini"
        ],
        [
          "Massimiliano",
          "Pontil"
        ]
      ],
      "title": "A Speaker Adaptive DNN Training Approach for Speaker-Independent Acoustic Inversion",
      "original": "0804",
      "page_count": 5,
      "order": 208,
      "p1": "984",
      "pn": "988",
      "abstract": [
        "We address the speaker-independent acoustic inversion (AI) problem,\nalso referred to as acoustic-to-articulatory mapping. The scarce availability\nof multi-speaker articulatory data makes it difficult to learn a mapping\nwhich generalizes from a limited number of training speakers and reliably\nreconstructs the articulatory movements of unseen speakers. In this\npaper, we propose a Multi-task Learning (MTL)-based approach that explicitly\nseparates the modeling of each training speaker AI peculiarities from\nthe modeling of AI characteristics that are shared by all speakers.\nOur approach stems from the well known Regularized MTL approach and\nextends it to feed-forward deep neural networks (DNNs). Given multiple\ntraining speakers, we learn for each an acoustic-to-articulatory mapping\nrepresented by a DNN. Then, through an iterative procedure, we search\nfor a canonical speaker-independent DNN that is &#8220;similar&#8221;\nto all speaker-dependent DNNs. The degree of similarity is controlled\nby a regularization parameter. We report experiments on the University\nof Wisconsin X-ray Microbeam Database under different training/testing\nexperimental settings. The results obtained indicate that our MTL-trained\ncanonical DNN largely outperforms a standardly trained (i.e., single\ntask learning-based) speaker independent DNN.\n"
      ],
      "doi": "10.21437/Interspeech.2017-804"
    },
    "uchida17_interspeech": {
      "authors": [
        [
          "Hidetsugu",
          "Uchida"
        ],
        [
          "Daisuke",
          "Saito"
        ],
        [
          "Nobuaki",
          "Minematsu"
        ]
      ],
      "title": "Acoustic-to-Articulatory Mapping Based on Mixture of Probabilistic Canonical Correlation Analysis",
      "original": "1010",
      "page_count": 5,
      "order": 209,
      "p1": "989",
      "pn": "993",
      "abstract": [
        "In this paper, we propose a novel acoustic-to-articulatory mapping\nmodel based on mixture of probabilistic canonical correlation analysis\n(mPCCA). In PCCA, it is assumed that two different kinds of data are\nobserved as results from different linear transforms of a common latent\nvariable. It is expected that this variable represents a common factor\nwhich is inherent in the different domains, such as acoustic and articulatory\nfeature spaces. mPCCA is an expansion of PCCA and it can model a much\nmore complex structure. In mPCCA, covariance matrices of a joint probabilistic\ndistribution of acoustic-articulatory data are structuralized reasonably\nby using transformation coefficients of the linear transforms. Even\nif the number of components in mPCCA increases, the structuralized\ncovariance matrices can be expected to avoid over-fitting. Training\nand mapping processes of the mPCCA-based mapping model are reasonably\nderived by using the EM algorithm. Experiments using MOCHA-TIMIT show\nthat the proposed mapping method has achieved better mapping performance\nthan the conventional GMM-based mapping.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1010"
    },
    "sorensen17b_interspeech": {
      "authors": [
        [
          "Tanner",
          "Sorensen"
        ],
        [
          "Asterios",
          "Toutios"
        ],
        [
          "Johannes",
          "T\u00f6ger"
        ],
        [
          "Louis",
          "Goldstein"
        ],
        [
          "Shrikanth S.",
          "Narayanan"
        ]
      ],
      "title": "Test-Retest Repeatability of Articulatory Strategies Using Real-Time Magnetic Resonance Imaging",
      "original": "1488",
      "page_count": 5,
      "order": 210,
      "p1": "994",
      "pn": "998",
      "abstract": [
        "Real-time magnetic resonance imaging (rtMRI) provides information about\nthe dynamic shaping of the vocal tract during speech production. This\npaper introduces and evaluates a method for quantifying articulatory\nstrategies using rtMRI. The method decomposes the formation and release\nof a constriction in the vocal tract into the contributions of individual\narticulators such as the jaw, tongue, lips, and velum. The method uses\nan anatomically guided factor analysis and dynamical principles from\nthe framework of Task Dynamics. We evaluated the method within a test-retest\nrepeatability framework. We imaged healthy volunteers (n = 8, 4 females,\n4 males) in two scans on the same day and quantified inter-study agreement\nwith the intraclass correlation coefficient and mean within-subject\nstandard deviation. The evaluation established a limit on effect size\nand intra-group differences in articulatory strategy which can be studied\nusing the method.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1488"
    },
    "snyder17_interspeech": {
      "authors": [
        [
          "David",
          "Snyder"
        ],
        [
          "Daniel",
          "Garcia-Romero"
        ],
        [
          "Daniel",
          "Povey"
        ],
        [
          "Sanjeev",
          "Khudanpur"
        ]
      ],
      "title": "Deep Neural Network Embeddings for Text-Independent Speaker Verification",
      "original": "0620",
      "page_count": 5,
      "order": 211,
      "p1": "999",
      "pn": "1003",
      "abstract": [
        "This paper investigates replacing i-vectors for text-independent speaker\nverification with embeddings extracted from a feed-forward deep neural\nnetwork. Long-term speaker characteristics are captured in the network\nby a temporal pooling layer that aggregates over the input speech.\nThis enables the network to be trained to discriminate between speakers\nfrom variable-length speech segments. After training, utterances are\nmapped directly to fixed-dimensional speaker embeddings and pairs of\nembeddings are scored using a PLDA-based backend. We compare performance\nwith a traditional i-vector baseline on NIST SRE 2010 and 2016. We\nfind that the embeddings outperform i-vectors for short speech segments\nand are competitive on long duration test conditions. Moreover, the\ntwo representations are complementary, and their fusion improves on\nthe baseline at all operating points. Similar systems have recently\nshown promising results when trained on very large proprietary datasets,\nbut to the best of our knowledge, these are the best results reported\nfor speaker-discriminative neural networks when trained and tested\non publicly available corpora.\n"
      ],
      "doi": "10.21437/Interspeech.2017-620"
    },
    "villalba17_interspeech": {
      "authors": [
        [
          "Jes\u00fas",
          "Villalba"
        ],
        [
          "Niko",
          "Br\u00fcmmer"
        ],
        [
          "Najim",
          "Dehak"
        ]
      ],
      "title": "Tied Variational Autoencoder Backends for i-Vector Speaker Recognition",
      "original": "1018",
      "page_count": 5,
      "order": 212,
      "p1": "1004",
      "pn": "1008",
      "abstract": [
        "Probabilistic linear discriminant analysis (PLDA) is the de facto standard\nfor backends in i-vector speaker recognition. If we try to extend the\nPLDA paradigm using non-linear models, e.g., deep neural networks,\nthe posterior distributions of the latent variables and the marginal\nlikelihood become intractable. In this paper, we propose to approach\nthis problem using stochastic gradient variational Bayes. We generalize\nthe PLDA model to let i-vectors depend non-linearly on the latent factors.\nWe approximate the evidence lower bound (ELBO) by Monte Carlo sampling\nusing the reparametrization trick. This enables us to optimize of the\nELBO using backpropagation to jointly estimate the parameters that\ndefine the model and the approximate posteriors of the latent factors.\nWe also present a reformulation of the likelihood ratio, which we call\nQ-scoring. Q-scoring makes possible to efficiently score the speaker\nverification trials for this model. Experimental results on NIST SRE10\nsuggest that more data might be required to exploit the potential of\nthis method.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1018"
    },
    "ranjan17_interspeech": {
      "authors": [
        [
          "Shivesh",
          "Ranjan"
        ],
        [
          "John H.L.",
          "Hansen"
        ]
      ],
      "title": "Improved Gender Independent Speaker Recognition Using Convolutional Neural Network Based Bottleneck Features",
      "original": "1182",
      "page_count": 5,
      "order": 213,
      "p1": "1009",
      "pn": "1013",
      "abstract": [
        "This paper proposes a novel framework to improve performance of gender\nindependent i-Vector PLDA based speaker recognition using convolutional\nneural network (CNN). Convolutional layers of a CNN offer robustness\nto variations in input features including those due to gender. A CNN\nis trained for ASR with a linear bottleneck layer. Bottleneck features\nextracted using the CNN are then used to train a gender-independent\nUBM to obtain frame posteriors for training an i-Vector extractor matrix.\nTo preserve speaker specific information, a hybrid approach to training\nthe i-Vector extractor matrix using MFCC features with corresponding\nframe posteriors derived from bottleneck features is proposed. On the\nNIST SRE10 C5 condition pooled trials, our approach reduces the EER\nand minDCF 2010 by +14.62% and +14.42% respectively compared to a standard\nmfcc based gender-independent speaker recognition system.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1182"
    },
    "shon17_interspeech": {
      "authors": [
        [
          "Suwon",
          "Shon"
        ],
        [
          "Seongkyu",
          "Mun"
        ],
        [
          "Wooil",
          "Kim"
        ],
        [
          "Hanseok",
          "Ko"
        ]
      ],
      "title": "Autoencoder Based Domain Adaptation for Speaker Recognition Under Insufficient Channel Information",
      "original": "0049",
      "page_count": 5,
      "order": 214,
      "p1": "1014",
      "pn": "1018",
      "abstract": [
        "In real-life conditions, mismatch between development and test domain\ndegrades speaker recognition performance. To solve the issue, many\nresearchers explored domain adaptation approaches using matched in-domain\ndataset. However, adaptation would be not effective if the dataset\nis insufficient to estimate channel variability of the domain. In this\npaper, we explore the problem of performance degradation under such\na situation of insufficient channel information. In order to exploit\nlimited in-domain dataset effectively, we propose an unsupervised domain\nadaptation approach using Autoencoder based Domain Adaptation (AEDA).\nThe proposed approach combines an autoencoder with a denoising autoencoder\nto adapt resource-rich development dataset to test domain. The proposed\ntechnique is evaluated on the Domain Adaptation Challenge 13 experimental\nprotocols that is widely used in speaker recognition for domain mismatched\ncondition. The results show significant improvements over baselines\nand results from other prior studies.\n"
      ],
      "doi": "10.21437/Interspeech.2017-49"
    },
    "khosravani17_interspeech": {
      "authors": [
        [
          "Abbas",
          "Khosravani"
        ],
        [
          "Mohammad Mehdi",
          "Homayounpour"
        ]
      ],
      "title": "Nonparametrically Trained Probabilistic Linear Discriminant Analysis for i-Vector Speaker Verification",
      "original": "0829",
      "page_count": 5,
      "order": 215,
      "p1": "1019",
      "pn": "1023",
      "abstract": [
        "In this paper we propose to estimate the parameters of the probabilistic\nlinear discriminant analysis (PLDA) in text-independent i-vector speaker\nverification framework using a nonparametric form rather than maximum\nlikelihood estimation (MLE) obtained by an EM algorithm. In this approach\nthe between-speaker covariance matrix that represents global information\nabout the speaker variability is replaced with a local estimation computed\non a nearest neighbor basis for each target speaker. The nonparametric\nbetween- and within-speaker scatter matrices can better exploit the\ndiscriminant information in training data and is more adapted to sample\ndistribution especially when it does not satisfy Gaussian assumption\nas in i-vectors without length-normalization. We evaluated this approach\non the recent NIST 2016 speaker recognition evaluation (SRE) as well\nas NIST 2010 core condition and found significant performance improvement\ncompared with a generatively trained PLDA model.\n"
      ],
      "doi": "10.21437/Interspeech.2017-829"
    },
    "jorrin17_interspeech": {
      "authors": [
        [
          "Jes\u00fas",
          "Jorr\u00edn"
        ],
        [
          "Paola",
          "Garc\u00eda"
        ],
        [
          "Luis",
          "Buera"
        ]
      ],
      "title": "DNN Bottleneck Features for Speaker Clustering",
      "original": "0144",
      "page_count": 5,
      "order": 216,
      "p1": "1024",
      "pn": "1028",
      "abstract": [
        "In this work, we explore deep neural network bottleneck features (BNF)\nin the context of speaker clustering. A straightforward manner to deal\nwith speaker clustering is to reuse the bottleneck features extracted\nfrom speaker recognition. However, the selection of a bottleneck architecture\nor nonlinearity impacts the performance of both systems. In this work,\nwe analyze the bottleneck features obtained for speaker recognition\nand test them in a speaker clustering scenario. We observe that there\nare deep neural network topologies that work better for both cases,\neven when their classification criteria (senone classification) is\nloosely met. We present results that outperform a traditional MFCC\nsystem by 21% for speaker recognition and between 20% and 37% in clustering\nusing the same topology.\n"
      ],
      "doi": "10.21437/Interspeech.2017-144"
    },
    "aare17_interspeech": {
      "authors": [
        [
          "K\u00e4tlin",
          "Aare"
        ],
        [
          "P\u00e4rtel",
          "Lippus"
        ],
        [
          "Juraj",
          "\u0160imko"
        ]
      ],
      "title": "Creak as a Feature of Lexical Stress in Estonian",
      "original": "1155",
      "page_count": 5,
      "order": 217,
      "p1": "1029",
      "pn": "1033",
      "abstract": [
        "In addition to typological, turn-taking or sociolinguistic factors,\npresence of creaky voice in spontaneous interaction is also influenced\nby the syntactic and phonological properties of speech. For example,\ncreaky voice is reportedly more frequent in function words than content\nwords, has been observed to accompany unstressed syllables and ends\nof phrases, and is associated with relaxation and reduced speech.<br\nstyle=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  In Estonian, creaky voice is frequently used by all speakers.\nIn this paper, we observe the use of creaky voice in spontaneous Estonian\nin connection to syllabic properties of words, lexical stress, word\nclass, lengthening, and timing in phrases.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  The results indicate\nthat creak occurs less in syllables with primary stress than in unstressed\nsyllables. However, syllables with secondary stress are most frequently\ncreaky. In content words, the primary stressed syllables creak less\nfrequently and unstressed syllables more frequently compared to function\nwords. The stress-related pattern is similar in both function and content\nwords, but more contrastive in content words. The probability of creakiness\nincreases considerably with non-final lengthening within words, and\nfor all syllables towards the end of the intonational phrase.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1155"
    },
    "yanushevskaya17_interspeech": {
      "authors": [
        [
          "Irena",
          "Yanushevskaya"
        ],
        [
          "Ailbhe N\u00ed",
          "Chasaide"
        ],
        [
          "Christer",
          "Gobl"
        ]
      ],
      "title": "Cross-Speaker Variation in Voice Source Correlates of Focus and Deaccentuation",
      "original": "1535",
      "page_count": 5,
      "order": 218,
      "p1": "1034",
      "pn": "1038",
      "abstract": [
        "This paper describes cross-speaker variation in the voice source correlates\nof focal accentuation and deaccentuation. A set of utterances with\nvaried narrow focus placement as well as broad focus and deaccented\nrenditions were produced by six speakers of English. These were manually\ninverse filtered and parameterized on a pulse-by-pulse basis using\nthe LF source model. Z-normalized F0, EE, OQ and RD parameters (selected\nthrough correlation and factor analysis) were used to generate speaker\nspecific baseline voice profiles and to explore cross-speaker variation\nin focal and non-focal (post- and prefocal) syllables. As expected,\nsource parameter values were found to differ in the focal and postfocal\nportions of the utterance. For four of the six speakers the measures\nrevealed a trend of tenser phonation on the focal syllable (an increase\nin EE and F0 and typically, a decrease in OQ and RD) as well as increased\nlaxness in the postfocal part of the utterance. For two of the speakers,\nhowever, the measurements showed a different trend. These speakers\nhad very high F0 and often high EE on the focal accent. In these cases,\nRD and OQ values tended to be raised rather than lowered. The possible\nreasons for these differences are discussed.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1535"
    },
    "kalita17_interspeech": {
      "authors": [
        [
          "Sishir",
          "Kalita"
        ],
        [
          "Wendy",
          "Lalhminghlui"
        ],
        [
          "Luke",
          "Horo"
        ],
        [
          "Priyankoo",
          "Sarmah"
        ],
        [
          "S.R. Mahadeva",
          "Prasanna"
        ],
        [
          "Samarendra",
          "Dandapat"
        ]
      ],
      "title": "Acoustic Characterization of Word-Final Glottal Stops in Mizo and Assam Sora",
      "original": "0604",
      "page_count": 5,
      "order": 219,
      "p1": "1039",
      "pn": "1043",
      "abstract": [
        "The present work proposed an approach to characterize the word-final\nglottal stops in Mizo and Assam Sora language. Generally, glottal stops\nhave more strong glottal and ventricular constriction at the coda position\nthan at the onset. However, the primary source characteristics of glottal\nstops are irregular glottal cycles, abrupt glottal closing, and reduced\nopen cycle. These changes will not only affect the vocal quality parameters\nbut may also significantly affect the vocal tract characteristics due\nto changes in the subglottal coupling behavior. This motivates to analyze\nthe dynamic vocal tract characteristics in terms of source behavior,\napart from the excitation source features computed from the Linear\nPrediction (LP) residual for the acoustic characterization of the word-final\nglottal stops. The dominant resonance frequency (DRF) of the vocal\ntract using Hilbert Envelope of Numerator Group Delay (HNGD) are extracted\nat every sample instants as a cue to study this deviation. The gradual\nincrease in the DRF and significantly lower duration for which subglottal\ncoupling is occurring is observed for the glottal stop region for both\nthe languages.\n"
      ],
      "doi": "10.21437/Interspeech.2017-604"
    },
    "mokhtari17_interspeech": {
      "authors": [
        [
          "Parham",
          "Mokhtari"
        ],
        [
          "Hiroshi",
          "Ando"
        ]
      ],
      "title": "Iterative Optimal Preemphasis for Improved Glottal-Flow Estimation by Iterative Adaptive Inverse Filtering",
      "original": "0079",
      "page_count": 5,
      "order": 220,
      "p1": "1044",
      "pn": "1048",
      "abstract": [
        "Iterative adaptive inverse filtering (IAIF) [1] remains among the state-of-the-art\nalgorithms for estimating  glottal flow from the recorded speech signal.\nHere, we re-examine IAIF in light of its foundational, classical model\nof voiced (non-nasalized) speech, wherein the overall spectral tilt\nis caused only by lip-radiation and glottal effects, while the vocal-tract\ntransfer function contains formant peaks but is otherwise not tilted.\nIn contrast, IAIF initially models and cancels the formants after only\na first-order preemphasis of the speech signal, which is generally\nnot enough to completely remove spectral tilt.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  Iterative optimal\npreemphasis (IOP) is therefore proposed to replace IAIF&#8217;s initial\nstep. IOP is a rapidly converging algorithm that models a signal (then\ninverse-filters it) with one real pole (zero) at a time, until spectral\ntilt is flattened. IOP-IAIF is evaluated on sustained /a/ in a range\nof voice qualities from weak-breathy to shouted-tense. Compared with\nstandard IAIF, IOP-IAIF yields: (i) an acceptable glottal flow even\nfor a weak breathy voice that the standard algorithm failed to handle;\n(ii) generally smoother glottal flows that nevertheless retain pulse\nshape and closed phase; and (iii) enhanced separation of voice qualities\nin both normalized amplitude quotient (NAQ) and glottal harmonic spectra.\n"
      ],
      "doi": "10.21437/Interspeech.2017-79"
    },
    "sheena17_interspeech": {
      "authors": [
        [
          "Yaniv",
          "Sheena"
        ],
        [
          "M\u00ed\u0161a",
          "Hejn\u00e1"
        ],
        [
          "Yossi",
          "Adi"
        ],
        [
          "Joseph",
          "Keshet"
        ]
      ],
      "title": "Automatic Measurement of Pre-Aspiration",
      "original": "0870",
      "page_count": 5,
      "order": 221,
      "p1": "1049",
      "pn": "1053",
      "abstract": [
        "Pre-aspiration is defined as the period of glottal friction occurring\nin sequences of vocalic/consonantal sonorants and phonetically voiceless\nobstruents. We propose two machine learning methods for automatic measurement\nof pre-aspiration duration: a feedforward neural network, which works\nat the frame level; and a structured prediction model, which relies\non manually designed feature functions, and works at the segment level.\nThe input for both algorithms is a speech signal of an arbitrary length\ncontaining a single obstruent, and the output is a pair of times which\nconstitutes the pre-aspiration boundaries. We train both models on\na set of manually annotated examples. Results suggest that the structured\nmodel is superior to the frame-based model as it yields higher accuracy\nin predicting the boundaries and generalizes to new speakers and new\nlanguages. Finally, we demonstrate the applicability of our structured\nprediction algorithm by replicating linguistic analysis of pre-aspiration\nin Aberystwyth English with high correlation.\n"
      ],
      "doi": "10.21437/Interspeech.2017-870"
    },
    "nara17_interspeech": {
      "authors": [
        [
          "Kiranpreet",
          "Nara"
        ]
      ],
      "title": "Acoustic and Electroglottographic Study of Breathy and Modal Vowels as Produced by Heritage and Native Gujarati Speakers",
      "original": "1774",
      "page_count": 5,
      "order": 222,
      "p1": "1054",
      "pn": "1058",
      "abstract": [
        "While all languages of the world use modal phonation, many also rely\non other phonation types such as breathy or creaky voice. For example,\nGujarati, an Indo-Aryan language, makes a distinction between breathy\nand modal phonation among consonants and vowels: /b<sup>&#614;</sup>a&#638;/\n&#8216;burden&#8217;, /ba&#638;/ &#8216;twelve&#8217;, and /ba&#x324;&#638;/\n&#8216;outside&#8217; [1, 2]. This study, which is a replication and\nan extension of Khan [3], aims to determine the acoustic and articulatory\nparameters that distinguish breathy and modal vowels. The participants\nof this study are heritage and native Gujarati speakers.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  The materials consisted\nof 40 target words with the modal and breathy pairs of the three vowel\nqualities: /a/ vs /a&#x324;/, /e/ vs /e&#x324;/, and /o/ vs /o&#x324;/.\nThe participants uttered the words in the context of a sentence. Acoustic\nmeasurements such as H1-H2, H1-A1, harmonic-to-noise ratio and articulatory\nmeasurements such as contact quotient were calculated throughout the\nvowel duration.<br style=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  The results of the Smoothing Spline ANOVA analyses indicated that\nmeasures such as H1-A1, harmonic to noise ratio, and contact quotient\ndistinguished modal and breathy vowels for native speakers. Heritage\nspeakers also had a contrast between breathy and modal vowels, however\nthe contrast is not as robust as that of native speakers.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1774"
    },
    "wang17e_interspeech": {
      "authors": [
        [
          "Xin",
          "Wang"
        ],
        [
          "Shinji",
          "Takaki"
        ],
        [
          "Junichi",
          "Yamagishi"
        ]
      ],
      "title": "An RNN-Based Quantized F0 Model with Multi-Tier Feedback Links for Text-to-Speech Synthesis",
      "original": "0246",
      "page_count": 5,
      "order": 223,
      "p1": "1059",
      "pn": "1063",
      "abstract": [
        "A recurrent-neural-network-based F0 model for text-to-speech (TTS)\nsynthesis that generates F0 contours given textual features is proposed.\nIn contrast to related F0 models, the proposed one is designed to learn\nthe temporal correlation of F0 contours at multiple levels. The frame-level\ncorrelation is covered by feeding back the F0 output of the previous\nframe as the additional input of the current frame; meanwhile, the\ncorrelation over long-time spans is similarly modeled but by using\nF0 features aggregated over the phoneme and syllable. Another difference\nis that the output of the proposed model is not the interpolated continuous-valued\nF0 contour but rather a sequence of discrete symbols, including quantized\nF0 levels and a symbol for the unvoiced condition. By using the discrete\nF0 symbols, the proposed model avoids the influence of artificially\ninterpolated F0 curves. Experiments demonstrated that the proposed\nF0 model, which was trained using a dropout strategy, generated smooth\nF0 contours with relatively better perceived quality than those from\nbaseline RNN models.\n"
      ],
      "doi": "10.21437/Interspeech.2017-246"
    },
    "klimkov17_interspeech": {
      "authors": [
        [
          "Viacheslav",
          "Klimkov"
        ],
        [
          "Adam",
          "Nadolski"
        ],
        [
          "Alexis",
          "Moinet"
        ],
        [
          "Bartosz",
          "Putrycz"
        ],
        [
          "Roberto",
          "Barra-Chicote"
        ],
        [
          "Thomas",
          "Merritt"
        ],
        [
          "Thomas",
          "Drugman"
        ]
      ],
      "title": "Phrase Break Prediction for Long-Form Reading TTS: Exploiting Text Structure Information",
      "original": "0419",
      "page_count": 5,
      "order": 224,
      "p1": "1064",
      "pn": "1068",
      "abstract": [
        "Phrasing structure is one of the most important factors in increasing\nthe naturalness of text-to-speech (TTS) systems, in particular for\nlong-form reading. Most existing TTS systems are optimized for isolated\nshort sentences, and completely discard the larger context or structure\nof the text.<br style=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  This paper presents how we have built phrasing models based on\ndata extracted from audiobooks. We investigate how various types of\ntextual features can improve phrase break prediction: part-of-speech\n(POS), guess POS (GPOS), dependency tree features and word embeddings.\nThese features are fed into a bidirectional LSTM or a CART baseline.\nThe resulting systems are compared using both objective and subjective\nevaluations. Using BiLSTM and word embeddings proves to be beneficial.\n"
      ],
      "doi": "10.21437/Interspeech.2017-419"
    },
    "tanaka17_interspeech": {
      "authors": [
        [
          "Kou",
          "Tanaka"
        ],
        [
          "Hirokazu",
          "Kameoka"
        ],
        [
          "Tomoki",
          "Toda"
        ],
        [
          "Satoshi",
          "Nakamura"
        ]
      ],
      "title": "Physically Constrained Statistical F<SUB>0</SUB> Prediction for Electrolaryngeal Speech Enhancement",
      "original": "0688",
      "page_count": 5,
      "order": 225,
      "p1": "1069",
      "pn": "1073",
      "abstract": [
        "Electrolaryngeal (EL) speech produced by a laryngectomee using an electrolarynx\nto mechanically generate artificial excitation sounds severely suffers\nfrom unnatural fundamental frequency (F<SUB>0</SUB>) patterns caused\nby monotonic excitation sounds. To address this issue, we have previously\nproposed EL speech enhancement systems using statistical F<SUB>0</SUB>\npattern prediction methods based on a Gaussian Mixture Model (GMM),\nmaking it possible to predict the underlying F<SUB>0</SUB> pattern\nof EL speech from its spectral feature sequence. Our previous work\nrevealed that the naturalness of the predicted F<SUB>0</SUB> pattern\ncan be improved by incorporating a physically based generative model\nof F<SUB>0</SUB> patterns into the GMM-based statistical F<SUB>0</SUB>\nprediction system within a Product-of-Expert framework. However, one\ndrawback of this method is that it requires an iterative procedure\nto obtain a predicted F<SUB>0</SUB> pattern, making it difficult to\nrealize a real-time system. In this paper, we propose yet another approach\nto physically based statistical F<SUB>0</SUB> pattern prediction by\nusing a HMM-GMM framework. This approach is noteworthy in that it allows\nto generate an F<SUB>0</SUB> pattern that is both statistically likely\nand physically natural without iterative procedures. Experimental results\ndemonstrated that the proposed method was capable of generating F<SUB>0</SUB>\npatterns more similar to those in normal speech than the conventional\nGMM-based method.\n"
      ],
      "doi": "10.21437/Interspeech.2017-688"
    },
    "hojo17_interspeech": {
      "authors": [
        [
          "Nobukatsu",
          "Hojo"
        ],
        [
          "Yasuhito",
          "Ohsugi"
        ],
        [
          "Yusuke",
          "Ijima"
        ],
        [
          "Hirokazu",
          "Kameoka"
        ]
      ],
      "title": "DNN-SPACE: DNN-HMM-Based Generative Model of Voice F<SUB>0</SUB> Contours for Statistical Phrase/Accent Command Estimation",
      "original": "0719",
      "page_count": 5,
      "order": 226,
      "p1": "1074",
      "pn": "1078",
      "abstract": [
        "This paper proposes a method to extract prosodic features from a speech\nsignal by leveraging auxiliary linguistic information. A prosodic feature\nextractor called the statistical phrase/accent command estimation (SPACE)\nhas recently been proposed. This extractor is based on a statistical\nmodel formulated as a stochastic counterpart of the Fujisaki model,\na well-founded mathematical model representing the control mechanism\nof vocal fold vibration. The key idea of this approach is that a phrase/accent\ncommand pair sequence is modeled as an output sequence of a path-restricted\nhidden Markov model (HMM) so that estimating the state transition amounts\nto estimating the phrase/accent commands. Since the phrase and accent\ncommands are related to linguistic information, we may expect to improve\nthe command estimation accuracy by using them as auxiliary information\nfor the inference. To model the relationship between the phrase/accent\ncommands and linguistic information, we construct a deep neural network\n(DNN) that maps the linguistic feature vectors to the state posterior\nprobabilities of the HMM. Thus, given a pitch contour and linguistic\ninformation, we can estimate phrase/accent commands via state decoding.\nWe call this method &#8220;DNN-SPACE.&#8221; Experimental results revealed\nthat using linguistic information was effective in improving the command\nestimation accuracy.\n"
      ],
      "doi": "10.21437/Interspeech.2017-719"
    },
    "malisz17_interspeech": {
      "authors": [
        [
          "Zofia",
          "Malisz"
        ],
        [
          "Harald",
          "Berthelsen"
        ],
        [
          "Jonas",
          "Beskow"
        ],
        [
          "Joakim",
          "Gustafson"
        ]
      ],
      "title": "Controlling Prominence Realisation in Parametric DNN-Based Speech Synthesis",
      "original": "1355",
      "page_count": 5,
      "order": 227,
      "p1": "1079",
      "pn": "1083",
      "abstract": [
        "This work aims to improve text-to-speech synthesis for Wikipedia by\nadvancing and implementing models of prosodic prominence. We propose\na new system architecture with explicit prominence modeling and test\nthe first component of the architecture. We automatically extract a\nphonetic feature related to prominence from the speech signal in the\nARCTIC corpus. We then modify the label files and train an experimental\nTTS system based on the feature using Merlin, a statistical-parametric\nDNN-based engine. Test sentences with contrastive prominence on the\nword-level are synthesised and separate listening tests a) evaluating\nthe level of prominence control in generated speech, and b) naturalness,\nare conducted. Our results show that the prominence feature-enhanced\nsystem successfully places prominence on the appropriate words and\nincreases perceived naturalness relative to the baseline.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1355"
    },
    "betz17_interspeech": {
      "authors": [
        [
          "Simon",
          "Betz"
        ],
        [
          "Jana",
          "Vo\u00dfe"
        ],
        [
          "Sina",
          "Zarrie\u00df"
        ],
        [
          "Petra",
          "Wagner"
        ]
      ],
      "title": "Increasing Recall of Lengthening Detection via Semi-Automatic Classification",
      "original": "1528",
      "page_count": 5,
      "order": 228,
      "p1": "1084",
      "pn": "1088",
      "abstract": [
        "Lengthening is the ideal hesitation strategy for synthetic speech and\ndialogue systems: it is unobtrusive and hard to notice, because it\noccurs frequently in everyday speech before phrase boundaries, in accentuation,\nand in hesitation. Despite its elusiveness, it allows valuable extra\ntime for computing or information highlighting in incremental spoken\ndialogue systems. The elusiveness of the matter, however, poses a challenge\nfor extracting lengthening instances from corpus data: we suspect a\nrecall problem, as human annotators might not be able to consistently\nlabel lengthening instances. We address this issue by filtering corpus\ndata for instances of lengthening, using a simple classification method,\nbased on a threshold for normalized phone duration. The output is then\nmanually labeled for disfluency. This is compared to an existing, fully\nmanual disfluency annotation, showing that recall is significantly\nhigher with semi-automatic pre-classification. This shows that it is\ninevitable to use semi-automatic pre-selection to gather enough candidate\ndata points for manual annotation and subsequent lengthening analyses.\nAlso, it is desirable to further increase the performance of the automatic\nclassification. We evaluate in detail human versus semi-automatic annotation\nand train another classifier on the resulting dataset to check the\nintegrity of the disfluent &#8211; non-disfluent distinction.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1528"
    },
    "satt17_interspeech": {
      "authors": [
        [
          "Aharon",
          "Satt"
        ],
        [
          "Shai",
          "Rozenberg"
        ],
        [
          "Ron",
          "Hoory"
        ]
      ],
      "title": "Efficient Emotion Recognition from Speech Using Deep Learning on Spectrograms",
      "original": "0200",
      "page_count": 5,
      "order": 229,
      "p1": "1089",
      "pn": "1093",
      "abstract": [
        "We present a new implementation of emotion recognition from the para-lingual\ninformation in the speech, based on a deep neural network, applied\ndirectly to spectrograms. This new method achieves higher recognition\naccuracy compared to previously published results, while also limiting\nthe latency. It processes the speech input in smaller segments &#8212;\nup to 3 seconds, and splits a longer input into non-overlapping parts\nto reduce the prediction latency.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  The deep network comprises\ncommon neural network tools &#8212; convolutional and recurrent networks\n&#8212; which are shown to effectively learn the information that represents\nemotions directly from spectrograms. Convolution-only lower-complexity\ndeep network achieves a prediction accuracy of 66% over four emotions\n(tested on IEMOCAP &#8212; a common evaluation corpus), while a combined\nconvolution-LSTM higher-complexity model achieves 68%.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  The use of spectrograms\nin the role of speech-representing features enables effective handling\nof background non-speech signals such as music (excl. singing) and\ncrowd noise, even at noise levels comparable with the speech signal\nlevels. Using harmonic modeling to remove non-speech components from\nthe spectrogram, we demonstrate significant improvement of the emotion\nrecognition accuracy in the presence of unknown background non-speech\nsignals.\n"
      ],
      "doi": "10.21437/Interspeech.2017-200"
    },
    "zhang17b_interspeech": {
      "authors": [
        [
          "Ruo",
          "Zhang"
        ],
        [
          "Ando",
          "Atsushi"
        ],
        [
          "Satoshi",
          "Kobashikawa"
        ],
        [
          "Yushi",
          "Aono"
        ]
      ],
      "title": "Interaction and Transition Model for Speech Emotion Recognition in Dialogue",
      "original": "0713",
      "page_count": 4,
      "order": 230,
      "p1": "1094",
      "pn": "1097",
      "abstract": [
        "In this paper we propose a novel emotion recognition method modeling\ninteraction and transition in dialogue. Conventional emotion recognition\nutilizes intra-features such as MFCCs or F0s within individual utterance.\nHowever, human perceive emotions not only through individual utterances\nbut also by contextual information. The proposed method takes in account\nthe contextual effect of utterance in dialogue, which the conventional\nmethod fails to. Proposed method introduces Emotion Interaction and\nTransition (EIT) models which is constructed by end-to-end LSTMs. The\ninputs of EIT model are the previous emotions of both target and opponent\nspeaker, estimated by state-of-the-art utterance emotion recognition\nmodel. The experimental results show that the proposed method improves\noverall accuracy and average precision by a relative error reduction\nof 18.8% and 22.6% respectively.\n"
      ],
      "doi": "10.21437/Interspeech.2017-713"
    },
    "gideon17_interspeech": {
      "authors": [
        [
          "John",
          "Gideon"
        ],
        [
          "Soheil",
          "Khorram"
        ],
        [
          "Zakaria",
          "Aldeneh"
        ],
        [
          "Dimitrios",
          "Dimitriadis"
        ],
        [
          "Emily Mower",
          "Provost"
        ]
      ],
      "title": "Progressive Neural Networks for Transfer Learning in Emotion Recognition",
      "original": "1637",
      "page_count": 5,
      "order": 231,
      "p1": "1098",
      "pn": "1102",
      "abstract": [
        "Many paralinguistic tasks are closely related and thus representations\nlearned in one domain can be leveraged for another. In this paper,\nwe investigate how knowledge can be transferred between three paralinguistic\ntasks: speaker, emotion, and gender recognition. Further, we extend\nthis problem to cross-dataset tasks, asking how knowledge captured\nin one emotion dataset can be transferred to another. We focus on progressive\nneural networks and compare these networks to the conventional deep\nlearning method of pre-training and fine-tuning. Progressive neural\nnetworks provide a way to transfer knowledge and avoid the forgetting\neffect present when pre-training neural networks on different tasks.\nOur experiments demonstrate that: (1) emotion recognition can benefit\nfrom using representations originally learned for different paralinguistic\ntasks and (2) transfer learning can effectively leverage additional\ndatasets to improve the performance of emotion recognition systems.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1637"
    },
    "parthasarathy17_interspeech": {
      "authors": [
        [
          "Srinivas",
          "Parthasarathy"
        ],
        [
          "Carlos",
          "Busso"
        ]
      ],
      "title": "Jointly Predicting Arousal, Valence and Dominance with Multi-Task Learning",
      "original": "1494",
      "page_count": 5,
      "order": 232,
      "p1": "1103",
      "pn": "1107",
      "abstract": [
        "An appealing representation of emotions is the use of emotional attributes\nsuch as arousal (passive versus active), valence (negative versus positive)\nand dominance (weak versus strong). While previous studies have considered\nthese dimensions as orthogonal descriptors to represent emotions, there\nare strong theoretical and practical evidences showing the interrelation\nbetween these emotional attributes. This observation suggests that\npredicting emotional attributes with a unified framework should outperform\nmachine learning algorithms that separately predict each attribute.\nThis study presents methods to jointly learn emotional attributes by\nexploiting their interdependencies. The framework relies on  multi-task\nlearning (MTL) implemented with  deep neural networks (DNN) with shared\nhidden layers. The framework provides a principled approach to learn\nshared feature representations that maximize the performance of regression\nmodels. The results of within-corpus and cross-corpora evaluation show\nthe benefits of MTL over  single task learning (STL). MTL achieves\ngains on  concordance correlation coefficient (CCC) as high as 4.7%\nfor within-corpus evaluations, and 14.0% for cross-corpora evaluations.\nThe visualization of the activations of the last hidden layers illustrates\nthat MTL creates better feature representation. The best structure\nhas shared layers followed by attribute-dependent layers, capturing\nbetter the relation between attributes.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1494"
    },
    "le17b_interspeech": {
      "authors": [
        [
          "Duc",
          "Le"
        ],
        [
          "Zakaria",
          "Aldeneh"
        ],
        [
          "Emily Mower",
          "Provost"
        ]
      ],
      "title": "Discretized Continuous Speech Emotion Recognition with Multi-Task Deep Recurrent Neural Network",
      "original": "0094",
      "page_count": 5,
      "order": 233,
      "p1": "1108",
      "pn": "1112",
      "abstract": [
        "Estimating continuous emotional states from speech as a function of\ntime has traditionally been framed as a regression problem. In this\npaper, we present a novel approach that moves the problem into the\nclassification domain by discretizing the training labels at different\nresolutions. We employ a multi-task deep bidirectional long-short term\nmemory (BLSTM) recurrent neural network (RNN) trained with cost-sensitive\nCross Entropy loss to model these labels jointly. We introduce an emotion\ndecoding algorithm that incorporates long- and short-term temporal\nproperties of the signal to produce more robust time series estimates.\nWe show that our proposed approach achieves competitive audio-only\nperformance on the RECOLA dataset, relative to previously published\nworks as well as other strong regression baselines. This work provides\na link between regression and classification, and contributes an alternative\napproach for continuous emotion recognition.\n"
      ],
      "doi": "10.21437/Interspeech.2017-94"
    },
    "kim17d_interspeech": {
      "authors": [
        [
          "Jaebok",
          "Kim"
        ],
        [
          "Gwenn",
          "Englebienne"
        ],
        [
          "Khiet P.",
          "Truong"
        ],
        [
          "Vanessa",
          "Evers"
        ]
      ],
      "title": "Towards Speech Emotion Recognition &#8220;in the Wild&#8221; Using Aggregated Corpora and Deep Multi-Task Learning",
      "original": "0736",
      "page_count": 5,
      "order": 234,
      "p1": "1113",
      "pn": "1117",
      "abstract": [
        "One of the challenges in Speech Emotion Recognition (SER) &#8220;in\nthe wild&#8221; is the large mismatch between training and test data\n(e.g. speakers and tasks). In order to improve the generalisation capabilities\nof the emotion models, we propose to use Multi-Task Learning (MTL)\nand use gender and naturalness as auxiliary tasks in deep neural networks.\nThis method was evaluated in within-corpus and various cross-corpus\nclassification experiments that simulate conditions &#8220;in the wild&#8221;.\nIn comparison to Single-Task Learning (STL) based state of the art\nmethods, we found that our MTL method proposed improved performance\nsignificantly. Particularly, models using both gender and naturalness\nachieved more gains than those using either gender or naturalness separately.\nThis benefit was also found in the high-level representations of the\nfeature space, obtained from our method proposed, where discriminative\nemotional clusters could be observed.\n"
      ],
      "doi": "10.21437/Interspeech.2017-736"
    },
    "tamamori17_interspeech": {
      "authors": [
        [
          "Akira",
          "Tamamori"
        ],
        [
          "Tomoki",
          "Hayashi"
        ],
        [
          "Kazuhiro",
          "Kobayashi"
        ],
        [
          "Kazuya",
          "Takeda"
        ],
        [
          "Tomoki",
          "Toda"
        ]
      ],
      "title": "Speaker-Dependent WaveNet Vocoder",
      "original": "0314",
      "page_count": 5,
      "order": 235,
      "p1": "1118",
      "pn": "1122",
      "abstract": [
        "In this study, we propose a speaker-dependent WaveNet vocoder, a method\nof synthesizing speech waveforms with WaveNet, by utilizing acoustic\nfeatures from existing vocoder as auxiliary features of WaveNet. It\nis expected that WaveNet can learn a sample-by-sample correspondence\nbetween speech waveform and acoustic features. The advantage of the\nproposed method is that it does not require (1) explicit modeling of\nexcitation signals and (2) various assumptions, which are based on\nprior knowledge specific to speech. We conducted both subjective and\nobjective evaluation experiments on CMU-ARCTIC database. From the results\nof the objective evaluation, it was demonstrated that the proposed\nmethod could generate high-quality speech with phase information recovered,\nwhich was lost by a mel-cepstrum vocoder. From the results of the subjective\nevaluation, it was demonstrated that the sound quality of the proposed\nmethod was significantly improved from mel-cepstrum vocoder, and the\nproposed method could capture source excitation information more accurately.\n"
      ],
      "doi": "10.21437/Interspeech.2017-314"
    },
    "gu17_interspeech": {
      "authors": [
        [
          "Yu",
          "Gu"
        ],
        [
          "Zhen-Hua",
          "Ling"
        ]
      ],
      "title": "Waveform Modeling Using Stacked Dilated Convolutional Neural Networks for Speech Bandwidth Extension",
      "original": "0336",
      "page_count": 5,
      "order": 236,
      "p1": "1123",
      "pn": "1127",
      "abstract": [
        "This paper presents a waveform modeling and generation method for speech\nbandwidth extension (BWE) using stacked dilated convolutional neural\nnetworks (CNNs) with causal or non-causal convolutional layers. Such\ndilated CNNs describe the predictive distribution for each wideband\nor high-frequency speech sample conditioned on the input narrowband\nspeech samples. Distinguished from conventional frame-based BWE approaches,\nthe proposed methods can model the speech waveforms directly and therefore\navert the spectral conversion and phase estimation problems. Experimental\nresults prove that the BWE methods proposed in this paper can achieve\nbetter performance than the state-of-the-art frame-based approach utilizing\nrecurrent neural networks (RNNs) incorporating long short-term memory\n(LSTM) cells in subjective preference tests.\n"
      ],
      "doi": "10.21437/Interspeech.2017-336"
    },
    "takaki17_interspeech": {
      "authors": [
        [
          "Shinji",
          "Takaki"
        ],
        [
          "Hirokazu",
          "Kameoka"
        ],
        [
          "Junichi",
          "Yamagishi"
        ]
      ],
      "title": "Direct Modeling of Frequency Spectra and Waveform Generation Based on Phase Recovery for DNN-Based Speech Synthesis",
      "original": "0488",
      "page_count": 5,
      "order": 237,
      "p1": "1128",
      "pn": "1132",
      "abstract": [
        "In statistical parametric speech synthesis (SPSS) systems using the\nhigh-quality vocoder, acoustic features such as mel-cepstrum coefficients\nand F0 are predicted from linguistic features in order to utilize the\nvocoder to generate speech waveforms. However, the generated speech\nwaveform generally suffers from quality deterioration such as buzziness\ncaused by utilizing the vocoder. Although several attempts such as\nimproving an excitation model have been investigated to alleviate the\nproblem, it is difficult to completely avoid it if the SPSS system\nis based on the vocoder. To overcome this problem, there have recently\nbeen attempts to directly model waveform samples. Superior performance\nhas been demonstrated, but computation time and latency are still issues.\nWith the aim to construct another type of DNN-based speech synthesizer\nwith neither the vocoder nor computational explosion, we investigated\ndirect modeling of frequency spectra and waveform generation based\non phase recovery. In this framework, STFT spectral amplitudes that\ninclude harmonic information derived from F0 are directly predicted\nthrough a DNN-based acoustic model and we use Griffin and Lim&#8217;s\napproach to recover phase and generate waveforms. The experimental\nresults showed that the proposed system synthesized speech without\nbuzziness and outperformed one generated from a conventional system\nusing the vocoder.\n"
      ],
      "doi": "10.21437/Interspeech.2017-488"
    },
    "ronanki17_interspeech": {
      "authors": [
        [
          "Srikanth",
          "Ronanki"
        ],
        [
          "Oliver",
          "Watts"
        ],
        [
          "Simon",
          "King"
        ]
      ],
      "title": "A Hierarchical Encoder-Decoder Model for Statistical Parametric Speech Synthesis",
      "original": "0628",
      "page_count": 5,
      "order": 238,
      "p1": "1133",
      "pn": "1137",
      "abstract": [
        "Current approaches to statistical parametric speech synthesis using\nNeural Networks generally require input at the same temporal resolution\nas the output, typically a frame every 5ms, or in some cases at waveform\nsampling rate. It is therefore necessary to fabricate highly-redundant\nframe-level (or sample-level) linguistic features at the input. This\npaper proposes the use of a hierarchical encoder-decoder model to perform\nthe sequence-to-sequence regression in a way that takes the input linguistic\nfeatures at their original timescales, and preserves the relationships\nbetween words, syllables and phones. The proposed model is designed\nto make more effective use of suprasegmental features than conventional\narchitectures, as well as being computationally efficient. Experiments\nwere conducted on prosodically-varied audiobook material because the\nuse of supra-segmental features is thought to be particularly important\nin this case. Both objective measures and results from subjective listening\ntests, which asked listeners to focus on prosody, show that the proposed\nmethod performs significantly better than a conventional architecture\nthat requires the linguistic input to be at the acoustic frame rate.<br\nstyle=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  We provide code and a recipe to enable our system to be reproduced\nusing the Merlin toolkit.\n"
      ],
      "doi": "10.21437/Interspeech.2017-628"
    },
    "kobayashi17_interspeech": {
      "authors": [
        [
          "Kazuhiro",
          "Kobayashi"
        ],
        [
          "Tomoki",
          "Hayashi"
        ],
        [
          "Akira",
          "Tamamori"
        ],
        [
          "Tomoki",
          "Toda"
        ]
      ],
      "title": "Statistical Voice Conversion with WaveNet-Based Waveform Generation",
      "original": "0986",
      "page_count": 5,
      "order": 239,
      "p1": "1138",
      "pn": "1142",
      "abstract": [
        "This paper presents a statistical voice conversion (VC) technique with\ntheWaveNet-based waveform generation. VC based on a Gaussian mixture\nmodel (GMM) makes it possible to convert the speaker identity of a\nsource speaker into that of a target speaker. However, in the conventional\nvocoding process, various factors such as F<SUB>0</SUB> extraction\nerrors, parameterization errors and over-smoothing effects of converted\nfeature trajectory cause the modeling errors of the speech waveform,\nwhich usually bring about sound quality degradation of the converted\nvoice. To address this issue, we apply a direct waveform generation\ntechnique based on a WaveNet vocoder to VC. In the proposed method,\nfirst, the acoustic features of the source speaker are converted into\nthose of the target speaker based on the GMM. Then, the waveform samples\nof the converted voice are generated based on the WaveNet vocoder conditioned\non the converted acoustic features. In this paper, to investigate the\nmodeling accuracies of the converted speech waveform, we compare several\ntypes of the acoustic features for training and synthesizing based\non the WaveNet vocoder. The experimental results confirmed that the\nproposed VC technique achieves higher conversion accuracy on speaker\nindividuality with comparable sound quality compared to the conventional\nVC technique.\n"
      ],
      "doi": "10.21437/Interspeech.2017-986"
    },
    "wan17_interspeech": {
      "authors": [
        [
          "Vincent",
          "Wan"
        ],
        [
          "Yannis",
          "Agiomyrgiannakis"
        ],
        [
          "Hanna",
          "Silen"
        ],
        [
          "Jakub",
          "V\u00edt"
        ]
      ],
      "title": "Google&#8217;s Next-Generation Real-Time Unit-Selection Synthesizer Using Sequence-to-Sequence LSTM-Based Autoencoders",
      "original": "1107",
      "page_count": 5,
      "order": 240,
      "p1": "1143",
      "pn": "1147",
      "abstract": [
        "A neural network model that significant improves unit-selection-based\nText-To-Speech synthesis is presented. The model employs a sequence-to-sequence\nLSTM-based autoencoder that compresses the acoustic and linguistic\nfeatures of each unit to a fixed-size vector referred to as an  embedding.\nUnit-selection is facilitated by formulating the target cost as an\nL<SUB>2</SUB> distance in the embedding space. In open-domain speech\nsynthesis the method achieves a 0.2 improvement in the MOS, while for\nlimited-domain it reaches the cap of 4.5 MOS. Furthermore, the new\nTTS system halves the gap between the previous unit-selection system\nand WaveNet in terms of quality while retaining low computational cost\nand latency.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1107"
    },
    "kain17_interspeech": {
      "authors": [
        [
          "Alexander",
          "Kain"
        ],
        [
          "Max Del",
          "Giudice"
        ],
        [
          "Kris",
          "Tjaden"
        ]
      ],
      "title": "A Comparison of Sentence-Level Speech Intelligibility Metrics",
      "original": "0567",
      "page_count": 5,
      "order": 241,
      "p1": "1148",
      "pn": "1152",
      "abstract": [
        "We examine existing and novel automatically-derived acoustic metrics\nthat are predictive of speech intelligibility. We hypothesize that\nthe degree of variability in feature space is correlated with the extent\nof a speaker&#8217;s phonemic inventory, their degree of articulatory\ndisplacements, and thus with their degree of perceived speech intelligibility.\nWe begin by using fully-automatic F1/F2 formant frequency trajectories\nfor both vowel space area calculation and as input to a proposed class-separability\nmetric. We then switch to representing vowels by means of short-term\nspectral features, and measure vowel separability in that space. Finally,\nwe consider the case where phonetic labeling is unavailable; here we\ncalculate short-term spectral features for the entire speech utterance\nand then estimate their entropy based on the length of a minimum spanning\ntree. In an alternative approach, we propose to first segment the speech\nsignal using a hidden Markov model, and then calculate spectral feature\nseparability based on the automatically-derived classes. We apply all\napproaches to a database with healthy controls as well as speakers\nwith mild dysarthria, and report the resulting coefficients of determination.\n"
      ],
      "doi": "10.21437/Interspeech.2017-567"
    },
    "irino17_interspeech": {
      "authors": [
        [
          "Toshio",
          "Irino"
        ],
        [
          "Eri",
          "Takimoto"
        ],
        [
          "Toshie",
          "Matsui"
        ],
        [
          "Roy D.",
          "Patterson"
        ]
      ],
      "title": "An Auditory Model of Speaker Size Perception for Voiced Speech Sounds",
      "original": "0196",
      "page_count": 5,
      "order": 242,
      "p1": "1153",
      "pn": "1157",
      "abstract": [
        "An auditory model was developed to explain the results of behavioral\nexperiments on perception of speaker size with voiced speech sounds.\nIt is based on the dynamic, compressive gammachirp (dcGC) filterbank\nand a weighting function (SSI weight) derived from a theory of size-shape\nsegregation in the auditory system. Voiced words with and without high-frequency\nemphasis (+6 dB/octave) were produced using a speech vocoder (STRAIGHT).\nThe SSI weighting function reduces the effect of glottal pulse excitation\nin voiced speech, which, in turn, makes it possible for the model to\nexplain the individual subject variability in the data.\n"
      ],
      "doi": "10.21437/Interspeech.2017-196"
    },
    "bosch17_interspeech": {
      "authors": [
        [
          "L. ten",
          "Bosch"
        ],
        [
          "L.",
          "Boves"
        ],
        [
          "M.",
          "Ernestus"
        ]
      ],
      "title": "The Recognition of Compounds: A Computational Account",
      "original": "1048",
      "page_count": 5,
      "order": 243,
      "p1": "1158",
      "pn": "1162",
      "abstract": [
        "This paper investigates the processes in comprehending spoken noun-noun\ncompounds, using data from the BALDEY database. BALDEY contains lexicality\njudgments and reaction times (RTs) for Dutch stimuli for which also\nlinguistic information is included. Two different approaches are combined.\nThe first is based on regression by Dynamic Survival Analysis, which\nmodels decisions and RTs as a consequence of the fact that a cumulative\ndensity function exceeds some threshold. The parameters of that function\nare estimated from the observed RT data. The second approach is based\non DIANA, a process-oriented computational model of human word comprehension,\nwhich simulates the comprehension process with the acoustic stimulus\nas input. DIANA gives the identity and the number of the word candidates\nthat are activated at each 10 ms time step.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  Both approaches show\nhow the processes involved in comprehending compounds change during\na stimulus. Survival Analysis shows that the impact of word duration\nvaries during the course of a stimulus. The density of word and non-word\nhypotheses in DIANA shows a corresponding pattern with different regimes.\nWe show how the approaches complement each other, and discuss additional\nways in which data and process models can be combined.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1048"
    },
    "jahromi17_interspeech": {
      "authors": [
        [
          "Mohsen Zareian",
          "Jahromi"
        ],
        [
          "Jan",
          "\u00d8stergaard"
        ],
        [
          "Jesper",
          "Jensen"
        ]
      ],
      "title": "Humans do not Maximize the Probability of Correct Decision When Recognizing DANTALE Words in Noise",
      "original": "1158",
      "page_count": 5,
      "order": 244,
      "p1": "1163",
      "pn": "1167",
      "abstract": [
        "Inspired by the DANTALE II listening test paradigm, which is used for\ndetermining the intelligibility of noisy speech, we assess the hypothesis\nthat humans maximize the probability of correct decision when recognizing\nwords contaminated by additive Gaussian, speech-shaped noise. We first\npropose a statistical Gaussian communication and classification scenario,\nwhere word models are built from short term spectra of human speech,\nand optimal classifiers in the sense of maximum a posteriori estimation\nare derived. Then, we perform a listening test, where the participants\nare instructed to make their best guess of words contaminated with\nspeech-shaped Gaussian noise. Comparing the human&#8217;s performance\nto that of the optimal classifier reveals that at high SNR, humans\nperform comparable to the optimal classifier. However, at low SNR,\nthe human performance is inferior to that of the optimal classifier.\nThis shows that, at least in this specialized task, humans are generally\nnot able to maximize the probability of correct decision, when recognizing\nwords.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1158"
    },
    "huber17_interspeech": {
      "authors": [
        [
          "Rainer",
          "Huber"
        ],
        [
          "Constantin",
          "Spille"
        ],
        [
          "Bernd T.",
          "Meyer"
        ]
      ],
      "title": "Single-Ended Prediction of Listening Effort Based on Automatic Speech Recognition",
      "original": "1360",
      "page_count": 5,
      "order": 245,
      "p1": "1168",
      "pn": "1172",
      "abstract": [
        "A new, single-ended, i.e. reference-free measure for the prediction\nof perceived listening effort of noisy speech is presented. It is based\non phoneme posterior probabilities (or posteriorgrams) obtained from\na deep neural network of an automatic speech recognition system. Additive\nnoisy or other distortions of speech tend to smear the posteriorgrams.\nThe smearing is quantified by a performance measure, which is used\nas a predictor for the perceived listening effort required to understand\nthe noisy speech. The proposed measure was evaluated using a database\nobtained from the subjective evaluation of noise reduction algorithms\nof commercial hearing aids. Listening effort ratings of processed noisy\nspeech samples were gathered from 20 hearing-impaired subjects. Averaged\nsubjective ratings were compared with corresponding predictions computed\nby the proposed new method, the ITU-T standard P.563 for single-ended\nspeech quality assessment, the American National Standard ANIQUE+ for\nsingle-ended speech quality assessment, and a single-ended SNR estimator.\nThe proposed method achieved a good correlation with mean subjective\nratings and clearly outperformed the standard speech quality measures\nand the SNR estimator.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1360"
    },
    "neufeld17_interspeech": {
      "authors": [
        [
          "Chris",
          "Neufeld"
        ]
      ],
      "title": "Modeling Categorical Perception with the Receptive Fields of Auditory Neurons",
      "original": "1611",
      "page_count": 5,
      "order": 246,
      "p1": "1173",
      "pn": "1177",
      "abstract": [
        "This paper demonstrates that a low-level, linear description of the\nresponse properties of auditory neurons can exhibit some of the high-level\nproperties of the categorical perception of human speech. In particular,\nit is shown that the non-linearities observed in the human perception\nof speech sounds which span a categorical boundaries can be understood\nas arising rather naturally from a low-level statistical description\nof phonemic contrasts in the time-frequency plane, understood here\nas the receptive field of auditory neurons. The TIMIT database was\nused to train a model auditory neuron which discriminates between /s/\nand /sh/, and a computer simulation was conducted which demonstrates\nthat the neuron responds categorically to a linear continuum of synthetic\nfricative sounds which span the /s/-/sh/ boundary. The response of\nthe model provides a good fit to human labeling behavior, and in addition,\nis able to account for asymmetries in reaction time across the two\ncategories.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1611"
    },
    "wang17f_interspeech": {
      "authors": [
        [
          "Yannan",
          "Wang"
        ],
        [
          "Jun",
          "Du"
        ],
        [
          "Li-Rong",
          "Dai"
        ],
        [
          "Chin-Hui",
          "Lee"
        ]
      ],
      "title": "A Maximum Likelihood Approach to Deep Neural Network Based Nonlinear Spectral Mapping for Single-Channel Speech Separation",
      "original": "0830",
      "page_count": 5,
      "order": 247,
      "p1": "1178",
      "pn": "1182",
      "abstract": [
        "In contrast to the conventional minimum mean squared error (MMSE) training\ncriterion for nonlinear spectral mapping based on deep neural networks\n(DNNs), we propose a probabilistic learning framework to estimate the\nDNN parameters for single-channel speech separation. A statistical\nanalysis of the prediction error vector at the DNN output reveals that\nit follows a unimodal density for each log power spectral component.\nBy characterizing the prediction error vector as a multivariate Gaussian\ndensity with zero mean vector and an unknown covariance matrix, we\npresent a maximum likelihood (ML) approach to DNN parameter learning.\nOur experiments on the Speech Separation Challenge (SSC) corpus show\nthat the proposed learning approach can achieve a better generalization\ncapability and a faster convergence than MMSE-based DNN learning. Furthermore,\nwe demonstrate that the ML-trained DNN consistently outperforms MMSE-trained\nDNN in all the objective measures of speech quality and intelligibility\nin single-channel speech separation.\n"
      ],
      "doi": "10.21437/Interspeech.2017-830"
    },
    "higuchi17_interspeech": {
      "authors": [
        [
          "Takuya",
          "Higuchi"
        ],
        [
          "Keisuke",
          "Kinoshita"
        ],
        [
          "Marc",
          "Delcroix"
        ],
        [
          "Kate\u0159ina",
          "\u017dmol\u00edkov\u00e1"
        ],
        [
          "Tomohiro",
          "Nakatani"
        ]
      ],
      "title": "Deep Clustering-Based Beamforming for Separation with Unknown Number of Sources",
      "original": "0721",
      "page_count": 5,
      "order": 248,
      "p1": "1183",
      "pn": "1187",
      "abstract": [
        "This paper extends a deep clustering algorithm for use with time-frequency\nmasking-based beamforming and perform separation with an unknown number\nof sources. Deep clustering is a recently proposed single-channel source\nseparation algorithm, which projects inputs into the embedding space\nand performs clustering in the embedding domain. In deep clustering,\nbi-directional long short-term memory (BLSTM) recurrent neural networks\nare trained to make embedding vectors orthogonal for different speakers\nand concurrent for the same speaker. Then, by clustering the embedding\nvectors at test time, we can estimate time-frequency masks for separation.\nIn this paper, we extend the deep clustering algorithm to a multiple\nmicrophone setup and incorporate deep clustering-based time-frequency\nmask estimation into masking-based beamforming, which has been shown\nto be more effective than masking for automatic speech recognition.\nMoreover, we perform source counting by computing the rank of the covariance\nmatrix of the embedding vectors. With our proposed approach, we can\nperform masking-based beamforming in a multiple-speaker case without\nknowing the number of speakers. Experimental results show that our\nproposed deep clustering-based beamformer achieves comparable source\nseparation performance to that obtained with a complex Gaussian mixture\nmodel-based beamformer, which requires the number of sources in advance\nfor mask estimation.\n"
      ],
      "doi": "10.21437/Interspeech.2017-721"
    },
    "pirhosseinloo17_interspeech": {
      "authors": [
        [
          "Shadi",
          "Pirhosseinloo"
        ],
        [
          "Kostas",
          "Kokkinakis"
        ]
      ],
      "title": "Time-Frequency Masking for Blind Source Separation with Preserved Spatial Cues",
      "original": "0066",
      "page_count": 5,
      "order": 249,
      "p1": "1188",
      "pn": "1192",
      "abstract": [
        "In this paper, we address the problem of speech source separation by\nrelying on time-frequency binary masks to segregate binaural mixtures.\nWe describe an algorithm which can tackle reverberant mixtures and\ncan extract the original sources while preserving their original spatial\nlocations. The performance of the proposed algorithm is evaluated objectively\nand subjectively, by assessing the estimated interaural time differences\nversus their theoretical values and by testing for localization acuity\nin normal-hearing listeners for different spatial locations in a reverberant\nroom. Experimental results indicate that the proposed algorithm is\ncapable of preserving the spatial information of the recovered source\nsignals while keeping the signal-to-distortion and signal-to-interference\nratios high.\n"
      ],
      "doi": "10.21437/Interspeech.2017-66"
    },
    "chien17b_interspeech": {
      "authors": [
        [
          "Jen-Tzung",
          "Chien"
        ],
        [
          "Kuan-Ting",
          "Kuo"
        ]
      ],
      "title": "Variational Recurrent Neural Networks for Speech Separation",
      "original": "0832",
      "page_count": 5,
      "order": 250,
      "p1": "1193",
      "pn": "1197",
      "abstract": [
        "We present a new stochastic learning machine for speech separation\nbased on the variational recurrent neural network (VRNN). This VRNN\nis constructed from the perspectives of generative stochastic network\nand variational auto-encoder. The idea is to faithfully characterize\nthe randomness of hidden state of a recurrent neural network through\nvariational learning. The neural parameters under this latent variable\nmodel are estimated by maximizing the variational lower bound of log\nmarginal likelihood. An inference network driven by the variational\ndistribution is trained from a set of mixed signals and the associated\nsource targets. A novel supervised VRNN is developed for speech separation.\nThe proposed VRNN provides a stochastic point of view which accommodates\nthe uncertainty in hidden states and facilitates the analysis of model\nconstruction. The masking function is further employed in network outputs\nfor speech separation. The benefit of using VRNN is demonstrated by\nthe experiments on monaural speech separation.\n"
      ],
      "doi": "10.21437/Interspeech.2017-832"
    },
    "andrei17_interspeech": {
      "authors": [
        [
          "Valentin",
          "Andrei"
        ],
        [
          "Horia",
          "Cucu"
        ],
        [
          "Corneliu",
          "Burileanu"
        ]
      ],
      "title": "Detecting Overlapped Speech on Short Timeframes Using Deep Learning",
      "original": "0188",
      "page_count": 5,
      "order": 251,
      "p1": "1198",
      "pn": "1202",
      "abstract": [
        "The intent of this work is to demonstrate how deep learning techniques\ncan be successfully used to detect overlapped speech on independent\nshort timeframes. A secondary objective is to provide an understanding\non how the duration of the signal frame influences the accuracy of\nthe method. We trained a deep neural network with heterogeneous layers\nand obtained close to 80% inference accuracy on frames going as low\nas 25 milliseconds. The proposed system provides higher detection quality\nthan existing work and can predict overlapped speech with up to 3 simultaneous\nspeakers. The method exposes low response latency and does not require\na high amount of computing power.\n"
      ],
      "doi": "10.21437/Interspeech.2017-188"
    },
    "li17f_interspeech": {
      "authors": [
        [
          "Xu",
          "Li"
        ],
        [
          "Junfeng",
          "Li"
        ],
        [
          "Yonghong",
          "Yan"
        ]
      ],
      "title": "Ideal Ratio Mask Estimation Using Deep Neural Networks for Monaural Speech Segregation in Noisy Reverberant Conditions",
      "original": "0549",
      "page_count": 5,
      "order": 252,
      "p1": "1203",
      "pn": "1207",
      "abstract": [
        "Monaural speech segregation is an important problem in robust speech\nprocessing and has been formulated as a supervised learning problem.\nIn supervised learning methods, the ideal binary mask (IBM) is usually\nused as the target because of its simplicity and large speech intelligibility\ngains. Recently, the ideal ratio mask (IRM) has been found to improve\nthe speech quality over the IBM. However, the IRM was originally defined\nin anechoic conditions and did not consider the effect of reverberation.\nIn this paper, the IRM is extended to reverberant conditions where\nthe direct sound and early reflections of target speech are regarded\nas the desired signal. Deep neural networks (DNNs) is employed to estimate\nthe extended IRM in the noisy reverberant conditions. The estimated\nIRM is then applied to the noisy reverberant mixture for speech segregation.\nExperimental results show that the estimated IRM provides substantial\nimprovements in speech intelligibility and speech quality over the\nunprocessed mixture signals under various noisy and reverberant conditions.\n"
      ],
      "doi": "10.21437/Interspeech.2017-549"
    },
    "quiroz17_interspeech": {
      "authors": [
        [
          "Sergio I.",
          "Quiroz"
        ],
        [
          "Marzena",
          "\u017bygis"
        ]
      ],
      "title": "The Vocative Chant and Beyond: German Calling Melodies Under Routine and Urgent Contexts",
      "original": "1635",
      "page_count": 5,
      "order": 253,
      "p1": "1208",
      "pn": "1212",
      "abstract": [
        "This paper investigates calling melodies produced by 21 Standard German\nnative speakers on a discourse completion task across two contexts:\n(i) routine context &#8212; calling a child from afar to come in for\ndinner; (ii) urgent context &#8212; calling a child from afar for a\nchastising. The intent of this investigation is to bring attention\nto other calling melodies found in German beside the vocative chant\nand to give an insight to their acoustic profile.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  Three major melodies\nwere identified in the two contexts: vocative chant (100% of routine\ncontext productions), urgent call (100% of male urgent context productions,\n52.2% female productions), and stern call (47.8% female urgent context\nproductions). A subsequent quantitative analysis was carried out on\nthese calls across these parameters: (i) tonal scaling at tonal landmarks;\n(ii) proportional alignment of selected tonal landmarks with respect\nto the stressed or last vowel; and (iii) amplitude (integral and RMS)\nand (iv) duration of the stressed vowel, stressed syllable, and word.\nThe resulting data were analyzed using a linear mixed model approach.<br\nstyle=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  The results point to significant differences in the contours produced\nin the aforementioned parameters. We also proposed a phonological description\nof the contours in the framework of Autosegmental-Metrical Phonology.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1635"
    },
    "simko17_interspeech": {
      "authors": [
        [
          "Juraj",
          "\u0160imko"
        ],
        [
          "Antti",
          "Suni"
        ],
        [
          "Katri",
          "Hiovain"
        ],
        [
          "Martti",
          "Vainio"
        ]
      ],
      "title": "Comparing Languages Using Hierarchical Prosodic Analysis",
      "original": "1044",
      "page_count": 5,
      "order": 254,
      "p1": "1213",
      "pn": "1217",
      "abstract": [
        "We present a novel, data-driven approach to assessing mutual similarities\nand differences among a group of languages, based on purely prosodic\ncharacteristics, namely f<SUB>0</SUB> and energy envelope signals.\nThese signals are decomposed using continuous wavelet transform; the\ncomponents represent f<SUB>0</SUB> and energy patterns on three levels\nof prosodic hierarchy roughly corresponding to syllables, words and\nphrases. Unigram language models with states derived from a combination\nof &#916;-features obtained from these components are trained and compared\nusing a mutual perplexity measure. In this pilot study we apply this\napproach to a small corpus of spoken material from seven languages\n(Estonian, Finnish, Hungarian, German, Swedish, Russian and Slovak)\nwith a rich history of mutual language contacts. We present similarity\ntrees (dendrograms) derived from the models using the hierarchically\ndecomposed prosodic signals separately as well as combined, and compare\nthem with patterns obtained from non-decomposed signals. We show that\n(1) plausible similarity patterns, reflecting language family relationships\nand the known contact history can be obtained even from a relatively\nsmall data set, and (2) the hierarchical decomposition approach using\nboth f<SUB>0</SUB> and energy provides the most comprehensive results.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1044"
    },
    "ip17_interspeech": {
      "authors": [
        [
          "Martin Ho Kwan",
          "Ip"
        ],
        [
          "Anne",
          "Cutler"
        ]
      ],
      "title": "Intonation Facilitates Prediction of Focus Even in the Presence of Lexical Tones",
      "original": "0264",
      "page_count": 5,
      "order": 255,
      "p1": "1218",
      "pn": "1222",
      "abstract": [
        "In English and Dutch, listeners entrain to prosodic contours to predict\nwhere focus will fall in an utterance. However, is this strategy universally\navailable, even in languages with different phonological systems? In\na phoneme detection experiment, we examined whether prosodic entrainment\nis also found in Mandarin Chinese, a tone language, where in principle\nthe use of pitch for lexical identity may take precedence over the\nuse of pitch cues to salience. Consistent with the results from Germanic\nlanguages, response times were facilitated when preceding intonation\npredicted accent on the target-bearing word. Acoustic analyses revealed\ngreater F0 range in the preceding intonation of the predicted-accent\nsentences. These findings have implications for how universal and language-specific\nmechanisms interact in the processing of salience.\n"
      ],
      "doi": "10.21437/Interspeech.2017-264"
    },
    "zahner17_interspeech": {
      "authors": [
        [
          "Katharina",
          "Zahner"
        ],
        [
          "Heather",
          "Kember"
        ],
        [
          "Bettina",
          "Braun"
        ]
      ],
      "title": "Mind the Peak: When  Museum is Temporarily Understood as Musical in Australian English",
      "original": "0839",
      "page_count": 5,
      "order": 256,
      "p1": "1223",
      "pn": "1227",
      "abstract": [
        "Intonation languages signal pragmatic functions (e.g. information structure)\nby means of different pitch accent types. Acoustically, pitch accent\ntypes differ in the alignment of pitch peaks (and valleys) in regard\nto stressed syllables, which makes the position of pitch peaks an \nunreliable cue to lexical stress (even though pitch peaks and lexical\nstress often coincide in intonation languages). We here investigate\nthe effect of  pitch accent type on lexical activation in English.\nResults of a visual-world eye-tracking study show that Australian English\nlisteners temporarily activate SWW-words ( musical) if presented with\nWSW-words ( museum) with early-peak accents (H+!H*), compared to medial-peak\naccents (L+H*). Thus, in addition to signalling pragmatic functions,\nthe alignment of tonal targets immediately affects lexical activation\nin English.\n"
      ],
      "doi": "10.21437/Interspeech.2017-839"
    },
    "rognoni17_interspeech": {
      "authors": [
        [
          "Luca",
          "Rognoni"
        ],
        [
          "Judith",
          "Bishop"
        ],
        [
          "Miriam",
          "Corris"
        ]
      ],
      "title": "Pashto Intonation Patterns",
      "original": "1353",
      "page_count": 5,
      "order": 257,
      "p1": "1228",
      "pn": "1232",
      "abstract": [
        "A hand-labelled Pashto speech data set containing spontaneous conversations\nis analysed in order to propose an intonational inventory of Pashto.\nBasic intonation patterns observed in the language are summarised.\nThe relationship between pitch accent and part of speech (PoS), which\nwas also annotated for each word in the data set, is briefly addressed.<br\nstyle=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  The results are compared with the intonational literature on Persian,\na better-described and closely-related language. The results show that\nPashto intonation patterns are similar to Persian, as well as reflecting\ncommon intonation patterns such as falling tone for statements and\nWH-questions, and yes/no questions ending in a rising tone. The data\nalso show that the most frequently used intonation pattern in Pashto\nis the so-called hat pattern. The distribution of pitch accent is quite\nfree both in Persian and Pashto, but there is a stronger association\nof pitch accent with content than with function words, as is typical\nof stress-accent languages.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  The phonetic realisation\nof focus appears to be conveyed with the same acoustic cues as in Persian,\nwith a higher pitch excursion and longer duration of the stressed syllable\nof the word in focus. The data also suggest that post-focus compression\n(PFC) is present in Pashto.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1353"
    },
    "maekawa17_interspeech": {
      "authors": [
        [
          "Kikuo",
          "Maekawa"
        ]
      ],
      "title": "A New Model of Final Lowering in Spontaneous Monologue",
      "original": "0175",
      "page_count": 5,
      "order": 258,
      "p1": "1233",
      "pn": "1237",
      "abstract": [
        "F0 downtrend observed in spontaneous monologues in the Corpus of Spontaneous\nJapanese was analyzed with special attention to the modeling of final\nlowering. In addition to the previous finding that the domain of final\nlowering covers all tones in the final accentual phrase, it turned\nout that the last L tone in the penultimate accentual phrase played\nimportant role in the control of final lowering. It is this tone that\nfirst reached the bottom of the speaker&#8217;s pitch range in the\ntime course of utterance; it also turned out that the phonetic realization\nof this tone is the most stable of all tones in terms of the F0 variability.\nRegression model of F0 downtrends is generated by generalized linear\nmixed-effect modeling and evaluated by cross-validation. The mean prediction\nerror of z-normalized F0 values in the best model was 0.25 standard\ndeviation.\n"
      ],
      "doi": "10.21437/Interspeech.2017-175"
    },
    "ma17c_interspeech": {
      "authors": [
        [
          "Xi",
          "Ma"
        ],
        [
          "Zhiyong",
          "Wu"
        ],
        [
          "Jia",
          "Jia"
        ],
        [
          "Mingxing",
          "Xu"
        ],
        [
          "Helen",
          "Meng"
        ],
        [
          "Lianhong",
          "Cai"
        ]
      ],
      "title": "Speech Emotion Recognition with Emotion-Pair Based Framework Considering Emotion Distribution Information in Dimensional Emotion Space",
      "original": "0619",
      "page_count": 5,
      "order": 259,
      "p1": "1238",
      "pn": "1242",
      "abstract": [
        "In this work, an emotion-pair based framework is proposed for speech\nemotion recognition, which constructs more discriminative feature subspaces\nfor every two different emotions (emotion-pair) to generate more precise\nemotion bi-classification results. Furthermore, it is found that in\nthe dimensional emotion space, the distances between some of the archetypal\nemotions are closer than the others. Motivated by this, a Naive Bayes\nclassifier based decision fusion strategy is proposed, which aims at\ncapturing such useful emotion distribution information in deciding\nthe final emotion category for emotion recognition. We evaluated the\nclassification framework on the USC IEMOCAP database. Experimental\nresults demonstrate that the proposed method outperforms the hierarchical\nbinary decision tree approach on both weighted accuracy (WA) and unweighted\naccuracy (UA). Moreover, our framework possesses the advantages that\nit can be fully automatically generated without empirical guidance\nand is easier to be parallelized.\n"
      ],
      "doi": "10.21437/Interspeech.2017-619"
    },
    "sahu17_interspeech": {
      "authors": [
        [
          "Saurabh",
          "Sahu"
        ],
        [
          "Rahul",
          "Gupta"
        ],
        [
          "Ganesh",
          "Sivaraman"
        ],
        [
          "Wael",
          "AbdAlmageed"
        ],
        [
          "Carol",
          "Espy-Wilson"
        ]
      ],
      "title": "Adversarial Auto-Encoders for Speech Based Emotion Recognition",
      "original": "1421",
      "page_count": 5,
      "order": 260,
      "p1": "1243",
      "pn": "1247",
      "abstract": [
        "Recently, generative adversarial networks and adversarial auto-encoders\nhave gained a lot of attention in machine learning community due to\ntheir exceptional performance in tasks such as digit classification\nand face recognition. They map the auto-encoder&#8217;s bottleneck\nlayer output (termed as code vectors) to different noise Probability\nDistribution Functions (PDFs), that can be further regularized to cluster\nbased on class information. In addition, they also allow a generation\nof synthetic samples by sampling the code vectors from the mapped PDFs.\nInspired by these properties, we investigate the application of adversarial\nauto-encoders to the domain of emotion recognition. Specifically, we\nconduct experiments on the following two aspects: (i) their ability\nto encode high dimensional feature vector representations for emotional\nutterances into a compressed space (with a minimal loss of emotion\nclass discriminability in the compressed space), and (ii) their ability\nto regenerate synthetic samples in the original feature space, to be\nlater used for purposes such as training emotion recognition classifiers.\nWe demonstrate promise of adversarial auto-encoders with regards to\nthese aspects on the Interactive Emotional Dyadic Motion Capture (IEMOCAP)\ncorpus and present our analysis.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1421"
    },
    "dang17_interspeech": {
      "authors": [
        [
          "Ting",
          "Dang"
        ],
        [
          "Vidhyasaharan",
          "Sethu"
        ],
        [
          "Julien",
          "Epps"
        ],
        [
          "Eliathamby",
          "Ambikairajah"
        ]
      ],
      "title": "An Investigation of Emotion Prediction Uncertainty Using Gaussian Mixture Regression",
      "original": "0512",
      "page_count": 5,
      "order": 261,
      "p1": "1248",
      "pn": "1252",
      "abstract": [
        "Existing continuous emotion prediction systems implicitly assume that\nprediction certainty does not vary with time. However, perception differences\namong raters and other possible sources of variability suggest that\nprediction certainty varies with time, which warrants deeper consideration.\nIn this paper, the correlation between the inter-rater variability\nand the uncertainty of predicted emotion is firstly studied. A new\nparadigm that estimates the uncertainty in prediction is proposed based\non the strong correlation uncovered in the RECOLA database. This is\nimplemented by including the inter-rater variability as a representation\nof the uncertainty information in a probabilistic Gaussian Mixture\nRegression (GMR) model. In addition, we investigate the correlation\nbetween the uncertainty and the performance of a typical emotion prediction\nsystem utilizing average rating as the ground truth, by comparing the\nprediction performance in the lower and higher uncertainty regions.\nAs expected, it is observed that the performance in lower uncertainty\nregions is better than that in higher uncertainty regions, providing\na path for improving emotion prediction systems.\n"
      ],
      "doi": "10.21437/Interspeech.2017-512"
    },
    "khorram17_interspeech": {
      "authors": [
        [
          "Soheil",
          "Khorram"
        ],
        [
          "Zakaria",
          "Aldeneh"
        ],
        [
          "Dimitrios",
          "Dimitriadis"
        ],
        [
          "Melvin",
          "McInnis"
        ],
        [
          "Emily Mower",
          "Provost"
        ]
      ],
      "title": "Capturing Long-Term Temporal Dependencies with Convolutional Networks for Continuous Emotion Recognition",
      "original": "0548",
      "page_count": 5,
      "order": 262,
      "p1": "1253",
      "pn": "1257",
      "abstract": [
        "The goal of continuous emotion recognition is to assign an emotion\nvalue to every frame in a sequence of acoustic features. We show that\nincorporating long-term temporal dependencies is critical for continuous\nemotion recognition tasks. To this end, we first investigate architectures\nthat use dilated convolutions. We show that even though such architectures\noutperform previously reported systems, the output signals produced\nfrom such architectures undergo erratic changes between consecutive\ntime steps. This is inconsistent with the slow moving ground-truth\nemotion labels that are obtained from human annotators. To deal with\nthis problem, we model a downsampled version of the input signal and\nthen generate the output signal through upsampling. Not only does the\nresulting downsampling/upsampling network achieve good performance,\nit also generates smooth output trajectories. Our method yields the\nbest known audio-only performance on the RECOLA dataset.\n"
      ],
      "doi": "10.21437/Interspeech.2017-548"
    },
    "chasaide17_interspeech": {
      "authors": [
        [
          "Ailbhe N\u00ed",
          "Chasaide"
        ],
        [
          "Irena",
          "Yanushevskaya"
        ],
        [
          "Christer",
          "Gobl"
        ]
      ],
      "title": "Voice-to-Affect Mapping: Inferences on Language Voice Baseline Settings",
      "original": "1181",
      "page_count": 5,
      "order": 263,
      "p1": "1258",
      "pn": "1262",
      "abstract": [
        "Modulations of the voice convey affect, and the precise mapping of\nvoice-to-affect may vary for different languages. However, affect-related\nmodulations occur relative to the baseline affect-neutral voice, which\ntends to differ from language to language. Little is known about the\ncharacteristic long-term voice settings for different languages, and\nhow they influence the use of voice quality to signal affect. In this\npaper, data from a voice-to-affect perception test involving Russian,\nEnglish, Spanish and Japanese subjects is re-examined to glean insights\nconcerning likely baseline settings in these languages. The test used\nsynthetic stimuli with different voice qualities (modelled on a male\nvoice), with or without extreme f<SUB>0</SUB> contours as might be\nassociated with affect. Cross-language differences in affect ratings\nfor modal and tense voice suggest that the baseline in Spanish and\nJapanese is inherently tenser than in Russian and English, and that\nas a corollary, tense voice serves as a more potent cue to high-activation\naffects in the latter languages. A relatively tenser baseline in Japanese\nand Spanish is further suggested by the fact that tense voice can be\nassociated with  intimate, a low activation state, just as readily\nas with the high-activation state  interested.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1181"
    },
    "neumann17_interspeech": {
      "authors": [
        [
          "Michael",
          "Neumann"
        ],
        [
          "Ngoc Thang",
          "Vu"
        ]
      ],
      "title": "Attentive Convolutional Neural Network Based Speech Emotion Recognition: A Study on the Impact of Input Features, Signal Length, and Acted Speech",
      "original": "0917",
      "page_count": 5,
      "order": 264,
      "p1": "1263",
      "pn": "1267",
      "abstract": [
        "Speech emotion recognition is an important and challenging task in\nthe realm of human-computer interaction. Prior work proposed a variety\nof models and feature sets for training a system. In this work, we\nconduct extensive experiments using an attentive convolutional neural\nnetwork with multi-view learning objective function. We compare system\nperformance using different lengths of the input signal, different\ntypes of acoustic features and different types of emotion speech (improvised/scripted).\nOur experimental results on the Interactive Emotional Motion Capture\n(IEMOCAP) database reveal that the recognition performance strongly\ndepends on the type of speech data independent of the choice of input\nfeatures. Furthermore, we achieved state-of-the-art results on the\nimprovised speech data of IEMOCAP.\n"
      ],
      "doi": "10.21437/Interspeech.2017-917"
    },
    "miyoshi17_interspeech": {
      "authors": [
        [
          "Hiroyuki",
          "Miyoshi"
        ],
        [
          "Yuki",
          "Saito"
        ],
        [
          "Shinnosuke",
          "Takamichi"
        ],
        [
          "Hiroshi",
          "Saruwatari"
        ]
      ],
      "title": "Voice Conversion Using Sequence-to-Sequence Learning of Context Posterior Probabilities",
      "original": "0247",
      "page_count": 5,
      "order": 265,
      "p1": "1268",
      "pn": "1272",
      "abstract": [
        "Voice conversion (VC) using sequence-to-sequence learning of context\nposterior probabilities is proposed. Conventional VC using shared context\nposterior probabilities predicts target speech parameters from the\ncontext posterior probabilities estimated from the source speech parameters.\nAlthough conventional VC can be built from non-parallel data, it is\ndifficult to convert speaker individuality such as phonetic property\nand speaking rate contained in the posterior probabilities because\nthe source posterior probabilities are directly used for predicting\ntarget speech parameters. In this work, we assume that the training\ndata partly include parallel speech data and propose sequence-to-sequence\nlearning between the source and target posterior probabilities. The\nconversion models perform non-linear and variable-length transformation\nfrom the source probability sequence to the target one. Further, we\npropose a joint training algorithm for the modules. In contrast to\nconventional VC, which separately trains the speech recognition that\nestimates posterior probabilities and the speech synthesis that predicts\ntarget speech parameters, our proposed method jointly trains these\nmodules along with the proposed probability conversion modules. Experimental\nresults demonstrate that our approach outperforms the conventional\nVC.\n"
      ],
      "doi": "10.21437/Interspeech.2017-247"
    },
    "hsu17_interspeech": {
      "authors": [
        [
          "Wei-Ning",
          "Hsu"
        ],
        [
          "Yu",
          "Zhang"
        ],
        [
          "James",
          "Glass"
        ]
      ],
      "title": "Learning Latent Representations for Speech Generation and Transformation",
      "original": "0349",
      "page_count": 5,
      "order": 266,
      "p1": "1273",
      "pn": "1277",
      "abstract": [
        "An ability to model a generative process and learn a latent representation\nfor speech in an unsupervised fashion will be crucial to process vast\nquantities of unlabelled speech data. Recently, deep probabilistic\ngenerative models such as Variational Autoencoders (VAEs) have achieved\ntremendous success in modeling natural images. In this paper, we apply\na convolutional VAE to model the generative process of natural speech.\nWe derive latent space arithmetic operations to disentangle learned\nlatent representations. We demonstrate the capability of our model\nto modify the phonetic content or the speaker identity for speech segments\nusing the derived operations, without the need for parallel supervisory\ndata.\n"
      ],
      "doi": "10.21437/Interspeech.2017-349"
    },
    "hashimoto17_interspeech": {
      "authors": [
        [
          "Tetsuya",
          "Hashimoto"
        ],
        [
          "Hidetsugu",
          "Uchida"
        ],
        [
          "Daisuke",
          "Saito"
        ],
        [
          "Nobuaki",
          "Minematsu"
        ]
      ],
      "title": "Parallel-Data-Free Many-to-Many Voice Conversion Based on DNN Integrated with Eigenspace Using a Non-Parallel Speech Corpus",
      "original": "0961",
      "page_count": 5,
      "order": 267,
      "p1": "1278",
      "pn": "1282",
      "abstract": [
        "This paper proposes a novel approach to parallel-data-free and many-to-many\nvoice conversion (VC). As 1-to-1 conversion has less flexibility, researchers\nfocus on many-to-many conversion, where speaker identity is often represented\nusing speaker space bases. In this case, utterances of the same sentences\nhave to be collected from many speakers. This study aims at overcoming\nthis constraint to realize a parallel-data-free and many-to-many conversion.\nThis is made possible by integrating deep neural networks (DNNs) with\neigenspace using a non-parallel speech corpus. In our previous study,\nmany-to-many conversion was implemented using DNN, whose training was\nassisted by EVGMM conversion. By realizing the function of EVGMM equivalently\nby constructing eigenspace with a non-parallel speech corpus, the desired\nconversion is made possible. A key technique here is to estimate covariance\nterms without given parallel data between source and target speakers.\nExperiments show that objective assessment scores are comparable to\nthose of the baseline system trained with parallel data.\n"
      ],
      "doi": "10.21437/Interspeech.2017-961"
    },
    "kaneko17_interspeech": {
      "authors": [
        [
          "Takuhiro",
          "Kaneko"
        ],
        [
          "Hirokazu",
          "Kameoka"
        ],
        [
          "Kaoru",
          "Hiramatsu"
        ],
        [
          "Kunio",
          "Kashino"
        ]
      ],
      "title": "Sequence-to-Sequence Voice Conversion with Similarity Metric Learned Using Generative Adversarial Networks",
      "original": "0970",
      "page_count": 5,
      "order": 268,
      "p1": "1283",
      "pn": "1287",
      "abstract": [
        "We propose a training framework for sequence-to-sequence voice conversion\n(SVC). A well-known problem regarding a conventional VC framework is\nthat acoustic-feature sequences generated from a converter tend to\nbe over-smoothed, resulting in buzzy-sounding speech. This is because\na particular form of similarity metric or distribution for parameter\ntraining of the acoustic model is assumed so that the generated feature\nsequence that averagely fits the training target example is considered\noptimal. This over-smoothing occurs as long as a manually constructed\nsimilarity metric is used. To overcome this limitation, our proposed\nSVC framework uses a similarity metric implicitly derived from a generative\nadversarial network, enabling the measurement of the distance in the\nhigh-level abstract space. This would enable the model to mitigate\nthe over-smoothing problem caused in the low-level data space. Furthermore,\nwe use convolutional neural networks to model the long-range context-dependencies.\nThis also enables the similarity metric to have a shift-invariant property;\nthus, making the model robust against misalignment errors involved\nin the parallel data. We tested our framework on a non-native-to-native\nVC task. The experimental results revealed that the use of the proposed\nframework had a certain effect in improving naturalness, clarity, and\nspeaker individuality.\n"
      ],
      "doi": "10.21437/Interspeech.2017-970"
    },
    "ardaillon17_interspeech": {
      "authors": [
        [
          "Luc",
          "Ardaillon"
        ],
        [
          "Axel",
          "Roebel"
        ]
      ],
      "title": "A Mouth Opening Effect Based on Pole Modification for Expressive Singing Voice Transformation",
      "original": "1453",
      "page_count": 5,
      "order": 269,
      "p1": "1288",
      "pn": "1292",
      "abstract": [
        "Improving expressiveness in singing voice synthesis systems requires\nto perform realistic timbre transformations, e.g. for varying voice\nintensity. In order to sing louder, singers tend to open their mouth\nmore widely, which changes the vocal tract&#8217;s shape and resonances.\nThis study shows, by means of signal analysis and simulations, that\nthe main effect of mouth opening is an increase of the 1<SUP>st</SUP>\nformant&#8217;s frequency (F<SUB>1</SUB>) and a decrease of its bandwidth\n(BW<SUB>1</SUB>). From these observations, we then propose a rule for\nproducing a mouth opening effect, by modifying F<SUB>1</SUB> and BW<SUB>1</SUB>,\nand an approach to apply this effect on real voice sounds. This approach\nis based on pole modification, by changing the AR coefficients of an\nestimated all-pole model of the spectral envelope. Finally, listening\ntests have been conducted to evaluate the effectiveness of the proposed\neffect.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1453"
    },
    "mohammadi17_interspeech": {
      "authors": [
        [
          "Seyed Hamidreza",
          "Mohammadi"
        ],
        [
          "Alexander",
          "Kain"
        ]
      ],
      "title": "Siamese Autoencoders for Speech Style Extraction and Switching Applied to Voice Identification and Conversion",
      "original": "1434",
      "page_count": 5,
      "order": 270,
      "p1": "1293",
      "pn": "1297",
      "abstract": [
        "We propose an architecture called siamese autoencoders for extracting\nand switching pre-determined styles of speech signals while retaining\nthe content. We apply this architecture to a voice conversion task\nin which we define the content to be the linguistic message and the\nstyle to be the speaker&#8217;s voice. We assume two or more data streams\nwith the same content but unique styles. The architecture is composed\nof two or more separate but shared-weight autoencoders that are joined\nby loss functions at the hidden layers. A hidden vector is composed\nof style and content sub-vectors and the loss functions constrain the\nencodings to decompose style and content. We can select an intended\ntarget speaker either by supplying the associated style vector, or\nby extracting a new style vector from a new utterance, using a proposed\nstyle extraction algorithm. We focus on in-training speakers but perform\nsome initial experiments for out-of-training speakers as well. We propose\nand study several types of loss functions. The experiment results show\nthat the proposed many-to-many model is able to convert voices successfully;\nhowever, its performance does not surpass that of the state-of-the-art\none-to-one model&#8217;s.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1434"
    },
    "sak17_interspeech": {
      "authors": [
        [
          "Ha\u015fim",
          "Sak"
        ],
        [
          "Matt",
          "Shannon"
        ],
        [
          "Kanishka",
          "Rao"
        ],
        [
          "Fran\u00e7oise",
          "Beaufays"
        ]
      ],
      "title": "Recurrent Neural Aligner: An Encoder-Decoder Neural Network Model for Sequence to Sequence Mapping",
      "original": "1705",
      "page_count": 5,
      "order": 271,
      "p1": "1298",
      "pn": "1302",
      "abstract": [
        "We introduce an encoder-decoder recurrent neural network model called\nRecurrent Neural Aligner (RNA) that can be used for sequence to sequence\nmapping tasks. Like connectionist temporal classification (CTC) models,\nRNA defines a probability distribution over target label sequences\nincluding blank labels corresponding to each time step in input. The\nprobability of a label sequence is calculated by marginalizing over\nall possible blank label positions. Unlike CTC, RNA does not make a\nconditional independence assumption for label predictions; it uses\nthe predicted label at time t-1 as an additional input to the recurrent\nmodel when predicting the label at time t. We apply this model to end-to-end\nspeech recognition. RNA is capable of streaming recognition since the\ndecoder does not employ attention mechanism. The model is trained on\ntranscribed acoustic data to predict graphemes and no external language\nand pronunciation models are used for decoding. We employ an approximate\ndynamic programming method to optimize negative log likelihood, and\na sampling-based sequence discriminative training technique to fine-tune\nthe model to minimize expected word error rate. We show that the model\nachieves competitive accuracy without using an external language model\nnor doing beam search decoding.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1705"
    },
    "pundak17_interspeech": {
      "authors": [
        [
          "Golan",
          "Pundak"
        ],
        [
          "Tara N.",
          "Sainath"
        ]
      ],
      "title": "Highway-LSTM and Recurrent Highway Networks for Speech Recognition",
      "original": "0429",
      "page_count": 5,
      "order": 272,
      "p1": "1303",
      "pn": "1307",
      "abstract": [
        "Recently, very deep networks, with as many as hundreds of layers, have\nshown great success in image classification tasks. One key component\nthat has enabled such deep models is the use of &#8220;skip connections&#8221;,\nincluding either residual or highway connections, to alleviate the\nvanishing and exploding gradient problems. While these connections\nhave been explored for speech, they have mainly been explored for feed-forward\nnetworks. Since recurrent structures, such as LSTMs, have produced\nstate-of-the-art results on many of our Voice Search tasks, the goal\nof this work is to thoroughly investigate different approaches to adding\ndepth to recurrent structures. Specifically, we experiment with novel\nHighway-LSTM models with bottlenecks skip connections and show that\na 10 layer model can outperform a state-of-the-art 5 layer LSTM model\nwith the same number of parameters by 2% relative WER. In addition,\nwe experiment with Recurrent Highway layers and find these to be on\npar with Highway-LSTM models, when given sufficient depth.\n"
      ],
      "doi": "10.21437/Interspeech.2017-429"
    },
    "ravanelli17_interspeech": {
      "authors": [
        [
          "Mirco",
          "Ravanelli"
        ],
        [
          "Philemon",
          "Brakel"
        ],
        [
          "Maurizio",
          "Omologo"
        ],
        [
          "Yoshua",
          "Bengio"
        ]
      ],
      "title": "Improving Speech Recognition by Revising Gated Recurrent Units",
      "original": "0775",
      "page_count": 5,
      "order": 273,
      "p1": "1308",
      "pn": "1312",
      "abstract": [
        "Speech recognition is largely taking advantage of deep learning, showing\nthat substantial benefits can be obtained by modern Recurrent Neural\nNetworks (RNNs). The most popular RNNs are Long Short-Term Memory (LSTMs),\nwhich typically reach state-of-the-art performance in many tasks thanks\nto their ability to learn long-term dependencies and robustness to\nvanishing gradients. Nevertheless, LSTMs have a rather complex design\nwith three multiplicative gates, that might impair their efficient\nimplementation. An attempt to simplify LSTMs has recently led to Gated\nRecurrent Units (GRUs), which are based on just two multiplicative\ngates.<br style=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  This paper builds on these efforts by further revising GRUs and\nproposing a simplified architecture potentially more suitable for speech\nrecognition. The contribution of this work is two-fold. First, we suggest\nto remove the reset gate in the GRU design, resulting in a more efficient\nsingle-gate architecture. Second, we propose to replace tanh with ReLU\nactivations in the state update equations. Results show that, in our\nimplementation, the revised architecture reduces the per-epoch training\ntime with more than 30% and consistently improves recognition performance\nacross different tasks, input features, and noisy conditions when compared\nto a standard GRU.\n"
      ],
      "doi": "10.21437/Interspeech.2017-775"
    },
    "chien17c_interspeech": {
      "authors": [
        [
          "Jen-Tzung",
          "Chien"
        ],
        [
          "Chen",
          "Shen"
        ]
      ],
      "title": "Stochastic Recurrent Neural Network for Speech Recognition",
      "original": "0856",
      "page_count": 5,
      "order": 274,
      "p1": "1313",
      "pn": "1317",
      "abstract": [
        "This paper presents a new stochastic learning approach to construct\na latent variable model for recurrent neural network (RNN) based speech\nrecognition. A hybrid generative and discriminative stochastic network\nis implemented to build a deep classification model. In the implementation,\nwe conduct stochastic modeling for hidden states of recurrent neural\nnetwork based on the variational auto-encoder. The randomness of hidden\nneurons is represented by the Gaussian distribution with mean and variance\nparameters driven by neural weights and learned from variational inference.\nImportantly, the class labels of input speech frames are incorporated\nto regularize this deep model to sample the informative and discriminative\nfeatures for reconstruction of classification outputs. We accordingly\npropose the stochastic RNN (SRNN) to reflect the probabilistic property\nin RNN classification system. A  stochastic error backpropagation algorithm\nis implemented. The experiments on speech recognition using TIMIT and\nAurora4 show the merit of the proposed SRNN.\n"
      ],
      "doi": "10.21437/Interspeech.2017-856"
    },
    "ratajczak17_interspeech": {
      "authors": [
        [
          "Martin",
          "Ratajczak"
        ],
        [
          "Sebastian",
          "Tschiatschek"
        ],
        [
          "Franz",
          "Pernkopf"
        ]
      ],
      "title": "Frame and Segment Level Recurrent Neural Networks for Phone Classification",
      "original": "1064",
      "page_count": 5,
      "order": 275,
      "p1": "1318",
      "pn": "1322",
      "abstract": [
        "We introduce a simple and efficient frame and segment level RNN model\n(FS-RNN) for phone classification. It processes the input at  frame\nlevel and  segment level by bidirectional gated RNNs. This type of\nprocessing is important to exploit the (temporal) information more\neffectively compared to  (i) models which solely process the input\nat frame level and  (ii) models which process the input on segment\nlevel using features obtained by heuristic aggregation of frame level\nfeatures. Furthermore, we incorporated the activations of the last\nhidden layer of the FS-RNN as an additional feature type in a neural\nhigher-order CRF (NHO-CRF). In experiments, we demonstrated excellent\nperformance on the TIMIT phone classification task, reporting a performance\nof 13.8% phone error rate for the FS-RNN model and 11.9% when combined\nwith the NHO-CRF. In both cases we significantly exceeded the state-of-the-art\nperformance.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1064"
    },
    "han17_interspeech": {
      "authors": [
        [
          "Kyu J.",
          "Han"
        ],
        [
          "Seongjun",
          "Hahm"
        ],
        [
          "Byung-Hak",
          "Kim"
        ],
        [
          "Jungsuk",
          "Kim"
        ],
        [
          "Ian",
          "Lane"
        ]
      ],
      "title": "Deep Learning-Based Telephony Speech Recognition in the Wild",
      "original": "1695",
      "page_count": 5,
      "order": 276,
      "p1": "1323",
      "pn": "1327",
      "abstract": [
        "In this paper, we explore the effectiveness of a variety of Deep Learning-based\nacoustic models for conversational telephony speech, specifically TDNN,\nbLSTM and CNN-bLSTM models. We evaluated these models on both research\ntestsets, such as Switchboard and CallHome, as well as recordings from\na real-world call-center application. Our best single system, consisting\nof a single CNN-bLSTM acoustic model, obtained a WER of 5.7% on the\nSwitchboard testset, and in combination with other models a WER of\n5.3% was obtained. On the CallHome testset a WER of 10.1% was achieved\nwith model combination. On the test data collected from real-world\ncall-centers, even with model adaptation using application specific\ndata, the WER was significantly higher at 15.0%. We performed an error\nanalysis on the real-world data and highlight the areas where speech\nrecognition still has challenges.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1695"
    },
    "lee17_interspeech": {
      "authors": [
        [
          "Kong Aik",
          "Lee"
        ],
        [
          "SRE\u201916 I4U",
          "Group"
        ]
      ],
      "title": "The I4U Mega Fusion and Collaboration for NIST Speaker Recognition Evaluation 2016",
      "original": "0203",
      "page_count": 5,
      "order": 277,
      "p1": "1328",
      "pn": "1332",
      "abstract": [
        "The 2016  speaker recognition evaluation (SRE&#8217;16) is the latest\nedition in the series of benchmarking events conducted by the National\nInstitute of Standards and Technology (NIST). I4U is a joint entry\nto SRE&#8217;16 as the result from the collaboration and active exchange\nof information among researchers from sixteen  Institutes and  Universities\nacross  4 continents. The joint submission and several of its 32 sub-systems\nwere among top-performing systems. A lot of efforts have been devoted\nto two major challenges, namely, unlabeled training data and dataset\nshift from  Switchboard-Mixer to the new  Call My Net dataset. This\npaper summarizes the lessons learned, presents our shared view from\nthe sixteen research groups on recent advances, major paradigm shift,\nand common tool chain used in speaker recognition as we have witnessed\nin SRE&#8217;16. More importantly, we look into the intriguing question\nof fusing a large ensemble of sub-systems and the potential benefit\nof large-scale collaboration.\n"
      ],
      "doi": "10.21437/Interspeech.2017-203"
    },
    "torrescarrasquillo17_interspeech": {
      "authors": [
        [
          "Pedro A.",
          "Torres-Carrasquillo"
        ],
        [
          "Fred",
          "Richardson"
        ],
        [
          "Shahan",
          "Nercessian"
        ],
        [
          "Douglas",
          "Sturim"
        ],
        [
          "William",
          "Campbell"
        ],
        [
          "Youngjune",
          "Gwon"
        ],
        [
          "Swaroop",
          "Vattam"
        ],
        [
          "Najim",
          "Dehak"
        ],
        [
          "Harish",
          "Mallidi"
        ],
        [
          "Phani Sankar",
          "Nidadavolu"
        ],
        [
          "Ruizhi",
          "Li"
        ],
        [
          "Reda",
          "Dehak"
        ]
      ],
      "title": "The MIT-LL, JHU and LRDE NIST 2016 Speaker Recognition Evaluation System",
      "original": "0537",
      "page_count": 5,
      "order": 278,
      "p1": "1333",
      "pn": "1337",
      "abstract": [
        "In this paper, the NIST 2016 SRE system that resulted from the collaboration\nbetween MIT Lincoln Laboratory and the team at Johns Hopkins University\nis presented. The submissions for the 2016 evaluation consisted of\nthree fixed condition submissions and a single system open condition\nsubmission. The primary submission on the fixed (and core) condition\nresulted in an actual DCF of .618. Details of the submissions are discussed\nalong with some discussion and observations of the 2016 evaluation\ncampaign.\n"
      ],
      "doi": "10.21437/Interspeech.2017-537"
    },
    "colibro17_interspeech": {
      "authors": [
        [
          "Daniele",
          "Colibro"
        ],
        [
          "Claudio",
          "Vair"
        ],
        [
          "Emanuele",
          "Dalmasso"
        ],
        [
          "Kevin",
          "Farrell"
        ],
        [
          "Gennady",
          "Karvitsky"
        ],
        [
          "Sandro",
          "Cumani"
        ],
        [
          "Pietro",
          "Laface"
        ]
      ],
      "title": "Nuance - Politecnico di Torino&#8217;s 2016 NIST Speaker Recognition Evaluation System",
      "original": "0797",
      "page_count": 5,
      "order": 279,
      "p1": "1338",
      "pn": "1342",
      "abstract": [
        "This paper describes the Nuance&#8211;Politecnico di Torino (NPT) speaker\nrecognition system submitted to the NIST SRE16 evaluation campaign.\nIncluded are the results of post-evaluation tests, focusing on the\nanalysis of the performance of generative and discriminative classifiers,\nand of score normalization. The submitted system combines the results\nof four GMM-IVector models, two DNN-IVector models and a GMM-SVM acoustic\nsystem. Each system exploits acoustic front-end parameters that differ\nby feature type and dimension. We analyze the main components of our\nsubmission, which contributed to obtaining 8.1% EER and 0.532 actual\nC<SUB>primary</SUB> in the challenging SRE16 Fixed condition.\n"
      ],
      "doi": "10.21437/Interspeech.2017-797"
    },
    "zhang17c_interspeech": {
      "authors": [
        [
          "Chunlei",
          "Zhang"
        ],
        [
          "Fahimeh",
          "Bahmaninezhad"
        ],
        [
          "Shivesh",
          "Ranjan"
        ],
        [
          "Chengzhu",
          "Yu"
        ],
        [
          "Navid",
          "Shokouhi"
        ],
        [
          "John H.L.",
          "Hansen"
        ]
      ],
      "title": "UTD-CRSS Systems for 2016 NIST Speaker Recognition Evaluation",
      "original": "0555",
      "page_count": 5,
      "order": 280,
      "p1": "1343",
      "pn": "1347",
      "abstract": [
        "This study describes systems submitted by the Center for Robust Speech\nSystems (CRSS) from the University of Texas at Dallas (UTD) to the\n2016 National Institute of Standards and Technology (NIST) Speaker\nRecognition Evaluation (SRE).We developed 4 UBM and DNN i-vector based\nspeaker recognition systems with alternate data sets and feature representations.\nGiven that the emphasis of the NIST SRE 2016 is on language mismatch\nbetween training and enrollment/test data, so-called domain mismatch,\nin our system development we focused on: (i) utilizing unlabeled in-domain\ndata for centralizing i-vectors to alleviate the domain mismatch; (ii)\nselecting the proper data sets and optimizing configurations for training\nLDA/PLDA; (iii) introducing a newly proposed dimension reduction technique\nwhich incorporates unlabeled in-domain data before PLDA training; (iv)\nunsupervised speaker clustering of unlabeled data and using them alone\nor with previous SREs for PLDA training, and finally (v) score calibration\nusing unlabeled data with &#8220;pseudo&#8221; speaker labels generated\nfrom speaker clustering. NIST evaluations show that our proposed methods\nwere very successful for the given task.\n"
      ],
      "doi": "10.21437/Interspeech.2017-555"
    },
    "plchot17_interspeech": {
      "authors": [
        [
          "Old\u0159ich",
          "Plchot"
        ],
        [
          "Pavel",
          "Mat\u011bjka"
        ],
        [
          "Anna",
          "Silnova"
        ],
        [
          "Ond\u0159ej",
          "Novotn\u00fd"
        ],
        [
          "Mireia Diez",
          "S\u00e1nchez"
        ],
        [
          "Johan",
          "Rohdin"
        ],
        [
          "Ond\u0159ej",
          "Glembek"
        ],
        [
          "Niko",
          "Br\u00fcmmer"
        ],
        [
          "Albert",
          "Swart"
        ],
        [
          "Jes\u00fas",
          "Jorr\u00edn-Prieto"
        ],
        [
          "Paola",
          "Garc\u00eda"
        ],
        [
          "Luis",
          "Buera"
        ],
        [
          "Patrick",
          "Kenny"
        ],
        [
          "Jahangir",
          "Alam"
        ],
        [
          "Gautam",
          "Bhattacharya"
        ]
      ],
      "title": "Analysis and Description of ABC Submission to NIST SRE 2016",
      "original": "1498",
      "page_count": 5,
      "order": 281,
      "p1": "1348",
      "pn": "1352",
      "abstract": [
        "We present a condensed description and analysis of the joint submission\nfor NIST SRE 2016, by Agnitio, BUT and CRIM (ABC). We concentrate on\nchallenges that arose during development and we analyze the results\nobtained on the evaluation data and on our development sets. We show\nthat testing on mismatched, non-English and short duration data introduced\nin NIST SRE 2016 is a difficult problem for current state-of-the-art\nsystems. Testing on this data brought back the issue of score normalization\nand it also revealed that the bottleneck features (BN), which are superior\nwhen used for telephone English, are lacking in performance against\nthe standard acoustic features like Mel Frequency Cepstral Coefficients\n(MFCCs). We offer ABC&#8217;s insights, findings and suggestions for\nbuilding a robust system suitable for mismatched, non-English and relatively\nnoisy data such as those in NIST SRE 2016.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1498"
    },
    "sadjadi17_interspeech": {
      "authors": [
        [
          "Seyed Omid",
          "Sadjadi"
        ],
        [
          "Timoth\u00e9e",
          "Kheyrkhah"
        ],
        [
          "Audrey",
          "Tong"
        ],
        [
          "Craig",
          "Greenberg"
        ],
        [
          "Douglas",
          "Reynolds"
        ],
        [
          "Elliot",
          "Singer"
        ],
        [
          "Lisa",
          "Mason"
        ],
        [
          "Jaime",
          "Hernandez-Cordero"
        ]
      ],
      "title": "The 2016 NIST Speaker Recognition Evaluation",
      "original": "0458",
      "page_count": 5,
      "order": 282,
      "p1": "1353",
      "pn": "1357",
      "abstract": [
        "In 2016, the National Institute of Standards and Technology (NIST)\nconducted the most recent in an ongoing series of speaker recognition\nevaluations (SRE) to foster research in robust text-independent speaker\nrecognition, as well as measure performance of current state-of-the-art\nsystems. Compared to previous NIST SREs, SRE16 introduced several new\naspects including: an entirely online evaluation platform, a  fixed\ntraining data condition, more variability in test segment duration\n(uniformly distributed between 10s and 60s), the use of non-English\n(Cantonese, Cebuano, Mandarin and Tagalog) conversational telephone\nspeech (CTS) collected outside North America, and providing labeled\nand unlabeled development (a.k.a. validation) sets for system hyperparameter\ntuning and adaptation. The introduction of the new non-English CTS\ndata made SRE16 more challenging due to domain/channel and language\nmismatches as compared to previous SREs. A total of 66 research organizations\nfrom industry and academia registered for SRE16, out of which 43 teams\nsubmitted 121 valid system outputs that produced scores. This paper\npresents an overview of the evaluation and analysis of system performance\nover all primary evaluation conditions. Initial results indicate that\neffective use of the development data was essential for the top performing\nsystems, and that domain/channel, language, and duration mismatch had\nan adverse impact on system performance.\n"
      ],
      "doi": "10.21437/Interspeech.2017-458"
    },
    "kawahara17b_interspeech": {
      "authors": [
        [
          "Hideki",
          "Kawahara"
        ],
        [
          "Ken-Ichi",
          "Sakakibara"
        ],
        [
          "Masanori",
          "Morise"
        ],
        [
          "Hideki",
          "Banno"
        ],
        [
          "Tomoki",
          "Toda"
        ],
        [
          "Toshio",
          "Irino"
        ]
      ],
      "title": "A New Cosine Series Antialiasing Function and its Application to Aliasing-Free Glottal Source Models for Speech and Singing Synthesis",
      "original": "0015",
      "page_count": 5,
      "order": 283,
      "p1": "1358",
      "pn": "1362",
      "abstract": [
        "We formulated and implemented a procedure to generate aliasing-free\nexcitation source signals. It uses a new antialiasing filter in the\ncontinuous time domain followed by an IIR digital filter for response\nequalization. We introduced a cosine-series-based general design procedure\nfor the new antialiasing function. We applied this new procedure to\nimplement the antialiased Fujisaki-Ljungqvist model. We also applied\nit to revise our previous implementation of the antialiased Fant-Liljencrants\nmodel. A combination of these signals and a lattice implementation\nof the time varying vocal tract model provides a reliable and flexible\nbasis to test f<SUB>o</SUB> extractors and source aperiodicity analysis\nmethods. MATLAB implementations of these antialiased excitation source\nmodels are available as part of our open source tools for speech science.\n"
      ],
      "doi": "10.21437/Interspeech.2017-15"
    },
    "lopez17_interspeech": {
      "authors": [
        [
          "Ana Ram\u00edrez",
          "L\u00f3pez"
        ],
        [
          "Shreyas",
          "Seshadri"
        ],
        [
          "Lauri",
          "Juvela"
        ],
        [
          "Okko",
          "R\u00e4s\u00e4nen"
        ],
        [
          "Paavo",
          "Alku"
        ]
      ],
      "title": "Speaking Style Conversion from Normal to Lombard Speech Using a Glottal Vocoder and Bayesian GMMs",
      "original": "0400",
      "page_count": 5,
      "order": 284,
      "p1": "1363",
      "pn": "1367",
      "abstract": [
        "Speaking style conversion is the technology of converting natural speech\nsignals from one style to another. In this study, we focus on normal-to-Lombard\nconversion. This can be used, for example, to enhance the intelligibility\nof speech in noisy environments. We propose a parametric approach that\nuses a vocoder to extract speech features. These features are mapped\nusing Bayesian GMMs from utterances spoken in normal style to the corresponding\nfeatures of Lombard speech. Finally, the mapped features are converted\nto a Lombard speech waveform with the vocoder. Two vocoders were compared\nin the proposed normal-to-Lombard conversion: a recently developed\nglottal vocoder that decomposes speech into glottal flow excitation\nand vocal tract, and the widely used STRAIGHT vocoder. The conversion\nquality was evaluated in two subjective listening tests measuring subjective\nsimilarity and naturalness. The similarity test results show that the\nsystem is able to convert normal speech into Lombard speech for the\ntwo vocoders. However, the subjective naturalness of the converted\nLombard speech was clearly better using the glottal vocoder in comparison\nto STRAIGHT.\n"
      ],
      "doi": "10.21437/Interspeech.2017-400"
    },
    "juvela17_interspeech": {
      "authors": [
        [
          "Lauri",
          "Juvela"
        ],
        [
          "Bajibabu",
          "Bollepalli"
        ],
        [
          "Junichi",
          "Yamagishi"
        ],
        [
          "Paavo",
          "Alku"
        ]
      ],
      "title": "Reducing Mismatch in Training of DNN-Based Glottal Excitation Models in a Statistical Parametric Text-to-Speech System",
      "original": "0848",
      "page_count": 5,
      "order": 285,
      "p1": "1368",
      "pn": "1372",
      "abstract": [
        "Neural network-based models that generate glottal excitation waveforms\nfrom acoustic features have been found to give improved quality in\nstatistical parametric speech synthesis. Until now, however, these\nmodels have been trained separately from the acoustic model. This creates\nmismatch between training and synthesis, as the synthesized acoustic\nfeatures used for the excitation model input differ from the original\ninputs, with which the model was trained on. Furthermore, due to the\nerrors in predicting the vocal tract filter, the original excitation\nwaveforms do not provide perfect reconstruction of the speech waveform\neven if predicted without error. To address these issues and to make\nthe excitation model more robust against errors in acoustic modeling,\nthis paper proposes two modifications to the excitation model training\nscheme. First, the excitation model is trained in a connected manner,\nwith inputs generated by the acoustic model. Second, the target glottal\nwaveforms are re-estimated by performing glottal inverse filtering\nwith the predicted vocal tract filters. The results show that both\nof these modifications improve performance measured in MSE and MFCC\ndistortion, and slightly improve the subjective quality of the synthetic\nspeech.\n"
      ],
      "doi": "10.21437/Interspeech.2017-848"
    },
    "sorin17_interspeech": {
      "authors": [
        [
          "Alexander",
          "Sorin"
        ],
        [
          "Slava",
          "Shechtman"
        ],
        [
          "Asaf",
          "Rendel"
        ]
      ],
      "title": "Semi Parametric Concatenative TTS with Instant Voice Modification Capabilities",
      "original": "1202",
      "page_count": 5,
      "order": 286,
      "p1": "1373",
      "pn": "1377",
      "abstract": [
        "Recently, a glottal vocoder has been integrated in the IBM concatenative\nTTS system and certain configurable global voice transformations were\ndefined in the vocoder parameter space. The vocoder analysis employs\na novel robust glottal source parameter estimation strategy. The vocoder\nis applied to the voiced speech only, while unvoiced speech is kept\nunparameterized, thus contributing to the perceived naturalness of\nthe synthesized speech.<br style=\"mso-data-placement:same-cell;\" /><br\nstyle=\"mso-data-placement:same-cell;\" />\n  The semi-parametric system\nenables independent modifications of the glottal source and vocal tract\ncomponents on-the-fly by embedding the voice transformations in the\nsynthesis process. The transformations effect ranges from slight voice\naltering to a complete change of the perceived speaker personality.\nPitch modifications enhance these changes. At the same time, the voice\ntransformations are simple enough to be easily controlled externally\nto the system. This allows the users either to fine tune the voice\nsound or to create instantly multiple distinct virtual voices. In both\ncases, the synthesis is based on a large and meticulously cleaned concatenative\nTTS voice with a broad phonetic coverage. In this paper we present\nthe system and provide subjective evaluations of its voice modification\ncapabilities.<br style=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  The technology presented in this paper is implemented in IBM Watson\nTTS service.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1202"
    },
    "manriquez17_interspeech": {
      "authors": [
        [
          "Rodrigo",
          "Manr\u00edquez"
        ],
        [
          "Sean D.",
          "Peterson"
        ],
        [
          "Pavel",
          "Prado"
        ],
        [
          "Patricio",
          "Orio"
        ],
        [
          "Mat\u00edas",
          "Za\u00f1artu"
        ]
      ],
      "title": "Modeling Laryngeal Muscle Activation Noise for Low-Order Physiological Based Speech Synthesis",
      "original": "1722",
      "page_count": 5,
      "order": 287,
      "p1": "1378",
      "pn": "1382",
      "abstract": [
        "Physiological-based synthesis using low order lumped-mass models of\nphonation have been shown to mimic and predict complex physical phenomena\nobserved in normal and pathological speech production, and have received\nsignificant attention due to their ability to efficiently perform comprehensive\nparametric investigations that are cost prohibitive with more advanced\ncomputational tools. Even though these numerical models have been shown\nto be useful research and clinical tools, several physiological aspects\nof them remain to be explored. One of the key components that has been\nneglected is the natural fluctuation of the laryngeal muscle activity\nthat affects the configuration of the model parameters. In this study,\na physiologically-based laryngeal muscle activation model that accounts\nfor random fluctuations is proposed. The method is expected to improve\nthe ability to model muscle related pathologies, such as muscle tension\ndysphonia and Parkinson&#8217;s disease. The mathematical framework\nand underlying assumptions are described, and the effects of the added\nrandom muscle activity is tested in a well-known body-cover model of\nthe vocal folds with acoustic propagation and interaction. Initial\nsimulations illustrate that the random fluctuations in the muscle activity\nimpact the resulting kinematics to varying degrees depending on the\nlaryngeal configuration.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1722"
    },
    "espic17_interspeech": {
      "authors": [
        [
          "Felipe",
          "Espic"
        ],
        [
          "Cassia Valentini",
          "Botinhao"
        ],
        [
          "Simon",
          "King"
        ]
      ],
      "title": "Direct Modelling of Magnitude and Phase Spectra for Statistical Parametric Speech Synthesis",
      "original": "1647",
      "page_count": 5,
      "order": 288,
      "p1": "1383",
      "pn": "1387",
      "abstract": [
        "We propose a simple new representation for the FFT spectrum tailored\nto statistical parametric speech synthesis. It consists of four feature\nstreams that describe magnitude, phase and fundamental frequency using\nreal numbers. The proposed feature extraction method does not attempt\nto decompose the speech structure (e.g., into source+filter or harmonics+noise).\nBy avoiding the simplifications inherent in decomposition, we can dramatically\nreduce the &#8220;phasiness&#8221; and &#8220;buzziness&#8221; typical\nof most vocoders. The method uses simple and computationally cheap\noperations and can operate at a lower frame rate than the 200 frames-per-second\ntypical in many systems. It avoids heuristics and methods requiring\napproximate or iterative solutions, including phase unwrapping.<br\nstyle=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  Two DNN-based acoustic models were built &#8212; from male and\nfemale speech data &#8212; using the Merlin toolkit. Subjective comparisons\nwere made with a state-of-the-art baseline, using the STRAIGHT vocoder.\nIn all variants tested, and for both male and female voices, the proposed\nmethod substantially outperformed the baseline. We provide source code\nto enable our complete system to be replicated.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1647"
    },
    "kember17_interspeech": {
      "authors": [
        [
          "Heather",
          "Kember"
        ],
        [
          "Ann-Kathrin",
          "Grohe"
        ],
        [
          "Katharina",
          "Zahner"
        ],
        [
          "Bettina",
          "Braun"
        ],
        [
          "Andrea",
          "Weber"
        ],
        [
          "Anne",
          "Cutler"
        ]
      ],
      "title": "Similar Prosodic Structure Perceived Differently in German and English",
      "original": "0544",
      "page_count": 5,
      "order": 289,
      "p1": "1388",
      "pn": "1392",
      "abstract": [
        "English and German have similar prosody, but their speakers realize\nsome pitch falls (not rises) in subtly different ways. We here test\nfor asymmetry in perception. An ABX discrimination task requiring F0\nslope or duration judgements on isolated vowels revealed no cross-language\ndifference in duration or F0 fall discrimination, but discrimination\nof rises (realized similarly in each language) was less accurate for\nEnglish than for German listeners. This unexpected finding may reflect\ngreater sensitivity to rising patterns by German listeners, or reduced\nsensitivity by English listeners as a result of extensive exposure\nto phrase-final rises (&#8220;uptalk&#8221;) in their language.\n"
      ],
      "doi": "10.21437/Interspeech.2017-544"
    },
    "hou17_interspeech": {
      "authors": [
        [
          "Luying",
          "Hou"
        ],
        [
          "Bert Le",
          "Bruyn"
        ],
        [
          "Ren\u00e9",
          "Kager"
        ]
      ],
      "title": "Disambiguate or not? &#8212; The Role of Prosody in Unambiguous and Potentially Ambiguous Anaphora Production in Strictly Mandarin Parallel Structures",
      "original": "1214",
      "page_count": 5,
      "order": 290,
      "p1": "1393",
      "pn": "1397",
      "abstract": [
        "It has been observed that the interpretation of pronouns can depend\non their accentuation patterns in parallel sentences as &#8220;John\nhit Bill and then George hit him&#8221;, in which &#8216;him&#8217;\nrefers to Bill when unaccented but shifts to John when accented. While\naccentuation is widely regarded as a means of disambiguation, some\nstudies have noticed that it also extends to unambiguous anaphors [7&#8211;10].\nFrom the perspective of production, however, no strong experimental\nconfirmation was found for the &#8216;shift&#8217; function of accented\npronouns, which is due to the fact that production research has mainly\nfocused on corpora [5, 6]. Hence, the nature of the accent on anaphors\nstill remains obscure. By manipulating  referential shift and  ambiguity,\nthis study explores the role of prosody in anaphora production in strictly\nMandarin parallel structures. The results reveal a significantly higher\nF<SUB>0</SUB> and longer duration for anaphors in referentially shifted\nconditions, suggesting that anaphoric accentuation signals a referential\nchange in strictly parallel structures in Mandarin. No evidence was\nfound that ambiguity plays a role in anaphoric accentuation. This finding\nchallenges the general view on accented pronouns and will deepen our\nunderstanding on semantics-prosody relationship.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1214"
    },
    "athanasopoulou17_interspeech": {
      "authors": [
        [
          "Angeliki",
          "Athanasopoulou"
        ],
        [
          "Irene",
          "Vogel"
        ],
        [
          "Hossep",
          "Dolatian"
        ]
      ],
      "title": "Acoustic Properties of Canonical and Non-Canonical Stress in French, Turkish, Armenian and Brazilian Portuguese",
      "original": "1514",
      "page_count": 5,
      "order": 291,
      "p1": "1398",
      "pn": "1402",
      "abstract": [
        "Languages are often categorized as having either predictable (fixed\nor quantity-sensitive) or non-predictable stress. Despite their name,\nfixed stress languages may have exceptions, so in fact, their stress\ndoes not always appear in the same position. Since predictability has\nbeen shown to affect certain speech phenomena, with additional or redundant\nacoustic cues being provided when the linguistic content is less predictable\n(e.g., Smooth Signal Redundancy Hypothesis), we investigate whether,\nand to what extent, the predictability of stress position affects the\nmanifestation of stress in different languages. We examine the acoustic\nproperties of stress in three languages classified as having fixed\nstress (Turkish, French, Armenian), with exceptions, and in one language\nwith non-predictable-stress, Brazilian Portuguese. Specifically, we\ncompare the manifestation of stress in the canonical stress (typically\n&#8220;fixed&#8221;) position with its manifestation in the non-canonical\n(exceptional) position, where it would potentially be less predictable.\nWe also compare these patterns with the manifestation of stress in\nPortuguese, in both the &#8220;default&#8221; penultimate and the less\ncommon final position. Our results show that stress is manifested quite\nsimilarly in canonical and non-canonical positions in the &#8220;fixed&#8221;\nstress languages and stress is most clearly produced when it is least\npredictable.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1514"
    },
    "plug17_interspeech": {
      "authors": [
        [
          "Leendert",
          "Plug"
        ],
        [
          "Rachel",
          "Smith"
        ]
      ],
      "title": "Phonological Complexity, Segment Rate and Speech Tempo Perception",
      "original": "0987",
      "page_count": 4,
      "order": 292,
      "p1": "1403",
      "pn": "1406",
      "abstract": [
        "Studies of speech tempo commonly use syllable or segment rate as a\nproxy measure for perceived tempo. In languages whose phonologies allow\nsubstantial syllable complexity these measures can produce figures\non quite different scales; however, little is known about the correlation\nbetween syllable and segment rate measurements on the one hand and\nna&#239;ve listeners&#8217; tempo judgements on the other.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  We follow up on the\nfindings of one relevant study on German [1], which suggest that listeners\nattend to both syllable and segment rates in making tempo estimates,\nthrough a weighted average of the rates in which syllable rate carries\nmore weight. We report on an experiment in which we manipulate phonological\ncomplexity in English utterance pairs that are constant in syllable\nrate. Listeners decide for each pair which utterance sounds faster.\nOur results suggest that differences in segment rate that do not correspond\nto differences in syllable rate have little impact on perceived speech\ntempo in English.\n"
      ],
      "doi": "10.21437/Interspeech.2017-987"
    },
    "yang17_interspeech": {
      "authors": [
        [
          "Jing",
          "Yang"
        ],
        [
          "Yu",
          "Zhang"
        ],
        [
          "Aijun",
          "Li"
        ],
        [
          "Li",
          "Xu"
        ]
      ],
      "title": "On the Duration of Mandarin Tones",
      "original": "0029",
      "page_count": 5,
      "order": 293,
      "p1": "1407",
      "pn": "1411",
      "abstract": [
        "The present study compared the duration of Mandarin tones in three\ntypes of speech contexts: isolated monosyllables, formal text-reading\npassages, and casual conversations. A total of 156 adult speakers was\nrecruited. The speech materials included 44 monosyllables recorded\nfrom each of 121 participants, 18 passages read by 2 participants,\nand 20 conversations conducted by 33 participants. The duration pattern\nof the four lexical tones in the isolated monosyllables was consistent\nwith the pattern described in previous literature. However, the duration\nof the four lexical tones became much shorter and tended to converge\nto that of the neutral tone (i.e., tone 0) in the text-reading and\nconversational speech. The maximum-likelihood estimator revealed that\nthe durational cue contributed to tone recognition in the isolated\nmonosyllables. With a single speaker, the average tone recognition\nbased on duration alone could reach approximately 65% correct. As the\nnumber of speakers increased (e.g., &#8805; 4), tone recognition performance\ndropped to approximately 45% correct. In conversational speech, the\nmaximum likelihood estimation of tones based on duration cues was only\n23% correct. The tone duration provided little useful cue to differentiate\nMandarin tonal identity in everyday situations.\n"
      ],
      "doi": "10.21437/Interspeech.2017-29"
    },
    "ewald17_interspeech": {
      "authors": [
        [
          "Otto",
          "Ewald"
        ],
        [
          "Eva Liina",
          "Asu"
        ],
        [
          "Susanne",
          "Sch\u00f6tz"
        ]
      ],
      "title": "The Formant Dynamics of Long Close Vowels in Three Varieties of Swedish",
      "original": "1134",
      "page_count": 5,
      "order": 294,
      "p1": "1412",
      "pn": "1416",
      "abstract": [
        "This study compares the acoustic realisation of /i&#720; y&#720; &#649;&#720;\nu&#720;/ in three varieties of Swedish: Central Swedish, Estonian Swedish,\nand Finland Swedish. Vowel tokens were extracted from isolated words\nproduced by six elderly female speakers from each variety. Trajectories\nof the first three formants were modelled with discrete cosine transform\n(DCT) coefficients, enabling the comparison of the formant means as\nwell as the direction and magnitude of the formant movement. Cross-dialectal\ndifferences were found in all measures and in all vowels. The most\nnoteworthy feature of the Estonian Swedish long close vowel inventory\nis the lack of /y&#720;/. For Finland Swedish it was shown that /i&#720;/\nand /y&#720;/ are more close than in Central Swedish. The realisation\nof /&#649;&#720;/ varies from front in Central Swedish, to central\nin Estonian Swedish, and back in Finland Swedish. On average, the Central\nSwedish vowels exhibited a higher degree of formant movement than the\nvowels in the other two varieties. In the present study, regional variation\nin Swedish vowels was for the first time investigated using DCT coefficients.\nThe results stress the importance of taking formant dynamics into account\neven in the analysis of nominal monophthongs.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1134"
    },
    "qian17_interspeech": {
      "authors": [
        [
          "Yao",
          "Qian"
        ],
        [
          "Keelan",
          "Evanini"
        ],
        [
          "Xinhao",
          "Wang"
        ],
        [
          "Chong Min",
          "Lee"
        ],
        [
          "Matthew",
          "Mulholland"
        ]
      ],
      "title": "Bidirectional LSTM-RNN for Improving Automated Assessment of Non-Native Children&#8217;s Speech",
      "original": "0250",
      "page_count": 5,
      "order": 295,
      "p1": "1417",
      "pn": "1421",
      "abstract": [
        "Recent advances in ASR and spoken language processing have led to improved\nsystems for automated assessment for spoken language. However, it is\nstill challenging for automated scoring systems to achieve high performance\nin terms of the agreement with human experts when applied to non-native\nchildren&#8217;s spontaneous speech. The subpar performance is mainly\ncaused by the relatively low recognition rate on non-native children&#8217;s\nspeech. In this paper, we investigate different neural network architectures\nfor improving non-native children&#8217;s speech recognition and the\nimpact of the features extracted from the corresponding ASR output\non the automated assessment of speaking proficiency. Experimental results\nshow that bidirectional LSTM-RNN can outperform feed-forward DNN in\nASR, with an overall relative WER reduction of 13.4%. The improved\nspeech recognition can then boost the language proficiency assessment\nperformance. Correlations between the rounded automated scores and\nexpert scores range from 0.66 to 0.70 for the three speaking tasks\nstudied, similar to the human-human agreement levels for these tasks.\n"
      ],
      "doi": "10.21437/Interspeech.2017-250"
    },
    "yue17_interspeech": {
      "authors": [
        [
          "Junwei",
          "Yue"
        ],
        [
          "Fumiya",
          "Shiozawa"
        ],
        [
          "Shohei",
          "Toyama"
        ],
        [
          "Yutaka",
          "Yamauchi"
        ],
        [
          "Kayoko",
          "Ito"
        ],
        [
          "Daisuke",
          "Saito"
        ],
        [
          "Nobuaki",
          "Minematsu"
        ]
      ],
      "title": "Automatic Scoring of Shadowing Speech Based on DNN Posteriors and Their DTW",
      "original": "0728",
      "page_count": 5,
      "order": 296,
      "p1": "1422",
      "pn": "1426",
      "abstract": [
        "Shadowing has become a well-known method to improve learners&#8217;\noverall proficiency. Our previous studies realized automatic scoring\nof shadowing speech using HMM phoneme posteriors, called GOP (Goodness\nof Pronunciation) and learners&#8217; TOEIC scores were predicted adequately.\nIn this study, we enhance our studies from multiple angles: 1) a much\nlarger amount of shadowing speech is collected, 2) manual scoring of\nthese utterances is done by two native teachers, 3) DNN posteriors\nare introduced instead of HMM ones, 4) language-independent shadowing\nassessment based on posteriors-based DTW (Dynamic Time Warping) is\nexamined. Experiments suggest that, compared to HMM, DNN can improve\nteacher-machine correlation largely by 0.37 and DTW based on DNN posteriors\nshows as high correlation as 0.74 even when posterior calculation is\ndone using a different language from the target language of learning.\n"
      ],
      "doi": "10.21437/Interspeech.2017-728"
    },
    "lee17b_interspeech": {
      "authors": [
        [
          "Chong Min",
          "Lee"
        ],
        [
          "Su-Youn",
          "Yoon"
        ],
        [
          "Xihao",
          "Wang"
        ],
        [
          "Matthew",
          "Mulholland"
        ],
        [
          "Ikkyu",
          "Choi"
        ],
        [
          "Keelan",
          "Evanini"
        ]
      ],
      "title": "Off-Topic Spoken Response Detection Using Siamese Convolutional Neural Networks",
      "original": "1174",
      "page_count": 5,
      "order": 297,
      "p1": "1427",
      "pn": "1431",
      "abstract": [
        "In this study, we developed an off-topic response detection system\nto be used in the context of the automated scoring of non-native English\nspeakers&#8217; spontaneous speech. Based on transcriptions generated\nfrom an ASR system trained on non-native speakers&#8217; speech and\nvarious semantic similarity features, the system classified each test\nresponse as an on-topic or off-topic response. The recent success of\ndeep neural networks (DNN) in text similarity detection led us to explore\nDNN-based document similarity features. Specifically, we used a siamese\nadaptation of the convolutional network, due to its efficiency in learning\nsimilarity patterns simultaneously from both responses and questions\nused to elicit responses. In addition, a baseline system was developed\nusing a standard vector space model (VSM) trained on sample responses\nfor each question. The accuracy of the siamese CNN-based system was\n0.97 and there was a 50% relative error reduction compared to the standard\nVSM-based system. Furthermore, the accuracy of the siamese CNN-based\nsystem was consistent across different questions.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1174"
    },
    "arora17_interspeech": {
      "authors": [
        [
          "Vipul",
          "Arora"
        ],
        [
          "Aditi",
          "Lahiri"
        ],
        [
          "Henning",
          "Reetz"
        ]
      ],
      "title": "Phonological Feature Based Mispronunciation Detection and Diagnosis Using Multi-Task DNNs and Active Learning",
      "original": "1350",
      "page_count": 5,
      "order": 298,
      "p1": "1432",
      "pn": "1436",
      "abstract": [
        "This paper presents a phonological feature based computer aided pronunciation\ntraining system for the learners of a new language (L2). Phonological\nfeatures allow analysing the learners&#8217; mispronunciations systematically\nand rendering the feedback more effectively. The proposed acoustic\nmodel consists of a multi-task deep neural network, which uses a shared\nrepresentation for estimating the phonological features and HMM state\nprobabilities. Moreover, an active learning based scheme is proposed\nto efficiently deal with the cost of annotation, which is done by expert\nteachers, by selecting the most informative samples for annotation.\nExperimental evaluations are carried out for German and Italian native-speakers\nspeaking English. For mispronunciation detection, the proposed feature-based\nsystem outperforms conventional GOP measure and classifier based methods,\nwhile providing more detailed diagnosis. Evaluations also demonstrate\nthe advantage of active learning based sampling over random sampling.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1350"
    },
    "proenca17_interspeech": {
      "authors": [
        [
          "Jorge",
          "Proen\u00e7a"
        ],
        [
          "Carla",
          "Lopes"
        ],
        [
          "Michael",
          "Tjalve"
        ],
        [
          "Andreas",
          "Stolcke"
        ],
        [
          "Sara",
          "Candeias"
        ],
        [
          "Fernando",
          "Perdig\u00e3o"
        ]
      ],
      "title": "Detection of Mispronunciations and Disfluencies in Children Reading Aloud",
      "original": "1522",
      "page_count": 5,
      "order": 299,
      "p1": "1437",
      "pn": "1441",
      "abstract": [
        "To automatically evaluate the performance of children reading aloud\nor to follow a child&#8217;s reading in reading tutor applications,\ndifferent types of reading disfluencies and mispronunciations must\nbe accounted for. In this work, we aim to detect most of these disfluencies\nin sentence and pseudoword reading. Detecting incorrectly pronounced\nwords, and quantifying the quality of word pronunciations, is arguably\nthe hardest task. We approach the challenge as a two-step process.\nFirst, a segmentation using task-specific lattices is performed, while\ndetecting repetitions and false starts and providing candidate segments\nfor words. Then, candidates are classified as mispronounced or not,\nusing multiple features derived from likelihood ratios based on phone\ndecoding and forced alignment, as well as additional meta-information\nabout the word. Several classifiers were explored (linear fit, neural\nnetworks, support vector machines) and trained after a feature selection\nstage to avoid overfitting. Improved results are obtained using feature\ncombination compared to using only the log likelihood ratio of the\nreference word (22% versus 27% miss rate at constant 5% false alarm\nrate).\n"
      ],
      "doi": "10.21437/Interspeech.2017-1522"
    },
    "escuderomancebo17_interspeech": {
      "authors": [
        [
          "David",
          "Escudero-Mancebo"
        ],
        [
          "C\u00e9sar",
          "Gonz\u00e1lez-Ferreras"
        ],
        [
          "Lourdes",
          "Aguilar"
        ],
        [
          "Eva",
          "Estebas-Vilaplana"
        ]
      ],
      "title": "Automatic Assessment of Non-Native Prosody by Measuring Distances on Prosodic Label Sequences",
      "original": "0366",
      "page_count": 5,
      "order": 300,
      "p1": "1442",
      "pn": "1446",
      "abstract": [
        "The aim of this paper is to investigate how automatic prosodic labeling\nsystems contribute to the evaluation of non-native pronunciation. In\nparticular, it examines the efficiency of a group of metrics to evaluate\nthe prosodic competence of non-native speakers, based on the information\nprovided by sequences of labels in the analysis of both native and\nnon-native speech. A group of Sp_ToBI labels were obtained by means\nof an automatic labeling system for the speech of native and non-native\nspeakers who read the same texts. The metrics assessed the differences\nin the prosodic labels for both speech samples. The results showed\nthe efficiency of the metrics to set apart both groups of speakers.\nFurthermore, they exhibited how non-native speakers (American and Japanese\nspeakers) improved their Spanish productions after doing a set of listening\nand repeating activities. Finally, this study also shows that the results\nprovided by the metrics are correlated with the scores given by human\nevaluators on the productions of the different speakers.\n"
      ],
      "doi": "10.21437/Interspeech.2017-366"
    },
    "ward17_interspeech": {
      "authors": [
        [
          "Nigel G.",
          "Ward"
        ],
        [
          "Jason C.",
          "Carlson"
        ],
        [
          "Olac",
          "Fuentes"
        ],
        [
          "Diego",
          "Castan"
        ],
        [
          "Elizabeth E.",
          "Shriberg"
        ],
        [
          "Andreas",
          "Tsiartas"
        ]
      ],
      "title": "Inferring Stance from Prosody",
      "original": "0159",
      "page_count": 5,
      "order": 301,
      "p1": "1447",
      "pn": "1451",
      "abstract": [
        "Speech conveys many things beyond content, including aspects of stance\nand attitude that have not been much studied. Considering 14 aspects\nof stance as they occur in radio news stories, we investigated the\nextent to which they could be inferred from prosody. By using time-spread\nprosodic features and by aggregating local estimates, many aspects\nof stance were at least somewhat predictable, with results significantly\nbetter than chance for many stance aspects, including, across English,\nMandarin and Turkish, good, typical, local, background, new information,\nand relevant to a large group.\n"
      ],
      "doi": "10.21437/Interspeech.2017-159"
    },
    "levow17_interspeech": {
      "authors": [
        [
          "Gina-Anne",
          "Levow"
        ],
        [
          "Richard A.",
          "Wright"
        ]
      ],
      "title": "Exploring Dynamic Measures of Stance in Spoken Interaction",
      "original": "1706",
      "page_count": 5,
      "order": 302,
      "p1": "1452",
      "pn": "1456",
      "abstract": [
        "Stance-taking, the expression of opinions or attitudes, informs the\nprocess of negotiation, argumentation, and decision-making. While receiving\nsignificant attention in text materials in work on the related areas\nof subjectivity and sentiment analysis, the expression of stance in\nspeech remains less explored. Prior analysis of the acoustics of stance-expression\nin conversational speech has identified some significant differences\nacross dimensions of stance-related behavior. However, that analysis,\nas in much prior work, relied on simple functionals of pitch, energy,\nand duration, including maxima, minima, means, and ranges. In contrast,\nthe current work focuses on exploiting measures that capture the dynamics\nof the pitch and energy contour. We employ features based on subband\nautocorrelation measures of pitch change and variants of the modulation\nspectrum. Using a corpus of conversational speech manually annotated\nfor dimensions of stance-taking, we demonstrate that these measures\nof pitch and energy dynamics can help to characterize and distinguish\namong stance-related behaviors in speech.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1706"
    },
    "barriere17_interspeech": {
      "authors": [
        [
          "Valentin",
          "Barriere"
        ],
        [
          "Chlo\u00e9",
          "Clavel"
        ],
        [
          "Slim",
          "Essid"
        ]
      ],
      "title": "Opinion Dynamics Modeling for Movie Review Transcripts Classification with Hidden Conditional Random Fields",
      "original": "1035",
      "page_count": 5,
      "order": 303,
      "p1": "1457",
      "pn": "1461",
      "abstract": [
        "In this paper, the main goal is to detect a movie reviewer&#8217;s\nopinion using hidden conditional random fields. This model allows us\nto capture the dynamics of the reviewer&#8217;s opinion in the transcripts\nof long unsegmented audio reviews that are analyzed by our system.\nHigh level linguistic features are computed at the level of inter-pausal\nsegments. The features include syntactic features, a statistical word\nembedding model and subjectivity lexicons. The proposed system is evaluated\non the ICT-MMMO corpus. We obtain a F1-score of 82%, which is better\nthan logistic regression and recurrent neural network approaches. We\nalso offer a discussion that sheds some light on the capacity of our\nsystem to adapt the word embedding model learned from general written\ntexts data to spoken movie reviews and thus model the dynamics of the\nopinion.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1035"
    },
    "luo17_interspeech": {
      "authors": [
        [
          "Qinyi",
          "Luo"
        ],
        [
          "Rahul",
          "Gupta"
        ],
        [
          "Shrikanth S.",
          "Narayanan"
        ]
      ],
      "title": "Transfer Learning Between Concepts for Human Behavior Modeling: An Application to Sincerity and Deception Prediction",
      "original": "0121",
      "page_count": 5,
      "order": 304,
      "p1": "1462",
      "pn": "1466",
      "abstract": [
        "Transfer learning (TL) involves leveraging information from sources\noutside the domain at hand for enhancing model performances. Popular\nTL methods either directly use the data or adapt the models learned\non out-of-domain resources and incorporate them within in-domain models.\nTL methods have shown promise in several applications such as text\nclassification, cross-domain language classification and emotion recognition.\nIn this paper, we propose TL methods to computational human behavioral\ntrait modeling. Many behavioral traits are abstract constructs (e.g.,\nsincerity of an individual), and are often conceptually related to\nother constructs (e.g., level of deception) making TL methods an attractive\noption for their modeling. We consider the problem of automatically\npredicting human sincerity and deception from behavioral data while\nleveraging transfer of knowledge from each other. We compare our methods\nagainst baseline models trained only on in-domain data. Our best models\nachieve an Unweighted Average Recall (UAR) of 72.02% in classifying\ndeception (baseline: 69.64%). Similarly, applied methods achieve Spearman&#8217;s/Pearson&#8217;s\ncorrelation values of 49.37%/48.52% between true and predicted sincerity\nscores (baseline: 46.51%/41.58%), indicating the success and the potential\nof TL for such human behavior tasks.\n"
      ],
      "doi": "10.21437/Interspeech.2017-121"
    },
    "schroder17_interspeech": {
      "authors": [
        [
          "Anne",
          "Schr\u00f6der"
        ],
        [
          "Simon",
          "Stone"
        ],
        [
          "Peter",
          "Birkholz"
        ]
      ],
      "title": "The Sound of Deception &#8212; What Makes a Speaker Credible?",
      "original": "0384",
      "page_count": 5,
      "order": 305,
      "p1": "1467",
      "pn": "1471",
      "abstract": [
        "The detection of deception in human speech is a difficult task but\ncan be performed above chance level by human listeners even when only\naudio data is provided. Still, it is highly contested, which speech\nfeatures could be used to help identify lies. In this study, we examined\na set of phonetic and paralinguistic cues and their influence on the\ncredibility of speech using an analysis-by-synthesis approach. 33 linguistically\nneutral utterances with different manipulated cues (unfilled pauses,\nphonation type, higher speech rate, tremolo and raised F0) were synthesized\nusing articulatory synthesis. These utterances were presented to 50\nsubjects who were asked to choose the more credible utterance. From\nthose choices, a credibility score was calculated for each cue. The\nresults show a significant increase in credibility when a tremolo is\ninserted or the breathiness is increased, and a decrease in credibility\nwhen a pause is inserted or the F0 is raised. Other cues also had a\nsignificant, but less pronounced influence on the credibility while\nsome only showed trends. In summary, the study showed that the credibility\nof a factually unverifiable utterance is in parts controlled by the\npresented paralinguistic cues.\n"
      ],
      "doi": "10.21437/Interspeech.2017-384"
    },
    "mendels17_interspeech": {
      "authors": [
        [
          "Gideon",
          "Mendels"
        ],
        [
          "Sarah Ita",
          "Levitan"
        ],
        [
          "Kai-Zhan",
          "Lee"
        ],
        [
          "Julia",
          "Hirschberg"
        ]
      ],
      "title": "Hybrid Acoustic-Lexical Deep Learning Approach for Deception Detection",
      "original": "1723",
      "page_count": 5,
      "order": 306,
      "p1": "1472",
      "pn": "1476",
      "abstract": [
        "Automatic deception detection is an important problem with far-reaching\nimplications for many disciplines. We present a series of experiments\naimed at automatically detecting deception from speech. We use the\nColumbia X-Cultural Deception (CXD) Corpus, a large-scale corpus of\nwithin-subject deceptive and non-deceptive speech, for training and\nevaluating our models. We compare the use of spectral, acoustic-prosodic,\nand lexical feature sets, using different machine learning models.\nFinally, we design a single hybrid deep model with both acoustic and\nlexical features trained jointly that achieves state-of-the-art results\non the CXD corpus.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1723"
    },
    "swart17_interspeech": {
      "authors": [
        [
          "Albert",
          "Swart"
        ],
        [
          "Niko",
          "Br\u00fcmmer"
        ]
      ],
      "title": "A Generative Model for Score Normalization in Speaker Recognition",
      "original": "0137",
      "page_count": 5,
      "order": 307,
      "p1": "1477",
      "pn": "1481",
      "abstract": [
        "We propose a theoretical framework for thinking about score normalization,\nwhich confirms that normalization is not needed under (admittedly fragile)\nideal conditions. If, however, these conditions are not met, e.g. under\ndata-set shift between training and runtime, our theory reveals dependencies\nbetween scores that could be exploited by strategies such as score\nnormalization. Indeed, it has been demonstrated over and over experimentally,\nthat various ad-hoc score normalization recipes do work. We present\na first attempt at using probability theory to design a generative\nscore-space normalization model which gives similar improvements to\nZT-norm on the text-dependent RSR 2015 database.\n"
      ],
      "doi": "10.21437/Interspeech.2017-137"
    },
    "dey17_interspeech": {
      "authors": [
        [
          "Subhadeep",
          "Dey"
        ],
        [
          "Srikanth",
          "Madikeri"
        ],
        [
          "Petr",
          "Motlicek"
        ],
        [
          "Marc",
          "Ferras"
        ]
      ],
      "title": "Content Normalization for Text-Dependent Speaker Verification",
      "original": "1419",
      "page_count": 5,
      "order": 308,
      "p1": "1482",
      "pn": "1486",
      "abstract": [
        "Subspace based techniques, such as i-vector and Joint Factor Analysis\n(JFA) have shown to provide state-of-the-art performance for fixed\nphrase based text-dependent speaker verification. However, the error\nrates of such systems on the random digit task of RSR dataset are higher\nthan that of Gaussian Mixture Model-Universal Background Model (GMM-UBM).\nIn this paper, we aim at improving i-vector system by normalizing the\ncontent of the enrollment data to match the test data. We estimate\ni-vectors for each frames of a speech utterance (also called online\ni-vectors). The largest similarity scores across frames between enrollment\nand test are taken using these online i-vectors to obtain speaker verification\nscores. Experiments on Part3 of RSR corpora show that the proposed\napproach achieves 12% relative improvement in equal error rate over\na GMM-UBM based baseline system.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1419"
    },
    "zhang17d_interspeech": {
      "authors": [
        [
          "Chunlei",
          "Zhang"
        ],
        [
          "Kazuhito",
          "Koishida"
        ]
      ],
      "title": "End-to-End Text-Independent Speaker Verification with Triplet Loss on Short Utterances",
      "original": "1608",
      "page_count": 5,
      "order": 309,
      "p1": "1487",
      "pn": "1491",
      "abstract": [
        "Text-independent speaker verification against short utterances is still\nchallenging despite of recent advances in the field of speaker recognition\nwith i-vector framework. In general, to get a robust i-vector representation,\na satisfying amount of data is needed in the MAP adaptation step, which\nis hard to meet under short duration constraint. To overcome this,\nwe present an end-to-end system which directly learns a mapping from\nspeech features to a compact fixed length speaker discriminative embedding\nwhere the Euclidean distance is employed for measuring similarity within\ntrials. To learn the feature mapping, a modified Inception Net with\nresidual block is proposed to optimize the triplet loss function. The\ninput of our end-to-end system is a fixed length spectrogram converted\nfrom an arbitrary length utterance. Experiments show that our system\nconsistently outperforms a conventional i-vector system on short duration\nspeaker verification tasks. To test the limit under various duration\nconditions, we also demonstrate how our end-to-end system behaves with\ndifferent duration from 2s&#8211;4s.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1608"
    },
    "yu17_interspeech": {
      "authors": [
        [
          "Hong",
          "Yu"
        ],
        [
          "Zheng-Hua",
          "Tan"
        ],
        [
          "Zhanyu",
          "Ma"
        ],
        [
          "Jun",
          "Guo"
        ]
      ],
      "title": "Adversarial Network Bottleneck Features for Noise Robust Speaker Verification",
      "original": "0883",
      "page_count": 5,
      "order": 310,
      "p1": "1492",
      "pn": "1496",
      "abstract": [
        "In this paper, we propose a noise robust bottleneck feature representation\nwhich is generated by an adversarial network (AN). The AN includes\ntwo cascade connected networks, an encoding network (EN) and a discriminative\nnetwork (DN). Mel-frequency cepstral coefficients (MFCCs) of clean\nand noisy speech are used as input to the EN and the output of the\nEN is used as the noise robust feature. The EN and DN are trained in\nturn, namely, when training the DN, noise types are selected as the\ntraining labels and when training the EN, all labels are set as the\nsame, i.e., the clean speech label, which aims to make the AN features\ninvariant to noise and thus achieve noise robustness. We evaluate the\nperformance of the proposed feature on a Gaussian Mixture Model-Universal\nBackground Model based speaker verification system, and make comparison\nto MFCC features of speech enhanced by short-time spectral amplitude\nminimum mean square error (STSA-MMSE) and deep neural network-based\nspeech enhancement (DNN-SE) methods. Experimental results on the RSR2015\ndatabase show that the proposed AN bottleneck feature (AN-BN) dramatically\noutperforms the STSA-MMSE and DNN-SE based MFCCs for different noise\ntypes and signal-to-noise ratios. Furthermore, the AN-BN feature is\nable to improve the speaker verification performance under the clean\ncondition.\n"
      ],
      "doi": "10.21437/Interspeech.2017-883"
    },
    "wang17g_interspeech": {
      "authors": [
        [
          "Shuai",
          "Wang"
        ],
        [
          "Yanmin",
          "Qian"
        ],
        [
          "Kai",
          "Yu"
        ]
      ],
      "title": "What Does the Speaker Embedding Encode?",
      "original": "1125",
      "page_count": 5,
      "order": 311,
      "p1": "1497",
      "pn": "1501",
      "abstract": [
        "Developing a good speaker embedding has received tremendous interest\nin the speech community. Speaker representations such as i-vector,\nd-vector have shown their superiority in speaker recognition, speaker\nadaptation and other related tasks. However, not much is known about\nwhich properties are exactly encoded in these speaker embeddings. In\nthis work, we make an in-depth investigation on three kinds of speaker\nembeddings, i.e. i-vector, d-vector and RNN/LSTM based sequence-vector\n(s-vector). Classification tasks are carefully designed to facilitate\nbetter understanding of these encoded speaker representations. Their\nabilities of encoding different properties are revealed and compared,\nsuch as speaker identity, gender, speaking rate, text content and channel\ninformation. Moreover, a new architecture is proposed to integrate\ndifferent speaker embeddings, so that the advantages can be combined.\nThe new advanced speaker embedding (i-s-vector) outperforms the others,\nand shows a more than 50% EER reduction compared to the i-vector baseline\non the RSR2015 content mismatch trials.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1125"
    },
    "ma17d_interspeech": {
      "authors": [
        [
          "Jianbo",
          "Ma"
        ],
        [
          "Vidhyasaharan",
          "Sethu"
        ],
        [
          "Eliathamby",
          "Ambikairajah"
        ],
        [
          "Kong Aik",
          "Lee"
        ]
      ],
      "title": "Incorporating Local Acoustic Variability Information into Short Duration Speaker Verification",
      "original": "0266",
      "page_count": 5,
      "order": 312,
      "p1": "1502",
      "pn": "1506",
      "abstract": [
        "State-of-the-art speaker verification systems are based on the total\nvariability model to compactly represent the acoustic space. However,\nshort duration utterances only contain limited phonetic content, potentially\nresulting in an incomplete representation being captured by the total\nvariability model thus leading to poor speaker verification performance.\nIn this paper, a technique to incorporate component-wise local acoustic\nvariability information into the speaker verification framework is\nproposed. Specifically, Gaussian Probabilistic Linear Discriminant\nAnalysis (G-PLDA) of the supervector space, with a block diagonal covariance\nassumption, is used in conjunction with the traditional total variability\nmodel. Experimental results obtained using the NIST SRE 2010 dataset\nshow that the incorporation of the proposed method leads to relative\nimprovements of 20.48% and 18.99% in the 3 second condition for male\nand female speech respectively.\n"
      ],
      "doi": "10.21437/Interspeech.2017-266"
    },
    "zhong17_interspeech": {
      "authors": [
        [
          "Jinghua",
          "Zhong"
        ],
        [
          "Wenping",
          "Hu"
        ],
        [
          "Frank K.",
          "Soong"
        ],
        [
          "Helen",
          "Meng"
        ]
      ],
      "title": "DNN i-Vector Speaker Verification with Short, Text-Constrained Test Utterances",
      "original": "1036",
      "page_count": 5,
      "order": 313,
      "p1": "1507",
      "pn": "1511",
      "abstract": [
        "We investigate how to improve the performance of DNN i-vector based\nspeaker verification for short, text-constrained test utterances, e.g.\nconnected digit strings. A text-constrained verification, due to its\nsmaller, limited vocabulary, can deliver better performance than a\ntext-independent one for a short utterance. We study the problem with\n&#8220;phonetically aware&#8221; Deep Neural Net (DNN) in its capability\non &#8220;stochastic phonetic-alignment&#8221; in constructing supervectors\nand estimating the corresponding i-vectors with two speech databases:\na large vocabulary, conversational, speaker independent database (Fisher)\nand a small vocabulary, continuous digit database (RSR2015 Part III).\nThe phonetic alignment efficiency and resultant speaker verification\nperformance are compared with differently sized senone sets which can\ncharacterize the phonetic pronunciations of utterances in the two databases.\nPerformance on RSR2015 Part III evaluation shows a relative improvement\nof EER, i.e., 7.89% for male speakers and 3.54% for female speakers\nwith only digit related senones. The DNN bottleneck features were also\nstudied to investigate their capability of extracting phonetic sensitive\ninformation which is useful for text-independent or text-constrained\nspeaker verifications. We found that by tandeming MFCC with bottleneck\nfeatures, EERs can be further reduced.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1036"
    },
    "vestman17_interspeech": {
      "authors": [
        [
          "Ville",
          "Vestman"
        ],
        [
          "Dhananjaya",
          "Gowda"
        ],
        [
          "Md.",
          "Sahidullah"
        ],
        [
          "Paavo",
          "Alku"
        ],
        [
          "Tomi",
          "Kinnunen"
        ]
      ],
      "title": "Time-Varying Autoregressions for Speaker Verification in Reverberant Conditions",
      "original": "0734",
      "page_count": 5,
      "order": 314,
      "p1": "1512",
      "pn": "1516",
      "abstract": [
        "In poor room acoustics conditions, speech signals received by a microphone\nmight become corrupted by the signals&#8217; delayed versions that\nare reflected from the room surfaces (e.g. wall, floor). This phenomenon,\nreverberation, drops the accuracy of automatic speaker verification\nsystems by causing mismatch between the training and testing. Since\nreverberation causes temporal smearing to the signal, one way to tackle\nits effects is to study robust feature extraction, particularly based\non long-time temporal feature extraction. This approach has been adopted\npreviously in the form of 2-dimensional autoregressive (2DAR) feature\nextraction scheme by using frequency domain linear prediction (FDLP).\nIn 2DAR, FDLP processing is followed by time domain linear prediction\n(TDLP). In the current study, we propose modifying the latter part\nof the 2DAR feature extraction scheme by replacing TDLP with time-varying\nlinear prediction (TVLP) to add an extra layer of temporal processing.\nOur speaker verification experiments using the proposed features with\nthe text-dependent RedDots corpus show small but consistent improvements\nin clean and reverberant conditions (up to 6.5%) over the 2DAR features\nand large improvements over the MFCC features in reverberant conditions\n(up to 46.5%).\n"
      ],
      "doi": "10.21437/Interspeech.2017-734"
    },
    "bhattacharya17_interspeech": {
      "authors": [
        [
          "Gautam",
          "Bhattacharya"
        ],
        [
          "Jahangir",
          "Alam"
        ],
        [
          "Patrick",
          "Kenny"
        ]
      ],
      "title": "Deep Speaker Embeddings for Short-Duration Speaker Verification",
      "original": "1575",
      "page_count": 5,
      "order": 315,
      "p1": "1517",
      "pn": "1521",
      "abstract": [
        "The performance of a state-of-the-art speaker verification system is\nseverely degraded when it is presented with trial recordings of short\nduration. In this work we propose to use deep neural networks to learn\nshort-duration speaker embeddings. We focus on the 5s-5s condition,\nwherein both sides of a verification trial are 5 seconds long. In our\nprevious work we established that learning a non-linear mapping from\ni-vectors to speaker labels is beneficial for speaker verification\n[1]. In this work we take the idea of learning a speaker classifier\none step further &#8212; we apply deep neural networks directly to\ntime-frequency speech representations. We propose two feed-forward\nnetwork architectures for this task. Our best model is based on a deep\nconvolutional architecture wherein recordings are treated as images.\nFrom our experimental findings we advocate treating utterances as images\nor &#8216;speaker snapshots&#8217;, much like in face recognition.\nOur convolutional speaker embeddings perform significantly better than\ni-vectors when scoring is done using cosine distance, where the relative\nimprovement is 23.5%. The proposed deep embeddings combined with cosine\ndistance also outperform a state-of-the-art i-vector verification system\nby 1%, providing further empirical evidence in favor of our learned\nspeaker features.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1575"
    },
    "park17b_interspeech": {
      "authors": [
        [
          "Soo Jin",
          "Park"
        ],
        [
          "Gary",
          "Yeung"
        ],
        [
          "Jody",
          "Kreiman"
        ],
        [
          "Patricia A.",
          "Keating"
        ],
        [
          "Abeer",
          "Alwan"
        ]
      ],
      "title": "Using Voice Quality Features to Improve Short-Utterance, Text-Independent Speaker Verification Systems",
      "original": "0157",
      "page_count": 5,
      "order": 316,
      "p1": "1522",
      "pn": "1526",
      "abstract": [
        "Due to within-speaker variability in phonetic content and/or speaking\nstyle, the performance of automatic speaker verification (ASV) systems\ndegrades especially when the enrollment and test utterances are short.\nThis study examines how different types of variability influence performance\nof ASV systems. Speech samples (&#60; 2 sec) from the UCLA Speaker\nVariability Database containing 5 different read sentences by 200 speakers\nwere used to study content variability. Other samples (about 5 sec)\nthat contained speech directed towards pets, characterized by exaggerated\nprosody, were used to analyze style variability. Using the i-vector/PLDA\nframework, the ASV system error rate with MFCCs had a relative increase\nof at least 265% and 730% in content-mismatched and style-mismatched\ntrials, respectively. A set of features that represents voice quality\n(F0, F1, F2, F3, H1-H2, H2-H4, H4-H2k, A1, A2, A3, and CPP) was also\nused. Using score fusion with MFCCs, all conditions saw decreases in\nerror rates. In addition, using the NIST SRE10 database, score fusion\nprovided relative improvements of 11.78% for 5-second utterances, 12.41%\nfor 10-second utterances, and a small improvement for long utterances\n(about 5 min). These results suggest that voice quality features can\nimprove short-utterance text-independent ASV system performance.\n"
      ],
      "doi": "10.21437/Interspeech.2017-157"
    },
    "lee17c_interspeech": {
      "authors": [
        [
          "Kong Aik",
          "Lee"
        ],
        [
          "Haizhou",
          "Li"
        ]
      ],
      "title": "Gain Compensation for Fast i-Vector Extraction Over Short Duration",
      "original": "0108",
      "page_count": 5,
      "order": 317,
      "p1": "1527",
      "pn": "1531",
      "abstract": [
        "I-vector is widely described as a compact and effective representation\nof speech utterances for speaker recognition. Standard i-vector extraction\ncould be an expensive task for applications where computing resource\nis limited, for instance, on handheld devices. Fast approximate inference\nof i-vector aims to reduce the computational cost required in i-vector\nextraction where run-time requirement is critical. Most fast approaches\nhinge on certain assumptions to approximate the i-vector inference\nformulae with little loss of accuracy. In this paper, we analyze the\n uniform assumption that we had proposed earlier. We show that the\nassumption generally hold for long utterances but inadequate for utterances\nof short duration. We then propose to compensate for the negative effects\nby applying a simple gain factor on the i-vectors estimated from short\nutterances. The assertion is confirmed through analysis and experiments\nconducted on NIST SRE&#8217;08 and SRE&#8217;10 datasets.\n"
      ],
      "doi": "10.21437/Interspeech.2017-108"
    },
    "heo17_interspeech": {
      "authors": [
        [
          "Hee-soo",
          "Heo"
        ],
        [
          "Jee-weon",
          "Jung"
        ],
        [
          "IL-ho",
          "Yang"
        ],
        [
          "Sung-hyun",
          "Yoon"
        ],
        [
          "Ha-jin",
          "Yu"
        ]
      ],
      "title": "Joint Training of Expanded End-to-End DNN for Text-Dependent Speaker Verification",
      "original": "1050",
      "page_count": 5,
      "order": 318,
      "p1": "1532",
      "pn": "1536",
      "abstract": [
        "We propose an expanded end-to-end DNN architecture for speaker verification\nbased on b-vectors as well as d-vectors. We embedded the components\nof a speaker verification system such as modeling frame-level features,\nextracting utterance-level features, dimensionality reduction of utterance-level\nfeatures, and trial-level scoring in an expanded end-to-end DNN architecture.\nThe main contribution of this paper is that, instead of using DNNs\nas parts of the system trained independently, we train the whole system\njointly with a fine-tune cost after pre-training each part. The experimental\nresults show that the proposed system outperforms the baseline d-vector\nsystem and i-vector PLDA system.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1050"
    },
    "chen17f_interspeech": {
      "authors": [
        [
          "Chen",
          "Chen"
        ],
        [
          "Jiqing",
          "Han"
        ],
        [
          "Yilin",
          "Pan"
        ]
      ],
      "title": "Speaker Verification via Estimating Total Variability Space Using Probabilistic Partial Least Squares",
      "original": "0633",
      "page_count": 5,
      "order": 319,
      "p1": "1537",
      "pn": "1541",
      "abstract": [
        "The i-vector framework is one of the most popular methods in speaker\nverification, and estimating a total variability space (TVS) is a key\npart in the i-vector framework. Current estimation methods pay less\nattention on the discrimination of TVS, but the discrimination is so\nimportant that it will influence the improvement of performance. So\nwe focus on the discrimination of TVS to achieve a better performance.\nIn this paper, a discriminative estimating method of TVS based on probabilistic\npartial least squares (PPLS) is proposed. In this method, the discrimination\nis improved by using the priori information (labels) of speaker, so\nboth the correlation of intra-class and the discrimination of interclass\nare fully utilized. Meanwhile, it also introduces a probabilistic view\nof the partial least squares (PLS) method to overcome the disadvantage\nof high computational complexity and the inability of channel compensation.\nAnd also this proposed method can achieve a better performance than\nthe traditional TVS estimation method as well as the PLS-based method.\n"
      ],
      "doi": "10.21437/Interspeech.2017-633"
    },
    "li17g_interspeech": {
      "authors": [
        [
          "Lantian",
          "Li"
        ],
        [
          "Yixiang",
          "Chen"
        ],
        [
          "Ying",
          "Shi"
        ],
        [
          "Zhiyuan",
          "Tang"
        ],
        [
          "Dong",
          "Wang"
        ]
      ],
      "title": "Deep Speaker Feature Learning for Text-Independent Speaker Verification",
      "original": "0452",
      "page_count": 5,
      "order": 320,
      "p1": "1542",
      "pn": "1546",
      "abstract": [
        "Recently deep neural networks (DNNs) have been used to learn speaker\nfeatures. However, the quality of the learned features is not sufficiently\ngood, so a complex back-end model, either neural or probabilistic,\nhas to be used to address the residual uncertainty when applied to\nspeaker verification. This paper presents a convolutional time-delay\ndeep neural network structure (CT-DNN) for speaker feature learning.\nOur experimental results on the  Fisher database demonstrated that\nthis CT-DNN can produce high-quality speaker features: even with a\nsingle feature (0.3 seconds including the context), the EER can be\nas low as 7.68%. This effectively confirmed that the speaker trait\nis largely a deterministic short-time property rather than a long-time\ndistributional pattern, and therefore can be extracted from just dozens\nof frames.\n"
      ],
      "doi": "10.21437/Interspeech.2017-452"
    },
    "bousquet17_interspeech": {
      "authors": [
        [
          "Pierre-Michel",
          "Bousquet"
        ],
        [
          "Mickael",
          "Rouvier"
        ]
      ],
      "title": "Duration Mismatch Compensation Using Four-Covariance Model and Deep Neural Network for Speaker Verification",
      "original": "0093",
      "page_count": 5,
      "order": 321,
      "p1": "1547",
      "pn": "1551",
      "abstract": [
        "Duration mismatch between enrollment and test utterances still remains\na major concern for reliability of real-life speaker recognition applications.\nTwo approaches are proposed here to deal with this case when using\nthe i-vector representation. The first one is an adaptation of Gaussian\nProbabilistic Linear Discriminant Analysis (PLDA) modeling, which can\nbe extended to the case of any shift between i-vectors drawn from two\ndistinct distributions. The second one attempts to map i-vectors of\ntruncated segments of an utterance to the i-vector of the full segment,\nby the use of deep neural networks (DNN). Our results show that both\nnew approaches outperform the standard PLDA by about 10% relative,\nnoting that these back-end methods could complement those quantifying\nthe i-vector uncertainty during its extraction process, in the case\nof duration gap.\n"
      ],
      "doi": "10.21437/Interspeech.2017-93"
    },
    "mccree17_interspeech": {
      "authors": [
        [
          "Alan",
          "McCree"
        ],
        [
          "Gregory",
          "Sell"
        ],
        [
          "Daniel",
          "Garcia-Romero"
        ]
      ],
      "title": "Extended Variability Modeling and Unsupervised Adaptation for PLDA Speaker Recognition",
      "original": "1586",
      "page_count": 5,
      "order": 322,
      "p1": "1552",
      "pn": "1556",
      "abstract": [
        "Probabilistic Linear Discriminant Analysis (PLDA) continues to be the\nmost effective approach for speaker recognition in the i-vector space.\nThis paper extends the PLDA model to include both enrollment and test\ncut duration as well as to distinguish between session and channel\nvariability. In addition, we address the task of unsupervised adaptation\nto unknown new domains in two ways: speaker-dependent PLDA parameters\nand cohort score normalization using Bayes rule. Experimental results\non the NIST SRE16 task show that these principled techniques provide\nstate-of-the-art performance with negligible increase in complexity\nover a PLDA baseline.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1586"
    },
    "borgstrom17_interspeech": {
      "authors": [
        [
          "Bengt J.",
          "Borgstr\u00f6m"
        ],
        [
          "Elliot",
          "Singer"
        ],
        [
          "Douglas",
          "Reynolds"
        ],
        [
          "Seyed Omid",
          "Sadjadi"
        ]
      ],
      "title": "Improving the Effectiveness of Speaker Verification Domain Adaptation with Inadequate In-Domain Data",
      "original": "0438",
      "page_count": 5,
      "order": 323,
      "p1": "1557",
      "pn": "1561",
      "abstract": [
        "This paper addresses speaker verification domain adaptation with inadequate\nin-domain data. Specifically, we explore the cases where in-domain\ndata sets do not include speaker labels, contain speakers with few\nsamples, or contain speakers with low channel diversity. Existing domain\nadaptation methods are reviewed, and their shortcomings are discussed.\nWe derive an unsupervised version of fully Bayesian adaptation which\nreduces the reliance on rich in-domain data. When applied to domain\nadaptation with inadequate in-domain data, the proposed approach yields\ncompetitive results when the samples per speaker are reduced, and outperforms\nexisting supervised methods when the channel diversity is low, even\nwithout requiring speaker labels. These results are validated on the\nNIST SRE16, which uses a highly inadequate in-domain data set.\n"
      ],
      "doi": "10.21437/Interspeech.2017-438"
    },
    "tan17_interspeech": {
      "authors": [
        [
          "Zhili",
          "Tan"
        ],
        [
          "Man-Wai",
          "Mak"
        ]
      ],
      "title": "i-Vector DNN Scoring and Calibration for Noise Robust Speaker Verification",
      "original": "0656",
      "page_count": 5,
      "order": 324,
      "p1": "1562",
      "pn": "1566",
      "abstract": [
        "This paper proposes applying multi-task learning to train deep neural\nnetworks (DNNs) for calibrating the PLDA scores of speaker verification\nsystems under noisy environments. To facilitate the DNNs to learn the\nmain task (calibration), several auxiliary tasks were introduced, including\nthe prediction of SNR and duration from i-vectors and classifying whether\nan i-vector pair belongs to the same speaker or not. The possibility\nof replacing the PLDA model by a DNN during the scoring stage is also\nexplored. Evaluations on noise contaminated speech suggest that the\nauxiliary tasks are important for the DNNs to learn the main calibration\ntask and that the uncalibrated PLDA scores are an essential input to\nthe DNNs. Without this input, the DNNs can only predict the score shifts\naccurately, suggesting that the PLDA model is indispensable.\n"
      ],
      "doi": "10.21437/Interspeech.2017-656"
    },
    "matejka17_interspeech": {
      "authors": [
        [
          "Pavel",
          "Mat\u011bjka"
        ],
        [
          "Ond\u0159ej",
          "Novotn\u00fd"
        ],
        [
          "Old\u0159ich",
          "Plchot"
        ],
        [
          "Luk\u00e1\u0161",
          "Burget"
        ],
        [
          "Mireia Diez",
          "S\u00e1nchez"
        ],
        [
          "Jan",
          "\u010cernock\u00fd"
        ]
      ],
      "title": "Analysis of Score Normalization in Multilingual Speaker Recognition",
      "original": "0803",
      "page_count": 5,
      "order": 325,
      "p1": "1567",
      "pn": "1571",
      "abstract": [
        "NIST Speaker Recognition Evaluation 2016 has revealed the importance\nof score normalization for mismatched data conditions. This paper analyzes\nseveral score normalization techniques for test conditions with multiple\nlanguages. The best performing one for a PLDA classifier is an adaptive\ns-norm with 30% relative improvement over the system without any score\nnormalization. The analysis shows that the adaptive score normalization\n(using top scoring files per trial) selects cohorts that in 68% contain\nrecordings from the same language and in 92% of the same gender as\nthe enrollment and test recordings. Our results suggest that the data\nto select score normalization cohorts should be a pool of several languages\nand channels and if possible, its subset should contain data from the\ntarget domain.\n"
      ],
      "doi": "10.21437/Interspeech.2017-803"
    },
    "silnova17_interspeech": {
      "authors": [
        [
          "Anna",
          "Silnova"
        ],
        [
          "Luk\u00e1\u0161",
          "Burget"
        ],
        [
          "Jan",
          "\u010cernock\u00fd"
        ]
      ],
      "title": "Alternative Approaches to Neural Network Based Speaker Verification",
      "original": "1062",
      "page_count": 4,
      "order": 326,
      "p1": "1572",
      "pn": "1575",
      "abstract": [
        "Just like in other areas of automatic speech processing, feature extraction\nbased on bottleneck neural networks was recently found very effective\nfor the speaker verification task. However, better results are usually\nreported with more complex neural network architectures (e.g. stacked\nbottlenecks), which are difficult to reproduce. In this work, we experiment\nwith the so called deep features, which are based on a simple feed-forward\nneural network architecture. We study various forms of applying deep\nfeatures to i-vector/PDA based speaker verification. With proper settings,\nbetter verification performance can be obtained by means of this simple\narchitecture as compared to the more elaborate bottleneck features.\nAlso, we further experiment with multi-task training, where the neural\nnetwork is trained for both speaker recognition and senone recognition\nobjectives. Results indicate that, with a careful weighting of the\ntwo objectives, multi-task training can result in significantly better\nperforming deep features.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1062"
    },
    "travadi17_interspeech": {
      "authors": [
        [
          "Ruchir",
          "Travadi"
        ],
        [
          "Shrikanth S.",
          "Narayanan"
        ]
      ],
      "title": "A Distribution Free Formulation of the Total Variability Model",
      "original": "0219",
      "page_count": 5,
      "order": 327,
      "p1": "1576",
      "pn": "1580",
      "abstract": [
        "The Total Variability Model (TVM) [1] has been widely used in audio\nsignal processing as a framework for capturing differences in feature\nspace distributions across variable length sequences by mapping them\ninto a fixed-dimensional representation. Its formulation requires making\nan assumption about the source data distribution being a Gaussian Mixture\nModel (GMM). In this paper, we show that it is possible to arrive at\nthe same model formulation without requiring such an assumption about\ndistribution of the data, by showing asymptotic normality of the statistics\nused to estimate the model. We highlight some connections between TVM\nand heteroscedastic Principal Component Analysis (PCA), as well as\nthe matrix completion problem, which lead to a computationally efficient\nformulation of the Maximum Likelihood estimation problem for the model.\n"
      ],
      "doi": "10.21437/Interspeech.2017-219"
    },
    "rahman17_interspeech": {
      "authors": [
        [
          "Md. Hafizur",
          "Rahman"
        ],
        [
          "Ivan",
          "Himawan"
        ],
        [
          "David",
          "Dean"
        ],
        [
          "Sridha",
          "Sridharan"
        ]
      ],
      "title": "Domain Mismatch Modeling of Out-Domain i-Vectors for PLDA Speaker Verification",
      "original": "0668",
      "page_count": 5,
      "order": 328,
      "p1": "1581",
      "pn": "1585",
      "abstract": [
        "The state-of-the-art i-vector based probabilistic linear discriminant\nanalysis (PLDA) trained on non-target (or out-domain) data significantly\naffects the speaker verification performance due to the domain mismatch\nbetween training and evaluation data. To improve the speaker verification\nperformance, sufficient amount of domain mismatch compensated out-domain\ndata must be used to train the PLDA models successfully. In this paper,\nwe propose a domain mismatch modeling (DMM) technique using maximum-a-posteriori\n(MAP) estimation to model and compensate the domain variability from\nthe out-domain training i-vectors. From our experimental results, we\nfound that the DMM technique can achieve at least a 24% improvement\nin EER over an out-domain only baseline when speaker labels are available.\nFurther improvement of 3% is obtained when combining DMM with domain-invariant\ncovariance normalization (DICN) approach. The DMM/DICN combined technique\nis shown to perform better than in-domain PLDA system with only 200\nlabeled speakers or 2,000 unlabeled i-vectors.\n"
      ],
      "doi": "10.21437/Interspeech.2017-668"
    },
    "cheng17_interspeech": {
      "authors": [
        [
          "Gaofeng",
          "Cheng"
        ],
        [
          "Vijayaditya",
          "Peddinti"
        ],
        [
          "Daniel",
          "Povey"
        ],
        [
          "Vimal",
          "Manohar"
        ],
        [
          "Sanjeev",
          "Khudanpur"
        ],
        [
          "Yonghong",
          "Yan"
        ]
      ],
      "title": "An Exploration of Dropout with LSTMs",
      "original": "0129",
      "page_count": 5,
      "order": 329,
      "p1": "1586",
      "pn": "1590",
      "abstract": [
        "Long Short-Term Memory networks (LSTMs) are a component of many state-of-the-art\nDNN-based speech recognition systems. Dropout is a popular method to\nimprove generalization in DNN training. In this paper we describe extensive\nexperiments in which we investigated the best way to combine dropout\nwith LSTMs &#8212; specifically, projected LSTMs (LSTMP). We investigated\nvarious locations in the LSTM to place the dropout (and various combinations\nof locations), and a variety of dropout schedules. Our optimized recipe\ngives consistent improvements in WER across a range of datasets, including\nSwitchboard, TED-LIUM and AMI.\n"
      ],
      "doi": "10.21437/Interspeech.2017-129"
    },
    "kim17e_interspeech": {
      "authors": [
        [
          "Jaeyoung",
          "Kim"
        ],
        [
          "Mostafa",
          "El-Khamy"
        ],
        [
          "Jungwon",
          "Lee"
        ]
      ],
      "title": "Residual LSTM: Design of a Deep Recurrent Architecture for Distant Speech Recognition",
      "original": "0477",
      "page_count": 5,
      "order": 330,
      "p1": "1591",
      "pn": "1595",
      "abstract": [
        "In this paper, a novel architecture for a deep recurrent neural network,\nresidual LSTM is introduced. A plain LSTM has an internal memory cell\nthat can learn long term dependencies of sequential data. It also provides\na temporal shortcut path to avoid vanishing or exploding gradients\nin the temporal domain. The residual LSTM provides an additional spatial\nshortcut path from lower layers for efficient training of deep networks\nwith multiple LSTM layers. Compared with the previous work, highway\nLSTM, residual LSTM separates a spatial shortcut path with temporal\none by using output layers, which can help to avoid a conflict between\nspatial and temporal-domain gradient flows. Furthermore, residual LSTM\nreuses the output projection matrix and the output gate of LSTM to\ncontrol the spatial information flow instead of additional gate networks,\nwhich effectively reduces more than 10% of network parameters. An experiment\nfor distant speech recognition on the AMI SDM corpus shows that 10-layer\nplain and highway LSTM networks presented 13.7% and 6.2% increase in\nWER over 3-layer baselines, respectively. On the contrary, 10-layer\nresidual LSTM networks provided the lowest WER 41.0%, which corresponds\nto 3.3% and 2.8% WER reduction over plain and highway LSTM networks,\nrespectively.\n"
      ],
      "doi": "10.21437/Interspeech.2017-477"
    },
    "tran17_interspeech": {
      "authors": [
        [
          "Dung T.",
          "Tran"
        ],
        [
          "Marc",
          "Delcroix"
        ],
        [
          "Shigeki",
          "Karita"
        ],
        [
          "Michael",
          "Hentschel"
        ],
        [
          "Atsunori",
          "Ogawa"
        ],
        [
          "Tomohiro",
          "Nakatani"
        ]
      ],
      "title": "Unfolded Deep Recurrent Convolutional Neural Network with Jump Ahead Connections for Acoustic Modeling",
      "original": "0873",
      "page_count": 5,
      "order": 331,
      "p1": "1596",
      "pn": "1600",
      "abstract": [
        "Recurrent neural networks (RNNs) with jump ahead connections have been\nused in the computer vision tasks. Still, they have not been investigated\nwell for automatic speech recognition (ASR) tasks. In other words,\nunfolded RNN has been shown to be an effective model for acoustic modeling\ntasks. This paper investigates how to elaborate a sophisticated unfolded\ndeep RNN architecture in which recurrent connections use a convolutional\nneural network (CNN) to model a short-term dependence between hidden\nstates. In this study, our unfolded RNN architecture is a CNN that\nprocess a sequence of input features sequentially. Each time step,\nthe CNN inputs a small block of the input features and the output of\nthe hidden layer from the preceding block in order to compute the output\nof its hidden layer. In addition, by exploiting either one or multiple\njump ahead connections between time steps, our network can learn long-term\ndependencies more effectively. We carried experiments on the CHiME\n3 task showing the effectiveness of our proposed approach.\n"
      ],
      "doi": "10.21437/Interspeech.2017-873"
    },
    "karita17_interspeech": {
      "authors": [
        [
          "Shigeki",
          "Karita"
        ],
        [
          "Atsunori",
          "Ogawa"
        ],
        [
          "Marc",
          "Delcroix"
        ],
        [
          "Tomohiro",
          "Nakatani"
        ]
      ],
      "title": "Forward-Backward Convolutional LSTM for Acoustic Modeling",
      "original": "0554",
      "page_count": 5,
      "order": 332,
      "p1": "1601",
      "pn": "1605",
      "abstract": [
        "An automatic speech recognition (ASR) performance has greatly improved\nwith the introduction of convolutional neural network (CNN) or long-short\nterm memory (LSTM) for acoustic modeling. Recently, a convolutional\nLSTM (CLSTM) has been proposed to directly use convolution operation\nwithin the LSTM blocks and combine the advantages of both CNN and LSTM\nstructures into a single architecture. This paper presents the first\nattempt to use CLSTMs for acoustic modeling. In addition, we propose\na new forward-backward architecture to exploit long-term left/right\ncontext efficiently. The proposed scheme combines forward and backward\nLSTMs at different time points of an utterance with the aim of modeling\nlong term frame invariant information such as speaker characteristics,\nchannel etc. Furthermore, the proposed forward-backward architecture\ncan be trained with truncated back-propagation-through-time unlike\nconventional bidirectional LSTM (BLSTM) architectures. Therefore, we\nare able to train deeply stacked CLSTM acoustic models, which is practically\nchallenging with conventional BLSTMs. Experimental results show that\nboth CLSTM and forward-backward LSTM improve word error rates significantly\ncompared to standard CNN and LSTM architectures.\n"
      ],
      "doi": "10.21437/Interspeech.2017-554"
    },
    "ark17_interspeech": {
      "authors": [
        [
          "Sercan \u00d6.",
          "Ar\u0131k"
        ],
        [
          "Markus",
          "Kliegl"
        ],
        [
          "Rewon",
          "Child"
        ],
        [
          "Joel",
          "Hestness"
        ],
        [
          "Andrew",
          "Gibiansky"
        ],
        [
          "Chris",
          "Fougner"
        ],
        [
          "Ryan",
          "Prenger"
        ],
        [
          "Adam",
          "Coates"
        ]
      ],
      "title": "Convolutional Recurrent Neural Networks for Small-Footprint Keyword Spotting",
      "original": "1737",
      "page_count": 5,
      "order": 333,
      "p1": "1606",
      "pn": "1610",
      "abstract": [
        "Keyword spotting (KWS) constitutes a major component of human-technology\ninterfaces. Maximizing the detection accuracy at a low false alarm\n(FA) rate, while minimizing the footprint size, latency and complexity\nare the goals for KWS. Towards achieving them, we study Convolutional\nRecurrent Neural Networks (CRNNs). Inspired by large-scale state-of-the-art\nspeech recognition systems, we combine the strengths of convolutional\nlayers and recurrent layers to exploit local structure and long-range\ncontext. We analyze the effect of architecture parameters, and propose\ntraining strategies to improve performance. With only &#126;230k parameters,\nour CRNN model yields acceptably low latency, and achieves 97.71% accuracy\nat 0.5 FA/hour for 5 dB signal-to-noise ratio.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1737"
    },
    "wu17c_interspeech": {
      "authors": [
        [
          "Chunyang",
          "Wu"
        ],
        [
          "Mark J.F.",
          "Gales"
        ]
      ],
      "title": "Deep Activation Mixture Model for Speech Recognition",
      "original": "1233",
      "page_count": 5,
      "order": 334,
      "p1": "1611",
      "pn": "1615",
      "abstract": [
        "Deep learning approaches achieve state-of-the-art performance in a\nrange of applications, including speech recognition. However, the parameters\nof the deep neural network (DNN) are hard to interpret, which makes\nregularisation and adaptation to speaker or acoustic conditions challenging.\nThis paper proposes the deep activation mixture model (DAMM) to address\nthese problems. The output of one hidden layer is modelled as the sum\nof a mixture and residual models. The mixture model forms an activation\nfunction contour while the residual one models fluctuations around\nthe contour. The use of the mixture model gives two advantages: First,\nit introduces a novel regularisation on the DNN. Second, it allows\nnovel adaptation schemes. The proposed approach is evaluated on a large-vocabulary\nU.S. English broadcast news task. It yields a slightly better performance\nthan the DNN baselines, and on the utterance-level unsupervised adaptation,\nthe adapted DAMM acquires further performance gains.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1233"
    },
    "heck17_interspeech": {
      "authors": [
        [
          "Michael",
          "Heck"
        ],
        [
          "Masayuki",
          "Suzuki"
        ],
        [
          "Takashi",
          "Fukuda"
        ],
        [
          "Gakuto",
          "Kurata"
        ],
        [
          "Satoshi",
          "Nakamura"
        ]
      ],
      "title": "Ensembles of Multi-Scale VGG Acoustic Models",
      "original": "0920",
      "page_count": 5,
      "order": 335,
      "p1": "1616",
      "pn": "1620",
      "abstract": [
        "We present our work on constructing multi-scale deep convolutional\nneural networks for automatic speech recognition. Several VGG nets\nhave been trained that differ solely in the kernel size of the convolutional\nlayers. The general idea is that receptive fields of varying sizes\nmatch structures of different scales, thus supporting more robust recognition\nwhen combined appropriately. We construct a large multi-scale system\nby means of system combination. We use ROVER and the fusion of posterior\npredictions as examples of late combination, and knowledge distillation\nusing soft labels from a model ensemble as a way of early combination.\nIn this work, distillation is approached from the perspective of knowledge\ntransfer pre-training, which is followed by a fine-tuning on the original\nhard labels. Our results show that it is possible to bundle the individual\nrecognition strengths of the VGGs in a much simpler CNN architecture\nthat yields equal performance with the best late combination.\n"
      ],
      "doi": "10.21437/Interspeech.2017-920"
    },
    "grosz17_interspeech": {
      "authors": [
        [
          "Tam\u00e1s",
          "Gr\u00f3sz"
        ],
        [
          "G\u00e1bor",
          "Gosztolya"
        ],
        [
          "L\u00e1szl\u00f3",
          "T\u00f3th"
        ]
      ],
      "title": "Training Context-Dependent DNN Acoustic Models Using Probabilistic Sampling",
      "original": "0338",
      "page_count": 5,
      "order": 336,
      "p1": "1621",
      "pn": "1625",
      "abstract": [
        "In current HMM/DNN speech recognition systems, the purpose of the DNN\ncomponent is to estimate the posterior probabilities of tied triphone\nstates. In most cases the distribution of these states is uneven, meaning\nthat we have a markedly different number of training samples for the\nvarious states. This imbalance of the training data is a source of\nsuboptimality for most machine learning algorithms, and DNNs are no\nexception. A straightforward solution is to re-sample the data, either\nby upsampling the rarer classes or by downsampling the more common\nclasses. Here, we experiment with the so-called probabilistic sampling\nmethod that applies downsampling and upsampling at the same time. For\nthis, it defines a new class distribution for the training data, which\nis a linear combination of the original and the uniform class distributions.\nAs an extension to previous studies, we propose a new method to re-estimate\nthe class priors, which is required to remedy the mismatch between\nthe training and the test data distributions introduced by re-sampling.\nUsing probabilistic sampling and the proposed modification we report\n5% and 6% relative error rate reductions on the TED-LIUM and on the\nAMI corpora, respectively.\n"
      ],
      "doi": "10.21437/Interspeech.2017-338"
    },
    "grosz17b_interspeech": {
      "authors": [
        [
          "Tam\u00e1s",
          "Gr\u00f3sz"
        ],
        [
          "G\u00e1bor",
          "Gosztolya"
        ],
        [
          "L\u00e1szl\u00f3",
          "T\u00f3th"
        ]
      ],
      "title": "A Comparative Evaluation of GMM-Free State Tying Methods for ASR",
      "original": "0899",
      "page_count": 5,
      "order": 337,
      "p1": "1626",
      "pn": "1630",
      "abstract": [
        "Deep neural network (DNN) based speech recognizers have recently replaced\nGaussian mixture (GMM) based systems as the state-of-the-art. While\nsome of the modeling techniques developed for the GMM based framework\nmay directly be applied to HMM/DNN systems, others may be inappropriate.\nOne such example is the creation of context-dependent tied states,\nfor which an efficient decision tree state tying method exists. The\ntied states used to train DNNs are usually obtained using the same\ntying algorithm, even though it is based on likelihoods of Gaussians,\nhence it is more appropriate for HMM/GMMs. Recently, however, several\nrefinements have been published which seek to adapt the state tying\nalgorithm to the HMM/DNN hybrid architecture. Unfortunately, these\nstudies reported results on different (and sometimes very small) datasets,\nwhich does not allow their direct comparison. Here, we tested four\nof these methods on the same LVCSR task, and compared their performance\nunder the same circumstances. We found that, besides changing the input\nof the context-dependent state tying algorithm, it is worth adjusting\nthe tying criterion as well. The methods which utilized a decision\ncriterion designed directly for neural networks consistently, and significantly,\noutperformed those which employed the standard Gaussian-based algorithm.\n"
      ],
      "doi": "10.21437/Interspeech.2017-899"
    },
    "wang17h_interspeech": {
      "authors": [
        [
          "Yiming",
          "Wang"
        ],
        [
          "Vijayaditya",
          "Peddinti"
        ],
        [
          "Hainan",
          "Xu"
        ],
        [
          "Xiaohui",
          "Zhang"
        ],
        [
          "Daniel",
          "Povey"
        ],
        [
          "Sanjeev",
          "Khudanpur"
        ]
      ],
      "title": "Backstitch: Counteracting Finite-Sample Bias via Negative Steps",
      "original": "1323",
      "page_count": 5,
      "order": 338,
      "p1": "1631",
      "pn": "1635",
      "abstract": [
        "In this paper we describe a modification to Stochastic Gradient Descent\n(SGD) that improves generalization to unseen data. It consists of doing\ntwo steps for each minibatch: a backward step with a small negative\nlearning rate, followed by a forward step with a larger learning rate.\nThe idea was initially inspired by ideas from adversarial training,\nbut we show that it can be viewed as a crude way of canceling out certain\nsystematic biases that come from training on finite data sets. The\nmethod gives &#126; 10% relative improvement over our best acoustic\nmodels based on lattice-free MMI, across multiple datasets with 100&#8211;300\nhours of data.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1323"
    },
    "takeda17_interspeech": {
      "authors": [
        [
          "Ryu",
          "Takeda"
        ],
        [
          "Kazuhiro",
          "Nakadai"
        ],
        [
          "Kazunori",
          "Komatani"
        ]
      ],
      "title": "Node Pruning Based on Entropy of Weights and Node Activity for Small-Footprint Acoustic Model Based on Deep Neural Networks",
      "original": "0779",
      "page_count": 5,
      "order": 339,
      "p1": "1636",
      "pn": "1640",
      "abstract": [
        "This paper describes a node-pruning method for an acoustic model based\non deep neural networks (DNNs). Node pruning is a promising method\nto reduce the memory usage and computational cost of DNNs. A score\nfunction is defined to measure the importance of each node, and less\nimportant nodes are pruned. The entropy of the activity of each node\nhas been used as a score function to find nodes with outputs that do\nnot change at all. We introduce entropy of weights of each node to\nconsider the number of weights and their patterns of each node. Because\nthe number of weights and the patterns differ at each layer, the importance\nof the node should also be measured using the related weights of the\ntarget node. We then propose a score function that integrates the entropy\nof weights and node activity, which will prune less important nodes\nmore efficiently. Experimental results showed that the proposed pruning\nmethod successfully reduced the number of parameters by about 6% without\nany accuracy loss compared with a score function based only on the\nentropy of node activity.\n"
      ],
      "doi": "10.21437/Interspeech.2017-779"
    },
    "variani17_interspeech": {
      "authors": [
        [
          "Ehsan",
          "Variani"
        ],
        [
          "Tom",
          "Bagby"
        ],
        [
          "Erik",
          "McDermott"
        ],
        [
          "Michiel",
          "Bacchiani"
        ]
      ],
      "title": "End-to-End Training of Acoustic Models for Large Vocabulary Continuous Speech Recognition with TensorFlow",
      "original": "1284",
      "page_count": 5,
      "order": 340,
      "p1": "1641",
      "pn": "1645",
      "abstract": [
        "This article discusses strategies for end-to-end training of state-of-the-art\nacoustic models for Large Vocabulary Continuous Speech Recognition\n(LVCSR), with the goal of leveraging TensorFlow components so as to\nmake efficient use of large-scale training sets, large model sizes,\nand high-speed computation units such as Graphical Processing Units\n(GPUs). Benchmarks are presented that evaluate the efficiency of different\napproaches to batching of training data, unrolling of recurrent acoustic\nmodels, and device placement of TensorFlow variables and operations.\nAn overall training architecture developed in light of those findings\nis then described. The approach makes it possible to take advantage\nof both data parallelism and high speed computation on GPU for state-of-the-art\nsequence training of acoustic models. The effectiveness of the design\nis evaluated for different training schemes and model sizes, on a 15,000\nhour Voice Search task.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1284"
    },
    "sim17_interspeech": {
      "authors": [
        [
          "Khe Chai",
          "Sim"
        ],
        [
          "Arun",
          "Narayanan"
        ]
      ],
      "title": "An Efficient Phone N-Gram Forward-Backward Computation Using Dense Matrix Multiplication",
      "original": "1557",
      "page_count": 5,
      "order": 341,
      "p1": "1646",
      "pn": "1650",
      "abstract": [
        "The forward-backward algorithm is commonly used to train neural network\nacoustic models when optimizing a sequence objective like MMI and sMBR.\nRecent work on lattice-free MMI training of neural network acoustic\nmodels shows that the forward-backward algorithm can be computed efficiently\nin the probability domain as a series of sparse matrix multiplications\nusing GPUs. In this paper, we present a more efficient way of computing\nforward-backward using a dense matrix multiplication approach. We do\nthis by exploiting the block-diagonal structure of the n-gram state\ntransition matrix; instead of multiplying large sparse matrices, the\nproposed method involves a series of smaller dense matrix multiplications,\nwhich can be computed in parallel. Efficient implementation can be\neasily achieved by leveraging on the optimized matrix multiplication\nroutines provided by standard libraries, such as NumPy and TensorFlow.\nRuntime benchmarks show that the dense multiplication method is consistently\nfaster than the sparse multiplication method (on both CPUs and GPUs),\nwhen applied to a 4-gram phone language model. This is still the case\neven when the sparse multiplication method uses a more compact finite\nstate model representation by excluding unseen n-grams.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1557"
    },
    "tuske17_interspeech": {
      "authors": [
        [
          "Zolt\u00e1n",
          "T\u00fcske"
        ],
        [
          "Wilfried",
          "Michel"
        ],
        [
          "Ralf",
          "Schl\u00fcter"
        ],
        [
          "Hermann",
          "Ney"
        ]
      ],
      "title": "Parallel Neural Network Features for Improved Tandem Acoustic Modeling",
      "original": "1747",
      "page_count": 5,
      "order": 342,
      "p1": "1651",
      "pn": "1655",
      "abstract": [
        "The combination of acoustic models or features is a standard approach\nto exploit various knowledge sources. This paper investigates the concatenation\nof different bottleneck (BN) neural network (NN) outputs for tandem\nacoustic modeling. Thus, combination of NN features is performed via\nGaussian mixture models (GMM). Complementarity between the NN feature\nrepresentations is attained by using various network topologies: LSTM\nrecurrent, feed-forward, and hierarchical, as well as different non-linearities:\nhyperbolic tangent, sigmoid, and rectified linear units. Speech recognition\nexperiments are carried out on various tasks: telephone conversations,\nSkype calls, as well as broadcast news and conversations. Results indicate\nthat LSTM based tandem approach is still competitive, and such tandem\nmodel can challenge comparable hybrid systems. The traditional steps\nof tandem modeling, speaker adaptive and sequence discriminative GMM\ntraining, improve the tandem results further. Furthermore, these &#8220;old-fashioned&#8221;\nsteps remain applicable after the concatenation of multiple neural\nnetwork feature streams. Exploiting the parallel processing of input\nfeature streams, it is shown that 2&#8211;5% relative improvement could\nbe achieved over the single best BN feature set. Finally, we also report\nresults after neural network based language model rescoring and examine\nthe system combination possibilities using such complex tandem models.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1747"
    },
    "tang17_interspeech": {
      "authors": [
        [
          "Qingming",
          "Tang"
        ],
        [
          "Weiran",
          "Wang"
        ],
        [
          "Karen",
          "Livescu"
        ]
      ],
      "title": "Acoustic Feature Learning via Deep Variational Canonical Correlation Analysis",
      "original": "1581",
      "page_count": 5,
      "order": 343,
      "p1": "1656",
      "pn": "1660",
      "abstract": [
        "We study the problem of acoustic feature learning in the setting where\nwe have access to another (non-acoustic) modality for feature learning\nbut not at test time. We use deep variational canonical correlation\nanalysis (VCCA), a recently proposed deep generative method for multi-view\nrepresentation learning. We also extend VCCA with improved latent variable\npriors and with adversarial learning. Compared to other techniques\nfor multi-view feature learning, VCCA&#8217;s advantages include an\nintuitive latent variable interpretation and a variational lower bound\nobjective that can be trained end-to-end efficiently. We compare VCCA\nand its extensions with previous feature learning methods on the University\nof Wisconsin X-ray Microbeam Database, and show that VCCA-based feature\nlearning improves over previous methods for speaker-independent phonetic\nrecognition.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1581"
    },
    "masumura17_interspeech": {
      "authors": [
        [
          "Ryo",
          "Masumura"
        ],
        [
          "Taichi",
          "Asami"
        ],
        [
          "Hirokazu",
          "Masataki"
        ],
        [
          "Ryo",
          "Ishii"
        ],
        [
          "Ryuichiro",
          "Higashinaka"
        ]
      ],
      "title": "Online End-of-Turn Detection from Speech Based on Stacked Time-Asynchronous Sequential Networks",
      "original": "0651",
      "page_count": 5,
      "order": 344,
      "p1": "1661",
      "pn": "1665",
      "abstract": [
        "This paper presents a novel modeling called stacked time-asynchronous\nsequential networks (STASNs) for online end-of-turn detection. An online\nend-of-turn detection that determines turn-taking points in a real-time\nmanner is an essential component for human-computer interaction systems.\nIn this study, we use long-range sequential information of multiple\ntime-asynchronous sequential features, such as prosodic, phonetic,\nand lexical sequential features, to enhance online end-of-turn detection\nperformance. Our key idea is to embed individual sequential features\nin a fixed-length continuous representation by using sequential networks.\nThis enables us to simultaneously handle multiple time-asynchronous\nsequential features for end-of-turn detection. STASNs can embed all\nof the sequential information between a start-of-conversation and the\ncurrent end-of-utterance in a fixed-length continuous representation\nthat can be directly used for classification by stacking multiple sequential\nnetworks. Experiments show that STASNs outperforms conventional modeling\nwith limited sequential information. Furthermore, STASNs with senone\nbottleneck features extracted using senone-based deep neural networks\nhave superior performance without requiring lexical features decoded\nby an automatic speech recognition process.\n"
      ],
      "doi": "10.21437/Interspeech.2017-651"
    },
    "wodarczak17_interspeech": {
      "authors": [
        [
          "Marcin",
          "W\u0142odarczak"
        ],
        [
          "Kornel",
          "Laskowski"
        ],
        [
          "Mattias",
          "Heldner"
        ],
        [
          "K\u00e4tlin",
          "Aare"
        ]
      ],
      "title": "Improving Prediction of Speech Activity Using Multi-Participant Respiratory State",
      "original": "1176",
      "page_count": 5,
      "order": 345,
      "p1": "1666",
      "pn": "1670",
      "abstract": [
        "One consequence of situated face-to-face conversation is the co-observability\nof participants&#8217; respiratory movements and sounds. We explore\nwhether this information can be exploited in predicting incipient speech\nactivity. Using a methodology called stochastic turn-taking modeling,\nwe compare the performance of a model trained on speech activity alone\nto one additionally trained on static and dynamic lung volume features.\nThe methodology permits automatic discovery of temporal dependencies\nacross participants and feature types. Our experiments show that respiratory\ninformation substantially lowers cross-entropy rates, and that this\ngeneralizes to unseen data.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1176"
    },
    "heeman17_interspeech": {
      "authors": [
        [
          "Peter A.",
          "Heeman"
        ],
        [
          "Rebecca",
          "Lunsford"
        ]
      ],
      "title": "Turn-Taking Offsets and Dialogue Context",
      "original": "1495",
      "page_count": 5,
      "order": 346,
      "p1": "1671",
      "pn": "1675",
      "abstract": [
        "A number of researchers have studied turn-taking offsets in human-human\ndialogues. However, that work collapses over a wide number of different\nturn-taking contexts. In this work, we delve into the turn-taking delays\nbased on different contexts. We show that turn-taking behavior, both\nwho tends to take the turn next, and the turn-taking delays, are dependent\non the previous speech act type, the upcoming speech act, and the nature\nof the dialogue. This strongly suggests that in studying turn-taking,\nall turn-taking events should not be grouped together. This also suggests\nthat delays are due to cognitive processing of what to say, rather\nthan whether a speaker should take the turn.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1495"
    },
    "maier17_interspeech": {
      "authors": [
        [
          "Angelika",
          "Maier"
        ],
        [
          "Julian",
          "Hough"
        ],
        [
          "David",
          "Schlangen"
        ]
      ],
      "title": "Towards Deep End-of-Turn Prediction for Situated Spoken Dialogue Systems",
      "original": "1593",
      "page_count": 5,
      "order": 347,
      "p1": "1676",
      "pn": "1680",
      "abstract": [
        "We address the challenge of improving live end-of-turn detection for\nsituated spoken dialogue systems. While traditionally silence thresholds\nhave been used to detect the user&#8217;s end-of-turn, such an approach\nlimits the system&#8217;s potential fluidity in interaction, restricting\nit to a purely reactive paradigm. By contrast, here we present a system\nwhich takes a predictive approach. The user&#8217;s end-of-turn is\npredicted live as acoustic features and words are consumed by the system.\nWe compare the benefits of live lexical and acoustic information by\nfeature analysis and testing equivalent models with different feature\nsets with a common deep learning architecture, a Long Short-Term Memory\n(LSTM) network. We show the usefulness of incremental enriched language\nmodel features in particular. Training and testing onWizard-of-Oz data\ncollected to train an agent in a simple virtual world, we are successful\nin improving over a reactive baseline in terms of reducing latency\nwhilst minimising the cut-in rate.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1593"
    },
    "ishimoto17_interspeech": {
      "authors": [
        [
          "Yuichi",
          "Ishimoto"
        ],
        [
          "Takehiro",
          "Teraoka"
        ],
        [
          "Mika",
          "Enomoto"
        ]
      ],
      "title": "End-of-Utterance Prediction by Prosodic Features and Phrase-Dependency Structure in Spontaneous Japanese Speech",
      "original": "0837",
      "page_count": 5,
      "order": 348,
      "p1": "1681",
      "pn": "1685",
      "abstract": [
        "This study is aimed at uncovering a way that participants in conversation\npredict end-of-utterance for spontaneous Japanese speech. In spontaneous\neveryday conversation, the participants must predict the ends of utterances\nof a speaker to perform smooth turn-taking without too much gap. We\nconsider that they utilize not only syntactic factors but also prosodic\nfactors for the end-of-utterance prediction because of the difficulty\nof prediction of a syntactic completion point in spontaneous Japanese.\nIn previous studies, we found that prosodic features changed significantly\nin the final accentual phrase. However, it is not clear what prosodic\nfeatures support the prediction. In this paper, we focused on dependency\nstructure among bunsetsu-phrases as the syntactic factor, and investigated\nthe relation between the phrase-dependency and prosodic features. The\nresults showed that the average fundamental frequency and the average\nintensity for accentual phrases did not decline until the modified\nphrase appeared. Next, to predict the end of utterance from the syntactic\nand prosodic features, we constructed a generalized linear mixed model.\nThe model provided higher accuracy than using the prosodic features\nonly. These suggest the possibility that prosodic changes and phrase-dependency\nrelations inform the hearer that the utterance is approaching its end.\n"
      ],
      "doi": "10.21437/Interspeech.2017-837"
    },
    "liu17_interspeech": {
      "authors": [
        [
          "Chaoran",
          "Liu"
        ],
        [
          "Carlos",
          "Ishi"
        ],
        [
          "Hiroshi",
          "Ishiguro"
        ]
      ],
      "title": "Turn-Taking Estimation Model Based on Joint Embedding of Lexical and Prosodic Contents",
      "original": "0965",
      "page_count": 5,
      "order": 349,
      "p1": "1686",
      "pn": "1690",
      "abstract": [
        "A natural conversation involves rapid exchanges of turns while talking.\nTaking turns at appropriate timing or intervals is a requisite feature\nfor a dialog system as a conversation partner. This paper proposes\na model that estimates the timing of turn-taking during verbal interactions.\nUnlike previous studies, our proposed model does not rely on a silence\nregion between sentences since a dialog system must respond without\nlarge gaps or overlaps. We propose a Recurrent Neural Network (RNN)\nbased model that takes the joint embedding of lexical and prosodic\ncontents as its input to classify utterances into turn-taking related\nclasses and estimates the turn-taking timing. To this end, we trained\na neural network to embed the lexical contents, the fundamental frequencies,\nand the speech power into a joint embedding space. To learn meaningful\nembedding spaces, the prosodic features from each single utterance\nare pre-trained using RNN and combined with utterance lexical embedding\nas the input of our proposed model. We tested this model on a spontaneous\nconversation dataset and confirmed that it outperformed the use of\nword embedding-based features.\n"
      ],
      "doi": "10.21437/Interspeech.2017-965"
    },
    "inaguma17_interspeech": {
      "authors": [
        [
          "Hirofumi",
          "Inaguma"
        ],
        [
          "Koji",
          "Inoue"
        ],
        [
          "Masato",
          "Mimura"
        ],
        [
          "Tatsuya",
          "Kawahara"
        ]
      ],
      "title": "Social Signal Detection in Spontaneous Dialogue Using Bidirectional LSTM-CTC",
      "original": "0457",
      "page_count": 5,
      "order": 350,
      "p1": "1691",
      "pn": "1695",
      "abstract": [
        "Non-verbal speech cues such as laughter and fillers, which are collectively\ncalled social signals, play an important role in human communication.\nTherefore, detection of them would be useful for dialogue systems to\ninfer speaker&#8217;s intentions, emotions and engagements. The conventional\napproaches are based on frame-wise classifiers, which require precise\ntime-alignment of these events for training. This work investigates\nthe Connectionist Temporal Classification (CTC) approach which can\nlearn an alignment between the input and its target label sequence.\nThis allows for robust detection of the events and efficient training\nwithout precise time information. Experimental evaluations with various\nsettings demonstrate that CTC based on bidirectional LSTM outperforms\nthe conventional DNN and HMM based methods.\n"
      ],
      "doi": "10.21437/Interspeech.2017-457"
    },
    "rahimi17_interspeech": {
      "authors": [
        [
          "Zahra",
          "Rahimi"
        ],
        [
          "Anish",
          "Kumar"
        ],
        [
          "Diane",
          "Litman"
        ],
        [
          "Susannah",
          "Paletz"
        ],
        [
          "Mingzhi",
          "Yu"
        ]
      ],
      "title": "Entrainment in Multi-Party Spoken Dialogues at Multiple Linguistic Levels",
      "original": "1568",
      "page_count": 5,
      "order": 351,
      "p1": "1696",
      "pn": "1700",
      "abstract": [
        "Linguistic entrainment, the phenomena whereby dialogue partners speak\nmore similarly to each other in a variety of dimensions, is key to\nthe success and naturalness of interactions. While there is considerable\nevidence for both lexical and acoustic-prosodic entrainment, little\nwork has been conducted to investigate the relationship between these\ntwo different modalities using the same measures in the same dialogues,\nspecifically in multi-party dialogue. In this paper, we measure lexical\nand acoustic-prosodic entrainment for multi-party teams to explore\nwhether entrainment occurs at multiple levels during conversation and\nto understand the relationship between these two modalities.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1568"
    },
    "reverdy17_interspeech": {
      "authors": [
        [
          "Justine",
          "Reverdy"
        ],
        [
          "Carl",
          "Vogel"
        ]
      ],
      "title": "Measuring Synchrony in Task-Based Dialogues",
      "original": "1604",
      "page_count": 5,
      "order": 352,
      "p1": "1701",
      "pn": "1705",
      "abstract": [
        "In many contexts from casual everyday conversations to formal discussions,\npeople tend to repeat their interlocutors, and themselves. This phenomenon\nnot only yields random repetitions one might expect from a natural\nZipfian distribution of linguistic forms, but also projects underlying\ndiscourse mechanisms and rhythms that researchers have suggested establishes\nconversational involvement and may support communicative progress towards\nmutual understanding. In this paper, advances in an automated method\nfor assessing interlocutor synchrony in task-based Human-to-Human interactions\nare reported. The method focuses on dialogue structure, rather than\ntemporal distance, measuring repetition between speakers and their\ninterlocutors last n-turns (n = 1, however far back in the conversation\nthat might have been) rather than utterances during a prior window\nfixed by duration. The significance of distinct linguistic levels of\nrepetition are assessed by observing contrasts between actual and randomized\ndialogues, in order to provide a quantifying measure of communicative\nsuccess. Definite patterns of repetitions where identified, notably\nin contrasting the role of participants (as information giver or follower).\nThe extent to which those interacted sometime surprisingly with gender,\neye-contact and familiarity is the principal contribution of this work.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1604"
    },
    "crook17_interspeech": {
      "authors": [
        [
          "Paul",
          "Crook"
        ],
        [
          "Alex",
          "Marin"
        ]
      ],
      "title": "Sequence to Sequence Modeling for User Simulation in Dialog Systems",
      "original": "0161",
      "page_count": 5,
      "order": 353,
      "p1": "1706",
      "pn": "1710",
      "abstract": [
        "User simulators are a principal offline method for training and evaluating\nhuman-computer dialog systems. In this paper, we examine simple sequence-to-sequence\nneural network architectures for training end-to-end, natural language\nto natural language, user simulators, using only raw logs of previous\ninteractions without any additional human labelling. We compare the\nneural network-based simulators with a language model (LM)-based approach\nfor creating natural language user simulators. Using both an automatic\nevaluation using LM perplexity and a human evaluation, we demonstrate\nthat the sequence-to-sequence approaches outperform the LM-based method.\nWe show correlation between LM perplexity and the human evaluation\non this task, and discuss the benefits of different neural network\narchitecture variations.\n"
      ],
      "doi": "10.21437/Interspeech.2017-161"
    },
    "ramanarayanan17b_interspeech": {
      "authors": [
        [
          "Vikram",
          "Ramanarayanan"
        ],
        [
          "Patrick L.",
          "Lange"
        ],
        [
          "Keelan",
          "Evanini"
        ],
        [
          "Hillary R.",
          "Molloy"
        ],
        [
          "David",
          "Suendermann-Oeft"
        ]
      ],
      "title": "Human and Automated Scoring of Fluency, Pronunciation and Intonation During Human&#8211;Machine Spoken Dialog Interactions",
      "original": "1213",
      "page_count": 5,
      "order": 354,
      "p1": "1711",
      "pn": "1715",
      "abstract": [
        "We present a spoken dialog-based framework for the computer-assisted\nlanguage learning (CALL) of conversational English. In particular,\nwe leveraged the open-source HALEF dialog framework to develop a job\ninterview conversational application. We then used crowdsourcing to\ncollect multiple interactions with the system from non-native English\nspeakers. We analyzed human-rated scores of the recorded dialog data\non three different scoring dimensions critical to the delivery of conversational\nEnglish &#8212; fluency, pronunciation and intonation/stress &#8212;\nand further examined the efficacy of automatically-extracted, hand-curated\nspeech features in predicting each of these sub-scores. Machine learning\nexperiments showed that trained scoring models generally perform at\npar with the human inter-rater agreement baseline in predicting human-rated\nscores of conversational proficiency.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1213"
    },
    "ando17_interspeech": {
      "authors": [
        [
          "Atsushi",
          "Ando"
        ],
        [
          "Ryo",
          "Masumura"
        ],
        [
          "Hosana",
          "Kamiyama"
        ],
        [
          "Satoshi",
          "Kobashikawa"
        ],
        [
          "Yushi",
          "Aono"
        ]
      ],
      "title": "Hierarchical LSTMs with Joint Learning for Estimating Customer Satisfaction from Contact Center Calls",
      "original": "0725",
      "page_count": 5,
      "order": 355,
      "p1": "1716",
      "pn": "1720",
      "abstract": [
        "This paper presents a joint modeling of both turn-level and call-level\ncustomer satisfaction in contact center dialogue. Our key idea is to\ndirectly apply turn-level estimation results to call-level estimation\nand optimize them jointly; previous work treated both estimations as\nbeing independent. Proposed joint modeling is achieved by stacking\ntwo types of long short-term memory recurrent neural networks (LSTM-RNNs).\nThe lower layer employs LSTM-RNN for sequential labeling of turn-level\ncustomer satisfaction in which each label is estimated from context\ninformation extracted from not only the target turn but also the surrounding\nturns. The upper layer uses another LSTM-RNN to estimate call-level\ncustomer satisfaction labels from all information of estimated turn-level\ncustomer satisfaction. These two networks can be efficiently optimized\nby joint learning of both types of labels. Experiments show that the\nproposed method outperforms a conventional support vector machine based\nmethod in terms of both turn-level and call-level customer satisfaction\nwith relative error reductions of over 20%.\n"
      ],
      "doi": "10.21437/Interspeech.2017-725"
    },
    "ultes17_interspeech": {
      "authors": [
        [
          "Stefan",
          "Ultes"
        ],
        [
          "Pawe\u0142",
          "Budzianowski"
        ],
        [
          "I\u00f1igo",
          "Casanueva"
        ],
        [
          "Nikola",
          "Mrk\u0161i\u0107"
        ],
        [
          "Lina",
          "Rojas-Barahona"
        ],
        [
          "Pei-Hao",
          "Su"
        ],
        [
          "Tsung-Hsien",
          "Wen"
        ],
        [
          "Milica",
          "Ga\u0161i\u0107"
        ],
        [
          "Steve",
          "Young"
        ]
      ],
      "title": "Domain-Independent User Satisfaction Reward Estimation for Dialogue Policy Learning",
      "original": "1032",
      "page_count": 5,
      "order": 356,
      "p1": "1721",
      "pn": "1725",
      "abstract": [
        "Learning suitable and well-performing dialogue behaviour in statistical\nspoken dialogue systems has been in the focus of research for many\nyears. While most work which is based on reinforcement learning employs\nan objective measure like task success for modelling the reward signal,\nwe propose to use a reward based on user satisfaction. We will show\nin simulated experiments that a live user satisfaction estimation model\nmay be applied resulting in higher estimated satisfaction whilst achieving\nsimilar success rates. Moreover, we will show that one satisfaction\nestimation model which has been trained on one domain may be applied\nin many other domains which cover a similar task. We will verify our\nfindings by employing the model to one of the domains for learning\na policy from real users and compare its performance to policies using\nthe user satisfaction and task success acquired directly from the users\nas reward.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1032"
    },
    "nakamura17_interspeech": {
      "authors": [
        [
          "Shizuka",
          "Nakamura"
        ],
        [
          "Ryosuke",
          "Nakanishi"
        ],
        [
          "Katsuya",
          "Takanashi"
        ],
        [
          "Tatsuya",
          "Kawahara"
        ]
      ],
      "title": "Analysis of the Relationship Between Prosodic Features of Fillers and its Forms or Occurrence Positions",
      "original": "1006",
      "page_count": 5,
      "order": 357,
      "p1": "1726",
      "pn": "1730",
      "abstract": [
        "Fillers are involved in the ease of understanding by listeners and\nturn-taking. However, the knowledge about its prosodic features is\ninsufficient, and its modeling has not been done either. For these\nreasons, there is insufficient knowledge to generate natural and appropriate\nfillers in a dialog system at present. Therefore, for the purpose of\nclarifying the prosodic features of fillers, its relationship with\noccurrence positions or forms were analyzed in this research. &#8216;Ano&#8217;\nand &#8216;Eto&#8217; were used as forms, non-/boundary of Dialog Act\nand non-/turn-taking for occurrence positions. Duration, F0, and intensity\nwere utilized as prosodic features. As a result, the followings were\nfound out: the prosodic features are different depending on the difference\nof the occurrence positions even for fillers of the same form, and\nsimilar prosodic features are found between the same occurrence positions\neven in different forms.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1006"
    },
    "fatima17_interspeech": {
      "authors": [
        [
          "Syeda Narjis",
          "Fatima"
        ],
        [
          "Engin",
          "Erzin"
        ]
      ],
      "title": "Cross-Subject Continuous Emotion Recognition Using Speech and Body Motion in Dyadic Interactions",
      "original": "1413",
      "page_count": 5,
      "order": 358,
      "p1": "1731",
      "pn": "1735",
      "abstract": [
        "Dyadic interactions encapsulate rich emotional exchange between interlocutors\nsuggesting a multimodal, cross-speaker and cross-dimensional continuous\nemotion dependency. This study explores the dynamic inter-attribute\nemotional dependency at the cross-subject level with implications to\ncontinuous emotion recognition based on speech and body motion cues.\nWe propose a novel two-stage Gaussian Mixture Model mapping framework\nfor the continuous emotion recognition problem. In the first stage,\nwe perform continuous emotion recognition (CER) of both speakers from\nspeech and body motion modalities to estimate activation, valence and\ndominance (AVD) attributes. In the second stage, we improve the first\nstage estimates by performing CER of the selected speaker using her/his\nspeech and body motion modalities as well as using the estimated affective\nattribute(s) of the other speaker. Our experimental evaluations indicate\nthat the second stage, cross-subject continuous emotion recognition\n(CSCER), provides complementary information to recognize the affective\nstate, and delivers promising improvements for the continuous emotion\nrecognition problem.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1413"
    },
    "elsner17_interspeech": {
      "authors": [
        [
          "Micha",
          "Elsner"
        ],
        [
          "Kiwako",
          "Ito"
        ]
      ],
      "title": "An Automatically Aligned Corpus of Child-Directed Speech",
      "original": "0379",
      "page_count": 5,
      "order": 359,
      "p1": "1736",
      "pn": "1740",
      "abstract": [
        "Forced alignment would enable phonetic analyses of child directed speech\n(CDS) corpora which have existing transcriptions. But existing alignment\nsystems are inaccurate due to the atypical phonetics of CDS. We adapt\na Kaldi forced alignment system to CDS by extending the dictionary\nand providing it with heuristically-derived hints for vowel locations.\nUsing this system, we present a new time-aligned CDS corpus with a\nmillion aligned segments. We manually correct a subset of the corpus\nand demonstrate that our system is 70% accurate. Both our automatic\nand manually corrected alignments are publically available at  osf.io/ke44q.\n"
      ],
      "doi": "10.21437/Interspeech.2017-379"
    },
    "bohn17_interspeech": {
      "authors": [
        [
          "Ocke-Schwen",
          "Bohn"
        ],
        [
          "Trine",
          "Askj\u00e6r-J\u00f8rgensen"
        ]
      ],
      "title": "A Comparison of Danish Listeners&#8217; Processing Cost in Judging the Truth Value of Norwegian, Swedish, and English Sentences",
      "original": "0009",
      "page_count": 4,
      "order": 360,
      "p1": "1741",
      "pn": "1744",
      "abstract": [
        "The present study used a sentence verification task to assess the processing\ncost involved in native Danish listeners&#8217; attempts to comprehend\ntrue/false statements spoken in Danish, Norwegian, Swedish, and English.\nThree groups of native Danish listeners heard 40 sentences each which\nwere translation equivalents, and assessed the truth value of these\nstatements. Group 1 heard sentences in Danish and Norwegian, Group\n2 in Danish and Swedish, and Group 3 in Danish and English. Response\ntime and proportion of correct responses were used as indices of processing\ncost. Both measures indicate that the processing cost for native Danish\nlisteners in comprehending Danish and English statements is equivalent,\nwhereas Norwegian and Swedish statements incur a much higher cost,\nboth in terms of response time and correct assessments. The results\nare discussed with regard to the costs of inter-Scandinavian and English\nlingua franca communication.\n"
      ],
      "doi": "10.21437/Interspeech.2017-9"
    },
    "kleber17_interspeech": {
      "authors": [
        [
          "Felicitas",
          "Kleber"
        ]
      ],
      "title": "On the Role of Temporal Variability in the Acquisition of the German Vowel Length Contrast",
      "original": "1282",
      "page_count": 5,
      "order": 361,
      "p1": "1745",
      "pn": "1749",
      "abstract": [
        "This study is part of a larger project investigating the acquisition\nof stable vowel-plus-consonant timing patterns needed to convey the\nphonemic vowel length and the voicing contrast in German. The research\nis motivated by findings showing greater temporal variability in children\nuntil the age of 12. The specific aims of the current study were to\ntest (1) whether temporal variability in the production of the vowel\nlength contrast decreases with increasing age (in general and more\nso when the variability is speech rate induced) and (2) whether duration\ncues are perceived more categorically with increasing age. Production\nand perception data were obtained from eleven preschool, five school\nchildren and eleven adults. Results revealed that children produce\nthe quantity contrast with temporal patterns that are similar to adults&#8217;\npatterns, although vowel duration was overall longer and variability\nslightly higher in faster speech and younger children. Apart from that,\nthe two groups of children did not differ in production. In perception,\nhowever, school children&#8217;s response patterns to a continuum from\na long vowel to a short vowel word were in between those of adults\nand preschool children. Findings are discussed with respect to motor\ncontrol and phonemic abstraction.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1282"
    },
    "reidy17_interspeech": {
      "authors": [
        [
          "Patrick F.",
          "Reidy"
        ],
        [
          "Mary E.",
          "Beckman"
        ],
        [
          "Jan",
          "Edwards"
        ],
        [
          "Benjamin",
          "Munson"
        ]
      ],
      "title": "A Data-Driven Approach for Perceptually Validated Acoustic Features for Children&#8217;s Sibilant Fricative Productions",
      "original": "1607",
      "page_count": 5,
      "order": 362,
      "p1": "1750",
      "pn": "1754",
      "abstract": [
        "Both perceptual and acoustic studies of children&#8217;s speech independently\nsuggest that phonological contrasts are continuously refined during\nacquisition. This paper considers two traditional acoustic features\nfor the &#8216;s&#8217;-vs.-&#8216;sh&#8217; contrast (centroid and\npeak frequencies) and a novel feature learned from data, evaluating\nthese features relative to perceptual ratings of children&#8217;s productions.<br\nstyle=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  Productions of sibilant fricatives were elicited from 16 adults\nand 69 preschool children. A second group of adults rated the children&#8217;s\nproductions on a visual analog scale (VAS). Each production was rated\nby multiple listeners; mean VAS score for each production was used\nas its perceptual goodness rating. For each production from the repetition\ntask, a psychoacoustic spectrum was estimated by passing it through\na filter bank that modeled the auditory periphery. From these spectra\ncentroid and peak frequencies were computed, two traditional features\nfor a sibilant fricative&#8217;s place of articulation. A novel acoustic\nmeasure was derived by inputting the spectra to a graph-based dimensionality-reduction\nalgorithm.<br style=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  Simple regression analyses indicated that a greater amount of\nvariance in the VAS scores was explained by the novel feature (adjusted\nR<SUP>2</SUP> = 0.569) than by either centroid (adjusted R<SUP>2</SUP>\n= 0.468) or peak frequency (adjusted R<SUP>2</SUP> = 0.254).\n"
      ],
      "doi": "10.21437/Interspeech.2017-1607"
    },
    "xiao17_interspeech": {
      "authors": [
        [
          "Yujia",
          "Xiao"
        ],
        [
          "Frank K.",
          "Soong"
        ]
      ],
      "title": "Proficiency Assessment of ESL Learner&#8217;s Sentence Prosody with TTS Synthesized Voice as Reference",
      "original": "0064",
      "page_count": 5,
      "order": 363,
      "p1": "1755",
      "pn": "1759",
      "abstract": [
        "We investigate how to assess the prosody quality of an ESL learner&#8217;s\nspoken sentence against native speaker&#8217;s natural recording or\nTTS synthesized voice. A spoken English utterance read by an ESL leaner\nis compared with the recording of a native speaker, or TTS voice. The\ncorresponding F0 contours (with voicings) and breaks are compared at\nthe mapped syllable level via a DTW. The correlations between the prosody\npatterns of learner and native speaker (or TTS voice) of the same sentence\nare computed after the speech rates and F0 distributions between speakers\nare equalized. Based upon collected native and non-native speakers&#8217;\ndatabases and correlation coefficients, we use Gaussian mixtures to\nmodel them as continuous distributions for training a two-class (native\nvs non-native) neural net classifier. We found that classification\naccuracy between using native speaker&#8217;s and TTS reference is\nclose, i.e., 91.2% vs 88.1%. To assess the prosody proficiency of an\nESL learner with one sentence input, the prosody patterns of our high\nquality TTS is almost as effective as those of native speakers&#8217;\nrecordings, which are more expensive and inconvenient to collect.\n"
      ],
      "doi": "10.21437/Interspeech.2017-64"
    },
    "chen17g_interspeech": {
      "authors": [
        [
          "Si",
          "Chen"
        ],
        [
          "Yunjuan",
          "He"
        ],
        [
          "Chun Wah",
          "Yuen"
        ],
        [
          "Bei",
          "Li"
        ],
        [
          "Yike",
          "Yang"
        ]
      ],
      "title": "Mechanisms of Tone Sandhi Rule Application by Non-Native Speakers",
      "original": "0143",
      "page_count": 5,
      "order": 364,
      "p1": "1760",
      "pn": "1764",
      "abstract": [
        "This study is the first to examine acquisition of two Mandarin tone\nsandhi rules by Cantonese speakers. It designs both real and different\ntypes of wug words to test whether learners may exploit a lexical or\ncomputation mechanism in tone sandhi rule application. We also statistically\ncompared their speech production with Beijing Mandarin speakers. The\nresults of functional data analysis showed that non-native speakers\napplied tone sandhi rules both to real and wug words in a similar manner,\nindicating that they might utilize a computation mechanism and compute\nthe rules under phonological conditions. No significant differences\nin applying these two phonological rules on reading wug words also\nsuggest no bias in the application of these two rules. However, their\nspeech production differed from native speakers. The application of\nthird tone sandhi rule was more categorical than native speakers in\nthat Cantonese speakers tended to neutralize the sandhi Tone 3 more\nwith Tone 2 produced in isolation compared to native speakers. Also,\nCantonese speakers might not have applied half-third tone sandhi rule\nfully since they tended to raise f0 values more at the end of vowels.\n"
      ],
      "doi": "10.21437/Interspeech.2017-143"
    },
    "wiener17_interspeech": {
      "authors": [
        [
          "Seth",
          "Wiener"
        ]
      ],
      "title": "Changes in Early L2 Cue-Weighting of Non-Native Speech: Evidence from Learners of Mandarin Chinese",
      "original": "0289",
      "page_count": 5,
      "order": 365,
      "p1": "1765",
      "pn": "1769",
      "abstract": [
        "This study examined how cue-weighting of a non-native speech cue changes\nduring early adult second language (L2) acquisition. Ten native English\nspeaking learners of Mandarin Chinese performed a speeded AX-discrimination\ntask during months 1, 2, and 3 of a first-year Chinese course. Results\nwere compared to ten native Mandarin speakers. Learners&#8217; reaction\ntime and d-prime results became more native-like after two months of\nclassroom study but plateaued thereafter. Multidimensional scaling\nresults showed a similar shift to more native-like cue-weighting as\nlearners attended more to pitch direction and less to pitch height.\nDespite the improvements, learners&#8217; month 3 configuration of\ncue-weighting differed from that of native speakers; learners appeared\nto weight pitch end points rather than overall pitch directions. These\nresults suggest that learners&#8217; warping of the weights of dimensions\nunderlying the perceptual space changes rapidly during early acquisition\nand can plateau like other measures of L2 acquisition. Previous perceptual\nlearning studies may have only captured initial L2 perception gains,\nnot the learning plateau that often follows. New methods of perceptual\nlearning, especially for tonal languages, are needed to advance learners\noff the plateau.\n"
      ],
      "doi": "10.21437/Interspeech.2017-289"
    },
    "chen17h_interspeech": {
      "authors": [
        [
          "Ying",
          "Chen"
        ],
        [
          "Eric",
          "Pederson"
        ]
      ],
      "title": "Directing Attention During Perceptual Training: A Preliminary Study of Phonetic Learning in Southern Min by Mandarin Speakers",
      "original": "1600",
      "page_count": 5,
      "order": 366,
      "p1": "1770",
      "pn": "1774",
      "abstract": [
        "Previous studies have shown that directing learners&#8217; attention\nduring perceptual training facilitates detection and learning of unfamiliar\nconsonant categories [1, 2]. The current study asks whether this attentional\ndirecting can also facilitate other types of phonetic learning. Monolingual\nMandarin speakers were divided into two groups directed to learn either\n1) the consonants or 2) the tones in an identification training task\nwith the same set of Southern Min monosyllabic words containing the\nconsonants /p<SUP>h</SUP>, p, b, k<SUP>h</SUP>, k, &#609;, t&#597;<SUP>h</SUP>,\nt&#597;, &#597;/ and the tones (55, 33, 22, 24, 41). All subjects were\nalso tested with an AXB discrimination task (with a distinct set of\nSouthern Min words) before and after the training. Unsurprisingly,\nboth groups improved accuracy in the sound type to which they attended.\nHowever, the consonant-attending group did not improve in discriminating\ntones after training and neither did the tone-attending group in discriminating\nconsonants &#8212; despite both groups having equal exposure to the\nsame training stimuli. When combined with previous results for consonant\nand vowel training, these results suggest that explicitly directing\nlearners&#8217; attention has a broadly facilitative effect on phonetic\nlearning including of tonal contrasts.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1600"
    },
    "luo17b_interspeech": {
      "authors": [
        [
          "Dean",
          "Luo"
        ],
        [
          "Ruxin",
          "Luo"
        ],
        [
          "Lixin",
          "Wang"
        ]
      ],
      "title": "Prosody Analysis of L2 English for Naturalness Evaluation Through Speech Modification",
      "original": "0332",
      "page_count": 4,
      "order": 367,
      "p1": "1775",
      "pn": "1778",
      "abstract": [
        "This study investigates how different prosodic features affect native\nspeakers&#8217; naturalness judgement of L2 English speech by Chinese\nstudents. Through subjective judgment by native speakers and objectively\nmeasured prosodic features, timing and pitch related prosodic features,\nas well as segmental goodness of pronunciation have been found to play\nkey roles in native speakers&#8217; perception of naturalness. In order\nto eliminate segmental factors, we used accent conversion techniques\nthat modify native reference speech with learners&#8217; erroneous\nprosodic cues without altering segmental properties. Experimental results\nshow that without interference of segmental factors, both timing and\npitch features affect naturalness of L2 speech. Timing plays a more\ncrucial role in naturalness than pitch. Accent modification that corrects\ntiming or pitch errors can improve naturalness of the speech.\n"
      ],
      "doi": "10.21437/Interspeech.2017-332"
    },
    "grigonyte17_interspeech": {
      "authors": [
        [
          "Gintar\u0117",
          "Grigonyt\u0117"
        ],
        [
          "Gerold",
          "Schneider"
        ]
      ],
      "title": "Measuring Encoding Efficiency in Swedish and English Language Learner Speech Production",
      "original": "0337",
      "page_count": 5,
      "order": 368,
      "p1": "1779",
      "pn": "1783",
      "abstract": [
        "We use n-gram language models to investigate how far language approximates\nan optimal code for human communication in terms of Information Theory\n[1], and what differences there are between Learner proficiency levels.\nAlthough the language of lower level learners is simpler, it is less\noptimal in terms of information theory, and as a consequence more difficult\nto process.\n"
      ],
      "doi": "10.21437/Interspeech.2017-337"
    },
    "hanulikova17_interspeech": {
      "authors": [
        [
          "Adriana",
          "Hanul\u00edkov\u00e1"
        ],
        [
          "Jenny",
          "Ekstr\u00f6m"
        ]
      ],
      "title": "Lexical Adaptation to a Novel Accent in German: A Comparison Between German, Swedish, and Finnish Listeners",
      "original": "0369",
      "page_count": 5,
      "order": 369,
      "p1": "1784",
      "pn": "1788",
      "abstract": [
        "Listeners usually adjust rapidly to unfamiliar regional and foreign\naccents in their native (L1) language. Non-native (L2) listeners, however,\nusually struggle when confronted with unfamiliar accents in their non-native\nlanguage. The present study asks how native language background of\nL2 speakers influences lexical adjustments in a novel accent of German,\nin which several vowels were systematically lowered. We measured word\njudgments on a lexical decision task before and after exposure to a\n15-min story in the novel dialect, and compared German, Swedish and\nFinnish listeners&#8217; performance. Swedish is a Germanic language\nand shares with German a number of lexical roots and a relatively large\nvowel inventory. Finnish is a Finno-Ugric language and differs substantially\nfrom Germanic languages in both lexicon and phonology. The results\nwere as predicted: descriptively, all groups showed a similar pattern\nof adaptation to the accented speech, but only German and Swedish participants\nshowed a significant effect. Lexical and phonological relatedness between\nthe native and non-native languages may thus positively influence lexical\nadaptation in an unfamiliar accent.\n"
      ],
      "doi": "10.21437/Interspeech.2017-369"
    },
    "fernandez17_interspeech": {
      "authors": [
        [
          "Alejandra Keidel",
          "Fern\u00e1ndez"
        ],
        [
          "Thomas",
          "H\u00f6rberg"
        ]
      ],
      "title": "Qualitative Differences in L3 Learners&#8217; Neurophysiological Response to L1 versus L2 Transfer",
      "original": "0743",
      "page_count": 5,
      "order": 370,
      "p1": "1789",
      "pn": "1793",
      "abstract": [
        "Third language (L3) acquisition differs from first language (L1) and\nsecond language (L2) acquisition. There are different views on whether\nL1 or L2 is of primary influence on L3 acquisition in terms of transfer.\nThis study examines differences in the event-related brain potentials\n(ERP) response to agreement incongruencies between L1 Spanish speakers\nand L3 Spanish learners, comparing response differences to incongruencies\nthat are transferrable from the learners&#8217; L1 (Swedish), or their\nL2 (English). Whereas verb incongruencies, available in L3 learners&#8217;\nL2 but not their L1, engendered a similar response for L1 speakers\nand L3 learners, adjective incongruencies, available in L3 learners&#8217;\nL1 but not their L2, elicited responses that differed between groups:\nAdjective incongruencies engendered a negativity in the 450&#8211;550\nms time window for L1 speakers only. Both congruent and incongruent\nadjectives also engendered an enhanced P3 wave in L3 learners compared\nto L1 speakers. Since the P300 correlates with task-related, strategic\nprocessing, this indicates that L3 learners process grammatical features\nthat are transferrable from their L1 in a less automatic mode than\nfeatures that are transferrable from their L2. L3 learners therefore\nseem to benefit more from their knowledge of their L2 than their knowledge\nof their L1.\n"
      ],
      "doi": "10.21437/Interspeech.2017-743"
    },
    "sjons17_interspeech": {
      "authors": [
        [
          "Johan",
          "Sjons"
        ],
        [
          "Thomas",
          "H\u00f6rberg"
        ],
        [
          "Robert",
          "\u00d6stling"
        ],
        [
          "Johannes",
          "Bjerva"
        ]
      ],
      "title": "Articulation Rate in Swedish Child-Directed Speech Increases as a Function of the Age of the Child Even When Surprisal is Controlled for",
      "original": "1052",
      "page_count": 5,
      "order": 371,
      "p1": "1794",
      "pn": "1798",
      "abstract": [
        "In earlier work, we have shown that articulation rate in Swedish child-directed\nspeech (CDS) increases as a function of the age of the child, even\nwhen utterance length and differences in articulation rate between\nsubjects are controlled for. In this paper we show on utterance level\nin spontaneous Swedish speech that i) for the youngest children, articulation\nrate in CDS is lower than in adult-directed speech (ADS), ii) there\nis a significant negative correlation between articulation rate and\nsurprisal (the negative log probability) in ADS, and iii) the increase\nin articulation rate in Swedish CDS as a function of the age of the\nchild holds, even when surprisal along with utterance length and differences\nin articulation rate between speakers are controlled for. These results\nindicate that adults adjust their articulation rate to make it fit\nthe linguistic capacity of the child.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1052"
    },
    "zhang17e_interspeech": {
      "authors": [
        [
          "Kaile",
          "Zhang"
        ],
        [
          "Gang",
          "Peng"
        ]
      ],
      "title": "The Relationship Between the Perception and Production of Non-Native Tones",
      "original": "0714",
      "page_count": 5,
      "order": 372,
      "p1": "1799",
      "pn": "1803",
      "abstract": [
        "To further investigate the relationship between non-native tone perception\nand production, the present study trained Mandarin speakers to learn\nCantonese lexical tones with a speech shadowing paradigm. After two\nweeks&#8217; training, both Mandarin speakers&#8217; Cantonese tone\nperception and their production had improved significantly. The overall\nperformances in Cantonese tone perception and production are moderately\ncorrelated, but the degree of performance change after training among\nthe two modalities shows no correlation, suggesting that non-native\ntone perception and production might be partially correlated, but that\nthe improvement of the two modalities is not synchronous. A comparison\nbetween the present study and previous studies on non-native tone learning\nindicates that experience in lexical tone processing might be important\nin forming the correlation between tone perception and production.\nMandarin speakers showed greater improvement in Cantonese tone perception\nthan in production after training, indicating that second language\n(L2) perception might precede production. Besides, both the first language\n(L1) and L2 tonal systems showed an influence on Mandarin speakers&#8217;\nlearning of Cantonese tones.\n"
      ],
      "doi": "10.21437/Interspeech.2017-714"
    },
    "marklund17_interspeech": {
      "authors": [
        [
          "Ellen",
          "Marklund"
        ],
        [
          "El\u00edsabet Eir",
          "Cortes"
        ],
        [
          "Johan",
          "Sjons"
        ]
      ],
      "title": "MMN Responses in Adults After Exposure to Bimodal and Unimodal Frequency Distributions of Rotated Speech",
      "original": "1110",
      "page_count": 5,
      "order": 373,
      "p1": "1804",
      "pn": "1808",
      "abstract": [
        "The aim of the present study is to further the understanding of the\nrelationship between perceptual categorization and exposure to different\nfrequency distributions of sounds. Previous studies have shown that\nspeech sound discrimination proficiency is influenced by exposure to\ndifferent distributions of speech sound continua varying along one\nor several acoustic dimensions, both in adults and in infants. In the\ncurrent study, adults were presented with either a bimodal or a unimodal\nfrequency distribution of spectrally rotated sounds along a continuum\n(a vowel continuum before rotation). Categorization of the sounds,\nquantified as amplitude of the event-related potential (ERP) component\nmismatch negativity (MMN) in response to two of the sounds, was measured\nbefore and after exposure. It was expected that the bimodal group would\nhave a larger MMN amplitude after exposure whereas the unimodal group\nwould have a smaller MMN amplitude after exposure. Contrary to expectations,\nthe MMN amplitude was smaller overall after exposure, and no difference\nwas found between groups. This suggests that either the previously\nreported sensitivity to frequency distributions of speech sounds is\nnot present for non-speech sounds, or the MMN amplitude is not a sensitive\nenough measure of categorization to detect an influence from passive\nexposure, or both.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1110"
    },
    "berisha17_interspeech": {
      "authors": [
        [
          "Visar",
          "Berisha"
        ],
        [
          "Julie",
          "Liss"
        ],
        [
          "Timothy",
          "Huston"
        ],
        [
          "Alan",
          "Wisler"
        ],
        [
          "Yishan",
          "Jiao"
        ],
        [
          "Jonathan",
          "Eig"
        ]
      ],
      "title": "Float Like a Butterfly Sting Like a Bee: Changes in Speech Preceded Parkinsonism Diagnosis for Muhammad Ali",
      "original": "0025",
      "page_count": 5,
      "order": 374,
      "p1": "1809",
      "pn": "1813",
      "abstract": [
        "Early identification of the onset of neurological disease is critical\nfor testing drugs or interventions to halt or slow progression. Speech\nproduction has been proposed as an early indicator of neurological\nimpairment. However, for speech to be useful for early detection, speech\nchanges should be measurable from uncontrolled conversational speech\ncollected passively in natural recording environments over extended\nperiods of time. Such longitudinal speech data sets for testing the\nrobustness of algorithms are difficult to acquire. In this paper, we\nexploit YouTube interviews from Muhammad Ali from 1968 to 1981, before\nhis 1984 diagnosis of parkinsonism. The interviews are unscripted,\nconversational in nature, and of varying fidelity. We measured changes\nin speech production from the Ali interviews and analyzed these changes\nrelative to a coded registry of blows Mr. Ali received in each of his\nboxing matches over time. This provided a rich and unique opportunity\nto evaluate speech change as both a function of disease progression\nand as a function of fight history. Multivariate analyses revealed\nchanges in prosody and articulation consistent with hypokinetic dysarthria\nover time, and a relationship between reduced speech intonation and\nthe amount of time elapsed since the most recent fight preceding the\ninterview.\n"
      ],
      "doi": "10.21437/Interspeech.2017-25"
    },
    "castellana17_interspeech": {
      "authors": [
        [
          "Antonella",
          "Castellana"
        ],
        [
          "Andreas",
          "Selamtzis"
        ],
        [
          "Giampiero",
          "Salvi"
        ],
        [
          "Alessio",
          "Carullo"
        ],
        [
          "Arianna",
          "Astolfi"
        ]
      ],
      "title": "Cepstral and Entropy Analyses in Vowels Excerpted from Continuous Speech of Dysphonic and Control Speakers",
      "original": "0335",
      "page_count": 5,
      "order": 375,
      "p1": "1814",
      "pn": "1818",
      "abstract": [
        "There is a growing interest in Cepstral and Entropy analyses of voice\nsamples for defining a vocal health indicator, due to their reliability\nin investigating both regular and irregular voice signals. The purpose\nof this study is to determine whether the Cepstral Peak Prominence\nSmoothed (CPPS) and Sample Entropy (SampEn) could differentiate dysphonic\nspeakers from normal speakers in vowels excerpted from readings and\nto compare their discrimination power. Results are reported for 33\npatients and 31 controls, who read a standardized phonetically balanced\npassage while wearing a head mounted microphone. Vowels were excerpted\nfrom recordings using Automatic Speech Recognition and, after obtaining\na measure for each vowel, individual distributions and their descriptive\nstatistics were considered for CPPS and SampEn. The Receiver Operating\nCurve analysis revealed that the mean of the distributions was the\nparameter with the highest discrimination power for both CPPS and SampEn.\nCPPS showed a higher diagnostic precision than SampEn, exhibiting an\nArea Under Curve (AUC) of 0.85 compared to 0.72. A negative correlation\nbetween the parameters was found (Spearman; &#961; = -0.61), with higher\nSampEn corresponding to lower CPPS. The automatic method used in this\nstudy could provide support to voice monitorings in clinic and during\nindividual&#8217;s daily activities.\n"
      ],
      "doi": "10.21437/Interspeech.2017-335"
    },
    "bandini17b_interspeech": {
      "authors": [
        [
          "Andrea",
          "Bandini"
        ],
        [
          "Jordan R.",
          "Green"
        ],
        [
          "Lorne",
          "Zinman"
        ],
        [
          "Yana",
          "Yunusova"
        ]
      ],
      "title": "Classification of Bulbar ALS from Kinematic Features of the Jaw and Lips: Towards Computer-Mediated Assessment",
      "original": "0478",
      "page_count": 5,
      "order": 376,
      "p1": "1819",
      "pn": "1823",
      "abstract": [
        "Recent studies demonstrated that lip and jaw movements during speech\nmay provide important information for the diagnosis of amyotrophic\nlateral sclerosis (ALS) and for understanding its progression. A thorough\ninvestigation of these movements is essential for the development of\nintelligent video- or optically-based facial tracking systems that\ncould assist with early diagnosis and progress monitoring. In this\npaper, we investigated the potential for a novel and expanded set of\nkinematic features obtained from lips and jaw to classify articulatory\ndata into three stages of bulbar disease progression (i.e., pre-symptomatic,\nearly symptomatic, and late symptomatic). Feature selection methods\n(Relief-F and mRMR) and classification algorithm (SVM) were used for\nthis purpose. Results showed that even with a limited number of kinematic\nfeatures it was possible to obtain good classification accuracy (nearly\n80%). Given the recent development of video-based markerless methods\nfor tracking speech movements, these results provide strong rationale\nfor supporting the development of portable and cheap systems for monitoring\nthe orofacial function in ALS.\n"
      ],
      "doi": "10.21437/Interspeech.2017-478"
    },
    "adiga17_interspeech": {
      "authors": [
        [
          "Nagaraj",
          "Adiga"
        ],
        [
          "Vikram",
          "C.M."
        ],
        [
          "Keerthi",
          "Pullela"
        ],
        [
          "S.R. Mahadeva",
          "Prasanna"
        ]
      ],
      "title": "Zero Frequency Filter Based Analysis of Voice Disorders",
      "original": "0589",
      "page_count": 5,
      "order": 377,
      "p1": "1824",
      "pn": "1828",
      "abstract": [
        "Pitch period and amplitude perturbations are widely used parameters\nto discriminate normal and voice disorder speech. Instantaneous pitch\nperiod and amplitude of glottal vibrations directly from the speech\nwaveform may not give an accurate estimation of jitter and shimmer.\nIn this paper, the significance of epochs (glottal closure instants)\nand strength of excitation (SoE) derived from the zero-frequency filter\n(ZFF) are exploited to discriminate the voice disorder and normal speech.\nPitch epoch derived from ZFF is used to compute the jitter, and SoE\nderived around each epoch is used compute the shimmer. The derived\nepoch-based features are analyzed on the some of the voice disorders\nlike Parkinson&#8217;s disease, vocal fold paralysis, cyst, and gastroesophageal\nreflux disease. The significance of proposed epoch-based features for\ndiscriminating normal and pathological voices is analyzed and compared\nwith the state-of-the-art methods using a support vector machine classifier.\nThe results show that epoch-based features performed significantly\nbetter than other methods both in clean and noisy conditions.\n"
      ],
      "doi": "10.21437/Interspeech.2017-589"
    },
    "k17_interspeech": {
      "authors": [
        [
          "Nikitha",
          "K."
        ],
        [
          "Sishir",
          "Kalita"
        ],
        [
          "C.M.",
          "Vikram"
        ],
        [
          "M.",
          "Pushpavathi"
        ],
        [
          "S.R. Mahadeva",
          "Prasanna"
        ]
      ],
      "title": "Hypernasality Severity Analysis in Cleft Lip and Palate Speech Using Vowel Space Area",
      "original": "1245",
      "page_count": 5,
      "order": 378,
      "p1": "1829",
      "pn": "1833",
      "abstract": [
        "Vowel space area (VSA) refers to a two-dimensional area, which is bounded\nby lines joining F<SUB>1</SUB>and F<SUB>2</SUB> coordinates of vowels.\nIn the speech of individuals with cleft lip and palate (CLP), the effect\nof hypernasality introduces the pole-zero pairs in the speech spectrum,\nwhich will shift the formants of a target sound. As a result, vowel\nspace in hypernasal speech gets affected. In this work, analysis of\nvowel space area in normal, mild and moderate-severe hypernasality\ngroups is analyzed and compared across the three groups. Also, the\neffect of hypernasality severity ratings across different phonetic\ncontexts i.e,  /p/, /t/, and  /k/ is studied. The results revealed\nthat VSA is reduced in CLP children, compared to control participants,\nacross sustained vowels and different phonetic contexts. Compared to\nnormal, the reduction in the vowel space is more for the moderate-severe\nhypernasality group than that of mild. The CLP group exhibited a trend\nof having larger VSA for  /p/, followed by  /t/, and lastly by  /k/.\nThe statistical analysis revealed overall significant difference among\nthe three groups (p &#60; 0.05).\n"
      ],
      "doi": "10.21437/Interspeech.2017-1245"
    },
    "laaridh17_interspeech": {
      "authors": [
        [
          "Imed",
          "Laaridh"
        ],
        [
          "Waad Ben",
          "Kheder"
        ],
        [
          "Corinne",
          "Fredouille"
        ],
        [
          "Christine",
          "Meunier"
        ]
      ],
      "title": "Automatic Prediction of Speech Evaluation Metrics for Dysarthric Speech",
      "original": "1363",
      "page_count": 5,
      "order": 379,
      "p1": "1834",
      "pn": "1838",
      "abstract": [
        "During the last decades, automatic speech processing systems witnessed\nan important progress and achieved remarkable reliability. As a result,\nsuch technologies have been exploited in new areas and applications\nincluding medical practice. In disordered speech evaluation context,\nperceptual evaluation is still the most common method used in clinical\npractice for the diagnosing and the following of the condition progression\nof patients despite its well documented limits (such as subjectivity).<br\nstyle=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  In this paper, we propose an automatic approach for the prediction\nof dysarthric speech evaluation metrics (intelligibility, severity,\narticulation impairment) based on the representation of the speech\nacoustics in the total variability subspace based on the i-vectors\nparadigm. The proposed approach, evaluated on 129 French dysarthric\nspeakers from the  DesPhoAPady and  VML databases, is proven to be\nefficient for the modeling of patient&#8217;s production and capable\nof detecting the evolution of speech quality. Also, low RMSE and high\ncorrelation measures are obtained between automatically predicted metrics\nand perceptual evaluations.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1363"
    },
    "klumpp17_interspeech": {
      "authors": [
        [
          "Philipp",
          "Klumpp"
        ],
        [
          "Thomas",
          "Janu"
        ],
        [
          "Tom\u00e1s",
          "Arias-Vergara"
        ],
        [
          "J.C.",
          "V\u00e1squez-Correa"
        ],
        [
          "Juan Rafael",
          "Orozco-Arroyave"
        ],
        [
          "Elmar",
          "N\u00f6th"
        ]
      ],
      "title": "Apkinson &#8212; A Mobile Monitoring Solution for Parkinson&#8217;s Disease",
      "original": "0416",
      "page_count": 5,
      "order": 380,
      "p1": "1839",
      "pn": "1843",
      "abstract": [
        "In this paper we want to present our work on a smartphone application\nwhich aims to provide a mobile monitoring solution for patients suffering\nfrom Parkinson&#8217;s disease. By unobtrusively analyzing the speech\nsignal during phone calls and with a dedicated speech test, we want\nto be able to determine the severity and the progression of Parkinson&#8217;s\ndisease for a patient much more frequently than it would be possible\nwith regular check-ups.<br style=\"mso-data-placement:same-cell;\" /><br\nstyle=\"mso-data-placement:same-cell;\" />\n  The application consists\nof four major parts. There is a phone call detection which triggers\nthe whole processing chain. Secondly, there is the phone call recording\nwhich has proven to be more challenging than expected. The signal analysis,\nanother crucial component, is still in development for the phone call\nanalysis. Additionally, the application collects several pieces of\nmeta information about the calls to put the results into deeper context.<br\nstyle=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  After describing how the speech signal is affected by Parkinson&#8217;s\ndisease, we sketch the overall application architecture and explain\nthe four major parts of the current implementation in further detail.\nWe then present the promising results achieved with the first version\nof a dedicated speech test. In the end, we outline how the project\ncould receive further improvements in the future.\n"
      ],
      "doi": "10.21437/Interspeech.2017-416"
    },
    "hlavnicka17_interspeech": {
      "authors": [
        [
          "Jan",
          "Hlavni\u010dka"
        ],
        [
          "Tereza",
          "Tykalov\u00e1"
        ],
        [
          "Roman",
          "\u010cmejla"
        ],
        [
          "Ji\u0159\u00ed",
          "Klemp\u00ed\u0159"
        ],
        [
          "Ev\u017een",
          "R\u016f\u017ei\u010dka"
        ],
        [
          "Jan",
          "Rusz"
        ]
      ],
      "title": "Dysprosody Differentiate Between Parkinson&#8217;s Disease, Progressive Supranuclear Palsy, and Multiple System Atrophy",
      "original": "0762",
      "page_count": 5,
      "order": 381,
      "p1": "1844",
      "pn": "1848",
      "abstract": [
        "Parkinson&#8217;s disease (PD), progressive supranuclear palsy (PSP),\nand multiple system atrophy (MSA) are distinctive neurodegenerative\ndisorders, which manifest similar motor features. Their differentiation\nis crucial but difficult. Dysfunctional speech, especially dysprosody,\nis a common symptom accompanying PD, PSP, and MSA from early stages.\nWe hypothesized that automated analysis of monologue could provide\nspeech patterns distinguishing PD, PSP, and MSA. We analyzed speech\nrecordings of 16 patients with PSP, 20 patients with MSA, and 23 patients\nwith PD. Our findings revealed that deviant pause production differentiated\nbetween PSP, MSA, and PD. In addition, PSP showed greater deficits\nin speech respiration when compared to MSA and PD. Automated analysis\nof connected speech is easy to administer and could provide valuable\ninformation about underlying pathology for differentiation between\nPSP, MSA, and PD.\n"
      ],
      "doi": "10.21437/Interspeech.2017-762"
    },
    "tu17b_interspeech": {
      "authors": [
        [
          "Ming",
          "Tu"
        ],
        [
          "Visar",
          "Berisha"
        ],
        [
          "Julie",
          "Liss"
        ]
      ],
      "title": "Interpretable Objective Assessment of Dysarthric Speech Based on Deep Neural Networks",
      "original": "1222",
      "page_count": 5,
      "order": 382,
      "p1": "1849",
      "pn": "1853",
      "abstract": [
        "Improved performance in speech applications using deep neural networks\n(DNNs) has come at the expense of reduced model interpretability. For\nconsumer applications this is not a problem; however, for health applications,\nclinicians must be able to interpret why a predictive model made the\ndecision that it did. In this paper, we propose an interpretable model\nfor objective assessment of dysarthric speech for speech therapy applications\nbased on DNNs. Our model aims to predict a general impression of the\nseverity of the speech disorder; however, instead of directly generating\na severity prediction from a high-dimensional input acoustic feature\nspace, we add an intermediate interpretable layer that acts as a bottle-neck\nfeature extractor and constrains the solution space of the DNNs. During\ninference, the model provides an estimate of severity at the output\nof the network and a set of explanatory features from the intermediate\nlayer of the network that explain the final decision. We evaluate the\nperformance of the model on a dysarthric speech dataset and show that\nthe proposed model provides an interpretable output that is highly\ncorrelated with the subjective evaluation of Speech-Language Pathologists\n(SLPs).\n"
      ],
      "doi": "10.21437/Interspeech.2017-1222"
    },
    "vachhani17_interspeech": {
      "authors": [
        [
          "Bhavik",
          "Vachhani"
        ],
        [
          "Chitralekha",
          "Bhat"
        ],
        [
          "Biswajit",
          "Das"
        ],
        [
          "Sunil Kumar",
          "Kopparapu"
        ]
      ],
      "title": "Deep Autoencoder Based Speech Features for Improved Dysarthric Speech Recognition",
      "original": "1318",
      "page_count": 5,
      "order": 383,
      "p1": "1854",
      "pn": "1858",
      "abstract": [
        "Dysarthria is a motor speech disorder, resulting in mumbled, slurred\nor slow speech that is generally difficult to understand by both humans\nand machines. Traditional Automatic Speech Recognizers (ASR) perform\npoorly on dysarthric speech recognition tasks. In this paper, we propose\nthe use of deep autoencoders to enhance the Mel Frequency Cepstral\nCoefficients (MFCC) based features in order to improve dysarthric speech\nrecognition. Speech from healthy control speakers is used to train\nan autoencoder which is in turn used to obtain improved feature representation\nfor dysarthric speech. Additionally, we analyze the use of severity\nbased tempo adaptation followed by autoencoder based speech feature\nenhancement. All evaluations were carried out on Universal Access dysarthric\nspeech corpus. An overall absolute improvement of 16% was achieved\nusing tempo adaptation followed by autoencoder based speech front end\nrepresentation for DNN-HMM based dysarthric speech recognition.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1318"
    },
    "lilley17_interspeech": {
      "authors": [
        [
          "Jason",
          "Lilley"
        ],
        [
          "Madhavi",
          "Ratnagiri"
        ],
        [
          "H. Timothy",
          "Bunnell"
        ]
      ],
      "title": "Prediction of Speech Delay from Acoustic Measurements",
      "original": "1740",
      "page_count": 5,
      "order": 384,
      "p1": "1859",
      "pn": "1863",
      "abstract": [
        "Speech delay is characterized by a difficulty with producing or perceiving\nthe sounds of language in comparison to one&#8217;s peers. It is a\ncommon problem in young children, occurring at a rate of about 5%.\nThere are high rates of co-occurring problems with language, reading,\nlearning, and social interactions, so intervention is needed for most.\nThe Goldman-Fristoe Test of Articulation (GFTA) is a standardized tool\nfor the assessment of consonant articulation in American English children.\nGFTA scores are normalized for age and can be used to help diagnose\nand assess speech delay. The GFTA was administered to 65 young children,\na mixture of delayed children and controls. Their productions of the\n39 GFTA words spoken in isolation were recorded and aligned to 3-state\nhidden Markov models. Seven measurements (state log likelihoods, state\ndurations, and total duration) were extracted from each target segment\nin each word. From a subset of these measures, cross-validated statistical\nmodels were used to predict the children&#8217;s GFTA scores and whether\nthey were delayed. The measurements most useful for prediction came\nprimarily from approximants /r, l/. An analysis of the predictors and\ndiscussion of the implications will be provided.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1740"
    },
    "li17h_interspeech": {
      "authors": [
        [
          "Aijun",
          "Li"
        ],
        [
          "Hua",
          "Zhang"
        ],
        [
          "Wen",
          "Sun"
        ]
      ],
      "title": "The Frequency Range of &#8220;The Ling Six Sounds&#8221; in Standard Chinese",
      "original": "0329",
      "page_count": 5,
      "order": 385,
      "p1": "1864",
      "pn": "1868",
      "abstract": [
        "&#8220;The Ling Six Sounds&#8221; are a range of speech sounds encompassing\nthe speech frequencies that are widely used clinically to verify the\neffectiveness of hearing aid fitting in children. This study focused\non the spectral features of the six sounds in Standard Chinese. We\nexamined the frequency range of /m, u, a, i, &#642;, s/ as well as\nthree consonants in syllables, i.e., /m(o)/, /&#642;(&#x285;)/, and\n/s(&#x27f;)/. We presented the frequency distribution of these sounds.\nBased on this, we further proposed guidelines to improve &#8220;the\nLing Six-Sound Test&#8221; regarding tones in Standard Chinese. We\nalso suggested further studies in other dialects/languages spoken in\nChina with regard to their phonological specifics.\n"
      ],
      "doi": "10.21437/Interspeech.2017-329"
    },
    "gu17b_interspeech": {
      "authors": [
        [
          "Wentao",
          "Gu"
        ],
        [
          "Jiao",
          "Yin"
        ],
        [
          "James",
          "Mahshie"
        ]
      ],
      "title": "Production of Sustained Vowels and Categorical Perception of Tones in Mandarin Among Cochlear-Implanted Children",
      "original": "1698",
      "page_count": 5,
      "order": 386,
      "p1": "1869",
      "pn": "1873",
      "abstract": [
        "This study investigated both production and perception of Mandarin\nspeech, comparing two groups of 4-to-5-year-old children, a normal-hearing\n(NH) group and a cochlear-implanted (CI) hearing-impaired group; the\nperception ability of the CI group was tested under two conditions,\nwith and without hearing aids. In the production study, the participants\nwere asked to produce sustained vowels /a/, /i/ and /u/, on which a\nset of acoustic parameters were then measured. In comparison to the\nNH group, the CI group showed a higher F<SUB>0</SUB>, a higher H1-H2,\nand a smaller acoustic space for vowels, demonstrating both phonatory\nand articulatory impairments. In the perception study, the identification\ntests of two tone-pairs in Mandarin (T1-T2 and T1-T4) were conducted,\nusing two sets of synthetic speech stimuli varying only along F<SUB>0</SUB>\ncontinua. All groups/conditions showed categorical effects in perception.\nThe CI group in the unimodal condition showed little difference from\nnormal, while in the bimodal condition the categorical effect became\nweaker in identifying the T1-T4 continuum, with the category boundary\nmore biased to T4. This suggests that bimodal CI children may need\nmore fine grain adjustments of hearing aids to take full advantage\nof the bimodal technology.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1698"
    },
    "kumar17b_interspeech": {
      "authors": [
        [
          "Anurag",
          "Kumar"
        ],
        [
          "Benjamin",
          "Elizalde"
        ],
        [
          "Bhiksha",
          "Raj"
        ]
      ],
      "title": "Audio Content Based Geotagging in Multimedia",
      "original": "0040",
      "page_count": 5,
      "order": 387,
      "p1": "1874",
      "pn": "1878",
      "abstract": [
        "In this paper we propose methods to extract geographically relevant\ninformation in a multimedia recording using its audio content. Our\nmethod primarily is based on the fact that urban acoustic environment\nconsists of a variety of sounds. Hence, location information can be\ninferred from the composition of sound events/classes present in the\naudio. More specifically, we adopt matrix factorization techniques\nto obtain semantic content of recording in terms of different sound\nclasses. We use semi-NMF to for to do audio semantic content analysis\nusing MFCCs. These semantic information are then combined to identify\nthe location of recording. We show that these semantic content based\ngeotagging can perform significantly better than state of art methods.\n"
      ],
      "doi": "10.21437/Interspeech.2017-40"
    },
    "huang17d_interspeech": {
      "authors": [
        [
          "Zhaoqiong",
          "Huang"
        ],
        [
          "Zhanzhong",
          "Cao"
        ],
        [
          "Dongwen",
          "Ying"
        ],
        [
          "Jielin",
          "Pan"
        ],
        [
          "Yonghong",
          "Yan"
        ]
      ],
      "title": "Time Delay Histogram Based Speech Source Separation Using a Planar Array",
      "original": "0055",
      "page_count": 5,
      "order": 388,
      "p1": "1879",
      "pn": "1883",
      "abstract": [
        "Bin-wise time delay is a valuable clue to form the time-frequency (TF)\nmask for speech source separation on the two-microphone array. On widely\nspaces microphones, however, the time delay estimation suffers from\nspatial aliasing. Although histogram is a simple and effective method\nto tackle the problem of spatial aliasing, it can not be directly applied\non planar arrays. This paper proposes a histogram-based method to separate\nmultiple speech sources on the arbitrary-size planar array, where the\nspatial aliasing is resisted. Time delay histogram is firstly utilized\nto estimate the delays of multiple sources on each microphone pair.\nThe estimated delays on all pairs are then incorporated into an azimuth\nhistogram by means of the pairwise combination test. From the azimuth\nhistogram, the direction-of-arrivals (DOAs) and the number of sources\nare obtained. Eventually, the TF mask is determined based on the estimated\nDOAs. Some experiments were conducted under various conditions, confirming\nthe superiority of the proposed method.\n"
      ],
      "doi": "10.21437/Interspeech.2017-55"
    },
    "pradhan17_interspeech": {
      "authors": [
        [
          "Gayadhar",
          "Pradhan"
        ],
        [
          "Avinash",
          "Kumar"
        ],
        [
          "S.",
          "Shahnawazuddin"
        ]
      ],
      "title": "Excitation Source Features for Improving the Detection of Vowel Onset and Offset Points in a Speech Sequence",
      "original": "0135",
      "page_count": 5,
      "order": 389,
      "p1": "1884",
      "pn": "1888",
      "abstract": [
        "The task of detecting the vowel regions in a given speech signal is\na challenging problem. Over the years, several works on accurate detection\nof vowel regions and the corresponding vowel onset points (VOPs) and\nvowel end points (VEPs) have been reported. A novel front-end feature\nextraction technique exploiting the temporal and spectral characteristics\nof the excitation source information in the speech signal is proposed\nin this paper to improve the detection of vowel regions, VOPs and VEPs.\nTo do the same, a three-class classifiers (vowel, non-vowel and silence)\nis developed on the TIMIT database using the proposed features as well\nas mel-frequency cepstral coefficients (MFCC). Statistical modeling\nbased on deep neural network has been employed for learning the parameters.\nUsing the developed three-class classifier, a given speech sample is\nthen forced aligned against the trained acoustic models to detect the\nvowel regions. The use of proposed feature results in detection of\nvowel regions quite different from those obtained through the MFCC.\nExploiting the differences in the evidences obtained by using the two\nkinds of features, a technique to combine the evidences is also proposed\nin order to get a better estimate of the VOPs and VEPs.\n"
      ],
      "doi": "10.21437/Interspeech.2017-135"
    },
    "gao17_interspeech": {
      "authors": [
        [
          "Wei",
          "Gao"
        ],
        [
          "Roberto",
          "Togneri"
        ],
        [
          "Victor",
          "Sreeram"
        ]
      ],
      "title": "A Contrast Function and Algorithm for Blind Separation of Audio Signals",
      "original": "0189",
      "page_count": 5,
      "order": 390,
      "p1": "1889",
      "pn": "1893",
      "abstract": [
        "This paper presents a contrast function and associated algorithm for\nblind separation of audio signals. The contrast function is based on\nsecond-order statistics to minimize the ratio between the product of\nthe diagonal entries and the determinant of the covariance matrix.\nThe contrast function can be minimized by a batch and adaptive gradient\ndescent method to formulate a blind source separation algorithm. Experimental\nresults on realistic audio signals show that the proposed algorithm\nyielded comparable separation performance with benchmark algorithms\nfor speech signals, and outperformed benchmark algorithms for music\nsignals.\n"
      ],
      "doi": "10.21437/Interspeech.2017-189"
    },
    "xu17_interspeech": {
      "authors": [
        [
          "Chenglin",
          "Xu"
        ],
        [
          "Xiong",
          "Xiao"
        ],
        [
          "Sining",
          "Sun"
        ],
        [
          "Wei",
          "Rao"
        ],
        [
          "Eng Siong",
          "Chng"
        ],
        [
          "Haizhou",
          "Li"
        ]
      ],
      "title": "Weighted Spatial Covariance Matrix Estimation for MUSIC Based TDOA Estimation of Speech Source",
      "original": "0199",
      "page_count": 5,
      "order": 391,
      "p1": "1894",
      "pn": "1898",
      "abstract": [
        "We study the estimation of time difference of arrival (TDOA) under\nnoisy and reverberant conditions. Conventional TDOA estimation methods\nsuch as MUltiple SIgnal Classification (MUSIC) are not robust to noise\nand reverberation due to the distortion in the spatial covariance matrix\n(SCM). To address this issue, this paper proposes a robust SCM estimation\nmethod, called weighted SCM (WSCM). In the WSCM estimation, each time-frequency\n(TF) bin of the input signal is weighted by a TF mask which is 0 for\nnon-speech TF bins and 1 for speech TF bins in ideal case. In practice,\nthe TF mask takes values between 0 and 1 that are predicted by a long\nshort term memory (LSTM) network trained from a large amount of simulated\nnoisy and reverberant data. The use of mask weights significantly reduces\nthe contribution of low SNR TF bins to the SCM estimation, hence improves\nthe robustness of MUSIC. Experimental results on both simulated and\nreal data show that we have significantly improved the robustness of\nMUSIC by using the weighted SCM.\n"
      ],
      "doi": "10.21437/Interspeech.2017-199"
    },
    "guo17b_interspeech": {
      "authors": [
        [
          "Feng",
          "Guo"
        ],
        [
          "Yuhang",
          "Cao"
        ],
        [
          "Zheng",
          "Liu"
        ],
        [
          "Jiaen",
          "Liang"
        ],
        [
          "Baoqing",
          "Li"
        ],
        [
          "Xiaobing",
          "Yuan"
        ]
      ],
      "title": "Speaker Direction-of-Arrival Estimation Based on Frequency-Independent Beampattern",
      "original": "0229",
      "page_count": 5,
      "order": 392,
      "p1": "1899",
      "pn": "1903",
      "abstract": [
        "The differential microphone array (DMA) becomes more and more popular\nrecently. In this paper, we derive the relationship between the direction-of-arrival\n(DoA) and DMA&#8217;s frequency-independent beampatterns. The derivation\ndemonstrates that the DoA can be yielded by solving a trigonometric\npolynomial. Taking the dipoles as a special case of this relationship,\nwe propose three methods to estimate the DoA based on the dipoles.\nHowever, we find these methods are vulnerable to the axial directions\nunder the reverberation environment. Fortunately, they can complement\neach other owing to their robustness to different angles. Hence, to\nincrease the robustness to the reverberation, we proposed another new\napproach by combining the advantages of these three dipole-based methods\nfor the speaker DoA estimation. Both simulations and experiments show\nthat the proposed method not only outperforms the traditional methods\nfor small aperture array but also is much more computationally efficient\nwith avoiding the spatial spectrum search.\n"
      ],
      "doi": "10.21437/Interspeech.2017-229"
    },
    "wang17i_interspeech": {
      "authors": [
        [
          "Xianyun",
          "Wang"
        ],
        [
          "Changchun",
          "Bao"
        ],
        [
          "Feng",
          "Bao"
        ]
      ],
      "title": "A Mask Estimation Method Integrating Data Field Model for Speech Enhancement",
      "original": "0271",
      "page_count": 5,
      "order": 393,
      "p1": "1904",
      "pn": "1908",
      "abstract": [
        "In most approaches based on computational auditory scene analysis (CASA),\nthe ideal binary mask (IBM) is often used for noise reduction. However,\nit is almost impossible to obtain the IBM result. The error in IBM\nestimation may greatly violate smooth evolution nature of speech because\nof the energy absence in many speech-dominated time-frequency (T-F)\nunits. To reduce the error, the ideal ratio mask (IRM) via modeling\nthe spatial dependencies of speech spectrum is used as an optimal target\nmask because the predictive ratio mask is less sensitive to the error\nthan the predictive binary mask. In this paper, we introduce a data\nfield (DF) to model the spatial dependencies of the cochleagram for\nobtaining the ratio mask. Firstly, initial T-F units of noise and speech\nare obtained from noisy speech. Then we can calculate the forms of\nthe potentials of noise and speech. Subsequently, their optimal potentials\nwhich reflect their respective distribution of potential field are\nobtained by the optimal influence factors of speech and noise. Finally,\nwe exploit the potentials of speech and noise to obtain the ratio mask.\nExperimental results show that the proposed method can obtain a better\nperformance than the reference methods in speech quality.\n"
      ],
      "doi": "10.21437/Interspeech.2017-271"
    },
    "shannon17_interspeech": {
      "authors": [
        [
          "Matt",
          "Shannon"
        ],
        [
          "Gabor",
          "Simko"
        ],
        [
          "Shuo-Yiin",
          "Chang"
        ],
        [
          "Carolina",
          "Parada"
        ]
      ],
      "title": "Improved End-of-Query Detection for Streaming Speech Recognition",
      "original": "0496",
      "page_count": 5,
      "order": 394,
      "p1": "1909",
      "pn": "1913",
      "abstract": [
        "In many streaming speech recognition applications such as voice search\nit is important to determine quickly and accurately when the user has\nfinished speaking their query. A conventional approach to this task\nis to declare end-of-query whenever a fixed interval of silence is\ndetected by a voice activity detector (VAD) trained to classify each\nframe as speech or silence. However silence detection and end-of-query\ndetection are fundamentally different tasks, and the criterion used\nduring VAD training may not be optimal. In particular the conventional\napproach ignores potential acoustic cues such as filler sounds and\npast speaking rate which may indicate whether a given pause is temporary\nor query-final. In this paper we present a simple modification to make\nthe conventional VAD training criterion more closely related to end-of-query\ndetection. A unidirectional long short-term memory architecture allows\nthe system to remember past acoustic events, and the training criterion\nincentivizes the system to learn to use any acoustic cues relevant\nto predicting future user intent. We show experimentally that this\napproach improves latency at a given accuracy by around 100 ms for\nend-of-query detection for voice search.\n"
      ],
      "doi": "10.21437/Interspeech.2017-496"
    },
    "he17_interspeech": {
      "authors": [
        [
          "Di",
          "He"
        ],
        [
          "Zuofu",
          "Cheng"
        ],
        [
          "Mark",
          "Hasegawa-Johnson"
        ],
        [
          "Deming",
          "Chen"
        ]
      ],
      "title": "Using Approximated Auditory Roughness as a Pre-Filtering Feature for Human Screaming and Affective Speech AED",
      "original": "0593",
      "page_count": 5,
      "order": 395,
      "p1": "1914",
      "pn": "1918",
      "abstract": [
        "Detecting human screaming, shouting, and other verbal manifestations\nof fear and anger are of great interest to security Audio Event Detection\n(AED) systems. The Internet of Things (IoT) approach allows wide-covering,\npowerful AED systems to be distributed across the Internet. But a good\n feature to  pre-filter the audio is critical to these systems. This\nwork evaluates the potential of detecting screaming and affective speech\nusing Auditory Roughness and proposes a very light-weight approximation\nmethod. Our approximation uses a similar amount of Multiple Add Accumulate\n(MAA) compared to short-term energy (STE), and at least 10&#215; less\nMAA than MFCC. We evaluated the performance of our approximated roughness\non the Mandarin Affective Speech corpus and a subset of the Youtube\nAudioSet for screaming against other low-complexity features. We show\nthat our approximated roughness returns higher accuracy.\n"
      ],
      "doi": "10.21437/Interspeech.2017-593"
    },
    "zegers17_interspeech": {
      "authors": [
        [
          "Jeroen",
          "Zegers"
        ],
        [
          "Hugo",
          "Van hamme"
        ]
      ],
      "title": "Improving Source Separation via Multi-Speaker Representations",
      "original": "0754",
      "page_count": 5,
      "order": 396,
      "p1": "1919",
      "pn": "1923",
      "abstract": [
        "Lately there have been novel developments in deep learning towards\nsolving the cocktail party problem. Initial results are very promising\nand allow for more research in the domain. One technique that has not\nyet been explored in the neural network approach to this task is speaker\nadaptation. Intuitively, information on the speakers that we are trying\nto separate seems fundamentally important for the speaker separation\ntask. However, retrieving this speaker information is challenging since\nthe speaker identities are not known a priori and multiple speakers\nare simultaneously active. There is thus some sort of chicken and egg\nproblem. To tackle this, source signals and i-vectors are estimated\nalternately. We show that blind multi-speaker adaptation improves the\nresults of the network and that (in our case) the network is not capable\nof adequately retrieving this useful speaker information itself.\n"
      ],
      "doi": "10.21437/Interspeech.2017-754"
    },
    "yang17b_interspeech": {
      "authors": [
        [
          "Bing",
          "Yang"
        ],
        [
          "Hong",
          "Liu"
        ],
        [
          "Cheng",
          "Pang"
        ]
      ],
      "title": "Multiple Sound Source Counting and Localization Based on Spatial Principal Eigenvector",
      "original": "0940",
      "page_count": 5,
      "order": 397,
      "p1": "1924",
      "pn": "1928",
      "abstract": [
        "Multiple sound source localization remains a challenging issue due\nto the interaction between sources. Although traditional approaches\ncan locate multiple sources effectively, most of them require the number\nof sound sources as a priori knowledge. However, the number of sound\nsources is generally unknown in practical applications. To overcome\nthis problem, a spatial principal eigenvector based approach is proposed\nto estimate the number and the direction of arrivals (DOAs) of multiple\nspeech sources. Firstly, a time-frequency (TF) bin weighting scheme\nis utilized to select the TF bins dominated by single source. Then,\nfor these selected bins, the spatial principal eigenvectors are extracted\nto construct a contribution function which is used to simultaneously\nestimate the number of sources and corresponding coarse DOAs. Finally,\nthe coarse DOA estimations are refined by iteratively optimizing the\nassignment of selected TF bins to each source. Experimental results\nvalidate that the proposed approach yields favorable performance for\nmultiple sound source counting and localization in the environment\nwith different levels of noise and reverberation.\n"
      ],
      "doi": "10.21437/Interspeech.2017-940"
    },
    "karthik17_interspeech": {
      "authors": [
        [
          "Girija Ramesan",
          "Karthik"
        ],
        [
          "Prasanta Kumar",
          "Ghosh"
        ]
      ],
      "title": "Subband Selection for Binaural Speech Source Localization",
      "original": "0954",
      "page_count": 5,
      "order": 398,
      "p1": "1929",
      "pn": "1933",
      "abstract": [
        "We consider the task of speech source localization using binaural cues,\nnamely interaural time and level difference (ITD &amp; ILD). A typical\napproach is to process binaural speech using gammatone filters and\ncalculate frame-level ITD and ILD in each subband. The ITD, ILD and\ntheir combination (ITLD) in each subband are statistically modelled\nusing Gaussian mixture models for every direction during training.\nGiven a binaural test-speech, the source is localized using maximum\nlikelihood criterion assuming that the binaural cues in each subband\nare independent. We, in this work, investigate the robustness of each\nsubband for localization and compare their performance against the\nfull-band scheme with 32 gammatone filters. We propose a subband selection\nprocedure using the training data where subbands are rank ordered based\non their localization performance. Experiments on Subject 003 from\nthe CIPIC database reveal that, for high SNRs, the ITD and ITLD of\njust one subband centered at 296Hz is sufficient to yield localization\naccuracy identical to that of the full-band scheme with a test-speech\nof duration 1sec. At low SNRs, in case of ITD, the selected subbands\nare found to perform better than the full-band scheme.\n"
      ],
      "doi": "10.21437/Interspeech.2017-954"
    },
    "chen17i_interspeech": {
      "authors": [
        [
          "Bo-Rui",
          "Chen"
        ],
        [
          "Huang-Yi",
          "Lee"
        ],
        [
          "Yi-Wen",
          "Liu"
        ]
      ],
      "title": "Unmixing Convolutive Mixtures by Exploiting Amplitude Co-Modulation: Methods and Evaluation on Mandarin Speech Recordings",
      "original": "1227",
      "page_count": 4,
      "order": 399,
      "p1": "1934",
      "pn": "1937",
      "abstract": [
        "This paper presents and evaluates two frequency-domain methods for\nmulti-channel sound source separation. The sources are assumed to couple\nto the microphones with unknown room responses. Independent component\nanalysis (ICA) is applied in the frequency domain to obtain maximally\nindependent amplitude envelopes (AEs) at every frequency. Due to the\nnature of ICA, the AEs across frequencies need to be  de-permuted.\nTo this end, we seek to assign AEs to the same source solely based\non the correlation in their magnitude variation against time. The resulted\ntime-varying spectra are inverse Fourier transformed to synthesize\nseparated signals. Objective evaluation showed that both methods achieve\na signal-to-interference ratio (SIR) that is comparable to Mazur et\nal (2013). In addition, we created spoken Mandarin materials and recruited\nage-matched subjects to perform word-by-word transcription. Results\nshowed that, first, speech intelligibility significantly improved after\nunmixing. Secondly, while both methods achieved similar SIR, the subjects\npreferred to listen to the results that were post-processed to ensure\na speech-like spectral shape; the mean opinion scores were 2.9 vs.\n4.3 (out of 5) between the two methods. The present results may provide\nsuggestions regarding deployment of the correlation-based source separation\nalgorithms into devices with limited computational resources.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1227"
    },
    "tao17_interspeech": {
      "authors": [
        [
          "Fei",
          "Tao"
        ],
        [
          "Carlos",
          "Busso"
        ]
      ],
      "title": "Bimodal Recurrent Neural Network for Audiovisual Voice Activity Detection",
      "original": "1573",
      "page_count": 5,
      "order": 400,
      "p1": "1938",
      "pn": "1942",
      "abstract": [
        " Voice activity detection (VAD) is an important preprocessing step\nin speech-based systems, especially for emerging hand-free intelligent\nassistants. Conventional VAD systems relying on audio-only features\nare normally impaired by noise in the environment. An alternative approach\nto address this problem is  audiovisual VAD (AV-VAD) systems. Modeling\ntiming dependencies between acoustic and visual features is a challenge\nin AV-VAD. This study proposes a bimodal  recurrent neural network\n(RNN) which combines audiovisual features in a principled, unified\nframework, capturing the timing dependency within modalities and across\nmodalities. Each modality is modeled with separate  bidirectional long\nshort-term memory (BLSTM) networks. The output layers are used as input\nof another BLSTM network. The experimental evaluation considers a large\naudiovisual corpus with clean and noisy recordings to assess the robustness\nof the approach. The proposed approach outperforms audio-only VAD by\n7.9% (absolute) under clean/ideal conditions (i.e.,  high definition\n(HD) camera, close-talk microphone). The proposed solution outperforms\nthe audio-only VAD system by 18.5% (absolute) when the conditions are\nmore challenging (i.e., camera and microphone from a tablet with noise\nin the environment). The proposed approach shows the best performance\nand robustness across a varieties of conditions, demonstrating its\npotential for real-world applications.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1573"
    },
    "maas17_interspeech": {
      "authors": [
        [
          "Roland",
          "Maas"
        ],
        [
          "Ariya",
          "Rastrow"
        ],
        [
          "Kyle",
          "Goehner"
        ],
        [
          "Gautam",
          "Tiwari"
        ],
        [
          "Shaun",
          "Joseph"
        ],
        [
          "Bj\u00f6rn",
          "Hoffmeister"
        ]
      ],
      "title": "Domain-Specific Utterance End-Point Detection for Speech Recognition",
      "original": "1673",
      "page_count": 5,
      "order": 401,
      "p1": "1943",
      "pn": "1947",
      "abstract": [
        "The task of automatically detecting the end of a device-directed user\nrequest is particularly challenging in case of switching short command\nand long free-form utterances. While low-latency end-pointing configurations\ntypically lead to good user experiences in the case of short requests,\nsuch as &#8220;play music&#8221;, it can be too aggressive in domains\nwith longer free-form queries, where users tend to pause noticeably\nbetween words and hence are easily cut off prematurely. We previously\nproposed an approach for accurate end-pointing by continuously estimating\npause duration features over all active recognition hypotheses. In\nthis paper, we study the behavior of these pause duration features\nand infer domain-dependent parametrizations. We furthermore propose\nto adapt the end-pointer aggressiveness on-the-fly by comparing the\nViterbi scores of active short command vs. long free-form decoding\nhypotheses. The experimental evaluation evidences a 18% relative reduction\nin word error rate on free-form requests while maintaining low latency\non short queries.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1673"
    },
    "kothapally17_interspeech": {
      "authors": [
        [
          "Vinay",
          "Kothapally"
        ],
        [
          "John H.L.",
          "Hansen"
        ]
      ],
      "title": "Speech Detection and Enhancement Using Single Microphone for Distant Speech Applications in Reverberant Environments",
      "original": "1760",
      "page_count": 5,
      "order": 402,
      "p1": "1948",
      "pn": "1952",
      "abstract": [
        "It is well known that in reverberant environments, the human auditory\nsystem has the ability to pre-process reverberant signals to compensate\nfor reflections and obtain effective cues for improved recognition.\nIn this study, we propose such a preprocessing technique for combined\ndetection and enhancement of speech using a single microphone in reverberant\nenvironments for distant speech applications. The proposed system employs\na framework where the target speech is synthesized using continuous\nauditory masks estimated from sub-band signals. Linear gammatone analysis/synthesis\nfilter banks are used as an auditory model for sub-band processing.\nThe performance of the proposed system is evaluated on the UT-DistantReverb\ncorpus which consists of speech recorded in a reverberant racquetball\ncourt (T60&#126;9000 msec). The current system shows an average improvement\nof 15% STNR over an existing single-channel dereverberation algorithm\nand 17% improvement in detecting speech frames over G729B, SOHN &amp;\nCombo-SAD unsupervised speech activity detectors on actual reverberant\nand noisy environments.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1760"
    },
    "wu17d_interspeech": {
      "authors": [
        [
          "Yi-Chiao",
          "Wu"
        ],
        [
          "Hsin-Te",
          "Hwang"
        ],
        [
          "Syu-Siang",
          "Wang"
        ],
        [
          "Chin-Cheng",
          "Hsu"
        ],
        [
          "Yu",
          "Tsao"
        ],
        [
          "Hsin-Min",
          "Wang"
        ]
      ],
      "title": "A Post-Filtering Approach Based on Locally Linear Embedding Difference Compensation for Speech Enhancement",
      "original": "0062",
      "page_count": 5,
      "order": 403,
      "p1": "1953",
      "pn": "1957",
      "abstract": [
        "This paper presents a novel difference compensation post-filtering\napproach based on the locally linear embedding (LLE) algorithm for\nspeech enhancement (SE). The main goal of the proposed post-filtering\napproach is to further suppress residual noises in SE-processed signals\nto attain improved speech quality and intelligibility. The proposed\nsystem can be divided into offline and online stages. In the offline\nstage, we prepare paired differences: the estimated difference of SE-processed\nspeech; noisy speech and the ground-truth difference of clean speech;\nnoisy speech. In the online stage, on the basis of estimated difference\nof a test utterance, we first predict the corresponding ground-truth\ndifference based on the LLE algorithm, and then compensate the noisy\nspeech with the predicted difference. In this study, we integrate a\ndeep denoising autoencoder (DDAE) SE method with the proposed LLE-based\ndifference compensation post-filtering approach. The experiment results\nreveal that the proposed post-filtering approach obviously enhanced\nthe speech quality and intelligibility of the DDAE-based SE-processed\nspeech in different noise types and signal-to-noise-ratio levels.\n"
      ],
      "doi": "10.21437/Interspeech.2017-62"
    },
    "zhang17f_interspeech": {
      "authors": [
        [
          "Hui",
          "Zhang"
        ],
        [
          "Xueliang",
          "Zhang"
        ],
        [
          "Guanglai",
          "Gao"
        ]
      ],
      "title": "Multi-Target Ensemble Learning for Monaural Speech Separation",
      "original": "0240",
      "page_count": 5,
      "order": 404,
      "p1": "1958",
      "pn": "1962",
      "abstract": [
        "Speech separation can be formulated as a supervised learning problem\nwhere a machine is trained to cast the acoustic features of the noisy\nspeech to a time-frequency mask, or the spectrum of the clean speech.\nThese two categories of speech separation methods can be generally\nreferred as the masking-based and the mapping-based methods, but none\nof them can perfectly estimate the clean speech, since any target can\nonly describe a part of the characteristics of the speech. However,\nthe estimated masks and speech spectrum can, sometimes, be complementary\nas the speech is described from different perspectives. In this paper,\nby adopting an ensemble framework, a multi-target deep neural network\n(DNN) based method is proposed, which combines the masking-based and\nthe mapping-based strategies, and the DNN is trained to jointly estimate\nthe time-frequency masks and the clean spectrum. We show that as expected\nthe mask and speech spectrum based targets yield partly complementary\nestimates, and the separation performance can be improved by merging\nthese estimates. Furthermore, a merging model trained jointly with\nthe multi-target DNN is developed. Experimental results indicate that\nthe proposed multi-target DNN based method outperforms the DNN based\nalgorithm which optimizes a single target.\n"
      ],
      "doi": "10.21437/Interspeech.2017-240"
    },
    "ogawa17_interspeech": {
      "authors": [
        [
          "Atsunori",
          "Ogawa"
        ],
        [
          "Keisuke",
          "Kinoshita"
        ],
        [
          "Marc",
          "Delcroix"
        ],
        [
          "Tomohiro",
          "Nakatani"
        ]
      ],
      "title": "Improved Example-Based Speech Enhancement by Using Deep Neural Network Acoustic Model for Noise Robust Example Search",
      "original": "0543",
      "page_count": 5,
      "order": 405,
      "p1": "1963",
      "pn": "1967",
      "abstract": [
        "Example-based speech enhancement is a promising single-channel approach\nfor coping with highly nonstationary noise. Given a noisy speech input,\nit first searches in a noisy speech corpus for the noisy speech examples\nthat best match the input. Then, it concatenates the clean speech examples\nthat are paired with the matched noisy examples to obtain an estimate\nof the underlying clean speech component in the input. The quality\nof the enhanced speech depends on how accurate an example search can\nbe performed given a noisy speech input. The example search is conventionally\nperformed using a Gaussian mixture model (GMM) with mel-frequency cepstral\ncoefficient features (MFCCs). To improve the noise robustness of the\nGMM-based example search, instead of using noise sensitive MFCCs, we\nhave proposed using bottleneck features (BNFs), which are extracted\nfrom a deep neural network-based acoustic model (DNN-AM) built for\nautomatic speech recognition. In this paper, instead of using a GMM\nwith noise robust BNFs, we propose the direct use of a DNN-AM in the\nexample search to further improve its noise robustness. Experimental\nresults on the Aurora4 corpus show that the DNN-AM-based example search\nsteadily improves the enhanced speech quality compared with the GMM-based\nexample search using BNFs.\n"
      ],
      "doi": "10.21437/Interspeech.2017-543"
    },
    "gelderblom17_interspeech": {
      "authors": [
        [
          "Femke B.",
          "Gelderblom"
        ],
        [
          "Tron V.",
          "Tronstad"
        ],
        [
          "Erlend Magnus",
          "Viggen"
        ]
      ],
      "title": "Subjective Intelligibility of Deep Neural Network-Based Speech Enhancement",
      "original": "1041",
      "page_count": 5,
      "order": 406,
      "p1": "1968",
      "pn": "1972",
      "abstract": [
        "Recent literature indicates increasing interest in deep neural networks\nfor use in speech enhancement systems. Currently, these systems are\nmostly evaluated through objective measures of speech quality and/or\nintelligibility. Subjective intelligibility evaluations of these systems\nhave so far not been reported. In this paper we report the results\nof a speech recognition test with 15 participants, where the participants\nwere asked to pick out words in background noise before and after enhancement\nusing a common deep neural network approach. We found that, although\nthe objective measure STOI predicts that intelligibility should improve\nor at the very least stay the same, the speech recognition threshold,\nwhich is a measure of intelligibility, deteriorated by 4 dB. These\nresults indicate that STOI is not a good predictor for the subjective\nintelligibility of deep neural network-based speech enhancement systems.\nWe also found that the postprocessing technique of global variance\nnormalisation does not significantly affect subjective intelligibility.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1041"
    },
    "koutsogiannaki17_interspeech": {
      "authors": [
        [
          "Maria",
          "Koutsogiannaki"
        ],
        [
          "Holly",
          "Francois"
        ],
        [
          "Kihyun",
          "Choo"
        ],
        [
          "Eunmi",
          "Oh"
        ]
      ],
      "title": "Real-Time Modulation Enhancement of Temporal Envelopes for Increasing Speech Intelligibility",
      "original": "1157",
      "page_count": 5,
      "order": 407,
      "p1": "1973",
      "pn": "1977",
      "abstract": [
        "In this paper, a novel approach is introduced for performing real-time\nspeech modulation enhancement to increase speech intelligibility in\nnoise. The proposed modulation enhancement technique operates independently\nin the frequency and time domains. In the frequency domain, a compression\nfunction is used to perform energy reallocation within a frame. This\ncompression function contains novel scaling operations to ensure speech\nquality. In the time domain, a mathematical equation is introduced\nto reallocate energy from the louder to the quieter parts of the speech.\nThis proposed mathematical equation ensures that the long-term energy\nof the speech is preserved independently of the amount of compression,\nhence gaining full control of the time-energy reallocation in real-time.\nEvaluations on intelligibility and quality show that the suggested\napproach increases the intelligibility of speech while maintaining\nthe overall energy and quality of the speech signal.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1157"
    },
    "hirsch17_interspeech": {
      "authors": [
        [
          "Hans-G\u00fcnter",
          "Hirsch"
        ],
        [
          "Michael",
          "Gref"
        ]
      ],
      "title": "On the Influence of Modifying Magnitude and Phase Spectrum to Enhance Noisy Speech Signals",
      "original": "1173",
      "page_count": 5,
      "order": 408,
      "p1": "1978",
      "pn": "1982",
      "abstract": [
        "Neural networks have proven their ability to be usefully applied as\ncomponent of a speech enhancement system. This is based on the known\nfeature of neural nets to map regions inside a feature space to other\nregions. It can be taken to map noisy magnitude spectra to clean spectra.\nThis way the net can be used to substitute an adaptive filtering in\nthe spectral domain. We set up such a system and compared its performance\nagainst a known adaptive filtering approach in terms of speech quality\nand in terms of recognition rate. It is a still not fully answered\nquestion how far the speech quality can be enhanced by modifying not\nonly the magnitude but also the spectral phase and how this phase modification\ncould be realized. Before trying to use a neural network for a possible\nmodification of the phase spectrum we ran a set of oracle experiments\nto find out how far the quality can be improved by modifying the magnitude\nand/or the phase spectrum in voiced segments. It turns out that the\nsimultaneous modification of magnitude and phase spectrum has the potential\nfor a considerable improvement of the speech quality in comparison\nto modifying the magnitude or the phase only.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1173"
    },
    "rehr17_interspeech": {
      "authors": [
        [
          "Robert",
          "Rehr"
        ],
        [
          "Timo",
          "Gerkmann"
        ]
      ],
      "title": "MixMax Approximation as a Super-Gaussian Log-Spectral Amplitude Estimator for Speech Enhancement",
      "original": "1243",
      "page_count": 5,
      "order": 409,
      "p1": "1983",
      "pn": "1987",
      "abstract": [
        "For single-channel speech enhancement, most commonly, the noisy observation\nis described as the sum of the clean speech signal and the noise signal.\nFor machine learning based enhancement schemes where speech and noise\nare modeled in the log-spectral domain, however, the log-spectrum of\nthe noisy observation can be described as the maximum of the speech\nand noise log-spectrum to simplify statistical inference. This approximation\nis referred to as MixMax model or log-max approximation. In this paper,\nwe show how this approximation can be used in combination with non-trained,\nblind speech and noise power estimators derived in the spectral domain.\nOur findings allow to interpret the MixMax based clean speech estimator\nas a super-Gaussian log-spectral amplitude estimator. This MixMax based\nestimator is embedded in a pre-trained speech enhancement scheme and\ncompared to a log-spectral amplitude estimator based on an additive\nmixing model. Instrumental measures indicate that the MixMax based\nestimator causes less musical tones while it virtually yields the same\nquality for the enhanced speech signal.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1243"
    },
    "marxer17_interspeech": {
      "authors": [
        [
          "Ricard",
          "Marxer"
        ],
        [
          "Jon",
          "Barker"
        ]
      ],
      "title": "Binary Mask Estimation Strategies for Constrained Imputation-Based Speech Enhancement",
      "original": "1257",
      "page_count": 5,
      "order": 410,
      "p1": "1988",
      "pn": "1992",
      "abstract": [
        "In recent years, speech enhancement by analysis-resynthesis has emerged\nas an alternative to conventional noise filtering approaches. Analysis-resynthesis\nreplaces noisy speech with a signal that has been reconstructed from\na clean speech model. It can deliver high-quality signals with no residual\nnoise, but at the expense of losing information from the original signal\nthat is not well-represented by the model. A recent compromise solution,\ncalled constrained resynthesis, solves this problem by only resynthesising\nspectro-temporal regions that are estimated to be masked by noise (conditioned\non the evidence in the unmasked regions). In this paper we first extend\nthe approach by: i) introducing multi-condition training and a deep\ndiscriminative model for the analysis stage; ii) introducing an improved\nresynthesis model that captures within-state cross-frequency dependencies.\nWe then extend the previous stationary-noise evaluation by using real\ndomestic audio noise from the CHiME-2 evaluation. We compare various\nmask estimation strategies while varying the degree of constraint by\ntuning the threshold for reliable speech detection. PESQ and log-spectral\ndistance measures show that although mask estimation remains a challenge,\nit is only necessary to estimate a few reliable signal regions in order\nto achieve performance close to that achieved with an optimal oracle\nmask.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1257"
    },
    "park17c_interspeech": {
      "authors": [
        [
          "Se Rim",
          "Park"
        ],
        [
          "Jin Won",
          "Lee"
        ]
      ],
      "title": "A Fully Convolutional Neural Network for Speech Enhancement",
      "original": "1465",
      "page_count": 5,
      "order": 411,
      "p1": "1993",
      "pn": "1997",
      "abstract": [
        "The presence of babble noise degrades hearing intelligibility of human\nspeech greatly. However, removing the babble without creating artifacts\nin human speech is a challenging task in a low SNR environment. Here,\nwe sought to solve the problem by finding a &#8216;mapping&#8217; between\nnoisy speech spectra and clean speech spectra via supervised learning.\nSpecifically, we propose using fully Convolutional Neural Networks,\nwhich consist of lesser number of parameters than fully connected networks.\nThe proposed network, Redundant Convolutional Encoder Decoder (R-CED),\ndemonstrates that a convolutional network can be 12 times smaller than\na recurrent network and yet achieves better performance, which shows\nits applicability for an embedded system.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1465"
    },
    "li17i_interspeech": {
      "authors": [
        [
          "Li",
          "Li"
        ],
        [
          "Hirokazu",
          "Kameoka"
        ],
        [
          "Tomoki",
          "Toda"
        ],
        [
          "Shoji",
          "Makino"
        ]
      ],
      "title": "Speech Enhancement Using Non-Negative Spectrogram Models with Mel-Generalized Cepstral Regularization",
      "original": "1492",
      "page_count": 5,
      "order": 412,
      "p1": "1998",
      "pn": "2002",
      "abstract": [
        "Spectral domain speech enhancement algorithms based on non-negative\nspectrogram models such as non-negative matrix factorization (NMF)\nand non-negative matrix factor deconvolution are powerful in terms\nof signal recovery accuracy, however they do not directly lead to an\nenhancement in the feature domain (e.g., cepstral domain) or in terms\nof perceived quality. We have previously proposed a method that makes\nit possible to enhance speech in the spectral and cepstral domains\nsimultaneously. Although this method was shown to be effective, the\ndevised algorithm was computationally demanding. This paper proposes\nyet another formulation that allows for a fast implementation by replacing\nthe regularization term with a divergence measure between the NMF model\nand the mel-generalized cepstral (MGC) representation of the target\nspectrum. Since the MGC is an auditory-motivated representation of\nan audio signal widely used in parametric speech synthesis, we also\nexpect the proposed method to have an effect in enhancing the perceived\nquality. Experimental results revealed the effectiveness of the proposed\nmethod in terms of both the signal-to-distortion ratio and the cepstral\ndistance.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1492"
    },
    "websdale17_interspeech": {
      "authors": [
        [
          "Danny",
          "Websdale"
        ],
        [
          "Ben",
          "Milner"
        ]
      ],
      "title": "A Comparison of Perceptually Motivated Loss Functions for Binary Mask Estimation in Speech Separation",
      "original": "1504",
      "page_count": 5,
      "order": 413,
      "p1": "2003",
      "pn": "2007",
      "abstract": [
        "This work proposes and compares perceptually motivated loss functions\nfor deep learning based binary mask estimation for speech separation.\nPrevious loss functions have focused on maximising classification accuracy\nof mask estimation but we now propose loss functions that aim to maximise\nthe hit minus false-alarm (HIT-FA) rate which is known to correlate\nmore closely to speech intelligibility. The baseline loss function\nis binary cross-entropy (CE), a standard loss function used in binary\nmask estimation, which maximises classification accuracy. We propose\nfirst a loss function that maximises the HIT-FA rate instead of classification\naccuracy. We then propose a second loss function that is a hybrid between\nCE and HIT-FA, providing a balance between classification accuracy\nand HIT-FA rate. Evaluations of the perceptually motivated loss functions\nwith the GRID database show improvements to HIT-FA rate and ESTOI across\nbabble and factory noises. Further tests then explore application of\nthe perceptually motivated loss functions to a larger vocabulary dataset.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1504"
    },
    "michelsanti17_interspeech": {
      "authors": [
        [
          "Daniel",
          "Michelsanti"
        ],
        [
          "Zheng-Hua",
          "Tan"
        ]
      ],
      "title": "Conditional Generative Adversarial Networks for Speech Enhancement and Noise-Robust Speaker Verification",
      "original": "1620",
      "page_count": 5,
      "order": 414,
      "p1": "2008",
      "pn": "2012",
      "abstract": [
        "Improving speech system performance in noisy environments remains a\nchallenging task, and speech enhancement (SE) is one of the effective\ntechniques to solve the problem. Motivated by the promising results\nof generative adversarial networks (GANs) in a variety of image processing\ntasks, we explore the potential of conditional GANs (cGANs) for SE,\nand in particular, we make use of the image processing framework proposed\nby Isola et al. [1] to learn a mapping from the spectrogram of noisy\nspeech to an enhanced counterpart. The SE cGAN consists of two networks,\ntrained in an adversarial manner: a generator that tries to enhance\nthe input noisy spectrogram, and a discriminator that tries to distinguish\nbetween enhanced spectrograms provided by the generator and clean ones\nfrom the database using the noisy spectrogram as a condition. We evaluate\nthe performance of the cGAN method in terms of perceptual evaluation\nof speech quality (PESQ), short-time objective intelligibility (STOI),\nand equal error rate (EER) of speaker verification (an example application).\nExperimental results show that the cGAN method overall outperforms\nthe classical short-time spectral amplitude minimum mean square error\n(STSA-MMSE) SE algorithm, and is comparable to a deep neural network-based\nSE approach (DNN-SE).\n"
      ],
      "doi": "10.21437/Interspeech.2017-1620"
    },
    "qian17b_interspeech": {
      "authors": [
        [
          "Kaizhi",
          "Qian"
        ],
        [
          "Yang",
          "Zhang"
        ],
        [
          "Shiyu",
          "Chang"
        ],
        [
          "Xuesong",
          "Yang"
        ],
        [
          "Dinei",
          "Flor\u00eancio"
        ],
        [
          "Mark",
          "Hasegawa-Johnson"
        ]
      ],
      "title": "Speech Enhancement Using Bayesian Wavenet",
      "original": "1672",
      "page_count": 5,
      "order": 415,
      "p1": "2013",
      "pn": "2017",
      "abstract": [
        "In recent years, deep learning has achieved great success in speech\nenhancement. However, there are two major limitations regarding existing\nworks. First, the Bayesian framework is not adopted in many such deep-learning-based\nalgorithms. In particular, the prior distribution for speech in the\nBayesian framework has been shown useful by regularizing the output\nto be in the speech space, and thus improving the performance. Second,\nthe majority of the existing methods operate on the frequency domain\nof the noisy speech, such as spectrogram and its variations. The clean\nspeech is then reconstructed using the approach of overlap-add, which\nis limited by its inherent performance upper bound. This paper presents\na Bayesian speech enhancement framework, called BaWN (Bayesian WaveNet),\nwhich directly operates on raw audio samples. It adopts the recently\nannounced WaveNet, which is shown to be effective in modeling conditional\ndistributions of speech samples while generating natural speech. Experiments\nshow that BaWN is able to recover clean and natural speech.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1672"
    },
    "zhang17g_interspeech": {
      "authors": [
        [
          "Xueliang",
          "Zhang"
        ],
        [
          "DeLiang",
          "Wang"
        ]
      ],
      "title": "Binaural Reverberant Speech Separation Based on Deep Neural Networks",
      "original": "0297",
      "page_count": 5,
      "order": 416,
      "p1": "2018",
      "pn": "2022",
      "abstract": [
        "Supervised learning has exhibited great potential for speech separation\nin recent years. In this paper, we focus on separating target speech\nin reverberant conditions from binaural inputs using supervised learning.\nSpecifically, deep neural network (DNN) is constructed to map from\nboth spectral and spatial features to a training target. For spectral\nfeatures extraction, we first convert binaural inputs into a single\nsignal by applying a fixed beamformer. A new spatial feature is proposed\nand extracted to complement spectral features. The training target\nis the recently suggested ideal ratio mask (IRM). Systematic evaluations\nand comparisons show that the proposed system achieves good separation\nperformance and substantially outperforms existing algorithms under\nchallenging multi-source and reverberant environments.\n"
      ],
      "doi": "10.21437/Interspeech.2017-297"
    },
    "zorila17_interspeech": {
      "authors": [
        [
          "Tudor-C\u0103t\u0103lin",
          "Zoril\u0103"
        ],
        [
          "Yannis",
          "Stylianou"
        ]
      ],
      "title": "On the Quality and Intelligibility of Noisy Speech Processed for Near-End Listening Enhancement",
      "original": "1225",
      "page_count": 5,
      "order": 417,
      "p1": "2023",
      "pn": "2027",
      "abstract": [
        "Most current techniques for near-end speech intelligibility enhancement\nhave focused on processing clean input signals, however, in realistic\nenvironments, the input is often noisy. Processing noisy speech for\nintelligibility enhancement using algorithms developed for clean signals\ncan lower the perceptual quality of the samples when they are listened\nin quiet. Here we address the quality loss in these conditions by combining\nnoise reduction with a multi-band version of a state-of-the-art intelligibility\nenhancer for clean speech that is based on spectral shaping and dynamic\nrange compression (SSDRC). Subjective quality and intelligibility assessments\nwith noisy input speech showed that: (a) In quiet near-end conditions,\nthe proposed system outperformed the baseline SSDRC in terms of Mean\nOpinion Score (MOS); (b) In speech-shaped near-end noise, the proposed\nsystem improved the intelligibility of unprocessed speech by a factor\nlarger than three at the lowest tested signal-to-noise ratio (SNR)\nhowever, overall, it yielded lower recognition scores than the standard\nSSDRC.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1225"
    },
    "meermeier17_interspeech": {
      "authors": [
        [
          "Ralf",
          "Meermeier"
        ],
        [
          "Sean",
          "Colbath"
        ]
      ],
      "title": "Applications of the BBN Sage Speech Processing Platform",
      "original": "2017",
      "page_count": 2,
      "order": 418,
      "p1": "2028",
      "pn": "2029",
      "abstract": [
        "As a follow-up to our paper at Interspeech 2016 [1], we propose to\nshowcase various applications that now all use BBN&#8217;s Sage Speech\nProcessing Platform, demonstrating the platform&#8217;s versatility\nand ease of integration.<br style=\"mso-data-placement:same-cell;\" /><br\nstyle=\"mso-data-placement:same-cell;\" />\n  In particular, we will showcase\n1)  BBN TransTalk: A turn-based speech-to-speech translation program\nrunning entirely on an Android smartphone, alongside a custom 3D-printed\nperipheral for it. 2) A continuous transcription and translation application\nrunning on a Raspberry Pi 3) An offline OCR application utilizing Sage,\nrunning on a COTS Windows laptop.\n"
      ]
    },
    "cernak17_interspeech": {
      "authors": [
        [
          "Milos",
          "Cernak"
        ],
        [
          "Alain",
          "Komaty"
        ],
        [
          "Amir",
          "Mohammadi"
        ],
        [
          "Andr\u00e9",
          "Anjos"
        ],
        [
          "S\u00e9bastien",
          "Marcel"
        ]
      ],
      "title": "Bob Speaks Kaldi",
      "original": "2025",
      "page_count": 2,
      "order": 419,
      "p1": "2030",
      "pn": "2031",
      "abstract": [
        "This paper introduces and demonstrates Kaldi integration into Bob signal-processing\nand machine learning toolbox. The motivation for this integration is\ntwo-fold. Firstly, Bob benefits from using advanced speech processing\ntools developed in Kaldi. Secondly, Kaldi benefits from using complementary\nBob modules, such as modulation-based VAD with an adaptive thresholding.\nIn addition, Bob is designed as an open science tool, and this integration\nmight offer to the Kaldi speech community a framework for better reproducibility\nof state-of-the-art research results.\n"
      ]
    },
    "lenarczyk17_interspeech": {
      "authors": [
        [
          "Micha\u0142",
          "Lenarczyk"
        ]
      ],
      "title": "Real Time Pitch Shifting with Formant Structure Preservation Using the Phase Vocoder",
      "original": "2028",
      "page_count": 2,
      "order": 420,
      "p1": "2032",
      "pn": "2033",
      "abstract": [
        "Pitch shifting in speech is presented based on the use of the phase\nvocoder in combination with spectral whitening and envelope reconstruction,\napplied respectively before and after the transformation. A band preservation\ntechnique is introduced to contain quality degradation when downscaling\nthe pitch. The transposition ratio is fixed in advance by selecting\nanalysis and synthesis window sizes. Real time performance is demonstrated\nfor window sizes having adequate factorization required by fast Fourier\ntransformation.\n"
      ]
    },
    "chennupati17_interspeech": {
      "authors": [
        [
          "Nivedita",
          "Chennupati"
        ],
        [
          "B.H.V.S. Narayana",
          "Murthy"
        ],
        [
          "B.",
          "Yegnanarayana"
        ]
      ],
      "title": "A Signal Processing Approach for Speaker Separation Using SFF Analysis",
      "original": "2043",
      "page_count": 2,
      "order": 421,
      "p1": "2034",
      "pn": "2035",
      "abstract": [
        "Multi-speaker separation is necessary to increase intelligibility of\nspeech signals or to improve accuracy of speech recognition systems.\nIdeal binary mask (IBM) has set a gold standard for speech separation\nby suppressing the undesired speakers and also by increasing intelligibility\nof the desired speech. In this work, single frequency filtering (SFF)\nanalysis is used to estimate the mask closer to IBM for speaker separation.\nThe SFF analysis gives good temporal resolution for extracting features\nsuch as glottal closure instants (GCIs), and high spectral resolution\nfor resolving harmonics. The temporal resolution in SFF gives impulse\nlocations, which are used to calculate the time delay. The delay compensation\nbetween two microphone signals reinforces the impulses corresponding\nto one of the speakers. The spectral resolution of the SFF is exploited\nto estimate the masks using the SFF magnitude spectra on the enhanced\nimpulse-like sequence corresponding to one of the speakers. The estimated\nmask is used to refine the SFF magnitude. The refined SFF magnitude\nalong with the phase of the mixed microphone signal is used to obtain\nspeaker separation. Performance of proposed algorithm is demonstrated\nusing multi-speaker data collected in a real room environment.\n"
      ]
    },
    "stemmer17_interspeech": {
      "authors": [
        [
          "Georg",
          "Stemmer"
        ],
        [
          "Munir",
          "Georges"
        ],
        [
          "Joachim",
          "Hofer"
        ],
        [
          "Piotr",
          "Rozen"
        ],
        [
          "Josef",
          "Bauer"
        ],
        [
          "Jakub",
          "Nowicki"
        ],
        [
          "Tobias",
          "Bocklet"
        ],
        [
          "Hannah R.",
          "Colett"
        ],
        [
          "Ohad",
          "Falik"
        ],
        [
          "Michael",
          "Deisher"
        ],
        [
          "Sylvia J.",
          "Downing"
        ]
      ],
      "title": "Speech Recognition and Understanding on Hardware-Accelerated DSP",
      "original": "2056",
      "page_count": 2,
      "order": 422,
      "p1": "2036",
      "pn": "2037",
      "abstract": [
        "A smart home controller that responds to natural language input is\ndemonstrated on an Intel embedded processor. This device contains two\nDSP cores and a neural network co-processor which share 4MB SRAM. An\nembedded configuration of the Intel RealSpeech speech recognizer and\nintent extraction engine runs on the DSP cores with neural network\noperations offloaded to the co-processor. The prototype demonstrates\nthat continuous speech recognition and understanding is possible on\nhardware with very low power consumption. As an example application,\ncontrol of lights in a home via natural language is shown. An Intel\ndevelopment kit is demonstrated together with a set of tools. Conference\nattendees are encouraged to interact with the demo and development\nsystem.\n"
      ]
    },
    "tsuji17_interspeech": {
      "authors": [
        [
          "Sho",
          "Tsuji"
        ],
        [
          "Christina",
          "Bergmann"
        ],
        [
          "Molly",
          "Lewis"
        ],
        [
          "Mika",
          "Braginsky"
        ],
        [
          "Page",
          "Piccinini"
        ],
        [
          "Michael C.",
          "Frank"
        ],
        [
          "Alejandrina",
          "Cristia"
        ]
      ],
      "title": "MetaLab: A Repository for Meta-Analyses on Language Development, and More",
      "original": "2053",
      "page_count": 2,
      "order": 423,
      "p1": "2038",
      "pn": "2039",
      "abstract": [
        "MetaLab is a growing database of meta-analyses, shared in a github\nrepository and via an interactive website. This website contains interactive\ntools for community-augmented meta-analyses, power analyses, and experimental\nplanning. It currently contains a dozen meta-analyses spanning a number\nof phenomena in early language acquisition research, including infants&#8217;\nvowel discrimination, acoustic wordform segmentation, and distributional\nlearning in the laboratory. During the Show and Tell, we will demonstrate\nhow to use the online visualization tools, download data, and re-use\nour analysis scripts for other research purposes. We expect MetaLab\ndata to be particularly useful to researchers interested in early speech\nperception. Additionally, the infrastructure and tools can be adopted\nby speech scientists seeking to perform and utilize (meta-)meta-analyses\nin other fields.\n"
      ]
    },
    "daniel17_interspeech": {
      "authors": [
        [
          "Adrien",
          "Daniel"
        ]
      ],
      "title": "Evolving Recurrent Neural Networks That Process and Classify Raw Audio in a Streaming Fashion",
      "original": "2030",
      "page_count": 2,
      "order": 424,
      "p1": "2040",
      "pn": "2041",
      "abstract": [
        "The paper describes a neuroevolution-based novel approach to train\nrecurrent neural networks that can process and classify audio directly\nfrom the raw waveform signal, without any assumption on the signal\nitself, on the features that should be extracted, or on the required\nnetwork topology to perform the task. Resulting networks are relatively\nsmall in memory size, and their usage in a streaming fashion makes\nthem particularly suited to embedded real-time applications.\n"
      ]
    },
    "milosevic17_interspeech": {
      "authors": [
        [
          "Milana",
          "Milo\u0161evi\u0107"
        ],
        [
          "Ulrike",
          "Glavitsch"
        ]
      ],
      "title": "Combining Gaussian Mixture Models and Segmental Feature Models for Speaker Recognition",
      "original": "2032",
      "page_count": 2,
      "order": 425,
      "p1": "2042",
      "pn": "2043",
      "abstract": [
        "In most speaker recognition systems speech utterances are not constrained\nin content or language. In a text-dependent speaker recognition system\nlexical content of speech and language are known in advance. The goal\nof this paper is to show that this information can be used by a segmental\nfeatures (SF) approach to improve a standard Gaussian mixture model\nwith MFCC features (GMM-MFCC). Speech features such as mean energy,\ndelta energy, pitch, delta pitch, the formants F1&#8211;F4 and their\nbandwidths B1&#8211;B4 and the difference between F2 and F1 are calculated\non segments and are associated to phonemes and phoneme groups for each\nspeaker. The SF and GMM-MFCC approaches are combined by multiplying\nthe outputs of two classifiers. All the experiments are performed on\nthe two versions of TEVOID: TEVOID16 with 16 and the upgraded TEVOID50\nwith 50 speakers. On TEVOID16, SF achieves 84.23%, GMM-MFCC 91.75%,\nand the combined approach gives 95.12% recognition rate. On TEVOID50,\nthe SF approach gives 68.69%, while both GMM-MFCC and the combined\nmodel achieve 95.84% recognition rate. On both databases, the number\nof male/female confusions decreased for the combined model. These results\nare promising for using segmental features to improve the recognition\nrate of text-dependent systems.\n"
      ]
    },
    "hagerer17_interspeech": {
      "authors": [
        [
          "Gerhard",
          "Hagerer"
        ],
        [
          "Nicholas",
          "Cummins"
        ],
        [
          "Florian",
          "Eyben"
        ],
        [
          "Bj\u00f6rn",
          "Schuller"
        ]
      ],
      "title": "&#8220;Did you laugh enough today?&#8221; &#8212; Deep Neural Networks for Mobile and Wearable Laughter Trackers",
      "original": "2036",
      "page_count": 2,
      "order": 426,
      "p1": "2044",
      "pn": "2045",
      "abstract": [
        "In this paper we describe a mobile and wearable devices app that recognises\nlaughter from speech in real-time. The laughter detection is based\non a deep neural network architecture, which runs smoothly and robustly,\neven natively on a smartwatch. Further, this paper presents results\ndemonstrating that our approach achieves state-of-the-art laughter\ndetection performance on the SSPNet Vocalization Corpus (SVC) from\nthe 2013 Interspeech Computational Paralinguistics Challenge Social\nSignals Sub-Challenge. As this technology is tailored for mobile and\nwearable devices, it enables and motivates many new use cases, for\nexample, deployment in health care settings such as laughter tracking\nfor psychological coaching, depression monitoring, and therapies.\n"
      ]
    },
    "jeon17_interspeech": {
      "authors": [
        [
          "Kwang Myung",
          "Jeon"
        ],
        [
          "Nam Kyun",
          "Kim"
        ],
        [
          "Chan Woong",
          "Kwak"
        ],
        [
          "Jung Min",
          "Moon"
        ],
        [
          "Hong Kook",
          "Kim"
        ]
      ],
      "title": "Low-Frequency Ultrasonic Communication for Speech Broadcasting in Public Transportation",
      "original": "2037",
      "page_count": 2,
      "order": 427,
      "p1": "2046",
      "pn": "2047",
      "abstract": [
        "Speech broadcasting via loudspeakers is widely used in public transportation\nto send broadcast notifications. However, listeners often fail to catch\nspoken context from speech broadcasts due to excessive environmental\nnoise. We propose an ultrasonic communication method that can be applied\nto loudspeaker-based speech broadcasting to cope with this issue. In\nother words, text notifications are modulated and carried over low-frequency\nultrasonic waves through loudspeakers to the microphones of each potential\nlistener&#8217;s mobile device. Then, the received ultrasonic stream\nis demodulated back into the text and the listener hears the notification\ncontext by a text-to-speech engine embedded in each mobile device.\nSuch a transmission system is realized with a 20 kHz carrier frequency\nbecause it is inaudible to most listeners but capable of being used\nin communication between a loudspeaker and microphone. In addition,\nthe performance of the proposed ultrasonic communication method is\nevaluated by measuring the success rate of transmitted words under\nvarious signal-to-noise ratio conditions.\n"
      ]
    },
    "wood17_interspeech": {
      "authors": [
        [
          "Sean U.N.",
          "Wood"
        ],
        [
          "Jean",
          "Rouat"
        ]
      ],
      "title": "Real-Time Speech Enhancement with GCC-NMF: Demonstration on the Raspberry Pi and NVIDIA Jetson",
      "original": "2039",
      "page_count": 2,
      "order": 428,
      "p1": "2048",
      "pn": "2049",
      "abstract": [
        "We demonstrate a real-time, open source implementation of the online\nGCC-NMF stereo speech enhancement algorithm. While the system runs\non a variety of operating systems and hardware platforms, we highlight\nits potential for real-world mobile use by presenting it on two embedded\nsystems: the Raspberry Pi 3 and the NVIDIA Jetson TX1. The effect of\nvarious algorithm parameters on subjective enhancement quality may\nbe explored interactively via a graphical user interface, with the\nresults heard in real-time. The trade-off between interference suppression\nand target fidelity is controlled by manipulating the parameters of\nthe coefficient masking function. Increasing the pre-learned dictionary\nsize improves overall speech enhancement quality at increased computational\ncost. We show that real-time GCC-NMF has potential for real-world application,\nremaining purely unsupervised and retaining the simplicity and flexibility\nof offline GCC-NMF.\n"
      ]
    },
    "rouhe17_interspeech": {
      "authors": [
        [
          "Aku",
          "Rouhe"
        ],
        [
          "Reima",
          "Karhila"
        ],
        [
          "Peter",
          "Smit"
        ],
        [
          "Mikko",
          "Kurimo"
        ]
      ],
      "title": "Reading Validation for Pronunciation Evaluation in the Digitala Project",
      "original": "2033",
      "page_count": 2,
      "order": 429,
      "p1": "2050",
      "pn": "2051",
      "abstract": [
        "We describe a recognition, validation and segmentation system as an\nintelligent preprocessor for automatic pronunciation evaluation. The\nsystem is developed for large-scale high stake foreign language tests,\nwhere it is necessary to reduce human workload and ensure fair evaluation.\n"
      ]
    },
    "pelachaud17_interspeech": {
      "authors": [
        [
          "Catherine",
          "Pelachaud"
        ]
      ],
      "title": "Conversing with Social Agents That Smile and Laugh",
      "original": "3003",
      "page_count": 1,
      "order": 430,
      "p1": "2052",
      "pn": "2052",
      "abstract": [
        "Our aim is to create virtual conversational partners. As such we have\ndeveloped computational models to enrich virtual characters with socio-emotional\ncapabilities that are communicated through multimodal behaviors. The\napproach we follow to build interactive and expressive interactants\nrelies on theories from human and social sciences as well as data analysis\nand  user-perception-based design. We have explored specific social\nsignals such as smile and laughter, capturing their variation in production\nbut also their different communicative functions and their impact in\nhuman-agent interaction. Lately we have been interested in modeling\nagents with social attitudes. Our aim is to model how social attitudes\ncolor the multimodal behaviors of the agents. We have gathered a corpus\nof dyads that was annotated along two layers: social attitudes and\nnonverbal behaviors. By applying sequence mining methods we have extracted\nbehavior patterns involved in the change of perception of an attitude.\nWe are particularly interested in capturing the behaviors that correspond\nto a change of perception of an attitude. In this talk I will present\nthe GRETA/VIB platform where our research is implemented.\n"
      ]
    },
    "papadopoulos17_interspeech": {
      "authors": [
        [
          "Pavlos",
          "Papadopoulos"
        ],
        [
          "Ruchir",
          "Travadi"
        ],
        [
          "Colin",
          "Vaz"
        ],
        [
          "Nikolaos",
          "Malandrakis"
        ],
        [
          "Ulf",
          "Hermjakob"
        ],
        [
          "Nima",
          "Pourdamghani"
        ],
        [
          "Michael",
          "Pust"
        ],
        [
          "Boliang",
          "Zhang"
        ],
        [
          "Xiaoman",
          "Pan"
        ],
        [
          "Di",
          "Lu"
        ],
        [
          "Ying",
          "Lin"
        ],
        [
          "Ond\u0159ej",
          "Glembek"
        ],
        [
          "Murali Karthick",
          "Baskar"
        ],
        [
          "Martin",
          "Karafi\u00e1t"
        ],
        [
          "Luk\u00e1\u0161",
          "Burget"
        ],
        [
          "Mark",
          "Hasegawa-Johnson"
        ],
        [
          "Heng",
          "Ji"
        ],
        [
          "Jonathan",
          "May"
        ],
        [
          "Kevin",
          "Knight"
        ],
        [
          "Shrikanth S.",
          "Narayanan"
        ]
      ],
      "title": "Team ELISA System for DARPA LORELEI Speech Evaluation 2016",
      "original": "0180",
      "page_count": 5,
      "order": 431,
      "p1": "2053",
      "pn": "2057",
      "abstract": [
        "In this paper, we describe the system designed and developed by team\nELISA for DARPA&#8217;s LORELEI (Low Resource Languages for Emergent\nIncidents) pilot speech evaluation. The goal of the LORELEI program\nis to guide rapid resource deployment for humanitarian relief (e.g.\nfor natural disasters), with a focus on &#8220;low-resource&#8221;\nlanguage locations, where the cost of developing technologies for automated\nhuman language tools can be prohibitive both in monetary terms and\ntimewise. In this phase of the program, the speech evaluation consisted\nof three separate tasks: detecting presence of an incident, classifying\nincident type, and classifying incident type along with identifying\nthe location where it occurs. The performance metric was area under\ncurve of precision-recall curves. Team ELISA competed against five\nother teams and won all the subtasks.\n"
      ],
      "doi": "10.21437/Interspeech.2017-180"
    },
    "mihajlik17_interspeech": {
      "authors": [
        [
          "P\u00e9ter",
          "Mihajlik"
        ],
        [
          "Lili",
          "Szab\u00f3"
        ],
        [
          "Bal\u00e1zs",
          "Tarj\u00e1n"
        ],
        [
          "Andr\u00e1s",
          "Balog"
        ],
        [
          "Krisztina",
          "R\u00e1bai"
        ]
      ],
      "title": "First Results in Developing a Medieval Latin Language Charter Dictation System for the East-Central Europe Region",
      "original": "1558",
      "page_count": 5,
      "order": 432,
      "p1": "2058",
      "pn": "2062",
      "abstract": [
        "Latin had served as an official language across Europe from the Roman\nEmpire until the 19<SUP>th</SUP> century. As a result, vast amount\nof Latin language historical documents (charters, account books) survived\nfrom the Middle Ages, waiting for recovery. In the digitization process,\ntremendous human efforts are needed for the transliteration of textual\ncontent, as the applicability of optical character recognition techniques\nis often limited. In the era of Digital Humanities our aim is to accelerate\nthe transcription by using automatic speech recognition technology.\nWe introduce the challenges and our initial results in developing a\nreal-time, medieval Latin language LVCSR dictation system for East-Central\nEurope (ECE). In this region, the pronunciation and usage of medieval\nLatin is considered to be roughly uniform. At this phase of the research,\ntherefore, Latin speech data was not collected for acoustic model training\nbut only for test purposes &#8212; from a selection of ECE countries.\nOur experimental results, however, suggest that ECE Latin varies significantly\ndepending on the primary national language on both acoustic-phonetic\nand grammatical levels. On the other hand, unexpectedly low word error\nrates are obtained for several speakers whose native language is completely\nuncovered by the applied training data.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1558"
    },
    "watson17_interspeech": {
      "authors": [
        [
          "C.I.",
          "Watson"
        ],
        [
          "P.J.",
          "Keegan"
        ],
        [
          "M.A.",
          "Maclagan"
        ],
        [
          "R.",
          "Harlow"
        ],
        [
          "J.",
          "King"
        ]
      ],
      "title": "The Motivation and Development of  MPAi, a M&#257;ori Pronunciation Aid",
      "original": "0215",
      "page_count": 5,
      "order": 433,
      "p1": "2063",
      "pn": "2067",
      "abstract": [
        "This paper outlines the motivation and development of a pronunciation\naid ( MPAi) for the M&#257;ori language, the language of the indigenous\npeople of New Zealand. M&#257;ori is threatened and after a break in\ntransmission the language is currently undergoing revitalization. The\ndata for the aid has come from a corpus of 60 speakers (men and women).\nThe language aid allows users to model their speech against exemplars\nfrom young speakers or older speakers of M&#257;ori. This is important,\nbecause of the status of the elders in the M&#257;ori speaking community,\nbut it also recognizes that M&#257;ori is undergoing substantial vowel\nchange. The pronunciation aid gives feedback on vowel production via\nformant analysis, and selected words via speech recognition. The evaluation\nof the aid by 22 language teachers is presented and the resulting changes\nare discussed.\n"
      ],
      "doi": "10.21437/Interspeech.2017-215"
    },
    "feng17_interspeech": {
      "authors": [
        [
          "Siyuan",
          "Feng"
        ],
        [
          "Tan",
          "Lee"
        ]
      ],
      "title": "On the Linguistic Relevance of Speech Units Learned by Unsupervised Acoustic Modeling",
      "original": "0300",
      "page_count": 5,
      "order": 434,
      "p1": "2068",
      "pn": "2072",
      "abstract": [
        "Unsupervised acoustic modeling is an important and challenging problem\nin spoken language technology development for low-resource languages.\nIt aims at automatically learning a set of speech units from un-transcribed\ndata. These learned units are expected to be related to fundamental\nlinguistic units that constitute the concerned language. Formulated\nas a clustering problem, unsupervised acoustic modeling methods are\noften evaluated in terms of average purity or similar types of performance\nmeasures. They do not provide detailed insights on the fitness of individual\nlearned units and the relation between them. This paper presents an\ninvestigation on the linguistic relevance of learned speech units based\non Kullback-Leibler (KL) divergence. A symmetric KL divergence metric\nis used to measure the distance between each pair of learned unit and\nground-truth phoneme of the target language. Experimental analysis\non a multilingual database shows that KL divergence is consistent with\npurity in evaluating clustering results. The deviation between a learned\nunit and its closest ground-truth phoneme is comparable to the inherent\nvariability of the phoneme. The learned speech units have a good coverage\nof linguistically defined phonemes. However, there are certain phonemes\nthat can not be covered, for example, the retroflex final /er/ in Mandarin.\n"
      ],
      "doi": "10.21437/Interspeech.2017-300"
    },
    "das17_interspeech": {
      "authors": [
        [
          "Amit",
          "Das"
        ],
        [
          "Mark",
          "Hasegawa-Johnson"
        ],
        [
          "Karel",
          "Vesel\u00fd"
        ]
      ],
      "title": "Deep Auto-Encoder Based Multi-Task Learning Using Probabilistic Transcriptions",
      "original": "0582",
      "page_count": 5,
      "order": 435,
      "p1": "2073",
      "pn": "2077",
      "abstract": [
        "We examine a scenario where we have no access to native transcribers\nin the target language. This is typical of language communities that\nare under-resourced. However, turkers (online crowd workers) available\nin online marketplaces can serve as valuable alternative resources\nfor providing transcripts in the target language. We assume that the\nturkers neither speak nor have any familiarity with the target language.\nThus, they are unable to distinguish all phone pairs in the target\nlanguage; their transcripts therefore specify, at best, a probability\ndistribution called a probabilistic transcript (PT). Standard deep\nneural network (DNN) training using PTs do not necessarily improve\nerror rates. Previously reported results have demonstrated some success\nby adopting the multi-task learning (MTL) approach. In this study,\nwe report further improvements by introducing a deep auto-encoder based\nMTL. This method leverages large amounts of untranscribed data in the\ntarget language in addition to the PTs obtained from turkers. Furthermore,\nto encourage transfer learning in the feature space, we also examine\nthe effect of using monophones from transcripts in well-resourced languages.\nWe report consistent improvement in phone error rates (PER) for Swahili,\nAmharic, Dinka, and Mandarin.\n"
      ],
      "doi": "10.21437/Interspeech.2017-582"
    },
    "gutkin17_interspeech": {
      "authors": [
        [
          "Alexander",
          "Gutkin"
        ],
        [
          "Richard",
          "Sproat"
        ]
      ],
      "title": "Areal and Phylogenetic Features for Multilingual Speech Synthesis",
      "original": "0160",
      "page_count": 5,
      "order": 436,
      "p1": "2078",
      "pn": "2082",
      "abstract": [
        "We introduce phylogenetic and areal language features to the domain\nof multilingual text-to-speech synthesis. Intuitively, enriching the\nexisting universal phonetic features with cross-lingual shared representations\nshould benefit the multilingual acoustic models and help to address\nissues like data scarcity for low-resource languages. We investigate\nthese representations using the acoustic models based on long short-term\nmemory recurrent neural networks. Subjective evaluations conducted\non eight languages from diverse language families show that sometimes\nphylogenetic and areal representations lead to significant multilingual\nsynthesis quality improvements. To help better leverage these novel\nfeatures, improving the baseline phonetic representation may be necessary.\n"
      ],
      "doi": "10.21437/Interspeech.2017-160"
    },
    "hall17_interspeech": {
      "authors": [
        [
          "Kathleen Currie",
          "Hall"
        ],
        [
          "Scott",
          "Mackie"
        ],
        [
          "Michael",
          "Fry"
        ],
        [
          "Oksana",
          "Tkachman"
        ]
      ],
      "title": " SLPAnnotator: Tools for Implementing Sign Language Phonetic Annotation",
      "original": "0636",
      "page_count": 5,
      "order": 437,
      "p1": "2083",
      "pn": "2087",
      "abstract": [
        "This paper introduces a new resource for building phonetically transcribed\ncorpora of signed languages. The free, open-source software tool, \nSLPAnnotator, is designed to facilitate the transcription of hand configurations\nusing a slightly modified version of the Sign Language Phonetic Annotation\n(SLPA) system ([1], [2], [3], [4]; see also [5]).<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  While the SLPA system\nis extremely phonetically detailed, it can be seen as cumbersome and,\nperhaps, harder for humans to use and interpret than other transcription\nsystems (e.g. Prosodic Model Handshape Coding, [6]).  SLPAnnotator\nis designed to bridge the gap between such systems by automating some\nof the transcription process, providing users with informative references\nabout possible configurations as they are coding, giving continuously\nupdatable access to a visual model of the transcribed handshape, and\nallowing users to verify that transcribed handshapes are both phonologically\nand anatomically plausible. Finally,  SLPAnnotator is designed to interface\nwith other analysis tools, such as  Phonological CorpusTools ([7],\n[8]), to allow for subsequent phonological analysis of the resulting\nsign language corpora.\n"
      ],
      "doi": "10.21437/Interspeech.2017-636"
    },
    "schwarz17_interspeech": {
      "authors": [
        [
          "Iris-Corinna",
          "Schwarz"
        ],
        [
          "Noor",
          "Botros"
        ],
        [
          "Alekzandra",
          "Lord"
        ],
        [
          "Amelie",
          "Marcusson"
        ],
        [
          "Henrik",
          "Tidelius"
        ],
        [
          "Ellen",
          "Marklund"
        ]
      ],
      "title": "The LENA System Applied to Swedish: Reliability of the Adult Word Count Estimate",
      "original": "1287",
      "page_count": 5,
      "order": 438,
      "p1": "2088",
      "pn": "2092",
      "abstract": [
        "The Language Environment Analysis system LENA is used to capture day-long\nrecordings of children&#8217;s natural audio environment. The system\nperforms automated segmentation of the recordings and provides estimates\nfor various measures. One of those measures is Adult Word Count (AWC),\nan approximation of the number of words spoken by adults in close proximity\nto the child. The LENA system was developed for and trained on American\nEnglish, but it has also been evaluated on its performance when applied\nto Spanish, Mandarin and French. The present study is the first evaluation\nof the LENA system applied to Swedish, and focuses on the AWC estimate.\nTwelve five-minute segments were selected at random from each of four\nday-long recordings of 30-month-old children. Each of these 48 segments\nwas transcribed by two transcribers, and both number of words and number\nof vowels were calculated (inter-transcriber reliability for words:\nr = .95, vowels: r = .93). Both counts correlated with the LENA system&#8217;s\nAWC estimate for the same segments (words: r = .67, vowels: r = .66).\nThe reliability of the AWC as estimated by the LENA system when applied\nto Swedish is therefore comparable to its reliability for Spanish,\nMandarin and French.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1287"
    },
    "casillas17_interspeech": {
      "authors": [
        [
          "Marisa",
          "Casillas"
        ],
        [
          "Andrei",
          "Amatuni"
        ],
        [
          "Amanda",
          "Seidl"
        ],
        [
          "Melanie",
          "Soderstrom"
        ],
        [
          "Anne S.",
          "Warlaumont"
        ],
        [
          "Elika",
          "Bergelson"
        ]
      ],
      "title": "What do Babies Hear? Analyses of Child- and Adult-Directed Speech",
      "original": "1409",
      "page_count": 5,
      "order": 439,
      "p1": "2093",
      "pn": "2097",
      "abstract": [
        "Child-directed speech is argued to facilitate language development,\nand is found cross-linguistically and cross-culturally to varying degrees.\nHowever, previous research has generally focused on short samples of\nchild-caregiver interaction, often in the lab or with experimenters\npresent. We test the generalizability of this phenomenon with an initial\ndescriptive analysis of the speech heard by young children in a large,\nunique collection of naturalistic, daylong home recordings. Trained\nannotators coded automatically-detected adult speech &#8216;utterances&#8217;\nfrom 61 homes across 4 North American cities, gathered from children\n(age 2&#8211;24 months) wearing audio recorders during a typical day.\nCoders marked the speaker gender (male/female) and intended addressee\n(child/adult), yielding 10,886 addressee and gender tags from 2,523\nminutes of audio (cf. HB-CHAAC Interspeech ComParE challenge; Schuller\net al., in press). Automated speaker-diarization (LENA) incorrectly\ngender-tagged 30% of male adult utterances, compared to manually-coded\nconsensus. Furthermore, we find effects of SES and gender on child-directed\nand overall speech, increasing child-directed speech with child age,\nand interactions of speaker gender, child gender, and child age: female\ncaretakers increased their child-directed speech more with age than\nmale caretakers did, but only for male infants. Implications for language\nacquisition and existing classification algorithms are discussed.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1409"
    },
    "casillas17b_interspeech": {
      "authors": [
        [
          "Marisa",
          "Casillas"
        ],
        [
          "Elika",
          "Bergelson"
        ],
        [
          "Anne S.",
          "Warlaumont"
        ],
        [
          "Alejandrina",
          "Cristia"
        ],
        [
          "Melanie",
          "Soderstrom"
        ],
        [
          "Mark",
          "VanDam"
        ],
        [
          "Han",
          "Sloetjes"
        ]
      ],
      "title": "A New Workflow for Semi-Automatized Annotations: Tests with Long-Form Naturalistic Recordings of Childrens Language Environments",
      "original": "1418",
      "page_count": 5,
      "order": 440,
      "p1": "2098",
      "pn": "2102",
      "abstract": [
        "Interoperable annotation formats are fundamental to the utility, expansion,\nand sustainability of collective data repositories. In language development\nresearch, shared annotation schemes have been critical to facilitating\nthe transition from raw acoustic data to searchable, structured corpora.\nCurrent schemes typically require comprehensive and manual annotation\nof utterance boundaries and orthographic speech content, with an additional,\noptional range of tags of interest. These schemes have been enormously\nsuccessful for datasets on the scale of dozens of recording hours but\nare untenable for long-format recording corpora, which routinely contain\nhundreds to thousands of audio hours. Long-format corpora would benefit\ngreatly from (semi-)automated analyses, both on the earliest steps\nof annotation &#8212; voice activity detection, utterance segmentation,\nand speaker diarization &#8212; as well as later steps &#8212; e.g.,\nclassification-based codes such as child-vs-adult-directed speech,\nand speech recognition to produce phonetic/orthographic representations.\nWe present an annotation workflow specifically designed for long-format\ncorpora which can be tailored by individual researchers and which interfaces\nwith the current dominant scheme for short-format recordings. The workflow\nallows semi-automated annotation and analyses at higher linguistic\nlevels. We give one example of how the workflow has been successfully\nimplemented in a large cross-database project.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1418"
    },
    "bergmann17_interspeech": {
      "authors": [
        [
          "Christina",
          "Bergmann"
        ],
        [
          "Sho",
          "Tsuji"
        ],
        [
          "Alejandrina",
          "Cristia"
        ]
      ],
      "title": "Top-Down versus Bottom-Up Theories of Phonological Acquisition: A Big Data Approach",
      "original": "1443",
      "page_count": 5,
      "order": 441,
      "p1": "2103",
      "pn": "2107",
      "abstract": [
        "Recent work has made available a number of standardized meta-analyses\nbearing on various aspects of infant language processing. We utilize\ndata from two such meta-analyses (discrimination of vowel contrasts\nand word segmentation, i.e., recognition of word forms extracted from\nrunning speech) to assess whether the published body of empirical evidence\nsupports a bottom-up versus a top-down theory of early phonological\ndevelopment by leveling the power of results from thousands of infants.\nWe predicted that if infants can rely purely on auditory experience\nto develop their phonological categories, then vowel discrimination\nand word segmentation should develop in parallel, with the latter being\npotentially lagged compared to the former. However, if infants crucially\nrely on word form information to build their phonological categories,\nthen development at the word level must precede the acquisition of\nnative sound categories. Our results do not support the latter prediction.\nWe discuss potential implications and limitations, most saliently that\nword forms are only one top-down level proposed to affect phonological\ndevelopment, with other proposals suggesting that top-down pressures\nemerge from lexical (i.e., word-meaning pairs) development. This investigation\nalso highlights general procedures by which standardized meta-analyses\nmay be reused to answer theoretical questions spanning across phenomena.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1443"
    },
    "tsuji17b_interspeech": {
      "authors": [
        [
          "Sho",
          "Tsuji"
        ],
        [
          "Alejandrina",
          "Cristia"
        ]
      ],
      "title": "Which Acoustic and Phonological Factors Shape Infants&#8217; Vowel Discrimination? Exploiting Natural Variation in InPhonDB",
      "original": "1468",
      "page_count": 5,
      "order": 442,
      "p1": "2108",
      "pn": "2112",
      "abstract": [
        "A key research question in early language acquisition concerns the\ndevelopment of infants&#8217; ability to discriminate sounds, and the\nfactors structuring discrimination abilities. Vowel discrimination,\nin particular, has been studied using a range of tasks, experimental\nparadigms, and stimuli over the past 40 years, work recently compiled\nin a meta-analysis. We use this meta-analysis to assess whether there\nis statistical evidence for the following factors affecting effect\nsizes across studies: (1) the order in which the two vowel stimuli\nare presented; and (2) the distance between the vowels, measured acoustically\nin terms of spectral and quantity differences. The magnitude of effect\nsizes analysis revealed order effects consistent with the Natural Referent\nVowels framework, with greater effect sizes when the second vowel was\nmore peripheral than the first. Additionally, we find that spectral\nacoustic distinctiveness is a consistent predictor of studies&#8217;\neffect sizes, while temporal distinctiveness did not predict effect\nsize magnitude. None of these factors interacted significantly with\nage. We discuss implications of these results for language acquisition,\nand more generally developmental psychology, research.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1468"
    },
    "chasaide17b_interspeech": {
      "authors": [
        [
          "Ailbhe N\u00ed",
          "Chasaide"
        ],
        [
          "Neasa N\u00ed",
          "Chiar\u00e1in"
        ],
        [
          "Christoph",
          "Wendler"
        ],
        [
          "Harald",
          "Berthelsen"
        ],
        [
          "Andy",
          "Murphy"
        ],
        [
          "Christer",
          "Gobl"
        ]
      ],
      "title": "The ABAIR Initiative: Bringing Spoken Irish into the Digital Space",
      "original": "1407",
      "page_count": 5,
      "order": 443,
      "p1": "2113",
      "pn": "2117",
      "abstract": [
        "The processes of language demise take hold when a language ceases to\nbelong to the mainstream of life&#8217;s activities. Digital communication\ntechnology increasingly pervades all aspects of modern life. Languages\nnot digitally &#8216;available&#8217; are ever more marginalised, whereas\na digital presence often yields unexpected opportunities to integrate\nthe language into the mainstream. The ABAIR initiative embraces three\ncentral aspects of speech technology development for Irish (Gaelic):\nthe provision of technology-oriented linguistic-phonetic resources;\nthe building and perfecting of core speech technologies; and the development\nof technology applications, which exploit both the technologies and\nthe linguistic resources. The latter enable the public, learners, and\nthose with disabilities to integrate Irish into their day-to-day usage.\nThis paper outlines some of the specific linguistic and sociolinguistic\nchallenges and the approaches adopted to address them. Although machine-learning\napproaches are helping to speed up the process of technology provision,\nthe ABAIR experience highlights how phonetic-linguistic resources are\nalso crucial to the development process. For the endangered language,\nlinguistic resources are central to many applications that impact on\nlanguage usage. The sociolinguistic context and the needs of potential\nend users should be central considerations in setting research priorities\nand deciding on methods.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1407"
    },
    "saeb17_interspeech": {
      "authors": [
        [
          "Armin",
          "Saeb"
        ],
        [
          "Raghav",
          "Menon"
        ],
        [
          "Hugh",
          "Cameron"
        ],
        [
          "William",
          "Kibira"
        ],
        [
          "John",
          "Quinn"
        ],
        [
          "Thomas",
          "Niesler"
        ]
      ],
      "title": "Very Low Resource Radio Browsing for Agile Developmental and Humanitarian Monitoring",
      "original": "0880",
      "page_count": 5,
      "order": 444,
      "p1": "2118",
      "pn": "2122",
      "abstract": [
        "We present a radio browsing system developed on a very small corpus\nof annotated speech by using semi-supervised training of multilingual\nDNN/HMM acoustic models. This system is intended to support relief\nand developmental programmes by the United Nations (UN) in parts of\nAfrica where the spoken languages are extremely under resourced. We\nassume the availability of 12 minutes of annotated speech in the target\nlanguage, and show how this can best be used to develop an acoustic\nmodel. First, a multilingual DNN/HMM is trained using Acholi as the\ntarget language and Luganda, Ugandan English and South African English\nas source languages. We show that the lowest word error rates are achieved\nby using this model to label further untranscribed target language\ndata and then developing SGMM acoustic model from the extended dataset.\nThe performance of an ASR system trained in this way is sufficient\nfor keyword detection that yields useful and actionable near real-time\ninformation to developmental organisations.\n"
      ],
      "doi": "10.21437/Interspeech.2017-880"
    },
    "malandrakis17_interspeech": {
      "authors": [
        [
          "Nikolaos",
          "Malandrakis"
        ],
        [
          "Ond\u0159ej",
          "Glembek"
        ],
        [
          "Shrikanth S.",
          "Narayanan"
        ]
      ],
      "title": "Extracting Situation Frames from Non-English Speech: Evaluation Framework and Pilot Results",
      "original": "0226",
      "page_count": 5,
      "order": 445,
      "p1": "2123",
      "pn": "2127",
      "abstract": [
        "This paper describes the first evaluation framework for the extraction\nof Situation Frames &#8212; structures describing humanitarian assistance\nneeds &#8212; from non-English speech audio, conducted for the DARPA\nLORELEI (Low Resource Languages for Emergent Incidents) program. Participants\nin LORELEI had to process audio from a variety of sources, in non-English\nlanguages, and extract the information required to populate Situation\nFrames describing whether any need is mentioned, the type of need present\nand where the need exists. The evaluation was conducted over a period\nof 10 days and attracted submissions from 6 teams, each team spanning\nmultiple organizations. Performance was evaluated using precision-recall\ncurves. The results are encouraging, with most teams showing some capability\nto detect the type of situation discussed, but more work will be required\nto connect needs to specific locations.\n"
      ],
      "doi": "10.21437/Interspeech.2017-226"
    },
    "kocharov17_interspeech": {
      "authors": [
        [
          "Daniil",
          "Kocharov"
        ],
        [
          "Tatiana",
          "Kachkovskaia"
        ],
        [
          "Pavel",
          "Skrelin"
        ]
      ],
      "title": "Eliciting Meaningful Units from Speech",
      "original": "0855",
      "page_count": 5,
      "order": 446,
      "p1": "2128",
      "pn": "2132",
      "abstract": [
        "Elicitation of information structure from speech is a crucial step\nin automatic speech understanding. In terms of both production and\nperception, we consider intonational phrase to be the basic meaningful\nunit of information structure in speech. The current paper presents\na method of detecting these units in speech by processing both the\nrecorded speech and its textual representation. Using syntactic information,\nwe split text into small groups of words closely connected with each\nother. Assuming that intonational phrases are built from these small\ngroups, we use acoustic information to reveal their actual boundaries.\nThe procedure was initially developed for processing Russian speech,\nand we have achieved the best published results for this language with\nF1 equal to 0.91. We assume that it may be adapted for other languages\nthat have some amount of read speech resources, including under-resourced\nlanguages. For comparison we have evaluated it on English material\n(Boston University Radio Speech Corpus). Our results, F1 of 0.76, are\ncomparable with the top systems designed for English.\n"
      ],
      "doi": "10.21437/Interspeech.2017-855"
    },
    "bhati17_interspeech": {
      "authors": [
        [
          "Saurabhchand",
          "Bhati"
        ],
        [
          "Shekhar",
          "Nayak"
        ],
        [
          "K. Sri Rama",
          "Murty"
        ]
      ],
      "title": "Unsupervised Speech Signal to Symbol Transformation for Zero Resource Speech Applications",
      "original": "1476",
      "page_count": 5,
      "order": 447,
      "p1": "2133",
      "pn": "2137",
      "abstract": [
        "Zero resource speech processing refers to a scenario where no or minimal\ntranscribed data is available. In this paper, we propose a three-step\nunsupervised approach to zero resource speech processing, which does\nnot require any other information/dataset. In the first step, we segment\nthe speech signal into phoneme-like units, resulting in a large number\nof varying length segments. The second step involves clustering the\nvarying-length segments into a finite number of clusters so that each\nsegment can be labeled with a cluster index. The unsupervised transcriptions,\nthus obtained, can be thought of as a sequence of virtual phone labels.\nIn the third step, a deep neural network classifier is trained to map\nthe feature vectors extracted from the signal to its corresponding\nvirtual phone label. The virtual phone posteriors extracted from the\nDNN are used as features in the zero resource speech processing. The\neffectiveness of the proposed approach is evaluated on both ABX and\nspoken term discovery tasks (STD) using spontaneous American English\nand Tsonga language datasets, provided as part of zero resource 2015\nchallenge. It is observed that the proposed system outperforms baselines,\nsupplied along the datasets, in both the tasks without any task specific\nmodifications.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1476"
    },
    "gauthier17_interspeech": {
      "authors": [
        [
          "Elodie",
          "Gauthier"
        ],
        [
          "Laurent",
          "Besacier"
        ],
        [
          "Sylvie",
          "Voisin"
        ]
      ],
      "title": "Machine Assisted Analysis of Vowel Length Contrasts in Wolof",
      "original": "0268",
      "page_count": 5,
      "order": 448,
      "p1": "2138",
      "pn": "2142",
      "abstract": [
        "Growing digital archives and improving algorithms for automatic analysis\nof text and speech create new research opportunities for fundamental\nresearch in phonetics. Such empirical approaches allow statistical\nevaluation of a much larger set of hypothesis about phonetic variation\nand its conditioning factors (among them geographical / dialectal variants).\nThis paper illustrates this vision and proposes to challenge automatic\nmethods for the analysis of a not easily observable phenomenon: vowel\nlength contrast. We focus on Wolof, an under-resourced language from\nSub-Saharan Africa. In particular, we propose multiple features to\nmake a fine evaluation of the degree of length contrast under different\nfactors such as: read  vs semi-spontaneous speech; standard  vs dialectal\nWolof. Our measures made fully automatically on more than 20k vowel\ntokens show that our proposed features can highlight different degrees\nof contrast for each vowel considered. We notably show that contrast\nis weaker in semi-spontaneous speech and in a non standard semi-spontaneous\ndialect.\n"
      ],
      "doi": "10.21437/Interspeech.2017-268"
    },
    "glarner17_interspeech": {
      "authors": [
        [
          "Thomas",
          "Glarner"
        ],
        [
          "Benedikt",
          "Boenninghoff"
        ],
        [
          "Oliver",
          "Walter"
        ],
        [
          "Reinhold",
          "Haeb-Umbach"
        ]
      ],
      "title": "Leveraging Text Data for Word Segmentation for Underresourced Languages",
      "original": "1262",
      "page_count": 5,
      "order": 449,
      "p1": "2143",
      "pn": "2147",
      "abstract": [
        "In this contribution we show how to exploit text data to support word\ndiscovery from audio input in an underresourced target language. Given\naudio, of which a certain amount is transcribed at the word level,\nand additional unrelated text data, the approach is able to learn a\nprobabilistic mapping from acoustic units to characters and utilize\nit to segment the audio data into words without the need of a pronunciation\ndictionary. This is achieved by three components: an unsupervised acoustic\nunit discovery system, a supervisedly trained acoustic unit-to-grapheme\nconverter, and a word discovery system, which is initialized with a\nlanguage model trained on the text data. Experiments for multiple setups\nshow that the initialization of the language model with text data improves\nthe word segmentation performance by a large margin.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1262"
    },
    "zhuang17_interspeech": {
      "authors": [
        [
          "Xiaodan",
          "Zhuang"
        ],
        [
          "Arnab",
          "Ghoshal"
        ],
        [
          "Antti-Veikko",
          "Rosti"
        ],
        [
          "Matthias",
          "Paulik"
        ],
        [
          "Daben",
          "Liu"
        ]
      ],
      "title": "Improving DNN Bluetooth Narrowband Acoustic Models by Cross-Bandwidth and Cross-Lingual Initialization",
      "original": "1129",
      "page_count": 5,
      "order": 450,
      "p1": "2148",
      "pn": "2152",
      "abstract": [
        "The success of deep neural network (DNN) acoustic models is partly\nowed to large amounts of training data available for different applications.\nThis work investigates ways to improve DNN acoustic models for Bluetooth\nnarrowband mobile applications when relatively small amounts of in-domain\ntraining data are available. To address the challenge of limited in-domain\ndata, we use cross-bandwidth and cross-lingual transfer learning methods\nto leverage knowledge from other domains with more training data (different\nbandwidth and/or languages). Specifically, narrowband DNNs in a target\nlanguage are initialized using the weights of DNNs trained on bandlimited\nwide-band data in the same language or those trained on a different\n(resource-rich) language. We investigate multiple recipes involving\nsuch methods with different data resources. For all languages in our\nexperiments, these recipes achieve up to 45% relative WER reduction,\ncompared to training solely on the Bluetooth narrowband data in the\ntarget language. Furthermore, these recipes are very beneficial even\nwhen over two hundred hours of manually transcribed in-domain data\nis available, and we can achieve better accuracy than the baselines\nwith as little as 20 hours of in-domain data.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1129"
    },
    "abraham17_interspeech": {
      "authors": [
        [
          "Basil",
          "Abraham"
        ],
        [
          "S.",
          "Umesh"
        ],
        [
          "Neethu Mariam",
          "Joy"
        ]
      ],
      "title": "Joint Estimation of Articulatory Features and Acoustic Models for Low-Resource Languages",
      "original": "1028",
      "page_count": 5,
      "order": 451,
      "p1": "2153",
      "pn": "2157",
      "abstract": [
        "Using articulatory features for speech recognition improves the performance\nof low-resource languages. One way to obtain articulatory features\nis by using an articulatory classifier (pseudo-articulatory features).\nThe performance of the articulatory features depends on the efficacy\nof this classifier. But, training such a robust classifier for a low-resource\nlanguage is constrained due to the limited amount of training data.\nWe can overcome this by training the articulatory classifier using\na high resource language. This classifier can then be used to generate\narticulatory features for the low-resource language. However, this\ntechnique fails when high and low-resource languages have mismatches\nin their environmental conditions. In this paper, we address both the\naforementioned problems by jointly estimating the articulatory features\nand low-resource acoustic model. The experiments were performed on\ntwo low-resource Indian languages namely, Hindi and Tamil. English\nwas used as the high-resource language. A relative improvement of 23%\nand 10% were obtained for Hindi and Tamil, respectively.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1028"
    },
    "abraham17b_interspeech": {
      "authors": [
        [
          "Basil",
          "Abraham"
        ],
        [
          "Tejaswi",
          "Seeram"
        ],
        [
          "S.",
          "Umesh"
        ]
      ],
      "title": "Transfer Learning and Distillation Techniques to Improve the Acoustic Modeling of Low Resource Languages",
      "original": "1009",
      "page_count": 5,
      "order": 452,
      "p1": "2158",
      "pn": "2162",
      "abstract": [
        "Deep neural networks (DNN) require large amount of training data to\nbuild robust acoustic models for speech recognition tasks. Our work\nis intended in improving the low-resource language acoustic model to\nreach a performance comparable to that of a high-resource scenario\nwith the help of data/model parameters from other high-resource languages.\nWe explore transfer learning and distillation methods, where a complex\nhigh resource model guides or supervises the training of low resource\nmodel. The techniques include (i) multi-lingual framework of borrowing\ndata from high-resource language while training the low-resource acoustic\nmodel. The KL divergence based constraints are added to make the model\nbiased towards low-resource language, (ii) distilling knowledge from\nthe complex high-resource model to improve the low-resource acoustic\nmodel. The experiments were performed on three Indian languages namely\nHindi, Tamil and Kannada. All the techniques gave improved performance\nand the multi-lingual framework with KL divergence regularization giving\nthe best results. In all the three languages a performance close to\nor better than high-resource scenario was obtained.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1009"
    },
    "helgadottir17_interspeech": {
      "authors": [
        [
          "Inga R\u00fan",
          "Helgad\u00f3ttir"
        ],
        [
          "R\u00f3bert",
          "Kjaran"
        ],
        [
          "Anna Bj\u00f6rk",
          "Nikul\u00e1sd\u00f3ttir"
        ],
        [
          "J\u00f3n",
          "Gu\u00f0nason"
        ]
      ],
      "title": "Building an ASR Corpus Using Althingi&#8217;s Parliamentary Speeches",
      "original": "0903",
      "page_count": 5,
      "order": 453,
      "p1": "2163",
      "pn": "2167",
      "abstract": [
        "Acoustic data acquisition for under-resourced languages is an important\nand challenging task. In the Icelandic parliament, Althingi, all performed\nspeeches are transcribed manually and published as text on Althingi&#8217;s\nweb page. To reduce the manual work involved, an automatic speech recognition\nsystem is being developed for Althingi. In this paper the development\nof a speech corpus suitable for the training of a parliamentary ASR\nsystem is described. Text and audio data of manually transcribed speeches\nwere processed to build an aligned, segmented corpus, whereby language\nspecific tasks had to be developed specially for Icelandic. The resulting\ncorpus of 542 hours of speech is freely available on http://www.malfong.is.\nFirst experiments with an ASR system trained on the Althingi corpus\nhave been conducted, showing promising results. Word error rate of\n16.38% was obtained using time-delay deep neural network (TD-DNN) and\n14.76% was obtained using long-short term memory recurrent neural network\n(LSTM-RNN) architecture. The Althingi corpus is to our knowledge the\nlargest speech corpus currently available in Icelandic. The corpus\nas well as the developed methods for corpus creation constitute a valuable\nresource for further developments within Icelandic language technology.\n"
      ],
      "doi": "10.21437/Interspeech.2017-903"
    },
    "alumae17_interspeech": {
      "authors": [
        [
          "Tanel",
          "Alum\u00e4e"
        ],
        [
          "Andrus",
          "Paats"
        ],
        [
          "Ivo",
          "Fridolin"
        ],
        [
          "Einar",
          "Meister"
        ]
      ],
      "title": "Implementation of a Radiology Speech Recognition System for Estonian Using Open Source Software",
      "original": "0928",
      "page_count": 5,
      "order": 454,
      "p1": "2168",
      "pn": "2172",
      "abstract": [
        "Speech recognition has become increasingly popular in radiology reporting\nin the last decade. However, developing a speech recognition system\nfor a new language in a highly specific domain requires a lot of resources,\nexpert knowledge and skills. Therefore, commercial vendors do not offer\nready-made radiology speech recognition systems for less-resourced\nlanguages.<br style=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  This paper describes the implementation of a radiology speech\nrecognition system for Estonian, a language with less than one million\nnative speakers. The system was developed in partnership with a hospital\nthat provided a corpus of written reports for language modeling purposes.\nRewrite rules for pre-processing training texts and postprocessing\nrecognition results were created manually based on a small parallel\ncorpus created by the hospital&#8217;s radiologists, using the Thrax\ntoolkit. Deep neural network based acoustic models were trained based\non 216 hours of out-of-domain data and adapted on 14 hours of spoken\nradiology data, using the Kaldi toolkit. The current word error rate\nof the system is 5.4%. The system is in active use in real clinical\nenvironment.\n"
      ],
      "doi": "10.21437/Interspeech.2017-928"
    },
    "gunason17_interspeech": {
      "authors": [
        [
          "J\u00f3n",
          "Gu\u00f0nason"
        ],
        [
          "Matth\u00edas",
          "P\u00e9tursson"
        ],
        [
          "R\u00f3bert",
          "Kjaran"
        ],
        [
          "Simon",
          "Kl\u00fcpfel"
        ],
        [
          "Anna Bj\u00f6rk",
          "Nikul\u00e1sd\u00f3ttir"
        ]
      ],
      "title": "Building ASR Corpora Using Eyra",
      "original": "1352",
      "page_count": 5,
      "order": 455,
      "p1": "2173",
      "pn": "2177",
      "abstract": [
        "Building acoustic databases for speech recognition is very important\nfor under-resourced languages. To build a speech recognition system,\na large amount of speech data from a considerable number of participants\nneeds to be collected. Eyra is a toolkit that can be used to gather\nacoustic data from a large number of participants in a relatively straight\nforward fashion. Predetermined prompts are downloaded onto a client,\ntypically run on a smartphone, where the participant reads them aloud\nso that the recording and its corresponding prompt can be uploaded.\nThis paper presents the Eyra toolkit, its quality control routines\nand annotation mechanism. The quality control relies on a forced-alignment\nmodule, which gives feedback to the participant, and an annotation\nmodule which allows data collectors to rate the read prompts after\nthey are uploaded to the system. The paper presents an analysis of\nthe performance of the quality control and describes two data collections\nfor Icelandic and Javanese.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1352"
    },
    "niekerk17_interspeech": {
      "authors": [
        [
          "Daniel van",
          "Niekerk"
        ],
        [
          "Charl van",
          "Heerden"
        ],
        [
          "Marelie",
          "Davel"
        ],
        [
          "Neil",
          "Kleynhans"
        ],
        [
          "Oddur",
          "Kjartansson"
        ],
        [
          "Martin",
          "Jansche"
        ],
        [
          "Linne",
          "Ha"
        ]
      ],
      "title": "Rapid Development of TTS Corpora for Four South African Languages",
      "original": "1139",
      "page_count": 5,
      "order": 456,
      "p1": "2178",
      "pn": "2182",
      "abstract": [
        "This paper describes the development of text-to-speech corpora for\nfour South African languages. The approach followed investigated the\npossibility of using low-cost methods including informal recording\nenvironments and untrained volunteer speakers. This objective and the\nadditional future goal of expanding the corpus to increase coverage\nof South Africa&#8217;s 11 official languages necessitated experimenting\nwith multi-speaker and code-switched data. The process and relevant\nobservations are detailed throughout. The latest version of the corpora\nare available for download under an open-source licence and will likely\nsee further development and refinement in future.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1139"
    },
    "gutkin17b_interspeech": {
      "authors": [
        [
          "Alexander",
          "Gutkin"
        ]
      ],
      "title": "Uniform Multilingual Multi-Speaker Acoustic Model for Statistical Parametric Speech Synthesis of Low-Resourced Languages",
      "original": "0037",
      "page_count": 5,
      "order": 457,
      "p1": "2183",
      "pn": "2187",
      "abstract": [
        "Acquiring data for text-to-speech (TTS) systems is expensive. This\ntypically requires large amounts of training data, which is not available\nfor low-resourced languages. Sometimes small amounts of data can be\ncollected, while often no data may be available at all. This paper\npresents an acoustic modeling approach utilizing long short-term memory\n(LSTM) recurrent neural networks (RNN) aimed at partially addressing\nthe language data scarcity problem. Unlike speaker-adaptation systems\nthat aim to preserve speaker similarity across languages, the salient\nfeature of the proposed approach is that, once constructed, the resulting\nsystem does not need retraining to cope with the previously unseen\nlanguages. This is due to language and speaker-agnostic model topology\nand universal linguistic feature set. Experiments on twelve languages\nshow that the system is able to produce intelligible and sometimes\nnatural output when a language is unseen. We also show that, when small\namounts of training data are available, pooling the data sometimes\nimproves the overall intelligibility and naturalness. Finally, we show\nthat sometimes having a multilingual system with no prior exposure\nto the language is better than building single-speaker system from\nsmall amounts of data for that language.\n"
      ],
      "doi": "10.21437/Interspeech.2017-37"
    },
    "mendelson17b_interspeech": {
      "authors": [
        [
          "Joseph",
          "Mendelson"
        ],
        [
          "Pilar",
          "Oplustil"
        ],
        [
          "Oliver",
          "Watts"
        ],
        [
          "Simon",
          "King"
        ]
      ],
      "title": "Nativization of Foreign Names in TTS for Automatic Reading of World News in Swahili",
      "original": "1398",
      "page_count": 5,
      "order": 458,
      "p1": "2188",
      "pn": "2192",
      "abstract": [
        "When a text-to-speech (TTS) system is required to speak world news,\na large fraction of the words to be spoken will be proper names originating\nin a wide variety of languages. Phonetization of these names based\non target language letter-to-sound rules will typically be inadequate.\nThis is detrimental not only during synthesis, when inappropriate phone\nsequences are produced, but also during training, if the system is\ntrained on data from the same domain. This is because poor phonetization\nduring forced alignment based on hidden Markov models can pollute the\nwhole model set, resulting in degraded alignment even of normal target-language\nwords. This paper presents four techniques designed to address this\nissue in the context of a Swahili TTS system: automatic transcription\nof proper names based on a lexicon from a better-resourced language;\nthe addition of a parallel phone set and special part-of-speech tag\nexclusively dedicated to proper names; a manually-crafted phone mapping\nwhich allows substitutions for potentially more accurate phones in\nproper names during forced alignment; the addition in proper names\nof a grapheme-derived frame-level feature, supplementing the standard\nphonetic inputs to the acoustic model. We present results from objective\nand subjective evaluations of systems built using these four techniques.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1398"
    },
    "tong17b_interspeech": {
      "authors": [
        [
          "Rong",
          "Tong"
        ],
        [
          "Nancy F.",
          "Chen"
        ],
        [
          "Bin",
          "Ma"
        ]
      ],
      "title": "Multi-Task Learning for Mispronunciation Detection on Singapore Children&#8217;s Mandarin Speech",
      "original": "0520",
      "page_count": 5,
      "order": 459,
      "p1": "2193",
      "pn": "2197",
      "abstract": [
        "Speech technology for children is more challenging than for adults,\nbecause there is a lack of children&#8217;s speech corpora. Moreover,\nthere is higher heterogeneity in children&#8217;s speech due to variability\nin anatomy across age and gender, larger variance in speaking rate\nand vocal effort, and immature command of word usage, grammar, and\nlinguistic structure. Speech productions from Singapore children possess\neven more variability due to the multilingual environment in the city-state,\ncausing inter-influences from Chinese languages (e.g., Hokkien and\nMandarin), English dialects (e.g., American and British), and Indian\nlanguages (e.g., Hindi and Tamil). In this paper, we show that acoustic\nmodeling of children&#8217;s speech can leverage on a larger set of\nadult data. We compare two data augmentation approaches for children&#8217;s\nacoustic modeling. The first approach disregards the child and adult\ncategories and consolidates the two datasets together as one entire\nset. The second approach is multi-task learning: during training the\nacoustic characteristics of adults and children are jointly learned\nthrough shared hidden layers of the deep neural network, yet they still\nretain their respective targets using two distinct softmax layers.\nWe empirically show that the multi-task learning approach outperforms\nthe baseline in both speech recognition and computer-assisted pronunciation\ntraining.\n"
      ],
      "doi": "10.21437/Interspeech.2017-520"
    },
    "larsen17_interspeech": {
      "authors": [
        [
          "Elin",
          "Larsen"
        ],
        [
          "Alejandrina",
          "Cristia"
        ],
        [
          "Emmanuel",
          "Dupoux"
        ]
      ],
      "title": "Relating Unsupervised Word Segmentation to Reported Vocabulary Acquisition",
      "original": "0937",
      "page_count": 5,
      "order": 460,
      "p1": "2198",
      "pn": "2202",
      "abstract": [
        "A range of computational approaches have been used to model the discovery\nof word forms from continuous speech by infants. Typically, these algorithms\nare evaluated with respect to the ideal &#8216;gold standard&#8217;\nword segmentation and lexicon. These metrics assess how well an algorithm\nmatches the adult state, but may not reflect the intermediate states\nof the child&#8217;s lexical development. We set up a new evaluation\nmethod based on the correlation between word frequency counts derived\nfrom the application of an algorithm onto a corpus of child-directed\nspeech, and the proportion of infants knowing those words, according\nto parental reports. We evaluate a representative set of 4 algorithms,\napplied to transcriptions of the Brent corpus, which have been phonologized\nusing either phonemes or syllables as basic units. Results show remarkable\nvariation in the extent to which these 8 algorithm-unit combinations\npredicted infant vocabulary, with some of these predictions surpassing\nthose derived from the adult gold standard segmentation. We argue that\ninfant vocabulary prediction provides a useful complement to traditional\nevaluation; for example, the best predictor model was also one of the\nworst in terms of segmentation score, and there was no clear relationship\nbetween token or boundary F-score and vocabulary prediction.\n"
      ],
      "doi": "10.21437/Interspeech.2017-937"
    },
    "wiren17_interspeech": {
      "authors": [
        [
          "Mats",
          "Wir\u00e9n"
        ],
        [
          "Kristina N.",
          "Bj\u00f6rkenstam"
        ],
        [
          "Robert",
          "\u00d6stling"
        ]
      ],
      "title": "Modelling the Informativeness of Non-Verbal Cues in Parent-Child Interaction",
      "original": "1143",
      "page_count": 5,
      "order": 461,
      "p1": "2203",
      "pn": "2207",
      "abstract": [
        "Non-verbal cues from speakers, such as eye gaze and hand positions,\nplay an important role in word learning [1]. This is consistent with\nthe notion that for meaning to be reconstructed, acoustic patterns\nneed to be linked to time-synchronous patterns from at least one other\nmodality [2]. In previous studies of a multimodally annotated corpus\nof parent-child interaction, we have shown that parents interacting\nwith infants at the early word-learning stage (7&#8211;9 months) display\na large amount of time-synchronous patterns, but that this behaviour\ntails off with increasing age of the children [3]. Furthermore, we\nhave attempted to quantify the  informativeness of the different non-verbal\ncues, that is, to what extent they actually help to discriminate between\ndifferent possible referents, and how critical the timing of the cues\nis [4]. The purpose of this paper is to generalise our earlier model\nby quantifying informativeness resulting from non-verbal cues occurring\nboth before and after their associated verbal references.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1143"
    },
    "marklund17b_interspeech": {
      "authors": [
        [
          "Ellen",
          "Marklund"
        ],
        [
          "David",
          "Pagmar"
        ],
        [
          "Tove",
          "Gerholm"
        ],
        [
          "Lisa",
          "Gustavsson"
        ]
      ],
      "title": "Computational Simulations of Temporal Vocalization Behavior in Adult-Child Interaction",
      "original": "1289",
      "page_count": 5,
      "order": 462,
      "p1": "2208",
      "pn": "2212",
      "abstract": [
        "The purpose of the present study was to introduce a computational simulation\nof timing in child-adult interaction. The simulation uses temporal\ninformation from real adult-child interactions as default temporal\nbehavior of two simulated agents. Dependencies between the agents&#8217;\nbehavior are added, and how the simulated interactions compare to real\ninteraction data as a result is investigated. In the present study,\nthe real data consisted of transcriptions of a mother interacting with\nher 12-month-old child, and the data simulated was vocalizations. The\nfirst experiment shows that although the two agents generate vocalizations\naccording to the temporal characteristics of the interlocutors in the\nreal data, simulated interaction with no contingencies between the\ntwo agents&#8217; behavior differs from real interaction data. In the\nsecond experiment, a contingency was introduced to the simulation:\nthe likelihood that the adult agent initiated a vocalization if the\nchild agent was already vocalizing. Overall, the simulated data is\nmore similar to the real interaction data when the adult agent is less\nlikely to start speaking while the child agent vocalizes. The results\nare in line with previous studies on turn-taking in parent-child interaction\nat comparable ages. This illustrates that computational simulations\nare useful tools when investigating parent-child interactions.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1289"
    },
    "strombergsson17_interspeech": {
      "authors": [
        [
          "Sofia",
          "Str\u00f6mbergsson"
        ],
        [
          "Jens",
          "Edlund"
        ],
        [
          "Jana",
          "G\u00f6tze"
        ],
        [
          "Kristina Nilsson",
          "Bj\u00f6rkenstam"
        ]
      ],
      "title": "Approximating Phonotactic Input in Children&#8217;s Linguistic Environments from Orthographic Transcripts",
      "original": "1634",
      "page_count": 5,
      "order": 463,
      "p1": "2213",
      "pn": "2217",
      "abstract": [
        "Child-directed spoken data is the ideal source of support for claims\nabout children&#8217;s linguistic environments. However, phonological\ntranscriptions of child-directed speech are scarce, compared to sources\nlike adult-directed speech or text data. Acquiring reliable descriptions\nof children&#8217;s phonological environments from more readily accessible\nsources would mean considerable savings of time and money. The first\nstep towards this goal is to quantify the reliability of descriptions\nderived from such secondary sources.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  We investigate how\nphonological distributions vary across different modalities (spoken\nvs. written), and across the age of the intended audience (children\nvs. adults). Using a previously unseen collection of Swedish adult-\nand child-directed spoken and written data, we combine lexicon look-up\nand grapheme-to-phoneme conversion to approximate phonological characteristics.\nThe analysis shows distributional differences across datasets both\nfor single phonemes and for longer phoneme sequences. Some of these\nare predictably attributed to lexical and contextual characteristics\nof text vs. speech.<br style=\"mso-data-placement:same-cell;\" /><br\nstyle=\"mso-data-placement:same-cell;\" />\n  The generated phonological\ntranscriptions are remarkably reliable. The differences in phonological\ndistributions between child-directed speech and secondary sources highlight\na need for compensatory measures when relying on written data or on\nadult-directed spoken data, and/or for continued collection of actual\nchild-directed speech in research on children&#8217;s language environments.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1634"
    },
    "chaabouni17_interspeech": {
      "authors": [
        [
          "Rahma",
          "Chaabouni"
        ],
        [
          "Ewan",
          "Dunbar"
        ],
        [
          "Neil",
          "Zeghidour"
        ],
        [
          "Emmanuel",
          "Dupoux"
        ]
      ],
      "title": "Learning Weakly Supervised Multimodal Phoneme Embeddings",
      "original": "1689",
      "page_count": 5,
      "order": 464,
      "p1": "2218",
      "pn": "2222",
      "abstract": [
        "Recent works have explored deep architectures for learning multimodal\nspeech representation (e.g. audio and images, articulation and audio)\nin a supervised way. Here we investigate the role of combining different\nspeech modalities, i.e. audio and visual information representing the\nlips&#8217; movements, in a weakly supervised way using Siamese networks\nand lexical same-different side information. In particular, we ask\nwhether one modality can benefit from the other to provide a richer\nrepresentation for phone recognition in a weakly supervised setting.\nWe introduce mono-task and multi-task methods for merging speech and\nvisual modalities for phone recognition. The mono-task learning consists\nin applying a Siamese network on the concatenation of the two modalities,\nwhile the multi-task learning receives several different combinations\nof modalities at train time. We show that multi-task learning enhances\ndiscriminability for visual and multimodal inputs while minimally impacting\nauditory inputs. Furthermore, we present a qualitative analysis of\nthe obtained phone embeddings, and show that cross-modal visual input\ncan improve the discriminability of phonological features which are\nvisually discernable (rounding, open/close, labial place of articulation),\nresulting in representations that are closer to abstract linguistic\nfeatures than those based on audio only.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1689"
    },
    "obuchi17_interspeech": {
      "authors": [
        [
          "Yasunari",
          "Obuchi"
        ]
      ],
      "title": "Personalized Quantification of Voice Attractiveness in Multidimensional Merit Space",
      "original": "0130",
      "page_count": 5,
      "order": 465,
      "p1": "2223",
      "pn": "2227",
      "abstract": [
        "Voice attractiveness is an indicator which is somehow objective and\nsomehow subjective. It would be helpful to assume that each voice has\nits own attractiveness. However, the paired comparison results of human\nlisteners sometimes include inconsistency. In this paper, we propose\na multidimensional mapping scheme of voice attractiveness, which explains\nthe existence of objective merit values of voices and subjective preference\nof listeners. Paired comparison is modeled in a probabilistic framework,\nand the optimal mapping is obtained from the paired comparison results\non the maximum likelihood criterion.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  The merit values can\nbe estimated from the acoustic feature using the machine learning framework.\nWe show how the estimation process works using real database consisting\nof common Japanese greeting utterances. Experiments using 1- and 2-\ndimensional merit spaces confirm that the comparison result prediction\nfrom the acoustic feature becomes more accurate in the 2-dimensional\ncase.\n"
      ],
      "doi": "10.21437/Interspeech.2017-130"
    },
    "bosker17_interspeech": {
      "authors": [
        [
          "Hans Rutger",
          "Bosker"
        ]
      ],
      "title": "The Role of Temporal Amplitude Modulations in the Political Arena: Hillary Clinton vs. Donald Trump",
      "original": "0142",
      "page_count": 5,
      "order": 466,
      "p1": "2228",
      "pn": "2232",
      "abstract": [
        "Speech is an acoustic signal with inherent amplitude modulations in\nthe 1&#8211;9 Hz range. Recent models of speech perception propose\nthat this rhythmic nature of speech is central to speech recognition.\nMoreover, rhythmic amplitude modulations have been shown to have beneficial\neffects on language processing and the subjective impression listeners\nhave of the speaker. This study investigated the role of amplitude\nmodulations in the political arena by comparing the speech produced\nby Hillary Clinton and Donald Trump in the three presidential debates\nof 2016.<br style=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  Inspection of the modulation spectra, revealing the spectral content\nof the two speakers&#8217; amplitude envelopes after matching for overall\nintensity, showed considerably greater power in Clinton&#8217;s modulation\nspectra (compared to Trump&#8217;s) across the three debates, particularly\nin the 1&#8211;9 Hz range. The findings suggest that Clinton&#8217;s\nspeech had a more pronounced temporal envelope with rhythmic amplitude\nmodulations below 9 Hz, with a preference for modulations around 3\nHz. This may be taken as evidence for a more structured temporal organization\nof syllables in Clinton&#8217;s speech, potentially due to more frequent\nuse of preplanned utterances. Outcomes are interpreted in light of\nthe potential beneficial effects of a rhythmic temporal envelope on\nintelligibility and speaker perception.\n"
      ],
      "doi": "10.21437/Interspeech.2017-142"
    },
    "gallardo17b_interspeech": {
      "authors": [
        [
          "Laura Fern\u00e1ndez",
          "Gallardo"
        ],
        [
          "Rafael Zequeira",
          "Jim\u00e9nez"
        ],
        [
          "Sebastian",
          "M\u00f6ller"
        ]
      ],
      "title": "Perceptual Ratings of Voice Likability Collected Through In-Lab Listening Tests vs. Mobile-Based Crowdsourcing",
      "original": "0326",
      "page_count": 5,
      "order": 467,
      "p1": "2233",
      "pn": "2237",
      "abstract": [
        "Human perceptions of speaker characteristics, needed to perform automatic\npredictions from speech features, have generally been collected by\nconducting demanding in-lab listening tests under controlled conditions.\nConcurrently, crowdsourcing has emerged as a valuable approach for\nrunning user studies through surveys or quantitative ratings. Micro-task\ncrowdsourcing markets enable the completion of small tasks (commonly\nof minutes or seconds), rewarding users with micro-payments. This paradigm\npermits effortless collection of user input from a large and diverse\npool of participants at low cost. This paper presents different auditory\ntests for collecting perceptual voice likability ratings employing\na common set of 30 male and female voices. These tests are based on\ndirect scaling and on paired-comparisons, and were conducted in the\nlaboratory and via crowdsourcing using micro-tasks. Design considerations\nare proposed for adapting the laboratory listening tests to a mobile-based\ncrowdsourcing platform to obtain trustworthy listeners&#8217; answers.\nOur likability scores obtained by the different test approaches are\nhighly correlated. This outcome motivates the use of crowdsourcing\nfor future listening tests investigating e.g. speaker characterization,\nreducing the efforts involved in engaging participants and administering\nthe tests on-site.\n"
      ],
      "doi": "10.21437/Interspeech.2017-326"
    },
    "trouvain17_interspeech": {
      "authors": [
        [
          "J\u00fcrgen",
          "Trouvain"
        ],
        [
          "Frank",
          "Zimmerer"
        ]
      ],
      "title": "Attractiveness of French Voices for German Listeners &#8212; Results from Native and Non-Native Read Speech",
      "original": "0367",
      "page_count": 5,
      "order": 468,
      "p1": "2238",
      "pn": "2242",
      "abstract": [
        "This study investigated how the perceived attractiveness of voices\nwas influenced by a foreign language, a foreign accent, and the level\nof fluency in the foreign language. Stimuli were taken from a French-German\ncorpus of read speech with German native speakers as raters. Additional\nfactors were stimulus length (syllable or entire sentence) and sex\n(of the raters and speakers). Results with German native raters reveal\nthat stimuli spanning just a syllable were judged significantly less\nattractive than those containing a sentence, and that stimuli from\nFrench speakers were assessed as more attractive than those of German\nspeakers. This backs the clich&#233; that French has an attractive\nimage for German listeners. An analysis of the best vs. the worst rated\nsentences suggest that an individual mix of voice quality, disfluency\nmanagement, prosodic behaviour and pronunciation precision is responsible\nfor the results.\n"
      ],
      "doi": "10.21437/Interspeech.2017-367"
    },
    "schweitzer17b_interspeech": {
      "authors": [
        [
          "Antje",
          "Schweitzer"
        ],
        [
          "Natalie",
          "Lewandowski"
        ],
        [
          "Daniel",
          "Duran"
        ]
      ],
      "title": "Social Attractiveness in Dialogs",
      "original": "0833",
      "page_count": 5,
      "order": 469,
      "p1": "2243",
      "pn": "2247",
      "abstract": [
        "This study investigates how acoustic and lexical properties of spontaneous\nspeech in dialogs affect perceived social attractiveness in terms of\nspeaker likeability, friendliness, competence, and self-confidence.\nWe analyze a database of longer spontaneous dialogs between German\nfemale speakers and the mutual ratings that dialog partners assigned\nto one another after every conversation. Thus the ratings reflect long-term\nimpressions based on dialog behavior. Using linear mixed models, we\ninvestigate both classical acoustic-prosodic and lexical parameters\nas well as parameters that capture the degree of speakers&#8217; adaptation,\nor &#8220;convergence&#8221;, of these parameters to each other. Specifically\nwe find that likeability is correlated with the speaker&#8217;s lexical\nconvergence as well as with her convergence in f<SUB>0</SUB> peak height.\nFriendliness is significantly related to variation in intensity. For\ncompetence, the proportion of positive words in the dialog, variation\nin shimmer, and overall phonetic convergence are significant correlates.\nSelf-confidence finally is related to several prosodic, phonetic, and\nlexical adaptation parameters. In some cases, the effect depends on\nwhether interlocutors also had eye contact during their conversation.\nTaken together, these findings provide evidence that in addition to\nclassical parameters, convergence parameters play an important role\nin the mutual perception of social attractiveness.\n"
      ],
      "doi": "10.21437/Interspeech.2017-833"
    },
    "novaktot17_interspeech": {
      "authors": [
        [
          "Eszter",
          "Nov\u00e1k-T\u00f3t"
        ],
        [
          "Oliver",
          "Niebuhr"
        ],
        [
          "Aoju",
          "Chen"
        ]
      ],
      "title": "A Gender Bias in the Acoustic-Melodic Features of Charismatic Speech?",
      "original": "1349",
      "page_count": 5,
      "order": 470,
      "p1": "2248",
      "pn": "2252",
      "abstract": [
        "Previous studies proved the immense importance of nonverbal skills\nwhen it comes to being persuasive and coming across as charismatic.\nIt was also found that men sound more convincing and persuasive (i.e.\naltogether more charismatic) than women under otherwise comparable\nconditions. This gender bias is investigated in the present study by\nanalyzing and comparing acoustic-melodic charisma features of male\nand female business executives. In line with the gender bias in perception,\nour results show that female CEOs who are judged to be similarly charismatic\nas their male counterpart(s) produce more and stronger acoustic charisma\ncues. This suggests that there is a gender bias which is compensated\nfor by making a greater effort on the part of the female speakers.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1349"
    },
    "michalsky17_interspeech": {
      "authors": [
        [
          "Jan",
          "Michalsky"
        ],
        [
          "Heike",
          "Schoormann"
        ]
      ],
      "title": "Pitch Convergence as an Effect of Perceived Attractiveness and Likability",
      "original": "1520",
      "page_count": 4,
      "order": 471,
      "p1": "2253",
      "pn": "2256",
      "abstract": [
        "While there is a growing body of research on which and how pitch features\nare perceived as attractive or likable, there are few studies investigating\nhow the impression of a speaker as attractive or likable affects the\nspeech behavior of his/her interlocutor. Recent studies have shown\nthat perceived attractiveness and likability may not only have an effect\non a speaker&#8217;s pitch features in isolation but also on the prosodic\nentrainment. It has been shown that how speakers synchronize their\npitch features relatively to their interlocutor is affected by such\nimpressions. This study investigates pitch convergence, examining whether\nspeakers become more similar over the course of a conversation depending\non perceived attractiveness and/or likability. The expected pitch convergence\nis thereby investigated on two levels, over the entire conversation\n(globally) as well as turn-wise (locally). The results from a speed\ndating experiment with 98 mixed-sex dialogues of heterosexual singles\nshow that speakers become more similar globally and locally over time\nboth in register and range. Furthermore, the degree of pitch convergence\nis greatly affected by perceived attractiveness and likability with\neffects differing between attractiveness and likability as well as\nbetween the global and the local level.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1520"
    },
    "jiao17_interspeech": {
      "authors": [
        [
          "Li",
          "Jiao"
        ],
        [
          "Chengxia",
          "Wang"
        ],
        [
          "Cristiane",
          "Hsu"
        ],
        [
          "Peter",
          "Birkholz"
        ],
        [
          "Yi",
          "Xu"
        ]
      ],
      "title": "Does Posh English Sound Attractive?",
      "original": "1691",
      "page_count": 5,
      "order": 472,
      "p1": "2257",
      "pn": "2261",
      "abstract": [
        "Poshness refers to how much a British English speaker sounds upper\nclass when they talk. Popular descriptions of posh English mostly focus\non vocabulary, accent and phonology. This study tests the hypothesis\nthat, as a social index, poshness is also manifested via phonetic properties\nknown to encode vocal attractiveness. Specifically, posh English, because\nof its impression of being detached, authoritative and condescending,\nwould more closely resemble an attractive male voice than an attractive\nfemale voice. In four experiments, we tested this hypothesis by acoustically\nmanipulating Cambridge-accented English utterances by a male and a\nfemale speaker through PSOLA resynthesis, and having native speakers\nof British English judge how posh or attractive each utterance sounds.\nThe manipulated acoustic dimensions are formant dispersion, pitch shift\nand speech rate. Initial results from the first two experiments showed\na trend in the hypothesized direction for the male speakers&#8217;\nutterances. But for the female utterances there was a ceiling effect\ndue to the frequent alternation of speaker gender within the same test\nsession. When the two speakers&#8217; utterances were separated by\nblocks in the third and fourth experiments, a clearer support for the\nmain hypothesis was found.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1691"
    },
    "baumann17_interspeech": {
      "authors": [
        [
          "Timo",
          "Baumann"
        ]
      ],
      "title": "Large-Scale Speaker Ranking from Crowdsourced Pairwise Listener Ratings",
      "original": "1697",
      "page_count": 5,
      "order": 473,
      "p1": "2262",
      "pn": "2266",
      "abstract": [
        "Speech quality and likability is a multi-faceted phenomenon consisting\nof a combination of perceptory features that cannot easily be computed\nnor weighed automatically. Yet, it is often easy to decide which of\ntwo voices one likes better, even though it would be hard to describe\nwhy, or to name the underlying basic perceptory features. Although\nlikability is inherently subjective and individual preferences differ\nfrequently, generalizations are useful and there is often a broad intersubjective\nconsensus about whether one speaker is more likable than another. However,\nbreaking down likability rankings into pairwise comparisons leads to\na quadratic explosion of rating pairs. We present a methodology and\nsoftware to efficiently create a likability ranking for many speakers\nfrom crowdsourced pairwise likability ratings. We collected pairwise\nlikability ratings for many (&#62;220) speakers from many raters (&#62;160)\nand turn these ratings into one likability ranking. We investigate\nthe resulting speaker ranking stability under different conditions:\nlimiting the number of ratings and the dependence on rater and speaker\ncharacteristics. We also analyze the ranking wrt. acoustic correlates\nto find out what factors influence likability. We publish our ranking\nand the underlying ratings in order to facilitate further research.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1697"
    },
    "signorello17_interspeech": {
      "authors": [
        [
          "Rosario",
          "Signorello"
        ],
        [
          "Sergio",
          "Hassid"
        ],
        [
          "Didier",
          "Demolin"
        ]
      ],
      "title": "Aerodynamic Features of French Fricatives",
      "original": "0285",
      "page_count": 5,
      "order": 474,
      "p1": "2267",
      "pn": "2271",
      "abstract": [
        "The present research investigates the aerodynamic features of French\nfricative consonants using direct measurement of subglottal air pressure\nby tracheal puncture (Ps) synchronized with intraoral air pressure\n(Po), oral airflow (Oaf) and acoustic measurements. Data were collected\nfrom four Belgian French speakers&#8217; productions of CVCV pseudowords\nincluding voiceless and voiced fricatives [f, v, s, z, &#643;, &#658;].\nThe goals of this study are: (i) to predict the starting, central,\nand releasing points of frication based on the measurements of Ps,\nPo, and Oaf; (ii) to compare voiceless and voiced fricatives and their\nplaces of articulation; and (iii) to provide reference values for the\naerodynamic features of fricatives for further linguistic, clinical,\nphysical and computational modeling research.\n"
      ],
      "doi": "10.21437/Interspeech.2017-285"
    },
    "serrurier17_interspeech": {
      "authors": [
        [
          "Antoine",
          "Serrurier"
        ],
        [
          "Pierre",
          "Badin"
        ],
        [
          "Louis-Jean",
          "Bo\u00eb"
        ],
        [
          "Laurent",
          "Lamalle"
        ],
        [
          "Christiane",
          "Neuschaefer-Rube"
        ]
      ],
      "title": "Inter-Speaker Variability: Speaker Normalisation and Quantitative Estimation of Articulatory Invariants in Speech Production for French",
      "original": "1126",
      "page_count": 5,
      "order": 475,
      "p1": "2272",
      "pn": "2276",
      "abstract": [
        "Speech production can be analysed in terms of universal articulatory-acoustic\nphonemic units shared between speakers. However, morphological differences\nbetween speakers and idiosyncratic articulatory strategies lead to\nlarge inter-speaker articulatory variability. Relationships between\nstrategy and morphology have already been pinpointed in the literature.\nThis study aims thus at generalising existing results on a larger database\nfor the entire vocal tract (VT) and at quantifying phoneme-specific\ninter-speaker articulatory invariants. Midsagittal MRI of 11 French\nspeakers for 62 vowels and consonants were recorded and VT contours\nmanually edited. A procedure of normalisation of VT contours between\nspeakers, based on the use of mean VT contours, led to an overall reduction\nof inter-speaker VT contours variance of 88%. On the opposite, the\nsagittal function (i.e. the transverse sagittal distance along the\nVT midline), which is the main determinant of the acoustic output,\nhad an overall amplitude variance decrease of only 37%, suggesting\nthat the speakers adapt their strategy to their morphology to achieve\nproper acoustic goals. Moreover, articulatory invariants were identified\non the sagittal variance distribution along the VT as the regions with\nlower variability. These regions correspond to the classical places\nof articulation and are associated with higher acoustic sensitivity\nfunction levels.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1126"
    },
    "patil17b_interspeech": {
      "authors": [
        [
          "Nimisha",
          "Patil"
        ],
        [
          "Timothy",
          "Greer"
        ],
        [
          "Reed",
          "Blaylock"
        ],
        [
          "Shrikanth S.",
          "Narayanan"
        ]
      ],
      "title": "Comparison of Basic Beatboxing Articulations Between Expert and Novice Artists Using Real-Time Magnetic Resonance Imaging",
      "original": "1190",
      "page_count": 5,
      "order": 476,
      "p1": "2277",
      "pn": "2281",
      "abstract": [
        "Real-time Magnetic Resonance Imaging (rtMRI) was used to examine mechanisms\nof sound production in five beatboxers. rtMRI was found to be an effective\ntool with which to study the articulatory dynamics of this form of\nhuman vocal production; it provides a dynamic view of the entire midsagittal\nvocal tract and at a frame rate (83 fps) sufficient to observe the\nmovement and coordination of critical articulators. The artists&#8217;\nrepertoires included percussion elements generated using a wide range\nof articulatory and airstream mechanisms. Analysis of three common\nbeatboxing sounds resulted in the finding that advanced beatboxers\nproduce stronger ejectives and have greater control over different\nairstreams than novice beatboxers, to enhance the quality of their\nsounds. No difference in production mechanisms between males and females\nwas observed. These data offer insights into the ways in which articulators\ncan be trained and used to achieve specific acoustic goals.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1190"
    },
    "tang17b_interspeech": {
      "authors": [
        [
          "Keyi",
          "Tang"
        ],
        [
          "Negar M.",
          "Harandi"
        ],
        [
          "Jonghye",
          "Woo"
        ],
        [
          "Georges El",
          "Fakhri"
        ],
        [
          "Maureen",
          "Stone"
        ],
        [
          "Sidney",
          "Fels"
        ]
      ],
      "title": "Speaker-Specific Biomechanical Model-Based Investigation of a Simple Speech Task Based on Tagged-MRI",
      "original": "1576",
      "page_count": 5,
      "order": 477,
      "p1": "2282",
      "pn": "2286",
      "abstract": [
        "We create two 3D biomechanical speaker models matched to medical image\ndata of two healthy English speakers. We use a new, hybrid registration\ntechnique that morphs a generic 3D, biomechanical model to medical\nimages. The generic model of the head and neck includes jaw, tongue,\nsoft-palate, epiglottis, lips and face, and is capable of simulating\nupper-airway biomechanics. We use cine and tagged magnetic resonance\n(MR) images captured while our volunteers repeated a simple utterance\n(/&#x259;-gis/) synchronized to a metronome. We simulate our models\nbased on internal tongue tissue trajectories that we extract from tagged\nMR images, and use in an inverse solver. For areas without tracked\ndata points, the registered generic model moves based on the computed\nmuscle activations. Our modeling efforts include a wide range of speech\norgans illustrating the coupling complexity between the oral anatomy\nduring simple speech utterances.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1576"
    },
    "blaylock17_interspeech": {
      "authors": [
        [
          "Reed",
          "Blaylock"
        ],
        [
          "Nimisha",
          "Patil"
        ],
        [
          "Timothy",
          "Greer"
        ],
        [
          "Shrikanth S.",
          "Narayanan"
        ]
      ],
      "title": "Sounds of the Human Vocal Tract",
      "original": "1631",
      "page_count": 5,
      "order": 478,
      "p1": "2287",
      "pn": "2291",
      "abstract": [
        "Previous research suggests that beatboxers only use sounds that exist\nin the world&#8217;s languages. This paper provides evidence to the\ncontrary, showing that beatboxers use non-linguistic articulations\nand airstream mechanisms to produce many sound effects that have not\nbeen attested in any language. An analysis of real-time magnetic resonance\nvideos of beatboxing reveals that beatboxers produce non-linguistic\narticulations such as ingressive retroflex trills and ingressive lateral\nbilabial trills. In addition, beatboxers can use both lingual egressive\nand pulmonic ingressive airstreams, neither of which have been reported\nin any language.<br style=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  The results of this study affect our understanding of the limits\nof the human vocal tract, and address questions about the mental units\nthat encode music and phonological grammar.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1631"
    },
    "uezu17_interspeech": {
      "authors": [
        [
          "Yasufumi",
          "Uezu"
        ],
        [
          "Tokihiko",
          "Kaburagi"
        ]
      ],
      "title": "A Simulation Study on the Effect of Glottal Boundary Conditions on Vocal Tract Formants",
      "original": "1675",
      "page_count": 5,
      "order": 479,
      "p1": "2292",
      "pn": "2296",
      "abstract": [
        "In the source-filter theory, the complete closure of the glottis is\nassumed as a glottal boundary condition. However, such assumption of\nglottal closure in the source-filter theory is not strictly satisfied\nin actual utterance. Therefore, it is considered that acoustic features\nof the glottis and the subglottal region may affect vocal tract formants.\nIn this study, we investigated how differences in the glottal boundary\nconditions affect vocal tract formants by speech synthesis simulation\nusing speech production model. We synthesized five Japanese vowels\nusing the speech production model in consideration of the source-filter\ninteraction. This model consisted of the glottal area polynomial model\nand the acoustic tube model in the concatenation of the vocal tract,\nglottis, and the subglottis. From the results, it was found that the\nfirst formant frequency was affected more strongly by the boundary\nconditions, and also found that the open quotient may give the formant\nstronger effect than the maximum glottal width. In addition, formant\nfrequencies were also affected more strongly by subglottal impedance\nwhen the maximum glottal area was wider.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1675"
    },
    "gangamohan17_interspeech": {
      "authors": [
        [
          "P.",
          "Gangamohan"
        ],
        [
          "B.",
          "Yegnanarayana"
        ]
      ],
      "title": "A Robust and Alternative Approach to Zero Frequency Filtering Method for Epoch Extraction",
      "original": "1172",
      "page_count": 4,
      "order": 480,
      "p1": "2297",
      "pn": "2300",
      "abstract": [
        "During production of voiced speech, there exists impulse-like excitations\ndue to abrupt closure of vocal folds. These impulse-like excitations\nare often referred as epochs or glottal closure instants (GCIs). The\nzero frequency filtering (ZFF) method exploits the properties of impulse-like\nexcitation by passing a speech signal through the resonator whose pole\npair is located at 0 Hz. As the resonator is unstable, the polynomial\ngrowth/decay is observed in the filtered signal, thus requiring a trend\nremoval operation. It is observed that the length of the window for\ntrend removal operation is critical in speech signals where there are\nmore fluctuations in the fundamental frequency (F<SUB>0</SUB>). In\nthis paper, a simple finite impulse response (FIR) implementation is\nproposed. The FIR filter is designed by placing large number of zeros\nat (f_s)/(2)] Hz (f<SUB>s</SUB> represents the sampling frequency),\ncloser to the unit circle, in the z-plane. Experimental results show\nthat the proposed method is robust and computationally less complex\nwhen compared to the ZFF method.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1172"
    },
    "hua17_interspeech": {
      "authors": [
        [
          "Kanru",
          "Hua"
        ]
      ],
      "title": "Improving YANGsaf F0 Estimator with Adaptive Kalman Filter",
      "original": "0021",
      "page_count": 5,
      "order": 481,
      "p1": "2301",
      "pn": "2305",
      "abstract": [
        "We present improvements to the refinement stage of YANGsaf[1] (Yet\nANother Glottal source analysis framework), a recently published F0\nestimation algorithm by Kawahara  et al., for noisy/breathy speech\nsignals. The baseline system, based on time-warping and weighted average\nof multi-band instantaneous frequency estimates, is still sensitive\nto additive noise when none of the harmonic provide reliable frequency\nestimate at low SNR. We alleviate this problem by calibrating the weighted\naveraging process based on statistics gathered from a Monte-Carlo simulation,\nand applying Kalman filtering to refined F0 trajectory with time-varying\nmeasurement and process distributions. The improved algorithm, adYANGsaf\n(adaptive Yet ANother Glottal source analysis framework), achieves\nsignificantly higher accuracy and smoother F0 trajectory on noisy speech\nwhile retaining its accuracy on clean speech, with little computational\noverhead introduced.\n"
      ],
      "doi": "10.21437/Interspeech.2017-21"
    },
    "dhiman17_interspeech": {
      "authors": [
        [
          "Jitendra Kumar",
          "Dhiman"
        ],
        [
          "Nagaraj",
          "Adiga"
        ],
        [
          "Chandra Sekhar",
          "Seelamantula"
        ]
      ],
      "title": "A Spectro-Temporal Demodulation Technique for Pitch Estimation",
      "original": "1138",
      "page_count": 5,
      "order": 482,
      "p1": "2306",
      "pn": "2310",
      "abstract": [
        "We consider a two-dimensional demodulation framework for spectro-temporal\nanalysis of the speech signal. We construct narrowband (NB) speech\nspectrograms, and demodulate them using the Riesz transform, which\nis a two-dimensional extension of the Hilbert transform. The demodulation\nresults in time-frequency envelope (amplitude modulation or AM) and\ntime-frequency carrier (frequency modulation or FM). The AM corresponds\nto the vocal tract and is referred to as the vocal tract spectrogram.\nThe FM corresponds to the underlying excitation and is referred to\nas the carrier spectrogram. The carrier spectrogram exhibits a high\ndegree of time-frequency consistency for voiced sounds. For unvoiced\nsounds, such a structure is lacking. In addition, the carrier spectrogram\nreflects the fundamental frequency (F0) variation of the speech signal.\nWe develop a technique to determine the F0 from the carrier spectrogram.\nThe time-frequency consistency is used to determine which time-frequency\nregions correspond to voiced segments. Comparisons with the state-of-the-art\nF0 estimation algorithms show that the proposed F0 estimator has high\naccuracy for telephone channel speech and is robust to noise.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1138"
    },
    "miwa17_interspeech": {
      "authors": [
        [
          "Kenichiro",
          "Miwa"
        ],
        [
          "Masashi",
          "Unoki"
        ]
      ],
      "title": "Robust Method for Estimating F<SUB>0</SUB> of Complex Tone Based on Pitch Perception of Amplitude Modulated Signal",
      "original": "1061",
      "page_count": 5,
      "order": 483,
      "p1": "2311",
      "pn": "2315",
      "abstract": [
        "Estimating the fundamental frequency (F<SUB>0</SUB>) of a target sound\nin noisy reverberant environments is a challenging issue in not only\nsound analysis/synthesis but also sound enhancement. This paper proposes\na method for robustly and accurately estimating the F<SUB>0</SUB> of\na time-variant complex tone on the basis of an amplitude modulation/demodulation\ntechnique. It is based on the mechanism of the pitch perception of\namplitude modulated signal and the frame-work of power envelope restoration\nbased on the concept of modulation transfer function. Computer simulations\nwere carried out to discuss feasibility of the accuracy and robustness\nof the proposed method for estimating the F<SUB>0</SUB> in heavy noisy\nreverberant environments. The comparative results revealed that the\npercentage correct rates of the estimated F<SUB>0</SUB>s using five\nrecent methods (TEMPO2, YIN, PHIA, CmpCep, and SWIPE&#8217;) were drastically\nreduced as the SNR decreased and the reverberation time increased.\nThe results also demonstrated that the proposed method robustly and\naccurately estimated the F<SUB>0</SUB> in both heavy noisy and reverberant\nenvironments.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1061"
    },
    "graf17_interspeech": {
      "authors": [
        [
          "Simon",
          "Graf"
        ],
        [
          "Tobias",
          "Herbig"
        ],
        [
          "Markus",
          "Buck"
        ],
        [
          "Gerhard",
          "Schmidt"
        ]
      ],
      "title": "Low-Complexity Pitch Estimation Based on Phase Differences Between Low-Resolution Spectra",
      "original": "1254",
      "page_count": 5,
      "order": 484,
      "p1": "2316",
      "pn": "2320",
      "abstract": [
        "Detection of voiced speech and estimation of the pitch frequency are\nimportant tasks for many speech processing algorithms. Pitch information\ncan be used, e.g., to reconstruct voiced speech corrupted by noise.<br\nstyle=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  In automotive environments, driving noise especially affects voiced\nspeech portions in the lower frequencies. Pitch estimation is therefore\nimportant, e.g., for in-car-communication systems. Such systems amplify\nthe driver&#8217;s voice and allow for convenient conversations with\nbackseat passengers. Low latency is required for this application,\nwhich requires the use of short window lengths and short frame shifts\nbetween consecutive frames. Conventional pitch estimation techniques,\nhowever, rely on long windows that exceed the pitch period of human\nspeech. In particular, male speakers&#8217; low pitch frequencies are\ndifficult to resolve.<br style=\"mso-data-placement:same-cell;\" /><br\nstyle=\"mso-data-placement:same-cell;\" />\n  In this publication, we\nintroduce a technique that approaches pitch estimation from a different\nperspective. The pitch information is extracted based on phase differences\nbetween multiple low-resolution spectra instead of a single long window.\nThe technique benefits from the high temporal resolution provided by\nthe short frame shift and is capable to deal with the low spectral\nresolution caused by short window lengths. Using the new approach,\neven very low pitch frequencies can be estimated very efficiently.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1254"
    },
    "morise17b_interspeech": {
      "authors": [
        [
          "Masanori",
          "Morise"
        ]
      ],
      "title": "Harvest: A High-Performance Fundamental Frequency Estimator from Speech Signals",
      "original": "0068",
      "page_count": 5,
      "order": 485,
      "p1": "2321",
      "pn": "2325",
      "abstract": [
        "A fundamental frequency (F0) estimator named  Harvest is described.\nThe unique points of Harvest are that it can obtain a reliable F0 contour\nand reduce the error that the voiced section is wrongly identified\nas the unvoiced section. It consists of two steps: estimation of F0\ncandidates and generation of a reliable F0 contour on the basis of\nthese candidates. In the first step, the algorithm uses fundamental\ncomponent extraction by many band-pass filters with different center\nfrequencies and obtains the basic F0 candidates from filtered signals.\nAfter that, basic F0 candidates are refined and scored by using the\ninstantaneous frequency, and then several F0 candidates in each frame\nare estimated. Since the frame-by-frame processing based on the fundamental\ncomponent extraction is not robust against temporally local noise,\na connection algorithm using neighboring F0s is used in the second\nstep. The connection takes advantage of the fact that the F0 contour\ndoes not precipitously change in a short interval. We carried out an\nevaluation using two speech databases with electroglottograph (EGG)\nsignals to compare Harvest with several state-of-the-art algorithms.\nResults showed that Harvest achieved the best performance of all algorithms.\n"
      ],
      "doi": "10.21437/Interspeech.2017-68"
    },
    "stehwien17_interspeech": {
      "authors": [
        [
          "Sabrina",
          "Stehwien"
        ],
        [
          "Ngoc Thang",
          "Vu"
        ]
      ],
      "title": "Prosodic Event Recognition Using Convolutional Neural Networks with Context Information",
      "original": "1159",
      "page_count": 5,
      "order": 486,
      "p1": "2326",
      "pn": "2330",
      "abstract": [
        "This paper demonstrates the potential of convolutional neural networks\n(CNN) for detecting and classifying prosodic events on words, specifically\npitch accents and phrase boundary tones, from frame-based acoustic\nfeatures. Typical approaches use not only feature representations of\nthe word in question but also its surrounding context. We show that\nadding position features indicating the current word benefits the CNN.\nIn addition, this paper discusses the generalization from a speaker-dependent\nmodelling approach to a speaker-independent setup. The proposed method\nis simple and efficient and yields strong results not only in speaker-dependent\nbut also speaker-independent cases.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1159"
    },
    "galvez17_interspeech": {
      "authors": [
        [
          "Ramiro H.",
          "G\u00e1lvez"
        ],
        [
          "\u0160tefan",
          "Be\u0148u\u0161"
        ],
        [
          "Agust\u00edn",
          "Gravano"
        ],
        [
          "Marian",
          "Trnka"
        ]
      ],
      "title": "Prosodic Facilitation and Interference While Judging on the Veracity of Synthesized Statements",
      "original": "0453",
      "page_count": 5,
      "order": 487,
      "p1": "2331",
      "pn": "2335",
      "abstract": [
        "Two primary sources of information are provided in human speech. On\nthe one hand, the verbal channel encodes linguistic content, while\non the other hand, the vocal channel transmits paralinguistic information,\nmainly through prosody. In line with several studies that induce a\nconflict between these two channels to better understand the role of\nprosody, we conducted an experiment in which subjects had to listen\nto a series of statements synthesized with varying prosody and indicate\nif they believed them to be true or false. We find evidence suggesting\nthat acoustic/prosodic (a/p) features of the synthesized statements\naffect response times (a well-known proxy for cognitive load). Our\nresults suggest that prosody in synthesized speech may play a role\nof either facilitation or interference when subjects judge the truthfulness\nof a statement. Furthermore, we find that this pattern is amplified\nwhen the a/p features of the synthesized statements are analyzed relative\nto the subjects&#8217; own a/p features. This suggests that the entrainment\nof TTS voices has serious implications in the perceived trustworthiness\nof the system&#8217;s skills.\n"
      ],
      "doi": "10.21437/Interspeech.2017-453"
    },
    "zellers17_interspeech": {
      "authors": [
        [
          "Margaret",
          "Zellers"
        ],
        [
          "Antje",
          "Schweitzer"
        ]
      ],
      "title": "An Investigation of Pitch Matching Across Adjacent Turns in a Corpus of Spontaneous German",
      "original": "0811",
      "page_count": 5,
      "order": 488,
      "p1": "2336",
      "pn": "2340",
      "abstract": [
        "Speakers in conversations may adapt their turn pitch relative to that\nof preceding turns to signal alignment with their interlocutor. However,\nthe reference frame for pitch matching across turns is still unclear.\nResearchers studying pitch in the context of conversation have argued\nfor an initializing approach, in which turn pitch must be judged relative\nto pitch in preceding turns. However, perceptual studies have indicated\nthat listeners are able to reliably identify the location of pitch\nvalues within an individual speaker&#8217;s range; that is, even without\nconversational context, they are able to normalize to speakers. This\nwould imply that speakers might match normalized pitch instead of absolute\npitch. Using a combined quantitative-qualitative approach, we investigate\nthe relationship between pitch in adjacent turns in spontaneous German\nconversation. We use two different methods of evaluating pitch in adjacent\nturns, reflecting normalizing and initializing approaches respectively.\nWe find that the results are well correlated with conversational participants&#8217;\nevaluation of the conversation. Furthermore, evaluating locations with\nmatched or mismatched pitch can help distinguish between blind and\nface-to-face conversational situations, as well as identifying locations\nwhere specific discourse strategies (such as tag questions) have been\ndeployed.\n"
      ],
      "doi": "10.21437/Interspeech.2017-811"
    },
    "mukherjee17_interspeech": {
      "authors": [
        [
          "Sankar",
          "Mukherjee"
        ],
        [
          "Alessandro",
          "D\u2019Ausilio"
        ],
        [
          "No\u00ebl",
          "Nguyen"
        ],
        [
          "Luciano",
          "Fadiga"
        ],
        [
          "Leonardo",
          "Badino"
        ]
      ],
      "title": "The Relationship Between F0 Synchrony and Speech Convergence in Dyadic Interaction",
      "original": "0795",
      "page_count": 5,
      "order": 489,
      "p1": "2341",
      "pn": "2345",
      "abstract": [
        "Speech accommodation happens when two people engage in verbal conversation.\nIn this paper two types of accommodation are investigated &#8212; one\ndependent on cognitive, physiological, functional and social constraints\n(Convergence), the other dependent on linguistic and paralinguistic\nfactors (Synchrony). Convergence refers to the situation when two speakers&#8217;\nspeech characteristics move towards a common point. Synchrony happens\nif speakers&#8217; prosodic features become correlated over time. Here\nwe analyze relations between the two phenomena at the single word level.\nAlthough calculation of Synchrony is fairly straightforward, measuring\nConvergence is even more problematic as proved by a long history of\ndebates on how to define it. In this paper we consider Convergence\nas an emergent behavior and investigate it by developing a robust and\nautomatic method based on Gaussian Mixture Model (GMM). Our results\nshow that high Synchrony of F0 between two speakers leads to greater\namount of Convergence. This provides robust support for the idea that\nSynchrony and Convergence are interrelated processes, particularly\nin female participants.\n"
      ],
      "doi": "10.21437/Interspeech.2017-795"
    },
    "luque17_interspeech": {
      "authors": [
        [
          "Jordi",
          "Luque"
        ],
        [
          "Carlos",
          "Segura"
        ],
        [
          "Ariadna",
          "S\u00e1nchez"
        ],
        [
          "Mart\u00ed",
          "Umbert"
        ],
        [
          "Luis Angel",
          "Galindo"
        ]
      ],
      "title": "The Role of Linguistic and Prosodic Cues on the Prediction of Self-Reported Satisfaction in Contact Centre Phone Calls",
      "original": "0424",
      "page_count": 5,
      "order": 490,
      "p1": "2346",
      "pn": "2350",
      "abstract": [
        "Call Centre data is typically collected by organizations and corporations\nin order to ensure the quality of service, supporting for example mining\ncapabilities for monitoring customer satisfaction. In this work, we\nanalyze the significance of various acoustic features extracted from\ncustomer-agents&#8217; spoken interaction in predicting self-reported\nsatisfaction by the customer. We also investigate whether speech prosodic\nfeatures can deliver complementary information to speech transcriptions\nprovided by an ASR. We explore the possibility of using a deep neural\narchitecture to perform early feature fusion on both prosodic and linguistic\ninformation. Convolutional Neural Networks are trained on a combination\nof word embedding and acoustic features for the binary classification\ntask of &#8220;low&#8221; and &#8220;high&#8221; satisfaction prediction.\nWe conducted our experiments analysing real call-centre interactions\nof a large corporation in a Spanish spoken country. Our experiments\nshow that linguistic features can predict self-reported satisfaction\nmore accurately than those based on prosodic and conversational descriptors.\nWe also find that dialog turn-level conversational features generally\noutperforms frame-level signal descriptors. Finally, the fusion of\nlinguistic and prosodic features reports the best performance in our\nexperiments, suggesting the complementarity of the information conveyed\nby each set of behavioral representation.\n"
      ],
      "doi": "10.21437/Interspeech.2017-424"
    },
    "brusco17_interspeech": {
      "authors": [
        [
          "Pablo",
          "Brusco"
        ],
        [
          "Juan Manuel",
          "P\u00e9rez"
        ],
        [
          "Agust\u00edn",
          "Gravano"
        ]
      ],
      "title": "Cross-Linguistic Study of the Production of Turn-Taking Cues in American English and Argentine Spanish",
      "original": "0124",
      "page_count": 5,
      "order": 491,
      "p1": "2351",
      "pn": "2355",
      "abstract": [
        "We present the results of a series of machine learning experiments\naimed at exploring the differences and similarities in the production\nof turn-taking cues in American English and Argentine Spanish. An analysis\nof prosodic features automatically extracted from 21 dyadic conversations\n(12 En, 9 Sp) revealed that, when signaling Holds, speakers of both\nlanguages tend to use roughly the same combination of cues, characterized\nby a sustained final intonation, a shorter duration of turn-final inter-pausal\nunits, and a distinct voice quality. However, in speech preceding Smooth\nSwitches or Backchannels, we observe the existence of the same set\nof prosodic turn-taking cues in both languages, although the ways in\nwhich these cues are combined together to form complex signals differ.\nStill, we find that these differences do not degrade below chance the\nperformance of cross-linguistic systems for automatically detecting\nturn-taking signals. These results are relevant to the construction\nof multilingual spoken dialogue systems, which need to adapt not only\ntheir ASR modules but also the way prosodic turn-taking cues are synthesized\nand recognized.\n"
      ],
      "doi": "10.21437/Interspeech.2017-124"
    },
    "egorow17_interspeech": {
      "authors": [
        [
          "Olga",
          "Egorow"
        ],
        [
          "Andreas",
          "Wendemuth"
        ]
      ],
      "title": "Emotional Features for Speech Overlaps Classification",
      "original": "0087",
      "page_count": 5,
      "order": 492,
      "p1": "2356",
      "pn": "2360",
      "abstract": [
        "One interesting phenomenon of natural conversation is overlapping speech.\nBesides causing difficulties in automatic speech processing, such overlaps\ncarry information on the state of the overlapper: competitive overlaps\n(i.e. &#8220;interruptions&#8221;) can signal disagreement or the feeling\nof being overlooked, and cooperative overlaps (i.e. supportive interjections)\ncan signal agreement and interest. These hints can be used to improve\nhuman-machine interaction. In this paper we present an approach for\nautomatic classification of competitive and cooperative overlaps using\nthe emotional content of the speakers&#8217; utterances before and\nafter the overlap. For these experiments, we use real-world data from\nhuman-human interactions in call centres. We also compare our approach\nto standard acoustic classification on the same data and come to the\nconclusion, that emotional features are clearly superior to acoustic\nfeatures for this task, resulting in an unweighted average f-measure\nof 71.9%. But we also find that acoustic features should not be entirely\nneglected: using a late fusion procedure, we can further improve the\nunweighted average f-measure by 2.6%.\n"
      ],
      "doi": "10.21437/Interspeech.2017-87"
    },
    "chen17j_interspeech": {
      "authors": [
        [
          "Chin-Po",
          "Chen"
        ],
        [
          "Xian-Hong",
          "Tseng"
        ],
        [
          "Susan Shur-Fen",
          "Gau"
        ],
        [
          "Chi-Chun",
          "Lee"
        ]
      ],
      "title": "Computing Multimodal Dyadic Behaviors During Spontaneous Diagnosis Interviews Toward Automatic Categorization of Autism Spectrum Disorder",
      "original": "0563",
      "page_count": 5,
      "order": 493,
      "p1": "2361",
      "pn": "2365",
      "abstract": [
        "Autism spectrum disorder (ASD) is a highly-prevalent neural developmental\ndisorder often characterized by social communicative deficits and restricted\nrepetitive interest. The heterogeneous nature of ASD in its behavior\nmanifestations encompasses broad syndromes such as, Classical Autism\n(AD), High-functioning Autism (HFA), and Asperger syndrome (AS). In\nthis work, we compute a variety of multimodal behavior features, including\nbody movements, acoustic characteristics, and turn-taking events dynamics,\nof the participant, the investigator and the interaction between the\ntwo directly from audio-video recordings by leveraging the Autism Diagnostic\nObservational Schedule (ADOS) as a clinically-valid behavior data elicitation\ntechnique. Several of these signal-derived behavioral measures show\nstatistically significant differences among the three syndromes. Our\nanalyses indicate that these features may be pointing to the underlying\ndifferences in the behavior characterizations of social functioning\nbetween AD, AS, and HFA &#8212; corroborating some of the previous\nliterature. Further, our signal-derived behavior measures achieve competitive,\nsometimes exceeding, recognition accuracies in discriminating between\nthe three syndromes of ASD when compared to investigator&#8217;s clinical-rating\non participant&#8217;s social and communicative behaviors during ADOS.\n"
      ],
      "doi": "10.21437/Interspeech.2017-563"
    },
    "lin17_interspeech": {
      "authors": [
        [
          "Yun-Shao",
          "Lin"
        ],
        [
          "Chi-Chun",
          "Lee"
        ]
      ],
      "title": "Deriving  Dyad-Level Interaction Representation Using Interlocutors Structural and Expressive Multimodal Behavior Features",
      "original": "0569",
      "page_count": 5,
      "order": 494,
      "p1": "2366",
      "pn": "2370",
      "abstract": [
        "The overall interaction atmosphere is often a result of complex interplay\nbetween individual interlocutor&#8217;s behavior expressions and joint\nmanifestation of dyadic interaction dynamics. There is very limited\nwork, if any, that has computationally analyzed a human interaction\nat the dyad-level. Hence, in this work, we propose to compute an extensive\nnovel set of features representing multi-faceted aspects of a dyadic\ninteraction. These features are grouped into two broad categories:\n expressive and  structural behavior dynamics, where each captures\ninformation about within-speaker behavior manifestation, inter-speaker\nbehavior dynamics, durational and transitional statistics providing\nholistic behavior quantifications at the dyad-level. We carry out an\nexperiment of recognizing targeted  affective atmosphere using the\nproposed  expressive and  structural behavior dynamics features derived\nfrom audio and video modalities. Our experiment shows that the inclusion\nof both  expressive and  structural behavior dynamics is essential\nin achieving promising recognition accuracies across six different\nclasses (72.5%), where  structural-based features improve the recognition\nrates on classes of  sad and  surprise. Further analyses reveal important\naspects of multimodal behavior dynamics within dyadic interactions\nthat are related to the affective atmospheric scene.\n"
      ],
      "doi": "10.21437/Interspeech.2017-569"
    },
    "brueckner17_interspeech": {
      "authors": [
        [
          "Raymond",
          "Brueckner"
        ],
        [
          "Maximilian",
          "Schmitt"
        ],
        [
          "Maja",
          "Pantic"
        ],
        [
          "Bj\u00f6rn",
          "Schuller"
        ]
      ],
      "title": "Spotting Social Signals in Conversational Speech over IP: A Deep Learning Perspective",
      "original": "0635",
      "page_count": 5,
      "order": 495,
      "p1": "2371",
      "pn": "2375",
      "abstract": [
        "The automatic detection and classification of social signals is an\nimportant task, given the fundamental role nonverbal behavioral cues\nplay in human communication. We present the first cross-lingual study\non the detection of  laughter and  fillers in conversational and spontaneous\nspeech collected &#8216;in the wild&#8217; over IP (internet protocol).\nFurther, this is the first comparison of LSTM and GRU networks to shed\nlight on their performance differences. We report frame-based results\nin terms of the unweighted-average area-under-the-curve (UAAUC) measure\nand will shortly discuss its suitability for this task. In the mono-lingual\nsetup our best deep BLSTM system achieves 87.0% and 86.3% UAAUC for\nEnglish and German, respectively. Interestingly, the cross-lingual\nresults are only slightly lower, yielding 83.7% for a system trained\non English, but tested on German, and 85.0% in the opposite case. We\nshow that LSTM and GRU architectures are valid alternatives for e.\ng., on-line and compute-sensitive applications, since their application\nincurs a relative UAAUC decrease of only approximately 5% with respect\nto our best systems. Finally, we apply additional smoothing to correct\nfor erroneous spikes and drops in the posterior trajectories to obtain\nan additional gain in all setups.\n"
      ],
      "doi": "10.21437/Interspeech.2017-635"
    },
    "gosztolya17_interspeech": {
      "authors": [
        [
          "G\u00e1bor",
          "Gosztolya"
        ]
      ],
      "title": "Optimized Time Series Filters for Detecting Laughter and Filler Events",
      "original": "0932",
      "page_count": 5,
      "order": 496,
      "p1": "2376",
      "pn": "2380",
      "abstract": [
        "Social signal detection, that is, the task of identifying vocalizations\nlike laughter and filler events is a popular task within computational\nparalinguistics. Recent studies have shown that besides applying state-of-the-art\nmachine learning methods, it is worth making use of the contextual\ninformation and adjusting the frame-level scores based on the local\nneighbourhood. In this study we apply a weighted average time series\nsmoothing filter for laughter and filler event identification, and\nset the weights using a state-of-the-art optimization method, namely\nthe Covariance Matrix Adaptation Evolution Strategy (CMA-ES). Our results\nindicate that this is a viable way of improving the Area Under the\nCurve (AUC) scores: our resulting scores are much better than the accuracy\nscores of the raw likelihoods produced by Deep Neural Networks trained\non three different feature sets, and we also significantly outperform\nstandard time series filters as well as DNNs used for smoothing. Our\nscore achieved on the test set of a public English database containing\nspontaneous mobile phone conversations is the highest one published\nso far that was realized by feed-forward techniques.\n"
      ],
      "doi": "10.21437/Interspeech.2017-932"
    },
    "haider17_interspeech": {
      "authors": [
        [
          "Fasih",
          "Haider"
        ],
        [
          "Fahim A.",
          "Salim"
        ],
        [
          "Saturnino",
          "Luz"
        ],
        [
          "Carl",
          "Vogel"
        ],
        [
          "Owen",
          "Conlan"
        ],
        [
          "Nick",
          "Campbell"
        ]
      ],
      "title": "Visual, Laughter, Applause and Spoken Expression Features for Predicting Engagement Within TED Talks",
      "original": "1633",
      "page_count": 5,
      "order": 497,
      "p1": "2381",
      "pn": "2385",
      "abstract": [
        "There is an enormous amount of audio-visual content available on-line\nin the form of talks and presentations. The prospective users of the\ncontent face difficulties in finding the right content for them. However,\nautomatic detection of interesting (engaging vs. non-engaging) content\ncan help users to find the videos according to their preferences. It\ncan also be helpful for a recommendation and personalised video segmentation\nsystem. This paper presents a study of engagement based on TED talks\n(1338 videos) which are rated by on-line viewers (users). It proposes\nnovel models to predict the user&#8217;s (on-line viewers) engagement\nusing high-level visual features (camera angles), the audience&#8217;s\nlaughter and applause, and the presenter&#8217;s speech expressions.\nThe results show that these features contribute towards the prediction\nof user engagement in these talks. However, finding the engaging speech\nexpressions can also help a system in making summaries of TED Talks\n(video summarization) and creating feedback to presenters about their\nspeech expressions during talks.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1633"
    },
    "li17j_interspeech": {
      "authors": [
        [
          "Jinyu",
          "Li"
        ],
        [
          "Michael L.",
          "Seltzer"
        ],
        [
          "Xi",
          "Wang"
        ],
        [
          "Rui",
          "Zhao"
        ],
        [
          "Yifan",
          "Gong"
        ]
      ],
      "title": "Large-Scale Domain Adaptation via Teacher-Student Learning",
      "original": "0519",
      "page_count": 5,
      "order": 498,
      "p1": "2386",
      "pn": "2390",
      "abstract": [
        "High accuracy speech recognition requires a large amount of transcribed\ndata for supervised training. In the absence of such data, domain adaptation\nof a well-trained acoustic model can be performed, but even here, high\naccuracy usually requires significant labeled data from the target\ndomain. In this work, we propose an approach to domain adaptation that\ndoes not require transcriptions but instead uses a corpus of unlabeled\nparallel data, consisting of pairs of samples from the source domain\nof the well-trained model and the desired target domain. To perform\nadaptation, we employ teacher/student (T/S) learning, in which the\nposterior probabilities generated by the source-domain model can be\nused in lieu of labels to train the target-domain model. We evaluate\nthe proposed approach in two scenarios, adapting a clean acoustic model\nto noisy speech and adapting an adults&#8217; speech acoustic model\nto children&#8217;s speech. Significant improvements in accuracy are\nobtained, with reductions in word error rate of up to 44% over the\noriginal source model without the need for transcribed data in the\ntarget domain. Moreover, we show that increasing the amount of unlabeled\ndata results in additional model robustness, which is particularly\nbeneficial when using simulated training data in the target-domain.\n"
      ],
      "doi": "10.21437/Interspeech.2017-519"
    },
    "ahmad17_interspeech": {
      "authors": [
        [
          "W.",
          "Ahmad"
        ],
        [
          "S.",
          "Shahnawazuddin"
        ],
        [
          "H.K.",
          "Kathania"
        ],
        [
          "Gayadhar",
          "Pradhan"
        ],
        [
          "A.B.",
          "Samaddar"
        ]
      ],
      "title": "Improving Children&#8217;s Speech Recognition Through Explicit Pitch Scaling Based on Iterative Spectrogram Inversion",
      "original": "0302",
      "page_count": 5,
      "order": 499,
      "p1": "2391",
      "pn": "2395",
      "abstract": [
        "The task of transcribing children&#8217;s speech using statistical\nmodels trained on adults&#8217; speech is very challenging. Large mismatch\nin the acoustic and linguistic attributes of the training and test\ndata is reported to degrade the performance. In such speech recognition\ntasks, the differences in pitch (or fundamental frequency) between\nthe two groups of speakers is one among several mismatch factors. To\novercome the pitch mismatch, an existing pitch scaling technique based\non iterative spectrogram inversion is explored in this work. Explicit\npitch scaling is found to improve the recognition of children&#8217;s\nspeech under mismatched setup. In addition to that, we have also studied\nthe effect of discarding the phase information during spectrum reconstruction.\nThis is motivated by the fact that the dominant acoustic feature extraction\ntechniques make use of the magnitude spectrum only. On evaluating the\neffectiveness under mismatched testing scenario, the existing as well\nas the modified pitch scaling techniques result in very similar recognition\nperformances. Furthermore, we have explored the role of pitch scaling\non another speech recognition system which is trained on speech data\nfrom both adult and child speakers. Pitch scaling is noted to be effective\nfor children&#8217;s speech recognition in this case as well.\n"
      ],
      "doi": "10.21437/Interspeech.2017-302"
    },
    "xie17_interspeech": {
      "authors": [
        [
          "Xurong",
          "Xie"
        ],
        [
          "Xunying",
          "Liu"
        ],
        [
          "Tan",
          "Lee"
        ],
        [
          "Lan",
          "Wang"
        ]
      ],
      "title": "RNN-LDA Clustering for Feature Based DNN Adaptation",
      "original": "0368",
      "page_count": 5,
      "order": 500,
      "p1": "2396",
      "pn": "2400",
      "abstract": [
        "Model based deep neural network (DNN) adaptation approaches often require\nmulti-pass decoding in test time. Input feature based DNN adaptation,\nfor example, based on latent Dirichlet allocation (LDA) clustering,\nprovide a more efficient alternative. In conventional LDA clustering,\nthe transition and correlation between neighboring clusters is ignored.\nIn order to address this issue, a recurrent neural network (RNN) based\nclustering scheme is proposed to learn both the standard LDA cluster\nlabels and their natural correlation over time in this paper. In addition\nto directly using the resulting RNN-LDA as input features during DNN\nadaptation, a range of techniques were investigated to condition the\nDNN hidden layer parameters or activation outputs on the RNN-LDA features.\nOn a DARPA Gale Mandarin Chinese broadcast speech transcription task,\nthe proposed RNN-LDA cluster features adapted DNN system outperformed\nboth the baseline un-adapted DNN system and conventional LDA features\nadapted DNN system by 8% relative on the most difficult Phoenix TV\nsubset. Consistent improvements were also obtained after further combination\nwith model based adaptation approaches.\n"
      ],
      "doi": "10.21437/Interspeech.2017-368"
    },
    "arsikere17_interspeech": {
      "authors": [
        [
          "Harish",
          "Arsikere"
        ],
        [
          "Sri",
          "Garimella"
        ]
      ],
      "title": "Robust Online i-Vectors for Unsupervised Adaptation of DNN Acoustic Models: A Study in the Context of Digital Voice Assistants",
      "original": "1342",
      "page_count": 5,
      "order": 501,
      "p1": "2401",
      "pn": "2405",
      "abstract": [
        "Supplementing log filter-bank energies with i-vectors is a popular\nmethod for adaptive training of deep neural network acoustic models.\nWhile  offline i-vectors (the target utterance or other relevant adaptation\nmaterial is available for i-vector extraction prior to decoding) have\nbeen well studied, there is little analysis of  online i-vectors and\ntheir robustness in multi-user scenarios where speaker changes can\nbe frequent and unpredictable. The authors of [1] showed that online\nadaptation could be achieved through segmental i-vectors computed using\nthe hidden Markov model (HMM) state alignments of utterances decoded\nin the recent past. While this approach works well in general, it could\nbe rendered ineffective by speaker changes. In this paper, we study\nrobust extensions of the ideas proposed in [1] by: (a) updating i-vectors\non a per-frame basis based on the incoming target utterance, and (b)\nusing lattice posteriors instead of one-best HMM state alignments.\nExperiments with different i-vector implementations show that: (a)\nwhen speaker changes occur, lattice-based frame-level i-vectors provide\nup to 6% word error rate reduction relative to the baseline [1], and\n(b) online i-vectors are more effective, in general, when the microphone\ncharacteristics of test utterances are not seen in training.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1342"
    },
    "srinivasamurthy17_interspeech": {
      "authors": [
        [
          "Ajay",
          "Srinivasamurthy"
        ],
        [
          "Petr",
          "Motlicek"
        ],
        [
          "Ivan",
          "Himawan"
        ],
        [
          "Gy\u00f6rgy",
          "Szasz\u00e1k"
        ],
        [
          "Youssef",
          "Oualil"
        ],
        [
          "Hartmut",
          "Helmke"
        ]
      ],
      "title": "Semi-Supervised Learning with Semantic Knowledge Extraction for Improved Speech Recognition in Air Traffic Control",
      "original": "1446",
      "page_count": 5,
      "order": 502,
      "p1": "2406",
      "pn": "2410",
      "abstract": [
        "Automatic Speech Recognition (ASR) can introduce higher levels of automation\ninto Air Traffic Control (ATC), where spoken language is still the\npredominant form of communication. While ATC uses standard phraseology\nand a limited vocabulary, we need to adapt the speech recognition systems\nto local acoustic conditions and vocabularies at each airport to reach\noptimal performance. Due to continuous operation of ATC systems, a\nlarge and increasing amount of untranscribed speech data is available,\nallowing for semi-supervised learning methods to build and adapt ASR\nmodels. In this paper, we first identify the challenges in building\nASR systems for specific ATC areas and propose to utilize out-of-domain\ndata to build baseline ASR models. Then we explore different methods\nof data selection for adapting baseline models by exploiting the continuously\nincreasing untranscribed data. We develop a basic approach capable\nof exploiting semantic representations of ATC commands. We achieve\nrelative improvement in both word error rate (23.5%) and concept error\nrates (7%) when adapting ASR models to different ATC conditions in\na semi-supervised manner.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1446"
    },
    "kim17f_interspeech": {
      "authors": [
        [
          "Taesup",
          "Kim"
        ],
        [
          "Inchul",
          "Song"
        ],
        [
          "Yoshua",
          "Bengio"
        ]
      ],
      "title": "Dynamic Layer Normalization for Adaptive Neural Acoustic Modeling in Speech Recognition",
      "original": "0556",
      "page_count": 5,
      "order": 503,
      "p1": "2411",
      "pn": "2415",
      "abstract": [
        "Layer normalization is a recently introduced technique for normalizing\nthe activities of neurons in deep neural networks to improve the training\nspeed and stability. In this paper, we introduce a new layer normalization\ntechnique called   Dynamic Layer Normalization (DLN) for adaptive neural\nacoustic modeling in speech recognition. By dynamically generating\nthe scaling and shifting parameters in layer normalization, DLN adapts\nneural acoustic models to the acoustic variability arising from various\nfactors such as speakers, channel noises, and environments. Unlike\nother adaptive acoustic models, our proposed approach does not require\nadditional adaptation data or speaker information such as i-vectors.\nMoreover, the model size is fixed as it dynamically generates adaptation\nparameters. We apply our proposed DLN to deep bidirectional LSTM acoustic\nmodels and evaluate them on two benchmark datasets for large vocabulary\nASR experiments: WSJ and TED-LIUM release 2. The experimental results\nshow that our DLN improves neural acoustic models in terms of transcription\naccuracy by dynamically adapting to various speakers and environments.\n"
      ],
      "doi": "10.21437/Interspeech.2017-556"
    },
    "bosker17b_interspeech": {
      "authors": [
        [
          "Hans Rutger",
          "Bosker"
        ],
        [
          "Anne",
          "K\u00f6sem"
        ]
      ],
      "title": "An Entrained Rhythm&#8217;s Frequency, Not Phase, Influences Temporal Sampling of Speech",
      "original": "0073",
      "page_count": 5,
      "order": 504,
      "p1": "2416",
      "pn": "2420",
      "abstract": [
        "Brain oscillations have been shown to track the slow amplitude fluctuations\nin speech during comprehension. Moreover, there is evidence that these\nstimulus-induced cortical rhythms may persist even after the driving\nstimulus has ceased. However, how exactly this neural entrainment shapes\nspeech perception remains debated. This behavioral study investigated\nwhether and how the  frequency and  phase of an entrained rhythm would\ninfluence the temporal sampling of subsequent speech.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  In two behavioral\nexperiments, participants were presented with slow and fast isochronous\ntone sequences, followed by Dutch target words ambiguous between  as\n/&#593;s/ &#8220;ash&#8221; (with a short vowel) and  aas /a:s/ &#8220;bait&#8221;\n(with a long vowel). Target words were presented at various phases\nof the entrained rhythm. Both experiments revealed effects of the frequency\nof the tone sequence on target word perception: fast sequences biased\nlisteners to more long /a:s/ responses. However, no evidence for phase\neffects could be discerned.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  These findings show\nthat an entrained rhythm&#8217;s frequency, but not phase, influences\nthe temporal sampling of subsequent speech. These outcomes are compatible\nwith theories suggesting that sensory timing is evaluated relative\nto entrained  frequency. Furthermore, they suggest that  phase tracking\nof (syllabic) rhythms by theta oscillations plays a limited role in\nspeech parsing.\n"
      ],
      "doi": "10.21437/Interspeech.2017-73"
    },
    "wang17j_interspeech": {
      "authors": [
        [
          "Xiao",
          "Wang"
        ],
        [
          "Yanhui",
          "Zhang"
        ],
        [
          "Gang",
          "Peng"
        ]
      ],
      "title": "Context Regularity Indexed by Auditory N1 and P2 Event-Related Potentials",
      "original": "0658",
      "page_count": 5,
      "order": 505,
      "p1": "2421",
      "pn": "2425",
      "abstract": [
        "It is still a question of debate whether the N1-P2 complex is an index\nof low-level auditory processes or whether it can capture higher-order\ninformation encoded in the immediate context. To address this issue,\nthe current study examined the morphology of the N1-P2 complex as a\nfunction of context regularities instantiated at the sublexical level.\nWe presented two types of speech targets in isolation and in contexts\ncomprising sequences of Cantonese words sharing either the entire rime\nunits or just the rime segments (thus lacking lexical tone consistency).\nResults revealed a pervasive yet unequal attenuation of the N1 and\nP2 components: The degree of N1 attenuation tended to decrease while\nthat of P2 increased due to enhanced detectability of more regular\nspeech patterns, as well as their enhanced predictability in the immediate\ncontext. The distinct behaviors of N1 and P2 event-related potentials\ncould be explained by the influence of perceptual experience and the\nhierarchical encoding of context regularities.\n"
      ],
      "doi": "10.21437/Interspeech.2017-658"
    },
    "verma17_interspeech": {
      "authors": [
        [
          "Sakshi",
          "Verma"
        ],
        [
          "K.L.",
          "Prateek"
        ],
        [
          "Karthik",
          "Pandia"
        ],
        [
          "Nauman",
          "Dawalatabad"
        ],
        [
          "Rogier",
          "Landman"
        ],
        [
          "Jitendra",
          "Sharma"
        ],
        [
          "Mriganka",
          "Sur"
        ],
        [
          "Hema A.",
          "Murthy"
        ]
      ],
      "title": "Discovering Language in Marmoset Vocalization",
      "original": "0842",
      "page_count": 5,
      "order": 506,
      "p1": "2426",
      "pn": "2430",
      "abstract": [
        "Various studies suggest that marmosets ( Callithrix jacchus) show behavior\nsimilar to that of humans in many aspects. Analyzing their calls would\nnot only enable us to better understand these species but would also\ngive insights into the evolution of human languages and vocal tract.\nThis paper describes a technique to discover the patterns in marmoset\nvocalization in an unsupervised fashion. The proposed unsupervised\nclustering approach operates in two stages. Initially, voice activity\ndetection (VAD) is applied to remove silences and non-voiced regions\nfrom the audio. This is followed by a group-delay based segmentation\non the voiced regions to obtain smaller segments. In the second stage,\na two-tier clustering is performed on the segments obtained. Individual\nhidden Markov models (HMMs) are built for each of the segments using\na  multiple frame size and  multiple frame rate. The HMMs are then\nclustered until each cluster is made up of a large number of segments.\nOnce all the clusters get enough number of segments, one Gaussian mixture\nmodel (GMM) is built for each of the clusters. These clusters are then\nmerged using Kullback-Leibler (KL) divergence. The algorithm converges\nto the total number of distinct sounds in the audio, as evidenced by\nlistening tests.\n"
      ],
      "doi": "10.21437/Interspeech.2017-842"
    },
    "watanabe17_interspeech": {
      "authors": [
        [
          "Hiroki",
          "Watanabe"
        ],
        [
          "Hiroki",
          "Tanaka"
        ],
        [
          "Sakriani",
          "Sakti"
        ],
        [
          "Satoshi",
          "Nakamura"
        ]
      ],
      "title": "Subject-Independent Classification of Japanese Spoken Sentences by Multiple Frequency Bands Phase Pattern of EEG Response During Speech Perception",
      "original": "0854",
      "page_count": 5,
      "order": 507,
      "p1": "2431",
      "pn": "2435",
      "abstract": [
        "Recent speech perception models propose that neural oscillations in\ntheta band show phase locking to speech envelope to extract syllabic\ninformation and rapid temporal information is processed by the corresponding\nhigher frequency band (e.g., low gamma). It is suggested that phase-locked\nresponses to acoustic features show consistent patterns across subjects.\nPrevious magnetoencephalographic (MEG) experiment showed that subject-dependent\ntemplate matching classification by theta phase patterns could discriminate\nthree English spoken sentences. In this paper, we adopt electroencephalography\n(EEG) to the spoken sentence discrimination on Japanese language, and\nwe investigate the performances in various different settings by using:\n(1) template matching and support vector machine (SVM) classifiers;\n(2) subject dependent and independent models; (3) multiple frequency\nbands including theta, alpha, beta, low gamma, and the combination\nof all frequency bands. The performances in almost settings were higher\nthan the chance level. While performances of SVM and template matching\ndid not differ, the performance with combination of multiple frequency\nbands outperformed the one that trained only on single frequency bands.\nBest accuracies in subject dependent and independent models achieved\n55.2% by SVM on the combination of all frequency bands and 44.0% by\ntemplate matching on the combination of all frequency bands, respectively.\n"
      ],
      "doi": "10.21437/Interspeech.2017-854"
    },
    "rietmolen17_interspeech": {
      "authors": [
        [
          "No\u00e9mie te",
          "Rietmolen"
        ],
        [
          "Radouane El",
          "Yagoubi"
        ],
        [
          "Alain",
          "Ghio"
        ],
        [
          "Corine",
          "Ast\u00e9sano"
        ]
      ],
      "title": "The Phonological Status of the French Initial Accent and its Role in Semantic Processing: An Event-Related Potentials Study",
      "original": "0934",
      "page_count": 5,
      "order": 508,
      "p1": "2436",
      "pn": "2440",
      "abstract": [
        "French accentuation is held to belong to the level of the phrase. Consequently\nFrench is considered &#8216;a language without accent&#8217; with speakers\nthat are &#8216;deaf to stress&#8217;. Recent ERP-studies investigating\nthe French initial accent (IA) however demonstrate listeners not only\ndiscriminate between different stress patterns, but also prefer words\nto be marked with IA early in the process of speech comprehension.\nStill, as words were presented in isolation, it remains unclear whether\nthe preference applied to the lexical or to the phrasal level. In the\ncurrent ERP-study, we address this ambiguity and manipulate IA on words\nembedded in a sentence. Furthermore, we orthogonally manipulate semantic\ncongruity to investigate the interplay between accentuation and later\nspeech processing stages. Preliminary results on 14 participants reveal\na significant interaction effect: the centro-frontally located N400\nwas larger for words without IA, with a bigger effect for semantically\nincongruent sentences. This indicates that IA is encoded at a lexical\nlevel and facilitates semantic processing. Furthermore, as participants\nattended to the semantic content of the sentences, the finding underlines\nthe automaticity of stress processing. In sum, we demonstrate accentuation\nplays an important role in French speech comprehension and call for\nthe traditional view to be reconsidered.\n"
      ],
      "doi": "10.21437/Interspeech.2017-934"
    },
    "zhao17_interspeech": {
      "authors": [
        [
          "Bin",
          "Zhao"
        ],
        [
          "Jianwu",
          "Dang"
        ],
        [
          "Gaoyan",
          "Zhang"
        ]
      ],
      "title": "A Neuro-Experimental Evidence for the Motor Theory of Speech Perception",
      "original": "1741",
      "page_count": 5,
      "order": 509,
      "p1": "2441",
      "pn": "2445",
      "abstract": [
        "The somatotopic activation in the sensorimotor cortex during speech\ncomprehension has been redundantly documented and largely explained\nby the notion of embodied semantics, which suggests that processing\nauditory words referring to body movements recruits the same somatotopic\nregions for that action execution. For this issue, the motor theory\nof speech perception provided another explanation, suggesting that\nthe perception of speech sounds produced by a specific articulator\nmovement may recruit the motor representation of that articulator in\nthe precentral gyrus. To examine the latter theory, we used a set of\nChinese synonyms with different articulatory features, involving lip\ngestures (LipR) or not (LipN), and recorded the electroencephalographic\n(EEG) signals while subjects passively listened to them. It was found\nthat at about 200 ms post-onset, the event-related potential of LipR\nand LipN showed a significant polarity reversal near the precentral\nlip motor areas. EEG source reconstruction results also showed more\nobvious somatotopic activation in the lip region for the LipR than\nthe LipN. Our results provide a positive support for the effect of\narticulatory simulation on speech comprehension and basically agree\nwith the motor theory of speech perception.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1741"
    },
    "agrawal17_interspeech": {
      "authors": [
        [
          "Purvi",
          "Agrawal"
        ],
        [
          "Sriram",
          "Ganapathy"
        ]
      ],
      "title": "Speech Representation Learning Using Unsupervised Data-Driven Modulation Filtering for Robust ASR",
      "original": "0901",
      "page_count": 5,
      "order": 510,
      "p1": "2446",
      "pn": "2450",
      "abstract": [
        "The performance of an automatic speech recognition (ASR) system degrades\nseverely in noisy and reverberant environments in part due to the lack\nof robustness in the underlying representations used in the ASR system.\nOn the other hand, the auditory processing studies have shown the importance\nof modulation filtered spectrogram representations in robust human\nspeech recognition. Inspired by these evidences, we propose a speech\nrepresentation learning paradigm using data-driven 2-D spectro-temporal\nmodulation filter learning. In particular, multiple representations\nare derived using the convolutional restricted Boltzmann machine (CRBM)\nmodel in an unsupervised manner from the input speech spectrogram.\nA filter selection criteria based on average number of active hidden\nunits is also employed to select the representations for ASR. The experiments\nare performed on Wall Street Journal (WSJ) Aurora-4 database with clean\nand multi condition training setup. In these experiments, the ASR results\nobtained from the proposed modulation filtering approach shows significant\nrobustness to noise and channel distortions compared to other feature\nextraction methods (average relative improvements of 19% over baseline\nfeatures in clean training). Furthermore, the ASR experiments performed\non reverberant speech data from the REVERB challenge corpus highlight\nthe benefits of the proposed representation learning scheme for far\nfield speech recognition.\n"
      ],
      "doi": "10.21437/Interspeech.2017-901"
    },
    "mimura17_interspeech": {
      "authors": [
        [
          "Masato",
          "Mimura"
        ],
        [
          "Yoshiaki",
          "Bando"
        ],
        [
          "Kazuki",
          "Shimada"
        ],
        [
          "Shinsuke",
          "Sakai"
        ],
        [
          "Kazuyoshi",
          "Yoshii"
        ],
        [
          "Tatsuya",
          "Kawahara"
        ]
      ],
      "title": "Combined Multi-Channel NMF-Based Robust Beamforming for Noisy Speech Recognition",
      "original": "0642",
      "page_count": 5,
      "order": 511,
      "p1": "2451",
      "pn": "2455",
      "abstract": [
        "We propose a novel acoustic beamforming method using blind source separation\n(BSS) techniques based on non-negative matrix factorization (NMF).\nIn conventional mask-based approaches, hard or soft masks are estimated\nand beamforming is performed using speech and noise spatial covariance\nmatrices calculated from masked noisy observations, but the phase information\nof the target speech is not adequately preserved. In the proposed method,\nwe perform complex-domain source separation based on multi-channel\nNMF with rank-1 spatial model (rank-1 MNMF) to obtain a speech spatial\ncovariance matrix for estimating a steering vector for the target speech\nutilizing the separated speech observation in each time-frequency bin.\nThis accurate steering vector estimation is effectively combined with\nour novel noise mask prediction method using multi-channel robust NMF\n(MRNMF) to construct a Maximum Likelihood (ML) beamformer that achieved\na better speech recognition performance than a state-of-the-art DNN-based\nbeamformer with no environment-specific training. Superiority of the\nphase preserving source separation to real-valued masks in beamforming\nis also confirmed through ASR experiments.\n"
      ],
      "doi": "10.21437/Interspeech.2017-642"
    },
    "yu17b_interspeech": {
      "authors": [
        [
          "Dong",
          "Yu"
        ],
        [
          "Xuankai",
          "Chang"
        ],
        [
          "Yanmin",
          "Qian"
        ]
      ],
      "title": "Recognizing Multi-Talker Speech with Permutation Invariant Training",
      "original": "0305",
      "page_count": 5,
      "order": 512,
      "p1": "2456",
      "pn": "2460",
      "abstract": [
        "In this paper, we propose a novel technique for direct recognition\nof multiple speech streams given the single channel of mixed speech,\nwithout first separating them. Our technique is based on permutation\ninvariant training (PIT) for automatic speech recognition (ASR). In\nPIT-ASR, we compute the average cross entropy (CE) over all frames\nin the whole utterance for each possible output-target assignment,\npick the one with the minimum CE, and optimize for that assignment.\nPIT-ASR forces all the frames of the same speaker to be aligned with\nthe same output layer. This strategy elegantly solves the label permutation\nproblem and speaker tracing problem in one shot. Our experiments on\nartificially mixed AMI data showed that the proposed approach is very\npromising.\n"
      ],
      "doi": "10.21437/Interspeech.2017-305"
    },
    "tachioka17_interspeech": {
      "authors": [
        [
          "Yuuki",
          "Tachioka"
        ],
        [
          "Tomohiro",
          "Narita"
        ],
        [
          "Iori",
          "Miura"
        ],
        [
          "Takanobu",
          "Uramoto"
        ],
        [
          "Natsuki",
          "Monta"
        ],
        [
          "Shingo",
          "Uenohara"
        ],
        [
          "Ken\u2019ichi",
          "Furuya"
        ],
        [
          "Shinji",
          "Watanabe"
        ],
        [
          "Jonathan Le",
          "Roux"
        ]
      ],
      "title": "Coupled Initialization of Multi-Channel Non-Negative Matrix Factorization Based on Spatial and Spectral Information",
      "original": "0061",
      "page_count": 5,
      "order": 513,
      "p1": "2461",
      "pn": "2465",
      "abstract": [
        "Multi-channel non-negative matrix factorization (MNMF) is a multi-channel\nextension of NMF and often outperforms NMF because it can deal with\nspatial and spectral information simultaneously. On the other hand,\nMNMF has a larger number of parameters and its performance heavily\ndepends on the initial values. MNMF factorizes an observation matrix\ninto four matrices: spatial correlation, basis, cluster-indicator latent\nvariables, and activation matrices. This paper proposes effective initialization\nmethods for these matrices. First, the spatial correlation matrix,\nwhich shows the largest initial value dependencies, is initialized\nusing the cross-spectrum method from enhanced speech by binary masking.\nSecond, when the target is speech, constructing bases from phonemes\nexisting in an utterance can improve the performance: this paper proposes\na speech bases selection by using automatic speech recognition (ASR).\nThird, we also propose an initialization method for the cluster-indicator\nlatent variables that couple the spatial and spectral information,\nwhich can achieve the simultaneous optimization of above two matrices.\nExperiments on a noisy ASR task show that the proposed initialization\nsignificantly improves the performance of MNMF by reducing the initial\nvalue dependencies.\n"
      ],
      "doi": "10.21437/Interspeech.2017-61"
    },
    "loweimi17b_interspeech": {
      "authors": [
        [
          "Erfan",
          "Loweimi"
        ],
        [
          "Jon",
          "Barker"
        ],
        [
          "Thomas",
          "Hain"
        ]
      ],
      "title": "Channel Compensation in the Generalised Vector Taylor Series Approach to Robust ASR",
      "original": "0211",
      "page_count": 5,
      "order": 514,
      "p1": "2466",
      "pn": "2470",
      "abstract": [
        "Vector Taylor Series (VTS) is a powerful technique for robust ASR but,\nin its standard form, it can only be applied to log-filter bank and\nMFCC features. In earlier work, we presented a generalised VTS (gVTS)\nthat extends the applicability of VTS to front-ends which employ a\npower transformation non-linearity. gVTS was shown to provide performance\nimprovements in both clean and additive noise conditions. This paper\nmakes two novel contributions. Firstly, while the previous gVTS formulation\nassumed that noise was purely additive, we now derive gVTS formulae\nfor the case of speech in the presence of both additive noise and channel\ndistortion. Second, we propose a novel iterative method for estimating\nthe channel distortion which utilises gVTS itself and converges after\na few iterations. Since the new gVTS blindly assumes the existence\nof both additive noise and channel effects, it is important not to\nintroduce extra distortion when either are absent. Experimental results\nconducted on LVCSR Aurora-4 database show that the new formulation\npasses this test. In the presence of channel noise only, it provides\nrelative WER reductions of up to 30% and 26%, compared with previous\ngVTS and multi-style training with cepstral mean normalisation, respectively.\n"
      ],
      "doi": "10.21437/Interspeech.2017-211"
    },
    "king17_interspeech": {
      "authors": [
        [
          "Brian",
          "King"
        ],
        [
          "I-Fan",
          "Chen"
        ],
        [
          "Yonatan",
          "Vaizman"
        ],
        [
          "Yuzong",
          "Liu"
        ],
        [
          "Roland",
          "Maas"
        ],
        [
          "Sree Hari Krishnan",
          "Parthasarathi"
        ],
        [
          "Bj\u00f6rn",
          "Hoffmeister"
        ]
      ],
      "title": "Robust Speech Recognition via Anchor Word Representations",
      "original": "1570",
      "page_count": 5,
      "order": 515,
      "p1": "2471",
      "pn": "2475",
      "abstract": [
        "A challenge for speech recognition for voice-controlled household devices,\nlike the Amazon Echo or Google Home, is robustness against interfering\nbackground speech. Formulated as a far-field speech recognition problem,\nanother person or media device in proximity can produce background\nspeech that can interfere with the device-directed speech. We expand\non our previous work on device-directed speech detection in the far-field\nspeech setting and introduce two approaches for robust acoustic modeling.\nBoth methods are based on the idea of using an anchor word taken from\nthe device directed speech. Our first method employs a simple yet effective\nnormalization of the acoustic features by subtracting the mean derived\nover the anchor word. The second method utilizes an encoder network\nprojecting the anchor word onto a fixed-size embedding, which serves\nas an additional input to the acoustic model. The encoder network and\nacoustic model are jointly trained. Results on an in-house dataset\nreveal that, in the presence of background speech, the proposed approaches\ncan achieve up to 35% relative word error rate reduction.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1570"
    },
    "bapna17_interspeech": {
      "authors": [
        [
          "Ankur",
          "Bapna"
        ],
        [
          "Gokhan",
          "T\u00fcr"
        ],
        [
          "Dilek",
          "Hakkani-T\u00fcr"
        ],
        [
          "Larry",
          "Heck"
        ]
      ],
      "title": "Towards Zero-Shot Frame Semantic Parsing for Domain Scaling",
      "original": "0518",
      "page_count": 5,
      "order": 516,
      "p1": "2476",
      "pn": "2480",
      "abstract": [
        "State-of-the-art slot filling models for goal-oriented human/ machine\nconversational language understanding systems rely on deep learning\nmethods. While multi-task training of such models alleviates the need\nfor large in-domain annotated datasets, bootstrapping a semantic parsing\nmodel for a new domain using only the semantic frame, such as the back-end\nAPI or knowledge graph schema, is still one of the holy grail tasks\nof language understanding for dialogue systems. This paper proposes\na deep learning based approach that can utilize  only the slot description\nin context without the need for any labeled or unlabeled in-domain\nexamples, to quickly bootstrap a new domain. The main idea of this\npaper is to leverage the encoding of the slot names and descriptions\nwithin a multi-task deep learned slot filling model, to implicitly\nalign slots across domains. The proposed approach is promising for\nsolving the domain scaling problem and eliminating the need for any\nmanually annotated data or explicit schema alignment. Furthermore,\nour experiments on multiple domains show that this approach results\nin significantly better slot-filling performance when compared to using\nonly in-domain data, especially in the low data regime.\n"
      ],
      "doi": "10.21437/Interspeech.2017-518"
    },
    "georgiadou17_interspeech": {
      "authors": [
        [
          "Despoina",
          "Georgiadou"
        ],
        [
          "Vassilios",
          "Diakoloukas"
        ],
        [
          "Vassilios",
          "Tsiaras"
        ],
        [
          "Vassilios",
          "Digalakis"
        ]
      ],
      "title": "ClockWork-RNN Based Architectures for Slot Filling",
      "original": "1075",
      "page_count": 5,
      "order": 517,
      "p1": "2481",
      "pn": "2485",
      "abstract": [
        "A prevalent and challenging task in spoken language understanding is\nslot filling. Currently, the best approaches in this domain are based\non recurrent neural networks (RNNs). However, in their simplest form,\nRNNs cannot learn long-term dependencies in the data. In this paper,\nwe propose the use of ClockWork recurrent neural network (CW-RNN) architectures\nin the slot-filling domain. CW-RNN is a multi-timescale implementation\nof the simple RNN architecture, which has proven to be powerful since\nit maintains relatively small model complexity. In addition, CW-RNN\nexhibits a great ability to model long-term memory inherently. In our\nexperiments on the ATIS benchmark data set, we also evaluate several\nnovel variants of CW-RNN and we find that they significantly outperform\nsimple RNNs and they achieve results among the state-of-the-art, while\nretaining smaller complexity.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1075"
    },
    "jannet17_interspeech": {
      "authors": [
        [
          "Mohamed Ameur Ben",
          "Jannet"
        ],
        [
          "Olivier",
          "Galibert"
        ],
        [
          "Martine",
          "Adda-Decker"
        ],
        [
          "Sophie",
          "Rosset"
        ]
      ],
      "title": "Investigating the Effect of ASR Tuning on Named Entity Recognition",
      "original": "1482",
      "page_count": 5,
      "order": 518,
      "p1": "2486",
      "pn": "2490",
      "abstract": [
        "Information retrieval from speech is a key technology for many applications,\nas it allows access to large amounts of audio data. This technology\nrequires two major components: an automatic speech recognizer (ASR)\nand a text-based information retrieval module such as a key word extractor\nor a named entity recognizer (NER). When combining the two components,\nthe resulting final application needs to be globally optimized. However,\nASR and information retrieval are usually developed and optimized separately.\nThe ASR tends to be optimized to reduce the word error rate (WER),\na metric which does not take into account the contextual and syntactic\nroles of the words, which are valuable information for information\nretrieval systems. In this paper we investigate different ways to tune\nthe ASR for a speech-based NER system. In an end-to-end configuration\nwe also tested several ASR metrics, including WER, NE-WER and ATENE,\nas well as the use of an oracle during the development step. Our results\nshow that using a NER oracle to tune the system reduces the named entity\nrecognition error rate by more than 1% absolute, and using the ATENE\nmetric allows us to reduce it by more than 0.75%. We also show that\nthese optimization approaches favor a higher ASR language model weight\nwhich entails an overall gain in NER performance, despite a local increase\nof the WER.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1482"
    },
    "dinarelli17_interspeech": {
      "authors": [
        [
          "Marco",
          "Dinarelli"
        ],
        [
          "Vedran",
          "Vukotic"
        ],
        [
          "Christian",
          "Raymond"
        ]
      ],
      "title": "Label-Dependency Coding in Simple Recurrent Networks for Spoken Language Understanding",
      "original": "1480",
      "page_count": 5,
      "order": 519,
      "p1": "2491",
      "pn": "2495",
      "abstract": [
        "Modeling target label dependencies is important for sequence labeling\ntasks. This may become crucial in the case of Spoken Language Understanding\n(SLU) applications, especially for the slot-filling task where models\nhave to deal often with a high number of target labels. Conditional\nRandom Fields (CRF) were previously considered as the most efficient\nalgorithm in these conditions. More recently, different architectures\nof Recurrent Neural Networks (RNNs) have been proposed for the SLU\nslot-filling task. Most of them, however, have been successfully evaluated\non the simple ATIS database, on which it is difficult to draw significant\nconclusions. In this paper we propose new variants of RNNs able to\nlearn efficiently and effectively label dependencies by integrating\nlabel embeddings. We show first that modeling label dependencies is\nuseless on the (simple) ATIS database and unstructured models can produce\nstate-of-the-art results on this benchmark. On ATIS our new variants\nachieve the same results as state-of-the-art models, while being much\nsimpler. On the other hand, on the MEDIA benchmark, we show that the\nmodification introduced in the proposed RNN outperforms traditional\nRNNs and CRF models.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1480"
    },
    "meng17_interspeech": {
      "authors": [
        [
          "Zhong",
          "Meng"
        ],
        [
          "Biing-Hwang",
          "Juang"
        ]
      ],
      "title": "Minimum Semantic Error Cost Training of Deep Long Short-Term Memory Networks for Topic Spotting on Conversational Speech",
      "original": "0590",
      "page_count": 5,
      "order": 520,
      "p1": "2496",
      "pn": "2500",
      "abstract": [
        "The topic spotting performance on spontaneous conversational speech\ncan be significantly improved by operating a support vector machine\nwith a latent semantic rational kernel (LSRK) on the decoded word lattices\n(i.e., weighted finite-state transducers) of the speech [1]. In this\nwork, we propose the minimum semantic error cost (MSEC) training of\na deep bidirectional long short-term memory (BLSTM)-hidden Markov model\nacoustic model for generating lattices that are semantically accurate\nand are better suited for topic spotting with LSRK. With the MSEC training,\nthe expected semantic error cost of all possible word sequences on\nthe lattices is minimized given the reference. The word-word semantic\nerror cost is first computed from either the latent semantic analysis\nor distributed vector-space word representations learned from the recurrent\nneural networks and is then accumulated to form the expected semantic\nerror cost of the hypothesized word sequences. The proposed method\nachieves 3.5%&#8211;4.5% absolute topic classification accuracy improvement\nover the baseline BLSTM trained with cross-entropy on Switchboard-1\nRelease 2 dataset.\n"
      ],
      "doi": "10.21437/Interspeech.2017-590"
    },
    "liu17b_interspeech": {
      "authors": [
        [
          "Chunxi",
          "Liu"
        ],
        [
          "Jan",
          "Trmal"
        ],
        [
          "Matthew",
          "Wiesner"
        ],
        [
          "Craig",
          "Harman"
        ],
        [
          "Sanjeev",
          "Khudanpur"
        ]
      ],
      "title": "Topic Identification for Speech Without ASR",
      "original": "1093",
      "page_count": 5,
      "order": 521,
      "p1": "2501",
      "pn": "2505",
      "abstract": [
        "Modern topic identification (topic ID) systems for speech use automatic\nspeech recognition (ASR) to produce speech transcripts, and perform\nsupervised classification on such ASR outputs. However, under resource-limited\nconditions, the manually transcribed speech required to develop standard\nASR systems can be severely limited or unavailable. In this paper,\nwe investigate alternative unsupervised solutions to obtaining tokenizations\nof speech in terms of a vocabulary of automatically discovered word-like\nor phoneme-like units, without depending on the supervised training\nof ASR systems. Moreover, using automatic phoneme-like tokenizations,\nwe demonstrate that a convolutional neural network based framework\nfor learning spoken document representations provides competitive performance\ncompared to a standard bag-of-words representation, as evidenced by\ncomprehensive topic ID evaluations on both single-label and multi-label\nclassification tasks.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1093"
    },
    "liu17c_interspeech": {
      "authors": [
        [
          "Bing",
          "Liu"
        ],
        [
          "Ian",
          "Lane"
        ]
      ],
      "title": "An End-to-End Trainable Neural Network Model with Belief Tracking for Task-Oriented Dialog",
      "original": "1326",
      "page_count": 5,
      "order": 522,
      "p1": "2506",
      "pn": "2510",
      "abstract": [
        "We present a novel end-to-end trainable neural network model for task-oriented\ndialog systems. The model is able to track dialog state, issue API\ncalls to knowledge base (KB), and incorporate structured KB query results\ninto system responses to successfully complete task-oriented dialogs.\nThe proposed model produces well-structured system responses by jointly\nlearning belief tracking and KB result processing conditioning on the\ndialog history. We evaluate the model in a restaurant search domain\nusing a dataset that is converted from the second Dialog State Tracking\nChallenge (DSTC2) corpus. Experiment results show that the proposed\nmodel can robustly track dialog state given the dialog history. Moreover,\nour model demonstrates promising results in producing appropriate system\nresponses, outperforming prior end-to-end trainable neural network\nmodels using per-response accuracy evaluation metrics.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1326"
    },
    "cuayahuitl17_interspeech": {
      "authors": [
        [
          "Heriberto",
          "Cuay\u00e1huitl"
        ],
        [
          "Seunghak",
          "Yu"
        ]
      ],
      "title": "Deep Reinforcement Learning of Dialogue Policies with Less Weight Updates",
      "original": "1060",
      "page_count": 5,
      "order": 523,
      "p1": "2511",
      "pn": "2515",
      "abstract": [
        "Deep reinforcement learning dialogue systems are attractive because\nthey can jointly learn their feature representations and policies without\nmanual feature engineering. But its application is challenging due\nto slow learning. We propose a two-stage method for accelerating the\ninduction of single or multi-domain dialogue policies. While the first\nstage reduces the amount of weight updates over time, the second stage\nuses very limited minibatches (of as much as two learning experiences)\nsampled from experience replay memories. The former frequently updates\nthe weights of the neural nets at early stages of training, and decreases\nthe amount of updates as training progresses by performing updates\nduring exploration and by skipping updates during exploitation. The\nlearning process is thus accelerated through less weight updates in\nboth stages. An empirical evaluation in three domains (restaurants,\nhotels and tv guide) confirms that the proposed method trains policies\n5 times faster than a baseline without the proposed method. Our findings\nare useful for training larger-scale neural-based spoken dialogue systems.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1060"
    },
    "bayer17_interspeech": {
      "authors": [
        [
          "Ali Orkan",
          "Bayer"
        ],
        [
          "Evgeny A.",
          "Stepanov"
        ],
        [
          "Giuseppe",
          "Riccardi"
        ]
      ],
      "title": "Towards End-to-End Spoken Dialogue Systems with Turn Embeddings",
      "original": "1574",
      "page_count": 5,
      "order": 524,
      "p1": "2516",
      "pn": "2520",
      "abstract": [
        "Training task-oriented dialogue systems requires significant amount\nof manual effort and integration of many independently built components;\nmoreover, the pipeline is prone to error-propagation. End-to-end training\nhas been proposed to overcome these problems by training the whole\nsystem over the utterances of both dialogue parties. In this paper\nwe present an end-to-end spoken dialogue system architecture that is\nbased on turn embeddings. Turn embeddings encode a robust representation\nof user turns with a local dialogue history and they are trained using\nsequence-to-sequence models. Turn embeddings are trained by generating\nthe previous and the next turns of the dialogue and additionally perform\nspoken language understanding. The end-to-end spoken dialogue system\nis trained using the pre-trained turn embeddings in a stateful architecture\nthat considers the whole dialogue history. We observe that the proposed\nspoken dialogue system architecture outperforms the models based on\nlocal-only dialogue history and it is robust to automatic speech recognition\nerrors.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1574"
    },
    "akhtiamov17_interspeech": {
      "authors": [
        [
          "Oleg",
          "Akhtiamov"
        ],
        [
          "Maxim",
          "Sidorov"
        ],
        [
          "Alexey A.",
          "Karpov"
        ],
        [
          "Wolfgang",
          "Minker"
        ]
      ],
      "title": "Speech and Text Analysis for Multimodal Addressee Detection in Human-Human-Computer Interaction",
      "original": "0501",
      "page_count": 5,
      "order": 525,
      "p1": "2521",
      "pn": "2525",
      "abstract": [
        "The necessity of addressee detection arises in multiparty spoken dialogue\nsystems which deal with human-human-computer interaction. In order\nto cope with this kind of interaction, such a system is supposed to\ndetermine whether the user is addressing the system or another human.\nThe present study is focused on multimodal addressee detection and\ndescribes three levels of speech and text analysis: acoustical, syntactical,\nand lexical. We define the connection between different levels of analysis\nand the classification performance for different categories of speech\nand determine the dependence of addressee detection performance on\nspeech recognition accuracy. We also compare the obtained results with\nthe results of the original research performed by the authors of the\nSmart Video Corpus which we use in our computations. Our most effective\nmeta-classifier working with acoustical, syntactical, and lexical features\nreaches an unweighted average recall equal to 0.917 showing almost\na nine percent advantage over the best baseline model, though this\nbaseline classifier additionally uses head orientation data. We also\npropose a universal meta-model based on acoustical and syntactical\nanalysis, which may theoretically be applied in different domains.\n"
      ],
      "doi": "10.21437/Interspeech.2017-501"
    },
    "ramanarayanan17c_interspeech": {
      "authors": [
        [
          "Vikram",
          "Ramanarayanan"
        ],
        [
          "Chee Wee",
          "Leong"
        ],
        [
          "David",
          "Suendermann-Oeft"
        ]
      ],
      "title": " Rushing to Judgement: How do Laypeople Rate Caller Engagement in Thin-Slice Videos of Human&#8211;Machine Dialog?",
      "original": "1205",
      "page_count": 5,
      "order": 526,
      "p1": "2526",
      "pn": "2530",
      "abstract": [
        "We analyze the efficacy of a small crowd of na&#239;ve human raters\nin rating engagement during human&#8211;machine dialog interactions.\nEach rater viewed multiple 10 second, thin-slice videos of non-native\nEnglish speakers interacting with a computer-assisted language learning\n(CALL) system and rated how engaged and disengaged those callers were\nwhile interacting with the automated agent. We observe how the crowd&#8217;s\nratings compared to callers&#8217; self ratings of engagement, and\nfurther study how the distribution of these rating assignments vary\nas a function of whether the automated system or the caller was speaking.\nFinally, we discuss the potential applications and pitfalls of such\na crowdsourced paradigm in designing, developing and analyzing engagement-aware\ndialog systems.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1205"
    },
    "kraljevski17_interspeech": {
      "authors": [
        [
          "Ivan",
          "Kraljevski"
        ],
        [
          "Diane",
          "Hirschfeld"
        ]
      ],
      "title": "Hyperarticulation of Corrections in Multilingual Dialogue Systems",
      "original": "0753",
      "page_count": 5,
      "order": 527,
      "p1": "2531",
      "pn": "2535",
      "abstract": [
        "This present paper aims at answering the question whether there are\ndistinctive cross-linguistic differences associated with hyperarticulated\nspeech in correction dialogue acts. The objective is to assess the\neffort for adaptation of a multilingual dialogue system in 9 different\nlanguages, regarding the recovery strategies, particularly corrections.\nIf the presence of hyperarticulation significantly differs across languages,\nit will have a significant impact on the dialogue design and recovery\nstrategies.\n"
      ],
      "doi": "10.21437/Interspeech.2017-753"
    },
    "milde17_interspeech": {
      "authors": [
        [
          "Benjamin",
          "Milde"
        ],
        [
          "Christoph",
          "Schmidt"
        ],
        [
          "Joachim",
          "K\u00f6hler"
        ]
      ],
      "title": "Multitask Sequence-to-Sequence Models for Grapheme-to-Phoneme Conversion",
      "original": "1436",
      "page_count": 5,
      "order": 528,
      "p1": "2536",
      "pn": "2540",
      "abstract": [
        "Recently, neural sequence-to-sequence (Seq2Seq) models have been applied\nto the problem of grapheme-to-phoneme (G2P) conversion. These models\noffer a straightforward way of modeling the conversion by jointly learning\nthe alignment and translation of input to output tokens in an end-to-end\nfashion. However, until now this approach did not show improved error\nrates on its own compared to traditional joint-sequence based n-gram\nmodels for G2P. In this paper, we investigate how multitask learning\ncan improve the performance of Seq2Seq G2P models. A single Seq2Seq\nmodel is trained on multiple phoneme lexicon datasets containing multiple\nlanguages and phonetic alphabets. Although multi-language learning\ndoes not show improved error rates, combining standard datasets and\ncrawled data with different phonetic alphabets of the same language\nshows promising error reductions on English and German Seq2Seq G2P\nconversion. Finally, combining Seq2seq G2P models with standard n-grams\nbased models yields significant improvements over using either model\nalone.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1436"
    },
    "zhang17h_interspeech": {
      "authors": [
        [
          "Xiaohui",
          "Zhang"
        ],
        [
          "Vimal",
          "Manohar"
        ],
        [
          "Daniel",
          "Povey"
        ],
        [
          "Sanjeev",
          "Khudanpur"
        ]
      ],
      "title": "Acoustic Data-Driven Lexicon Learning Based on a Greedy Pronunciation Selection Framework",
      "original": "0588",
      "page_count": 5,
      "order": 529,
      "p1": "2541",
      "pn": "2545",
      "abstract": [
        "Speech recognition systems for irregularly-spelled languages like English\nnormally require hand-written pronunciations. In this paper, we describe\na system for automatically obtaining pronunciations of words for which\npronunciations are not available, but for which transcribed data exists.\nOur method integrates information from the letter sequence and from\nthe acoustic evidence. The novel aspect of the problem that we address\nis the problem of how to prune entries from such a lexicon (since,\nempirically, lexicons with too many entries do not tend to be good\nfor ASR performance). Experiments on various ASR tasks show that, with\nthe proposed framework, starting with an initial lexicon of several\nthousand words, we are able to learn a lexicon which performs close\nto a full expert lexicon in terms of WER performance on test data,\nand is better than lexicons built using G2P alone or with a pruning\ncriterion based on pronunciation probability.\n"
      ],
      "doi": "10.21437/Interspeech.2017-588"
    },
    "shinozaki17_interspeech": {
      "authors": [
        [
          "Takahiro",
          "Shinozaki"
        ],
        [
          "Shinji",
          "Watanabe"
        ],
        [
          "Daichi",
          "Mochihashi"
        ],
        [
          "Graham",
          "Neubig"
        ]
      ],
      "title": "Semi-Supervised Learning of a Pronunciation Dictionary from Disjoint Phonemic Transcripts and Text",
      "original": "1081",
      "page_count": 5,
      "order": 530,
      "p1": "2546",
      "pn": "2550",
      "abstract": [
        "While the performance of automatic speech recognition systems has recently\napproached human levels in some tasks, the application is still limited\nto specific domains. This is because system development relies on extensive\nsupervised training and expert tuning in the target domain. To solve\nthis problem, systems must become more self-sufficient, having the\nability to learn directly from speech and adapt to new tasks. One open\nquestion in this area is how to learn a pronunciation dictionary containing\nthe appropriate vocabulary. Humans can recognize words, even ones they\nhave never heard before, by reading text and understanding the context\nin which a word is used. However, this ability is missing in current\nspeech recognition systems. In this work, we propose a new framework\nthat automatically expands an initial pronunciation dictionary using\nindependently sampled acoustic and textual data. While the task is\nvery challenging and in its initial stage, we demonstrate that a model\nbased on Bayesian learning of Dirichlet processes can acquire word\npronunciations from phone transcripts and text of the WSJ data set.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1081"
    },
    "smit17_interspeech": {
      "authors": [
        [
          "Peter",
          "Smit"
        ],
        [
          "Sami",
          "Virpioja"
        ],
        [
          "Mikko",
          "Kurimo"
        ]
      ],
      "title": "Improved Subword Modeling for WFST-Based Speech Recognition",
      "original": "0103",
      "page_count": 5,
      "order": 531,
      "p1": "2551",
      "pn": "2555",
      "abstract": [
        "Because in agglutinative languages the number of observed word forms\nis very high, subword units are often utilized in speech recognition.\nHowever, the proper use of subword units requires careful consideration\nof details such as silence modeling, position-dependent phones, and\ncombination of the units. In this paper, we implement subword modeling\nin the Kaldi toolkit by creating modified lexicon by finite-state transducers\nto represent the subword units correctly. We experiment with multiple\ntypes of word boundary markers and achieve the best results by adding\na marker to the left or right side of a subword unit whenever it is\nnot preceded or followed by a word boundary, respectively. We also\ncompare three different toolkits that provide data-driven subword segmentations.\nIn our experiments on a variety of Finnish and Estonian datasets, the\nbest subword models do outperform word-based models and naive subword\nimplementations. The largest relative reduction in WER is a 23% over\nword-based models for a Finnish read speech dataset. The results are\nalso better than any previously published ones for the same datasets,\nand the improvement on all datasets is more than 5%.\n"
      ],
      "doi": "10.21437/Interspeech.2017-103"
    },
    "bruguier17_interspeech": {
      "authors": [
        [
          "Antoine",
          "Bruguier"
        ],
        [
          "Danushen",
          "Gnanapragasam"
        ],
        [
          "Leif",
          "Johnson"
        ],
        [
          "Kanishka",
          "Rao"
        ],
        [
          "Fran\u00e7oise",
          "Beaufays"
        ]
      ],
      "title": "Pronunciation Learning with RNN-Transducers",
      "original": "0047",
      "page_count": 5,
      "order": 532,
      "p1": "2556",
      "pn": "2560",
      "abstract": [
        "Most speech recognition systems rely on pronunciation dictionaries\nto provide accurate transcriptions. Typically, some pronunciations\nare carved manually, but many are produced using pronunciation learning\nalgorithms. Successful algorithms must have the ability to generate\nrich pronunciation variants, e.g. to accommodate words of foreign origin,\nwhile being robust to artifacts of the training data, e.g. noise in\nthe acoustic segments from which the pronunciations are learned if\nthe method uses acoustic signals. We propose a general finite-state\ntransducer (FST) framework to describe such algorithms. This representation\nis flexible enough to accommodate a wide variety of pronunciation learning\nalgorithms, including approaches that rely on the availability of acoustic\ndata, and methods that only rely on the spelling of the target words.\nIn particular, we show that the pronunciation FST can be built from\na recurrent neural network (RNN) and tuned to provide rich yet constrained\npronunciations. This new approach reduces the number of incorrect pronunciations\nlearned from Google Voice traffic by up to 25% relative.\n"
      ],
      "doi": "10.21437/Interspeech.2017-47"
    },
    "naaman17_interspeech": {
      "authors": [
        [
          "Einat",
          "Naaman"
        ],
        [
          "Yossi",
          "Adi"
        ],
        [
          "Joseph",
          "Keshet"
        ]
      ],
      "title": "Learning Similarity Functions for Pronunciation Variations",
      "original": "1117",
      "page_count": 5,
      "order": 533,
      "p1": "2561",
      "pn": "2565",
      "abstract": [
        "A significant source of errors in Automatic Speech Recognition (ASR)\nsystems is due to pronunciation variations which occur in spontaneous\nand conversational speech. Usually ASR systems use a finite lexicon\nthat provides one or more pronunciations for each word. In this paper,\nwe focus on learning a similarity function between two pronunciations.\nThe pronunciations can be the canonical and the surface pronunciations\nof the same word or they can be two surface pronunciations of different\nwords. This task generalizes problems such as lexical access (the problem\nof learning the mapping between words and their possible pronunciations),\nand defining word neighborhoods. It can also be used to dynamically\nincrease the size of the pronunciation lexicon, or in predicting ASR\nerrors. We propose two methods, which are based on recurrent neural\nnetworks, to learn the similarity function. The first is based on binary\nclassification, and the second is based on learning the ranking of\nthe pronunciations. We demonstrate the efficiency of our approach on\nthe task of lexical access using a subset of the Switchboard conversational\nspeech corpus. Results suggest that on this task our methods are superior\nto previous methods which are based on graphical Bayesian methods.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1117"
    },
    "gelly17_interspeech": {
      "authors": [
        [
          "G.",
          "Gelly"
        ],
        [
          "J.L.",
          "Gauvain"
        ]
      ],
      "title": "Spoken Language Identification Using LSTM-Based Angular Proximity",
      "original": "1334",
      "page_count": 5,
      "order": 534,
      "p1": "2566",
      "pn": "2570",
      "abstract": [
        "This paper describes the design of an acoustic language identification\n(LID) system based on LSTMs that directly maps a sequence of acoustic\nfeatures to a vector in a vector space where the angular proximity\ncorresponds to a measure of language/dialect similarity. A specific\narchitecture for the LSTM-based language vector extractor is introduced\nalong with the angular proximity loss function to train it. This new\nLSTM-based LID system is quicker to train than a standard RNN topology\nusing stacked layers trained with the cross-entropy loss function and\nobtains significantly lower language error rates. Experiments compare\nthis approach to our previous developments on the subject, as well\nas to two widely used LID techniques: a phonotactic system using DNN\nacoustic models and an i-vector system. Results are reported on two\ndifferent data sets: the 14 languages of NIST LRE07 and the 20 closely\nrelated languages and dialects of NIST LRE15. In addition to reporting\nthe NIST Cavg metric which served as the primary metric for the LRE07\nand LRE15 evaluations, the average LER is provided.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1334"
    },
    "jin17_interspeech": {
      "authors": [
        [
          "Ma",
          "Jin"
        ],
        [
          "Yan",
          "Song"
        ],
        [
          "Ian",
          "McLoughlin"
        ],
        [
          "Wu",
          "Guo"
        ],
        [
          "Li-Rong",
          "Dai"
        ]
      ],
      "title": "End-to-End Language Identification Using High-Order Utterance Representation with Bilinear Pooling",
      "original": "0044",
      "page_count": 5,
      "order": 535,
      "p1": "2571",
      "pn": "2575",
      "abstract": [
        "A key problem in spoken language identification (LID) is how to design\neffective representations which are specific to language information.\nRecent advances in deep neural networks have led to significant improvements\nin results, with deep end-to-end methods proving effective. This paper\nproposes a novel network which aims to model an effective representation\nfor high (first and second)-order statistics of LID-senones, defined\nas being LID analogues of senones in speech recognition. The high-order\ninformation extracted through bilinear pooling is robust to speakers,\nchannels and background noise. Evaluation with NIST LRE 2009 shows\nimproved performance compared to current state-of-the-art DBF/i-vector\nsystems, achieving over 33% and 20% relative equal error rate (EER)\nimprovement for 3s and 10s utterances and over 40% relative C<SUB>avg</SUB>\nimprovement for all durations.\n"
      ],
      "doi": "10.21437/Interspeech.2017-44"
    },
    "zhang17i_interspeech": {
      "authors": [
        [
          "Qian",
          "Zhang"
        ],
        [
          "John H.L.",
          "Hansen"
        ]
      ],
      "title": "Dialect Recognition Based on Unsupervised Bottleneck Features",
      "original": "0576",
      "page_count": 5,
      "order": 536,
      "p1": "2576",
      "pn": "2580",
      "abstract": [
        "Recently, bottleneck features (BNF) with an i-Vector strategy has been\nused for state-of-the-art language/dialect identification. However,\ntraditional bottleneck extraction requires an additional transcribed\ncorpus which is used for acoustic modeling. Alternatively, an unsupervised\nBNF extraction diagram is proposed in our study, which is derived from\nthe traditional structure but trained with an estimated phonetic label.\nThe proposed method is evaluated on a 4-way Chinese dialect dataset\nand a 5-way closely spaced Pan-Arabic corpus. Compared to a baseline\ni-Vector system based on acoustic features MFCCs, the proposed unsupervised\nBNF consistently achieves better performance across two corpora. Specifically,\nthe EER and overall performance C_avg * 100 are improved by a relative\n+48% and +52%, respectively. Even under the condition with limited\ntraining data, the proposed feature still achieves up to 24% relative\nimprovement compared to baseline, all without the need of a secondary\ntranscribed corpus.\n"
      ],
      "doi": "10.21437/Interspeech.2017-576"
    },
    "irtza17_interspeech": {
      "authors": [
        [
          "Saad",
          "Irtza"
        ],
        [
          "Vidhyasaharan",
          "Sethu"
        ],
        [
          "Eliathamby",
          "Ambikairajah"
        ],
        [
          "Haizhou",
          "Li"
        ]
      ],
      "title": "Investigating Scalability in Hierarchical Language Identification System",
      "original": "0596",
      "page_count": 5,
      "order": 537,
      "p1": "2581",
      "pn": "2585",
      "abstract": [
        "State-of-the-art language identification (LID) systems are not easily\nscalable to accommodate new languages. Specifically, as the number\nof target languages grows the error rate of these LID systems increases\nrapidly. This paper addresses such a challenge by adopting a hierarchical\nlanguage identification (HLID) framework. We demonstrate the superior\nscalability of the HLID framework. In particular, HLID only requires\nthe training of relevant nodes in a hierarchical structure instead\nof re-training the entire tree. Experiments conducted on a dataset\nthat combined languages from the NIST LRE 2007, 2009, 2011 and 2015\ndatabases show that as the number of target languages grows from 28\nto 42, the performance of a single level (non-hierarchical) system\ndeteriorates by around 11% while that of the hierarchical system only\ndeteriorates by about 3.4% in terms of C<SUB>avg</SUB>. Finally, experiments\nalso suggest that SVM based systems are more scalable than GPLDA based\nsystems.\n"
      ],
      "doi": "10.21437/Interspeech.2017-596"
    },
    "qian17c_interspeech": {
      "authors": [
        [
          "Yao",
          "Qian"
        ],
        [
          "Keelan",
          "Evanini"
        ],
        [
          "Xinhao",
          "Wang"
        ],
        [
          "David",
          "Suendermann-Oeft"
        ],
        [
          "Robert A.",
          "Pugh"
        ],
        [
          "Patrick L.",
          "Lange"
        ],
        [
          "Hillary R.",
          "Molloy"
        ],
        [
          "Frank K.",
          "Soong"
        ]
      ],
      "title": "Improving Sub-Phone Modeling for Better Native Language Identification with Non-Native English Speech",
      "original": "0245",
      "page_count": 5,
      "order": 538,
      "p1": "2586",
      "pn": "2590",
      "abstract": [
        "Identifying a speaker&#8217;s native language with his speech in a\nsecond language is useful for many human-machine voice interface applications.\nIn this paper, we use a sub-phone-based i-vector approach to identify\nnon-native English speakers&#8217; native languages by their English\nspeech input. Time delay deep neural networks (TDNN) are trained on\nLVCSR corpora for improving the alignment of speech utterances with\ntheir corresponding sub-phonemic &#8220;senone&#8221; sequences. The\nphonetic variability caused by a speaker&#8217;s native language can\nbe better modeled with the sub-phone models than the conventional phone\nmodel based approach. Experimental results on the database released\nfor the 2016 Interspeech ComParE Native Language challenge with 11\ndifferent L1s show that our system outperforms the best system by a\nlarge margin (87.2% UAR compared to 81.3% UAR for the best system from\nthe 2016 ComParE challenge).\n"
      ],
      "doi": "10.21437/Interspeech.2017-245"
    },
    "khurana17_interspeech": {
      "authors": [
        [
          "Sameer",
          "Khurana"
        ],
        [
          "Maryam",
          "Najafian"
        ],
        [
          "Ahmed",
          "Ali"
        ],
        [
          "Tuka Al",
          "Hanai"
        ],
        [
          "Yonatan",
          "Belinkov"
        ],
        [
          "James",
          "Glass"
        ]
      ],
      "title": "QMDIS: QCRI-MIT Advanced Dialect Identification System",
      "original": "1391",
      "page_count": 5,
      "order": 539,
      "p1": "2591",
      "pn": "2595",
      "abstract": [
        "As a continuation of our efforts towards tackling the problem of spoken\nDialect Identification (DID) for Arabic languages, we present the QCRI-MIT\nAdvanced Dialect Identification System (QMDIS). QMDIS is an automatic\nspoken DID system for Dialectal Arabic (DA). In this paper, we report\na comprehensive study of the three main components used in the spoken\nDID task: phonotactic, lexical and acoustic. We use Support Vector\nMachines (SVMs), Logistic Regression (LR) and Convolutional Neural\nNetworks (CNNs) as backend classifiers throughout the study. We perform\nall our experiments on a publicly available dataset and present new\nstate-of-the-art results. QMDIS discriminates between the five most\nwidely used dialects of Arabic: namely Egyptian, Gulf, Levantine, North\nAfrican, and Modern Standard Arabic (MSA).We report &#8776;73% accuracy\nfor system combination. All the data and the code used in our experiments\nare publicly available for research.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1391"
    },
    "alluri17b_interspeech": {
      "authors": [
        [
          "K.N.R.K. Raju",
          "Alluri"
        ],
        [
          "Sivanand",
          "Achanta"
        ],
        [
          "Sudarsana Reddy",
          "Kadiri"
        ],
        [
          "Suryakanth V.",
          "Gangashetty"
        ],
        [
          "Anil Kumar",
          "Vuppala"
        ]
      ],
      "title": "Detection of Replay Attacks Using Single Frequency Filtering Cepstral Coefficients",
      "original": "0256",
      "page_count": 5,
      "order": 540,
      "p1": "2596",
      "pn": "2600",
      "abstract": [
        "Automatic speaker verification systems are vulnerable to spoofing attacks.\nRecently, various countermeasures have been developed for detecting\nhigh technology attacks such as speech synthesis and voice conversion.\nHowever, there is a wide gap in dealing with replay attacks. In this\npaper, we propose a new feature for replay attack detection based on\nsingle frequency filtering (SFF), which provides high temporal and\nspectral resolution at each instant. Single frequency filtering cepstral\ncoefficients (SFFCC) with Gaussian mixture model classifier are used\nfor the experimentation on the standard BTAS-2016 corpus. The previously\nreported best result, which is based on constant Q cepstral coefficients\n(CQCC) achieved a half total error rate of 0.67% on this data-set.\nOur proposed method outperforms the state of the art (CQCC) with a\nhalf total error rate of 0.0002%.\n"
      ],
      "doi": "10.21437/Interspeech.2017-256"
    },
    "sailor17_interspeech": {
      "authors": [
        [
          "Hardik B.",
          "Sailor"
        ],
        [
          "Madhu R.",
          "Kamble"
        ],
        [
          "Hemant A.",
          "Patil"
        ]
      ],
      "title": "Unsupervised Representation Learning Using Convolutional Restricted Boltzmann Machine for Spoof Speech Detection",
      "original": "1393",
      "page_count": 5,
      "order": 541,
      "p1": "2601",
      "pn": "2605",
      "abstract": [
        "Speech Synthesis (SS) and Voice Conversion (VC) presents a genuine\nrisk of attacks for Automatic Speaker Verification (ASV) technology.\nIn this paper, we use our recently proposed unsupervised filterbank\nlearning technique using Convolutional Restricted Boltzmann Machine\n(ConvRBM) as a front-end feature representation. ConvRBM is trained\non training subset of ASV spoof 2015 challenge database. Analyzing\nthe filterbank trained on this dataset shows that ConvRBM learned more\nlow-frequency subband filters compared to training on natural speech\ndatabase such as TIMIT. The spoofing detection experiments were performed\nusing Gaussian Mixture Models (GMM) as a back-end classifier. ConvRBM-based\ncepstral coefficients (ConvRBM-CC) perform better than hand crafted\nMel Frequency Cepstral Coefficients (MFCC). On the evaluation set,\nConvRBM-CC features give an absolute reduction of 4.76% in Equal Error\nRate (EER) compared to MFCC features. Specifically, ConvRBM-CC features\nsignificantly perform better in both known attacks (1.93%) and unknown\nattacks (5.87%) compared to MFCC features.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1393"
    },
    "suthokumar17_interspeech": {
      "authors": [
        [
          "Gajan",
          "Suthokumar"
        ],
        [
          "Kaavya",
          "Sriskandaraja"
        ],
        [
          "Vidhyasaharan",
          "Sethu"
        ],
        [
          "Chamith",
          "Wijenayake"
        ],
        [
          "Eliathamby",
          "Ambikairajah"
        ]
      ],
      "title": "Independent Modelling of High and Low Energy Speech Frames for Spoofing Detection",
      "original": "0836",
      "page_count": 5,
      "order": 542,
      "p1": "2606",
      "pn": "2610",
      "abstract": [
        "Spoofing detection systems for automatic speaker verification have\nmoved from only modelling voiced frames to modelling all speech frames.\nUnvoiced speech has been shown to carry information about spoofing\nattacks and anti-spoofing systems may further benefit by treating voiced\nand unvoiced speech differently. In this paper, we separate speech\ninto low and high energy frames and independently model the distributions\nof both to form two spoofing detection systems that are then fused\nat the score level. Experiments conducted on the ASVspoof 2015, BTAS\n2016 and Spoofing and Anti-Spoofing (SAS) corpora demonstrate that\nthe proposed approach of fusing two independent high and low energy\nspoofing detection systems consistently outperforms the standard approach\nthat does not distinguish between high and low energy frames.\n"
      ],
      "doi": "10.21437/Interspeech.2017-836"
    },
    "sarkar17_interspeech": {
      "authors": [
        [
          "Achintya Kr.",
          "Sarkar"
        ],
        [
          "Md.",
          "Sahidullah"
        ],
        [
          "Zheng-Hua",
          "Tan"
        ],
        [
          "Tomi",
          "Kinnunen"
        ]
      ],
      "title": "Improving Speaker Verification Performance in Presence of Spoofing Attacks Using Out-of-Domain Spoofed Data",
      "original": "1758",
      "page_count": 5,
      "order": 543,
      "p1": "2611",
      "pn": "2615",
      "abstract": [
        "Automatic speaker verification (ASV) systems are vulnerable to spoofing\nattacks using speech generated by voice conversion and speech synthesis\ntechniques. Commonly, a countermeasure (CM) system is integrated with\nan ASV system for improved protection against spoofing attacks. But\nintegration of the two systems is challenging and often leads to increased\nfalse rejection rates. Furthermore, the performance of CM severely\ndegrades if in-domain development data are unavailable. In this study,\ntherefore, we propose a solution that uses two separate background\nmodels &#8212; one from human speech and another from spoofed data.\nDuring test, the ASV score for an input utterance is computed as the\ndifference of the log-likelihood against the target model and the combination\nof the log-likelihoods against two background models. Evaluation experiments\nare conducted using the joint ASV and CM protocol of ASVspoof 2015\ncorpus consisting of text-independent ASV tasks with short utterances.\nOur proposed system reduces error rates in the presence of spoofing\nattacks by using out-of-domain spoofed data for system development,\nwhile maintaining the performance for zero-effort imposter attacks\ncompared to the baseline system.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1758"
    },
    "nagrani17_interspeech": {
      "authors": [
        [
          "Arsha",
          "Nagrani"
        ],
        [
          "Joon Son",
          "Chung"
        ],
        [
          "Andrew",
          "Zisserman"
        ]
      ],
      "title": "VoxCeleb: A Large-Scale Speaker Identification Dataset",
      "original": "0950",
      "page_count": 5,
      "order": 544,
      "p1": "2616",
      "pn": "2620",
      "abstract": [
        "Most existing datasets for speaker identification contain samples obtained\nunder quite constrained conditions, and are usually hand-annotated,\nhence limited in size. The goal of this paper is to generate a large\nscale text-independent speaker identification dataset collected &#8216;in\nthe wild&#8217;.<br style=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  We make two contributions. First, we propose a fully automated\npipeline based on computer vision techniques to create the dataset\nfrom open-source media. Our pipeline involves obtaining videos from\nYouTube; performing active speaker verification using a two-stream\nsynchronization Convolutional Neural Network (CNN), and confirming\nthe identity of the speaker using CNN based facial recognition. We\nuse this pipeline to curate  VoxCeleb which contains hundreds of thousands\nof &#8216;real world&#8217; utterances for over 1,000 celebrities.<br\nstyle=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  Our second contribution is to apply and compare various state\nof the art speaker identification techniques on our dataset to establish\nbaseline performance. We show that a CNN based architecture obtains\nthe best performance for both identification and verification.\n"
      ],
      "doi": "10.21437/Interspeech.2017-950"
    },
    "jones17b_interspeech": {
      "authors": [
        [
          "Karen",
          "Jones"
        ],
        [
          "Stephanie",
          "Strassel"
        ],
        [
          "Kevin",
          "Walker"
        ],
        [
          "David",
          "Graff"
        ],
        [
          "Jonathan",
          "Wright"
        ]
      ],
      "title": "Call My Net Corpus: A Multilingual Corpus for Evaluation of Speaker Recognition Technology",
      "original": "1521",
      "page_count": 4,
      "order": 545,
      "p1": "2621",
      "pn": "2624",
      "abstract": [
        "The Call My Net 2015 (CMN15) corpus presents a new resource for Speaker\nRecognition Evaluation and related technologies. The corpus includes\nconversational telephone speech recordings for a total of 220 speakers\nspanning 4 languages: Tagalog, Cantonese, Mandarin and Cebuano. The\ncorpus includes 10 calls per speaker made under a variety of noise\nconditions. Calls were manually audited for language, speaker identity\nand overall quality. The resulting data has been used in the NIST 2016\nSRE Evaluation and will be published in the Linguistic Data Consortium\ncatalog. We describe the goals of the CMN15 corpus, including details\nof the collection protocol and auditing procedure and discussion of\nthe unique properties of this corpus compared to prior NIST SRE evaluation\ncorpora.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1521"
    },
    "weiss17_interspeech": {
      "authors": [
        [
          "Ron J.",
          "Weiss"
        ],
        [
          "Jan",
          "Chorowski"
        ],
        [
          "Navdeep",
          "Jaitly"
        ],
        [
          "Yonghui",
          "Wu"
        ],
        [
          "Zhifeng",
          "Chen"
        ]
      ],
      "title": "Sequence-to-Sequence Models Can Directly Translate Foreign Speech",
      "original": "0503",
      "page_count": 5,
      "order": 546,
      "p1": "2625",
      "pn": "2629",
      "abstract": [
        "We present a recurrent encoder-decoder deep neural network architecture\nthat directly translates speech in one language into text in another.\nThe model does not explicitly transcribe the speech into text in the\nsource language, nor does it require supervision from the ground truth\nsource language transcription during training. We apply a slightly\nmodified sequence-to-sequence with attention architecture that has\npreviously been used for speech recognition and show that it can be\nrepurposed for this more complex task, illustrating the power of attention-based\nmodels.<br style=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  A single model trained end-to-end obtains state-of-the-art performance\non the Fisher Callhome Spanish-English speech translation task, outperforming\na cascade of independently trained sequence-to-sequence speech recognition\nand machine translation models by 1.8 BLEU points on the Fisher test\nset. In addition, we find that making use of the training data in both\nlanguages by multi-task training sequence-to-sequence speech translation\nand recognition models with a shared encoder network can improve performance\nby a further 1.4 BLEU points.\n"
      ],
      "doi": "10.21437/Interspeech.2017-503"
    },
    "kano17_interspeech": {
      "authors": [
        [
          "Takatomo",
          "Kano"
        ],
        [
          "Sakriani",
          "Sakti"
        ],
        [
          "Satoshi",
          "Nakamura"
        ]
      ],
      "title": "Structured-Based Curriculum Learning for End-to-End English-Japanese Speech Translation",
      "original": "0944",
      "page_count": 5,
      "order": 547,
      "p1": "2630",
      "pn": "2634",
      "abstract": [
        "Sequence-to-sequence attentional-based neural network architectures\nhave been shown to provide a powerful model for machine translation\nand speech recognition. Recently, several works have attempted to extend\nthe models for end-to-end speech translation task. However, the usefulness\nof these models were only investigated on language pairs with similar\nsyntax and word order (e.g., English-French or English-Spanish). In\nthis work, we focus on end-to-end speech translation tasks on syntactically\ndistant language pairs (e.g., English-Japanese) that require distant\nword reordering. To guide the encoder-decoder attentional model to\nlearn this difficult problem, we propose a structured-based curriculum\nlearning strategy. Unlike conventional curriculum learning that gradually\nemphasizes difficult data examples, we formalize learning strategies\nfrom easier network structures to more difficult network structures.\nHere, we start the training with end-to-end encoder-decoder for speech\nrecognition or text-based machine translation task then gradually move\nto end-to-end speech translation task. The experiment results show\nthat the proposed approach could provide significant improvements in\ncomparison with the one without curriculum learning.\n"
      ],
      "doi": "10.21437/Interspeech.2017-944"
    },
    "ruiz17_interspeech": {
      "authors": [
        [
          "Nicholas",
          "Ruiz"
        ],
        [
          "Mattia Antonino Di",
          "Gangi"
        ],
        [
          "Nicola",
          "Bertoldi"
        ],
        [
          "Marcello",
          "Federico"
        ]
      ],
      "title": "Assessing the Tolerance of Neural Machine Translation Systems Against Speech Recognition Errors",
      "original": "1690",
      "page_count": 5,
      "order": 548,
      "p1": "2635",
      "pn": "2639",
      "abstract": [
        "Machine translation systems are conventionally trained on textual resources\nthat do not model phenomena that occur in spoken language. While the\nevaluation of neural machine translation systems on textual inputs\nis actively researched in the literature, little has been discovered\nabout the complexities of translating spoken language data with neural\nmodels. We introduce and motivate interesting problems one faces when\nconsidering the translation of automatic speech recognition (ASR) outputs\non neural machine translation (NMT) systems. We test the robustness\nof sentence encoding approaches for NMT encoder-decoder modeling, focusing\non word-based over byte-pair encoding. We compare the translation of\nutterances containing ASR errors in state-of-the-art NMT encoder-decoder\nsystems against a strong phrase-based machine translation baseline\nin order to better understand which phenomena present in ASR outputs\nare better represented under the NMT framework than approaches that\nrepresent translation as a linear model.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1690"
    },
    "do17b_interspeech": {
      "authors": [
        [
          "Quoc Truong",
          "Do"
        ],
        [
          "Sakriani",
          "Sakti"
        ],
        [
          "Satoshi",
          "Nakamura"
        ]
      ],
      "title": "Toward Expressive Speech Translation: A Unified Sequence-to-Sequence LSTMs Approach for Translating Words and Emphasis",
      "original": "0896",
      "page_count": 5,
      "order": 549,
      "p1": "2640",
      "pn": "2644",
      "abstract": [
        "Emphasis is an important piece of paralinguistic information that is\nused to express different intentions, attitudes, or convey emotion.\nRecent works have tried to translate emphasis by developing additional\nemphasis estimation and translation components apart from an existing\nspeech-to-speech translation (S2ST) system. Although these approaches\ncan preserve emphasis, they introduce more complexity to the translation\npipeline. The emphasis translation component has to wait for the target\nlanguage sentence and word alignments derived from a machine translation\nsystem, resulting in a significant translation delay. In this paper,\nwe proposed an approach that jointly trains and predicts words and\nemphasis in a unified architecture based on sequence-to-sequence models.\nThe proposed model not only speeds up the translation pipeline but\nalso allows us to perform joint training. Our experiments on the emphasis\nand word translation tasks showed that we could achieve comparable\nperformance for both tasks compared with previous approaches while\neliminating complex dependencies.\n"
      ],
      "doi": "10.21437/Interspeech.2017-896"
    },
    "cho17_interspeech": {
      "authors": [
        [
          "Eunah",
          "Cho"
        ],
        [
          "Jan",
          "Niehues"
        ],
        [
          "Alex",
          "Waibel"
        ]
      ],
      "title": "NMT-Based Segmentation and Punctuation Insertion for Real-Time Spoken Language Translation",
      "original": "1320",
      "page_count": 5,
      "order": 550,
      "p1": "2645",
      "pn": "2649",
      "abstract": [
        "Insertion of proper segmentation and punctuation into an ASR transcript\nis crucial not only for the performance of subsequent applications\nbut also for the readability of the text. In a simultaneous spoken\nlanguage translation system, the segmentation model has to fulfill\nreal-time constraints and minimize latency as well.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  In this paper, we\nshow the successful integration of an attentional encoder-decoder-based\nsegmentation and punctuation insertion model into a real-time spoken\nlanguage translation system. The proposed technique can be easily integrated\ninto the real-time framework and improve the punctuation performance\non reference transcripts as well as on ASR outputs. Compared to the\nconventional language model and prosody-based model, our experiments\non end-to-end spoken language translation show that translation performance\nis improved by 1.3 BLEU points by adopting the NMT-based punctuation\nmodel, maintaining low-latency.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1320"
    },
    "drude17_interspeech": {
      "authors": [
        [
          "Lukas",
          "Drude"
        ],
        [
          "Reinhold",
          "Haeb-Umbach"
        ]
      ],
      "title": "Tight Integration of Spatial and Spectral Features for BSS with Deep Clustering Embeddings",
      "original": "0187",
      "page_count": 5,
      "order": 551,
      "p1": "2650",
      "pn": "2654",
      "abstract": [
        "Recent advances in discriminatively trained mask estimation networks\nto extract a single source utilizing beamforming techniques demonstrate,\nthat the integration of statistical models and deep neural networks\n(DNNs) are a promising approach for robust automatic speech recognition\n(ASR) applications. In this contribution we demonstrate how discriminatively\ntrained embeddings on spectral features can be tightly integrated into\nstatistical model-based source separation to separate and transcribe\noverlapping speech. Good generalization to unseen spatial configurations\nis achieved by estimating a statistical model at test time, while still\nleveraging discriminative training of deep clustering embeddings on\na separate training set. We formulate an expectation maximization (EM)\nalgorithm which jointly estimates a model for deep clustering embeddings\nand complex-valued spatial observations in the short time Fourier transform\n(STFT) domain at test time. Extensive simulations confirm, that the\nintegrated model outperforms (a) a deep clustering model with a subsequent\nbeamforming step and (b) an EM-based model with a beamforming step\nalone in terms of signal to distortion ratio (SDR) and perceptually\nmotivated metric (PESQ) gains. ASR results on a reverberated dataset\nfurther show, that the aforementioned gains translate to reduced word\nerror rates (WERs) even in reverberant environments.\n"
      ],
      "doi": "10.21437/Interspeech.2017-187"
    },
    "zmolikova17_interspeech": {
      "authors": [
        [
          "Kate\u0159ina",
          "\u017dmol\u00edkov\u00e1"
        ],
        [
          "Marc",
          "Delcroix"
        ],
        [
          "Keisuke",
          "Kinoshita"
        ],
        [
          "Takuya",
          "Higuchi"
        ],
        [
          "Atsunori",
          "Ogawa"
        ],
        [
          "Tomohiro",
          "Nakatani"
        ]
      ],
      "title": "Speaker-Aware Neural Network Based Beamformer for Speaker Extraction in Speech Mixtures",
      "original": "0667",
      "page_count": 5,
      "order": 552,
      "p1": "2655",
      "pn": "2659",
      "abstract": [
        "In this work, we address the problem of extracting one target speaker\nfrom a multichannel mixture of speech. We use a neural network to estimate\nmasks to extract the target speaker and derive beamformer filters using\nthese masks, in a similar way as the recently proposed approach for\nextraction of speech in presence of noise. To overcome the permutation\nambiguity of neural network mask estimation, which arises in presence\nof multiple speakers, we propose to inform the neural network about\nthe target speaker so that it learns to follow the speaker characteristics\nthrough the utterance. We investigate and compare different methods\nof passing the speaker information to the network such as making one\nlayer of the network dependent on speaker characteristics. Experiments\non mixture of two speakers demonstrate that the proposed scheme can\ntrack and extract a target speaker for both closed and open speaker\nset cases.\n"
      ],
      "doi": "10.21437/Interspeech.2017-667"
    },
    "pfeifenberger17_interspeech": {
      "authors": [
        [
          "Lukas",
          "Pfeifenberger"
        ],
        [
          "Matthias",
          "Z\u00f6hrer"
        ],
        [
          "Franz",
          "Pernkopf"
        ]
      ],
      "title": "Eigenvector-Based Speech Mask Estimation Using Logistic Regression",
      "original": "1186",
      "page_count": 5,
      "order": 553,
      "p1": "2660",
      "pn": "2664",
      "abstract": [
        "In this paper, we use a logistic regression to learn a speech mask\nfrom the dominant eigenvector of the  Power Spectral Density (PSD)\nmatrix of a multi-channel speech signal corrupted by ambient noise.\nWe employ this speech mask to construct the  Generalized Eigenvalue\n(GEV) beamformer and a Wiener postfilter. Further, we extend the beamformer\nto compensate for speech distortions. We do not make any assumptions\nabout the array geometry or the characteristics of the speech and noise\nsources. Those parameters are learned from training data. Our assumptions\nare that the speaker may move slowly in the near-field of the array,\nand that the noise is in the far-field. We compare our speech enhancement\nsystem against recent contributions using the CHiME4 corpus. We show\nthat our approach yields superior results, both in terms of perceptual\nspeech quality and speech mask estimation error.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1186"
    },
    "wood17b_interspeech": {
      "authors": [
        [
          "Sean U.N.",
          "Wood"
        ],
        [
          "Jean",
          "Rouat"
        ]
      ],
      "title": "Real-Time Speech Enhancement with GCC-NMF",
      "original": "1458",
      "page_count": 5,
      "order": 554,
      "p1": "2665",
      "pn": "2669",
      "abstract": [
        "We develop an online variant of the GCC-NMF blind speech enhancement\nalgorithm and study its performance on two-channel mixtures of speech\nand real-world noise from the SiSEC separation challenge. While GCC-NMF\nperforms enhancement independently for each time frame, the NMF dictionary,\nits activation coefficients, and the target TDOA are derived using\nthe entire mixture signal, thus precluding its use online. Pre-learning\nthe NMF dictionary using the CHiME dataset and inferring its activation\ncoefficients online yields similar overall PEASS scores to the mixture-learned\nmethod, thus generalizing to new speakers, acoustic environments, and\nnoise conditions. Surprisingly, if we forgo coefficient inference altogether,\nthis approach outperforms both the mixture-learned method and most\nalgorithms from the SiSEC challenge to date. Furthermore, the trade-off\nbetween interference suppression and target fidelity may be controlled\nonline by adjusting the target TDOA window width. Finally, integrating\nonline target localization with max-pooled GCC-PHAT yields only somewhat\ndecreased performance compared to offline localization. We test a real-time\nimplementation of the online GCC-NMF blind speech enhancement system\non a variety of hardware platforms, with performance made to degrade\nsmoothly with decreasing computational power using smaller pre-learned\ndictionaries.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1458"
    },
    "ji17b_interspeech": {
      "authors": [
        [
          "Youna",
          "Ji"
        ],
        [
          "Jun",
          "Byun"
        ],
        [
          "Young-cheol",
          "Park"
        ]
      ],
      "title": "Coherence-Based Dual-Channel Noise Reduction Algorithm in a Complex Noisy Environment",
      "original": "1464",
      "page_count": 5,
      "order": 555,
      "p1": "2670",
      "pn": "2674",
      "abstract": [
        "In this paper, a coherence-based noise reduction algorithm is proposed\nfor a dual-channel speech enhancement system operating in a complex\nnoise environment. The spatial coherence between two omnidirectional\nmicrophones is one of the crucial information for the dual-channel\nspeech enhancement system. In this paper, we introduce a new model\nof coherence function for the complex noise environment in which a\ntarget speech coexists with a coherent interference and diffuse noise\naround. From the coherence model, three numerical methods of computing\nthe normalized signal to interference plus diffuse noise ratio (SINR),\nwhich is related to the Wiener filter gain, are derived. Objective\nparameters measured from the enhanced speech demonstrate superior performance\nof the proposed algorithm in terms of speech quality and intelligibility,\nover the conventional coherence-based noise reduction algorithm.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1464"
    },
    "zhang17j_interspeech": {
      "authors": [
        [
          "Yang",
          "Zhang"
        ],
        [
          "Dinei",
          "Flor\u00eancio"
        ],
        [
          "Mark",
          "Hasegawa-Johnson"
        ]
      ],
      "title": "Glottal Model Based Speech Beamforming for ad-hoc Microphone Arrays",
      "original": "1659",
      "page_count": 5,
      "order": 556,
      "p1": "2675",
      "pn": "2679",
      "abstract": [
        "We are interested in the task of speech beamforming in conference room\nmeetings, with microphones built in the electronic devices brought\nand casually placed by meeting participants. This task is challenging\nbecause of the inaccuracy in position and interference calibration\ndue to random microphone configuration, variance of microphone quality,\nreverberation etc. As a result, not many beamforming algorithms perform\nbetter than simply picking the closest microphone in this setting.\nWe propose a beamforming called Glottal Residual Assisted Beamforming\n(GRAB). It does not rely on any position or interference calibration.\nInstead, it incorporates a source-filter speech model and minimizes\nthe energy that cannot be accounted for by the model. Objective and\nsubjective evaluations on both simulation and real-world data show\nthat GRAB is able to suppress noise effectively while keeping the speech\nnatural and dry. Further analyses reveal that GRAB can distinguish\ncontaminated or reverberant channels and take appropriate action accordingly.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1659"
    },
    "liu17d_interspeech": {
      "authors": [
        [
          "Yuanyuan",
          "Liu"
        ],
        [
          "Tan",
          "Lee"
        ],
        [
          "P.C.",
          "Ching"
        ],
        [
          "Thomas K.T.",
          "Law"
        ],
        [
          "Kathy Y.S.",
          "Lee"
        ]
      ],
      "title": "Acoustic Assessment of Disordered Voice with Continuous Speech Based on Utterance-Level ASR Posterior Features",
      "original": "0280",
      "page_count": 5,
      "order": 557,
      "p1": "2680",
      "pn": "2684",
      "abstract": [
        "Most previous studies on acoustic assessment of disordered voice were\nfocused on extracting perturbation features from isolated vowels produced\nwith steady-state phonation. Natural speech, however, is considered\nto be more preferable in the aspects of flexibility, effectiveness\nand reliability for clinical practice. This paper presents an investigation\non applying automatic speech recognition (ASR) technology to disordered\nvoice assessment of Cantonese speakers. A DNN-based ASR system is trained\nusing phonetically-rich continuous utterances from normal speakers.\nIt was found that frame-level phone posteriors obtained from the ASR\nsystem are strongly correlated with the severity level of voice disorder.\nPhone posteriors in utterances with severe disorder exhibit significantly\nlarger variation than those with mild disorder. A set of utterance-level\nposterior features are computed to quantify such variation for pattern\nrecognition purpose. An SVM based classifier is used to classify an\ninput utterance into the categories of mild, moderate and severe disorder.\nThe two-class classification accuracy for mild and severe disorders\nis 90.3%, and significant confusion between mild and moderate disorders\nis observed. For some of the subjects with severe voice disorder, the\nclassification results are highly inconsistent among individual utterances.\nFurthermore, short utterances tend to have more classification errors.\n"
      ],
      "doi": "10.21437/Interspeech.2017-280"
    },
    "ylmaz17c_interspeech": {
      "authors": [
        [
          "Emre",
          "Y\u0131lmaz"
        ],
        [
          "Mario",
          "Ganzeboom"
        ],
        [
          "Catia",
          "Cucchiarini"
        ],
        [
          "Helmer",
          "Strik"
        ]
      ],
      "title": "Multi-Stage DNN Training for Automatic Recognition of Dysarthric Speech",
      "original": "0303",
      "page_count": 5,
      "order": 558,
      "p1": "2685",
      "pn": "2689",
      "abstract": [
        "Incorporating automatic speech recognition (ASR) in individualized\nspeech training applications is becoming more viable thanks to the\nimproved generalization capabilities of neural network-based acoustic\nmodels. The main problem in developing applications for dysarthric\nspeech is the relative in-domain data scarcity. Collecting representative\namounts of dysarthric speech data is difficult due to rigorous ethical\nand medical permission requirements, problems in accessing patients\nwho are generally vulnerable and often subject to altering health conditions\nand, last but not least, the high variability in speech resulting from\ndifferent pathological conditions. Developing such applications is\neven more challenging for languages which in general have fewer resources,\nfewer speakers and, consequently, also fewer patients than English,\nas in the case of a mid-sized language like Dutch. In this paper, we\ninvestigate a multi-stage deep neural network (DNN) training scheme\naimed at obtaining better modeling of dysarthric speech by using only\na small amount of in-domain training data. The results show that the\nsystem employing the proposed training scheme considerably improves\nthe recognition of Dutch dysarthric speech compared to a baseline system\nwith single-stage training only on a large amount of normal speech\nor a small amount of in-domain data.\n"
      ],
      "doi": "10.21437/Interspeech.2017-303"
    },
    "smith17_interspeech": {
      "authors": [
        [
          "Daniel",
          "Smith"
        ],
        [
          "Alex",
          "Sneddon"
        ],
        [
          "Lauren",
          "Ward"
        ],
        [
          "Andreas",
          "Duenser"
        ],
        [
          "Jill",
          "Freyne"
        ],
        [
          "David",
          "Silvera-Tawil"
        ],
        [
          "Angela",
          "Morgan"
        ]
      ],
      "title": "Improving Child Speech Disorder Assessment by Incorporating Out-of-Domain Adult Speech",
      "original": "0455",
      "page_count": 5,
      "order": 559,
      "p1": "2690",
      "pn": "2694",
      "abstract": [
        "This paper describes the continued development of a system to provide\nearly assessment of speech development issues in children and better\ntriaging to professional services. Whilst corpora of children&#8217;s\nspeech are increasingly available, recognition of disordered children&#8217;s\nspeech is still a data-scarce task. Transfer learning methods have\nbeen shown to be effective at leveraging out-of-domain data to improve\nASR performance in similar data-scarce applications. This paper combines\ntransfer learning, with previously developed methods for constrained\ndecoding based on expert speech pathology knowledge and knowledge of\nthe target text. Results of this study show that transfer learning\nwith out-of-domain adult speech can improve phoneme recognition for\ndisordered children&#8217;s speech. Specifically, a Deep Neural Network\n(DNN) trained on adult speech and fine-tuned on a corpus of disordered\nchildren&#8217;s speech reduced the phoneme error rate (PER) of a DNN\ntrained on a children&#8217;s corpus from 16.3% to 14.2%. Furthermore,\nthis fine-tuned DNN also improved the performance of a Hierarchal Neural\nNetwork based acoustic model previously used by the system with a PER\nof 19.3%. We close with a discussion of our planned future developments\nof the system.\n"
      ],
      "doi": "10.21437/Interspeech.2017-455"
    },
    "joy17b_interspeech": {
      "authors": [
        [
          "Neethu Mariam",
          "Joy"
        ],
        [
          "S.",
          "Umesh"
        ],
        [
          "Basil",
          "Abraham"
        ]
      ],
      "title": "On Improving Acoustic Models for TORGO Dysarthric Speech Database",
      "original": "0878",
      "page_count": 5,
      "order": 560,
      "p1": "2695",
      "pn": "2699",
      "abstract": [
        "Assistive technologies based on speech have been shown to improve the\nquality of life of people affected with dysarthria, a motor speech\ndisorder. Multiple ways to improve Gaussian mixture model-hidden Markov\nmodel (GMM-HMM) and deep neural network (DNN) based automatic speech\nrecognition (ASR) systems for TORGO database for dysarthric speech\nare explored in this paper. Past attempts in developing ASR systems\nfor TORGO database were limited to training just monophone models and\ndoing speaker adaptation over them. Although a recent work attempted\ntraining triphone and neural network models, parameters like the number\nof context dependent states, dimensionality of the principal component\nfeatures etc were not properly tuned. This paper develops speaker-specific\nASR models for each dysarthric speaker in TORGO database by tuning\nparameters of GMM-HMM model, number of layers and hidden nodes in DNN.\nEmploying dropout scheme and sequence discriminative training in DNN\nalso gave significant gains. Speaker adapted features like feature-space\nmaximum likelihood linear regression (FMLLR) are used to pass the speaker\ninformation to DNNs. To the best of our knowledge, this paper presents\nthe best recognition accuracies for TORGO database till date.\n"
      ],
      "doi": "10.21437/Interspeech.2017-878"
    },
    "simantiraki17_interspeech": {
      "authors": [
        [
          "Olympia",
          "Simantiraki"
        ],
        [
          "Paulos",
          "Charonyktakis"
        ],
        [
          "Anastasia",
          "Pampouchidou"
        ],
        [
          "Manolis",
          "Tsiknakis"
        ],
        [
          "Martin",
          "Cooke"
        ]
      ],
      "title": "Glottal Source Features for Automatic Speech-Based Depression Assessment",
      "original": "1251",
      "page_count": 5,
      "order": 561,
      "p1": "2700",
      "pn": "2704",
      "abstract": [
        "Depression is one of the most prominent mental disorders, with an increasing\nrate that makes it the fourth cause of disability worldwide. The field\nof automated depression assessment has emerged to aid clinicians in\nthe form of a decision support system. Such a system could assist as\na pre-screening tool, or even for monitoring high risk populations.\nRelated work most commonly involves multimodal approaches, typically\ncombining audio and visual signals to identify depression presence\nand/or severity. The current study explores categorical assessment\nof depression using audio features alone. Specifically, since depression-related\nvocal characteristics impact the glottal source signal, we examine\nPhase Distortion Deviation which has previously been applied to the\nrecognition of voice qualities such as hoarseness, breathiness and\ncreakiness, some of which are thought to be features of depressed speech.\nThe proposed method uses as features DCT-coefficients of the Phase\nDistortion Deviation for each frequency band. An automated machine\nlearning tool, Just Add Data, is used to classify speech samples. The\nmethod is evaluated on a benchmark dataset (AVEC2014), in two conditions:\nread-speech and spontaneous-speech. Our findings indicate that Phase\nDistortion Deviation is a promising audio-only feature for automated\ndetection and assessment of depressed speech.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1251"
    },
    "sadeghian17_interspeech": {
      "authors": [
        [
          "Roozbeh",
          "Sadeghian"
        ],
        [
          "J. David",
          "Schaffer"
        ],
        [
          "Stephen A.",
          "Zahorian"
        ]
      ],
      "title": "Speech Processing Approach for Diagnosing Dementia in an Early Stage",
      "original": "1712",
      "page_count": 5,
      "order": 562,
      "p1": "2705",
      "pn": "2709",
      "abstract": [
        "The clinical diagnosis of Alzheimer&#8217;s disease and other dementias\nis very challenging, especially in the early stages. Our hypothesis\nis that any disease that affects particular brain regions involved\nin speech production and processing will also leave detectable finger\nprints in the speech. Computerized analysis of speech signals and computational\nlinguistics have progressed to the point where an automatic speech\nanalysis system is a promising approach for a low-cost non-invasive\ndiagnostic tool for early detection of Alzheimer&#8217;s disease.<br\nstyle=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  We present empirical evidence that strong discrimination between\nsubjects with a diagnosis of probable Alzheimer&#8217;s versus matched\nnormal controls can be achieved with a combination of acoustic features\nfrom speech, linguistic features extracted from an automatically determined\ntranscription of the speech including punctuation, and results of a\nmini mental state exam (MMSE). We also show that discrimination is\nnearly as strong even if the MMSE is not used, which implies that a\nfully automated system is feasible. Since commercial automatic speech\nrecognition (ASR) tools were unable to provide transcripts for about\nhalf of our speech samples, a customized ASR system was developed.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1712"
    },
    "biadsy17_interspeech": {
      "authors": [
        [
          "Fadi",
          "Biadsy"
        ],
        [
          "Mohammadreza",
          "Ghodsi"
        ],
        [
          "Diamantino",
          "Caseiro"
        ]
      ],
      "title": "Effectively Building Tera Scale MaxEnt Language Models Incorporating Non-Linguistic Signals",
      "original": "1203",
      "page_count": 5,
      "order": 563,
      "p1": "2710",
      "pn": "2714",
      "abstract": [
        "Maximum Entropy (MaxEnt) language models are powerful models that can\nincorporate linguistic and non-linguistic contextual signals in a unified\nframework with a convex loss. MaxEnt models also have the advantage\nof scaling to large model and training data sizes We present the following\ntwo contributions to MaxEnt training: (1) By leveraging smaller amounts\nof transcribed data, we demonstrate that a MaxEnt LM trained on various\ntypes of corpora can be easily adapted to better match the test distribution\nof Automatic Speech Recognition (ASR); (2) A novel  adaptive-training\napproach that efficiently models multiple types of non-linguistic features\nin a universal model. We evaluate the impact of these approaches on\nGoogle&#8217;s state-of-the-art ASR for the task of voice-search transcription\nand dictation. Training 10B parameter models utilizing a corpus of\nup to 1T words, we show large reductions in word error rate from adaptation\nacross multiple languages. Also, human evaluations show significant\nimprovements on a wide range of domains from using non-linguistic features.\nFor example, adapting to geographical domains (e.g., US States and\ncities) affects about 4% of test utterances, with 2:1 win to loss ratio.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1203"
    },
    "deena17_interspeech": {
      "authors": [
        [
          "Salil",
          "Deena"
        ],
        [
          "Raymond W.M.",
          "Ng"
        ],
        [
          "Pranava",
          "Madhyastha"
        ],
        [
          "Lucia",
          "Specia"
        ],
        [
          "Thomas",
          "Hain"
        ]
      ],
      "title": "Semi-Supervised Adaptation of RNNLMs by Fine-Tuning with Domain-Specific Auxiliary Features",
      "original": "1598",
      "page_count": 5,
      "order": 564,
      "p1": "2715",
      "pn": "2719",
      "abstract": [
        "Recurrent neural network language models (RNNLMs) can be augmented\nwith auxiliary features, which can provide an extra modality on top\nof the words. It has been found that RNNLMs perform best when trained\non a large corpus of generic text and then fine-tuned on text corresponding\nto the sub-domain for which it is to be applied. However, in many cases\nthe auxiliary features are available for the sub-domain text but not\nfor the generic text. In such cases, semi-supervised techniques can\nbe used to infer such features for the generic text data such that\nthe RNNLM can be trained and then fine-tuned on the available in-domain\ndata with corresponding auxiliary features.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  In this paper, several\nnovel approaches are investigated for dealing with the semi-supervised\nadaptation of RNNLMs with auxiliary features as input. These approaches\ninclude: using zero features during training to mask the weights of\nthe feature sub-network; adding the feature sub-network only at the\ntime of fine-tuning; deriving the features using a parametric model\nand; back-propagating to infer the features on the generic text. These\napproaches are investigated and results are reported both in terms\nof PPL and WER on a multi-genre broadcast ASR task.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1598"
    },
    "singh17_interspeech": {
      "authors": [
        [
          "Mittul",
          "Singh"
        ],
        [
          "Youssef",
          "Oualil"
        ],
        [
          "Dietrich",
          "Klakow"
        ]
      ],
      "title": "Approximated and Domain-Adapted LSTM Language Models for First-Pass Decoding in Speech Recognition",
      "original": "0147",
      "page_count": 5,
      "order": 565,
      "p1": "2720",
      "pn": "2724",
      "abstract": [
        "Traditionally, short-range Language Models (LMs) like the conventional\nn-gram models have been used for language model adaptation. Recent\nwork has improved performance for such tasks using adapted long-span\nmodels like Recurrent Neural Network LMs (RNNLMs). With the first pass\nperformed using a large background n-gram LM, the adapted RNNLMs are\nmostly used to rescore lattices or N-best lists, as a second step in\nthe decoding process. Ideally, these adapted RNNLMs should be applied\nfor first-pass decoding. Thus, we introduce two ways of applying adapted\nlong-short-term-memory (LSTM) based RNNLMs for first-pass decoding.\nUsing available techniques to convert LSTMs to approximated versions\nfor first-pass decoding, we compare approximated LSTMs adapted in a\nFast Marginal Adaptation framework (FMA) and an approximated version\nof architecture-based-adaptation of LSTM. On a conversational speech\nrecognition task, these differently approximated and adapted LSTMs\ncombined with a trigram LM outperform other adapted and unadapted LMs.\nHere, the architecture-adapted LSTM combination obtains a 35.9% word\nerror rate (WER) and is outperformed by FMA-based LSTM combination\nobtaining the overall lowest WER of 34.4%.\n"
      ],
      "doi": "10.21437/Interspeech.2017-147"
    },
    "chelba17_interspeech": {
      "authors": [
        [
          "Ciprian",
          "Chelba"
        ],
        [
          "Diamantino",
          "Caseiro"
        ],
        [
          "Fadi",
          "Biadsy"
        ]
      ],
      "title": "Sparse Non-Negative Matrix Language Modeling: Maximum Entropy Flexibility on the Cheap",
      "original": "0493",
      "page_count": 5,
      "order": 566,
      "p1": "2725",
      "pn": "2729",
      "abstract": [
        "We present a new method for estimating the sparse non-negative model\n(SNM) by using a small amount of held-out data and the multinomial\nloss that is natural for language modeling; we validate it experimentally\nagainst the previous estimation method which uses leave-one-out on\ntraining data and a binary loss function and show that it performs\nequally well. Being able to train on held-out data is very important\nin practical situations where training data is mismatched from held-out/test\ndata. We find that fairly small amounts of held-out data (on the order\nof 30&#8211;70 thousand words) are sufficient for training the adjustment\nmodel, which is the only model component estimated using gradient descent;\nthe bulk of model parameters are relative frequencies counted on training\ndata.<br style=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  A second contribution is a comparison between SNM and the related\nclass of Maximum Entropy language models. While much cheaper computationally,\nwe show that SNM achieves slightly better perplexity results for the\nsame feature set and same speech recognition accuracy on voice search\nand short message dictation.\n"
      ],
      "doi": "10.21437/Interspeech.2017-493"
    },
    "kumar17c_interspeech": {
      "authors": [
        [
          "Manoj",
          "Kumar"
        ],
        [
          "Daniel",
          "Bone"
        ],
        [
          "Kelly",
          "McWilliams"
        ],
        [
          "Shanna",
          "Williams"
        ],
        [
          "Thomas D.",
          "Lyon"
        ],
        [
          "Shrikanth S.",
          "Narayanan"
        ]
      ],
      "title": "Multi-Scale Context Adaptation for Improving Child Automatic Speech Recognition in Child-Adult Spoken Interactions",
      "original": "0426",
      "page_count": 5,
      "order": 567,
      "p1": "2730",
      "pn": "2734",
      "abstract": [
        "The mutual influence of participant behavior in a dyadic interaction\nhas been studied for different modalities and quantified by computational\nmodels. In this paper, we consider the task of automatic recognition\nfor children&#8217;s speech, in the context of child-adult spoken interactions\nduring interviews of children suspected to have been maltreated. Our\nlong-term goal is to provide insights within this immensely important,\nsensitive domain through large-scale lexical and paralinguistic analysis.\nWe demonstrate improvement in child speech recognition accuracy by\nconditioning on both the domain and the interlocutor&#8217;s (adult)\nspeech. Specifically, we use information from the automatic speech\nrecognizer outputs of the adult&#8217;s speech, for which we have more\nreliable estimates, to modify the recognition system of child&#8217;s\nspeech in an unsupervised manner. By learning first at session level,\nand then at the utterance level, we demonstrate an absolute improvement\nof upto 28% WER and 55% perplexity over the baseline results. We also\nreport results of a parallel human speech recognition (HSR) experiment\nwhere annotators are asked to transcribe child&#8217;s speech under\ntwo conditions: with and without contextual speech information. Demonstrated\nASR improvements and the HSR experiment illustrate the importance of\ncontext in aiding child speech recognition, whether by humans or computers.\n"
      ],
      "doi": "10.21437/Interspeech.2017-426"
    },
    "zhu17_interspeech": {
      "authors": [
        [
          "Weiwu",
          "Zhu"
        ]
      ],
      "title": "Using Knowledge Graph and Search Query Click Logs in Statistical Language Model for Speech Recognition",
      "original": "1790",
      "page_count": 4,
      "order": 568,
      "p1": "2735",
      "pn": "2738",
      "abstract": [
        "This paper demonstrates how Knowledge Graph (KG) and Search Query Click\nLogs (SQCL) can be leveraged in statistical language models to improve\nnamed entity recognition for online speech recognition systems. Due\nto the missing in the training data, some named entities may be recognized\nas other common words that have the similar pronunciation. KG and SQCL\ncover comprehensive and fresh named entities and queries that can be\nused to mitigate the wrong recognition. First, all the entities located\nin the same area in KG are clustered together, and the queries that\ncontain the entity names are selected from SQCL as the training data\nof a geographical statistical language model for each entity cluster.\nThese geographical language models make the unseen named entities less\nlikely to occur during the model training, and can be dynamically switched\naccording to the user location in the recognition phase. Second, if\nany named entities are identified in the previous utterances within\na conversational dialog, the probability of the n-best word sequence\npaths that contain their related entities will be increased for the\ncurrent utterance by utilizing the entity relationships from KG and\nSQCL. This way can leverage the long-term contexts within the dialog.\nExperiments for the proposed approach on voice queries from a spoken\ndialog system yielded a 12.5% relative perplexity reduction in the\nlanguage model measurement, and a 1.1% absolute word error rate reduction\nin the speech recognition measurement.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1790"
    },
    "dimitriadis17_interspeech": {
      "authors": [
        [
          "Dimitrios",
          "Dimitriadis"
        ],
        [
          "Petr",
          "Fousek"
        ]
      ],
      "title": "Developing On-Line Speaker Diarization System",
      "original": "0166",
      "page_count": 5,
      "order": 569,
      "p1": "2739",
      "pn": "2743",
      "abstract": [
        "In this paper we describe the process of converting a research prototype\nsystem for Speaker Diarization into a fully deployed product running\nin real time and with low latency. The deployment is a part of the\nIBM Cloud Speech-to-Text (STT) Service. First, the prototype system\nis described and the requirements for the on-line, deployable system\nare introduced. Then we describe the technical approaches we took to\nsatisfy these requirements and discuss some of the challenges we have\nfaced. In particular, we present novel ideas for speeding up the system\nby using Automatic Speech Recognition (ASR) transcripts as an input\nto diarization, we introduce a concept of active window to keep the\ncomputational complexity linear, we improve the speaker model using\na new speaker-clustering algorithm, we automatically keep track of\nthe number of active speakers and we enable the users to set an operating\npoint on a continuous scale between low latency and optimal accuracy.\nThe deployed system has been tuned on real-life data reaching average\nSpeaker Error Rates around 3% and improving over the prototype system\nby about 10% relative.\n"
      ],
      "doi": "10.21437/Interspeech.2017-166"
    },
    "seshadri17_interspeech": {
      "authors": [
        [
          "Shreyas",
          "Seshadri"
        ],
        [
          "Ulpu",
          "Remes"
        ],
        [
          "Okko",
          "R\u00e4s\u00e4nen"
        ]
      ],
      "title": "Comparison of Non-Parametric Bayesian Mixture Models for Syllable Clustering and Zero-Resource Speech Processing",
      "original": "0339",
      "page_count": 5,
      "order": 570,
      "p1": "2744",
      "pn": "2748",
      "abstract": [
        "Zero-resource speech processing (ZS) systems aim to learn structural\nrepresentations of speech without access to labeled data. A starting\npoint for these systems is the extraction of syllable tokens utilizing\nthe rhythmic structure of a speech signal. Several recent ZS systems\nhave therefore focused on clustering such syllable tokens into linguistically\nmeaningful units. These systems have so far used heuristically set\nnumber of clusters, which can, however, be highly dataset dependent\nand cannot be optimized in actual unsupervised settings. This paper\nfocuses on improving the flexibility of ZS systems using Bayesian non-parametric\n(BNP) mixture models that are capable of simultaneously learning the\ncluster models as well as their number based on the properties of the\ndataset. We also compare different model design choices, namely priors\nover the weights and the cluster component models, as the impact of\nthese choices is rarely reported in the previous studies. Experiments\nare conducted using conversational speech from several languages. The\nmodels are first evaluated in a separate syllable clustering task and\nthen as a part of a full ZS system in order to examine the potential\nof BNP methods and illuminate the relative importance of different\nmodel design choices.\n"
      ],
      "doi": "10.21437/Interspeech.2017-339"
    },
    "proenca17b_interspeech": {
      "authors": [
        [
          "Jorge",
          "Proen\u00e7a"
        ],
        [
          "Carla",
          "Lopes"
        ],
        [
          "Michael",
          "Tjalve"
        ],
        [
          "Andreas",
          "Stolcke"
        ],
        [
          "Sara",
          "Candeias"
        ],
        [
          "Fernando",
          "Perdig\u00e3o"
        ]
      ],
      "title": "Automatic Evaluation of Children Reading Aloud on Sentences and Pseudowords",
      "original": "1541",
      "page_count": 5,
      "order": 571,
      "p1": "2749",
      "pn": "2753",
      "abstract": [
        "Reading aloud performance in children is typically assessed by teachers\non an individual basis, manually marking reading time and incorrectly\nread words. A computational tool that assists with recording reading\ntasks, automatically analyzing them and providing performance metrics\ncould be a significant help. Towards that goal, this work presents\nan approach to automatically predicting the overall reading aloud ability\nof primary school children (6&#8211;10 years old), based on the reading\nof sentences and pseudowords. The opinions of primary school teachers\nwere gathered as ground truth of performance, who provided 0&#8211;5\nscores closely related to the expectations at the end of each grade.\nTo predict these scores automatically, features based on reading speed\nand number of disfluencies were extracted, after an automatic disfluency\ndetection. Various regression models were trained, with Gaussian process\nregression giving best results for automatic features. Feature selection\nfrom both sentence and pseudoword reading tasks gave the closest predictions,\nwith a correlation of 0.944. Compared to the use of manual annotation\nwith the best correlation being 0.952, automatic annotation was only\n0.8% worse. Furthermore, the error rate of predicted scores relative\nto ground truth was found to be smaller than the deviation of evaluators&#8217;\nopinion per child.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1541"
    },
    "yoon17_interspeech": {
      "authors": [
        [
          "Su-Youn",
          "Yoon"
        ],
        [
          "Chong Min",
          "Lee"
        ],
        [
          "Ikkyu",
          "Choi"
        ],
        [
          "Xinhao",
          "Wang"
        ],
        [
          "Matthew",
          "Mulholland"
        ],
        [
          "Keelan",
          "Evanini"
        ]
      ],
      "title": "Off-Topic Spoken Response Detection with Word Embeddings",
      "original": "0388",
      "page_count": 5,
      "order": 572,
      "p1": "2754",
      "pn": "2758",
      "abstract": [
        "In this study, we developed an automated off-topic response detection\nsystem as a supplementary module for an automated proficiency scoring\nsystem for non-native English speakers&#8217; spontaneous speech. Given\na spoken response, the system first generates an automated transcription\nusing an ASR system trained on non-native speech, and then generates\na set of features to assess similarity to the question. In contrast\nto previous studies which required a large set of training responses\nfor each question, the proposed system only requires the question text,\nthus increasing the practical impact of the system, since new questions\ncan be added to a test dynamically. However, questions are typically\nshort and the traditional approach based on exact word matching does\nnot perform well. In order to address this issue, a set of features\nbased on neural embeddings and a convolutional neural network (CNN)\nwere used. A system based on the combination of all features achieved\nan accuracy of 87% on a balanced dataset, which was substantially higher\nthan the accuracy of a baseline system using question-based vector\nspace models (49%). Additionally, this system almost reached the accuracy\nof vector space based model using a large set of responses to test\nquestions (93%).\n"
      ],
      "doi": "10.21437/Interspeech.2017-388"
    },
    "li17k_interspeech": {
      "authors": [
        [
          "Wei",
          "Li"
        ],
        [
          "Nancy F.",
          "Chen"
        ],
        [
          "Sabato Marco",
          "Siniscalchi"
        ],
        [
          "Chin-Hui",
          "Lee"
        ]
      ],
      "title": "Improving Mispronunciation Detection for Non-Native Learners with Multisource Information and LSTM-Based Deep Models",
      "original": "0464",
      "page_count": 5,
      "order": 573,
      "p1": "2759",
      "pn": "2763",
      "abstract": [
        "In this paper, we utilize manner and place of articulation features\nand deep neural network models (DNNs) with long short-term memory (LSTM)\nto improve the detection performance of phonetic mispronunciations\nproduced by second language learners. First, we show that speech attribute\nscores are complementary to conventional phone scores, so they can\nbe concatenated as features to improve a baseline system based only\non phone information. Next, pronunciation representation, usually calculated\nby frame-level averaging in a DNN, is now learned by LSTM, which directly\nuses sequential context information to embed a sequence of pronunciation\nscores into a pronunciation vector to improve the performance of subsequent\nmispronunciation detectors. Finally, when both proposed techniques\nare incorporated into the baseline phone-based GOP (goodness of pronunciation)\nclassifier system trained on the same data, the integrated system reduces\nthe false acceptance rate (FAR) and false rejection rate (FRR) by 37.90%\nand 38.44% (relative), respectively, from the baseline system.\n"
      ],
      "doi": "10.21437/Interspeech.2017-464"
    },
    "tsujimura17_interspeech": {
      "authors": [
        [
          "Shoko",
          "Tsujimura"
        ],
        [
          "Kazumasa",
          "Yamamoto"
        ],
        [
          "Seiichi",
          "Nakagawa"
        ]
      ],
      "title": "Automatic Explanation Spot Estimation Method Targeted at Text and Figures in Lecture Slides",
      "original": "0750",
      "page_count": 5,
      "order": 574,
      "p1": "2764",
      "pn": "2768",
      "abstract": [
        "Because of the spread of the Internet in recent years, e-learning,\nwhich is a form of learning through the Internet, has been used in\nschool education. Many lecture videos delivered at The Open University\nof Japan show lecturers and lecture slides alternately. In such video\nstyle, it is hard to understand where on the slide the lecturer is\nexplaining. In this paper, we examined methods to automatically estimate\nspots where the lecturer explains on the slide using lecture speech\nand slide data. This technology is expected to help learners to study\nthe lectures. For itemized text slides, using DTW with word embedding\nbased distance, we obtained higher estimation accuracy than a previous\nwork. For slides containing figures, we estimated explanation spots\nusing image classification results and text in the charts. In addition,\nwe modified the lecture browsing system to indicate estimation results\non slides, and investigated the usefulness of indicating explanation\nspots by subjective evaluation with the system.\n"
      ],
      "doi": "10.21437/Interspeech.2017-750"
    },
    "kim17g_interspeech": {
      "authors": [
        [
          "Myungjong",
          "Kim"
        ],
        [
          "Beiming",
          "Cao"
        ],
        [
          "Ted",
          "Mau"
        ],
        [
          "Jun",
          "Wang"
        ]
      ],
      "title": "Multiview Representation Learning via Deep CCA for Silent Speech Recognition",
      "original": "0952",
      "page_count": 5,
      "order": 575,
      "p1": "2769",
      "pn": "2773",
      "abstract": [
        "Silent speech recognition (SSR) converts non-audio information such\nas articulatory (tongue and lip) movements to text. Articulatory movements\ngenerally have less information than acoustic features for speech recognition,\nand therefore, the performance of SSR may be limited. Multiview representation\nlearning, which can learn better representations by analyzing multiple\ninformation sources simultaneously, has been recently successfully\nused in speech processing and acoustic speech recognition. However,\nit has rarely been used in SSR. In this paper, we investigate SSR based\non multiview representation learning via canonical correlation analysis\n(CCA). When both acoustic and articulatory data are available during\ntraining, it is possible to effectively learn a representation of articulatory\nmovements from the multiview data with CCA. To further represent the\ncomplex structure of the multiview data, we apply deep CCA, where the\nfunctional form of the feature mapping is a deep neural network. This\napproach was evaluated in a speaker-independent SSR task using a data\nset collected from seven English speakers using an electromagnetic\narticulograph (EMA). Experimental results showed the effectiveness\nof the multiview representation learning via deep CCA over the CCA-based\nmultiview approach as well as baseline articulatory movement data on\nGaussian mixture model and deep neural network-based SSR systems.\n"
      ],
      "doi": "10.21437/Interspeech.2017-952"
    },
    "knill17_interspeech": {
      "authors": [
        [
          "K.M.",
          "Knill"
        ],
        [
          "Mark J.F.",
          "Gales"
        ],
        [
          "K.",
          "Kyriakopoulos"
        ],
        [
          "A.",
          "Ragni"
        ],
        [
          "Y.",
          "Wang"
        ]
      ],
      "title": "Use of Graphemic Lexicons for Spoken Language Assessment",
      "original": "0978",
      "page_count": 5,
      "order": 576,
      "p1": "2774",
      "pn": "2778",
      "abstract": [
        "Automatic systems for practice and exams are essential to support the\ngrowing worldwide demand for learning English as an additional language.\nAssessment of spontaneous spoken English is, however, currently limited\nin scope due to the difficulty of achieving sufficient automatic speech\nrecognition (ASR) accuracy. &#8220;Off-the-shelf&#8221; English ASR\nsystems cannot model the exceptionally wide variety of accents, pronunciations\nand recording conditions found in non-native learner data. Limited\ntraining data for different first languages (L1s), across all proficiency\nlevels, often with (at most) crowd-sourced transcriptions, limits the\nperformance of ASR systems trained on non-native English learner speech.\nThis paper investigates whether the effect of one source of error in\nthe system, lexical modelling, can be mitigated by using graphemic\nlexicons in place of phonetic lexicons based on native speaker pronunciations.\nGraphemic-based English ASR is typically worse than phonetic-based\ndue to the irregularity of English spelling-to-pronunciation but here\nlower word error rates are consistently observed with the graphemic\nASR. The effect of using graphemes on automatic assessment is assessed\non different grader feature sets: audio and fluency derived features,\nincluding some phonetic level features; and phone/grapheme distance\nfeatures which capture a measure of pronunciation ability.\n"
      ],
      "doi": "10.21437/Interspeech.2017-978"
    },
    "yi17_interspeech": {
      "authors": [
        [
          "Jiangyan",
          "Yi"
        ],
        [
          "Jianhua",
          "Tao"
        ],
        [
          "Zhengqi",
          "Wen"
        ],
        [
          "Ya",
          "Li"
        ]
      ],
      "title": "Distilling Knowledge from an Ensemble of Models for Punctuation Prediction",
      "original": "1079",
      "page_count": 5,
      "order": 577,
      "p1": "2779",
      "pn": "2783",
      "abstract": [
        "This paper proposes an approach to distill knowledge from an ensemble\nof models to a single deep neural network (DNN) student model for punctuation\nprediction. This approach makes the DNN student model mimic the behavior\nof the ensemble. The ensemble consists of three single models. Kullback-Leibler\n(KL) divergence is used to minimize the difference between the output\ndistribution of the DNN student model and the behavior of the ensemble.\nExperimental results on English IWSLT2011 dataset show that the ensemble\noutperforms the previous state-of-the-art model by up to 4.0% absolute\nin overall F<SUB>1</SUB>-score. The DNN student model also achieves\nup to 13.4% absolute overall F<SUB>1</SUB>-score improvement over the\nconventionally-trained baseline models.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1079"
    },
    "pusateri17_interspeech": {
      "authors": [
        [
          "Ernest",
          "Pusateri"
        ],
        [
          "Bharat Ram",
          "Ambati"
        ],
        [
          "Elizabeth",
          "Brooks"
        ],
        [
          "Ondrej",
          "Platek"
        ],
        [
          "Donald",
          "McAllaster"
        ],
        [
          "Venki",
          "Nagesha"
        ]
      ],
      "title": "A Mostly Data-Driven Approach to Inverse Text Normalization",
      "original": "1274",
      "page_count": 5,
      "order": 578,
      "p1": "2784",
      "pn": "2788",
      "abstract": [
        "For an automatic speech recognition system to produce sensibly formatted,\nreadable output, the spoken-form token sequence produced by the core\nspeech recognizer must be converted to a written-form string. This\nprocess is known as inverse text normalization (ITN). Here we present\na mostly data-driven ITN system that leverages a set of simple rules\nand a few hand-crafted grammars to cast ITN as a labeling problem.\nTo this labeling problem, we apply a compact bi-directional LSTM. We\nshow that the approach performs well using practical amounts of training\ndata.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1274"
    },
    "chen17k_interspeech": {
      "authors": [
        [
          "Wenda",
          "Chen"
        ],
        [
          "Mark",
          "Hasegawa-Johnson"
        ],
        [
          "Nancy F.",
          "Chen"
        ],
        [
          "Boon Pang",
          "Lim"
        ]
      ],
      "title": "Mismatched Crowdsourcing from Multiple Annotator Languages for Recognizing Zero-Resourced Languages: A Nullspace Clustering Approach",
      "original": "1567",
      "page_count": 5,
      "order": 579,
      "p1": "2789",
      "pn": "2793",
      "abstract": [
        "It is extremely challenging to create training labels for building\nacoustic models of zero-resourced languages, in which conventional\nresources required for model training &#8212; lexicons, transcribed\naudio, or in extreme cases even orthographic system or a viable phone\nset design for the language &#8212; are unavailable. Here, language\nmismatched transcripts, in which audio is transcribed in the orthographic\nsystem of a completely different language by possibly non-speakers\nof the target language may play a vital role. Such mismatched transcripts\nhave recently been successfully obtained through crowdsourcing and\nshown to be beneficial to ASR performance. This paper further studies\nthis problem of using mismatched crowdsourced transcripts in a tonal\nlanguage for which we have no standard orthography, and in which we\nmay not even know the phoneme inventory. It proposes methods to project\nthe multilingual mismatched transcriptions of a tonal language to the\ntarget phone segments. The results tested on Cantonese and Singapore\nHokkien have shown that the reconstructed phone sequences&#8217; accuracies\nhave absolute increment of more than 3% from those of previously proposed\nmonolingual probabilistic transcription methods. \n"
      ],
      "doi": "10.21437/Interspeech.2017-1567"
    },
    "gale17_interspeech": {
      "authors": [
        [
          "William",
          "Gale"
        ],
        [
          "Sarangarajan",
          "Parthasarathy"
        ]
      ],
      "title": "Experiments in Character-Level Neural Network Models for Punctuation",
      "original": "1710",
      "page_count": 5,
      "order": 580,
      "p1": "2794",
      "pn": "2798",
      "abstract": [
        "We explore character-level neural network models for inferring punctuation\nfrom text-only input. Punctuation inference is treated as a sequence\ntagging problem where the input is a sequence of un-punctuated characters,\nand the output is a corresponding sequence of punctuation tags. We\nexperiment with six architectures, all of which use a long short-term\nmemory (LSTM) network for sequence modeling. They differ in the way\nthe context and lookahead for a given character is derived: from simple\ncharacter embedding and delayed output to enable lookahead, to complex\nconvolutional neural networks (CNN) to capture context. We demonstrate\nthat the accuracy of proposed character-level models are competitive\nwith the accuracy of a state-of-the-art word-level Conditional Random\nField (CRF) baseline with carefully crafted features.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1710"
    },
    "kaushik17_interspeech": {
      "authors": [
        [
          "Lakshmish",
          "Kaushik"
        ],
        [
          "Abhijeet",
          "Sangwan"
        ],
        [
          "John H.L.",
          "Hansen"
        ]
      ],
      "title": "Multi-Channel Apollo Mission Speech Transcripts Calibration",
      "original": "1778",
      "page_count": 5,
      "order": 581,
      "p1": "2799",
      "pn": "2803",
      "abstract": [
        "NASA&#8217;s Apollo program is a great achievement of mankind in the\n20th century. Previously we had introduced UTD-CRSS Apollo data digitization\ninitiative where we proposed to digitize Apollo mission speech data\n(&#126;100,000 hours) and develop Spoken Language Technology based\nalgorithms to analyze and understand various aspects of conversational\nspeech[1]. A new 30 track analog audio decoder is designed to decode\n30 track Apollo analog tapes and is mounted on to the NASA Soundscriber\nanalog audio decoder (in place of single channel decoder). Using the\nnew decoder all 30 channels of data can be decoded simultaneously thereby\nreducing the digitization time significantly. We have digitized 19,000\nhours of data from Apollo missions (including entire Apollo-11, most\nof Apollo-13, Apollo-1, and Gemini-8 missions). Each audio track corresponds\nto a specific personnel/position in NASA mission control room or astronauts\nin space. Since many of the planned Apollo related spoken language\ntechnology approaches need transcripts we have developed an Apollo\nmission specific custom Deep Neural Networks (DNN) based Automatic\nSpeech Recognition (ASR) system. Apollo specific language models are\ndeveloped. Most audio channels are degraded due to high channel noise,\nsystem noise, attenuated signal bandwidth, transmission noise, cosmic\nnoise, analog tape static noise, noise due to tape aging, etc,. In\nthis paper we propose a novel method to improve the transcript quality\nby using Signal-to-Noise ratio of channels and N-Gram sentence similarity\nmetrics across data channels. The proposed method shows significant\nimprovement in transcript quality of noisy channels. The Word Error\nRate (WER) analysis of transcripts across channels shows significant\nreduction.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1778"
    },
    "mclaren17_interspeech": {
      "authors": [
        [
          "Mitchell",
          "McLaren"
        ],
        [
          "Luciana",
          "Ferrer"
        ],
        [
          "Diego",
          "Castan"
        ],
        [
          "Aaron",
          "Lawson"
        ]
      ],
      "title": "Calibration Approaches for Language Detection",
      "original": "0530",
      "page_count": 5,
      "order": 582,
      "p1": "2804",
      "pn": "2808",
      "abstract": [
        "To date, automatic spoken language detection research has largely been\nbased on a closed-set paradigm, in which the languages to be detected\nare known prior to system application. In actual practice, such systems\nmay face previously unseen languages (out-of-set (OOS) languages) which\nshould be rejected, a common problem that has received limited attention\nfrom the research community. In this paper, we focus on situations\nin which either (1) the system-modeled languages are not observed during\nuse or (2) the test data contains OOS languages that are unseen during\nmodeling or calibration. In these situations, the common multi-class\nobjective function for calibration of language-detection scores is\nproblematic. We describe how the assumptions of multi-class calibration\nare not always fulfilled in a practical sense and explore applying\nglobal and language-dependent binary objective functions to relax system\nconstraints. We contrast the benefits and sensitivities of the calibration\napproaches on practical scenarios by presenting results using both\nLRE09 data and 14 languages from the BABEL dataset. We show that the\nglobal binary approach is less sensitive to the characteristics of\nthe training data and that OOS modeling with individual detectors is\nthe best option when OOS test languages are not known to the system.\n"
      ],
      "doi": "10.21437/Interspeech.2017-530"
    },
    "fernando17_interspeech": {
      "authors": [
        [
          "Sarith",
          "Fernando"
        ],
        [
          "Vidhyasaharan",
          "Sethu"
        ],
        [
          "Eliathamby",
          "Ambikairajah"
        ],
        [
          "Julien",
          "Epps"
        ]
      ],
      "title": "Bidirectional Modelling for Short Duration Language Identification",
      "original": "0286",
      "page_count": 5,
      "order": 583,
      "p1": "2809",
      "pn": "2813",
      "abstract": [
        "Language identification (LID) systems typically employ i-vectors as\nfixed length representations of utterances. However, it may not be\npossible to reliably estimate i-vectors from short utterances, which\nin turn could lead to reduced language identification accuracy. Recently,\nLong Short Term Memory networks (LSTMs) have been shown to better model\nshort utterances in the context of language identification. This paper\nexplores the use of bidirectional LSTMs for language identification\nwith the aim of modelling temporal dependencies between past and future\nframe based features in short utterances. Specifically, an end-to-end\nsystem for short duration language identification employing bidirectional\nLSTM models of utterances is proposed. Evaluations on both NIST 2007\nand 2015 LRE show state-of-the-art performance.\n"
      ],
      "doi": "10.21437/Interspeech.2017-286"
    },
    "shen17b_interspeech": {
      "authors": [
        [
          "Peng",
          "Shen"
        ],
        [
          "Xugang",
          "Lu"
        ],
        [
          "Sheng",
          "Li"
        ],
        [
          "Hisashi",
          "Kawai"
        ]
      ],
      "title": "Conditional Generative Adversarial Nets Classifier for Spoken Language Identification",
      "original": "0553",
      "page_count": 5,
      "order": 584,
      "p1": "2814",
      "pn": "2818",
      "abstract": [
        "The i-vector technique using deep neural network has been successfully\napplied in spoken language identification systems. Neural network modeling\nshowed its effectiveness as both discriminant feature transformation\nand classification in many tasks, in particular with a large training\ndata set. However, on a small data set, neural networks suffer from\nthe overfitting problem which degrades the performance. Many strategies\nhave been investigated and used to improve the regularization for deep\nneural networks, for example, weigh decay, dropout, data augmentation.\nIn this paper, we study and use conditional generative adversarial\nnets as a classifier for the spoken language identification task. Unlike\nthe previous works on GAN for image generation, our purpose is to focus\non improving regularization of the neural network by jointly optimizing\nthe &#8220;Real/Fake&#8221; objective function and the categorical\nobjective function. Compared with dropout and data augmentation methods,\nthe proposed method obtained 29.7% and 31.8% relative improvement on\nNIST 2015 i-vector challenge data set for spoken language identification.\n"
      ],
      "doi": "10.21437/Interspeech.2017-553"
    },
    "miguel17_interspeech": {
      "authors": [
        [
          "Antonio",
          "Miguel"
        ],
        [
          "Jorge",
          "Llombart"
        ],
        [
          "Alfonso",
          "Ortega"
        ],
        [
          "Eduardo",
          "Lleida"
        ]
      ],
      "title": "Tied Hidden Factors in Neural Networks for End-to-End Speaker Recognition",
      "original": "1314",
      "page_count": 5,
      "order": 585,
      "p1": "2819",
      "pn": "2823",
      "abstract": [
        "In this paper we propose a method to model speaker and session variability\nand able to generate likelihood ratios using neural networks in an\nend-to-end phrase dependent speaker verification system. As in Joint\nFactor Analysis, the model uses tied hidden variables to model speaker\nand session variability and a MAP adaptation of some of the parameters\nof the model. In the training procedure our method jointly estimates\nthe network parameters and the values of the speaker and channel hidden\nvariables. This is done in a two-step backpropagation algorithm, first\nthe network weights and factor loading matrices are updated and then\nthe hidden variables, whose gradients are calculated by aggregating\nthe corresponding speaker or session frames, since these hidden variables\nare tied. The last layer of the network is defined as a linear regression\nprobabilistic model whose inputs are the previous layer outputs. This\nchoice has the advantage that it produces likelihoods and additionally\nit can be adapted during the enrolment using MAP without the need of\na gradient optimization. The decisions are made based on the ratio\nof the output likelihoods of two neural network models, speaker adapted\nand universal background model. The method was evaluated on the RSR2015\ndatabase.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1314"
    },
    "yun17_interspeech": {
      "authors": [
        [
          "Sungrack",
          "Yun"
        ],
        [
          "Hye Jin",
          "Jang"
        ],
        [
          "Taesu",
          "Kim"
        ]
      ],
      "title": "Speaker Clustering by Iteratively Finding Discriminative Feature Space and Cluster Labels",
      "original": "0923",
      "page_count": 5,
      "order": 586,
      "p1": "2824",
      "pn": "2828",
      "abstract": [
        "This paper presents a speaker clustering framework by iteratively performing\ntwo stages: a discriminative feature space is obtained given a cluster\nlabel set, and the cluster label set is updated using a clustering\nalgorithm given the feature space. In the iterations of two stages,\nthe cluster labels may be different from the true labels, and thus\nthe obtained feature space based on the labels may be inaccurately\ndiscriminated. However, by iteratively performing above two stages,\nmore accurate cluster labels and more discriminative feature space\ncan be obtained, and finally they are converged. In this research,\nthe linear discriminant analysis is used for discriminating the i-vector\nfeature space, and the variational Bayesian expectation-maximization\non Gaussian mixture model is used for clustering the i-vectors. Our\niterative clustering framework was evaluated using the database of\nkeyword utterances and compared with the recently-published approaches.\nIn all experiments, the results show that our framework outperforms\nthe other approaches and converges in a few iterations.\n"
      ],
      "doi": "10.21437/Interspeech.2017-923"
    },
    "vinals17_interspeech": {
      "authors": [
        [
          "Ignacio",
          "Vi\u00f1als"
        ],
        [
          "Alfonso",
          "Ortega"
        ],
        [
          "Jes\u00fas",
          "Villalba"
        ],
        [
          "Antonio",
          "Miguel"
        ],
        [
          "Eduardo",
          "Lleida"
        ]
      ],
      "title": "Domain Adaptation of PLDA Models in Broadcast Diarization by Means of Unsupervised Speaker Clustering",
      "original": "0084",
      "page_count": 5,
      "order": 587,
      "p1": "2829",
      "pn": "2833",
      "abstract": [
        "This work presents a new strategy to perform diarization dealing with\nhigh variability data, such as multimedia information in broadcast.\nThis variability is highly noticeable among domains (inter-domain variability\namong chapters, shows, genres, etc.). Therefore, each domain requires\nits own specific model to obtain the optimal results. We propose to\nadapt the PLDA models of our diarization system with in-domain unlabeled\ndata. To do it, we estimate pseudo-speaker labels by unsupervised speaker\nclustering. This new method has been included in a PLDA-based diarization\nsystem and evaluated on the Multi-Genre Broadcast 2015 Challenge data.\nGiven an audio, the system computes short-time i-vectors and clusters\nthem using a variational Bayesian PLDA model with hidden labels. The\nproposed method improves 25.41% relative w.r.t. the system without\nPLDA adaptation.\n"
      ],
      "doi": "10.21437/Interspeech.2017-84"
    },
    "india17_interspeech": {
      "authors": [
        [
          "Miquel",
          "India"
        ],
        [
          "Jos\u00e9 A.R.",
          "Fonollosa"
        ],
        [
          "Javier",
          "Hernando"
        ]
      ],
      "title": "LSTM Neural Network-Based Speaker Segmentation Using Acoustic and Language Modelling",
      "original": "0407",
      "page_count": 5,
      "order": 588,
      "p1": "2834",
      "pn": "2838",
      "abstract": [
        "This paper presents a new speaker change detection system based on\nLong Short-Term Memory (LSTM) neural networks using acoustic data and\nlinguistic content. Language modelling is combined with two different\nJoint Factor Analysis (JFA) acoustic approaches: i-vectors and speaker\nfactors. Both of them are compared with a baseline algorithm that uses\ncosine distance to detect speaker turn changes. LSTM neural networks\nwith both linguistic and acoustic features have been able to produce\na robust speaker segmentation. The experimental results show that our\nproposal clearly outperforms the baseline system.\n"
      ],
      "doi": "10.21437/Interspeech.2017-407"
    },
    "gresse17_interspeech": {
      "authors": [
        [
          "Adrien",
          "Gresse"
        ],
        [
          "Mickael",
          "Rouvier"
        ],
        [
          "Richard",
          "Dufour"
        ],
        [
          "Vincent",
          "Labatut"
        ],
        [
          "Jean-Fran\u00e7ois",
          "Bonastre"
        ]
      ],
      "title": "Acoustic Pairing of Original and Dubbed Voices in the Context of Video Game Localization",
      "original": "1311",
      "page_count": 5,
      "order": 589,
      "p1": "2839",
      "pn": "2843",
      "abstract": [
        "The aim of this research work is the development of an automatic voice\nrecommendation system for assisted voice casting. In this article,\nwe propose preliminary work on acoustic pairing of original and dubbed\nvoices. The voice segments are taken from a video game released in\ntwo different languages. The paired voice segments come from different\nlanguages but belong to the same video game character. Our wish is\nto exploit the relationship between a set of paired segments in order\nto model the perceptual aspects of a given character depending on the\ntarget language. We use a state-of-the-art approach in speaker recognition\n( i.e. based on the paradigm i-vector/PLDA). We first evaluate pairs\nof i-vectors using two different acoustic spaces, one for each of the\ntargeted languages. Secondly, we perform a transformation in order\nto project the source-language i-vector into the target language. The\nresults showed that this latest approach is able to improve significantly\nthe accuracy. Finally, we challenge the system ability to model the\nlatent information that holds the video-game character independently\nof the speaker, the linguistic content and the language.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1311"
    },
    "ajili17_interspeech": {
      "authors": [
        [
          "Moez",
          "Ajili"
        ],
        [
          "Jean-Fran\u00e7ois",
          "Bonastre"
        ],
        [
          "Waad Ben",
          "Kheder"
        ],
        [
          "Solange",
          "Rossato"
        ],
        [
          "Juliette",
          "Kahn"
        ]
      ],
      "title": "Homogeneity Measure Impact on Target and Non-Target Trials in Forensic Voice Comparison",
      "original": "0152",
      "page_count": 5,
      "order": 590,
      "p1": "2844",
      "pn": "2848",
      "abstract": [
        "It is common to see mobile recordings being presented as a forensic\ntrace in a court. In such cases, a forensic expert is asked to analyze\nboth suspect and criminal&#8217;s voice samples in order to determine\nthe strength-of-evidence. This process is known as  Forensic Voice\nComparison (FVC). The  Likelihood ratio (LR) framework is commonly\nused by the experts and quite often required by the expert&#8217;s\nassociations &#8220;best practice guides&#8221;. Nevertheless, the\nLR accepts some practical limitations due both to intrinsic aspects\nof its estimation process and the information used during the FVC process.\nThese aspects are embedded in a more general one, the lack of knowledge\non FVC reliability. The question of reliability remains a major challenge,\nparticularly for FVC systems where numerous variation factors like\nduration, noise, linguistic content or&#8230; within-speaker variability\nare not taken into account. Recently, we proposed an information theory-based\ncriterion able to estimate one of these factors, the homogeneity of\ninformation between the two sides of a FVC trial. Thanks to this new\ncriterion, we wish to explore new aspects of homogeneity in this article.\nWe wish to question the impact of homogeneity on reliability separately\non target and non-target trials. The study is performed using FABIOLE,\na publicly available database dedicated to this kind of studies with\na large number of recordings per target speaker. Our experiments report\nlarge differences of homogeneity impact between FVC genuine and impostor\ntrials. These results show clearly the importance of intra-speaker\nvariability effects in FVC reliability estimation. This study confirms\nalso the interest of homogeneity measure for FVC reliability.\n"
      ],
      "doi": "10.21437/Interspeech.2017-152"
    },
    "solewicz17_interspeech": {
      "authors": [
        [
          "Yosef A.",
          "Solewicz"
        ],
        [
          "Michael",
          "Jessen"
        ],
        [
          "David van der",
          "Vloed"
        ]
      ],
      "title": "Null-Hypothesis LLR: A Proposal for Forensic Automatic Speaker Recognition",
      "original": "1023",
      "page_count": 5,
      "order": 591,
      "p1": "2849",
      "pn": "2853",
      "abstract": [
        "A new method named Null-Hypothesis LLR (H<SUB>0</SUB>LLR) is proposed\nfor forensic automatic speaker recognition. The method takes into account\nthe fact that forensically realistic data are difficult to collect\nand that inter-individual variation is generally better represented\nthan intra-individual variation. According to the proposal, intra-individual\nvariation is modeled as a projection from case-customized inter-individual\nvariation. Calibrated log Likelihood Ratios (LLR) that are calculated\non the basis of the H<SUB>0</SUB>LLR method were tested on two corpora\nof forensically-founded telephone interception test sets, German-based\nGFS 2.0 and Dutch-based NFI-FRITS. Five automatic speaker recognition\nsystems were tested based on the scores or the LLRs provided by these\nsystems which form the input to H<SUB>0</SUB>LLR. Speaker-discrimination\nand calibration performance of H<SUB>0</SUB>LLR is comparable to the\nperformance indices of the system-internal LLR calculation methods.\nThis shows that external data and strategies that work with data outside\nthe forensic domain and without case customization are not necessary.\nIt is also shown that H<SUB>0</SUB>LLR leads to a reduction in the\ndiversity of LLR output patterns of different automatic systems. This\nis important for the credibility of the Likelihood Ratio framework\nin forensics, and its application in forensic automatic speaker recognition\nin particular.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1023"
    },
    "liu17e_interspeech": {
      "authors": [
        [
          "Gang",
          "Liu"
        ],
        [
          "Qi",
          "Qian"
        ],
        [
          "Zhibin",
          "Wang"
        ],
        [
          "Qingen",
          "Zhao"
        ],
        [
          "Tianzhou",
          "Wang"
        ],
        [
          "Hao",
          "Li"
        ],
        [
          "Jian",
          "Xue"
        ],
        [
          "Shenghuo",
          "Zhu"
        ],
        [
          "Rong",
          "Jin"
        ],
        [
          "Tuo",
          "Zhao"
        ]
      ],
      "title": "The Opensesame NIST 2016 Speaker Recognition Evaluation System",
      "original": "0997",
      "page_count": 5,
      "order": 592,
      "p1": "2854",
      "pn": "2858",
      "abstract": [
        "Last two decades have witnessed a significant progress in speaker recognition,\nas evidenced by the improving performance in the speaker recognition\nevaluations (SRE) hosted by NIST. Despite the progress, only a few\nresearch is focused on speaker recognition with short duration and\nlanguage mismatch condition, which often leads to poor recognition\nperformance. In NIST SRE2016, these concerns were first systematically\ninvestigated by the speaker recognition community. In this study, we\naddress these challenges from the viewpoint of feature extraction and\nmodeling. In particular, we improve the robustness of features by combining\nGMM and DNN based iVector extraction approaches, and improve the reliability\nof the back-end model by exploiting symmetric SVM that can effectively\nleverage the unlabeled data. Finally, we introduce distance metric\nlearning to improve the generalization capacity of the development\ndata that is usually in limited size. Then a fusion strategy is adopted\nto collectively boost the performance. The effectiveness of the proposed\nscheme for speaker recognition is demonstrated on SRE2016 evaluation\ndata: compared with DNN-iVector PLDA baseline system, our method yields\n25.6% relative improvement in terms of min_Cprimary.\n"
      ],
      "doi": "10.21437/Interspeech.2017-997"
    },
    "kumar17d_interspeech": {
      "authors": [
        [
          "Nagendra",
          "Kumar"
        ],
        [
          "Rohan Kumar",
          "Das"
        ],
        [
          "Sarfaraz",
          "Jelil"
        ],
        [
          "Dhanush",
          "B.K."
        ],
        [
          "H.",
          "Kashyap"
        ],
        [
          "K. Sri Rama",
          "Murty"
        ],
        [
          "Sriram",
          "Ganapathy"
        ],
        [
          "Rohit",
          "Sinha"
        ],
        [
          "S.R. Mahadeva",
          "Prasanna"
        ]
      ],
      "title": "IITG-Indigo System for NIST 2016 SRE Challenge",
      "original": "1307",
      "page_count": 5,
      "order": 593,
      "p1": "2859",
      "pn": "2863",
      "abstract": [
        "This paper describes the speaker verification (SV) system submitted\nto the NIST 2016 speaker recognition evaluation (SRE) challenge by\nIndian Institute of Technology Guwahati (IITG) under the fixed training\ncondition task. Various SV systems are developed following the idea-level\ncollaboration with two other Indian institutions. Unlike the previous\nSREs, this time the focus was on developing SV system using non-target\nlanguage speech data and a small amount unlabeled data from target\nlanguage/ dialects. For addressing these novel challenges, we tried\nexploring the fusion of systems created using different features, data\nconditioning, and classifiers. On NIST 2016 SRE evaluation data, the\npresented fused system resulted in actual detection cost function (\nactDCF) and equal error rate ( EER) of 0.81 and 12.91%, respectively.\nPost-evaluation, we explored a recently proposed pairwise support vector\nmachine classifier and applied adaptive S-norm to the decision scores\nbefore fusion. With these changes, the final system achieves the  actDCF\nand  EER of 0.67 and 11.63%, respectively.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1307"
    },
    "misra17_interspeech": {
      "authors": [
        [
          "Abhinav",
          "Misra"
        ],
        [
          "Shivesh",
          "Ranjan"
        ],
        [
          "John H.L.",
          "Hansen"
        ]
      ],
      "title": "Locally Weighted Linear Discriminant Analysis for Robust Speaker Verification",
      "original": "0581",
      "page_count": 5,
      "order": 594,
      "p1": "2864",
      "pn": "2868",
      "abstract": [
        "Channel compensation is an integral part for any state-of-the-art speaker\nrecognition system. Typically, Linear Discriminant Analysis (LDA) is\nused to suppress directions containing channel information. LDA assumes\na unimodal Gaussian distribution of the speaker samples to maximize\nthe ratio of the between-speaker variance to within-speaker variance.\nHowever, when speaker samples have multi-modal non-Gaussian distributions\ndue to channel or noise distortions, LDA fails to provide optimal performance.\nIn this study, we propose Locally Weighted Linear Discriminant Analysis\n(LWLDA). LWLDA computes the within-speaker scatter in a pairwise manner\nand then scales it by an affinity matrix so as to preserve the within-class\nlocal structure. This is in contrast to another recently proposed non-parametric\ndiscriminant analysis method called NDA. We show that LWLDA not only\nperforms better than NDA but also is computationally much less expensive.\nExperiments are performed using the DARPA Robust Automatic Transcription\nof Speech (RATS) corpus. Results indicate that LWLDA consistently outperforms\nboth LDA and NDA on all trial conditions.\n"
      ],
      "doi": "10.21437/Interspeech.2017-581"
    },
    "shon17b_interspeech": {
      "authors": [
        [
          "Suwon",
          "Shon"
        ],
        [
          "Seongkyu",
          "Mun"
        ],
        [
          "Hanseok",
          "Ko"
        ]
      ],
      "title": "Recursive Whitening Transformation for Speaker Recognition on Language Mismatched Condition",
      "original": "0545",
      "page_count": 5,
      "order": 595,
      "p1": "2869",
      "pn": "2873",
      "abstract": [
        "Recently in speaker recognition, performance degradation due to the\nchannel domain mismatched condition has been actively addressed. However,\nthe mismatches arising from language is yet to be sufficiently addressed.\nThis paper proposes an approach which employs recursive whitening transformation\nto mitigate the language mismatched condition. The proposed method\nis based on the multiple whitening transformation, which is intended\nto remove un-whitened residual components in the dataset associated\nwith i-vector length normalization. The experiments were conducted\non the Speaker Recognition Evaluation 2016 trials of which the task\nis non-English speaker recognition using development dataset consist\nof both a large scale out-of-domain (English) dataset and an extremely\nlow-quantity in-domain (non-English) dataset. For performance comparison,\nwe develop a state-of-the-art system using deep neural network and\nbottleneck feature, which is based on a phonetically aware model. From\nthe experimental results, along with other prior studies, effectiveness\nof the proposed method on language mismatched condition is validated.\n"
      ],
      "doi": "10.21437/Interspeech.2017-545"
    },
    "settle17_interspeech": {
      "authors": [
        [
          "Shane",
          "Settle"
        ],
        [
          "Keith",
          "Levin"
        ],
        [
          "Herman",
          "Kamper"
        ],
        [
          "Karen",
          "Livescu"
        ]
      ],
      "title": "Query-by-Example Search with Discriminative Neural Acoustic Word Embeddings",
      "original": "1592",
      "page_count": 5,
      "order": 596,
      "p1": "2874",
      "pn": "2878",
      "abstract": [
        "Query-by-example search often uses dynamic time warping (DTW) for comparing\nqueries and proposed matching segments. Recent work has shown that\ncomparing speech segments by representing them as fixed-dimensional\nvectors &#8212; acoustic word embeddings &#8212; and measuring their\nvector distance (e.g., cosine distance) can discriminate between words\nmore accurately than DTW-based approaches. We consider an approach\nto query-by-example search that embeds both the query and database\nsegments according to a neural model, followed by nearest-neighbor\nsearch to find the matching segments. Earlier work on embedding-based\nquery-by-example, using template-based acoustic word embeddings, achieved\ncompetitive performance. We find that our embeddings, based on recurrent\nneural networks trained to optimize word discrimination, achieve substantial\nimprovements in performance and run-time efficiency over the previous\napproaches.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1592"
    },
    "kaneko17b_interspeech": {
      "authors": [
        [
          "Daisuke",
          "Kaneko"
        ],
        [
          "Ryota",
          "Konno"
        ],
        [
          "Kazunori",
          "Kojima"
        ],
        [
          "Kazuyo",
          "Tanaka"
        ],
        [
          "Shi-wook",
          "Lee"
        ],
        [
          "Yoshiaki",
          "Itoh"
        ]
      ],
      "title": "Constructing Acoustic Distances Between Subwords and States Obtained from a Deep Neural Network for Spoken Term Detection",
      "original": "0634",
      "page_count": 5,
      "order": 597,
      "p1": "2879",
      "pn": "2883",
      "abstract": [
        "The detection of out-of-vocabulary (OOV) query terms is a crucial problem\nin spoken term detection (STD), because OOV query terms are likely.\nTo enable search of OOV query terms in STD systems, a query subword\nsequence is compared with subword sequences generated using an automatic\nspeech recognizer against spoken documents. When comparing two subword\nsequences, the edit distance is a typical distance between any two\nsubwords. We previously proposed an acoustic distance defined from\nstatistics between states of the hidden Markov model (HMM) and showed\nits effectiveness in STD [4]. This paper proposes an acoustic distance\nbetween subwords and HMM states where the posterior probabilities output\nby a deep neural network are used to improve the STD accuracy for OOV\nquery terms. Experiments are conducted to evaluate the performance\nof the proposed method, using the open test collections for the &#8220;Spoken&amp;Doc&#8221;\ntasks of the NTCIR-9 [13] and NTCIR-10 [14] workshops. The proposed\nmethod shows improvements in mean average precision.\n"
      ],
      "doi": "10.21437/Interspeech.2017-634"
    },
    "khokhlov17_interspeech": {
      "authors": [
        [
          "Yuri",
          "Khokhlov"
        ],
        [
          "Natalia",
          "Tomashenko"
        ],
        [
          "Ivan",
          "Medennikov"
        ],
        [
          "Aleksei",
          "Romanenko"
        ]
      ],
      "title": "Fast and Accurate OOV Decoder on High-Level Features",
      "original": "1367",
      "page_count": 5,
      "order": 598,
      "p1": "2884",
      "pn": "2888",
      "abstract": [
        "This work proposes a novel approach to out-of-vocabulary (OOV) keyword\nsearch (KWS) task. The proposed approach is based on using high-level\nfeatures from an automatic speech recognition (ASR) system, so called\n phoneme posterior based ( PPB) features, for decoding. These features\nare obtained by calculating time-dependent phoneme posterior probabilities\nfrom word lattices, followed by their smoothing. For the PPB features\nwe developed a special novel very fast, simple and efficient OOV decoder.\nExperimental results are presented on the Georgian language from the\nIARPA Babel Program, which was the test language in the OpenKWS 2016\nevaluation campaign. The results show that in terms of maximum term\nweighted value (MTWV) metric and computational speed, for single ASR\nsystems, the proposed approach significantly outperforms the state-of-the-art\napproach based on using in-vocabulary proxies for OOV keywords in the\nindexed database. The comparison of the two OOV KWS approaches on the\nfusion results of the nine different ASR systems demonstrates that\nthe proposed OOV decoder outperforms the proxy-based approach in terms\nof MTWV metric given the comparable processing speed. Other important\nadvantages of the OOV decoder include extremely low memory consumption\nand simplicity of its implementation and parameter optimization.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1367"
    },
    "chen17l_interspeech": {
      "authors": [
        [
          "Ying-Wen",
          "Chen"
        ],
        [
          "Kuan-Yu",
          "Chen"
        ],
        [
          "Hsin-Min",
          "Wang"
        ],
        [
          "Berlin",
          "Chen"
        ]
      ],
      "title": "Exploring the Use of Significant Words Language Modeling for Spoken Document Retrieval",
      "original": "0612",
      "page_count": 5,
      "order": 599,
      "p1": "2889",
      "pn": "2893",
      "abstract": [
        "Owing to the rapid global access to tremendous amounts of multimedia\nassociated with speech information on the Internet, spoken document\nretrieval (SDR) has become an emerging application recently. Apart\nfrom much effort devoted to developing robust indexing and modeling\ntechniques for spoken documents, a recent line of research targets\nat enriching and reformulating query representations in an attempt\nto enhance retrieval effectiveness. In practice, pseudo-relevance feedback\nis by far the most prevalent paradigm for query reformulation, which\nassumes that top-ranked feedback documents obtained from the initial\nround of retrieval are potentially relevant and can be exploited to\nreformulate the original query. Continuing this line of research, the\npaper presents a novel modeling framework, which aims at discovering\nsignificant words occurring in the feedback documents, to infer an\nenhanced query language model for SDR. Formally, the proposed framework\ntargets at extracting the essential words representing a common notion\nof relevance (i.e., the significant words which occur in almost all\nof the feedback documents), so as to deduce a new query language model\nthat captures these significant words and meanwhile modulates the influence\nof both highly frequent words and too specific words. Experiments conducted\non a benchmark SDR task demonstrate the performance merits of our proposed\nframework.\n"
      ],
      "doi": "10.21437/Interspeech.2017-612"
    },
    "tasaki17_interspeech": {
      "authors": [
        [
          "Hiroto",
          "Tasaki"
        ],
        [
          "Tomoyosi",
          "Akiba"
        ]
      ],
      "title": "Incorporating Acoustic Features for Spontaneous Speech Driven Content Retrieval",
      "original": "0893",
      "page_count": 5,
      "order": 600,
      "p1": "2894",
      "pn": "2898",
      "abstract": [
        "A speech-driven information retrieval system is expected to be useful\nfor gathering information with greater ease. In a conventional system,\nusers have to decide on the contents of their utterance before speaking,\nwhich takes quite a long time when their request is complicated. To\novercome that problem, it is required for the retrieval system to handle\na spontaneously spoken query directly. In this work, we propose an\nextension technique of spoken content retrieval (SCR) for effectively\nusing spontaneously spoken queries. Acoustic features of meaningful\nterms in the retrieval may have prominence compared to other terms.\nAlso, those terms will have linguistic specificity. From this assumption,\nwe predict the contribution of terms included in spontaneously spoken\nqueries using acoustic and linguistic features, and incorporate it\nin the query likelihood model (QLM) which is a probabilistic retrieval\nmodel. We verified the effectiveness of the proposed method through\nexperiments. Our proposed method was successful in improving retrieval\nperformance under various conditions.\n"
      ],
      "doi": "10.21437/Interspeech.2017-893"
    },
    "lu17b_interspeech": {
      "authors": [
        [
          "Bo-Ru",
          "Lu"
        ],
        [
          "Frank",
          "Shyu"
        ],
        [
          "Yun-Nung",
          "Chen"
        ],
        [
          "Hung-Yi",
          "Lee"
        ],
        [
          "Lin-Shan",
          "Lee"
        ]
      ],
      "title": "Order-Preserving Abstractive Summarization for Spoken Content Based on Connectionist Temporal Classification",
      "original": "0862",
      "page_count": 5,
      "order": 601,
      "p1": "2899",
      "pn": "2903",
      "abstract": [
        "Connectionist temporal classification (CTC) is a powerful approach\nfor sequence-to-sequence learning, and has been popularly used in speech\nrecognition. The central ideas of CTC include adding a label  &#8220;blank&#8221;\nduring training. With this mechanism, CTC eliminates the need of segment\nalignment, and hence has been applied to various sequence-to-sequence\nlearning problems. In this work, we applied CTC to abstractive summarization\nfor spoken content. The  &#8220;blank&#8221; in this case implies the\ncorresponding input data are less important or noisy; thus it can be\nignored. This approach was shown to outperform the existing methods\nin term of ROUGE scores over Chinese Giga-word and MATBN corpora. This\napproach also has the nice property that the ordering of words or characters\nin the input documents can be better preserved in the generated summaries.\n"
      ],
      "doi": "10.21437/Interspeech.2017-862"
    },
    "tsuchiya17_interspeech": {
      "authors": [
        [
          "Masatoshi",
          "Tsuchiya"
        ],
        [
          "Ryo",
          "Minamiguchi"
        ]
      ],
      "title": "Automatic Alignment Between Classroom Lecture Utterances and Slide Components",
      "original": "1752",
      "page_count": 5,
      "order": 602,
      "p1": "2904",
      "pn": "2908",
      "abstract": [
        "Multimodal alignment between classroom lecture utterances and lecture\nslide components is one of the crucial problems to realize a multimodal\ne-Learning application. This paper proposes the new method for the\nautomatic alignment, and formulates the alignment as the integer linear\nprogramming (ILP) problem to maximize the score function which consists\nof three factors: the similarity score between utterances and slide\ncomponents, the consistency of the explanation order, and the explanation\ncoverage of slide components. The experimental result on the Corpus\nof Japanese classroom Lecture Contents (CJLC) shows that the automatic\nalignment information acquired by the proposed method is effective\nto improve the performance of the automatic extraction of important\nutterances.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1752"
    },
    "lopezotero17_interspeech": {
      "authors": [
        [
          "Paula",
          "Lopez-Otero"
        ],
        [
          "Laura",
          "Docio-Fernandez"
        ],
        [
          "Carmen",
          "Garcia-Mateo"
        ]
      ],
      "title": "Compensating Gender Variability in Query-by-Example Search on Speech Using Voice Conversion",
      "original": "1183",
      "page_count": 5,
      "order": 603,
      "p1": "2909",
      "pn": "2913",
      "abstract": [
        "The huge amount of available spoken documents has raised the need for\ntools to perform automatic searches within large audio databases. These\ncollections usually consist of documents with a great variability regarding\nspeaker, language or recording channel, among others. Reducing this\nvariability would boost the performance of query-by-example search\non speech systems, especially in zero-resource systems that use acoustic\nfeatures for audio representation. Hence, in this work, a technique\nto compensate the variability caused by speaker gender is proposed.\nGiven a data collection composed of documents spoken by both male and\nfemale voices, every time a spoken query has to be searched, an alternative\nversion of the query on its opposite gender is generated using voice\nconversion. After that, the female version of the query is used to\nsearch within documents spoken by females and vice versa. Experimental\nvalidation of the proposed strategy shows an improvement of search\non speech performance caused by the reduction of gender variability.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1183"
    },
    "kumar17e_interspeech": {
      "authors": [
        [
          "Anjishnu",
          "Kumar"
        ],
        [
          "Pavankumar Reddy",
          "Muddireddy"
        ],
        [
          "Markus",
          "Dreyer"
        ],
        [
          "Bj\u00f6rn",
          "Hoffmeister"
        ]
      ],
      "title": "Zero-Shot Learning Across Heterogeneous Overlapping Domains",
      "original": "0516",
      "page_count": 5,
      "order": 604,
      "p1": "2914",
      "pn": "2918",
      "abstract": [
        "We present a zero-shot learning approach for text classification, predicting\nwhich natural language understanding domain can handle a given utterance.\nOur approach can predict domains at runtime that did not exist at training\ntime. We achieve this extensibility by learning to project utterances\nand domains into the same embedding space while generating each domain-specific\nembedding from a set of attributes that characterize the domain. Our\nmodel is a neural network trained via ranking loss. We evaluate the\nperformance of this zero-shot approach on a subset of a virtual assistant&#8217;s\nthird-party domains and show the effectiveness of the technique on\nnew domains not observed during training. We compare to generative\nbaselines and show that our approach requires less storage and performs\nbetter on new domains.\n"
      ],
      "doi": "10.21437/Interspeech.2017-516"
    },
    "tsunoo17_interspeech": {
      "authors": [
        [
          "Emiru",
          "Tsunoo"
        ],
        [
          "Peter",
          "Bell"
        ],
        [
          "Steve",
          "Renals"
        ]
      ],
      "title": "Hierarchical Recurrent Neural Network for Story Segmentation",
      "original": "0392",
      "page_count": 5,
      "order": 605,
      "p1": "2919",
      "pn": "2923",
      "abstract": [
        "A broadcast news stream consists of a number of stories and each story\nconsists of several sentences. We capture this structure using a hierarchical\nmodel based on a word-level Recurrent Neural Network (RNN) sentence\nmodeling layer and a sentence-level bidirectional Long Short-Term Memory\n(LSTM) topic modeling layer. First, the word-level RNN layer extracts\na vector embedding the sentence information from the given transcribed\nlexical tokens of each sentence. These sentence embedding vectors are\nfed into a bidirectional LSTM that models the sentence and topic transitions.\nA topic posterior for each sentence is estimated discriminatively and\na Hidden Markov model (HMM) follows to decode the story sequence and\nidentify story boundaries. Experiments on the topic detection and tracking\n(TDT2) task indicate that the hierarchical RNN topic modeling achieves\nthe best story segmentation performance with a higher F1-measure compared\nto conventional state-of-the-art methods. We also compare variations\nof our model to infer the optimal structure for the story segmentation\ntask.\n"
      ],
      "doi": "10.21437/Interspeech.2017-392"
    },
    "bouchekif17_interspeech": {
      "authors": [
        [
          "Abdessalam",
          "Bouchekif"
        ],
        [
          "Delphine",
          "Charlet"
        ],
        [
          "G\u00e9raldine",
          "Damnati"
        ],
        [
          "Nathalie",
          "Camelin"
        ],
        [
          "Yannick",
          "Est\u00e8ve"
        ]
      ],
      "title": "Evaluating Automatic Topic Segmentation as a Segment Retrieval Task",
      "original": "1231",
      "page_count": 5,
      "order": 606,
      "p1": "2924",
      "pn": "2928",
      "abstract": [
        "Several evaluation metrics have been proposed for topic segmentation.\nMost of them rely on the paradigm that segmentation is mainly a task\nthat detects boundaries, and thus are oriented on boundary detection\nevaluation. Nevertheless, this paradigm is not appropriate to get homogeneous\nchapters, which is one of the major applications of topic segmentation.\nFor instance on Broadcast News, topic segmentation enables users to\nwatch a chapter independently of the others.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  We propose to consider\nsegmentation as a task that detects homogeneous segments, and we propose\nevaluation metrics oriented on segment retrieval. The proposed metrics\nare experimented on various TV shows from different channels. Results\nare analysed and discussed, highlighting their relevance.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1231"
    },
    "bang17_interspeech": {
      "authors": [
        [
          "Jeong-Uk",
          "Bang"
        ],
        [
          "Mu-Yeol",
          "Choi"
        ],
        [
          "Sang-Hun",
          "Kim"
        ],
        [
          "Oh-Wook",
          "Kwon"
        ]
      ],
      "title": "Improving Speech Recognizers by Refining Broadcast Data with Inaccurate Subtitle Timestamps",
      "original": "0650",
      "page_count": 5,
      "order": 607,
      "p1": "2929",
      "pn": "2933",
      "abstract": [
        "This paper proposes an automatic method to refine broadcast data collected\nevery week for efficient acoustic model training. For training acoustic\nmodels, we use only audio signals, subtitle texts, and subtitle timestamps\naccompanied by recorded broadcast programs. However, the subtitle timestamps\nare often inaccurate due to inherent characteristics of closed captioning.\nIn the proposed method, we remove subtitle texts with low subtitle\nquality index, concatenate adjacent subtitle texts into a merged subtitle\ntext, and correct the timestamp of the merged subtitle text by adding\na margin. Then, a speech recognizer is used to obtain a hypothesis\ntext from the speech segment corresponding to the merged subtitle text.\nFinally, the refined speech segments to be used for acoustic model\ntraining, are generated by selecting the subparts of the merged subtitle\ntext that matches the hypothesis text. It is shown that the acoustic\nmodels trained by using refined broadcast data give significantly higher\nspeech recognition accuracy than those trained by using raw broadcast\ndata. Consequently, the proposed method can efficiently refine a large\namount of broadcast data with inaccurate timestamps taking about half\nof the time, compared with the previous approaches.\n"
      ],
      "doi": "10.21437/Interspeech.2017-650"
    },
    "svec17_interspeech": {
      "authors": [
        [
          "Jan",
          "\u0160vec"
        ],
        [
          "Josef V.",
          "Psutka"
        ],
        [
          "Lubo\u0161",
          "\u0160m\u00eddl"
        ],
        [
          "Jan",
          "Trmal"
        ]
      ],
      "title": "A Relevance Score Estimation for Spoken Term Detection Based on RNN-Generated Pronunciation Embeddings",
      "original": "1087",
      "page_count": 5,
      "order": 608,
      "p1": "2934",
      "pn": "2938",
      "abstract": [
        "In this paper, we present a novel method for term score estimation.\nThe method is primarily designed for scoring the out-of-vocabulary\nterms, however it could also estimate scores for in-vocabulary results.\nThe term score is computed as a cosine distance of two pronunciation\nembeddings. The first one is generated from the grapheme representation\nof the searched term, while the second one is computed from the recognized\nphoneme confusion network. The embeddings are generated by specifically\ntrained recurrent neural network built on the idea of Siamese neural\nnetworks. The RNN is trained from recognition results on word- and\nphone-level in an unsupervised fashion without need of any hand-labeled\ndata. The method is evaluated on the MALACH data in two languages,\nEnglish and Czech. The results are compared with two baseline methods\nfor OOV term detection.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1087"
    },
    "gallardo17c_interspeech": {
      "authors": [
        [
          "Laura Fern\u00e1ndez",
          "Gallardo"
        ],
        [
          "Sebastian",
          "M\u00f6ller"
        ],
        [
          "John",
          "Beerends"
        ]
      ],
      "title": "Predicting Automatic Speech Recognition Performance Over Communication Channels from Instrumental Speech Quality and Intelligibility Scores",
      "original": "0036",
      "page_count": 5,
      "order": 609,
      "p1": "2939",
      "pn": "2943",
      "abstract": [
        "The performance of automatic speech recognition based on coded-decoded\nspeech heavily depends on the quality of the transmitted signals, determined\nby channel impairments. This paper examines relationships between speech\nrecognition performance and measurements of speech quality and intelligibility\nover transmission channels. Different to previous studies, the effects\nof super-wideband transmissions are analyzed and compared to those\nof wideband and narrowband channels. Furthermore, intelligibility scores,\ngathered by conducting a listening test based on logatomes, are also\nconsidered for the prediction of automatic speech recognition results.\nThe modern instrumental measurement techniques POLQA and POLQA-based\nintelligibility have been respectively applied to estimate the quality\nand the intelligibility of transmitted speech. Based on our results,\npolynomial models are proposed that permit the prediction of speech\nrecognition accuracy from the subjective and instrumental measures,\ninvolving a number of channel distortions in the three bandwidths.\nThis approach can save the costs of performing automatic speech recognition\nexperiments and can be seen as a first step towards a useful tool for\ncommunication channel designers. \n"
      ],
      "doi": "10.21437/Interspeech.2017-36"
    },
    "botinhao17_interspeech": {
      "authors": [
        [
          "Cassia Valentini",
          "Botinhao"
        ],
        [
          "Junichi",
          "Yamagishi"
        ]
      ],
      "title": "Speech Intelligibility in Cars: The Effect of Speaking Style, Noise and Listener Age",
      "original": "0105",
      "page_count": 5,
      "order": 610,
      "p1": "2944",
      "pn": "2948",
      "abstract": [
        "Intelligibility of speech in noise becomes lower as the listeners age\nincreases, even when no apparent hearing impairment is present. The\nlosses are, however, different depending on the nature of the noise\nand the characteristics of the voice. In this paper we investigate\nthe effect that age, noise type and speaking style have on the intelligibility\nof speech reproduced by car loudspeakers. Using a binaural mannequin\nwe recorded a variety of voices and speaking styles played from the\naudio system of a car while driving in different conditions. We used\nthis material to create a listening test where participants were asked\nto transcribe what they could hear and recruited groups of young and\nolder adults to take part in it. We found that intelligibility scores\nof older participants were lower for the competing speaker and background\nmusic conditions. Results also indicate that clear and Lombard speech\nwas more intelligible than plain speech for both age groups. A mixed\neffect model revealed that the largest effect was the noise condition,\nfollowed by sentence type, speaking style, voice, age group and pure\ntone average.\n"
      ],
      "doi": "10.21437/Interspeech.2017-105"
    },
    "yamamoto17_interspeech": {
      "authors": [
        [
          "Katsuhiko",
          "Yamamoto"
        ],
        [
          "Toshio",
          "Irino"
        ],
        [
          "Toshie",
          "Matsui"
        ],
        [
          "Shoko",
          "Araki"
        ],
        [
          "Keisuke",
          "Kinoshita"
        ],
        [
          "Tomohiro",
          "Nakatani"
        ]
      ],
      "title": "Predicting Speech Intelligibility Using a Gammachirp Envelope Distortion Index Based on the Signal-to-Distortion Ratio",
      "original": "0170",
      "page_count": 5,
      "order": 611,
      "p1": "2949",
      "pn": "2953",
      "abstract": [
        "A new intelligibility prediction measure, called &#8220;Gammachirp\nEnvelope Distortion Index (GEDI)&#8221; is proposed for the evaluation\nof speech enhancement algorithms. This model calculates the signal-to-distortion\nratio (SDR) in envelope responses SDRenv derived from the gammachirp\nfilterbank outputs of clean and enhanced speech, and is an extension\nof the speech based envelope power spectrum model (sEPSM) to improve\nprediction and usability. An evaluation was performed by comparing\nhuman subjective results and model predictions for the speech intelligibility\nof noise-reduced sounds processed by spectral subtraction and a recent\nWiener filtering technique. The proposed GEDI predicted the subjective\nresults of the Wiener filtering better than those predicted by the\noriginal sEPSM and well-known conventional measures, i.e., STOI, CSII,\nand HASPI.\n"
      ],
      "doi": "10.21437/Interspeech.2017-170"
    },
    "chen17m_interspeech": {
      "authors": [
        [
          "Yafan",
          "Chen"
        ],
        [
          "Yong",
          "Xu"
        ],
        [
          "Jun",
          "Yang"
        ]
      ],
      "title": "Intelligibilities of Mandarin Chinese Sentences with Spectral &#8220;Holes&#8221;",
      "original": "0281",
      "page_count": 4,
      "order": 612,
      "p1": "2954",
      "pn": "2957",
      "abstract": [
        "The speech intelligibility of Mandarin Chinese sentences of various\nspectral regions, regarding band-stop conditions (one or two &#8220;holes&#8221;\nin the spectrum), was investigated through subjective listening tests.\nResults demonstrated significant effects on Mandarin Chinese sentence\nintelligibilities when a single or a pair of spectral holes was introduced.\nMeanwhile, it revealed the importance of the first and second formant\n(F1, F2) frequencies for the comprehension of Mandarin sentences. More\nimportantly, the first formant frequencies played a more primary role\nrather than those of the second formants. Sentence intelligibilities\ndeclined evidently with the lacking of F1 frequencies, but the effect\nbecame small when the spectrum holes covered more than 50% of F1 frequencies,\nand F2 frequencies came into a major play in the intelligibility of\nMandarin sentence.\n"
      ],
      "doi": "10.21437/Interspeech.2017-281"
    },
    "ward17b_interspeech": {
      "authors": [
        [
          "Lauren",
          "Ward"
        ],
        [
          "Ben",
          "Shirley"
        ],
        [
          "Yan",
          "Tang"
        ],
        [
          "William J.",
          "Davies"
        ]
      ],
      "title": "The Effect of Situation-Specific Non-Speech Acoustic Cues on the Intelligibility of Speech in Noise",
      "original": "0500",
      "page_count": 5,
      "order": 613,
      "p1": "2958",
      "pn": "2962",
      "abstract": [
        "In everyday life, speech is often accompanied by a situation-specific\nacoustic cue; a hungry bark as you ask  &#8216;Has anyone fed the dog?&#8217;.\nThis paper investigates the effect such cues have on speech intelligibility\nin noise and evaluates their interaction with the established effect\nof situation-specific semantic cues. This work is motivated by the\nintroduction of new object-based broadcast formats, which have the\npotential to optimise intelligibility by controlling the level of individual\nbroadcast audio elements, at point of service. Results of this study\nshow that situation-specific acoustic cues alone can improve word recognition\nin multi-talker babble by 69.5%, a similar amount to semantic cues.\nThe combination of both semantic and acoustic cues provide further\nimprovement of 106.0% compared with no cues, and 18.7% compared with\nsemantic cues only. Interestingly, whilst increasing subjective intelligibility\nof the target word, the presence of acoustic cues degraded the objective\nintelligibility of the speech-based semantic cues by 47.0% (equivalent\nto reducing the speech level by 4.5 dB). This paper discusses the interactions\nbetween the two types of cues and the implications that these results\nhave for assessing and improving the intelligibility of broadcast speech.\n"
      ],
      "doi": "10.21437/Interspeech.2017-500"
    },
    "andersen17_interspeech": {
      "authors": [
        [
          "Asger Heidemann",
          "Andersen"
        ],
        [
          "Jan Mark de",
          "Haan"
        ],
        [
          "Zheng-Hua",
          "Tan"
        ],
        [
          "Jesper",
          "Jensen"
        ]
      ],
      "title": "On the Use of Band Importance Weighting in the Short-Time Objective Intelligibility Measure",
      "original": "1043",
      "page_count": 5,
      "order": 614,
      "p1": "2963",
      "pn": "2967",
      "abstract": [
        "Speech intelligibility prediction methods are popular tools within\nthe speech processing community for objective evaluation of speech\nintelligibility of e.g. enhanced speech. The Short-Time Objective Intelligibility\n(STOI) measure has become highly used due to its simplicity and high\nprediction accuracy. In this paper we investigate the use of Band Importance\nFunctions (BIFs) in the STOI measure, i.e. of unequally weighting the\ncontribution of speech information from each frequency band. We do\nso by fitting BIFs to several datasets of measured intelligibility,\nand cross evaluating the prediction performance. Our findings indicate\nthat it is possible to improve prediction performance in specific situations.\nHowever, it has not been possible to find BIFs which systematically\nimprove prediction performance beyond the data used for fitting. In\nother words, we find no evidence that the performance of the STOI measure\ncan be improved considerably by extending it with a non-uniform BIF.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1043"
    },
    "spille17_interspeech": {
      "authors": [
        [
          "Constantin",
          "Spille"
        ],
        [
          "Bernd T.",
          "Meyer"
        ]
      ],
      "title": "Listening in the Dips: Comparing Relevant Features for Speech Recognition in Humans and Machines",
      "original": "1168",
      "page_count": 5,
      "order": 615,
      "p1": "2968",
      "pn": "2972",
      "abstract": [
        "In recent years, automatic speech recognition (ASR) systems gradually\ndecreased (and for some tasks closed) the gap between human and automatic\nspeech recognition. However, it is unclear if similar performance implies\nhumans and ASR systems to rely on similar signal cues. In the current\nstudy, ASR and HSR are compared using speech material from a matrix\nsentence test mixed with either a stationary speech-shaped noise (SSN)\nor amplitude-modulated SSN. Recognition performance of HSR and ASR\nis measured in term of the speech recognition threshold (SRT), i.e.,\nthe signal-to-noise ratio with 50% recognition rate and by comparing\npsychometric functions. ASR results are obtained with matched-trained\nDNN-based systems that use FBank features as input and compared to\nresults obtained from eight normal-hearing listeners and two established\nmodels of speech intelligibility. For both maskers, HSR and ASR achieve\nsimilar SRTs with an average deviation of only 0.4 dB. A relevance\npropagation algorithm is applied to identify features relevant for\nASR. The analysis shows that relevant features coincide either with\nspectral peaks of the speech signal or with dips of the noise masker,\nindicating that similar cues are important in HSR and ASR.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1168"
    },
    "sugai17_interspeech": {
      "authors": [
        [
          "Kosuke",
          "Sugai"
        ]
      ],
      "title": "Mental Representation of Japanese Mora; Focusing on its Intrinsic Duration",
      "original": "1720",
      "page_count": 5,
      "order": 616,
      "p1": "2973",
      "pn": "2977",
      "abstract": [
        "Japanese is one of the typical languages in which vowel quantity plays\na key role. In Japanese, a phonological structure called &#8220;mora&#8221;\nis a fundamental rhythmic unit, and theoretically, each mora is supposed\nto have a similar duration (isochronicity). The rhythm of a native\nlanguage has great importance on spoken language processing, including\nsecond language speaking; therefore, in order to get a clear picture\nof bottom-up speech processing, it is crucial to discern how morae\nare mentally represented. Various studies have been conducted to understand\nthe nature of speech processing as a cognitive construct; however,\nmost of this research was conducted with the target stimuli embedded\nin words or carrier sentences to clarify on specifically the relative\nduration of morae. In this study, two reaction-time experiments were\nconducted to investigate whether morae are mentally represented and\nhow long the duration is. The isolated vowels /i/, /e/, /a/, /o/, /u/,\nand syllable /tan/ were chosen as target stimuli, and the first morae\nwere digitally manipulated into 15 durations with 20 ms variations\nin length, from 150 ms to 330 ms. The results revealed the existence\nof a durational threshold between one and two morae, ranging around\n250 ms.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1720"
    },
    "ying17_interspeech": {
      "authors": [
        [
          "Jia",
          "Ying"
        ],
        [
          "Christopher",
          "Carignan"
        ],
        [
          "Jason A.",
          "Shaw"
        ],
        [
          "Michael",
          "Proctor"
        ],
        [
          "Donald",
          "Derrick"
        ],
        [
          "Catherine T.",
          "Best"
        ]
      ],
      "title": "Temporal Dynamics of Lateral Channel Formation in /l/: 3D EMA Data from Australian English",
      "original": "0765",
      "page_count": 5,
      "order": 617,
      "p1": "2978",
      "pn": "2982",
      "abstract": [
        "This study investigated the dynamics of lateral channel formation of\n/l/ in Australian-accented English (AusE) using 3D electromagnetic\narticulography (EMA). Coils were placed on the tongue both mid-sagitally\nand para-sagitally. We varied the vowel preceding /l/ between /&#618;/\nand /&#230;/, e.g.,  filbert vs.  talbot, and the syllable position\nof /l/, e.g., /&#39;t&#230;l.b&#x259;t/ vs. /&#39;t&#230;b.l&#x259;t/.\nThe articulatory analyses of lateral /l/ show that: (1) the mid-sagittal\ndelay (from the tongue tip gesture to the tongue middle/tongue back\ngesture) changes across different syllable positions and vowel contexts;\n(2) the para-sagittal lateralization duration remains the same across\nsyllable positions and vowel contexts; (3) the lateral formation reaches\nits peak earlier than the mid-sagittal gesture peak; (4) the magnitude\nof tongue asymmetrical lateralization is greater than the magnitude\nof tongue curvature in the coronal plane. We discuss these results\nin light of the temporal dynamics of lateral channel formation. We\ninterpret our results as evidence that the formation of the lateral\nchannel is the primary goal of /l/ production.\n"
      ],
      "doi": "10.21437/Interspeech.2017-765"
    },
    "klingler17_interspeech": {
      "authors": [
        [
          "Nicola",
          "Klingler"
        ],
        [
          "Sylvia",
          "Moosm\u00fcller"
        ],
        [
          "Hannes",
          "Scheutz"
        ]
      ],
      "title": "Vowel and Consonant Sequences in three Bavarian Dialects of Austria",
      "original": "1154",
      "page_count": 5,
      "order": 618,
      "p1": "2983",
      "pn": "2987",
      "abstract": [
        "In 1913, Anton Pfalz described a specific relation of vowel and consonant\nsequences for East Middle Bavarian dialects, located in the eastern\nparts of Austria. According to his observations, a long vowel is always\nfollowed by a lenis consonant, and a short vowel is always followed\nby a fortis consonant. Consequently, vowel duration depends on the\nquality of the following consonant. Phonetic examinations of what became\nto be known as the Pfalz&#8217;s Law yielded different results. Specifically,\nthe occurrence of a third category, namely a long vowel followed by\na fortis consonant, seems to be firmly embedded in East Middle Bavarian.<br\nstyle=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  Up till now, phonetic examinations concentrated on CVCV sequences.\nThe analysis of monosyllables and of sequences including consonant\nclusters has been largely neglected so far. In the present contribution,\nwe analyse the impact of initial and final consonant clusters in monosyllables\non the assumed relationship of vowel + consonant sequences. Thus, we\nincluded 18 speakers from three Bavarian varieties. The results show\nthat in all examined varieties long vowel + fortis consonants occur\nand that the cluster complexity has no influence on the absolute vowel\nduration, contradicting Pfalz&#8217;s Law.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1154"
    },
    "issa17_interspeech": {
      "authors": [
        [
          "Amel",
          "Issa"
        ]
      ],
      "title": "Acoustic Cues to the Singleton-Geminate Contrast: The Case of Libyan Arabic Sonorants",
      "original": "1609",
      "page_count": 5,
      "order": 619,
      "p1": "2988",
      "pn": "2992",
      "abstract": [
        "This study examines the acoustic correlates of the singleton and geminate\nconsonants in Tripolitanian Libyan Arabic (TLA). Several measurements\nwere obtained including target segment duration, preceding vowel duration,\nRMS amplitude for the singleton and geminate consonants, and F1, F2\nand F3 for the target consonants. The results confirm that the primary\nacoustic correlate that distinguishes singletons from geminates in\nTLA is duration regardless of sound type with the ratio of C to CC\nbeing 1 to 2.42. The duration of the preceding vowels is suggestive\nand may be considered as another cue to the distinction between them.\nThere was no evidence of differences in RMS amplitude between singleton\nand geminate consonants of any type. F1, F2 and F3 frequencies are\nfound to show similar patterns for singleton and geminate consonants\nfor all sound types, suggesting no gestural effects of gemination in\nTLA. Preliminary results from the phonetic cues investigated here suggest\nthat the acoustic distinction between singleton and geminate consonants\nin TLA is dependent mainly on durational correlates.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1609"
    },
    "brandt17_interspeech": {
      "authors": [
        [
          "Erika",
          "Brandt"
        ],
        [
          "Frank",
          "Zimmerer"
        ],
        [
          "Bistra",
          "Andreeva"
        ],
        [
          "Bernd",
          "M\u00f6bius"
        ]
      ],
      "title": "Mel-Cepstral Distortion of German Vowels in Different Information Density Contexts",
      "original": "0838",
      "page_count": 5,
      "order": 620,
      "p1": "2993",
      "pn": "2997",
      "abstract": [
        "This study investigated whether German vowels differ significantly\nfrom each other in mel-cepstral distortion (MCD) when they stand in\ndifferent information density (ID) contexts. We hypothesized that vowels\nin the same ID contexts are more similar to each other than vowels\nthat stand in different ID conditions. Read speech material from PhonDat2\nof 16 German natives (m = 10, f = 6) was analyzed. Bi-phone and word\nlanguage models were calculated based on DeWaC. To account for additional\nvariability in the data, prosodic factors, as well as corpus-specific\nfrequency values were also entered into the statistical models. Results\nshowed that vowels in different ID conditions were significantly different\nin their MCD values. Unigram word probability and corpus-specific word\nfrequency showed the expected effect on vowel similarity with a hierarchy\nbetween non-contrasting and contrasting conditions. However, these\ndid not form a homogeneous group since there were group-internal significant\ndifferences. The largest distance can be found between vowels produced\nat fast speech rate, and between unstressed vowels.\n"
      ],
      "doi": "10.21437/Interspeech.2017-838"
    },
    "boril17_interspeech": {
      "authors": [
        [
          "Tom\u00e1\u0161",
          "Bo\u0159il"
        ],
        [
          "Pavel",
          "\u0160turm"
        ],
        [
          "Radek",
          "Skarnitzl"
        ],
        [
          "Jan",
          "Vol\u00edn"
        ]
      ],
      "title": "Effect of Formant and F0 Discontinuity on Perceived Vowel Duration: Impacts for Concatenative Speech Synthesis",
      "original": "1161",
      "page_count": 5,
      "order": 621,
      "p1": "2998",
      "pn": "3002",
      "abstract": [
        "Unit selection systems of speech synthesis offer good overall quality,\nbut this may be countervailed by a sporadic and unpredictable occurrence\nof audible artifacts, such as discontinuities in F0 and the spectrum.\nInformal observations suggested that such breaks may have an effect\non perceived vowel duration. This study therefore investigates the\neffect of F0 and formant discontinuities on the perceived duration\nof vowels in Czech synthetic speech. Ten manipulations of F0, F1 and\nF2 were performed on target vowels in short synthesized phrases creating\nabrupt breaks in the contours at the midpoint of the vowels. Listeners\ndecided in a 2AFC task in which phrase the last syllable was longer.\nThe results showed that despite identical duration of the compared\nstimuli, vowels which were manipulated in the second part towards centralized\nvalues (i.e., less peripheral) were systematically considered to be\nshorter by the listeners than stimuli without such discontinuities,\nand vice versa. However, the influence seems to be distinct from an\noverall formant change (without a discontinuity) since a control stimulus\nin which the manipulation was performed within the entire vowel was\nnot perceived as significantly shorter or longer. No effect of F0 manipulations\nwas observed.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1161"
    },
    "tabain17_interspeech": {
      "authors": [
        [
          "Marija",
          "Tabain"
        ],
        [
          "Richard",
          "Beare"
        ]
      ],
      "title": "An Ultrasound Study of Alveolar and Retroflex Consonants in Arrernte: Stressed and Unstressed Syllables",
      "original": "0578",
      "page_count": 5,
      "order": 622,
      "p1": "3003",
      "pn": "3007",
      "abstract": [
        "This study presents ultrasound data from six female speakers of the\nCentral Australian language Arrernte. We focus on the apical stop contrast,\nalveolar /t/ versus retroflex /&#x288;/, which may be considered phonemically\nmarginal. We compare these sounds in stressed and unstressed position.\nConsistent with previous results on this apical contrast, we show that\nthere are minimal differences between the retroflex and the alveolar\nat stop offset; however, at stop onset, the retroflex has a higher\nfront portion of the tongue, and often a more forward posterior portion\nof the tongue. This difference between the alveolar and the retroflex\nis particularly remarked in unstressed prosodic context. This result\nconfirms our previous EPG and EMA results from two of the speakers\nin the present study, which showed that the most prototypical retroflex\nconsonant occurs in the unstressed prosodic position.\n"
      ],
      "doi": "10.21437/Interspeech.2017-578"
    },
    "gobl17_interspeech": {
      "authors": [
        [
          "Christer",
          "Gobl"
        ]
      ],
      "title": "Reshaping the Transformed LF Model: Generating the Glottal Source from the Waveshape Parameter  R<SUB>d</SUB>",
      "original": "1140",
      "page_count": 5,
      "order": 623,
      "p1": "3008",
      "pn": "3012",
      "abstract": [
        "Precise specification of the voice source would facilitate better modelling\nof expressive nuances in human spoken interaction. This paper focuses\non the transformed version of the widely used LF voice source model,\nand proposes an algorithm which makes it possible to use the waveshape\nparameter  R<SUB>d</SUB> to directly control the LF pulse, for more\neffective analysis and synthesis of voice modulations. The  R<SUB>d</SUB>\nparameter, capturing much of the natural covariation between glottal\nparameters, is central to the transformed LF model. It is used to predict\nthe standard  R-parameters, which in turn are used to synthesise the\nLF waveform. However, the LF pulse that results from these predictions\nmay have an  R<SUB>d</SUB> value noticeably different from the specified\n R<SUB>d</SUB>, yielding undesirable artefacts, particularly when the\nmodel is used for detailed analysis and synthesis of non-modal voice.\nA further limitation is that only a subset of possible  R<SUB>d</SUB>\nvalues can be used, to avoid conflicting LF parameter settings. To\neliminate these problems, a new iterative algorithm was developed based\non the Newton-Raphson method for two variables, but modified to include\nconstraints. This ensures that the correct  R<SUB>d</SUB> is always\nobtained and that the algorithm converges for effectively all permissible\n R<SUB>d</SUB> values.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1140"
    },
    "benus17_interspeech": {
      "authors": [
        [
          "\u0160tefan",
          "Be\u0148u\u0161"
        ],
        [
          "Juraj",
          "\u0160imko"
        ],
        [
          "Mona",
          "Lehtinen"
        ]
      ],
      "title": "Kinematic Signatures of Prosody in Lombard Speech",
      "original": "0722",
      "page_count": 5,
      "order": 624,
      "p1": "3013",
      "pn": "3017",
      "abstract": [
        "Human spoken interactions are embodied and situated. Better understanding\nof the restrictions and affordances this embodiment and situational\nawareness has on human speech informs the quest for more natural models\nof human-machine spoken interactions. Here we examine the articulatory\nrealization of communicative meanings expressed through f0 falling\nand rising prosodic boundaries in quiet and noisy conditions. Our data\nshow that 1) the effect of environmental noise is more robustly present\nin the post-boundary than the pre-boundary movements, 2) f0 falls and\nrises are only weakly differentiated in supra-laryngeal articulation\nand differ minimally in their response to noise, 3) individual speakers\nfind different solutions for achieving the communicative goals, and\n4) lip movements are affected by noise and boundary type more than\nthe tongue movements.\n"
      ],
      "doi": "10.21437/Interspeech.2017-722"
    },
    "jochim17b_interspeech": {
      "authors": [
        [
          "Markus",
          "Jochim"
        ],
        [
          "Felicitas",
          "Kleber"
        ]
      ],
      "title": "What do Finnish and Central Bavarian Have in Common? Towards an Acoustically Based Quantity Typology",
      "original": "1285",
      "page_count": 5,
      "order": 625,
      "p1": "3018",
      "pn": "3022",
      "abstract": [
        "The aim of this study was to investigate vowel and consonant quantity\nin Finnish, a typical quantity language, and to set up a reference\ncorpus for a large-scale project studying the diachronic development\nof quantity contrasts in German varieties. Although German is not considered\na quantity language, both tense and lax vowels and voiced and voiceless\nstops are differentiated by vowel and closure duration, respectively.\nThe role of these cues, however, has undergone different diachronic\nchanges in various German varieties. To understand the conditions for\nsuch prosodic changes, the present study investigates the stability\nof quantity relations in an undisputed quantity language. To this end,\nrecordings of words differing in vowel and stop length were obtained\nfrom seven older and six younger L1 Finnish speakers, both in a normal\nand a loud voice. We then measured vowel and stop duration and calculated\nthe vowel to vowel-plus-consonant ratio (a measure known to differentiate\nGerman VC sequences) as well as the geminate-to-singleton ratio. Results\nshow stability across age groups but variability across speech styles.\nMoreover, VC ratios were similar for Finnish and Bavarian German speakers.\nWe discuss our findings against the background of a typology of vowel\nand consonant quantity.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1285"
    },
    "nellore17_interspeech": {
      "authors": [
        [
          "Bhanu Teja",
          "Nellore"
        ],
        [
          "RaviShankar",
          "Prasad"
        ],
        [
          "Sudarsana Reddy",
          "Kadiri"
        ],
        [
          "Suryakanth V.",
          "Gangashetty"
        ],
        [
          "B.",
          "Yegnanarayana"
        ]
      ],
      "title": "Locating Burst Onsets Using SFF Envelope and Phase Information",
      "original": "1027",
      "page_count": 5,
      "order": 626,
      "p1": "3023",
      "pn": "3027",
      "abstract": [
        "Bursts are produced by closing the oral tract at a place of articulation\nand suddenly releasing the acoustic energy built-up behind the closure\nin the tract. The release of energy is an impulse-like behavior, and\nit is followed by a short duration of frication. The burst release\nis short and mostly weak in nature (compared to sonorant sounds), thus\nmaking it difficult to detect its presence in continuous speech. This\npaper attempts to identify burst onsets based on parameters derived\nfrom single frequency filtering (SFF) analysis of speech signals. The\nSFF envelope and phase information give good spectral and temporal\nresolutions of certain features of the signal. Signal reconstructed\nfrom the SFF phase information is shown to be useful in locating burst\nonsets. Entropy and spectral distance parameters from the SFF spectral\nenvelopes are used to refine the burst onset candidate set. The identified\nburst onset locations are compared with manual annotations in the TIMIT\ndatabase.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1027"
    },
    "ding17_interspeech": {
      "authors": [
        [
          "Hongwei",
          "Ding"
        ],
        [
          "Yuanyuan",
          "Zhang"
        ],
        [
          "Hongchao",
          "Liu"
        ],
        [
          "Chu-Ren",
          "Huang"
        ]
      ],
      "title": "A Preliminary Phonetic Investigation of Alphabetic Words in Mandarin Chinese",
      "original": "0876",
      "page_count": 5,
      "order": 627,
      "p1": "3028",
      "pn": "3032",
      "abstract": [
        "Chinese words written partly or fully in roman letters have gained\npopularity in Mandarin Chinese in the last few decades and an appendix\nof such Mandarin Alphabetical Words (MAWs) is included in the authoritative\ndictionary of Standard Mandarin. However, no transcription of MAWs\nhas been provided because it is not clear whether we should keep the\noriginal English pronunciation or transcribe MAWs with Mandarin Pinyin\nsystem. This study aims to investigate the phonetic adaptation of several\nmost frequent MAWs extracted from the corpus. We recruited eight students\nfrom Shanghai, 18 students from Shandong Province, and one student\nfrom the USA. All the subjects were asked to read both 24 Chinese sentences\nembedding the MAWs and all 26 letters of the English alphabet. The\nresults showed that Letters  A O N T were predominantly pronounced\nin Tone 1;  H was often produced with vowel epenthesis after the final\nconsonant; and  B was usually produced in Tone 2 by Shanghai speakers\nand in Tone 4 by Shandong speakers. We conclude that the phonetic adaptation\nof MAWs is influenced by the dialects of the speakers, tones of other\nChinese characters in the MAWs, as well as individual preferences.\n"
      ],
      "doi": "10.21437/Interspeech.2017-876"
    },
    "schatz17_interspeech": {
      "authors": [
        [
          "Thomas",
          "Schatz"
        ],
        [
          "Rory",
          "Turnbull"
        ],
        [
          "Francis",
          "Bach"
        ],
        [
          "Emmanuel",
          "Dupoux"
        ]
      ],
      "title": "A Quantitative Measure of the Impact of Coarticulation on Phone Discriminability",
      "original": "1306",
      "page_count": 5,
      "order": 628,
      "p1": "3033",
      "pn": "3037",
      "abstract": [
        "Acoustic realizations of a given phonetic segment are typically affected\nby coarticulation with the preceding and following phonetic context.\nWhile coarticulation has been extensively studied using descriptive\nphonetic measurements, little is known about the functional impact\nof coarticulation for speech processing. Here, we use DTW-based similarity\ndefined on raw acoustic features and ABX scores to derive a measure\nof the effect of coarticulation on phonetic discriminability. This\nmeasure does not rely on defining segment-specific phonetic cues (formants,\nduration, etc.) and can be applied systematically and automatically\nto any segment in large scale corpora. We illustrate our method using\nstimuli in English and Japanese. We confirm some expected trends, i.e.,\nstronger anticipatory than perseveratory coarticulation and stronger\ncoarticulation for lax/short vowels than for tense/long vowels. We\nthen quantify for the first time the impact of coarticulation across\ndifferent segment types (like vowels and consonants). We discuss how\nour metric and its possible extensions can help addressing current\nchallenges in the systematic study of coarticulation.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1306"
    },
    "lin17b_interspeech": {
      "authors": [
        [
          "Kin Wah Edward",
          "Lin"
        ],
        [
          "Hans",
          "Anderson"
        ],
        [
          "Clifford",
          "So"
        ],
        [
          "Simon",
          "Lui"
        ]
      ],
      "title": "Sinusoidal Partials Tracking for Singing Analysis Using the Heuristic of the Minimal Frequency and Magnitude Difference",
      "original": "0017",
      "page_count": 5,
      "order": 629,
      "p1": "3038",
      "pn": "3042",
      "abstract": [
        "We present a simple heuristic-based Sinusoidal Partial Tracking (PT)\nalgorithm for singing analysis. Our PT algorithm uses a heuristic of\nminimal frequency and magnitude difference to track sinusoidal partials\nin the popular music. An Ideal Binary Mask (IBM), which is created\nfrom the ground truth of the singing voice and the music accompaniment,\nis used to identify the sound source of the partials. In this justifiable\nway, we are able to assess the quality of the partials identified from\nthe PT algorithm. Using the iKala dataset along with the IBM and BSS\nEval 3.0 as a new method of quantifying the partials quality, the comparative\nresults show that our PT algorithm can achieve 0.8746 &#126; 1.7029\ndB GNSDR gain, compared to two common benchmarks, namely the MQ algorithm\nand the SMS-PT algorithm. Thus, our PT algorithm can be considered\nas a new benchmark of the PT algorithm used in singing analysis.\n"
      ],
      "doi": "10.21437/Interspeech.2017-17"
    },
    "phan17_interspeech": {
      "authors": [
        [
          "Huy",
          "Phan"
        ],
        [
          "Philipp",
          "Koch"
        ],
        [
          "Fabrice",
          "Katzberg"
        ],
        [
          "Marco",
          "Maass"
        ],
        [
          "Radoslaw",
          "Mazur"
        ],
        [
          "Alfred",
          "Mertins"
        ]
      ],
      "title": "Audio Scene Classification with Deep Recurrent Neural Networks",
      "original": "0101",
      "page_count": 5,
      "order": 630,
      "p1": "3043",
      "pn": "3047",
      "abstract": [
        "We introduce in this work an efficient approach for audio scene classification\nusing deep recurrent neural networks. An audio scene is firstly transformed\ninto a sequence of high-level label tree embedding feature vectors.\nThe vector sequence is then divided into multiple subsequences on which\na deep GRU-based recurrent neural network is trained for sequence-to-label\nclassification. The global predicted label for the entire sequence\nis finally obtained via aggregation of subsequence classification outputs.\nWe will show that our approach obtains an F1-score of 97.7% on the\nLITIS Rouen dataset, which is the largest dataset publicly available\nfor the task. Compared to the best previously reported result on the\ndataset, our approach is able to reduce the relative classification\nerror by 35.3%.\n"
      ],
      "doi": "10.21437/Interspeech.2017-101"
    },
    "sandsten17_interspeech": {
      "authors": [
        [
          "Maria",
          "Sandsten"
        ],
        [
          "Isabella",
          "Reinhold"
        ],
        [
          "Josefin",
          "Starkhammar"
        ]
      ],
      "title": "Automatic Time-Frequency Analysis of Echolocation Signals Using the Matched Gaussian Multitaper Spectrogram",
      "original": "0119",
      "page_count": 5,
      "order": 631,
      "p1": "3048",
      "pn": "3052",
      "abstract": [
        "High-resolution time-frequency (TF) images of multi-component signals\nare of great interest for visualization, feature extraction and estimation.\nThe matched Gaussian multitaper spectrogram has been proposed to optimally\nresolve multi-component transient functions of Gaussian shape. Hermite\nfunctions are used as multitapers and the weights of the different\nspectrogram functions are optimized. For a fixed number of multitapers,\nthe optimization gives the approximate Wigner distribution of the Gaussian\nshaped function. Increasing the number of multitapers gives a better\napproximation, i.e. a better resolution, but the cross-terms also become\nmore prominent for close TF components. In this submission, we evaluate\na number of different concentration measures to automatically estimate\nthe number of multitapers resulting in the optimal spectrogram for\nTF images of dolphin echolocation signals. The measures are evaluated\nfor different multi-component signals and noise levels and a suggestion\nof an automatic procedure for optimal TF analysis is given. The results\nare compared to other well known TF estimation algorithms and examples\nof real data measurements of echolocation signals from a beluga whale\n( Delphinapterus leucas) are presented.\n"
      ],
      "doi": "10.21437/Interspeech.2017-119"
    },
    "matousek17_interspeech": {
      "authors": [
        [
          "Jind\u0159ich",
          "Matou\u0161ek"
        ],
        [
          "Daniel",
          "Tihelka"
        ]
      ],
      "title": "Classification-Based Detection of Glottal Closure Instants from Speech Signals",
      "original": "0213",
      "page_count": 5,
      "order": 632,
      "p1": "3053",
      "pn": "3057",
      "abstract": [
        "In this paper a classification-based method for the automatic detection\nof glottal closure instants (GCIs) from the speech signal is proposed.\nPeaks in the speech waveforms are taken as candidates for GCI placements.\nA classification framework is used to train a classification model\nand to classify whether or not a peak corresponds to the GCI. We show\nthat the detection accuracy in terms of F1 score is 97.27%. In addition,\ndespite using the speech signal only, the proposed method behaves comparably\nto a method utilizing the glottal signal. The method is also compared\nwith three existing GCI detection algorithms on publicly available\ndatabases.\n"
      ],
      "doi": "10.21437/Interspeech.2017-213"
    },
    "qi17_interspeech": {
      "authors": [
        [
          "Xiaoke",
          "Qi"
        ],
        [
          "Jianhua",
          "Tao"
        ]
      ],
      "title": "A Domain Knowledge-Assisted Nonlinear Model for Head-Related Transfer Functions Based on Bottleneck Deep Neural Network",
      "original": "0222",
      "page_count": 5,
      "order": 633,
      "p1": "3058",
      "pn": "3062",
      "abstract": [
        "Many methods have been proposed for modeling head-related transfer\nfunctions (HRTFs) and yield a good performance level in terms of log-spectral\ndistortion (LSD). However, most of them utilize linear weighting to\nreconstruct or interpolate HRTFs, but not consider the inherent nonlinearity\nrelationship between the basis function and HRTFs. Motivated by this,\na domain knowledge-assisted nonlinear modeling method is proposed based\non bottleneck features. Domain knowledge is used in two aspects. One\nis to generate the input features derived from the solution to sound\nwave propagation equation at the physical level, and the other is to\ndesign the loss function for model training based on the knowledge\nof objective evaluation criterion, i.e., LSD. Furthermore, with utilizing\nthe strong representation ability of the bottleneck features, the nonlinear\nmodel has the potential to achieve a more accurate mapping. The objective\nand subjective experimental results show that the proposed method gains\nless LSD when compared with linear model, and the interpolated HRTFs\ncan generate a similar perception to those of the database.\n"
      ],
      "doi": "10.21437/Interspeech.2017-222"
    },
    "jesus17_interspeech": {
      "authors": [
        [
          "Luis M.T.",
          "Jesus"
        ],
        [
          "Bruno",
          "Rocha"
        ],
        [
          "Andreia",
          "Hall"
        ]
      ],
      "title": "Laryngeal Articulation During Trumpet Performance: An Exploratory Study",
      "original": "0315",
      "page_count": 5,
      "order": 634,
      "p1": "3063",
      "pn": "3067",
      "abstract": [
        "Music teacher&#8217;s reports suggest that the respiratory function\nand laryngeal control in wind instruments, stimulate muscular tension\nof the involved anatomical structure. However, the physiology and acoustics\nof the larynx during trumpet playing has seldom been studied. Therefore,\nthe current paper describes the laryngeal articulation during trumpet\nperformance with biomedical signals and auditory perception. The activation\nof laryngeal musculature of six professional trumpeters when playing\na standard musical passage was analysed using audio, electroglottography\n(EGG), oxygen saturation and heart rate signals. Two University trumpet\nteachers listened to the audio recordings, to evaluate the participants&#8217;\nlaryngeal effort (answers on a 100 mm Visual-Analogue-Scale (VAS):\n0 &#8220;no perceived effort&#8221;; 100 &#8220;extreme effort&#8221;).\nCorrelations between parameters extracted from the EGG data and the\nperception of the audio stimuli by the teachers were explored. Two\nhundred and fifty laryngeal articulations, where raising of the larynx\nand muscular effort were observed, were annotated and analysed. No\ncorrelation between the EGG data and the auditory evaluation was observed.\nHowever, both teachers perceived the laryngeal effort (VAS mean scores\n= 61&#177;14). Our findings show that EGG and auditory perception data\ncan provide new insights into laryngeal articulation and breathing\ncontrol that are key to low muscular tension.\n"
      ],
      "doi": "10.21437/Interspeech.2017-315"
    },
    "guan17_interspeech": {
      "authors": [
        [
          "Jian",
          "Guan"
        ],
        [
          "Xuan",
          "Wang"
        ],
        [
          "Pengming",
          "Feng"
        ],
        [
          "Jing",
          "Dong"
        ],
        [
          "Wenwu",
          "Wang"
        ]
      ],
      "title": "Matrix of Polynomials Model Based Polynomial Dictionary Learning Method for Acoustic Impulse Response Modeling",
      "original": "0395",
      "page_count": 5,
      "order": 635,
      "p1": "3068",
      "pn": "3072",
      "abstract": [
        "We study the problem of dictionary learning for signals that can be\nrepresented as polynomials or polynomial matrices, such as convolutive\nsignals with time delays or acoustic impulse responses. Recently, we\ndeveloped a method for polynomial dictionary learning based on the\nfact that a polynomial matrix can be expressed as a polynomial with\nmatrix coefficients, where the coefficient of the polynomial at each\ntime lag is a scalar matrix. However, a polynomial matrix can be also\nequally represented as a matrix with polynomial elements. In this paper,\nwe develop an alternative method for learning a polynomial dictionary\nand a sparse representation method for polynomial signal reconstruction\nbased on this model. The proposed methods can be used directly to operate\non the polynomial matrix without having to access its coefficients\nmatrices. We demonstrate the performance of the proposed method for\nacoustic impulse response modeling.\n"
      ],
      "doi": "10.21437/Interspeech.2017-395"
    },
    "hyder17_interspeech": {
      "authors": [
        [
          "Rakib",
          "Hyder"
        ],
        [
          "Shabnam",
          "Ghaffarzadegan"
        ],
        [
          "Zhe",
          "Feng"
        ],
        [
          "John H.L.",
          "Hansen"
        ],
        [
          "Taufiq",
          "Hasan"
        ]
      ],
      "title": "Acoustic Scene Classification Using a CNN-SuperVector System Trained with Auditory and Spectrogram Image Features",
      "original": "0431",
      "page_count": 5,
      "order": 636,
      "p1": "3073",
      "pn": "3077",
      "abstract": [
        "Enabling smart devices to infer about the environment using audio signals\nhas been one of the several long-standing challenges in machine listening.\nThe availability of public-domain datasets, e.g., Detection and Classification\nof Acoustic Scenes and Events (DCASE) 2016, enabled researchers to\ncompare various algorithms on standard predefined tasks. Most of the\ncurrent best performing individual acoustic scene classification systems\nutilize different spectrogram image based features with a Convolutional\nNeural Network (CNN) architecture. In this study, we first analyze\nthe performance of a state-of-the-art CNN system for different auditory\nimage and spectrogram features, including Mel-scaled, logarithmically\nscaled, linearly scaled filterbank spectrograms, and Stabilized Auditory\nImage (SAI) features. Next, we benchmark an MFCC based Gaussian Mixture\nModel (GMM) SuperVector (SV) system for acoustic scene classification.\nFinally, we utilize the activations from the final layer of the CNN\nto form a SuperVector (SV) and use them as feature vectors for a Probabilistic\nLinear Discriminative Analysis (PLDA) classifier. Experimental evaluation\non the DCASE 2016 database demonstrates the effectiveness of the proposed\nCNN-SV approach compared to conventional CNNs with a fully connected\nsoftmax output layer. Score fusion of individual systems provides up\nto 7% relative improvement in overall accuracy compared to the CNN\nbaseline system.\n"
      ],
      "doi": "10.21437/Interspeech.2017-431"
    },
    "feng17b_interspeech": {
      "authors": [
        [
          "Xue",
          "Feng"
        ],
        [
          "Brigitte",
          "Richardson"
        ],
        [
          "Scott",
          "Amman"
        ],
        [
          "James",
          "Glass"
        ]
      ],
      "title": "An Environmental Feature Representation for Robust Speech Recognition and for Environment Identification",
      "original": "0485",
      "page_count": 5,
      "order": 637,
      "p1": "3078",
      "pn": "3082",
      "abstract": [
        "In this paper we investigate environment feature representations, which\nwe refer to as e-vectors, that can be used for environment adaption\nin automatic speech recognition (ASR), and for environment identification.\nInspired by the fact that i-vectors in the total variability space\ncapture both speaker and channel environment variability, our proposed\ne-vectors are extracted from i-vectors. Two extraction methods are\nproposed: one is via linear discriminant analysis (LDA) projection,\nand the other via a bottleneck deep neural network (BN-DNN). Our evaluations\nshow that by augmenting DNN-HMM ASR systems with the proposed e-vectors\nfor environment adaptation, ASR performance is significantly improved.\nWe also demonstrate that the proposed e-vector yields promising results\non environment identification.\n"
      ],
      "doi": "10.21437/Interspeech.2017-485"
    },
    "xu17b_interspeech": {
      "authors": [
        [
          "Yong",
          "Xu"
        ],
        [
          "Qiuqiang",
          "Kong"
        ],
        [
          "Qiang",
          "Huang"
        ],
        [
          "Wenwu",
          "Wang"
        ],
        [
          "Mark D.",
          "Plumbley"
        ]
      ],
      "title": "Attention and Localization Based on a Deep Convolutional Recurrent Model for Weakly Supervised Audio Tagging",
      "original": "0486",
      "page_count": 5,
      "order": 638,
      "p1": "3083",
      "pn": "3087",
      "abstract": [
        "Audio tagging aims to perform multi-label classification on audio chunks\nand it is a newly proposed task in the Detection and Classification\nof Acoustic Scenes and Events 2016 (DCASE 2016) challenge. This task\nencourages research efforts to better analyze and understand the content\nof the huge amounts of audio data on the web. The difficulty in audio\ntagging is that it only has a chunk-level label without a frame-level\nlabel. This paper presents a weakly supervised method to not only predict\nthe tags but also indicate the temporal locations of the occurred acoustic\nevents. The attention scheme is found to be effective in identifying\nthe important frames while ignoring the unrelated frames. The proposed\nframework is a deep convolutional recurrent model with two auxiliary\nmodules: an attention module and a localization module. The proposed\nalgorithm was evaluated on the Task 4 of DCASE 2016 challenge. State-of-the-art\nperformance was achieved on the evaluation set with equal error rate\n(EER) reduced from 0.13 to 0.11, compared with the convolutional recurrent\nbaseline system.\n"
      ],
      "doi": "10.21437/Interspeech.2017-486"
    },
    "pan17_interspeech": {
      "authors": [
        [
          "Jing",
          "Pan"
        ],
        [
          "Ming",
          "Li"
        ],
        [
          "Zhanmei",
          "Song"
        ],
        [
          "Xin",
          "Li"
        ],
        [
          "Xiaolin",
          "Liu"
        ],
        [
          "Hua",
          "Yi"
        ],
        [
          "Manman",
          "Zhu"
        ]
      ],
      "title": "An Audio Based Piano Performance Evaluation Method Using Deep Neural Network Based Acoustic Modeling",
      "original": "0866",
      "page_count": 5,
      "order": 639,
      "p1": "3088",
      "pn": "3092",
      "abstract": [
        "In this paper, we propose an annotated piano performance evaluation\ndataset with 185 audio pieces and a method to evaluate the performance\nof piano beginners based on their audio recordings. The proposed framework\nincludes three parts: piano key posterior probability extraction, Dynamic\nTime Warping (DTW) based matching and performance score regression.\nFirst, a deep neural network model is trained to extract 88 dimensional\npiano key features from Constant-Q Transform (CQT) spectrum. The proposed\nacoustic model shows high robustness to the recording environments.\nSecond, we employ the DTW algorithm on the high-level piano key feature\nsequences to align the input with the template. Upon the alignment,\nwe extract multiple global matching features that could reflect the\nsimilarity between the input and the template. Finally, we apply linear\nregression upon these matching features with the scores annotated by\nexpertise in training data to estimate performance scores for test\naudio. Experimental results show that our automatic evaluation method\nachieves 2.64 average absolute score error in score range from 0 to\n100, and 0.73 average correlation coefficient on our in-house collected\nYCU-MPPE-II dataset.\n"
      ],
      "doi": "10.21437/Interspeech.2017-866"
    },
    "chowdhury17_interspeech": {
      "authors": [
        [
          "Shreyan",
          "Chowdhury"
        ],
        [
          "Tanaya",
          "Guha"
        ],
        [
          "Rajesh M.",
          "Hegde"
        ]
      ],
      "title": "Music Tempo Estimation Using Sub-Band Synchrony",
      "original": "1000",
      "page_count": 4,
      "order": 640,
      "p1": "3093",
      "pn": "3096",
      "abstract": [
        "Tempo estimation aims at estimating the pace of a musical piece measured\nin beats per minute. This paper presents a new tempo estimation method\nthat utilizes coherent energy changes across multiple frequency sub-bands\nto identify the onsets. A new measure, called the sub-band synchrony,\nis proposed to detect and quantify the coherent amplitude changes across\nmultiple sub-bands. Given a musical piece, our method first detects\nthe onsets using the sub-band synchrony measure. The periodicity of\nthe resulting onset curve, measured using the autocorrelation function,\nis used to estimate the tempo value. The performance of the sub-band\nsynchrony based tempo estimation method is evaluated on two music databases.\nExperimental results indicate a reasonable improvement in performance\nwhen compared to conventional methods of tempo estimation.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1000"
    },
    "wang17k_interspeech": {
      "authors": [
        [
          "Yun",
          "Wang"
        ],
        [
          "Florian",
          "Metze"
        ]
      ],
      "title": "A Transfer Learning Based Feature Extractor for Polyphonic Sound Event Detection Using Connectionist Temporal Classification",
      "original": "1469",
      "page_count": 5,
      "order": 641,
      "p1": "3097",
      "pn": "3101",
      "abstract": [
        "Sound event detection is the task of detecting the type, onset time,\nand offset time of sound events in audio streams. The mainstream solution\nis recurrent neural networks (RNNs), which usually predict the probability\nof each sound event at every time step. Connectionist temporal classification\n(CTC) has been applied in order to relax the need for exact annotations\nof onset and offset times; the CTC output layer is expected to generate\na peak for each event boundary where the acoustic signal is most salient.\nHowever, with limited training data, the CTC network has been found\nto train slowly, and generalize poorly to new data.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  In this paper, we\ntry to introduce knowledge learned from a much larger corpus into the\nCTC network. We train two variants of SoundNet, a deep convolutional\nnetwork that takes the audio tracks of videos as input, and tries to\napproximate the visual information extracted by an image recognition\nnetwork. A lower part of SoundNet or its variants is then used as a\nfeature extractor for the CTC network to perform sound event detection.\nWe show that the new feature extractor greatly accelerates the convergence\nof the CTC network, and slightly improves the generalization.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1469"
    },
    "mostafa17_interspeech": {
      "authors": [
        [
          "Naziba",
          "Mostafa"
        ],
        [
          "Pascale",
          "Fung"
        ]
      ],
      "title": "A Note Based Query By Humming System Using Convolutional Neural Network",
      "original": "1590",
      "page_count": 5,
      "order": 642,
      "p1": "3102",
      "pn": "3106",
      "abstract": [
        "In this paper, we propose a note-based query by humming (QBH) system\nwith Hidden Markov Model (HMM) and Convolutional Neural Network (CNN)\nsince note-based systems are much more efficient than the traditional\nframe-based systems. A note-based QBH system has two main components:\nhumming transcription and candidate melody retrieval.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  For humming transcription,\nwe are the first to use a hybrid model using HMM and CNN. We use CNN\nfor its ability to learn the features directly from raw audio data\nand for being able to model the locality and variability often present\nin a note and we use HMM for handling the variability across the time-axis.<br\nstyle=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  For candidate melody retrieval, we use locality sensitive hashing\nto narrow down the candidates for retrieval and dynamic time warping\nand earth mover&#8217;s distance for the final ranking of the selected\ncandidates.<br style=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  We show that our HMM-CNN humming transcription system outperforms\nother state of the art humming transcription systems by &#126;2% using\nthe transcription evaluation framework by Molina et. al and our overall\nquery by humming system has a Mean Reciprocal Rank of 0.92 using the\nstandard MIREX dataset, which is higher than other state of the art\nnote-based query by humming systems.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1590"
    },
    "sailor17b_interspeech": {
      "authors": [
        [
          "Hardik B.",
          "Sailor"
        ],
        [
          "Dharmesh M.",
          "Agrawal"
        ],
        [
          "Hemant A.",
          "Patil"
        ]
      ],
      "title": "Unsupervised Filterbank Learning Using Convolutional Restricted Boltzmann Machine for Environmental Sound Classification",
      "original": "0831",
      "page_count": 5,
      "order": 643,
      "p1": "3107",
      "pn": "3111",
      "abstract": [
        "In this paper, we propose to use Convolutional Restricted Boltzmann\nMachine (ConvRBM) to learn filterbank from the raw audio signals. ConvRBM\nis a generative model trained in an unsupervised way to model the audio\nsignals of arbitrary lengths. ConvRBM is trained using annealed dropout\ntechnique and parameters are optimized using Adam optimization. The\nsubband filters of ConvRBM learned from the ESC-50 database resemble\nFourier basis in the mid-frequency range while some of the low-frequency\nsubband filters resemble Gammatone basis. The auditory-like filterbank\nscale is nonlinear w.r.t. the center frequencies of the subband filters\nand follows the standard auditory scales. We have used our proposed\nmodel as a front-end for the Environmental Sound Classification (ESC)\ntask with supervised Convolutional Neural Network (CNN) as a back-end.\nUsing CNN classifier, the ConvRBM filterbank (ConvRBM-BANK) and its\nscore-level fusion with the Mel filterbank energies (FBEs) gave an\nabsolute improvement of 10.65%, and 18.70% in the classification accuracy,\nrespectively, over FBEs alone on the ESC-50 database. This shows that\nthe proposed ConvRBM filterbank also contains highly complementary\ninformation over the Mel filterbank, which is helpful in the ESC task.\n"
      ],
      "doi": "10.21437/Interspeech.2017-831"
    },
    "soni17_interspeech": {
      "authors": [
        [
          "Meet H.",
          "Soni"
        ],
        [
          "Rishabh",
          "Tak"
        ],
        [
          "Hemant A.",
          "Patil"
        ]
      ],
      "title": "Novel Shifted Real Spectrum for Exact Signal Reconstruction",
      "original": "1422",
      "page_count": 5,
      "order": 644,
      "p1": "3112",
      "pn": "3116",
      "abstract": [
        "Retrieval of the phase of a signal is one of the major problems in\nsignal processing. For an exact signal reconstruction, both magnitude,\nand phase spectrum of the signal is required. In many speech-based\napplications, only the magnitude spectrum is processed and the phase\nis ignored, which leads to degradation in the performance. Here, we\npropose a novel technique that enables the reconstruction of the speech\nsignal from magnitude spectrum only. We consider the even-odd part\ndecomposition of a causal sequence and process only on the real part\nof the DTFT of the signal. We propose the shifting of the real part\nof DTFT of the sequence to make it non-negative. By adding a constant\nof sufficient value to the real part of the DTFT, the exact signal\nreconstruction is possible from the magnitude or power spectrum alone.\nMoreover, we have compared our proposed approach with recently proposed\nphase retrieval method from magnitude spectrum of the Causal Delta\nDominant (CDD) signal. We found that the method of phase retrieval\nfrom CDD signal and proposed method are identical under certain approximation.\nHowever, proposed method involves the less computational cost for the\nexact processing of the signal.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1422"
    },
    "weiner17_interspeech": {
      "authors": [
        [
          "Jochen",
          "Weiner"
        ],
        [
          "Mathis",
          "Engelbart"
        ],
        [
          "Tanja",
          "Schultz"
        ]
      ],
      "title": "Manual and Automatic Transcriptions in Dementia Detection from Speech",
      "original": "0112",
      "page_count": 5,
      "order": 645,
      "p1": "3117",
      "pn": "3121",
      "abstract": [
        "As the population in developed countries is aging, larger numbers of\npeople are at risk of developing dementia. In the near future there\nwill be a need for time- and cost-efficient screening methods. Speech\ncan be recorded and analyzed in this manner, and as speech and language\nare affected early on in the course of dementia, automatic speech processing\ncan provide valuable support for such screening methods.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  We present two pipelines\nof feature extraction for dementia detection: the  manual pipeline\nuses manual transcriptions while the  fully automatic pipeline uses\ntranscriptions created by automatic speech recognition (ASR). The acoustic\nand linguistic features that we extract need no language specific tools\nother than the ASR system. Using these two different feature extraction\npipelines we automatically detect dementia. Our results show that the\nASR system&#8217;s transcription quality is a good single feature and\nthat the features extracted from automatic transcriptions perform similar\nor slightly better than the features extracted from the manual transcriptions.\n"
      ],
      "doi": "10.21437/Interspeech.2017-112"
    },
    "gupta17_interspeech": {
      "authors": [
        [
          "Rahul",
          "Gupta"
        ],
        [
          "Saurabh",
          "Sahu"
        ],
        [
          "Carol",
          "Espy-Wilson"
        ],
        [
          "Shrikanth S.",
          "Narayanan"
        ]
      ],
      "title": "An Affect Prediction Approach Through Depression Severity Parameter Incorporation in Neural Networks",
      "original": "0120",
      "page_count": 5,
      "order": 646,
      "p1": "3122",
      "pn": "3126",
      "abstract": [
        "Humans use emotional expressions to communicate their internal affective\nstates. These behavioral expressions are often multi-modal (e.g. facial\nexpression, voice and gestures) and researchers have proposed several\nschemes to predict the latent affective states based on these expressions.\nThe relationship between the latent affective states and their expression\nis hypothesized to be affected by several factors; depression disorder\nbeing one of them. Despite a wide interest in affect prediction, and\nseveral studies linking the effect of depression on affective expressions,\nonly a limited number of affect prediction models account for the depression\nseverity. In this work, we present a novel scheme that incorporates\ndepression severity as a parameter in Deep Neural Networks (DNNs).\nIn order to predict affective dimensions for an individual at hand,\nour scheme alters the DNN activation function based on the subject&#8217;s\ndepression severity. We perform experiments on affect prediction in\ntwo different sessions of the Audio-Visual Depressive language Corpus,\nwhich involves patients with varying degree of depression. Our results\nshow improvements in arousal and valence prediction on both the sessions\nusing the proposed DNN modeling. We also present analysis of the impact\nof such an alteration in DNNs during training and testing.\n"
      ],
      "doi": "10.21437/Interspeech.2017-120"
    },
    "gillespie17_interspeech": {
      "authors": [
        [
          "Stephanie",
          "Gillespie"
        ],
        [
          "Yash-Yee",
          "Logan"
        ],
        [
          "Elliot",
          "Moore"
        ],
        [
          "Jacqueline",
          "Laures-Gore"
        ],
        [
          "Scott",
          "Russell"
        ],
        [
          "Rupal",
          "Patel"
        ]
      ],
      "title": "Cross-Database Models for the Classification of Dysarthria Presence",
      "original": "0216",
      "page_count": 5,
      "order": 647,
      "p1": "3127",
      "pn": "3131",
      "abstract": [
        "Dysarthria is a motor speech disorder that impacts verbal articulation\nand co-ordination, resulting in slow, slurred and imprecise speech.\nAutomated classification of dysarthria subtypes and severities could\nprovide a useful clinical tool in assessing the onset and progress\nin treatment. This study represents a pilot project to train models\nto detect the presence of dysarthria in continuous speech. Subsets\nof the Universal Access Research Dataset (UA-Speech) and the Atlanta\nMotor Speech Disorders Corpus (AMSDC) database were utilized in a cross-database\ntraining strategy (training on UA-Speech / testing on AMSDC) to distinguish\nspeech with and without dysarthria. In addition to traditional spectral\nand prosodic features, the current study also includes features based\non the Teager Energy Operator (TEO) and the glottal waveform. Baseline\nresults on the UA-Speech dataset maximize word- and participant-level\naccuracies at 75.3% and 92.9% using prosodic features. However, the\ncross-training of UA-Speech tested on the AMSDC maximize word- and\nparticipant-level accuracies at 71.3% and 90% based on a TEO feature.\nThe results of this pilot study reinforce consideration of dysarthria\nsubtypes in cross-dataset training as well as highlight additional\nfeatures that may be sensitive to the presence of dysarthria in continuous\nspeech.\n"
      ],
      "doi": "10.21437/Interspeech.2017-216"
    },
    "novotny17_interspeech": {
      "authors": [
        [
          "M.",
          "Novotn\u00fd"
        ],
        [
          "Jan",
          "Rusz"
        ],
        [
          "K.",
          "Sp\u00e1lenka"
        ],
        [
          "Ji\u0159\u00ed",
          "Klemp\u00ed\u0159"
        ],
        [
          "D.",
          "Hor\u00e1kov\u00e1"
        ],
        [
          "Ev\u017een",
          "R\u016f\u017ei\u010dka"
        ]
      ],
      "title": "Acoustic Evaluation of Nasality in Cerebellar Syndromes",
      "original": "0381",
      "page_count": 5,
      "order": 648,
      "p1": "3132",
      "pn": "3136",
      "abstract": [
        "Although previous studies have reported the occurrence of velopharyngeal\nincompetence connected with ataxic dysarthria, there is a lack of evidence\nrelated to nasality assessment in cerebellar disorders. This is partly\ndue to the limited reliability of challenging analyses and partly due\nto nasality being a less pronounced manifestation of ataxic dysarthria.\nTherefore, we employed 1/3-octave spectra analysis as an objective\nmeasurement of nasality disturbances. We analyzed 20 subjects with\nmultiple system atrophy (MSA), 13 subjects with cerebellar ataxia (CA),\n20 subjects with multiple sclerosis (MS) and 20 healthy (HC) speakers.\nAlthough we did not detect the presence of hypernasality, our results\nshowed increased nasality fluctuation in 65% of MSA, 43% of CA and\n30% of MS subjects compared to 15% of HC speakers, suggesting inconsistent\nvelopharyngeal motor control. Furthermore, we found a statistically\nsignificant difference between MSA and HC participants (p&#60;0.001),\nand significant correlation between the natural history cerebellar\nsubscore and neuroprotection in Parkinson plus syndromes &#8212; Parkinson\nplus scale and nasality fluctuations in MSA (r=0.51, p&#60;0.05). In\nconclusion, acoustic analysis showed an increased presence of abnormal\nnasality fluctuations in all ataxic groups and revealed that nasality\nfluctuation is associated with distortion of cerebellar functions.\n"
      ],
      "doi": "10.21437/Interspeech.2017-381"
    },
    "hantke17_interspeech": {
      "authors": [
        [
          "Simone",
          "Hantke"
        ],
        [
          "Hesam",
          "Sagha"
        ],
        [
          "Nicholas",
          "Cummins"
        ],
        [
          "Bj\u00f6rn",
          "Schuller"
        ]
      ],
      "title": "Emotional Speech of Mentally and Physically Disabled Individuals: Introducing the EmotAsS Database and First Findings",
      "original": "0409",
      "page_count": 5,
      "order": 649,
      "p1": "3137",
      "pn": "3141",
      "abstract": [
        "The automatic recognition of emotion from speech is a mature research\nfield with a large number of publicly available corpora. However, to\nthe best of the authors knowledge, none of these datasets consist solely\nof emotional speech samples from individuals with mental, neurological\nand/or physical disabilities. Yet, such individuals could benefit from\nspeech-based assistive technologies to enhance their communication\nwith their environment and to manage their daily work process. With\nthe aim of advancing these technologies, we fill this void in emotional\nspeech resources by introducing the EmotAsS (Emotional Sensitivity\nAssistance System for People with Disabilities) corpus consisting of\nspontaneous emotional German speech data recorded from 17 mentally,\nneurologically and/or physically disabled participants in their daily\nwork environment, resulting in just under 11 hours of total speech\ntime and featuring approximately 12.7 k utterances after segmentation.\nTranscription was performed and labelling was carried out in seven\nemotional categories, as well as for the intelligibility of the speaker.\nWe present a set of baseline results, based on using standard acoustic\nand linguistic features, for arousal and valence emotion recognition.\n"
      ],
      "doi": "10.21437/Interspeech.2017-409"
    },
    "agurto17_interspeech": {
      "authors": [
        [
          "Carla",
          "Agurto"
        ],
        [
          "Raquel",
          "Norel"
        ],
        [
          "Rachel",
          "Ostrand"
        ],
        [
          "Gillinder",
          "Bedi"
        ],
        [
          "Harriet de",
          "Wit"
        ],
        [
          "Matthew J.",
          "Baggott"
        ],
        [
          "Matthew G.",
          "Kirkpatrick"
        ],
        [
          "Margaret",
          "Wardle"
        ],
        [
          "Guillermo A.",
          "Cecchi"
        ]
      ],
      "title": "Phonological Markers of Oxytocin and MDMA Ingestion",
      "original": "0621",
      "page_count": 5,
      "order": 650,
      "p1": "3142",
      "pn": "3146",
      "abstract": [
        "Speech data has the potential to become a powerful tool to provide\nquantitative information about emotion beyond that achieved by subjective\nassessments. Based on this concept, we investigate the use of speech\nto identify effects in subjects under the influence of two different\ndrugs: Oxytocin (OT) and 3,4-methylenedioxymethamphetamine (MDMA),\nalso known as ecstasy. We extract a set of informative phonological\nfeatures that can characterize emotion. Then, we perform classification\nto detect if the subject is under the influence of a drug. Our best\nresults show low error rates of 13% and 17% for the subject classification\nof OT and MDMA vs. placebo, respectively. We also analyze the performance\nof the features to differentiate the two levels of MDMA doses, obtaining\nan error rate of 19%. The results indicate that subtle emotional changes\ncan be detected in the context of drug use.\n"
      ],
      "doi": "10.21437/Interspeech.2017-621"
    },
    "mirheidari17_interspeech": {
      "authors": [
        [
          "Bahman",
          "Mirheidari"
        ],
        [
          "Daniel",
          "Blackburn"
        ],
        [
          "Kirsty",
          "Harkness"
        ],
        [
          "Traci",
          "Walker"
        ],
        [
          "Annalena",
          "Venneri"
        ],
        [
          "Markus",
          "Reuber"
        ],
        [
          "Heidi",
          "Christensen"
        ]
      ],
      "title": "An Avatar-Based System for Identifying Individuals Likely to Develop Dementia",
      "original": "0690",
      "page_count": 5,
      "order": 651,
      "p1": "3147",
      "pn": "3151",
      "abstract": [
        "This paper presents work on developing an automatic dementia screening\ntest based on patients&#8217; ability to interact and communicate &#8212;\na highly cognitively demanding process where early signs of dementia\ncan often be detected. Such a test would help general practitioners,\nwith no specialist knowledge, make better diagnostic decisions as current\ntests lack specificity and sensitivity. We investigate the feasibility\nof basing the test on conversations between a &#8216;talking head&#8217;\n(avatar) and a patient and we present a system for analysing such conversations\nfor signs of dementia in the patient&#8217;s speech and language. Previously\nwe proposed a semi-automatic system that transcribed conversations\nbetween patients and neurologists and extracted conversation analysis\nstyle features in order to differentiate between patients with progressive\nneurodegenerative dementia (ND) and functional memory disorders (FMD).\nDetermining who talks when in the conversations was performed manually.\nIn this study, we investigate a fully automatic system including speaker\ndiarisation, and the use of additional acoustic and lexical features.\nInitial results from a pilot study are presented which shows that the\navatar conversations can successfully classify ND/FMD with around 91%\naccuracy, which is in line with previous results for conversations\nthat were led by a neurologist.\n"
      ],
      "doi": "10.21437/Interspeech.2017-690"
    },
    "zhang17k_interspeech": {
      "authors": [
        [
          "Yue",
          "Zhang"
        ],
        [
          "Felix",
          "Weninger"
        ],
        [
          "Bj\u00f6rn",
          "Schuller"
        ]
      ],
      "title": "Cross-Domain Classification of Drowsiness in Speech: The Case of Alcohol Intoxication and Sleep Deprivation",
      "original": "1015",
      "page_count": 5,
      "order": 652,
      "p1": "3152",
      "pn": "3156",
      "abstract": [
        "In this work, we study the drowsy state of a speaker, induced by alcohol\nintoxication or sleep deprivation. In particular, we investigate the\ncoherence between the two pivotal causes of drowsiness, as featured\nin the Intoxication and Sleepiness tasks of the INTERSPEECH Speaker\nState Challenge. In this way, we aim to exploit the interrelations\nbetween these different, yet highly correlated speaker states, which\nneed to be reliably recognised in safety and security critical environments.\nTo this end, we perform cross-domain classification of alcohol intoxication\nand sleepiness, thus leveraging the acoustic similarities of these\nspeech phenomena for transfer learning. Further, we conducted in-depth\nfeature analysis to quantitatively assess the task relatedness and\nto determine the most relevant features for both tasks. To test our\nmethods in realistic contexts, we use the Alcohol Language Corpus and\nthe Sleepy Language Corpus containing in total 60 hours of genuine\nintoxicated and sleepy speech. In the result, cross-domain classification\ncombined with feature selection yields up to 60.3% unweighted average\nrecall, which is significantly above-chance (50%) and highly notable\ngiven the mismatch in the training and validation data. Finally, we\nshow that an effective, general drowsiness classifier can be obtained\nby aggregating the training data from both domains.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1015"
    },
    "lopezotero17b_interspeech": {
      "authors": [
        [
          "Paula",
          "Lopez-Otero"
        ],
        [
          "Laura",
          "Docio-Fernandez"
        ],
        [
          "Alberto",
          "Abad"
        ],
        [
          "Carmen",
          "Garcia-Mateo"
        ]
      ],
      "title": "Depression Detection Using Automatic Transcriptions of De-Identified Speech",
      "original": "1201",
      "page_count": 5,
      "order": 653,
      "p1": "3157",
      "pn": "3161",
      "abstract": [
        "Depression is a mood disorder that is usually addressed by outpatient\ntreatments in order to favour patient&#8217;s inclusion in society.\nThis leads to a need for novel automatic tools exploiting speech processing\napproaches that can help to monitor the emotional state of patients\nvia telephone or the Internet. However, the transmission, processing\nand subsequent storage of such sensitive data raises several privacy\nconcerns. Speech de-identification can be used to protect the patients&#8217;\nidentity. Nevertheless, these techniques modify the speech signal,\neventually affecting the performance of depression detection approaches\nbased on either speech characteristics or automatic transcriptions.\nThis paper presents a study on the influence of speech de-identification\nwhen using transcription-based approaches for depression detection.\nTo this effect, a system based on the global vectors method for natural\nlanguage processing is proposed. In contrast to previous works, two\nmain sources of nuisance have been considered: the de-identification\nprocess itself and the transcription errors introduced by the automatic\nrecognition of the patients&#8217; speech. Experimental validation\non the DAIC-WOZ corpus reveals very promising results, obtaining only\na slight performance degradation with respect to the use of manual\ntranscriptions.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1201"
    },
    "wankerl17_interspeech": {
      "authors": [
        [
          "Sebastian",
          "Wankerl"
        ],
        [
          "Elmar",
          "N\u00f6th"
        ],
        [
          "Stefan",
          "Evert"
        ]
      ],
      "title": "An N-Gram Based Approach to the Automatic Diagnosis of Alzheimer&#8217;s Disease from Spoken Language",
      "original": "1572",
      "page_count": 5,
      "order": 654,
      "p1": "3162",
      "pn": "3166",
      "abstract": [
        "Alzheimer&#8217;s disease (AD) is the most common cause of dementia\nand affects wide parts of the elderly population. Since there exists\nno cure for this illness, it is of particular interest to develop reliable\nand easy-to-use diagnostic methods to alleviate its effects. Speech\ncan be a useful indicator to reach this goal. We propose a purely statistical\napproach towards the automatic diagnosis of AD which is solely based\non n-gram models with subsequent evaluation of the perplexity and does\nnot incorporate any further linguistic features. Hence, it works independently\nof a concrete language. We evaluate our approach on the DementiaBank\nwhich contains spontaneous speech of test subjects describing a picture.\nUsing the Equal-Error-Rate as classification threshold, we achieve\nan accuracy of 77.1%. In addition to that, we studied the correlation\nbetween the calculated perplexities and the Mini-Mental State Examination\n(MMSE) scores of the test subjects. While there is little correlation\nfor the healthy control group, a higher correlation could be found\nwhen considering the demented speakers. This makes it reasonable to\nconclude that our approach reveals some of the cognitive limitations\nof AD patients and can help to better diagnose the disease based on\nspeech.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1572"
    },
    "mundnich17_interspeech": {
      "authors": [
        [
          "Karel",
          "Mundnich"
        ],
        [
          "Md.",
          "Nasir"
        ],
        [
          "Panayiotis",
          "Georgiou"
        ],
        [
          "Shrikanth S.",
          "Narayanan"
        ]
      ],
      "title": "Exploiting Intra-Annotator Rating Consistency Through Copeland&#8217;s Method for Estimation of Ground Truth Labels in Couples&#8217; Therapy",
      "original": "1599",
      "page_count": 5,
      "order": 655,
      "p1": "3167",
      "pn": "3171",
      "abstract": [
        "Behavioral and mental health research and its clinical applications\nwidely rely on quantifying human behavioral expressions. This often\nrequires human-derived behavioral annotations, which tend to be noisy,\nespecially when the psychological objects of interest are latent and\nsubjective in nature. This paper focuses on exploiting multiple human\nannotations toward improving reliability of the ensemble decision,\nby creating a ranking of the evaluated objects. To create this ranking,\nwe employ an adapted version of Copeland&#8217;s counting method, which\nresults in robust inter-annotator rankings and agreement. We use a\nsimple mapping between the ranked objects and the scale of evaluation,\nwhich preserves the original distribution of ratings, based on maximum\nlikelihood estimation. We apply the algorithm to ratings that lack\na ground truth. Therefore, we assess our algorithm in two ways: (1)\nby corrupting the annotations with different distributions of noise,\nand computing the inter-annotator agreement between the ensemble estimates\nderived from the original and corrupted data using Krippendorff&#8217;s\n&#945;; and (2) by replacing one annotator at a time with the ensemble\nestimate. Our results suggest that the proposed method provides a robust\nalternative that suffers less from individual annotator preferences/biases\nand scale misuse.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1599"
    },
    "pettorino17_interspeech": {
      "authors": [
        [
          "Massimo",
          "Pettorino"
        ],
        [
          "Wentao",
          "Gu"
        ],
        [
          "Pawe\u0142",
          "P\u00f3\u0142rola"
        ],
        [
          "Ping",
          "Fan"
        ]
      ],
      "title": "Rhythmic Characteristics of Parkinsonian Speech: A Study on Mandarin and Polish",
      "original": "0850",
      "page_count": 5,
      "order": 656,
      "p1": "3172",
      "pn": "3176",
      "abstract": [
        "Previous studies on Italian speech showed that the percentage of vocalic\nportion in the utterance (%V) and the duration of the interval between\ntwo consecutive vowel onset points (VtoV) were larger for parkinsonian\n(PD) than for healthy controls (HC). Especially, the values of %V were\ndistinctly separated between PD and HC. The present study aimed to\nfurther test the finding on Mandarin and Polish. Twenty-five Mandarin\nspeakers (13 PD and 12 HC matched on age) and thirty-one Polish speakers\n(18 PD and 13 HC matched on age) read aloud a passage of story. The\nrecorded speeches were segmented into vocalic and consonantal intervals,\nand then %V and VtoV were calculated. For both languages, VtoV overlapped\nbetween HC and PD. For Polish, %V was distinctly higher in PD than\nin HC, while for Mandarin there was no significant difference. It suggests\nthat %V could be used for automatic diagnosis of PD for Italian and\nPolish, but not for Mandarin. The effectiveness of the rhythmic metric\nappears to be language-dependent, varying with the rhythmic typology\nof the language.\n"
      ],
      "doi": "10.21437/Interspeech.2017-850"
    },
    "tu17c_interspeech": {
      "authors": [
        [
          "Jung-Yueh",
          "Tu"
        ],
        [
          "Janice Wing-Sze",
          "Wong"
        ],
        [
          "Jih-Ho",
          "Cha"
        ]
      ],
      "title": "Trisyllabic Tone 3 Sandhi Patterns in Mandarin Produced by Cantonese Speakers",
      "original": "0291",
      "page_count": 4,
      "order": 657,
      "p1": "3177",
      "pn": "3180",
      "abstract": [
        "The third tone sandhi in Mandarin is a well-studied rule, where a Tone\n3 followed by another Tone 3 is changed as a rising tone, similar to\nTone 2. This Tone 3 sandhi rule is straightforward in disyllabic words,\nwhich is phonetically driven for the ease of production. In three or\nmore than three syllables with Tone 3, however, the Tone 3 sandhi application\nis more complicated and involves both the prosodic and morph-syntactic\ndomains, which makes it difficult for L2 learners. This study aims\nto understand how L2 learners with another tone language experience\ncould master the Mandarin Tone 3 sandhi rule. Specifically, the study\ninvestigates the production of Tone 3 sandhi in trisyllabic Mandarin\nwords by Cantonese speakers. In the current study, 30 Cantonese speakers\nwere requested to produce 15 trisyllabic words (&#8220;1+[2+3]&#8221;\nand &#8220;[1+2]+3&#8221; sandhi patterns) and 5 hexasyllabic sentences\nwith Tone 3 in sequences. The analyses of results center on three major\ntypes of error patterns: overgeneralization, under application, and\ncombination. The findings are discussed with regard to the phono-syntactic\ninteractions of Tone 3 sandhi at the lexical and phrasal levels as\nwell as the influence of the Cantonese tonal system.\n"
      ],
      "doi": "10.21437/Interspeech.2017-291"
    },
    "sahkai17_interspeech": {
      "authors": [
        [
          "Heete",
          "Sahkai"
        ],
        [
          "Meelis",
          "Mihkla"
        ]
      ],
      "title": "Intonation of Contrastive Topic in Estonian",
      "original": "0840",
      "page_count": 5,
      "order": 658,
      "p1": "3181",
      "pn": "3185",
      "abstract": [
        "Contrastive topic is an information structural category that is usually\nassociated with a specific intonation, which tends to be similar across\nlanguages (a rising pitch accent). The aim of the present study is\nto examine whether this also true of Estonian. Three potential prosodic\ncorrelates of contrastive topics are examined: marking with a particular\npitch accent type, an emphatic realization of the pitch accent, and\na following prosodic boundary. With respect to pitch accent types,\nit is found that only two subjects out of eight distinguish sentences\nwith a contrastive topic from other types of information structure;\nthe contour bears resemblance to contrastive topic intonation in other\nlanguages (consisting of an H* accent on the contrastive topic and\nan HL* accent on the focus), but is not restricted to sentences with\ncontrastive topics. A more consistent correlate turns out to be an\nemphatic realization of the pitch accent carried by the contrastive\ntopic constituent. No evidence is found of a tendency to produce contrastive\ntopics as separate prosodic phrases.\n"
      ],
      "doi": "10.21437/Interspeech.2017-840"
    },
    "hao17_interspeech": {
      "authors": [
        [
          "Lixia",
          "Hao"
        ],
        [
          "Wei",
          "Zhang"
        ],
        [
          "Yanlu",
          "Xie"
        ],
        [
          "Jinsong",
          "Zhang"
        ]
      ],
      "title": "Reanalyze Fundamental Frequency Peak Delay in Mandarin",
      "original": "1235",
      "page_count": 5,
      "order": 659,
      "p1": "3186",
      "pn": "3190",
      "abstract": [
        "In Mandarin, Fundamental Frequency (F0) peak delay has been reported\nto occur frequently in the rising (R) tone or high (H) tone succeeding\nby a low (L) tone. Its occurrence was ascribed to articulatory constraints\nwithin a conflicting tonal context: a high offset target followed by\na low onset target. To further examine the underlying mechanism of\nthe phenomenon, the current study tests the possibility that valley\ndelay, as opposed to peak delay, may occur in an L+H tonal context;\nand peak or valley delay may also occur within a compatible tonal context\nwhere adjacent tonal values are identical or similar. An experiment\nwas done on Annotated Speech Corpus of Chinese Discourse to investigate\nthe frequency of occurrence and amount of peak and valley delay. The\nresults indicated that: F0 peak and valley delay frequently occurred\nin both conflicting and compatible tonal contexts; the phenomenon was\nfound extensively in R tone and F (falling) tone, but barely in H tone\nand L tone. The findings suggest that while peak or valley delay is\npartially due to articulatory constraints in certain tonal contexts,\nthe speakers&#8217; active effort-distribution strategy based on economical\nprinciple is also behind the phenomenon.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1235"
    },
    "michelas17_interspeech": {
      "authors": [
        [
          "Amandine",
          "Michelas"
        ],
        [
          "Cecile",
          "Cau"
        ],
        [
          "Maud",
          "Champagne-Lavau"
        ]
      ],
      "title": "How Does the Absence of Shared Knowledge Between Interlocutors Affect the Production of French Prosodic Forms?",
      "original": "1430",
      "page_count": 5,
      "order": 660,
      "p1": "3191",
      "pn": "3195",
      "abstract": [
        "We examine the hypothesis that modelling the addressee in spoken interaction\naffects the production of prosodic forms by the speaker. This question\nwas tested in an interactive paradigm that enabled us to measure prosodic\nvariations at two levels: the global/acoustic level and the phonological\none. We used a semi-spontaneous task in which French speakers gave\ninstructions to addressees about where to place a cross between different\nobjects (e.g.,  Tu mets la croix entre la souris bordeau et la maison\nbordeau; &#8216;You put the cross between the red mouse and the red\nhouse&#8217;). Each trial was composed of two noun-adjective fragments\nand the target was the second fragment. We manipulated (i) whether\nthe two interlocutors shared or didn&#8217;t share the same objects\nand (ii) the informational status of targets to obtain variations in\nabstract prosodic phrasing. We found that the absence of shared knowledge\nbetween interlocutors affected the speaker&#8217;s production of prosodic\nforms at the global/acoustic level (i.e., pitch range and speech rate)\nbut not at the phonological one (i.e., prosodic phrasing). These results\nare consistent with a mechanism in which global prosodic variations\nare influenced by audience design because they reflect the way that\nspeakers help addressees to understand speech.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1430"
    },
    "wagner17_interspeech": {
      "authors": [
        [
          "Michael",
          "Wagner"
        ],
        [
          "Michael",
          "McAuliffe"
        ]
      ],
      "title": "Three Dimensions of Sentence Prosody and Their (Non-)Interactions",
      "original": "1500",
      "page_count": 5,
      "order": 661,
      "p1": "3196",
      "pn": "3200",
      "abstract": [
        "Prosody simultaneously encodes different kinds of information, including\nthe type of speech act of an utterance (e.g., falling declarative vs.\nrising interrogative intonational tunes), the location of semantic\nfocus (via prosodic prominence), and syntactic constituent structure\n(via prosodic phrasing). The syntactic/ semantic functional dimensions\n(speech act, focus, constituency) are orthogonal to each other, but\nto which extent their prosodic correlates (tune, prominence, phrasing)\nare remains controversial. This paper takes a &#8216;bottom up&#8217;\napproach to test for interactions, and reports evidence that contrary\nto many current theories of sentence intonation, the cues to the three\ndimensions are often orthogonal where interactions are predicted.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1500"
    },
    "kleinhans17_interspeech": {
      "authors": [
        [
          "Janine",
          "Kleinhans"
        ],
        [
          "Mireia",
          "Farr\u00fas"
        ],
        [
          "Agust\u00edn",
          "Gravano"
        ],
        [
          "Juan Manuel",
          "P\u00e9rez"
        ],
        [
          "Catherine",
          "Lai"
        ],
        [
          "Leo",
          "Wanner"
        ]
      ],
      "title": "Using Prosody to Classify Discourse Relations",
      "original": "0710",
      "page_count": 5,
      "order": 662,
      "p1": "3201",
      "pn": "3205",
      "abstract": [
        "This work aims to explore the correlation between the discourse structure\nof a spoken monologue and its prosody by predicting discourse relations\nfrom different prosodic attributes. For this purpose, a corpus of semi-spontaneous\nmonologues in English has been automatically annotated according to\nthe Rhetorical Structure Theory, which models coherence in text via\nrhetorical relations. From corresponding audio files, prosodic features\nsuch as pitch, intensity, and speech rate have been extracted from\ndifferent contexts of a relation. Supervised classification tasks using\nSupport Vector Machines have been performed to find relationships between\nprosodic features and rhetorical relations. Preliminary results show\nthat intensity combined with other features extracted from intra- and\nintersegmental environments is the feature with the highest predictability\nfor a discourse relation. The prediction of rhetorical relations from\nprosodic features and their combinations is straightforwardly applicable\nto several tasks such as speech understanding or generation. Moreover,\nthe knowledge of how rhetorical relations should be marked in terms\nof prosody will serve as a basis to improve speech synthesis applications\nand make voices sound more natural and expressive.\n"
      ],
      "doi": "10.21437/Interspeech.2017-710"
    },
    "godoy17_interspeech": {
      "authors": [
        [
          "Elizabeth",
          "Godoy"
        ],
        [
          "James R.",
          "Williamson"
        ],
        [
          "Thomas F.",
          "Quatieri"
        ]
      ],
      "title": "Canonical Correlation Analysis and Prediction of Perceived Rhythmic Prominences and Pitch Tones in Speech",
      "original": "1585",
      "page_count": 5,
      "order": 663,
      "p1": "3206",
      "pn": "3210",
      "abstract": [
        "Speech prosody encodes information about language and communicative\nintent as well as speaker identity and state. Consequently, a host\nof speech technologies could benefit from increased understanding of\nprosodic phenomena and corresponding acoustics. A recently developed\ncomprehensive prosodic transcription system called RaP (Rhythm-and-Pitch)\nannotates both perceived rhythmic prominences and pitch tones in speech.\nUsing RaP-annotated speech corpora, the present work analyzes relationships\nbetween perceived prosodic events and acoustic features including syllable\nduration and novel measures of intensity and fundamental frequency.\nCanonical Correlation Analysis (CCA) reveals two dominant prosodic\ndimensions relating the acoustic features and RaP annotations. The\nfirst captures perceived prosodic emphasis of syllables indicated by\nstrong metrical beats and significant pitch variability (i.e. presence\nof either high or low pitch tones). Acoustically, this dimension is\ndescribed most by syllable duration followed by the mean intensity\nand fundamental frequency measures. The second CCA dimension then primarily\ndiscriminates pitch tone level (high versus low), indicated mainly\nby the mean fundamental frequency measure. Finally, within a leave-one-out\ncross-validation framework, RaP prosodic events are well-predicted\nfrom acoustic features (AUC between 0.78 and 0.84). Future work will\nexploit automated RaP labelling in contexts ranging from language learning\nto neurological disorder recognition.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1585"
    },
    "kakouros17_interspeech": {
      "authors": [
        [
          "Sofoklis",
          "Kakouros"
        ],
        [
          "Okko",
          "R\u00e4s\u00e4nen"
        ],
        [
          "Paavo",
          "Alku"
        ]
      ],
      "title": "Evaluation of Spectral Tilt Measures for Sentence Prominence Under Different Noise Conditions",
      "original": "1237",
      "page_count": 5,
      "order": 664,
      "p1": "3211",
      "pn": "3215",
      "abstract": [
        "Spectral tilt has been suggested to be a correlate of prominence in\nspeech, although several studies have not replicated this empirically.\nThis may be partially due to the lack of a standard method for tilt\nestimation from speech, rendering interpretations and comparisons between\nstudies difficult. In addition, little is known about the performance\nof tilt estimators for prominence detection in the presence of noise.\nIn this work, we investigate and compare several standard tilt measures\non quantifying prominence in spoken Dutch and under different levels\nof additive noise. We also compare these measures with other acoustic\ncorrelates of prominence, namely, energy, F0, and duration. Our results\nprovide further empirical support for the finding that tilt is a systematic\ncorrelate of prominence, at least in Dutch, even though energy, F0,\nand duration appear still to be more robust features for the task.\nIn addition, our results show that there are notable differences between\ndifferent tilt estimators in their ability to discriminate prominent\nwords from non-prominent ones in different levels of noise.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1237"
    },
    "kuang17_interspeech": {
      "authors": [
        [
          "Jianjing",
          "Kuang"
        ]
      ],
      "title": "Creaky Voice as a Function of Tonal Categories and Prosodic Boundaries",
      "original": "1578",
      "page_count": 5,
      "order": 665,
      "p1": "3216",
      "pn": "3220",
      "abstract": [
        "This study looks into the distribution of creaky voice in Mandarin\nin continuous speech. A creaky voice detector was used to automatically\ndetect the appearance of creaky voice in a large-scale Mandarin corpus\n(Sinica COSPRO corpus). As the prosodic information has been annotated\nin the corpus, we were able to look at the distribution of creaky voice\nas a function of the interaction between tone and prosodic structures.\nAs expected, among the five tonal categories (four lexical tones and\none neutral tone), creaky voice is most likely to occur with Tone 3\nand the neutral tone, followed by Tone 2 and Tone 4. Prosodic boundaries\nalso play important roles, as the likelihood of creak increases when\nthe prosodic boundaries are larger, regardless of the tonal categories.\nIt is also confirmed that the pitch range for the occurrence of creaky\nvoice is 110 Hz for male speakers and 170 Hz for female speakers, consistent\nwith previous small-scale studies. Finally, male speakers have a higher\noverall rate of creaky voice than female speakers. Altogether, this\nstudy validates the hypotheses from previous studies, and provides\na better understanding of voice-source variation in different prosodic\nconditions.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1578"
    },
    "skarnitzl17_interspeech": {
      "authors": [
        [
          "Radek",
          "Skarnitzl"
        ],
        [
          "Anders",
          "Eriksson"
        ]
      ],
      "title": "The Acoustics of Word Stress in Czech as a Function of Speaking Style",
      "original": "0417",
      "page_count": 5,
      "order": 666,
      "p1": "3221",
      "pn": "3225",
      "abstract": [
        "The study is part of a series of studies which examine the acoustic\ncorrelates of lexical stress in several typologically different languages,\nin three speech styles: spontaneous speech, phrase reading, and wordlist\nreading. This study focuses on Czech, a language with stress fixed\non the first syllable of a prosodic word, with no contrastive function\nat the level of individual words. The acoustic parameters examined\nhere are  F0-level, F0-variation, Duration, Sound Pressure Level, and\n Spectral Emphasis. Values for over 6,000 vowels were analyzed.<br\nstyle=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  Unlike the other languages examined so far, lexical stress in\nCzech is not manifested by clear prominence markings on the first,\nstressed syllable: the stressed syllable is neither higher, realized\nwith greater F0 variation, longer; nor does it have a higher SPL or\nhigher Spectral Emphasis. There are slight, but insignificant tendencies\npointing to a delayed rise, that is, to higher values of some of the\nacoustic parameters on the second, post-stressed syllable. Since lexical\nstress does not serve a contrastive function in Czech, the absence\nof acoustic marking on the stressed syllable is not surprising.\n"
      ],
      "doi": "10.21437/Interspeech.2017-417"
    },
    "wagner17b_interspeech": {
      "authors": [
        [
          "Petra",
          "Wagner"
        ],
        [
          "Nataliya",
          "Bryhadyr"
        ]
      ],
      "title": "What You See is What You Get Prosodically Less &#8212; Visibility Shapes Prosodic Prominence Production in Spontaneous Interaction",
      "original": "0177",
      "page_count": 5,
      "order": 667,
      "p1": "3226",
      "pn": "3230",
      "abstract": [
        "We investigated the expression of prosodic prominence related to unpredictability\nand relevance in spontaneous dyadic interactions in which interlocutors\ncould or could not see each other. Interactions between visibility\nand prominence were analyzed in a verbal version of the game TicTacToe.\nThis setting allows for disentangling different types of information\nstructure: early moves tend to be unpredictable, but are typically\nirrelevant for the immediate outcome of the game, while late moves\ntend to be predictable but relevant, as they usually prevent an opponent&#8217;s\nwinning move or constitute a winning move by themselves.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  Our analyses on German\nreveal that prominence expression is affected globally by visibility\nconditions: speech becomes overall softer and faster when interlocutors\ncan see each other. However, speakers differentiate unpredictability\nand relevance-related accents rather consistently using intensity cues\nboth under visibility and invisibility conditions. We also find that\npitch excursions related to prosodic information structure are not\naffected by visibility. Our findings support effort-optimization models\nof speech production, but also models that regard speech production\nas an integrated bimodal process with a high degree of congruency across\ndomains.\n"
      ],
      "doi": "10.21437/Interspeech.2017-177"
    },
    "hsu17b_interspeech": {
      "authors": [
        [
          "Yu-Yin",
          "Hsu"
        ],
        [
          "Anqi",
          "Xu"
        ]
      ],
      "title": "Focus Acoustics in Mandarin Nominals",
      "original": "1167",
      "page_count": 5,
      "order": 668,
      "p1": "3231",
      "pn": "3235",
      "abstract": [
        "In addition to deciding what to say, interlocutors have to decide how\nto say it. One of the important tasks of linguists is then to model\nhow differences in acoustic patterns influence the interpretation of\na sentence. In light of previous studies on how prosodic structure\nconvey discourse-level of information in a sentence, this study makes\nuse of a speech production experiment to investigate how expressions\nrelated to different information packaging, such as information focus,\ncorrective focus, and old information, are prosodically realized within\na complex nominal. Special attention was paid to the sequence of &#8220;numeral-classifier-noun&#8221;\nin Mandarin, which consists of closely related sub-syntactic units\ninternally, and provides a phonetically controlled environment comparable\nto previous phonetic studies on focus prominence at the sentential\nlevel. The result shows that a multi-dimensional strategy is used in\nfocus-marking, and that focus prosody is sensitive to the size of focus\ndomain and is observable in various lexical tonal environments in Mandarin.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1167"
    },
    "lundmark17_interspeech": {
      "authors": [
        [
          "Malin Svensson",
          "Lundmark"
        ],
        [
          "Gilbert",
          "Ambrazaitis"
        ],
        [
          "Otto",
          "Ewald"
        ]
      ],
      "title": "Exploring Multidimensionality: Acoustic and Articulatory Correlates of Swedish Word Accents",
      "original": "1502",
      "page_count": 5,
      "order": 669,
      "p1": "3236",
      "pn": "3240",
      "abstract": [
        "This study investigates acoustic and articulatory correlates of South\nSwedish word accents (Accent 1 vs. 2) &#8212; a tonal distinction traditionally\nassociated with F0 timing. The study is motivated by previous findings\non (i) the acoustic complexity of tonal prosody and (ii) tonal-articulatory\ninterplay in other languages.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  Acoustic and articulatory\n(EMA) data from two controlled experiments are reported (14 speakers\nin total; pilot EMA recordings with 2 speakers). Apart from the well-established\nF0 timing pattern, results of Experiment 1 reveal a longer duration\nof a post-stress consonant in Accent 2 than in Accent 1, a higher degree\nof creaky voice in Accent 1, as well as a deviant (two-peak) pitch\npattern in Accent 2 for one of eight discourse conditions used in the\nexperiment. Experiment 2 reveals an effect of word accent on vowel\narticulation, as the tongue body gesture target is reached earlier\nin Accent 2. It also suggests slight but (marginally) significant word-accent\neffects on word-initial gestural coordination, taking slightly different\nforms in the two speakers, as well as corresponding differences in\nword-initial formant patterns. Results are discussed concerning their\npotential perceptual relevance, as well as with reference to the c-center\neffect discussed within Articulatory Phonology.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1502"
    },
    "puga17_interspeech": {
      "authors": [
        [
          "Karin",
          "Puga"
        ],
        [
          "Robert",
          "Fuchs"
        ],
        [
          "Jane",
          "Setter"
        ],
        [
          "Peggy",
          "Mok"
        ]
      ],
      "title": "The Perception of English Intonation Patterns by German L2 Speakers of English",
      "original": "1279",
      "page_count": 5,
      "order": 670,
      "p1": "3241",
      "pn": "3245",
      "abstract": [
        "Previous research suggests that intonation is a particularly challenging\naspect of L2 speech learning. While most research focuses on speech\nproduction, we widen the focus and study the perception of intonation\nby L2 learners. We investigate whether advanced German learners of\nEnglish have knowledge of the appropriate English intonation patterns\nin a narrative context with different sentence types (e.g. statements,\nquestions). The results of a tonal pattern selection task indicate\nthat learners (n=20) performed similar to British English controls\n(n=25) for some sentence types (e.g. statements, yes/no-questions),\nbut performed significantly worse than the control group in the case\nof open and closed tag questions and the expression of sarcasm. The\nresults can be explained by the fact that tag questions are the only\nsentence type investigated that does not exist in the learners&#8217;\nL1, and sarcasm is not represented syntactically. This suggests that\nL1 influence can partly account for why some intonation patterns are\nmore challenging than others, and that contextualized knowledge of\nthe intonation patterns of the target language rather than knowledge\nof intonation patterns in isolation is crucial for the successful L2\nlearning of intonation.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1279"
    },
    "paradacabaleiro17_interspeech": {
      "authors": [
        [
          "Emilia",
          "Parada-Cabaleiro"
        ],
        [
          "Alice",
          "Baird"
        ],
        [
          "Anton",
          "Batliner"
        ],
        [
          "Nicholas",
          "Cummins"
        ],
        [
          "Simone",
          "Hantke"
        ],
        [
          "Bj\u00f6rn",
          "Schuller"
        ]
      ],
      "title": "The Perception of Emotions in Noisified Nonsense Speech",
      "original": "0104",
      "page_count": 5,
      "order": 671,
      "p1": "3246",
      "pn": "3250",
      "abstract": [
        "Noise pollution is part of our daily life, affecting millions of people,\nparticularly those living in urban environments. Noise alters our perception\nand decreases our ability to understand others. Considering this, speech\nperception in background noise has been extensively studied, showing\nthat especially white noise can damage listener perception. However,\nthe perception of emotions in noisified speech has not been explored\nwith as much depth. In the present study, we use artificial background\nnoise conditions, by applying noise to a subset of the GEMEP corpus\n(emotions expressed in nonsense speech). Noises were at varying intensities\nand &#8216;colours&#8217;; white, pink, and brownian. The categorical\nand dimensional perceptual test was completed by 26 listeners. The\nresults indicate that background noise conditions influence the perception\nof emotion in speech &#8212; pink noise most, brownian least. Worsened\nperception invokes higher confusion, especially with sadness, an emotion\nwith less pronounced prosodic characteristics. Yet, all this does not\nlead to a break-down of the &#8216;cognitive-emotional space&#8217;\nin a Non-metric MultiDimensional Scaling representation. The gender\nof speakers and the cultural background of listeners do not seem to\nplay a role.\n"
      ],
      "doi": "10.21437/Interspeech.2017-104"
    },
    "gibson17_interspeech": {
      "authors": [
        [
          "James",
          "Gibson"
        ],
        [
          "Do\u011fan",
          "Can"
        ],
        [
          "Panayiotis",
          "Georgiou"
        ],
        [
          "David C.",
          "Atkins"
        ],
        [
          "Shrikanth S.",
          "Narayanan"
        ]
      ],
      "title": "Attention Networks for Modeling Behaviors in Addiction Counseling",
      "original": "0218",
      "page_count": 5,
      "order": 672,
      "p1": "3251",
      "pn": "3255",
      "abstract": [
        "In psychotherapy interactions there are several desirable and undesirable\nbehaviors that give insight into the efficacy of the counselor and\nthe progress of the client. It is important to be able to identify\nwhen these target behaviors occur and what aspects of the interaction\nsignal their occurrence. Manual observation and annotation of these\nbehaviors is costly and time intensive. In this paper, we use long\nshort term memory networks equipped with an attention mechanism to\nprocess transcripts of addiction counseling sessions and predict prominent\ncounselor and client behaviors. We demonstrate that this approach gives\ncompetitive performance while also providing additional interpretability.\n"
      ],
      "doi": "10.21437/Interspeech.2017-218"
    },
    "wortwein17_interspeech": {
      "authors": [
        [
          "Torsten",
          "W\u00f6rtwein"
        ],
        [
          "Tadas",
          "Baltru\u0161aitis"
        ],
        [
          "Eugene",
          "Laksana"
        ],
        [
          "Luciana",
          "Pennant"
        ],
        [
          "Elizabeth S.",
          "Liebson"
        ],
        [
          "Dost",
          "\u00d6ng\u00fcr"
        ],
        [
          "Justin T.",
          "Baker"
        ],
        [
          "Louis-Philippe",
          "Morency"
        ]
      ],
      "title": "Computational Analysis of Acoustic Descriptors in Psychotic Patients",
      "original": "0466",
      "page_count": 5,
      "order": 673,
      "p1": "3256",
      "pn": "3260",
      "abstract": [
        "Various forms of psychotic disorders, including schizophrenia, can\ninfluence how we speak. Therefore, clinicians assess speech and language\nbehaviors of their patients. While it is difficult for humans to quantify\nspeech behaviors precisely, acoustic descriptors, such as tenseness\nof voice and speech rate, can be quantified automatically. In this\nwork, we identify previously unstudied acoustic descriptors related\nto the severity of psychotic symptoms within a clinical population\n(N=29). Our dataset consists of semi-structured interviews between\npatients and clinicians. Psychotic disorders are often characterized\nby two groups of symptoms: negative and positive. While negative symptoms\nare also prevalent in disorders such as depression, positive symptoms\nin psychotic disorders have rarely been studied from an acoustic and\ncomputational perspective. Our experiments show relationships between\npsychotic symptoms and acoustic descriptors related to voice quality\nconsistency, variation of speech rate and volume, vowel space, and\na parameter of glottal flow. Further, we show that certain acoustic\ndescriptors can track a patient&#8217;s state from admission to discharge.\nFinally, we demonstrate that measures from the Brief Psychiatric Rating\nScale (BPRS) can be estimated with acoustic descriptors.\n"
      ],
      "doi": "10.21437/Interspeech.2017-466"
    },
    "wu17e_interspeech": {
      "authors": [
        [
          "Ya-Tse",
          "Wu"
        ],
        [
          "Hsuan-Yu",
          "Chen"
        ],
        [
          "Yu-Hsien",
          "Liao"
        ],
        [
          "Li-Wei",
          "Kuo"
        ],
        [
          "Chi-Chun",
          "Lee"
        ]
      ],
      "title": "Modeling Perceivers Neural-Responses Using Lobe-Dependent Convolutional Neural Network to Improve Speech Emotion Recognition",
      "original": "0562",
      "page_count": 5,
      "order": 674,
      "p1": "3261",
      "pn": "3265",
      "abstract": [
        "Developing automatic emotion recognition by modeling expressive behaviors\nis becoming crucial in enabling the next generation design of human-machine\ninterface. Also, with the availability of functional magnetic resonance\nimaging (fMRI), researchers have also conducted studies into quantitative\nunderstanding of vocal emotion perception mechanism. In this work,\nour aim is two folds: 1) investigating whether the neural-responses\ncan be used to automatically decode the emotion labels of vocal stimuli,\nand 2) combining acoustic and fMRI features to improve the speech emotion\nrecognition accuracies. We introduce a novel framework of lobe-dependent\nconvolutional neural network (LD-CNN) to provide better modeling of\nperceivers neural-responses on vocal emotion. Furthermore, by fusing\nLD-CNN with acoustic features, we demonstrate an overall 63.17% accuracies\nin a four-class emotion recognition task (9.89% and 14.42% relative\nimprovement compared to the acoustic-only and the fMRI-only features).\nOur analysis further shows that temporal lobe possess the most information\nin decoding emotion labels; the fMRI and the acoustic information are\ncomplementary to each other, where neural-responses and acoustic features\nare better at discriminating along the valence and activation dimensions,\nrespectively.\n"
      ],
      "doi": "10.21437/Interspeech.2017-562"
    },
    "vlasenko17_interspeech": {
      "authors": [
        [
          "Bogdan",
          "Vlasenko"
        ],
        [
          "Hesam",
          "Sagha"
        ],
        [
          "Nicholas",
          "Cummins"
        ],
        [
          "Bj\u00f6rn",
          "Schuller"
        ]
      ],
      "title": "Implementing Gender-Dependent Vowel-Level Analysis for Boosting Speech-Based Depression Recognition",
      "original": "0887",
      "page_count": 5,
      "order": 675,
      "p1": "3266",
      "pn": "3270",
      "abstract": [
        "Whilst studies on emotion recognition show that gender-dependent analysis\ncan improve emotion classification performance, the potential differences\nin the manifestation of depression between male and female speech have\nyet to be fully explored. This paper presents a qualitative analysis\nof phonetically aligned acoustic features to highlight differences\nin the manifestation of depression. Gender-dependent analysis with\nphonetically aligned gender-dependent features are used for speech-based\ndepression recognition. The presented experimental study reveals gender\ndifferences in the effect of depression on vowel-level features. Considering\nthe experimental study, we also show that a small set of knowledge-driven\ngender-dependent vowel-level features can outperform state-of-the-art\nturn-level acoustic features when performing a binary depressed speech\nrecognition task. A combination of these preselected gender-dependent\nvowel-level features with turn-level standardised openSMILE features\nresults in additional improvement for depression recognition.\n"
      ],
      "doi": "10.21437/Interspeech.2017-887"
    },
    "siddique17_interspeech": {
      "authors": [
        [
          "Farhad Bin",
          "Siddique"
        ],
        [
          "Pascale",
          "Fung"
        ]
      ],
      "title": "Bilingual Word Embeddings for Cross-Lingual Personality Recognition Using Convolutional Neural Nets",
      "original": "1379",
      "page_count": 5,
      "order": 676,
      "p1": "3271",
      "pn": "3275",
      "abstract": [
        "We propose a multilingual personality classifier that uses text data\nfrom social media and Youtube Vlog transcriptions, and maps them into\nBig Five personality traits using a Convolutional Neural Network (CNN).\nWe first train unsupervised bilingual word embeddings from an English-Chinese\nparallel corpus, and use these trained word representations as input\nto our CNN. This enables our model to yield relatively high cross-lingual\nand multilingual performance on Chinese texts, after training on the\nEnglish dataset for example. We also train monolingual Chinese embeddings\nfrom a large Chinese text corpus and then train our CNN model on a\nChinese dataset consisting of conversational dialogue labeled with\npersonality. We achieve an average F-score of 66.1 in our multilingual\ntask compared to 63.3 F-score in cross-lingual, and 63.2 F-score in\nthe monolingual performance. \n"
      ],
      "doi": "10.21437/Interspeech.2017-1379"
    },
    "arimoto17_interspeech": {
      "authors": [
        [
          "Yoshiko",
          "Arimoto"
        ],
        [
          "Hiroki",
          "Mori"
        ]
      ],
      "title": "Emotion Category Mapping to Emotional Space by Cross-Corpus Emotion Labeling",
      "original": "0994",
      "page_count": 5,
      "order": 677,
      "p1": "3276",
      "pn": "3280",
      "abstract": [
        "The psychological classification of emotion has two main approaches.\nOne is emotion category, in which emotions are classified into discrete\nand fundamental groups; the other is emotion dimension, in which emotions\nare characterized by multiple continuous scales. The cognitive classification\nof emotion by humans perceived from speech is not sufficiently established.\nAlthough there have been several studies on such classification, they\ndid not discuss it deeply. Moreover, the relationship between emotion\ncategory and emotion dimension perceived from speech is not well studied.\nAiming to establish common emotion labels for emotional speech, this\nstudy elucidated the relationship between the emotion category and\nthe emotion dimension perceived by speech by conducting an experiment\nof cross-corpus emotion labeling with two different Japanese dialogue\ncorpora (Online Gaming Voice Chat Corpus with Emotional Label (OGVC)\nand Utsunomiya University Spoken Dialogue Database for Paralinguistic\nInformation Studies (UUDB)). A likelihood ratio test was conducted\nto assess the independency of one emotion category from the others\nin three-dimensional emotional space. This experiment revealed that\nmany emotion categories exhibited independency from the other emotion\ncategories. Only the neutral states did not exhibit independency from\nthe three emotions of sadness, disgust, and surprise.\n"
      ],
      "doi": "10.21437/Interspeech.2017-994"
    },
    "fayet17_interspeech": {
      "authors": [
        [
          "Cedric",
          "Fayet"
        ],
        [
          "Arnaud",
          "Delhay"
        ],
        [
          "Damien",
          "Lolive"
        ],
        [
          "Pierre-Fran\u00e7ois",
          "Marteau"
        ]
      ],
      "title": "Big Five vs. Prosodic Features as Cues to Detect Abnormality in SSPNET-Personality Corpus",
      "original": "1194",
      "page_count": 5,
      "order": 678,
      "p1": "3281",
      "pn": "3285",
      "abstract": [
        "This paper presents an attempt to evaluate three different sets of\nfeatures extracted from prosodic descriptors and Big Five traits for\nbuilding an anomaly detector. The Big Five model enables to capture\npersonality information. Big Five traits are extracted from a manual\nannotation while Prosodic features are extracted directly from the\nspeech signal. Two different anomaly detection methods are evaluated:\nGaussian Mixture Model (GMM) and One-Class SVM (OC-SVM), each one combined\nwith a threshold classification to decide the &#8220;normality&#8221;\nof a sample. The different combinations of models and feature sets\nare evaluated on the SSPNET-Personality corpus which has already been\nused in several experiments, including a previous work on separating\ntwo types of personality profiles in a supervised way. In this work,\nwe propose the above mentioned unsupervised or semi-supervised methods,\nand discuss their performance, to detect particular audio-clips produced\nby a speaker with an abnormal personality. Results show that using\nautomatically extracted prosodic features competes with the Big Five\ntraits. The overall detection performance achieved by the best model\nis around 0.8 (F1-measure).\n"
      ],
      "doi": "10.21437/Interspeech.2017-1194"
    },
    "akira17_interspeech": {
      "authors": [
        [
          "Hayakawa",
          "Akira"
        ],
        [
          "Carl",
          "Vogel"
        ],
        [
          "Saturnino",
          "Luz"
        ],
        [
          "Nick",
          "Campbell"
        ]
      ],
      "title": "Speech Rate Comparison When Talking to a System and Talking to a Human: A Study from a Speech-to-Speech, Machine Translation Mediated Map Task",
      "original": "1584",
      "page_count": 5,
      "order": 679,
      "p1": "3286",
      "pn": "3290",
      "abstract": [
        "This study focuses on the adaptation of subjects in Human-to-Human\n(H2H) communication in spontaneous dialogues in two different settings.\nThe speech rate of sixteen dialogues from the HCRC Map Task corpus\nhave been analyzed as direct H2H communication, while fifteen dialogues\nfrom the ILMT-s2s corpus have been analyzed as a Speech-to-Speech Machine\nTranslation (S2S-MT) mediated H2H communication comparison. The analysis\nshows that while the mean speech rate of the subjects in the two task\noriented corpora differ, in both corpora the role of the subject causes\na significant difference in the speech rate with the Information Giver\nusing a slower speech rate than the Information Follower. Also the\ndifferent settings of the dialogue recordings (with or without eye\ncontact in the HCRC corpus and with or without live video streaming\nin the ILMT-s2s corpus) only show a negligible difference in the speech\nrate. However, the gender of the subjects have provided an interesting\ndifference with the female subjects of the ILMT-s2s corpus using a\nslower speech rate than the male subjects, gender does not show any\ndifference in the HCRC corpus. This indicates that the difference is\nnot from performing the map task, but a result of their adaptation\nstrategy to the S2S-MT system.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1584"
    },
    "tseng17_interspeech": {
      "authors": [
        [
          "Shao-Yen",
          "Tseng"
        ],
        [
          "Brian",
          "Baucom"
        ],
        [
          "Panayiotis",
          "Georgiou"
        ]
      ],
      "title": "Approaching Human Performance in Behavior Estimation in Couples Therapy Using Deep Sentence Embeddings",
      "original": "1621",
      "page_count": 5,
      "order": 680,
      "p1": "3291",
      "pn": "3295",
      "abstract": [
        "Identifying complex behavior in human interactions for observational\nstudies often involves the tedious process of transcribing and annotating\nlarge amounts of data. While there is significant work towards accurate\ntranscription in Automatic Speech Recognition, automatic Natural Language\nUnderstanding of high-level human behaviors from the transcribed text\nis still at an early stage of development. In this paper we present\na novel approach for modeling human behavior using sentence embeddings\nand propose an automatic behavior annotation framework. We explore\nunsupervised methods of extracting semantic information, using  seq2seq\nmodels, into deep sentence embeddings and demonstrate that these embeddings\ncapture behaviorally meaningful information. Our proposed framework\nutilizes LSTM Recurrent Neural Networks to estimate behavior trajectories\nfrom these sentence embeddings. Finally, we employ fusion to compare\nour high-resolution behavioral trajectories with the coarse, session-level\nbehavioral ratings of human annotators in Couples Therapy. Our experiments\nshow that behavior annotation using this framework achieves better\nresults than prior methods and approaches or exceeds human performance\nin terms of annotator agreement.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1621"
    },
    "nasir17_interspeech": {
      "authors": [
        [
          "Md.",
          "Nasir"
        ],
        [
          "Brian",
          "Baucom"
        ],
        [
          "Craig J.",
          "Bryan"
        ],
        [
          "Shrikanth S.",
          "Narayanan"
        ],
        [
          "Panayiotis",
          "Georgiou"
        ]
      ],
      "title": "Complexity in Speech and its Relation to Emotional Bond in Therapist-Patient Interactions During Suicide Risk Assessment Interviews",
      "original": "1641",
      "page_count": 5,
      "order": 681,
      "p1": "3296",
      "pn": "3300",
      "abstract": [
        "In this paper, we analyze a 53-hour speech corpus of interactions of\nsoldiers who had recently attempted suicide or had strong suicidal\nideation conversing with their therapists. In particular, we study\nthe complexity in therapist-patient speech as a marker of their emotional\nbond. Emotional bond is the extent to which the patient feels understood\nby and connected to the therapist. First, we extract speech features\nfrom audio recordings of their interactions. Then, we consider the\nnonlinear time series representation of those features and compute\ncomplexity measures based on the Lyapunov coefficient and correlation\ndimension. For the majority of the subjects, we observe that speech\ncomplexity in therapist-patient pairs is higher for the interview sessions,\nwhen compared to that of the rest of their interactions (intervention\nand post-interview follow-up). This indicates that entrainment (adapting\nto each other&#8217;s speech) between the patient and the therapist\nis lower during the interview than regular interactions. This observation\nis consistent with prior studies in clinical psychology, considering\nthat assessment interviews typically involve the therapist asking routine\nquestions to enquire about the patient&#8217;s suicidal thoughts and\nfeelings. In addition, we find that complexity is negatively correlated\nwith the patient&#8217;s perceived emotional bond with the therapist.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1641"
    },
    "huang17e_interspeech": {
      "authors": [
        [
          "Zhaocheng",
          "Huang"
        ],
        [
          "Julien",
          "Epps"
        ]
      ],
      "title": "An Investigation of Emotion Dynamics and Kalman Filtering for Speech-Based Emotion Prediction",
      "original": "1707",
      "page_count": 5,
      "order": 682,
      "p1": "3301",
      "pn": "3305",
      "abstract": [
        "Despite recent interest in continuous prediction of dimensional emotions,\nthe dynamical aspect of emotions has received less attention in automated\nsystems. This paper investigates how emotion change can be effectively\nincorporated to improve continuous prediction of arousal and valence\nfrom speech. Significant correlations were found between emotion ratings\nand their dynamics during investigations on the RECOLA database, and\nhere we examine how to best exploit them using a Kalman filter. In\nparticular, we investigate the correlation between predicted arousal\nand valence dynamics with arousal and valence ground truth; the Kalman\nfilter internal delay for estimating the state transition matrix; the\nuse of emotion dynamics as a measurement input to a Kalman filter;\nand how multiple probabilistic Kalman filter outputs can be effectively\nfused. Evaluation results show that correct dynamics estimation and\ninternal delay settings allow up to 5% and 58% relative improvement\nin arousal and valence prediction respectively over existing Kalman\nfilter implementations. Fusion based on probabilistic Kalman filter\noutputs yields further gains.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1707"
    },
    "sadamitsu17_interspeech": {
      "authors": [
        [
          "Kugatsu",
          "Sadamitsu"
        ],
        [
          "Yukinori",
          "Homma"
        ],
        [
          "Ryuichiro",
          "Higashinaka"
        ],
        [
          "Yoshihiro",
          "Matsuo"
        ]
      ],
      "title": "Zero-Shot Learning for Natural Language Understanding Using Domain-Independent Sequential Structure and Question Types",
      "original": "0638",
      "page_count": 5,
      "order": 683,
      "p1": "3306",
      "pn": "3310",
      "abstract": [
        "Natural language understanding (NLU) is an important module of spoken\ndialogue systems. One of the difficulties when it comes to adapting\nNLU to new domains is the high cost of constructing new training data\nfor each domain. To reduce this cost, we propose a zero-shot learning\nof NLU that takes into account the sequential structures of sentences\ntogether with general question types across different domains. Experimental\nresults show that our methods achieve higher accuracy than baseline\nmethods in two completely different domains (insurance and sightseeing).\n"
      ],
      "doi": "10.21437/Interspeech.2017-638"
    },
    "sawada17_interspeech": {
      "authors": [
        [
          "Naoki",
          "Sawada"
        ],
        [
          "Ryo",
          "Masumura"
        ],
        [
          "Hiromitsu",
          "Nishizaki"
        ]
      ],
      "title": "Parallel Hierarchical Attention Networks with Shared Memory Reader for Multi-Stream Conversational Document Classification",
      "original": "0269",
      "page_count": 5,
      "order": 684,
      "p1": "3311",
      "pn": "3315",
      "abstract": [
        "This paper describes a novel classification method for multi-stream\nconversational documents. Documents of contact center dialogues or\nmeetings are often composed of multiple source documents that are transcriptions\nof the recordings of each speaker&#8217;s channel. To enhance the classification\nperformance of such multi-stream conversational documents, three main\nadvances over the previous method are introduced. The first is a parallel\nhierarchical attention network (PHAN) for multi-stream conversational\ndocument modeling. PHAN can precisely capture word and sentence structures\nof individual source documents and efficiently integrate them. The\nsecond is a shared memory reader that can yield a shared attention\nmechanism. The shared memory reader highlights common important information\nin a conversation. Our experiments on a call category classification\nin contact center dialogues show that PHAN together with the shared\nmemory reader outperforms the single document modeling method and previous\nmulti-stream document modeling method.\n"
      ],
      "doi": "10.21437/Interspeech.2017-269"
    },
    "morchid17_interspeech": {
      "authors": [
        [
          "Mohamed",
          "Morchid"
        ]
      ],
      "title": "Internal Memory Gate for Recurrent Neural Networks with Application to Spoken Language Understanding",
      "original": "0357",
      "page_count": 4,
      "order": 685,
      "p1": "3316",
      "pn": "3319",
      "abstract": [
        "Long Short-Term Memory (LSTM) Recurrent Neural Networks (RNN) require\n4 gates to learn short- and long-term dependencies for a given sequence\nof basic elements. Recently, &#8220;Gated Recurrent Unit&#8221; (GRU)\nhas been introduced and requires fewer gates than LSTM (reset and update\ngates), to code short- and long-term dependencies and reaches equivalent\nperformances to LSTM, with less processing time during the learning.\nThe &#8220;Leaky integration Unit&#8221; (LU) is a GRU with a single\ngate (update) that codes mostly long-term dependencies quicker than\nLSTM or GRU (small number of operations for learning). This paper proposes\na novel RNN that takes advantage of LSTM, GRU (short- and long-term\ndependencies) and the LU (fast learning) called &#8220;Internal Memory\nGate&#8221; (IMG). The effectiveness and the robustness of the proposed\nIMG-RNN is evaluated during a classification task of a small corpus\nof spoken dialogues from the DECODA project that allows us to evaluate\nthe capability of each RNN to code short-term dependencies. The experiments\nshow that IMG-RNNs reach better accuracies with a gain of 0.4 points\ncompared to LSTM- and GRU-RNNs and 0.7 points compared to the LU-RNN.\nMoreover, IMG-RNN requires less processing time than GRU or LSTM with\na gain of 19% and 50% respectively.\n"
      ],
      "doi": "10.21437/Interspeech.2017-357"
    },
    "korpusik17_interspeech": {
      "authors": [
        [
          "Mandy",
          "Korpusik"
        ],
        [
          "Zachary",
          "Collins"
        ],
        [
          "James",
          "Glass"
        ]
      ],
      "title": "Character-Based Embedding Models and Reranking Strategies for Understanding Natural Language Meal Descriptions",
      "original": "0422",
      "page_count": 5,
      "order": 686,
      "p1": "3320",
      "pn": "3324",
      "abstract": [
        "Character-based embedding models provide robustness for handling misspellings\nand typos in natural language. In this paper, we explore convolutional\nneural network based embedding models for handling out-of-vocabulary\nwords in a meal description food ranking task. We demonstrate that\ncharacter-based models combined with a standard word-based model improves\nthe top-5 recall of USDA database food items from 26.3% to 30.3% on\na test set of all USDA foods with typos simulated in 10% of the data.\nWe also propose a new reranking strategy for predicting the top USDA\nfood matches given a meal description, which significantly outperforms\nour prior method of n-best decoding with a finite state transducer,\nimproving the top-5 recall on the all USDA foods task from 20.7% to\n63.8%.\n"
      ],
      "doi": "10.21437/Interspeech.2017-422"
    },
    "parcollet17_interspeech": {
      "authors": [
        [
          "Titouan",
          "Parcollet"
        ],
        [
          "Mohamed",
          "Morchid"
        ],
        [
          "Georges",
          "Linar\u00e8s"
        ]
      ],
      "title": "Quaternion Denoising Encoder-Decoder for Theme Identification of Telephone Conversations",
      "original": "1029",
      "page_count": 4,
      "order": 687,
      "p1": "3325",
      "pn": "3328",
      "abstract": [
        "In the last decades, encoder-decoders or autoencoders (AE) have received\na great interest from researchers due to their capability to construct\nrobust representations of documents in a low dimensional subspace.\nNonetheless, autoencoders reveal little in way of spoken document internal\nstructure by only considering words or topics contained in the document\nas an isolate basic element, and tend to overfit with small corpus\nof documents. Therefore, Quaternion Multi-layer Perceptrons (QMLP)\nhave been introduced to capture such internal latent dependencies,\nwhereas denoising autoencoders (DAE) are composed with different stochastic\nnoises to better process small set of documents. This paper presents\na novel autoencoder based on both hitherto-proposed DAE (to manage\nsmall corpus) and the QMLP (to consider internal latent structures)\ncalled &#8220;Quaternion denoising encoder-decoder&#8221; (QDAE). Moreover,\nthe paper defines an original angular Gaussian noise adapted to the\nspecificity of hyper-complex algebra. The experiments, conduced on\na theme identification task of spoken dialogues from the DECODA framework,\nshow that the QDAE obtains the promising gains of 3% and 1.5% compared\nto the standard real valued denoising autoencoder and the QMLP respectively.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1029"
    },
    "simonnet17_interspeech": {
      "authors": [
        [
          "Edwin",
          "Simonnet"
        ],
        [
          "Sahar",
          "Ghannay"
        ],
        [
          "Nathalie",
          "Camelin"
        ],
        [
          "Yannick",
          "Est\u00e8ve"
        ],
        [
          "Renato De",
          "Mori"
        ]
      ],
      "title": "ASR Error Management for Improving Spoken Language Understanding",
      "original": "1178",
      "page_count": 5,
      "order": 688,
      "p1": "3329",
      "pn": "3333",
      "abstract": [
        "This paper addresses the problem of automatic speech recognition (ASR)\nerror detection and their use for improving spoken language understanding\n(SLU) systems. In this study, the SLU task consists in automatically\nextracting, from ASR transcriptions, semantic concepts and concept/values\npairs in a  e.g touristic information system. An approach is proposed\nfor enriching the set of semantic labels with error specific labels\nand by using a recently proposed neural approach based on word embeddings\nto compute well calibrated ASR confidence measures. Experimental results\nare reported showing that it is possible to decrease significantly\nthe Concept/Value Error Rate with a state of the art system, outperforming\npreviously published results performance on the same experimental data.\nIt also shown that combining an SLU approach based on conditional random\nfields with a neural encoder/decoder attention based architecture,\nit is possible to effectively identifying confidence islands and uncertain\nsemantic output segments useful for deciding appropriate error handling\nactions by the dialogue manager strategy.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1178"
    },
    "ma17e_interspeech": {
      "authors": [
        [
          "Mingbo",
          "Ma"
        ],
        [
          "Kai",
          "Zhao"
        ],
        [
          "Liang",
          "Huang"
        ],
        [
          "Bing",
          "Xiang"
        ],
        [
          "Bowen",
          "Zhou"
        ]
      ],
      "title": "Jointly Trained Sequential Labeling and Classification by Sparse Attention Neural Networks",
      "original": "1321",
      "page_count": 5,
      "order": 689,
      "p1": "3334",
      "pn": "3338",
      "abstract": [
        "Sentence-level classification and sequential labeling are two fundamental\ntasks in language understanding. While these two tasks are usually\nmodeled separately, in reality, they are often correlated, for example\nin intent classification and slot filling, or in topic classification\nand named-entity recognition. In order to utilize the potential benefits\nfrom their correlations, we propose a jointly trained model for learning\nthe two tasks simultaneously via Long Short-Term Memory (LSTM) networks.\nThis model predicts the sentence-level category and the word-level\nlabel sequence from the stepwise output hidden representations of LSTM.\nWe also introduce a novel mechanism of &#8220;sparse attention&#8221;\nto weigh words differently based on their semantic relevance to sentence-level\nclassification. The proposed method outperforms baseline models on\nATIS and TREC datasets.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1321"
    },
    "nayak17_interspeech": {
      "authors": [
        [
          "Neha",
          "Nayak"
        ],
        [
          "Dilek",
          "Hakkani-T\u00fcr"
        ],
        [
          "Marilyn",
          "Walker"
        ],
        [
          "Larry",
          "Heck"
        ]
      ],
      "title": "To Plan or not to Plan? Discourse Planning in Slot-Value Informed Sequence to Sequence Models for Language Generation",
      "original": "1525",
      "page_count": 5,
      "order": 690,
      "p1": "3339",
      "pn": "3343",
      "abstract": [
        "Natural language generation for task-oriented dialogue systems aims\nto effectively realize system dialogue actions. All natural language\ngenerators (NLGs) must realize grammatical, natural and appropriate\noutput, but in addition, generators for task-oriented dialogue must\nfaithfully perform a specific dialogue act that conveys specific semantic\ninformation, as dictated by the dialogue policy of the system dialogue\nmanager. Most previous work on deep learning methods for task-oriented\nNLG assumes that generation output can be an utterance skeleton. Utterances\nare delexicalized, with variable names for slots, which are then replaced\nwith actual values as part of post-processing. However, the value of\nslots do, in fact, influence the lexical selection in the surrounding\ncontext as well as the overall sentence plan. To model this effect,\nwe investigate sequence-to-sequence (seq2seq) models in which slot\nvalues are included as part of the input sequence and the output surface\nform. Furthermore, we study whether a separate sentence planning module\nthat decides on grouping of slot value mentions as input to the seq2seq\nmodel results in more natural sentences than a seq2seq model that aims\nto jointly learn the plan and the surface realization.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1525"
    },
    "riou17_interspeech": {
      "authors": [
        [
          "Matthieu",
          "Riou"
        ],
        [
          "Bassam",
          "Jabaian"
        ],
        [
          "St\u00e9phane",
          "Huet"
        ],
        [
          "Fabrice",
          "Lef\u00e8vre"
        ]
      ],
      "title": "Online Adaptation of an Attention-Based Neural Network for Natural Language Generation",
      "original": "0921",
      "page_count": 5,
      "order": 691,
      "p1": "3344",
      "pn": "3348",
      "abstract": [
        "Following some recent propositions to handle natural language generation\nin spoken dialog systems with long short-term memory recurrent neural\nnetwork models [1] we first investigate a variant thereof with the\nobjective of a better integration of the attention subnetwork. Then\nour main objective is to propose and evaluate a framework to adapt\nthe NLG module online through direct interactions with the users. When\ndoing so the basic way is to ask the user to utter an alternative sentence\nto express a particular dialog act. But then the system has to decide\nbetween using an automatic transcription or to ask for a manual transcription.\nTo do so a reinforcement learning approach based on an adversarial\nbandit scheme is retained. We show that by defining appropriately the\nrewards as a linear combination of expected payoffs and costs of acquiring\nthe new data provided by the user, a system design can balance between\nimproving the system&#8217;s performance towards a better match with\nthe user&#8217;s preferences and the burden associated with it.\n"
      ],
      "doi": "10.21437/Interspeech.2017-921"
    },
    "martinezhinarejos17_interspeech": {
      "authors": [
        [
          "Carlos-D.",
          "Mart\u00ednez-Hinarejos"
        ],
        [
          "Zuzanna",
          "Parcheta"
        ]
      ],
      "title": "Spanish Sign Language Recognition with Different Topology Hidden Markov Models",
      "original": "0275",
      "page_count": 5,
      "order": 692,
      "p1": "3349",
      "pn": "3353",
      "abstract": [
        "Natural language recognition techniques can be applied not only to\nspeech signals, but to other signals that represent natural language\nunits (e.g., words and sentences). This is the case of sign language\nrecognition, which is usually employed by deaf people to communicate.\nThe use of recognition techniques may allow this language users to\ncommunicate more independently with non-signal users. Several works\nhave been done for different variants of sign languages, but in most\ncases their vocabulary is quite limited and they only recognise gestures\ncorresponding to isolated words. In this work, we propose gesture recognisers\nwhich make use of typical Continuous Density Hidden Markov Model. They\nsolve not only the isolated word problem, but also the recognition\nof basic sentences using the Spanish Sign Language with a higher vocabulary\nthan in other approximations. Different topologies and Gaussian mixtures\nare studied. Results show that our proposal provides promising results\nthat are the first step to obtain a general automatic recognition of\nSpanish Sign Language.\n"
      ],
      "doi": "10.21437/Interspeech.2017-275"
    },
    "morales17_interspeech": {
      "authors": [
        [
          "Michelle Renee",
          "Morales"
        ],
        [
          "Stefan",
          "Scherer"
        ],
        [
          "Rivka",
          "Levitan"
        ]
      ],
      "title": "OpenMM: An Open-Source Multimodal Feature Extraction Tool",
      "original": "1382",
      "page_count": 5,
      "order": 693,
      "p1": "3354",
      "pn": "3358",
      "abstract": [
        "The primary use of speech is in face-to-face interactions and situational\ncontext and human behavior therefore intrinsically shape and affect\ncommunication. In order to usefully model situational awareness, machines\nmust have access to the same streams of information humans have access\nto. In other words, we need to provide machines with features that\nrepresent each communicative modality: face and gesture, voice and\nspeech, and language. This paper presents OpenMM: an open-source multimodal\nfeature extraction tool. We build upon existing open-source repositories\nto present the first publicly available tool for multimodal feature\nextraction. The tool provides a pipeline for researchers to easily\nextract visual and acoustic features. In addition, the tool also performs\nautomatic speech recognition (ASR) and then uses the transcripts to\nextract linguistic features. We evaluate the OpenMM&#8217;s multimodal\nfeature set on deception, depression and sentiment classification tasks\nand show its performance is very promising. This tool provides researchers\nwith a simple way of extracting multimodal features and consequently\na richer and more robust feature representation for machine learning\ntasks.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1382"
    },
    "huang17f_interspeech": {
      "authors": [
        [
          "Yuyun",
          "Huang"
        ],
        [
          "Emer",
          "Gilmartin"
        ],
        [
          "Nick",
          "Campbell"
        ]
      ],
      "title": "Speaker Dependency Analysis, Audiovisual Fusion Cues and a Multimodal BLSTM for Conversational Engagement Recognition",
      "original": "1496",
      "page_count": 5,
      "order": 694,
      "p1": "3359",
      "pn": "3363",
      "abstract": [
        "Conversational engagement is a multimodal phenomenon and an essential\ncue to assess both human-human and human-robot communication. Speaker-dependent\nand speaker-independent scenarios were addressed in our engagement\nstudy. Handcrafted audio-visual features were used. Fixed window sizes\nfor feature fusion method were analysed. Novel dynamic window size\nselection and multimodal bi-directional long short term memory (Multimodal\nBLSTM) approaches were proposed and evaluated for engagement level\nrecognition.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1496"
    },
    "hsu17c_interspeech": {
      "authors": [
        [
          "Chin-Cheng",
          "Hsu"
        ],
        [
          "Hsin-Te",
          "Hwang"
        ],
        [
          "Yi-Chiao",
          "Wu"
        ],
        [
          "Yu",
          "Tsao"
        ],
        [
          "Hsin-Min",
          "Wang"
        ]
      ],
      "title": "Voice Conversion from Unaligned Corpora Using Variational Autoencoding Wasserstein Generative Adversarial Networks",
      "original": "0063",
      "page_count": 5,
      "order": 695,
      "p1": "3364",
      "pn": "3368",
      "abstract": [
        "Building a voice conversion (VC) system from non-parallel speech corpora\nis challenging but highly valuable in real application scenarios. In\nmost situations, the source and the target speakers do not repeat the\nsame texts or they may even speak different languages. In this case,\none possible, although indirect, solution is to build a generative\nmodel for speech. Generative models focus on explaining the observations\nwith latent variables instead of learning a pairwise transformation\nfunction, thereby bypassing the requirement of speech frame alignment.\nIn this paper, we propose a non-parallel VC framework with a variational\nautoencoding Wasserstein generative adversarial network (VAW-GAN) that\nexplicitly considers a VC objective when building the speech model.\nExperimental results corroborate the capability of our framework for\nbuilding a VC system from unaligned data, and demonstrate improved\nconversion quality.\n"
      ],
      "doi": "10.21437/Interspeech.2017-63"
    },
    "nakashika17_interspeech": {
      "authors": [
        [
          "Toru",
          "Nakashika"
        ]
      ],
      "title": "CAB: An Energy-Based Speaker Clustering Model for Rapid Adaptation in Non-Parallel Voice Conversion",
      "original": "0133",
      "page_count": 5,
      "order": 696,
      "p1": "3369",
      "pn": "3373",
      "abstract": [
        "In this paper, a new energy-based probabilistic model, called CAB (Cluster\nAdaptive restricted Boltzmann machine), is proposed for voice conversion\n(VC) that does not require parallel data during the training and requires\nonly a small amount of speech data during the adaptation. Most of the\nexisting VC methods require parallel data for training. Recently, VC\nmethods that do not require parallel data (called non-parallel VCs)\nhave been also proposed and are attracting much attention because they\ndo not require prepared or recorded parallel speech data, unlike conventional\napproaches. The proposed CAB model is aimed at statistical non-parallel\nVC based on cluster adaptive training (CAT). This extends the VC method\nused in our previous model, ARBM (adaptive restricted Boltzmann machine).\nThe ARBM approach assumes that any speech signals can be decomposed\ninto speaker-invariant phonetic information and speaker-identity information\nusing the ARBM adaptation matrices of each speaker. VC is achieved\nby switching the source speaker&#8217;s identity into those of the\ntarget speaker while retaining the phonetic information obtained by\ndecomposition of the source speaker&#8217;s speech. In contrast, CAB\nspeaker identities are represented as cluster vectors that determine\nthe adaptation matrices. As the number of clusters is generally smaller\nthan the number of speakers, the number of model parameters can be\nreduced compared to ARBM, which enables rapid adaptation of a new speaker.\nOur experimental results show that the proposed method especially performed\nbetter than the ARBM approach, particularly in adaptation.\n"
      ],
      "doi": "10.21437/Interspeech.2017-133"
    },
    "aihara17_interspeech": {
      "authors": [
        [
          "Ryo",
          "Aihara"
        ],
        [
          "Tetsuya",
          "Takiguchi"
        ],
        [
          "Yasuo",
          "Ariki"
        ]
      ],
      "title": "Phoneme-Discriminative Features for Dysarthric Speech Conversion",
      "original": "0664",
      "page_count": 5,
      "order": 697,
      "p1": "3374",
      "pn": "3378",
      "abstract": [
        "We present in this paper a Voice Conversion (VC) method for a person\nwith dysarthria resulting from athetoid cerebral palsy. VC is being\nwidely researched in the field of speech processing because of increased\ninterest in using such processing in applications such as personalized\nText-To-Speech systems. A Gaussian Mixture Model (GMM)-based VC method\nhas been widely researched and Partial Least Square (PLS)-based VC\nhas been proposed to prevent the over-fitting problems associated with\nthe GMM-based VC method. In this paper, we present phoneme-discriminative\nfeatures, which are associated with PLS-based VC. Conventional VC methods\ndo not consider the phonetic structure of spectral features although\nphonetic structures are important for speech analysis. Especially for\ndysarthric speech, their phonetic structures are difficult to discriminate\nand discriminative learning will improve the conversion accuracy. This\npaper employs discriminative manifold learning. Spectral features are\nprojected into a subspace in which a near point with the same phoneme\nlabel is close to another and a near point with a different phoneme\nlabel is apart. Our proposed method was evaluated on dysarthric speaker\nconversion task which converts dysarthric voice into non-dysarthric\nspeech.\n"
      ],
      "doi": "10.21437/Interspeech.2017-664"
    },
    "wu17f_interspeech": {
      "authors": [
        [
          "Jie",
          "Wu"
        ],
        [
          "D.-Y.",
          "Huang"
        ],
        [
          "Lei",
          "Xie"
        ],
        [
          "Haizhou",
          "Li"
        ]
      ],
      "title": "Denoising Recurrent Neural Network for Deep Bidirectional LSTM Based Voice Conversion",
      "original": "0694",
      "page_count": 5,
      "order": 698,
      "p1": "3379",
      "pn": "3383",
      "abstract": [
        "The paper studies the post processing in deep bidirectional Long Short-Term\nMemory (DBLSTM) based voice conversion, where the statistical parameters\nare optimized to generate speech that exhibits similar properties to\ntarget speech. However, there always exists residual error between\nconverted speech and target one. We reformulate the residual error\nproblem as speech restoration, which aims to recover the target speech\nsamples from the converted ones. Specifically, we propose a denoising\nrecurrent neural network (DeRNN) by introducing regularization during\ntraining to shape the distribution of the converted data in latent\nspace. We compare the proposed approach with global variance (GV),\nmodulation spectrum (MS) and recurrent neural network (RNN) based postfilters,\nwhich serve a similar purpose. The subjective test results show that\nthe proposed approach significantly outperforms these conventional\napproaches in terms of quality and similarity.\n"
      ],
      "doi": "10.21437/Interspeech.2017-694"
    },
    "tanaka17b_interspeech": {
      "authors": [
        [
          "Kei",
          "Tanaka"
        ],
        [
          "Sunao",
          "Hara"
        ],
        [
          "Masanobu",
          "Abe"
        ],
        [
          "Masaaki",
          "Sato"
        ],
        [
          "Shogo",
          "Minagi"
        ]
      ],
      "title": "Speaker Dependent Approach for Enhancing a Glossectomy Patient&#8217;s Speech via GMM-Based Voice Conversion",
      "original": "0841",
      "page_count": 5,
      "order": 699,
      "p1": "3384",
      "pn": "3388",
      "abstract": [
        "In this paper, using GMM-based voice conversion algorithm, we propose\nto generate speaker-dependent mapping functions to improve the intelligibility\nof speech uttered by patients with a wide glossectomy. The speaker-dependent\napproach enables to generate the mapping functions that reconstruct\nmissing spectrum features of speech uttered by a patient without having\ninfluences of a speaker&#8217;s factor. The proposed idea is simple,\ni.e., to collect speech uttered by a patient before and after the glossectomy,\nbut in practice it is hard to ask patients to utter speech just for\ndeveloping algorithms. To confirm the performance of the proposed approach,\nin this paper, in order to simulate glossectomy patients, we fabricated\nan intraoral appliance which covers lower dental arch and tongue surface\nto restrain tongue movements. In terms of the Mel-frequency cepstrum\n(MFC) distance, by applying the voice conversion, the distances were\nreduced by 25% and 42% for speaker-dependent case and speaker-independent\ncase, respectively. In terms of phoneme intelligibility, dictation\ntests revealed that speech reconstructed by speaker-dependent approach\nalmost always showed better performance than the original speech uttered\nby simulated patients, while speaker-independent approach did not.\n"
      ],
      "doi": "10.21437/Interspeech.2017-841"
    },
    "kaneko17c_interspeech": {
      "authors": [
        [
          "Takuhiro",
          "Kaneko"
        ],
        [
          "Shinji",
          "Takaki"
        ],
        [
          "Hirokazu",
          "Kameoka"
        ],
        [
          "Junichi",
          "Yamagishi"
        ]
      ],
      "title": "Generative Adversarial Network-Based Postfilter for STFT Spectrograms",
      "original": "0962",
      "page_count": 5,
      "order": 700,
      "p1": "3389",
      "pn": "3393",
      "abstract": [
        "We propose a learning-based postfilter to reconstruct the high-fidelity\nspectral texture in short-term Fourier transform (STFT) spectrograms.\nIn speech-processing systems, such as speech synthesis, conversion,\nenhancement, separation, and coding, STFT spectrograms have been widely\nused as key acoustic representations. In these tasks, we normally need\nto precisely generate or predict the representations from inputs; however,\ngenerated spectra typically lack the fine structures that are close\nto those of the true data. To overcome these limitations and reconstruct\nspectra having finer structures, we propose a generative adversarial\nnetwork (GAN)-based postfilter that is implicitly optimized to match\nthe true feature distribution in adversarial learning. The challenge\nwith this postfilter is that a GAN cannot be easily trained for very\nhigh-dimensional data such as STFT spectra. We take a simple divide-and-concatenate\nstrategy. Namely, we first divide the spectrograms into multiple frequency\nbands with overlap, reconstruct the individual bands using the GAN-based\npostfilter trained for each band, and finally connect the bands with\noverlap. We tested our proposed postfilter on a deep neural network-based\ntext-to-speech task and confirmed that it was able to reduce the gap\nbetween synthesized and target spectra, even in the high-dimensional\nSTFT domain.\n"
      ],
      "doi": "10.21437/Interspeech.2017-962"
    },
    "bollepalli17_interspeech": {
      "authors": [
        [
          "Bajibabu",
          "Bollepalli"
        ],
        [
          "Lauri",
          "Juvela"
        ],
        [
          "Paavo",
          "Alku"
        ]
      ],
      "title": "Generative Adversarial Network-Based Glottal Waveform Model for Statistical Parametric Speech Synthesis",
      "original": "1288",
      "page_count": 5,
      "order": 701,
      "p1": "3394",
      "pn": "3398",
      "abstract": [
        "Recent studies have shown that text-to-speech synthesis quality can\nbe improved by using glottal vocoding. This refers to vocoders that\nparameterize speech into two parts, the glottal excitation and vocal\ntract, that occur in the human speech production apparatus. Current\nglottal vocoders generate the glottal excitation waveform by using\ndeep neural networks (DNNs). However, the squared error-based training\nof the present glottal excitation models is limited to generating conditional\naverage waveforms, which fails to capture the stochastic variation\nof the waveforms. As a result, shaped noise is added as post-processing.\nIn this study, we propose a new method for predicting glottal waveforms\nby generative adversarial networks (GANs). GANs are generative models\nthat aim to embed the data distribution in a latent space, enabling\ngeneration of new instances very similar to the original by randomly\nsampling the latent distribution. The glottal pulses generated by GANs\nshow a stochastic component similar to natural glottal pulses. In our\nexperiments, we compare synthetic speech generated using glottal waveforms\nproduced by both DNNs and GANs. The results show that the newly proposed\nGANs achieve synthesis quality comparable to that of widely-used DNNs,\nwithout using an additive noise component.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1288"
    },
    "luo17c_interspeech": {
      "authors": [
        [
          "Zhaojie",
          "Luo"
        ],
        [
          "Jinhui",
          "Chen"
        ],
        [
          "Tetsuya",
          "Takiguchi"
        ],
        [
          "Yasuo",
          "Ariki"
        ]
      ],
      "title": "Emotional Voice Conversion with Adaptive Scales F0 Based on Wavelet Transform Using Limited Amount of Emotional Data",
      "original": "0984",
      "page_count": 5,
      "order": 702,
      "p1": "3399",
      "pn": "3403",
      "abstract": [
        "Deep learning techniques have been successfully applied to speech processing.\nTypically, neural networks (NNs) are very effective in processing nonlinear\nfeatures, such as mel cepstral coefficients (MCC), which represent\nthe spectrum features in voice conversion (VC) tasks. Despite these\nsuccesses, the approach is restricted to problems with moderate dimension\nand sufficient data. Thus, in emotional VC tasks, it is hard to deal\nwith a simple representation of fundamental frequency (F0), which is\nthe most important feature in emotional voice representation, Another\nproblem is that there are insufficient emotional data for training.\nTo deal with these two problems, in this paper, we propose the adaptive\nscales continuous wavelet transform (AS-CWT) method to systematically\ncapture the F0 features of different temporal scales, which can represent\ndifferent prosodic levels ranging from micro-prosody to sentence levels.\nMeanwhile, we also use the pre-trained conversion functions obtained\nfrom other emotional datasets to synthesize new emotional data as additional\ntraining samples for target emotional voice conversion. Experimental\nresults indicate that our proposed method achieves the best performance\nin both objective and subjective evaluations.\n"
      ],
      "doi": "10.21437/Interspeech.2017-984"
    },
    "doddipatla17_interspeech": {
      "authors": [
        [
          "Rama",
          "Doddipatla"
        ],
        [
          "Norbert",
          "Braunschweiler"
        ],
        [
          "Ranniery",
          "Maia"
        ]
      ],
      "title": "Speaker Adaptation in DNN-Based Speech Synthesis Using d-Vectors",
      "original": "1038",
      "page_count": 5,
      "order": 703,
      "p1": "3404",
      "pn": "3408",
      "abstract": [
        "The paper presents a mechanism to perform speaker adaptation in speech\nsynthesis based on deep neural networks (DNNs). The mechanism extracts\nspeaker identification vectors, so-called  d-vectors, from the training\nspeakers and uses them jointly with the linguistic features to train\na multi-speaker DNN-based text-to-speech synthesizer (DNN-TTS). The\n d-vectors are derived by applying principal component analysis (PCA)\non the bottle-neck features of a speaker classifier network. At the\nadaptation stage, three variants are explored: (1)  d-vectors calculated\nusing data from the target speaker, or (2)  d-vectors calculated as\na weighted sum of  d-vectors from training speakers, or (3)  d-vectors\ncalculated as an average of the above two approaches. The proposed\nmethod of unsupervised adaptation using the  d-vector is compared with\nthe commonly used  i-vector based approach for speaker adaptation.\nListening tests show that: (1) for speech quality, the  d-vector based\napproach is significantly preferred over the  i-vector based approach.\nAll the  d-vector variants perform similar for speech quality; (2)\nfor speaker similarity, both  d-vector and  i-vector based adaptation\nwere found to perform similar, except a small significant preference\nfor the  d-vector calculated as an average over the  i-vector.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1038"
    },
    "li17l_interspeech": {
      "authors": [
        [
          "Runnan",
          "Li"
        ],
        [
          "Zhiyong",
          "Wu"
        ],
        [
          "Yishuang",
          "Ning"
        ],
        [
          "Lifa",
          "Sun"
        ],
        [
          "Helen",
          "Meng"
        ],
        [
          "Lianhong",
          "Cai"
        ]
      ],
      "title": "Spectro-Temporal Modelling with Time-Frequency LSTM and Structured Output Layer for Voice Conversion",
      "original": "1122",
      "page_count": 5,
      "order": 704,
      "p1": "3409",
      "pn": "3413",
      "abstract": [
        "From speech, speaker identity can be mostly characterized by the spectro-temporal\nstructures of spectrum. Although recent researches have demonstrated\nthe effectiveness of employing long short-term memory (LSTM) recurrent\nneural network (RNN) in voice conversion, traditional LSTM-RNN based\napproaches usually focus on temporal evolutions of speech features\nonly. In this paper, we improve the conventional LSTM-RNN method for\nvoice conversion by employing the two-dimensional time-frequency LSTM\n(TFLSTM) to model spectro-temporal warping along both time and frequency\naxes. A multi-task learned structured output layer (SOL) is afterward\nadopted to capture the dependencies between spectral and pitch parameters\nfor further improvement, where spectral parameter targets are conditioned\nupon pitch parameters prediction. Experimental results show the proposed\napproach outperforms conventional systems in speech quality and speaker\nsimilarity.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1122"
    },
    "ramos17_interspeech": {
      "authors": [
        [
          "Miguel Varela",
          "Ramos"
        ],
        [
          "Alan W.",
          "Black"
        ],
        [
          "Ramon Fernandez",
          "Astudillo"
        ],
        [
          "Isabel",
          "Trancoso"
        ],
        [
          "Nuno",
          "Fonseca"
        ]
      ],
      "title": "Segment Level Voice Conversion with Recurrent Neural Networks",
      "original": "1538",
      "page_count": 5,
      "order": 705,
      "p1": "3414",
      "pn": "3418",
      "abstract": [
        "Voice conversion techniques aim to modify a subject&#8217;s voice characteristics\nin order to mimic the one&#8217;s of another person. Due to the difference\nin utterance length between source and target speaker, state of the\nart voice conversion systems often rely on a frame alignment pre-processing\nstep. This step aligns the entire utterances with algorithms such as\ndynamic time warping (DTW) that introduce errors, hindering system\nperformance. In this paper we present a new technique that avoids the\nalignment of entire utterances at frame level, while keeping the local\ncontext during training. For this purpose, we combine an RNN model\nwith the use of phoneme or syllable-level information, obtained from\na speech recognition system. This system segments the utterances into\nsegments which then can be grouped into overlapping windows, providing\nthe needed context for the model to learn the temporal dependencies.\nWe show that with this approach, notable improvements can be attained\nover a state of the art RNN voice conversion system on the CMU ARCTIC\ndatabase. It is also worth noting that with this technique it is possible\nto halve the training data size and still outperform the baseline.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1538"
    },
    "moore17_interspeech": {
      "authors": [
        [
          "Roger K.",
          "Moore"
        ],
        [
          "Ben",
          "Mitchinson"
        ]
      ],
      "title": "Creating a Voice for  MiRo, the World&#8217;s First Commercial Biomimetic Robot",
      "original": "2022",
      "page_count": 2,
      "order": 706,
      "p1": "3419",
      "pn": "3420",
      "abstract": [
        "This paper introduces  MiRo &#8212; the world&#8217;s first commercial\nbiomimetic robot &#8212; and describes how its vocal system was designed\nusing a real-time parametric general-purpose mammalian vocal synthesiser\ntailored to the specific physical characteristics of the robot.  MiRo&#8217;s\ncapabilities will be demonstrated live during the hands-on interactive\n&#8216;Show &amp; Tell&#8217; session at INTERSPEECH-2017.\n"
      ]
    },
    "dominguez17_interspeech": {
      "authors": [
        [
          "M\u00f3nica",
          "Dom\u00ednguez"
        ],
        [
          "Mireia",
          "Farr\u00fas"
        ],
        [
          "Leo",
          "Wanner"
        ]
      ],
      "title": "A Thematicity-Based Prosody Enrichment Tool for CTS",
      "original": "2023",
      "page_count": 2,
      "order": 707,
      "p1": "3421",
      "pn": "3422",
      "abstract": [
        "This paper presents a demonstration of a stochastic prosody tool for\nenrichment of synthesized speech using SSML prosody tags applied over\nhierarchical thematicity spans in the context of a CTS application.\nThe motivation for using hierarchical thematicity is exemplified, together\nwith the capabilities of the module to generate a variety of SSML prosody\ntags within a controlled range of values depending on the input thematicity\nlabel.\n"
      ]
    },
    "gruber17_interspeech": {
      "authors": [
        [
          "Martin",
          "Gr\u016fber"
        ],
        [
          "Jind\u0159ich",
          "Matou\u0161ek"
        ],
        [
          "Zden\u011bk",
          "Hanzl\u00ed\u010dek"
        ],
        [
          "Jakub",
          "V\u00edt"
        ],
        [
          "Daniel",
          "Tihelka"
        ]
      ],
      "title": "WebSubDub &#8212; Experimental System for Creating High-Quality Alternative Audio Track for TV Broadcasting",
      "original": "2024",
      "page_count": 2,
      "order": 708,
      "p1": "3423",
      "pn": "3424",
      "abstract": [
        "This paper deals with a presentation of an experimental system (called\n WebSubDub) for creating a high-quality alternative audio track for\nTV broadcasting. The system is used to create subtitles for TV shows\nin such a format which allows to automatically generate an alternative\naudio track with multiple voices employing a specially adapted TTS\nsystem. This alternative audio track is intended for televiewers with\nslight hearing impairments, i.e. for a group of televiewers who encounter\nissues when perceiving the original audio track &#8212; especially\ndialogues with background music, background noise or emotional speech.\nThe system was developed in cooperation with Czech television, the\npublic service broadcaster in the Czech Republic.\n"
      ]
    },
    "juzova17_interspeech": {
      "authors": [
        [
          "Mark\u00e9ta",
          "J\u016fzov\u00e1"
        ],
        [
          "Daniel",
          "Tihelka"
        ],
        [
          "Jind\u0159ich",
          "Matou\u0161ek"
        ],
        [
          "Zden\u011bk",
          "Hanzl\u00ed\u010dek"
        ]
      ],
      "title": "Voice Conservation and TTS System for People Facing Total Laryngectomy",
      "original": "2026",
      "page_count": 2,
      "order": 709,
      "p1": "3425",
      "pn": "3426",
      "abstract": [
        "The presented paper is focused on the building of personalized text-to-speech\n(TTS) synthesis for people who are losing their voices due to fatal\ndiseases. The special conditions of this issue make the process different\nfrom preparing professional synthetic voices for commercial TTS systems\nand make it also more difficult. The whole process is described in\nthis paper and the first results of the personalized voice building\nare presented here as well.\n"
      ]
    },
    "ghone17_interspeech": {
      "authors": [
        [
          "Atish Shankar",
          "Ghone"
        ],
        [
          "Rachana",
          "Nerpagar"
        ],
        [
          "Pranaw",
          "Kumar"
        ],
        [
          "Arun",
          "Baby"
        ],
        [
          "Aswin",
          "Shanmugam"
        ],
        [
          "Sasikumar",
          "M."
        ],
        [
          "Hema A.",
          "Murthy"
        ]
      ],
      "title": "TBT (Toolkit to Build TTS): A High Performance Framework to Build Multiple Language HTS Voice",
      "original": "2042",
      "page_count": 2,
      "order": 710,
      "p1": "3427",
      "pn": "3428",
      "abstract": [
        "With the development of high quality TTS systems, application area\nof synthetic speech is increasing rapidly. Beyond the communication\naids for the visually impaired and vocally handicap, TTS voices are\nbeing used in various educational, telecommunication and multimedia\napplications. All around the world people are trying to build TTS voice\nfor their regional languages. TTS voice building requires a number\nof steps to follow and involves use of multiple tools, which makes\nit time consuming, tedious and perplexing to a user. This paper describes\na Toolkit developed for HMM-based TTS voice building that makes the\nprocess much easier and handy. The toolkit uses all required tools,\nviz. HTS, Festival, Festvox, Hybrid Segmentation Tool, etc. and handles\neach and every step starting from phone set creation, then prompt generation,\nhybrid segmentation, F0 range finding, voice building, and finally\nputting the built voice into Synthesis framework. Wherever possible\nit does parallel processing to reduce time. It saves manual effort\nand time to a large extent and enable a person to build TTS voice very\neasily. This toolkit is made available under Open Source license.\n"
      ]
    },
    "karhila17_interspeech": {
      "authors": [
        [
          "Reima",
          "Karhila"
        ],
        [
          "Sari",
          "Ylinen"
        ],
        [
          "Seppo",
          "Enarvi"
        ],
        [
          "Kalle",
          "Palom\u00e4ki"
        ],
        [
          "Aleksander",
          "Nikulin"
        ],
        [
          "Olli",
          "Rantula"
        ],
        [
          "Vertti",
          "Viitanen"
        ],
        [
          "Krupakar",
          "Dhinakaran"
        ],
        [
          "Anna-Riikka",
          "Smolander"
        ],
        [
          "Heini",
          "Kallio"
        ],
        [
          "Katja",
          "Junttila"
        ],
        [
          "Maria",
          "Uther"
        ],
        [
          "Perttu",
          "H\u00e4m\u00e4l\u00e4inen"
        ],
        [
          "Mikko",
          "Kurimo"
        ]
      ],
      "title": "SIAK &#8212; A Game for Foreign Language Pronunciation Learning",
      "original": "2046",
      "page_count": 2,
      "order": 711,
      "p1": "3429",
      "pn": "3430",
      "abstract": [
        "We introduce a digital game for children&#8217;s foreign-language learning\nthat uses automatic speech recognition (ASR) for evaluating children&#8217;s\nutterances. Our first prototype focuses on the learning of English\nwords and their pronunciation. The game connects to a network server,\nwhich handles the recognition and pronunciation grading of children&#8217;s\nforeign-language speech. The server is reusable for different applications.\nGiven suitable acoustic models, it can be used for grading pronunciations\nin any language.\n"
      ]
    },
    "larsson17_interspeech": {
      "authors": [
        [
          "Staffan",
          "Larsson"
        ],
        [
          "Alex",
          "Berman"
        ],
        [
          "Andreas",
          "Krona"
        ],
        [
          "Fredrik",
          "Kronlid"
        ]
      ],
      "title": "Integrating the Talkamatic Dialogue Manager with Alexa",
      "original": "2029",
      "page_count": 2,
      "order": 712,
      "p1": "3431",
      "pn": "3432",
      "abstract": [
        "This paper describes the integration of Amazon Alexa with the Talkamatic\nDialogue Manager (TDM), and shows how flexible dialogue skills and\nrapid prototyping of dialogue apps can be brought to the Alexa platform.\n"
      ]
    },
    "ahmed17_interspeech": {
      "authors": [
        [
          "Farhia",
          "Ahmed"
        ],
        [
          "Pierrette",
          "Bouillon"
        ],
        [
          "Chelle",
          "Destefano"
        ],
        [
          "Johanna",
          "Gerlach"
        ],
        [
          "Sonia",
          "Halimi"
        ],
        [
          "Angela",
          "Hooper"
        ],
        [
          "Manny",
          "Rayner"
        ],
        [
          "Herv\u00e9",
          "Spechbach"
        ],
        [
          "Irene",
          "Strasly"
        ],
        [
          "Nikos",
          "Tsourakis"
        ]
      ],
      "title": "A Robust Medical Speech-to-Speech/Speech-to-Sign Phraselator",
      "original": "2031",
      "page_count": 2,
      "order": 713,
      "p1": "3433",
      "pn": "3434",
      "abstract": [
        "We present BabelDr, a web-enabled spoken-input phraselator for medical\ndomains, which has been developed at Geneva University in a collaboration\nbetween a human language technology group and a group at the University\nhospital. The current production version of the system translates French\ninto Arabic, using exclusively rule-based methods, and has performed\ncredibly in simulated triaging tests with standardised patients. We\nalso present an experimental version which combines large-vocabulary\nrecognition with the main rule-based recogniser; offline tests on unseen\ndata suggest that the new architecture adds robustness while more than\nhalving the 2-best semantic error rate. The experimental version translates\nfrom spoken English into spoken French and also two sign languages.\n"
      ]
    },
    "duckhorn17_interspeech": {
      "authors": [
        [
          "Frank",
          "Duckhorn"
        ],
        [
          "Markus",
          "Huber"
        ],
        [
          "Werner",
          "Meyer"
        ],
        [
          "Oliver",
          "Jokisch"
        ],
        [
          "Constanze",
          "Tsch\u00f6pe"
        ],
        [
          "Matthias",
          "Wolff"
        ]
      ],
      "title": "Towards an Autarkic Embedded Cognitive User Interface",
      "original": "2041",
      "page_count": 2,
      "order": 714,
      "p1": "3435",
      "pn": "3436",
      "abstract": [
        "With this paper we present an overview of an autarkic embedded cognitive\nuser interface. It is realized in form of an integrated device able\nto communicate with the user over speech &amp; gesture recognition,\nspeech synthesis and a touch display. Semantic processing and cognitive\nbehaviour control support intuitive interaction and help controlling\narbitrary electronic devices. To ensure user privacy and to operate\nautonomously of network access all information processing is done on\nthe device.\n"
      ]
    },
    "winata17_interspeech": {
      "authors": [
        [
          "Genta Indra",
          "Winata"
        ],
        [
          "Onno",
          "Kampman"
        ],
        [
          "Yang",
          "Yang"
        ],
        [
          "Anik",
          "Dey"
        ],
        [
          "Pascale",
          "Fung"
        ]
      ],
      "title": "Nora the Empathetic Psychologist",
      "original": "2050",
      "page_count": 2,
      "order": 715,
      "p1": "3437",
      "pn": "3438",
      "abstract": [
        "Nora is a new dialog system that mimics a conversation with a psychologist\nby screening for stress, anxiety, and depression. She understands,\nempathizes, and adapts to users using emotional intelligence modules\ntrained via statistical modelling such as Convolutional Neural Networks.\nThese modules also enable her to personalize the content of each conversation.\n"
      ]
    },
    "alam17_interspeech": {
      "authors": [
        [
          "Hassan",
          "Alam"
        ],
        [
          "Aman",
          "Kumar"
        ],
        [
          "Manan",
          "Vyas"
        ],
        [
          "Tina",
          "Werner"
        ],
        [
          "Rachmat",
          "Hartono"
        ]
      ],
      "title": "Modifying Amazon&#8217;s Alexa ASR Grammar and Lexicon &#8212; A Case Study",
      "original": "2057",
      "page_count": 2,
      "order": 716,
      "p1": "3439",
      "pn": "3440",
      "abstract": [
        "In this proof-of-concept study we build a tool that modifies the grammar\nand the dictionary of an Automatic Speech Recognition (ASR) engine.\nWe evaluated our tool using Amazon&#8217;s Alexa ASR engine. The experiments\nshow that with our grammar and dictionary modification algorithms in\nthe military domain, the accuracy of the modified ASR went up significantly\n&#8212; from 20/100 correct to 80/100 correct.\n"
      ]
    },
    "lindblom17_interspeech": {
      "authors": [
        [
          "Bj\u00f6rn",
          "Lindblom"
        ]
      ],
      "title": "Re-Inventing Speech &#8212; The Biological Way",
      "original": "3004",
      "page_count": 1,
      "order": 717,
      "p1": "3441",
      "pn": "3441",
      "abstract": [
        "The mapping of the Speech Chain has so far been focused on the experimentally\nmore accessible links &#8212; e.g., acoustics &#8212; whereas the brain&#8217;s\nactivity during speaking and listening has understandably received\nless attention. That state of affairs is about to change now thanks\nto the new sophisticated tools offered by brain imaging technology.<br\nstyle=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  At present many key questions concerning human speech processes\nremain incompletely understood despite the significant research efforts\nof the past half century. As speech research goes neuro, we could do\nwith some better answers.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  In this paper I will\nattempt to shed some light on some of the issues. I will do so by heeding\nthe advice that Tinbergen<SUP>1</SUP> once gave his fellow biologists\non explaining behavior. I paraphrase: Nothing in biology makes sense\nunless you simultaneously look at it with the following questions at\nthe back of your mind: How did it evolve? How is it acquired? How does\nit work here and now?<br style=\"mso-data-placement:same-cell;\" /><br\nstyle=\"mso-data-placement:same-cell;\" />\n  Applying the Tinbergen strategy\nto speech I will, in broad strokes, trace a path from the small and\nfixed innate repertoires of non-human primates to the open-ended vocal\nsystems that humans learn today.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  Such an agenda will\nadmittedly identify serious gaps in our present knowledge but, importantly,\nit will also bring an overarching possibility:<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n   It will strongly\nsuggest the feasibility of bypassing the traditional linguistic operational\napproach to speech units and replacing it by a first-principles account\nanchored in biology.<br style=\"mso-data-placement:same-cell;\" /><br\nstyle=\"mso-data-placement:same-cell;\" />\n  I will argue that this is\nthe road-map we need for a more profound understanding of the fundamental\nnature spoken language and for educational, medical and technological\napplications.\n"
      ]
    },
    "schuller17_interspeech": {
      "authors": [
        [
          "Bj\u00f6rn",
          "Schuller"
        ],
        [
          "Stefan",
          "Steidl"
        ],
        [
          "Anton",
          "Batliner"
        ],
        [
          "Elika",
          "Bergelson"
        ],
        [
          "Jarek",
          "Krajewski"
        ],
        [
          "Christoph",
          "Janott"
        ],
        [
          "Andrei",
          "Amatuni"
        ],
        [
          "Marisa",
          "Casillas"
        ],
        [
          "Amanda",
          "Seidl"
        ],
        [
          "Melanie",
          "Soderstrom"
        ],
        [
          "Anne S.",
          "Warlaumont"
        ],
        [
          "Guillermo",
          "Hidalgo"
        ],
        [
          "Sebastian",
          "Schnieder"
        ],
        [
          "Clemens",
          "Heiser"
        ],
        [
          "Winfried",
          "Hohenhorst"
        ],
        [
          "Michael",
          "Herzog"
        ],
        [
          "Maximilian",
          "Schmitt"
        ],
        [
          "Kun",
          "Qian"
        ],
        [
          "Yue",
          "Zhang"
        ],
        [
          "George",
          "Trigeorgis"
        ],
        [
          "Panagiotis",
          "Tzirakis"
        ],
        [
          "Stefanos",
          "Zafeiriou"
        ]
      ],
      "title": "The INTERSPEECH 2017 Computational Paralinguistics Challenge: Addressee, Cold &amp; Snoring",
      "original": "0043",
      "page_count": 5,
      "order": 718,
      "p1": "3442",
      "pn": "3446",
      "abstract": [
        "The INTERSPEECH 2017 Computational Paralinguistics Challenge addresses\nthree different problems for the first time in research competition\nunder well-defined conditions: In the  Addressee sub-challenge, it\nhas to be determined whether speech produced by an adult is directed\ntowards another adult or towards a child; in the  Cold sub-challenge,\nspeech under cold has to be told apart from &#8216;healthy&#8217; speech;\nand in the  Snoring sub-challenge, four different types of snoring\nhave to be classified. In this paper, we describe these sub-challenges,\ntheir conditions, and the baseline feature extraction and classifiers,\nwhich include data-learnt feature representations by end-to-end learning\nwith convolutional and recurrent neural networks, and bag-of-audio-words\nfor the first time in the challenge series.\n"
      ],
      "doi": "10.21437/Interspeech.2017-43"
    },
    "krajewski17_interspeech": {
      "authors": [
        [
          "Jarek",
          "Krajewski"
        ],
        [
          "Sebastian",
          "Schieder"
        ],
        [
          "Anton",
          "Batliner"
        ]
      ],
      "title": "Description of the Upper Respiratory Tract Infection Corpus (URTIC)",
      "original": "abs11",
      "page_count": 0,
      "order": 719,
      "p1": "0",
      "pn": "",
      "abstract": [
        "(No abstract available at the time of publication)\n"
      ]
    },
    "janott17_interspeech": {
      "authors": [
        [
          "Christoph",
          "Janott"
        ],
        [
          "Anton",
          "Batliner"
        ]
      ],
      "title": "Description of the Munich-Passau Snore Sound Corpus (MPSSC)",
      "original": "abs12",
      "page_count": 0,
      "order": 720,
      "p1": "0",
      "pn": "",
      "abstract": [
        "(No abstract available at the time of publication)\n"
      ]
    },
    "bergelson17_interspeech": {
      "authors": [
        [
          "Elika",
          "Bergelson"
        ],
        [
          "Andrei",
          "Amatuni"
        ],
        [
          "Marisa",
          "Casillas"
        ],
        [
          "Amanda",
          "Seidl"
        ],
        [
          "Melanie",
          "Soderstrom"
        ],
        [
          "Anne S.",
          "Warlaumont"
        ]
      ],
      "title": "Description of the Homebank Child/Adult Addressee Corpus (HB-CHAAC)",
      "original": "abs13",
      "page_count": 0,
      "order": 721,
      "p1": "0",
      "pn": "",
      "abstract": [
        "(No abstract available at the time of publication)\n"
      ]
    },
    "huckvale17_interspeech": {
      "authors": [
        [
          "Mark",
          "Huckvale"
        ],
        [
          "Andr\u00e1s",
          "Beke"
        ]
      ],
      "title": "It Sounds Like You Have a Cold! Testing Voice Features for the Interspeech 2017 Computational Paralinguistics Cold Challenge",
      "original": "1261",
      "page_count": 5,
      "order": 722,
      "p1": "3447",
      "pn": "3451",
      "abstract": [
        "This paper describes an evaluation of four different voice feature\nsets for detecting symptoms of the common cold in speech as part of\nthe Interspeech 2017 Computational Paralinguistics Challenge. The challenge\ncorpus consists of 630 speakers in three partitions, of which approximately\none third had a &#8220;severe&#8221; cold at the time of recording.\nSuccess on the task is measured in terms of unweighted average recall\nof cold/not-cold classification from short extracts of the recordings.\nIn this paper we review previous voice features used for studying changes\nin health and devise four basic types of features for evaluation: voice\nquality features, vowel spectra features, modulation spectra features,\nand spectral distribution features. The evaluation shows that each\nfeature set provides some useful information to the task, with features\nfrom the modulation spectrogram being most effective. Feature-level\nfusion of the feature sets shows small performance improvements on\nthe development test set. We discuss the results in terms of the most\nsuitable features for detecting symptoms of cold and address issues\narising from the design of the challenge.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1261"
    },
    "cai17b_interspeech": {
      "authors": [
        [
          "Danwei",
          "Cai"
        ],
        [
          "Zhidong",
          "Ni"
        ],
        [
          "Wenbo",
          "Liu"
        ],
        [
          "Weicheng",
          "Cai"
        ],
        [
          "Gang",
          "Li"
        ],
        [
          "Ming",
          "Li"
        ]
      ],
      "title": "End-to-End Deep Learning Framework for Speech Paralinguistics Detection Based on Perception Aware Spectrum",
      "original": "1445",
      "page_count": 5,
      "order": 723,
      "p1": "3452",
      "pn": "3456",
      "abstract": [
        "In this paper, we propose an end-to-end deep learning framework to\ndetect speech paralinguistics using perception aware spectrum as input.\nExisting studies show that speech under cold has distinct variations\nof energy distribution on low frequency components compared with the\nspeech under &#8216;healthy&#8217; condition. This motivates us to\nuse perception aware spectrum as the input to an end-to-end learning\nframework with small scale dataset. In this work, we try both Constant\nQ Transform (CQT) spectrum and Gammatone spectrum in different end-to-end\ndeep learning networks, where both spectrums are able to closely mimic\nthe human speech perception and transform it into 2D images. Experimental\nresults show the effectiveness of the proposed perception aware spectrum\nwith end-to-end deep learning approach on Interspeech 2017 Computational\nParalinguistics  Cold sub-Challenge. The final fusion result of our\nproposed method is 8% better than that of the provided baseline in\nterms of UAR.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1445"
    },
    "wagner17c_interspeech": {
      "authors": [
        [
          "Johannes",
          "Wagner"
        ],
        [
          "Thiago",
          "Fraga-Silva"
        ],
        [
          "Yvan",
          "Josse"
        ],
        [
          "Dominik",
          "Schiller"
        ],
        [
          "Andreas",
          "Seiderer"
        ],
        [
          "Elisabeth",
          "Andr\u00e9"
        ]
      ],
      "title": "Infected Phonemes: How a Cold Impairs Speech on a Phonetic Level",
      "original": "1066",
      "page_count": 5,
      "order": 724,
      "p1": "3457",
      "pn": "3461",
      "abstract": [
        "The realization of language through vocal sounds involves a complex\ninterplay between the lungs, the vocal cords, and a series of resonant\nchambers (e.g. mouth and nasal cavities). Due to their connection to\nthe outside world, these body parts are popular spots for viruses and\nbacteria to enter the human organism. Affected people may suffer from\nan upper respiratory tract infection (URTIC) and consequently their\nvoice often sounds breathy, raspy or sniffly. In this paper, we investigate\nthe audible effects of a cold on a phonetic level. Results on a German\ncorpus show that the articulation of consonants is more impaired than\nthat of vowels. Surprisingly, nasal sounds do not follow this trend\nin our experiments. We finally try to predict a speaker&#8217;s health\ncondition by fusing decisions we derive from single phonemes. The presented\nwork is part of the INTERSPEECH 2017 Computational Paralinguistics\nChallenge.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1066"
    },
    "suresh17_interspeech": {
      "authors": [
        [
          "Akshay Kalkunte",
          "Suresh"
        ],
        [
          "Srinivasa Raghavan",
          "K.M."
        ],
        [
          "Prasanta Kumar",
          "Ghosh"
        ]
      ],
      "title": "Phoneme State Posteriorgram Features for Speech Based Automatic Classification of Speakers in Cold and Healthy Condition",
      "original": "1550",
      "page_count": 5,
      "order": 725,
      "p1": "3462",
      "pn": "3466",
      "abstract": [
        "We consider the problem of automatically detecting if a speaker is\nsuffering from common cold from his/her speech. When a speaker has\nsymptoms of cold, his/her voice quality changes compared to the normal\none. We hypothesize that such a change in voice quality could be reflected\nin lower likelihoods from a model built using normal speech. In order\nto capture this, we compute a 120-dimensional posteriorgram feature\nin each frame using Gaussian mixture model from 120 states of 40 three-states\nphonetic hidden Markov models trained on approximately 16.4 hours of\nnormal English speech. Finally, a fixed 5160-dimensional phoneme state\nposteriorgram (PSP) feature vector for each utterance is obtained by\ncomputing statistics from the posteriorgram feature trajectory. Experiments\non the 2017-Cold sub-challenge data show that when the decisions from\nbag-of-audio-words (BoAW) and end-to-end (e2e) are combined with those\nfrom PSP features with unweighted majority rule, the UAR on the development\nset becomes 69% which is 2.9% (absolute) better than the best of the\nUARs obtained by the baseline schemes. When the decisions from ComParE,\nBoAW and PSP features are combined with simple majority rule, it results\nin a UAR of 68.52% on the test set.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1550"
    },
    "nwe17_interspeech": {
      "authors": [
        [
          "Tin Lay",
          "Nwe"
        ],
        [
          "Huy Dat",
          "Tran"
        ],
        [
          "Wen Zheng Terence",
          "Ng"
        ],
        [
          "Bin",
          "Ma"
        ]
      ],
      "title": "An Integrated Solution for Snoring Sound Classification Using Bhattacharyya Distance Based GMM Supervectors with SVM, Feature Selection with Random Forest and Spectrogram with CNN",
      "original": "1794",
      "page_count": 5,
      "order": 726,
      "p1": "3467",
      "pn": "3471",
      "abstract": [
        "Snoring is caused by the narrowing of the upper airway and it is excited\nby different locations within the upper airways. This irregularity\ncould lead to the presence of Obstructive Sleep Apnea Syndrome (OSAS).\nDiagnosis of OSAS could therefore be made by snoring sound analysis.\nThis paper proposes the novel method to automatically classify snoring\nsounds by their excitation locations for ComParE2017 challenge. We\npropose 3 sub-systems for classification. In the first system, we propose\nto integrate Bhattacharyya distance based Gaussian Mixture Model (GMM)\nsupervectors to a set of static features provided by ComParE2017 challenge.\nThe Bhattacharyya distance based GMM supervectors characterize the\nspectral dissimilarity measure among snore sounds excited by different\nlocations. And, we employ Support Vector Machine (SVM) for classification.\nIn the second system, we perform feature selection on static features\nprovided by the challenge and conduct classification using Random Forest.\nIn the third system, we extract spectrogram from audio and employ Convolutional\nNeural Network (CNN) for snore sound classification. Then, we fuse\n3 sub-systems to produce final classification results. The experimental\nresults show that the proposed system performs better than the challenge\nbaseline.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1794"
    },
    "kitamura17_interspeech": {
      "authors": [
        [
          "Tatsuya",
          "Kitamura"
        ],
        [
          "Hironori",
          "Takemoto"
        ],
        [
          "Hisanori",
          "Makinae"
        ],
        [
          "Tetsutaro",
          "Yamaguchi"
        ],
        [
          "Kotaro",
          "Maki"
        ]
      ],
      "title": "Acoustic Analysis of Detailed Three-Dimensional Shape of the Human Nasal Cavity and Paranasal Sinuses",
      "original": "0107",
      "page_count": 5,
      "order": 727,
      "p1": "3472",
      "pn": "3476",
      "abstract": [
        "The nasal and paranasal cavities have a labyrinthine shape and their\nacoustic properties affect speech sounds. In this study, we explored\nthe transfer function of the nasal and paranasal cavities, as well\nas the contribution of each paranasal cavity, using acoustical and\nnumerical methods. A physical model of the nasal and paranasal cavities\nwas formed using data from a high-resolution 3D X-ray CT and a 3D printer.\nThe data was acquired from a female subject during silent nasal breathing.\nThe transfer function of the physical model was then measured by introducing\na white noise signal at the glottis and measuring its acoustic response\nat a point 20 mm away from the nostrils. We also calculated the transfer\nfunction of the 3D model using a finite-difference time-domain or FDTD\nmethod. The results showed that the gross shape and the frequency of\npeaks and dips of the measured and calculated transfer functions were\nsimilar, suggesting that both methods used in this study were reliable.\nThe results of FDTD simulations evaluating the paranasal sinuses individually\nsuggested that they contribute not only to spectral dips but also to\npeaks, which is contrary to the traditional theories regarding the\nproduction of speech sounds.\n"
      ],
      "doi": "10.21437/Interspeech.2017-107"
    },
    "arnela17_interspeech": {
      "authors": [
        [
          "Marc",
          "Arnela"
        ],
        [
          "Saeed",
          "Dabbaghchian"
        ],
        [
          "Oriol",
          "Guasch"
        ],
        [
          "Olov",
          "Engwall"
        ]
      ],
      "title": "A Semi-Polar Grid Strategy for the Three-Dimensional Finite Element Simulation of Vowel-Vowel Sequences",
      "original": "0448",
      "page_count": 5,
      "order": 728,
      "p1": "3477",
      "pn": "3481",
      "abstract": [
        "Three-dimensional computational acoustic models need very detailed\n3D vocal tract geometries to generate high quality sounds. Static geometries\ncan be obtained from Magnetic Resonance Imaging (MRI), but it is not\ncurrently possible to capture dynamic MRI-based geometries with sufficient\nspatial and time resolution. One possible solution consists in interpolating\nbetween static geometries, but this is a complex task. We instead propose\nherein to use a semi-polar grid to extract 2D cross-sections from the\nstatic 3D geometries, and then interpolate them to obtain the vocal\ntract dynamics. Other approaches such as the adaptive grid have also\nbeen explored. In this method, cross-sections are defined perpendicular\nto the vocal tract midline, as typically done in 1D to obtain the vocal\ntract area functions. However, intersections between adjacent cross-sections\nmay occur during the interpolation process, especially when the vocal\ntract midline quickly changes its orientation. In contrast, the semi-polar\ngrid prevents these intersections because the plane orientations are\nfixed over time. Finite element simulations of static vowels are first\nconducted, showing that 3D acoustic wave propagation is not significantly\naltered when the semi-polar grid is used instead of the adaptive grid.\nThe vowel-vowel sequence [&#593;i] is finally simulated to demonstrate\nthe method.\n"
      ],
      "doi": "10.21437/Interspeech.2017-448"
    },
    "vasudevan17_interspeech": {
      "authors": [
        [
          "Arvind",
          "Vasudevan"
        ],
        [
          "Victor",
          "Zappi"
        ],
        [
          "Peter",
          "Anderson"
        ],
        [
          "Sidney",
          "Fels"
        ]
      ],
      "title": "A Fast Robust 1D Flow Model for a Self-Oscillating Coupled 2D FEM Vocal Fold Simulation",
      "original": "0844",
      "page_count": 5,
      "order": 729,
      "p1": "3482",
      "pn": "3486",
      "abstract": [
        "A balance between the simplicity and speed of lumped-element vocal\nfold models and the completeness and complexity of continuum-models\nis required to achieve fast high-quality articulatory speech synthesis.\nWe develop and implement a novel self-oscillating vocal-fold model,\ncomposed of a 1D unsteady fluid model loosely coupled with a 2D FEM\nstructural model. The flow model is capable of robustly handling irregular\ngeometries, different boundary conditions, closure of the glottis and\nunsteady flow states. A method for a fast decoupled solution of the\nflow equations that does not require the computation of the Jacobian\nis provided. The model is coupled with a 2D real-time finite-difference\nwave-solver for simulating vocal tract acoustics and a 1D wave-reflection\nanalog representation of the trachea. The simulation results are shown\nto agree with existing data in literature, and give realistic pressure-velocity\ndistributions, glottal width and glottal flow values. In addition,\nthe model is more than an order of magnitude faster to run than comparable\n2D Navier-Stokes fluid solvers, while better capturing transitional\nflow than simple Bernoulli-based flow models. The vocal fold model\nprovides an alternative to simple lumped-element models for faster\nhigher-quality articulatory speech synthesis.\n"
      ],
      "doi": "10.21437/Interspeech.2017-844"
    },
    "murtola17_interspeech": {
      "authors": [
        [
          "Tiina",
          "Murtola"
        ],
        [
          "Jarmo",
          "Malinen"
        ]
      ],
      "title": "Waveform Patterns in Pitch Glides Near a Vocal Tract Resonance",
      "original": "0875",
      "page_count": 5,
      "order": 730,
      "p1": "3487",
      "pn": "3491",
      "abstract": [
        "A time-domain model of vowel production is used to simulate fundamental\nfrequency glides over the first vocal tract resonance. A vocal tract\ngeometry extracted from MRI data of a female speaker pronouncing [i]\nis used. The model contains direct feedback from the acoustic loads\nto vocal fold tissues and the inertial effect of the full air column\non the glottal flow. The simulations reveal that a perturbation pattern\nin the fundamental frequency, namely, a jump and locking to the vocal\ntract resonance, is accompanied by a specific pattern of glottal waveform\nchanges.\n"
      ],
      "doi": "10.21437/Interspeech.2017-875"
    },
    "degirmenci17_interspeech": {
      "authors": [
        [
          "Niyazi Cem",
          "Degirmenci"
        ],
        [
          "Johan",
          "Jansson"
        ],
        [
          "Johan",
          "Hoffman"
        ],
        [
          "Marc",
          "Arnela"
        ],
        [
          "Patricia",
          "S\u00e1nchez-Mart\u00edn"
        ],
        [
          "Oriol",
          "Guasch"
        ],
        [
          "Sten",
          "Ternstr\u00f6m"
        ]
      ],
      "title": "A Unified Numerical Simulation of Vowel Production That Comprises Phonation and the Emitted Sound",
      "original": "1239",
      "page_count": 5,
      "order": 731,
      "p1": "3492",
      "pn": "3496",
      "abstract": [
        "A unified approach for the numerical simulation of vowels is presented,\nwhich accounts for the self-oscillations of the vocal folds including\ncontact, the generation of acoustic waves and their propagation through\nthe vocal tract, and the sound emission outwards the mouth. A monolithic\nincompressible fluid-structure interaction model is used to simulate\nthe interaction between the glottal jet and the vocal folds, whereas\nthe contact model is addressed by means of a level set application\nof the Eikonal equation. The coupling with acoustics is done through\nan acoustic analogy stemming from a simplification of the acoustic\nperturbation equations. This coupling is one-way in the sense that\nthere is no feedback from the acoustics to the flow and mechanical\nfields.<br style=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  All the involved equations are solved together at each time step\nand in a single computational run, using the finite element method\n(FEM). As an application, the production of vowel [i] has been addressed.\nDespite the complexity of all physical phenomena to be simulated simultaneously,\nwhich requires resorting to massively parallel computing, the formant\nlocations of vowel [i] have been well recovered.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1239"
    },
    "dabbaghchian17_interspeech": {
      "authors": [
        [
          "Saeed",
          "Dabbaghchian"
        ],
        [
          "Marc",
          "Arnela"
        ],
        [
          "Olov",
          "Engwall"
        ],
        [
          "Oriol",
          "Guasch"
        ]
      ],
      "title": "Synthesis of VV Utterances from Muscle Activation to Sound with a 3D Model",
      "original": "1614",
      "page_count": 5,
      "order": 732,
      "p1": "3497",
      "pn": "3501",
      "abstract": [
        "We propose a method to automatically generate deformable 3D vocal tract\ngeometries from the surrounding structures in a biomechanical model.\nThis allows us to couple 3D biomechanics and acoustics simulations.\nThe basis of the simulations is muscle activation trajectories in the\nbiomechanical model, which move the articulators to the desired articulatory\npositions. The muscle activation trajectories for a vowel-vowel utterance\nare here defined through interpolation between the determined activations\nof the start and end vowel. The resulting articulatory trajectories\nof flesh points on the tongue surface and jaw are similar to corresponding\ntrajectories measured using Electromagnetic Articulography, hence corroborating\nthe validity of interpolating muscle activation. At each time step\nin the articulatory transition, a 3D vocal tract tube is created through\na cavity extraction method based on first slicing the geometry of the\narticulators with a semi-polar grid to extract the vocal tract contour\nin each plane and then reconstructing the vocal tract through a smoothed\n3D mesh-generation using the extracted contours. A finite element method\napplied to these changing 3D geometries simulates the acoustic wave\npropagation. We present the resulting acoustic pressure changes on\nthe vocal tract boundary and the formant transitions for the utterance\n[&#593;i].\n"
      ],
      "doi": "10.21437/Interspeech.2017-1614"
    },
    "mv17_interspeech": {
      "authors": [
        [
          "Achuth Rao",
          "M.V."
        ],
        [
          "Shivani",
          "Yadav"
        ],
        [
          "Prasanta Kumar",
          "Ghosh"
        ]
      ],
      "title": "A Dual Source-Filter Model of Snore Audio for Snorer Group Classification",
      "original": "1211",
      "page_count": 5,
      "order": 733,
      "p1": "3502",
      "pn": "3506",
      "abstract": [
        "Snoring is a common symptom of serious chronic disease known as obstructive\nsleep apnea (OSA). Knowledge about the location of obstruction site\n(V&#8212;Velum, O&#8212;Oropharyngeal lateral walls, T&#8212;Tongue,\nE&#8212;Epiglottis) in the upper airways is necessary for proper surgical\ntreatment. In this paper we propose a dual source-filter model similar\nto the source-filter model of speech to approximate the generation\nprocess of snore audio. The first filter models the vocal tract from\nlungs to the point of obstruction with white noise excitation from\nthe lungs. The second filter models the vocal tract from the obstruction\npoint to the lips/nose with impulse train excitation which represents\nvibrations at the point of obstruction. The filter coefficients are\nestimated using the closed and open phases of the snore beat cycle.\nVOTE classification is done by using SVM classifier and filter coefficients\nas features. The classification experiments are performed on the development\nset (283 snore audios) of the MUNICH-PASSAU SNORE SOUND CORPUS (MPSSC).We\nobtain an unweighted average recall (UAR) of 49.58%, which is higher\nthan the INTERSPEECH-2017 snoring sub-challenge baseline technique\nby &#126;3% (absolute).\n"
      ],
      "doi": "10.21437/Interspeech.2017-1211"
    },
    "freitag17_interspeech": {
      "authors": [
        [
          "Michael",
          "Freitag"
        ],
        [
          "Shahin",
          "Amiriparian"
        ],
        [
          "Nicholas",
          "Cummins"
        ],
        [
          "Maurice",
          "Gerczuk"
        ],
        [
          "Bj\u00f6rn",
          "Schuller"
        ]
      ],
      "title": "An &#8216;End-to-Evolution&#8217; Hybrid Approach for Snore Sound Classification",
      "original": "0173",
      "page_count": 5,
      "order": 734,
      "p1": "3507",
      "pn": "3511",
      "abstract": [
        "Whilst snoring itself is usually not harmful to a person&#8217;s health,\nit can be an indication of Obstructive Sleep Apnoea (OSA), a serious\nsleep-related disorder. As a result, studies into using snoring as\nacoustic based marker of OSA are gaining in popularity. Motivated by\nthis, the INTERSPEECH 2017 ComParE Snoring sub-challenge requires classification\nfrom which areas in the upper airways different snoring sounds originate.\nThis paper explores a hybrid approach combining evolutionary feature\nselection based on competitive swarm optimisation and deep convolutional\nneural networks (CNN). Feature selection is applied to novel deep spectrum\nfeatures extracted directly from spectrograms using pre-trained image\nclassification CNN. Key results presented demonstrate that our hybrid\napproach can substantially increase the performance of a linear support\nvector machine on a set of low-level features extracted from the Snoring\nsub-challenge data. Even without subset selection, the deep spectrum\nfeatures are sufficient to outperform the challenge baseline, and competitive\nswarm optimisation further improves system performance. In comparison\nto the challenge baseline, unweighted average recall is increased from\n40.6% to 57.6% on the development partition, and from 58.5% to 66.5%\non the test partition, using 2246 of the 4096 deep spectrum features.\n"
      ],
      "doi": "10.21437/Interspeech.2017-173"
    },
    "amiriparian17_interspeech": {
      "authors": [
        [
          "Shahin",
          "Amiriparian"
        ],
        [
          "Maurice",
          "Gerczuk"
        ],
        [
          "Sandra",
          "Ottl"
        ],
        [
          "Nicholas",
          "Cummins"
        ],
        [
          "Michael",
          "Freitag"
        ],
        [
          "Sergey",
          "Pugachevskiy"
        ],
        [
          "Alice",
          "Baird"
        ],
        [
          "Bj\u00f6rn",
          "Schuller"
        ]
      ],
      "title": "Snore Sound Classification Using Image-Based Deep Spectrum Features",
      "original": "0434",
      "page_count": 5,
      "order": 735,
      "p1": "3512",
      "pn": "3516",
      "abstract": [
        "In this paper, we propose a method for automatically detecting various\ntypes of snore sounds using image classification convolutional neural\nnetwork (CNN) descriptors extracted from audio file spectrograms. The\ndescriptors, denoted as deep spectrum features, are derived from forwarding\nspectrograms through very deep task-independent pre-trained CNNs. Specifically,\nactivations of fully connected layers from two common image classification\nCNNs, AlexNet and VGG19, are used as feature vectors. Moreover, we\ninvestigate the impact of differing spectrogram colour maps and two\nCNN architectures on the performance of the system. Results presented\nindicate that deep spectrum features extracted from the activations\nof the second fully connected layer of AlexNet using a viridis colour\nmap are well suited to the task. This feature space, when combined\nwith a support vector classifier, outperforms the more conventional\nknowledge-based features of 6 373 acoustic functionals used in the\nINTERSPEECH ComParE 2017 Snoring sub-challenge baseline system. In\ncomparison to the baseline, unweighted average recall is increased\nfrom 40.6% to 44.8% on the development partition, and from 58.5% to\n67.0% on the test partition.\n"
      ],
      "doi": "10.21437/Interspeech.2017-434"
    },
    "tavarez17_interspeech": {
      "authors": [
        [
          "David",
          "Tavarez"
        ],
        [
          "Xabier",
          "Sarasola"
        ],
        [
          "Agustin",
          "Alonso"
        ],
        [
          "Jon",
          "Sanchez"
        ],
        [
          "Luis",
          "Serrano"
        ],
        [
          "Eva",
          "Navas"
        ],
        [
          "Inma",
          "Hern\u00e1ez"
        ]
      ],
      "title": "Exploring Fusion Methods and Feature Space for the Classification of Paralinguistic Information",
      "original": "1378",
      "page_count": 5,
      "order": 736,
      "p1": "3517",
      "pn": "3521",
      "abstract": [
        "This paper introduces the different systems developed by Aholab Signal\nProcessing Laboratory for The INTERSPEECH 2017 Computational Paralinguistics\nChallenge, which includes three different subtasks: Addressee, Cold\nand Snoring classification. Several classification strategies and features\nrelated with the spectrum, prosody and phase have been tested separately\nand further combined by using different fusion techniques, such as\nearly fusion by means of multi-feature vectors, late fusion of the\nstandalone classifier scores and label fusion via weighted voting.\nThe obtained results show that the applied fusion methods improve the\nperformance of the standalone detectors and provide systems capable\nof outperforming the baseline systems in terms of UAR.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1378"
    },
    "gosztolya17b_interspeech": {
      "authors": [
        [
          "G\u00e1bor",
          "Gosztolya"
        ],
        [
          "R\u00f3bert",
          "Busa-Fekete"
        ],
        [
          "Tam\u00e1s",
          "Gr\u00f3sz"
        ],
        [
          "L\u00e1szl\u00f3",
          "T\u00f3th"
        ]
      ],
      "title": "DNN-Based Feature Extraction and Classifier Combination for Child-Directed Speech, Cold and Snoring Identification",
      "original": "0905",
      "page_count": 5,
      "order": 737,
      "p1": "3522",
      "pn": "3526",
      "abstract": [
        "In this study we deal with the three sub-challenges of the Interspeech\nComParE Challenge 2017, where the goal is to identify child-directed\nspeech, speakers having a cold, and different types of snoring sounds.\nFor the first two sub-challenges we propose a simple, two-step feature\nextraction and classification scheme: first we perform frame-level\nclassification via Deep Neural Networks (DNNs), and then we extract\nutterance-level features from the DNN outputs. By utilizing these features\nfor classification, we were able to match the performance of the standard\nparalinguistic approach (which involves extracting thousands of features,\nmany of them being completely irrelevant to the actual task). As for\nthe Snoring Sub-Challenge, we divided the recordings into segments,\nand averaged out some frame-level features segment-wise, which were\nthen used for utterance-level classification. When combining the predictions\nof the proposed approaches with those got by the standard paralinguistic\napproach, we managed to outperform the baseline values of the Cold\nand Snoring sub-challenges on the hidden test sets.\n"
      ],
      "doi": "10.21437/Interspeech.2017-905"
    },
    "kaya17_interspeech": {
      "authors": [
        [
          "Heysem",
          "Kaya"
        ],
        [
          "Alexey A.",
          "Karpov"
        ]
      ],
      "title": "Introducing Weighted Kernel Classifiers for Handling Imbalanced Paralinguistic Corpora: Snoring, Addressee and Cold",
      "original": "0653",
      "page_count": 5,
      "order": 738,
      "p1": "3527",
      "pn": "3531",
      "abstract": [
        "The field of paralinguistics is growing rapidly with a wide range of\napplications that go beyond recognition of emotions, laughter and personality.\nThe research flourishes in multiple directions such as signal representation\nand classification, addressing the issues of the domain. Apart from\nthe noise robustness, an important issue with real life data is the\nimbalanced nature: some classes of states/traits are under-represented.\nCombined with the high dimensionality of the feature vectors used in\nthe state-of-the-art analysis systems, this issue poses the threat\nof over-fitting. While the kernel trick can be employed to handle the\ndimensionality issue, regular classifiers inherently aim to minimize\nthe misclassification error and hence are biased towards the majority\nclass. A solution to this problem is over-sampling of the minority\nclass(es). However, this brings increased memory/computational costs,\nwhile not bringing any new information to the classifier. In this work,\nwe propose a new weighting scheme on instances of the original dataset,\nemploying Weighted Kernel Extreme Learning Machine, and inspired from\nthat, introducing the Weighted Partial Least Squares Regression based\nclassifier. The proposed methods are applied on all three INTERSPEECH\nComParE 2017 challenge corpora, giving better or competitive results\ncompared to the challenge baselines.\n"
      ],
      "doi": "10.21437/Interspeech.2017-653"
    },
    "steidl17_interspeech": {
      "authors": [
        [
          "Stefan",
          "Steidl"
        ]
      ],
      "title": "The INTERSPEECH 2017 Computational Paralinguistics Challenge: A Summary of Results",
      "original": "abs14",
      "page_count": 0,
      "order": 739,
      "p1": "0",
      "pn": "",
      "abstract": [
        "(No abstract available at the time of publication)\n"
      ]
    },
    "schuller17b_interspeech": {
      "authors": [
        [
          "Bj\u00f6rn",
          "Schuller"
        ],
        [
          "Anton",
          "Batliner"
        ]
      ],
      "title": "Discussion",
      "original": "abs15",
      "page_count": 0,
      "order": 740,
      "p1": "0",
      "pn": "",
      "abstract": [
        "(No abstract available at the time of publication)\n"
      ]
    },
    "toshniwal17_interspeech": {
      "authors": [
        [
          "Shubham",
          "Toshniwal"
        ],
        [
          "Hao",
          "Tang"
        ],
        [
          "Liang",
          "Lu"
        ],
        [
          "Karen",
          "Livescu"
        ]
      ],
      "title": "Multitask Learning with Low-Level Auxiliary Tasks for Encoder-Decoder Based Speech Recognition",
      "original": "1118",
      "page_count": 5,
      "order": 741,
      "p1": "3532",
      "pn": "3536",
      "abstract": [
        "End-to-end training of deep learning-based models allows for implicit\nlearning of intermediate representations based on the final task loss.\nHowever, the end-to-end approach ignores the useful domain knowledge\nencoded in explicit intermediate-level supervision. We hypothesize\nthat using intermediate representations as auxiliary supervision at\nlower levels of deep networks may be a good way of combining the advantages\nof end-to-end training and more traditional pipeline approaches. We\npresent experiments on conversational speech recognition where we use\nlower-level tasks, such as phoneme recognition, in a multitask training\napproach with an encoder-decoder model for direct character transcription.\nWe compare multiple types of lower-level tasks and analyze the effects\nof the auxiliary tasks. Our results on the Switchboard corpus show\nthat this approach improves recognition accuracy over a standard encoder-decoder\nmodel on the Eval2000 test set.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1118"
    },
    "shannon17b_interspeech": {
      "authors": [
        [
          "Matt",
          "Shannon"
        ]
      ],
      "title": "Optimizing Expected Word Error Rate via Sampling for Speech Recognition",
      "original": "0639",
      "page_count": 5,
      "order": 742,
      "p1": "3537",
      "pn": "3541",
      "abstract": [
        "State-level minimum Bayes risk (sMBR) training has become the de facto\nstandard for sequence-level training of speech recognition acoustic\nmodels. It has an elegant formulation using the expectation semiring,\nand gives large improvements in word error rate (WER) over models trained\nsolely using cross-entropy (CE) or connectionist temporal classification\n(CTC). sMBR training optimizes the expected number of frames at which\nthe reference and hypothesized acoustic states differ. It may be preferable\nto optimize the expected WER, but WER does not interact well with the\nexpectation semiring, and previous approaches based on computing expected\nWER exactly involve expanding the lattices used during training. In\nthis paper we show how to perform optimization of the expected WER\nby sampling paths from the lattices used during conventional sMBR training.\nThe gradient of the expected WER is itself an expectation, and so may\nbe approximated using Monte Carlo sampling. We show experimentally\nthat optimizing WER during acoustic model training gives 5% relative\nimprovement in WER over a well-tuned sMBR baseline on a 2-channel query\nrecognition task (Google Home).\n"
      ],
      "doi": "10.21437/Interspeech.2017-639"
    },
    "sainath17_interspeech": {
      "authors": [
        [
          "Tara N.",
          "Sainath"
        ],
        [
          "Vijayaditya",
          "Peddinti"
        ],
        [
          "Olivier",
          "Siohan"
        ],
        [
          "Arun",
          "Narayanan"
        ]
      ],
      "title": "Annealed f-Smoothing as a Mechanism to Speed up Neural Network Training",
      "original": "0231",
      "page_count": 5,
      "order": 743,
      "p1": "3542",
      "pn": "3546",
      "abstract": [
        "In this paper, we describe a method to reduce the overall number of\nneural network training steps, during both cross-entropy and sequence\ntraining stages. This is achieved through the interpolation of frame-level\nCE and sequence level SMBR criteria, during the sequence training stage.\nThis interpolation is known as f-smoothing and has previously been\njust used to prevent overfitting during sequence training. However,\nin this paper, we investigate its application to reduce the training\ntime. We explore different interpolation strategies to reduce the overall\ntraining steps; and achieve a reduction of up to 25% with almost no\ndegradation in word error rate (WER). Finally, we explore the generalization\nof f-smoothing to other tasks.\n"
      ],
      "doi": "10.21437/Interspeech.2017-231"
    },
    "meng17b_interspeech": {
      "authors": [
        [
          "Zhong",
          "Meng"
        ],
        [
          "Biing-Hwang",
          "Juang"
        ]
      ],
      "title": "Non-Uniform MCE Training of Deep Long Short-Term Memory Recurrent Neural Networks for Keyword Spotting",
      "original": "0583",
      "page_count": 5,
      "order": 744,
      "p1": "3547",
      "pn": "3551",
      "abstract": [
        "It has been shown in [1, 2] that improved performance can be achieved\nby formulating the keyword spotting as a non-uniform error automatic\nspeech recognition problem. In this work, we discriminatively train\na deep bidirectional long short-term memory (BLSTM) &#8212; hidden\nMarkov model (HMM) based acoustic model with non-uniform boosted minimum\nclassification error (BMCE) criterion which imposes more significant\nerror cost on the keywords than those on the non-keywords. By introducing\nthe BLSTM, the context information in both the past and the future\nare stored and updated to predict the desired output and the long-term\ndependencies within the speech signal are well captured. With non-uniform\nBMCE objective, the BLSTM is trained so that the recognition errors\nrelated to the keywords are remarkably reduced. The BLSTM is optimized\nusing back-propagation through time and stochastic gradient descent.\nThe keyword spotting system is implemented within weighted finite state\ntransducer framework. The proposed method achieves 5.49% and 7.37%\nabsolute figure-of-merit improvements respectively over the BLSTM and\nthe feedforward deep neural network baseline systems trained with cross-entropy\ncriterion for the keyword spotting task on Switchboard-1 Release 2\ndataset.\n"
      ],
      "doi": "10.21437/Interspeech.2017-583"
    },
    "dighe17_interspeech": {
      "authors": [
        [
          "Pranay",
          "Dighe"
        ],
        [
          "Afsaneh",
          "Asaei"
        ],
        [
          "Herv\u00e9",
          "Bourlard"
        ]
      ],
      "title": "Exploiting Eigenposteriors for Semi-Supervised Training of DNN Acoustic Models with Sequence Discrimination",
      "original": "1784",
      "page_count": 5,
      "order": 745,
      "p1": "3552",
      "pn": "3556",
      "abstract": [
        "Deep neural network (DNN) acoustic models yield posterior probabilities\nof senone classes. Recent studies support the existence of low-dimensional\nsubspaces underlying senone posteriors. Principal component analysis\n(PCA) is applied to identify eigenposteriors and perform low-dimensional\nprojection of the training data posteriors. The resulted enhanced posteriors\nare applied as soft targets for training better DNN acoustic model\nunder the student-teacher framework. The present work advances this\napproach by studying incorporation of sequence discriminative training.\nWe demonstrate how to combine the gains from eigenposterior based enhancement\nwith sequence discrimination to improve ASR using semi-supervised training.\nEvaluation on AMI meeting corpus yields nearly 4% absolute reduction\nin word error rate (WER) compared to the baseline DNN trained with\ncross entropy objective. In this context, eigenposterior enhancement\nof the soft targets is crucial to enable additive improvement using\nout-of-domain untranscribed data.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1784"
    },
    "yang17c_interspeech": {
      "authors": [
        [
          "Ming-Han",
          "Yang"
        ],
        [
          "Hung-Shin",
          "Lee"
        ],
        [
          "Yu-Ding",
          "Lu"
        ],
        [
          "Kuan-Yu",
          "Chen"
        ],
        [
          "Yu",
          "Tsao"
        ],
        [
          "Berlin",
          "Chen"
        ],
        [
          "Hsin-Min",
          "Wang"
        ]
      ],
      "title": "Discriminative Autoencoders for Acoustic Modeling",
      "original": "0221",
      "page_count": 5,
      "order": 746,
      "p1": "3557",
      "pn": "3561",
      "abstract": [
        "Speech data typically contain information irrelevant to automatic speech\nrecognition (ASR), such as speaker variability and channel/environmental\nnoise, lurking deep within acoustic features. Such unwanted information\nis always mixed together to stunt the development of an ASR system.\nIn this paper, we propose a new framework based on autoencoders for\nacoustic modeling in ASR. Unlike other variants of autoencoder neural\nnetworks, our framework is able to isolate phonetic components from\na speech utterance by simultaneously taking two kinds of objectives\ninto consideration. The first one relates to the minimization of reconstruction\nerrors and benefits to learn most salient and useful properties of\nthe data. The second one functions in the middlemost code layer, where\nthe categorical distribution of the context-dependent phone states\nis estimated for phoneme discrimination and the derivation of acoustic\nscores, the proximity relationship among utterances spoken by the same\nspeaker are preserved, and the intra-utterance noise is modeled and\nabstracted away. We describe the implementation of the discriminative\nautoencoders for training tri-phone acoustic models and present TIMIT\nphone recognition results, which demonstrate that our proposed method\noutperforms the conventional DNN-based approach.\n"
      ],
      "doi": "10.21437/Interspeech.2017-221"
    },
    "zajic17_interspeech": {
      "authors": [
        [
          "Zbyn\u011bk",
          "Zaj\u00edc"
        ],
        [
          "Marek",
          "Hr\u00faz"
        ],
        [
          "Lud\u011bk",
          "M\u00fcller"
        ]
      ],
      "title": "Speaker Diarization Using Convolutional Neural Network for Statistics Accumulation Refinement",
      "original": "0051",
      "page_count": 5,
      "order": 747,
      "p1": "3562",
      "pn": "3566",
      "abstract": [
        "The aim of this paper is to investigate the benefit of information\nfrom a speaker change detection system based on Convolutional Neural\nNetwork (CNN) when applied to the process of accumulation of statistics\nfor an i-vector generation. The investigation is carried out on the\nproblem of diarization. In our system, the output of the CNN is a probability\nvalue of a speaker change in a conversation for a given time segment.\nAccording to this probability, we cut the conversation into short segments\nthat are then represented by the i-vector (to describe a speaker in\nit). We propose a technique to utilize the information from the CNN\nfor the weighting of the acoustic data in a segment to refine the statistics\naccumulation process. This technique enables us to represent the speaker\nbetter in the final i-vector. The experiments on the English part of\nthe CallHome corpus show that our proposed refinement of the statistics\naccumulation is beneficial with the relative improvement of Diarization\nError Rate almost by 16% when compared to the speaker diarization system\nwithout statistics refinement.\n"
      ],
      "doi": "10.21437/Interspeech.2017-51"
    },
    "jati17_interspeech": {
      "authors": [
        [
          "Arindam",
          "Jati"
        ],
        [
          "Panayiotis",
          "Georgiou"
        ]
      ],
      "title": "Speaker2Vec: Unsupervised Learning and Adaptation of a Speaker Manifold Using Deep Neural Networks with an Evaluation on Speaker Segmentation",
      "original": "1650",
      "page_count": 5,
      "order": 748,
      "p1": "3567",
      "pn": "3571",
      "abstract": [
        "This paper presents a novel approach, we term  Speaker2Vec, to derive\na speaker-characteristics manifold learned in an unsupervised manner.\nThe proposed representation can be employed in different applications\nsuch as diarization, speaker identification or, as in our evaluation\ntest case, speaker segmentation. Speaker2Vec exploits large amounts\nof unlabeled training data and the assumption of short-term active-speaker\nstationarity to derive a speaker embedding using Deep Neural Networks\n(DNN). We assume that temporally-near speech segments belong to the\nsame speaker, and as such a joint representation connecting these nearby\nsegments can encode their common information. Thus, this bottleneck\nrepresentation will be capturing mainly speaker-specific information.\nSuch training can take place in a completely unsupervised manner. For\ntesting, our trained model generates the embeddings for the test audio,\nand applies a simple distance metric to detect speaker-change points.\nThe paper also proposes a strategy for unsupervised adaptation of the\nDNN models to the application domain. The proposed method outperforms\nthe state-of-the-art speaker segmentation algorithms and MFCC based\nbaseline methods on four evaluation datasets, while it allows for further\nimprovements by employing this embedding into supervised training methods.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1650"
    },
    "lan17_interspeech": {
      "authors": [
        [
          "Ga\u00ebl Le",
          "Lan"
        ],
        [
          "Delphine",
          "Charlet"
        ],
        [
          "Anthony",
          "Larcher"
        ],
        [
          "Sylvain",
          "Meignier"
        ]
      ],
      "title": "A Triplet Ranking-Based Neural Network for Speaker Diarization and Linking",
      "original": "0270",
      "page_count": 5,
      "order": 749,
      "p1": "3572",
      "pn": "3576",
      "abstract": [
        "This paper investigates a novel neural scoring method, based on conventional\n i-vectors, to perform speaker diarization and linking of large collections\nof recordings. Using triplet loss for training, the network projects\n i-vectors in a space that better separates speakers in terms of cosine\nsimilarity. Experiments are run on two French TV collections built\nfrom REPERE [1] and ETAPE [2] campaigns corpora, the system being trained\non French Radio data. Results indicate that the proposed approach outperforms\nconventional cosine and Probabilistic Linear Discriminant Analysis\nscoring methods on both within- and cross-recording diarization tasks,\nwith a Diarization Error Rate reduction of 14% in average.\n"
      ],
      "doi": "10.21437/Interspeech.2017-270"
    },
    "cohen17_interspeech": {
      "authors": [
        [
          "Yishai",
          "Cohen"
        ],
        [
          "Itshak",
          "Lapidot"
        ]
      ],
      "title": "Estimating Speaker Clustering Quality Using Logistic Regression",
      "original": "0492",
      "page_count": 5,
      "order": 750,
      "p1": "3577",
      "pn": "3581",
      "abstract": [
        "This paper focuses on estimating clustering validity by using logistic\nregression. For many applications it might be important to estimate\nthe quality of the clustering, e.g. in case of speech segments&#8217;\nclustering, make a decision whether to use the clustered data for speaker\nverification. In the case of short segments speakers clustering, the\ncommon criteria for cluster validity are  average cluster purity (ACP),\n average speaker purity (ASP) and K &#8212; the geometric mean between\nthe two measures. As in practice, true labels are not available for\nevaluation, hence they have to be estimated from the clustering itself.\nIn this paper, mean-shift clustering with PLDA score is applied in\norder to cluster short speaker segments represented as i-vectors. Different\nstatistical parameters are then estimated on the clustered data and\nare used to train logistic regression to estimate ACP, ASP and K. It\nwas found that logistic regression can be a good predictor of the actual\nACP, ASP and K, and yields reasonable information regarding the clustering\nquality.\n"
      ],
      "doi": "10.21437/Interspeech.2017-492"
    },
    "wisniewksi17_interspeech": {
      "authors": [
        [
          "Guillaume",
          "Wisniewksi"
        ],
        [
          "Herv\u00e9",
          "Bredin"
        ],
        [
          "G.",
          "Gelly"
        ],
        [
          "Claude",
          "Barras"
        ]
      ],
      "title": "Combining Speaker Turn Embedding and Incremental Structure Prediction for Low-Latency Speaker Diarization",
      "original": "1067",
      "page_count": 5,
      "order": 751,
      "p1": "3582",
      "pn": "3586",
      "abstract": [
        "Real-time speaker diarization has many potential applications, including\npublic security, biometrics or forensics. It can also significantly\nspeed up the indexing of increasingly large multimedia archives. In\nthis paper, we address the issue of low-latency speaker diarization\nthat consists in continuously detecting new or reoccurring speakers\nwithin an audio stream, and determining when each speaker is active\nwith a low latency ( e.g. every second). This is in contrast with most\nexisting approaches in speaker diarization that rely on multiple passes\nover the complete audio recording. The proposed approach combines speaker\nturn neural embeddings with an incremental structure prediction approach\ninspired by state-of-the-art Natural Language Processing models for\nPart-of-Speech tagging and dependency parsing. It can therefore leverage\nboth information describing the utterance and the inherent temporal\nstructure of interactions between speakers to learn, in supervised\nframework, to identify speakers. Experiments on the Etape broadcast\nnews benchmark validate the approach.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1067"
    },
    "bredin17_interspeech": {
      "authors": [
        [
          "Herv\u00e9",
          "Bredin"
        ]
      ],
      "title": " pyannote.metrics: A Toolkit for Reproducible Evaluation, Diagnostic, and Error Analysis of Speaker Diarization Systems",
      "original": "0411",
      "page_count": 5,
      "order": 752,
      "p1": "3587",
      "pn": "3591",
      "abstract": [
        " pyannote.metrics is an open-source Python library aimed at researchers\nworking in the wide area of speaker diarization. It provides a command\nline interface (CLI) to improve reproducibility and comparison of speaker\ndiarization research results. Through its application programming interface\n(API), a large set of evaluation metrics is available for diagnostic\npurposes of all modules of typical speaker diarization pipelines (speech\nactivity detection, speaker change detection, clustering, and identification).\nFinally, thanks to visualization capabilities, we show that it can\nalso be used for detailed error analysis purposes.  pyannote.metrics\ncan be downloaded from  http://pyannote.github.io.\n"
      ],
      "doi": "10.21437/Interspeech.2017-411"
    },
    "chen17n_interspeech": {
      "authors": [
        [
          "Zhipeng",
          "Chen"
        ],
        [
          "Ji",
          "Wu"
        ]
      ],
      "title": "A Rescoring Approach for Keyword Search Using Lattice Context Information",
      "original": "1328",
      "page_count": 5,
      "order": 753,
      "p1": "3592",
      "pn": "3596",
      "abstract": [
        "In this paper we present a rescoring approach for keyword search (KWS)\nbased on neural networks (NN). This approach exploits only the lattice\ncontext in a detected time interval instead of its corresponding audio.\nThe most informative arcs in lattice context are selected and represented\nas a matrix, where words on arcs are represented in an embedding space\nwith respect to their pronunciations. Then convolutional neural networks\n(CNNs) are employed to capture distinctive features from this matrix.\nA rescoring model is trained to minimize term-weighted sigmoid cross\nentropy so as to match the evaluation metric. Experiments on single-word\nqueries show that lattice context brings complementary gains over normalized\nposterior scores. Performance on both in-vocabulary (IV) and out-of-vocabulary\n(OOV) queries are improved by combining NN-based scores with standard\nposterior scores.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1328"
    },
    "trmal17_interspeech": {
      "authors": [
        [
          "Jan",
          "Trmal"
        ],
        [
          "Matthew",
          "Wiesner"
        ],
        [
          "Vijayaditya",
          "Peddinti"
        ],
        [
          "Xiaohui",
          "Zhang"
        ],
        [
          "Pegah",
          "Ghahremani"
        ],
        [
          "Yiming",
          "Wang"
        ],
        [
          "Vimal",
          "Manohar"
        ],
        [
          "Hainan",
          "Xu"
        ],
        [
          "Daniel",
          "Povey"
        ],
        [
          "Sanjeev",
          "Khudanpur"
        ]
      ],
      "title": "The Kaldi OpenKWS System: Improving Low Resource Keyword Search",
      "original": "0601",
      "page_count": 5,
      "order": 754,
      "p1": "3597",
      "pn": "3601",
      "abstract": [
        "The IARPA BABEL program has stimulated worldwide research in keyword\nsearch technology for low resource languages, and the NIST OpenKWS\nevaluations are the de facto benchmark test for such capabilities.\nThe 2016 OpenKWS evaluation featured Georgian speech, and had 10 participants\nfrom across the world. This paper describes the Kaldi system developed\nto assist IARPA in creating a competitive baseline against which participants\nwere evaluated, and to provide a truly open source system to all participants\nto support their research. This system handily met the BABEL program\ngoals of 0.60 ATWV and 50% WER, achieving 0.70 ATWV and 38% WER with\na single ASR system, i.e.  without ASR system combination. All except\none OpenKWS participant used Kaldi components in their submissions,\ntypically in conjunction with system combination. This paper therefore\ncomplements all other OpenKWS-based papers.\n"
      ],
      "doi": "10.21437/Interspeech.2017-601"
    },
    "khokhlov17b_interspeech": {
      "authors": [
        [
          "Yuri",
          "Khokhlov"
        ],
        [
          "Ivan",
          "Medennikov"
        ],
        [
          "Aleksei",
          "Romanenko"
        ],
        [
          "Valentin",
          "Mendelev"
        ],
        [
          "Maxim",
          "Korenevsky"
        ],
        [
          "Alexey",
          "Prudnikov"
        ],
        [
          "Natalia",
          "Tomashenko"
        ],
        [
          "Alexander",
          "Zatvornitsky"
        ]
      ],
      "title": "The STC Keyword Search System for OpenKWS 2016 Evaluation",
      "original": "1212",
      "page_count": 5,
      "order": 755,
      "p1": "3602",
      "pn": "3606",
      "abstract": [
        "This paper describes the keyword search system developed by the STC\nteam in the framework of OpenKWS 2016 evaluation. The acoustic modeling\ntechniques included i-vectors based speaker adaptation, multilingual\nspeaker-dependent bottleneck features, and a combination of feedforward\nand recurrent neural networks. To improve the language model, we augmented\nthe training data provided by the organizers with texts generated by\nthe character-level recurrent neural networks trained on different\ndata sets. This led to substantial reductions in the out-of-vocabulary\n(OOV) and word error rates. The OOV search problem was solved with\nthe help of a novel approach based on lattice generated phone posteriors\nand a highly optimized decoder. This approach outperformed familiar\nOOV search implementations in terms of speed and demonstrated comparable\nor better search quality.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  The system was among\nthe top three systems in the evaluation.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1212"
    },
    "sun17_interspeech": {
      "authors": [
        [
          "Ming",
          "Sun"
        ],
        [
          "David",
          "Snyder"
        ],
        [
          "Yixin",
          "Gao"
        ],
        [
          "Varun",
          "Nagaraja"
        ],
        [
          "Mike",
          "Rodehorst"
        ],
        [
          "Sankaran",
          "Panchapagesan"
        ],
        [
          "Nikko",
          "Strom"
        ],
        [
          "Spyros",
          "Matsoukas"
        ],
        [
          "Shiv",
          "Vitaladevuni"
        ]
      ],
      "title": "Compressed Time Delay Neural Network for Small-Footprint Keyword Spotting",
      "original": "0480",
      "page_count": 5,
      "order": 756,
      "p1": "3607",
      "pn": "3611",
      "abstract": [
        "In this paper we investigate a time delay neural network (TDNN) for\na keyword spotting task that requires low CPU, memory and latency.\nThe TDNN is trained with transfer learning and multi-task learning.\nTemporal subsampling enabled by the time delay architecture reduces\ncomputational complexity. We propose to apply singular value decomposition\n(SVD) to further reduce TDNN complexity. This allows us to first train\na larger full-rank TDNN model which is not limited by CPU/memory constraints.\nThe larger TDNN usually achieves better performance. Afterwards, its\nsize can be compressed by SVD to meet the budget requirements. Hidden\nMarkov models (HMM) are used in conjunction with the networks to perform\nkeyword detection and performance is measured in terms of area under\nthe curve (AUC) for detection error tradeoff (DET) curves. Our experimental\nresults on a large in-house far-field corpus show that the full-rank\nTDNN achieves a 19.7% DET AUC reduction compared to a similar-size\ndeep neural network (DNN) baseline. If we train a larger size full-rank\nTDNN first and then reduce it via SVD to the comparable size of the\nDNN, we obtain a 37.6% reduction in DET AUC compared to the DNN baseline.\n"
      ],
      "doi": "10.21437/Interspeech.2017-480"
    },
    "suzuki17b_interspeech": {
      "authors": [
        [
          "Masayuki",
          "Suzuki"
        ],
        [
          "Gakuto",
          "Kurata"
        ],
        [
          "Abhinav",
          "Sethy"
        ],
        [
          "Bhuvana",
          "Ramabhadran"
        ],
        [
          "Kenneth W.",
          "Church"
        ],
        [
          "Mark",
          "Drake"
        ]
      ],
      "title": "Symbol Sequence Search from Telephone Conversation",
      "original": "0904",
      "page_count": 5,
      "order": 757,
      "p1": "3612",
      "pn": "3616",
      "abstract": [
        "We propose a method for searching for symbol sequences in conversations.\nSymbol sequences can include phone numbers, credit card numbers, and\nany kind of ticket (identification) numbers and are often communicated\nin call center conversations. Automatic extraction of these from speech\nis a key to many automatic speech recognition (ASR) applications such\nas question answering and summarization. Compared with spoken term\ndetection (STD), symbol sequence searches have two additional problems.\nFirst, the entire symbol sequence is typically not observed continuously\nbut in sub sequences, where customers or agents speak these sequences\nin fragments, while the recipient repeats them to ensure they have\nthe correct sequence. Second, we have to distinguish between different\nsymbol sequences, for example, phone numbers versus ticket numbers\nor customer identification numbers. To deal with these problems, we\npropose to apply STD to symbol-sequence fragments and subsequently\nuse confidence scoring to obtain the entire symbol sequence. For the\nconfidence scoring, We propose a long short-term memory (LSTM) based\napproach that inputs word before and after fragments. We also propose\nto detect repetitions of fragments and use it for confidence scoring.\nOur proposed method achieves a 0.87 F-measure, in an eight-digit customer\nidentification number search task, when operating at 20.3% WER.\n"
      ],
      "doi": "10.21437/Interspeech.2017-904"
    },
    "gundogdu17_interspeech": {
      "authors": [
        [
          "Batuhan",
          "Gundogdu"
        ],
        [
          "Murat",
          "Saraclar"
        ]
      ],
      "title": "Similarity Learning Based Query Modeling for Keyword Search",
      "original": "1273",
      "page_count": 5,
      "order": 758,
      "p1": "3617",
      "pn": "3621",
      "abstract": [
        "In this paper, we propose a novel approach for query modeling using\nneural networks for posteriorgram based keyword search (KWS). We aim\nto help the conventional large vocabulary continuous speech recognition\n(LVCSR) based KWS systems, especially on out-of-vocabulary (OOV) terms\nby converting the task into a template matching problem, just like\nthe query-by-example retrieval tasks. For this, we use a dynamic time\nwarping (DTW) based similarity search on the speaker independent posteriorgram\nspace. In order to model the text queries as posteriorgrams, we propose\na non-symmetric Siamese neural network structure which both learns\na distance measure to be used in DTW and the frame representations\nfor this specific measure. We compare this new technique with similar\nDTW based systems using other distance measures and query modeling\ntechniques. We also apply system fusion of the proposed system with\nthe LVCSR based baseline KWS system. We show that, the proposed system\nworks significantly better than other similar systems. Furthermore,\nwhen combined with the LVSCR based baseline, the proposed system provides\nup to 37.9% improvement on OOV terms and 9.8% improvement on all terms.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1273"
    },
    "samui17_interspeech": {
      "authors": [
        [
          "Suman",
          "Samui"
        ],
        [
          "Indrajit",
          "Chakrabarti"
        ],
        [
          "Soumya K.",
          "Ghosh"
        ]
      ],
      "title": "Deep Recurrent Neural Network Based Monaural Speech Separation Using Recurrent Temporal Restricted Boltzmann Machines",
      "original": "0057",
      "page_count": 5,
      "order": 759,
      "p1": "3622",
      "pn": "3626",
      "abstract": [
        "This paper presents a single-channel speech separation method implemented\nwith a deep recurrent neural network (DRNN) using recurrent temporal\nrestricted Boltzmann machines (RTRBM). Although deep neural network\n(DNN) based speech separation (denoising task) methods perform quite\nwell compared to the conventional statistical model based speech enhancement\ntechniques, in DNN-based methods, the temporal correlations across\nspeech frames are often ignored, resulting in loss of spectral detail\nin the reconstructed output speech. In order to alleviate this issue,\none RTRBM is employed for modelling the acoustic features of input\n(mixture) signal and two RTRBMs are trained for the two training targets\n(source signals). Each RTRBM attempts to model the abstractions present\nin the training data at each time step as well as the temporal dependencies\nin the training data. The entire network (consisting of three RTRBMs\nand one recurrent neural network) can be fine-tuned by the joint optimization\nof the DRNN with an extra masking layer which enforces a reconstruction\nconstraint. The proposed method has been evaluated on the IEEE corpus\nand TIMIT dataset for speech denoising task. Experimental results have\nestablished that the proposed approach outperforms NMF and conventional\nDNN and DRNN-based speech enhancement methods.\n"
      ],
      "doi": "10.21437/Interspeech.2017-57"
    },
    "huang17g_interspeech": {
      "authors": [
        [
          "Qizheng",
          "Huang"
        ],
        [
          "Changchun",
          "Bao"
        ],
        [
          "Xianyun",
          "Wang"
        ]
      ],
      "title": "Improved Codebook-Based Speech Enhancement Based on MBE Model",
      "original": "0109",
      "page_count": 5,
      "order": 760,
      "p1": "3627",
      "pn": "3631",
      "abstract": [
        "This paper provides an improved codebook-based speech enhancement method\nusing multi-band excitation (MBE) model. It aims to remove the noise\nbetween the harmonics, which may exist in codebook-based enhanced speech.\nIn general, the proposed system is based on analysis-with-synthesis\n(AwS) framework. During the analysis stage, acoustic features are extracted\nincluding pitch, harmonic magnitude and voicing from noisy speech.\nThese parameters are obtained on the basis of the spectral magnitudes\nobtained by codebook-based method. During the synthesis stage, different\nsynthesis strategies for voiced and unvoiced speech are employed. Besides,\nthis paper introduces speech presence probability to modify the codebook-based\nWiener filter so that more accurate acoustic parameters can be obtained.\nThe proposed system can eliminate noise not only between the harmonics,\nbut also in the silent segments, especially in low SNR noise environment.\nExperiments show that, the performance of the proposed method is better\nthan traditional codebook-based method for different types of noise.\n"
      ],
      "doi": "10.21437/Interspeech.2017-109"
    },
    "chen17o_interspeech": {
      "authors": [
        [
          "Zhuo",
          "Chen"
        ],
        [
          "Yan",
          "Huang"
        ],
        [
          "Jinyu",
          "Li"
        ],
        [
          "Yifan",
          "Gong"
        ]
      ],
      "title": "Improving Mask Learning Based Speech Enhancement System with Restoration Layers and Residual Connection",
      "original": "0515",
      "page_count": 5,
      "order": 761,
      "p1": "3632",
      "pn": "3636",
      "abstract": [
        "For single-channel speech enhancement, mask learning based approach\nthrough neural network has been shown to outperform the feature mapping\napproach, and to be effective as a pre-processor for automatic speech\nrecognition. However, its assumption that the mixture and clean reference\nmust have the correspondent scale doesn&#8217;t hold in data collected\nfrom real world, and thus leads to significant performance degradation\non parallel recorded data. In this paper, we first extend the mask\nlearning based speech enhancement by integrating two types of restoration\nlayer to address the scale mismatch problem. We further propose a novel\nresidual learning based speech enhancement model via adding different\nshortcut connections to a feature mapping network. We show such a structure\ncan benefit from both the mask learning and the feature mapping. We\nevaluate the proposed speech enhancement models on CHiME 3 data. Without\nretraining the acoustic model, the best bi-direction LSTM with residue\nconnections yields 24.90% relative WER reduction on real data and 34.57%\nWER on simulated data.\n"
      ],
      "doi": "10.21437/Interspeech.2017-515"
    },
    "yan17_interspeech": {
      "authors": [
        [
          "Bi-Cheng",
          "Yan"
        ],
        [
          "Chin-Hong",
          "Shih"
        ],
        [
          "Shih-Hung",
          "Liu"
        ],
        [
          "Berlin",
          "Chen"
        ]
      ],
      "title": "Exploring Low-Dimensional Structures of Modulation Spectra for Robust Speech Recognition",
      "original": "0611",
      "page_count": 5,
      "order": 762,
      "p1": "3637",
      "pn": "3641",
      "abstract": [
        "Developments of noise robustness techniques are vital to the success\nof automatic speech recognition (ASR) systems in face of varying sources\nof environmental interference. Recent studies have shown that exploring\nlow-dimensional structures of speech features can yield good robustness.\nAlong this vein, research on low-rank representation (LRR), which considers\nthe intrinsic structures of speech features lying on some low dimensional\nsubspaces, has gained considerable interest from the ASR community.\nWhen speech features are contaminated with various types of environmental\nnoise, its corresponding modulation spectra can be regarded as superpositions\nof unstructured sparse noise over the inherent linguistic information.\nAs such, we in this paper endeavor to explore the low dimensional structures\nof modulation spectra, in the hope to obtain more noise-robust speech\nfeatures. The main contribution is that we propose a novel use of the\nLRR-based method to discover the subspace structures of modulation\nspectra, thereby alleviating the negative effects of noise interference.\nFurthermore, we also extensively compare our approach with several\nwell-practiced feature-based normalization methods. All experiments\nwere conducted and verified on the Aurora-4 database and task. The\nempirical results show that the proposed LRR-based method can provide\nsignificant word error reductions for a typical DNN-HMM hybrid ASR\nsystem.\n"
      ],
      "doi": "10.21437/Interspeech.2017-611"
    },
    "pascual17_interspeech": {
      "authors": [
        [
          "Santiago",
          "Pascual"
        ],
        [
          "Antonio",
          "Bonafonte"
        ],
        [
          "Joan",
          "Serr\u00e0"
        ]
      ],
      "title": "SEGAN: Speech Enhancement Generative Adversarial Network",
      "original": "1428",
      "page_count": 5,
      "order": 763,
      "p1": "3642",
      "pn": "3646",
      "abstract": [
        "Current speech enhancement techniques operate on the spectral domain\nand/or exploit some higher-level feature. The majority of them tackle\na limited number of noise conditions and rely on first-order statistics.\nTo circumvent these issues, deep networks are being increasingly used,\nthanks to their ability to learn complex functions from large example\nsets. In this work, we propose the use of generative adversarial networks\nfor speech enhancement. In contrast to current techniques, we operate\nat the waveform level, training the model end-to-end, and incorporate\n28 speakers and 40 different noise conditions into the same model,\nsuch that model parameters are shared across them. We evaluate the\nproposed model using an independent, unseen test set with two speakers\nand 20 alternative noise conditions. The enhanced samples confirm the\nviability of the proposed model, and both objective and subjective\nevaluations confirm the effectiveness of it. With that, we open the\nexploration of generative architectures for speech enhancement, which\nmay progressively incorporate further speech-centric design choices\nto improve their performance.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1428"
    },
    "maiti17_interspeech": {
      "authors": [
        [
          "Soumi",
          "Maiti"
        ],
        [
          "Michael I.",
          "Mandel"
        ]
      ],
      "title": "Concatenative Resynthesis Using Twin Networks",
      "original": "1653",
      "page_count": 5,
      "order": 764,
      "p1": "3647",
      "pn": "3651",
      "abstract": [
        "Traditional noise reduction systems modify a noisy signal to make it\nmore like the original clean signal. For speech, these methods suffer\nfrom two main problems: under-suppression of noise and over-suppression\nof target speech. Instead, synthesizing clean speech based on the noisy\nsignal could produce outputs that are both noise-free and high quality.\nOur previous work introduced such a system using concatenative synthesis,\nbut it required processing the clean speech at run time, which was\nslow and not scalable. In order to make such a system scalable, we\npropose here learning a similarity metric using two separate networks,\none network processing the clean segments offline and another processing\nthe noisy segments at run time. This system incorporates a ranking\nloss to optimize for the retrieval of appropriate clean speech segments.\nThis model is compared against our original on the CHiME2-GRID corpus,\nmeasuring ranking performance and subjective listening tests of resyntheses.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1653"
    },
    "stafylakis17_interspeech": {
      "authors": [
        [
          "Themos",
          "Stafylakis"
        ],
        [
          "Georgios",
          "Tzimiropoulos"
        ]
      ],
      "title": "Combining Residual Networks with LSTMs for Lipreading",
      "original": "0085",
      "page_count": 5,
      "order": 765,
      "p1": "3652",
      "pn": "3656",
      "abstract": [
        "We propose an end-to-end deep learning architecture for word-level\nvisual speech recognition. The system is a combination of spatiotemporal\nconvolutional, residual and bidirectional Long Short-Term Memory networks.\nWe train and evaluate it on the Lipreading In-The-Wild benchmark, a\nchallenging database of 500-size target-words consisting of 1.28sec\nvideo excerpts from BBC TV broadcasts. The proposed network attains\nword accuracy equal to 83.0%, yielding 6.8% absolute improvement over\nthe current state-of-the-art, without using information about word\nboundaries during training or testing.\n"
      ],
      "doi": "10.21437/Interspeech.2017-85"
    },
    "thangthai17_interspeech": {
      "authors": [
        [
          "Kwanchiva",
          "Thangthai"
        ],
        [
          "Richard",
          "Harvey"
        ]
      ],
      "title": "Improving Computer Lipreading via DNN Sequence Discriminative Training Techniques",
      "original": "0106",
      "page_count": 5,
      "order": 766,
      "p1": "3657",
      "pn": "3661",
      "abstract": [
        "Although there have been some promising results in computer lipreading,\nthere has been a paucity of data on which to train automatic systems.\nHowever the recent emergence of the TCD-TIMIT corpus, with around 6000\nwords, 59 speakers and seven hours of recorded audio-visual speech,\nallows the deployment of more recent techniques in audio-speech such\nas Deep Neural Networks (DNNs) and sequence discriminative training.<br\nstyle=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  In this paper we combine the DNN with a Hidden Markov Model (HMM)\nto the, so called, hybrid DNN-HMM configuration which we train using\na variety of sequence discriminative training methods. This is then\nfollowed with a weighted finite state transducer. The conclusion is\nthat the DNN offers very substantial improvement over a conventional\nclassifier which uses a Gaussian Mixture Model (GMM) to model the densities\neven when optimised with Speaker Adaptive Training. Sequence adaptive\ntraining offers further improvements depending on the precise variety\nemployed but those improvements are of the order of 10% improvement\nin word accuracy. Putting these two results together implies that lipreading\nis moving from something of rather esoteric interest to becoming a\npractical reality in the foreseeable future.\n"
      ],
      "doi": "10.21437/Interspeech.2017-106"
    },
    "wand17_interspeech": {
      "authors": [
        [
          "Michael",
          "Wand"
        ],
        [
          "J\u00fcrgen",
          "Schmidhuber"
        ]
      ],
      "title": "Improving Speaker-Independent Lipreading with Domain-Adversarial Training",
      "original": "0421",
      "page_count": 5,
      "order": 767,
      "p1": "3662",
      "pn": "3666",
      "abstract": [
        "We present a  Lipreading system, i.e. a speech recognition system using\nonly visual features, which uses  domain-adversarial training for speaker\nindependence. Domain-adversarial training is integrated into the optimization\nof a lipreader based on a stack of feedforward and LSTM (Long Short-Term\nMemory) recurrent neural networks, yielding an end-to-end trainable\nsystem which only requires a very small number of frames of  untranscribed\ntarget data to substantially improve the recognition accuracy on the\ntarget speaker. On pairs of different source and target speakers, we\nachieve a relative accuracy improvement of around 40% with only 15\nto 20 seconds of untranscribed target speech data. On multi-speaker\ntraining setups, the accuracy improvements are smaller but still substantial.\n"
      ],
      "doi": "10.21437/Interspeech.2017-421"
    },
    "abdelaziz17_interspeech": {
      "authors": [
        [
          "Ahmed Hussen",
          "Abdelaziz"
        ]
      ],
      "title": "Turbo Decoders for Audio-Visual Continuous Speech Recognition",
      "original": "0799",
      "page_count": 5,
      "order": 768,
      "p1": "3667",
      "pn": "3671",
      "abstract": [
        "Visual speech, i.e., video recordings of speakers&#8217; mouths, plays\nan important role in improving the robustness properties of automatic\nspeech recognition (ASR) against noise. Optimal fusion of audio and\nvideo modalities is still one of the major challenges that attracts\nsignificant interest in the realm of audio-visual ASR. Recently, turbo\ndecoders (TDs) have been successful in addressing the audio-visual\nfusion problem. The idea of the TD framework is to iteratively exchange\nsome kind of soft information between the audio and video decoders\nuntil convergence. The forward-backward algorithm (FBA) is mostly applied\nto the decoding graphs to estimate this soft information. Applying\nthe FBA to the complex graphs that are usually used in large vocabulary\ntasks may be computationally expensive. In this paper, I propose to\napply the forward-backward algorithm to a lattice of most likely state\nsequences instead of using the entire decoding graph. Using lattices\nallows for TD to be easily applied to large vocabulary tasks. The proposed\napproach is evaluated using the newly released TCD-TIMIT corpus, where\na standard recipe for large vocabulary ASR is employed. The modified\nTD performs significantly better than the feature and decision fusion\nmodels in all clean and noisy test conditions.\n"
      ],
      "doi": "10.21437/Interspeech.2017-799"
    },
    "csapo17_interspeech": {
      "authors": [
        [
          "Tam\u00e1s G\u00e1bor",
          "Csap\u00f3"
        ],
        [
          "Tam\u00e1s",
          "Gr\u00f3sz"
        ],
        [
          "G\u00e1bor",
          "Gosztolya"
        ],
        [
          "L\u00e1szl\u00f3",
          "T\u00f3th"
        ],
        [
          "Alexandra",
          "Mark\u00f3"
        ]
      ],
      "title": "DNN-Based Ultrasound-to-Speech Conversion for a Silent Speech Interface",
      "original": "0939",
      "page_count": 5,
      "order": 769,
      "p1": "3672",
      "pn": "3676",
      "abstract": [
        "In this paper we present our initial results in articulatory-to-acoustic\nconversion based on tongue movement recordings using Deep Neural Networks\n(DNNs). Despite the fact that deep learning has revolutionized several\nfields, so far only a few researchers have applied DNNs for this task.\nHere, we compare various possible feature representation approaches\ncombined with DNN-based regression. As the input, we recorded synchronized\n2D ultrasound images and speech signals. The task of the DNN was to\nestimate Mel-Generalized Cepstrum-based Line Spectral Pair (MGC-LSP)\ncoefficients, which then served as input to a standard pulse-noise\nvocoder for speech synthesis. As the raw ultrasound images have a relatively\nhigh resolution, we experimented with various feature selection and\ntransformation approaches to reduce the size of the feature vectors.\nThe synthetic speech signals resulting from the various DNN configurations\nwere evaluated both using objective measures and a subjective listening\ntest. We found that the representation that used several neighboring\nimage frames in combination with a feature selection method was preferred\nboth by the subjects taking part in the listening experiments, and\nin terms of the Normalized Mean Squared Error. Our results may be useful\nfor creating Silent Speech Interface applications in the future. \n"
      ],
      "doi": "10.21437/Interspeech.2017-939"
    },
    "kamper17_interspeech": {
      "authors": [
        [
          "Herman",
          "Kamper"
        ],
        [
          "Shane",
          "Settle"
        ],
        [
          "Gregory",
          "Shakhnarovich"
        ],
        [
          "Karen",
          "Livescu"
        ]
      ],
      "title": "Visually Grounded Learning of Keyword Prediction from Untranscribed Speech",
      "original": "0502",
      "page_count": 5,
      "order": 770,
      "p1": "3677",
      "pn": "3681",
      "abstract": [
        "During language acquisition, infants have the benefit of visual cues\nto ground spoken language. Robots similarly have access to audio and\nvisual sensors. Recent work has shown that images and spoken captions\ncan be mapped into a meaningful common space, allowing images to be\nretrieved using speech and vice versa. In this setting of images paired\nwith untranscribed spoken captions, we consider whether computer vision\nsystems can be used to obtain textual labels for the speech. Concretely,\nwe use an image-to-words multi-label visual classifier to tag images\nwith soft textual labels, and then train a neural network to map from\nthe speech to these soft targets. We show that the resulting speech\nsystem is able to predict which words occur in an utterance &#8212;\nacting as a spoken bag-of-words classifier &#8212; without seeing any\nparallel speech and text. We find that the model often confuses semantically\nrelated words, e.g. &#8220;man&#8221; and &#8220;person&#8221;, making\nit even more effective as a  semantic keyword spotter.\n"
      ],
      "doi": "10.21437/Interspeech.2017-502"
    },
    "chien17d_interspeech": {
      "authors": [
        [
          "Jen-Tzung",
          "Chien"
        ],
        [
          "Chen",
          "Shen"
        ]
      ],
      "title": "Deep Neural Factorization for Speech Recognition",
      "original": "0892",
      "page_count": 5,
      "order": 771,
      "p1": "3682",
      "pn": "3686",
      "abstract": [
        "Conventional speech recognition system is constructed by unfolding\nthe spectral-temporal input matrices into one-way vectors and using\nthese vectors to estimate the affine parameters of neural network according\nto the vector-based error back-propagation algorithm. System performance\nis constrained because the contextual correlations in frequency and\ntime horizons are disregarded and the spectral and temporal factors\nare excluded. This paper proposes a spectral-temporal factorized neural\nnetwork (STFNN) to tackle this weakness. The spectral-temporal structure\nis preserved and factorized in hidden layers through two ways of factor\nmatrices which are trained by using the factorized error backpropagation.\nAffine transformation in standard neural network is generalized to\nthe spectro-temporal factorization in STFNN. The structural features\nor patterns are extracted and forwarded towards the softmax outputs.\nA deep neural factorization is built by cascading a number of factorization\nlayers with fully-connected layers for speech recognition. An orthogonal\nconstraint is imposed in factor matrices for redundancy reduction.\nExperimental results show the merit of integrating the factorized features\nin deep feedforward and recurrent neural networks for speech recognition.\n"
      ],
      "doi": "10.21437/Interspeech.2017-892"
    },
    "vesely17_interspeech": {
      "authors": [
        [
          "Karel",
          "Vesel\u00fd"
        ],
        [
          "Luk\u00e1\u0161",
          "Burget"
        ],
        [
          "Jan",
          "\u010cernock\u00fd"
        ]
      ],
      "title": "Semi-Supervised DNN Training with Word Selection for ASR",
      "original": "1385",
      "page_count": 5,
      "order": 772,
      "p1": "3687",
      "pn": "3691",
      "abstract": [
        "Not all the questions related to the semi-supervised training of hybrid\nASR system with DNN acoustic model were already deeply investigated.\nIn this paper, we focus on the question of the granularity of confidences\n(per-sentence, per-word, per-frame), the question of how the data should\nbe used (data-selection by masks, or in mini-batch SGD with confidences\nas weights). Then, we propose to re-tune the system with the manually\ntranscribed data, both with the &#8216;frame CE&#8217; training and\n&#8216;sMBR&#8217; training.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  Our preferred semi-supervised\nrecipe which is both simple and efficient is following: we select words\naccording to the word accuracy we obtain on the development set. Such\nrecipe, which does not rely on a grid-search of the training hyper-parameter,\ngeneralized well for: Babel Vietnamese (transcribed 11h, untranscribed\n74h), Babel Bengali (transcribed 11h, untranscribed 58h) and our custom\nSwitchboard setup (transcribed 14h, untranscribed 95h). We obtained\nthe absolute WER improvements 2.5% for Vietnamese, 2.3% for Bengali\nand 3.2% for Switchboard.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1385"
    },
    "hou17b_interspeech": {
      "authors": [
        [
          "Junfeng",
          "Hou"
        ],
        [
          "Shiliang",
          "Zhang"
        ],
        [
          "Li-Rong",
          "Dai"
        ]
      ],
      "title": "Gaussian Prediction Based Attention for Online End-to-End Speech Recognition",
      "original": "0751",
      "page_count": 5,
      "order": 773,
      "p1": "3692",
      "pn": "3696",
      "abstract": [
        "Recently end-to-end speech recognition has obtained much attention.\nOne of the popular models to achieve end-to-end speech recognition\nis attention based encoder-decoder model, which usually generating\noutput sequences iteratively by attending the whole representations\nof the input sequences. However, predicting outputs until receiving\nthe whole input sequence is not practical for online or low time latency\nspeech recognition. In this paper, we present a simple but effective\nattention mechanism which can make the encoder-decoder model generate\noutputs without attending the entire input sequence and can apply to\nonline speech recognition. At each prediction step, the attention is\nassumed to be a time-moving gaussian window with variable size and\ncan be predicted by using previous input and output information instead\nof the content based computation on the whole input sequence. To further\nimprove the online performance of the model, we employ deep convolutional\nneural networks as encoder. Experiments show that the gaussian prediction\nbased attention works well and under the help of deep convolutional\nneural networks the online model achieves 19.5% phoneme error rate\nin TIMIT ASR task.\n"
      ],
      "doi": "10.21437/Interspeech.2017-751"
    },
    "fukuda17_interspeech": {
      "authors": [
        [
          "Takashi",
          "Fukuda"
        ],
        [
          "Masayuki",
          "Suzuki"
        ],
        [
          "Gakuto",
          "Kurata"
        ],
        [
          "Samuel",
          "Thomas"
        ],
        [
          "Jia",
          "Cui"
        ],
        [
          "Bhuvana",
          "Ramabhadran"
        ]
      ],
      "title": "Efficient Knowledge Distillation from an Ensemble of Teachers",
      "original": "0614",
      "page_count": 5,
      "order": 774,
      "p1": "3697",
      "pn": "3701",
      "abstract": [
        "This paper describes the effectiveness of knowledge distillation using\nteacher student training for building accurate and compact neural networks.\nWe show that with knowledge distillation, information from multiple\nacoustic models like very deep VGG networks and Long Short-Term Memory\n(LSTM) models can be used to train standard convolutional neural network\n(CNN) acoustic models for a variety of systems requiring a quick turnaround.\nWe examine two strategies to leverage multiple teacher labels for training\nstudent models. In the first technique, the weights of the student\nmodel are updated by switching teacher labels at the minibatch level.\nIn the second method, student models are trained on multiple streams\nof information from various teacher distributions via data augmentation.\nWe show that standard CNN acoustic models can achieve comparable recognition\naccuracy with much smaller number of model parameters compared to teacher\nVGG and LSTM acoustic models. Additionally we also investigate the\neffectiveness of using broadband teacher labels as privileged knowledge\nfor training better narrowband acoustic models within this framework.\nWe show the benefit of this simple technique by training narrowband\nstudent models with broadband teacher soft labels on the Aurora 4 task.\n"
      ],
      "doi": "10.21437/Interspeech.2017-614"
    },
    "prabhavalkar17b_interspeech": {
      "authors": [
        [
          "Rohit",
          "Prabhavalkar"
        ],
        [
          "Tara N.",
          "Sainath"
        ],
        [
          "Bo",
          "Li"
        ],
        [
          "Kanishka",
          "Rao"
        ],
        [
          "Navdeep",
          "Jaitly"
        ]
      ],
      "title": "An Analysis of &#8220;Attention&#8221; in Sequence-to-Sequence Models",
      "original": "0232",
      "page_count": 5,
      "order": 775,
      "p1": "3702",
      "pn": "3706",
      "abstract": [
        "In this paper, we conduct a detailed investigation of attention-based\nmodels for automatic speech recognition (ASR). First, we explore different\ntypes of attention, including &#8220;online&#8221; and &#8220;full-sequence&#8221;\nattention. Second, we explore different subword units to see how much\nof the end-to-end ASR process can reasonably be captured by an attention\nmodel. In experimental evaluations, we find that although attention\nis typically focused over a small region of the acoustics during each\nstep of next label prediction, &#8220;full-sequence&#8221; attention\noutperforms &#8220;online&#8221; attention, although this gap can be\nsignificantly reduced by increasing the length of the segments over\nwhich attention is computed. Furthermore, we find that context-independent\nphonemes are a reasonable sub-word unit for attention models. When\nused in the second-pass to rescore N-best hypotheses, these models\nprovide over a 10% relative improvement in word error rate.\n"
      ],
      "doi": "10.21437/Interspeech.2017-232"
    },
    "soltau17_interspeech": {
      "authors": [
        [
          "Hagen",
          "Soltau"
        ],
        [
          "Hank",
          "Liao"
        ],
        [
          "Ha\u015fim",
          "Sak"
        ]
      ],
      "title": "Neural Speech Recognizer: Acoustic-to-Word LSTM Model for Large Vocabulary Speech Recognition",
      "original": "1566",
      "page_count": 5,
      "order": 776,
      "p1": "3707",
      "pn": "3711",
      "abstract": [
        "We present results that show it is possible to build a competitive,\ngreatly simplified, large vocabulary continuous speech recognition\nsystem with whole words as acoustic units. We model the output vocabulary\nof about 100,000 words directly using deep bi-directional LSTM RNNs\nwith CTC loss. The model is trained on 125,000 hours of semi-supervised\nacoustic training data, which enables us to alleviate the data sparsity\nproblem for word models. We show that the CTC word models work very\nwell as an end-to-end all-neural speech recognition model without the\nuse of traditional context-dependent sub-word phone units that require\na pronunciation lexicon, and without any language model removing the\nneed to decode. We demonstrate that the CTC word models perform better\nthan a strong, more complex, state-of-the-art baseline with sub-word\nunits.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1566"
    },
    "guo17c_interspeech": {
      "authors": [
        [
          "Jinxi",
          "Guo"
        ],
        [
          "Usha Amrutha",
          "Nookala"
        ],
        [
          "Abeer",
          "Alwan"
        ]
      ],
      "title": "CNN-Based Joint Mapping of Short and Long Utterance i-Vectors for Speaker Verification Using Short Utterances",
      "original": "0430",
      "page_count": 5,
      "order": 777,
      "p1": "3712",
      "pn": "3716",
      "abstract": [
        "Text-independent speaker recognition using short utterances is a highly\nchallenging task due to the large variation and content mismatch between\nshort utterances. I-vector and probabilistic linear discriminant analysis\n(PLDA) based systems have become the standard in speaker verification\napplications, but they are less effective with short utterances. To\naddress this issue, we propose a novel method, which trains a convolutional\nneural network (CNN) model to map the i-vectors extracted from short\nutterances to the corresponding long-utterance i-vectors. In order\nto simultaneously learn the representation of the original short-utterance\ni-vectors and fit the target long-version i-vectors, we jointly train\na supervised-regression model with an autoencoder using CNNs. The trained\nCNN model is then used to generate the mapped version of short-utterance\ni-vectors in the evaluation stage. We compare our proposed CNN-based\njoint mapping method with a GMM-based joint modeling method under matched\nand mismatched PLDA training conditions. Experimental results using\nthe NIST SRE 2008 dataset show that the proposed technique achieves\nup to 30% relative improvement under duration mismatched PLDA-training\nconditions and outperforms the GMM-based method. The improved systems\nalso perform better compared with the matched-length PLDA training\ncondition using short utterances.\n"
      ],
      "doi": "10.21437/Interspeech.2017-430"
    },
    "ranjan17b_interspeech": {
      "authors": [
        [
          "Shivesh",
          "Ranjan"
        ],
        [
          "Abhinav",
          "Misra"
        ],
        [
          "John H.L.",
          "Hansen"
        ]
      ],
      "title": "Curriculum Learning Based Probabilistic Linear Discriminant Analysis for Noise Robust Speaker Recognition",
      "original": "1199",
      "page_count": 5,
      "order": 778,
      "p1": "3717",
      "pn": "3721",
      "abstract": [
        "This study introduces a novel Curriculum Learning based Probabilistic\nLinear Discriminant Analysis (CL-PLDA) algorithm for improving speaker\nrecognition in noisy conditions. CL-PLDA operates by initializing the\ntraining EM algorithm with cleaner data ( easy examples), and successively\nadds noisier data ( difficult examples) as the training progresses.\nThis curriculum learning based approach guides the parameters of CL-PLDA\nto better local minima compared to regular PLDA. We test CL-PLDA on\nspeaker verification task of the severely noisy and degraded DARPA\nRATS data, and show it to significantly outperform regular PLDA across\ntest-sets of varying duration.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1199"
    },
    "mahto17_interspeech": {
      "authors": [
        [
          "Shivangi",
          "Mahto"
        ],
        [
          "Hitoshi",
          "Yamamoto"
        ],
        [
          "Takafumi",
          "Koshinaka"
        ]
      ],
      "title": "i-Vector Transformation Using a Novel Discriminative Denoising Autoencoder for Noise-Robust Speaker Recognition",
      "original": "0731",
      "page_count": 5,
      "order": 779,
      "p1": "3722",
      "pn": "3726",
      "abstract": [
        "This paper proposes i-vector transformations using neural networks\nfor achieving noise-robust speaker recognition. A novel discriminative\ndenoising autoencoder (DDAE) is employed on i-vectors to remove additive\nnoise effects. The DDAE is trained to denoise and classify noisy i-vectors\nsimultaneously, making it possible to add discriminability to the denoised\ni-vectors. Speaker recognition experiments on the NIST SRE 2012 task\nshows 32% better error performance as compared to a baseline system.\nAlso, our proposed method outperforms such conventional methods as\nmulti-condition training and a basic denoising autoencoder.\n"
      ],
      "doi": "10.21437/Interspeech.2017-731"
    },
    "wang17l_interspeech": {
      "authors": [
        [
          "Qiongqiong",
          "Wang"
        ],
        [
          "Takafumi",
          "Koshinaka"
        ]
      ],
      "title": "Unsupervised Discriminative Training of PLDA for Domain Adaptation in Speaker Verification",
      "original": "0727",
      "page_count": 5,
      "order": 780,
      "p1": "3727",
      "pn": "3731",
      "abstract": [
        "This paper presents, for the first time, unsupervised discriminative\ntraining of probabilistic linear discriminant analysis (unsupervised\nDT-PLDA). While discriminative training avoids the problem of generative\ntraining based on probabilistic model assumptions that often do not\nagree with actual data, it has been difficult to apply it to unsupervised\nscenarios because it can fit data with almost any labels. This paper\nfocuses on unsupervised training of DT-PLDA in the application of domain\nadaptation in i-vector based speaker verification systems, using unlabeled\nin-domain data. The proposed method makes it possible to conduct discriminative\ntraining, i.e., estimation of model parameters and unknown labels,\nby employing data statistics as a regularization term in addition to\nthe original objective function in DT-PLDA. An experiment on a NIST\nSpeaker Recognition Evaluation task shows that the proposed method\noutperforms a conventional method using speaker clustering and performs\nalmost as well as supervised DT-PLDA.\n"
      ],
      "doi": "10.21437/Interspeech.2017-727"
    },
    "alam17b_interspeech": {
      "authors": [
        [
          "Jahangir",
          "Alam"
        ],
        [
          "Patrick",
          "Kenny"
        ],
        [
          "Gautam",
          "Bhattacharya"
        ],
        [
          "Marcel",
          "Kockmann"
        ]
      ],
      "title": "Speaker Verification Under Adverse Conditions Using i-Vector Adaptation and Neural Networks",
      "original": "1240",
      "page_count": 5,
      "order": 781,
      "p1": "3732",
      "pn": "3736",
      "abstract": [
        "The main challenges introduced in the 2016 NIST speaker recognition\nevaluation (SRE16) are domain mismatch between training and evaluation\ndata, duration variability in test recordings and unlabeled in-domain\ntraining data. This paper outlines the systems developed at CRIM for\nSRE16. To tackle the domain mismatch problem, we apply minimum divergence\ntraining to adapt a conventional i-vector extractor to the task domain.\nSpecifically, we take an out-of-domain trained i-vector extractor as\nan initialization and perform few iterations of minimum divergence\ntraining on the unlabeled data provided. Next, we non-linearly transform\nthe adapted i-vectors by learning a speaker classifier neural network.\nSpeaker features extracted from this network have been shown to be\nmore robust than i-vectors under domain mismatch conditions with a\nreduction in equal error rates of 2&#8211;3% absolute. Finally, we\npropose a new Beta-Bernoulli backend that models the features supplied\nby the speaker classifier network. Our best single system is the speaker\nclassifier network - Beta-Bernoulli backend combination. Overall system\nperformance was very satisfactory for the fixed condition task. With\nour submitted fused system we achieve an equal error rate of 9.89%.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1240"
    },
    "castan17_interspeech": {
      "authors": [
        [
          "Diego",
          "Castan"
        ],
        [
          "Mitchell",
          "McLaren"
        ],
        [
          "Luciana",
          "Ferrer"
        ],
        [
          "Aaron",
          "Lawson"
        ],
        [
          "Alicia",
          "Lozano-Diez"
        ]
      ],
      "title": "Improving Robustness of Speaker Recognition to New Conditions Using Unlabeled Data",
      "original": "0605",
      "page_count": 5,
      "order": 782,
      "p1": "3737",
      "pn": "3741",
      "abstract": [
        "Unsupervised techniques for the adaptation of speaker recognition are\nimportant due to the problem of condition mismatch that is prevalent\nwhen applying speaker recognition technology to new conditions and\nthe general scarcity of labeled &#8216;in-domain&#8217; data. In the\nrecent NIST 2016 Speaker Recognition Evaluation (SRE), symmetric score\nnormalization (S-norm) and calibration using unlabeled in-domain data\nwere shown to be beneficial. Because calibration requires speaker labels\nfor training, speaker-clustering techniques were used to generate pseudo-speakers\nfor learning calibration parameters in those cases where only unlabeled\nin-domain data was available. These methods performed well in the SRE16.\nIt is unclear, however, whether those techniques generalize well to\nother data sources. In this work, we benchmark these approaches on\nseveral distinctly different databases, after we describe our SRI-CON-UAM\nteam system submission for the NIST 2016 SRE. Our analysis shows that\nwhile the benefit of S-norm is also observed across other datasets,\napplying speaker-clustered calibration provides considerably greater\nbenefit to the system in the context of new acoustic conditions.\n"
      ],
      "doi": "10.21437/Interspeech.2017-605"
    },
    "abidi17_interspeech": {
      "authors": [
        [
          "K.",
          "Abidi"
        ],
        [
          "M.A.",
          "Menacer"
        ],
        [
          "Kamel",
          "Sma\u00efli"
        ]
      ],
      "title": "CALYOU: A Comparable Spoken Algerian Corpus Harvested from YouTube",
      "original": "1305",
      "page_count": 5,
      "order": 783,
      "p1": "3742",
      "pn": "3746",
      "abstract": [
        "This paper addresses the issue of comparability of comments extracted\nfrom Youtube. The comments concern spoken Algerian that could be either\nlocal Arabic, Modern Standard Arabic or French. This diversity of expression\ngives rise to a huge number of problems concerning the data processing.\nIn this article, several methods of alignment will be proposed and\ntested. The method which permits to best align is Word2Vec-based approach\nthat will be used iteratively. This recurrent call of Word2Vec allows\nus improve significantly the results of comparability. In fact, a dictionary-based\napproach leads to a Recall of 4, while our approach allows one to get\na Recall of 33 at rank 1. Thanks to this approach, we built from Youtube\nCALYOU, a Comparable Corpus of the spoken Algerian.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1305"
    },
    "narwekar17_interspeech": {
      "authors": [
        [
          "Abhishek",
          "Narwekar"
        ],
        [
          "Prasanta Kumar",
          "Ghosh"
        ]
      ],
      "title": "PRAV: A Phonetically Rich Audio Visual Corpus",
      "original": "0242",
      "page_count": 5,
      "order": 784,
      "p1": "3747",
      "pn": "3751",
      "abstract": [
        "This paper describes the acquisition of PRAV, a phonetically rich audio-visual\nCorpus. The PRAV Corpus contains audio as well as visual recordings\nof 2368 sentences from the TIMIT corpus each spoken by four subjects,\nmaking it the largest audio-visual corpus in the literature in terms\nof the number of sentences per subject. Visual features, comprising\nthe coordinates of points along the contour of the subjects lips, have\nbeen extracted for the entire PRAV Corpus using the Active Appearance\nModels (AAM) algorithm and have been made available along with the\naudio and video recordings. The subjects being Indian makes PRAV an\nideal resource for audio-visual speech study with non-native English\nspeakers. Moreover, this paper describes how the large number of sentences\nper subject makes the PRAV Corpus a significant dataset by highlighting\nits utility in exploring a number of potential research problems including\nvisual speech synthesis and perception studies.\n"
      ],
      "doi": "10.21437/Interspeech.2017-242"
    },
    "abdelaziz17b_interspeech": {
      "authors": [
        [
          "Ahmed Hussen",
          "Abdelaziz"
        ]
      ],
      "title": "NTCD-TIMIT: A New Database and Baseline for Noise-Robust Audio-Visual Speech Recognition",
      "original": "0860",
      "page_count": 5,
      "order": 785,
      "p1": "3752",
      "pn": "3756",
      "abstract": [
        "Although audio-visual speech is well known to improve the robustness\nproperties of automatic speech recognition (ASR) systems against noise,\nthe realm of audio-visual ASR (AV-ASR) has not gathered the research\nmomentum it deserves. This is mainly due to the lack of audio-visual\ncorpora and the need to combine two fields of knowledge: ASR and computer\nvision. This paper describes the NTCD-TIMIT database and baseline that\ncan overcome these two barriers and attract more research interest\nto AV-ASR. The NTCD-TIMIT corpus has been created by adding six noise\ntypes at a range of signal-to-noise ratios to the speech material of\nthe recently published TCD-TIMIT corpus. NTCD-TIMIT comprises visual\nfeatures that have been extracted from the TCD-TIMIT video recordings\nusing the visual front-end presented in this paper. The database contains\nalso Kaldi scripts for training and decoding audio-only, video-only,\nand audio-visual ASR models. The baseline experiments and results obtained\nusing these scripts are detailed in this paper.\n"
      ],
      "doi": "10.21437/Interspeech.2017-860"
    },
    "howcroft17_interspeech": {
      "authors": [
        [
          "David M.",
          "Howcroft"
        ],
        [
          "Dietrich",
          "Klakow"
        ],
        [
          "Vera",
          "Demberg"
        ]
      ],
      "title": "The Extended SPaRKy Restaurant Corpus: Designing a Corpus with Variable Information Density",
      "original": "1555",
      "page_count": 5,
      "order": 786,
      "p1": "3757",
      "pn": "3761",
      "abstract": [
        "Natural language generation (NLG) systems rely on corpora for both\nhand-crafted approaches in a traditional NLG architecture and for statistical\nend-to-end (learned) generation systems. Limitations in existing resources,\nhowever, make it difficult to develop systems which can vary the linguistic\nproperties of an utterance as needed. For example, when users&#8217;\nattention is split between a linguistic and a secondary task such as\ndriving, a generation system may need to reduce the information density\nof an utterance to compensate for the reduction in user attention.<br\nstyle=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  We introduce a new corpus in the restaurant recommendation and\ncomparison domain, collected in a paraphrasing paradigm, where subjects\nwrote texts targeting either a general audience or an elderly family\nmember. This design resulted in a corpus of more than 5000 texts which\nexhibit a variety of lexical and syntactic choices and differ with\nrespect to average word &amp; sentence length and surprisal. The corpus\nincludes two levels of meaning representation: flat &#8216;semantic\nstacks&#8217; for propositional content and Rhetorical Structure Theory\n(RST) relations between these propositions.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1555"
    },
    "mansikkaniemi17_interspeech": {
      "authors": [
        [
          "Andr\u00e9",
          "Mansikkaniemi"
        ],
        [
          "Peter",
          "Smit"
        ],
        [
          "Mikko",
          "Kurimo"
        ]
      ],
      "title": "Automatic Construction of the Finnish Parliament Speech Corpus",
      "original": "1115",
      "page_count": 5,
      "order": 787,
      "p1": "3762",
      "pn": "3766",
      "abstract": [
        "Automatic speech recognition (ASR) systems require large amounts of\ntranscribed speech data, for training state-of-the-art deep neural\nnetwork (DNN) acoustic models. Transcribed speech is a scarce and expensive\nresource, and ASR systems are prone to underperform in domains where\nthere is not a lot of training data available. In this work, we open\nup a vast and previously unused resource of transcribed speech for\nFinnish, by retrieving and aligning all the recordings and meeting\ntranscripts from the web portal of the Parliament of Finland. Short\nspeech-text segment pairs are retrieved from the audio and text material,\nby using the Levenshtein algorithm to align the first-pass ASR hypotheses\nwith the corresponding meeting transcripts. DNN acoustic models are\ntrained on the automatically constructed corpus, and performance is\ncompared to other models trained on a commercially available speech\ncorpus. Model performance is evaluated on Finnish parliament speech,\nby dividing the testing set into seen and unseen speakers. Performance\nis also evaluated on broadcast speech to test the general applicability\nof the parliament speech corpus. We also study the use of meeting transcripts\nin language model adaptation, to achieve additional gains in speech\nrecognition accuracy of Finnish parliament speech.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1115"
    },
    "abdo17_interspeech": {
      "authors": [
        [
          "Omnia",
          "Abdo"
        ],
        [
          "Sherif",
          "Abdou"
        ],
        [
          "Mervat",
          "Fashal"
        ]
      ],
      "title": "Building Audio-Visual Phonetically Annotated Arabic Corpus for Expressive Text to Speech",
      "original": "1357",
      "page_count": 5,
      "order": 788,
      "p1": "3767",
      "pn": "3771",
      "abstract": [
        "The present research aims to build an MSA audio-visual corpus. The\ncorpus is annotated both phonetically and visually and dedicated to\nemotional speech processing studies. The building of the corpus consists\nof 5 main stages: speaker selection, sentences selection, recording,\nannotation and evaluation. 500 sentences were critically selected based\non their phonemic distribution. The speaker was instructed to read\nthe same 500 sentences with 6 emotions (Happiness &#8211; Sadness &#8211;\nFear &#8211; Anger &#8211; Inquiry &#8211; Neutral). A sample of 50\nsentences was selected for annotation. The corpus evaluation modules\nwere: audio, visual and audio-visual subjective evaluation.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  The corpus evaluation\nprocess showed that happy, anger and inquiry emotions were better recognized\nvisually (94%, 96% and 96%) than audibly (63.6%, 74% and 74%) and the\naudio visual evaluation scores (96%, 89.6% and 80.8%). Sadness and\nfear emotion on the other hand were better recognized audibly (76.8%\nand 97.6%) than visually (58% and 78.8 %) and the audio visual evaluation\nscores were (65.6% and 90%).\n"
      ],
      "doi": "10.21437/Interspeech.2017-1357"
    },
    "hughes17_interspeech": {
      "authors": [
        [
          "Vincent",
          "Hughes"
        ],
        [
          "Paul",
          "Foulkes"
        ]
      ],
      "title": "What is the Relevant Population? Considerations for the Computation of Likelihood Ratios in Forensic Voice Comparison",
      "original": "1368",
      "page_count": 5,
      "order": 789,
      "p1": "3772",
      "pn": "3776",
      "abstract": [
        "In forensic voice comparison, it is essential to consider not only\nthe similarity between samples, but also the typicality of the evidence\nin the relevant population. This is explicit within the likelihood\nratio (LR) framework. A significant issue, however, is the definition\nof the  relevant population. This paper explores the complexity of\npopulation selection for voice evidence. We evaluate the effects of\npopulation specificity in terms of regional background on LR output\nusing combinations of the F1, F2, and F3 trajectories of the diphthong\n/a&#618;/. LRs were computed using development and reference data which\nwere regionally  matched (Standard Southern British English) and  mixed\n(general British English) relative to the test data. These conditions\nreflect the paradox that without knowing who the offender is, it is\nnot possible to know the population of which he is a member. Results\nshow that the more specific population produced stronger evidence and\nbetter system validity than the more general definition. However, as\nregion-specific voice features (lower formants) were removed, the difference\nin the output from the  matched and  mixed systems was reduced. This\nshows that the effects of population selection are dependent on the\nsociolinguistic constraints on the feature analysed.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1368"
    },
    "delvaux17_interspeech": {
      "authors": [
        [
          "V\u00e9ronique",
          "Delvaux"
        ],
        [
          "Lise",
          "Caucheteux"
        ],
        [
          "Kathy",
          "Huet"
        ],
        [
          "Myriam",
          "Piccaluga"
        ],
        [
          "Bernard",
          "Harmegnies"
        ]
      ],
      "title": "Voice Disguise vs. Impersonation: Acoustic and Perceptual Measurements of Vocal Flexibility in Non Experts",
      "original": "1080",
      "page_count": 5,
      "order": 790,
      "p1": "3777",
      "pn": "3781",
      "abstract": [
        "The aim of this study is to assess the potential for deliberately changing\none&#8217;s voice as a means to conceal or falsify identity, comparing\nacoustic and perceptual measurements of carefully controlled speech\nproductions.<br style=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  Twenty-two non expert speakers read a phonetically-balanced text\n5 times in various conditions including natural speech, free vocal\ndisguise (2 disguises per speaker), impersonation of a common target\nfor all speakers, impersonation of one specific target per speaker.\nLong-term average spectra (LTAS) were computed for each reading and\nmultiple pairwise comparisons were performed using the SDDD dissimilarity\nindex.<br style=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  The acoustic analysis showed that all speakers were able to deliberately\nchange their voice beyond self-typical natural variation, whether in\nattempting to simply disguise their identity or to impersonate a specific\ntarget. Although the magnitude of the acoustic changes was comparable\nin disguise vs. impersonation, overall it was limited in that it did\nnot achieved between-speaker variation levels. Perceptual judgements\nperformed on the same material revealed that naive listeners were better\nat discriminating between impersonators and targets than at simply\ndetecting voice disguise.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1080"
    },
    "wu17g_interspeech": {
      "authors": [
        [
          "Yaru",
          "Wu"
        ],
        [
          "Martine",
          "Adda-Decker"
        ],
        [
          "C\u00e9cile",
          "Fougeron"
        ],
        [
          "Lori",
          "Lamel"
        ]
      ],
      "title": "Schwa Realization in French: Using Automatic Speech Processing to Study Phonological and Socio-Linguistic Factors in Large Corpora",
      "original": "0470",
      "page_count": 5,
      "order": 791,
      "p1": "3782",
      "pn": "3786",
      "abstract": [
        "The study investigates different factors influencing schwa realization\nin French: phonological factors, speech style, gender, and socio-professional\nstatus. Three large corpora, two of public journalistic speech (ESTER\nand ETAPE) and one of casual speech (NCCFr) are used. The absence/presence\nof schwa is automatically decided via forced alignment, which has a\nsuccessful performance rate of 95%. Only polysyllabic words including\na potential schwa in the word-initial syllable are studied in order\nto control for variability in word structure and position. The effect\nof the left context, grouped into classes of a word final vowel or\nfinal consonant or a pause, is studied. Words preceded by a vowel (V#)\ntend to favor schwa deletion. Interestingly, words preceded by a consonant\nor a pause have similar behaviors: speakers tend to maintain schwa\nin both contexts. As can be expected, the more casual the speech, the\nmore frequently schwa is dropped. Males tend to delete more schwas\nthan females, and journalists are more likely to delete schwa than\npoliticians. These results suggest that beyond phonology, other factors\nsuch as gender, style and socio-professional status influence the realization\nof schwa.\n"
      ],
      "doi": "10.21437/Interspeech.2017-470"
    },
    "duran17_interspeech": {
      "authors": [
        [
          "Daniel",
          "Duran"
        ],
        [
          "Jagoda",
          "Bruni"
        ],
        [
          "Grzegorz",
          "Dogil"
        ],
        [
          "Justus",
          "Roux"
        ]
      ],
      "title": "The Social Life of Setswana Ejectives",
      "original": "0922",
      "page_count": 5,
      "order": 792,
      "p1": "3787",
      "pn": "3791",
      "abstract": [
        "This paper presents a first phonetic analysis of voiced, devoiced and\nejectivized stop sounds in Setswana taken from two different speech\ndatabases. It is observed that rules governing the voicing/devoicing\nprocesses depend on sociophonetic and ethnolinguistic factors. Speakers,\nespecially women, from the rural North West area of South Africa tend\nto preserve the phonologically stronger devoiced (or even ejectivized)\nforms, both in single standing plosives as well as in the post-nasal\ncontext (NC&#x325;). On the other hand, in the more industrialized\narea of Gauteng, voiced forms of plosives prevail. The empirically\nobserved data is modelled with  KaMoso, a computational multi-agent\nsimulation framework. So far, this framework focused on open social\nstructures ( whole world networks) that facilitate language modernization\nthrough exchange between different phonetic forms. The updated model\nhas been enriched with social/phonetic simulation scenarios in which\nspeech agents interact between each other in a so-called  parochial\nsetting, reflecting smaller, closed communities. Both configurations\ncorrespond to the sociopolitical changes that have been taking place\nin South Africa over the last decades, showing the differences in speech\nbetween women and men from rural and industrialized areas of the country.\n"
      ],
      "doi": "10.21437/Interspeech.2017-922"
    },
    "kohtz17_interspeech": {
      "authors": [
        [
          "Lea S.",
          "Kohtz"
        ],
        [
          "Oliver",
          "Niebuhr"
        ]
      ],
      "title": "How Long is Too Long? How Pause Features After Requests Affect the Perceived Willingness of Affirmative Answers",
      "original": "0050",
      "page_count": 5,
      "order": 793,
      "p1": "3792",
      "pn": "3796",
      "abstract": [
        "A perception experiment involving 28 German listeners is presented.\nIt investigates &#8212; for sequences of request, pause, and affirmative\nanswer &#8212; the effect of pause duration on the answerer&#8217;s\nperceived willingness to comply with the request. Replicating earlier\nresults on American English, perceived willingness was found to decrease\nwith increasing pause duration, particularly above a &#8220;tolerance\nthreshold&#8221; of 600 ms. Refining and qualifying this replicated\nresult, the perception experiment showed additional effects of speaking-rate\ncontext and pause quality (silence vs. breathing vs. caf&#233; noise)\non perceived willingness judgments. The overall results picture is\ndiscussed with respect to the origin of the &#8220;tolerance threshold&#8221;,\nthe status of breathing in speech, and the function of pauses in communication.\n"
      ],
      "doi": "10.21437/Interspeech.2017-50"
    },
    "gessinger17_interspeech": {
      "authors": [
        [
          "Iona",
          "Gessinger"
        ],
        [
          "Eran",
          "Raveh"
        ],
        [
          "S\u00e9bastien Le",
          "Maguer"
        ],
        [
          "Bernd",
          "M\u00f6bius"
        ],
        [
          "Ingmar",
          "Steiner"
        ]
      ],
      "title": "Shadowing Synthesized Speech &#8212; Segmental Analysis of Phonetic Convergence",
      "original": "1433",
      "page_count": 5,
      "order": 794,
      "p1": "3797",
      "pn": "3801",
      "abstract": [
        "To shed light on the question whether humans converge phonetically\nto synthesized speech, a shadowing experiment was conducted using three\ndifferent types of stimuli &#8212; natural speaker, diphone synthesis,\nand HMM synthesis. Three segment-level phonetic features of German\nthat are well-known to vary across native speakers were examined. The\nfirst feature triggered convergence in roughly one third of the cases\nfor all stimulus types. The second feature showed generally a small\namount of convergence, which may be due to the nature of the feature\nitself. Still the effect was strongest for the natural stimuli, followed\nby the HMM stimuli and weakest for the diphone stimuli. The effect\nof the third feature was clearly observable for the natural stimuli\nand less pronounced in the synthetic stimuli. This is presumably a\nresult of the partly insufficient perceptibility of this target feature\nin the synthetic stimuli and demonstrates the necessity of gaining\nfine-grained control over the synthesis output, should it be intended\nto implement capabilities of phonetic convergence on the segmental\nlevel in spoken dialogue systems.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1433"
    },
    "ghaffarzadegan17_interspeech": {
      "authors": [
        [
          "Shabnam",
          "Ghaffarzadegan"
        ],
        [
          "Attila",
          "Reiss"
        ],
        [
          "Mirko",
          "Ruhs"
        ],
        [
          "Robert",
          "Duerichen"
        ],
        [
          "Zhe",
          "Feng"
        ]
      ],
      "title": "Occupancy Detection in Commercial and Residential Environments Using Audio Signal",
      "original": "0524",
      "page_count": 5,
      "order": 795,
      "p1": "3802",
      "pn": "3806",
      "abstract": [
        "Occupancy detection, including presence detection and head count, as\none of the fast growing areas plays an important role in providing\nsafety, comfort and reducing energy consumption both in residential\nand commercial setups. The focus of this study is proposing affordable\nstrategies to increase occupancy detection performance in realistic\nscenarios using only audio signal collected from the environment. We\nuse approximately 100-hour of audio data in residential and commercial\nenvironments to analyze and evaluate our setup. In this study, we take\nadvantage of developments in feature selection methods to choose the\nmost relevant audio features for the task. Attribute and error vs.\nhuman activity analysis are also performed to gain a better understanding\nof the environmental sounds and possible solutions to enhance the performance.\nExperimental results confirm the effectiveness of audio sensor for\noccupancy detection using a cost effective system with presence detection\naccuracy of 96% and 99%, and the head count accuracy of 70% and 95%\nfor the residential and commercial setups, respectively.\n"
      ],
      "doi": "10.21437/Interspeech.2017-524"
    },
    "tran17b_interspeech": {
      "authors": [
        [
          "Huy Dat",
          "Tran"
        ],
        [
          "Wen Zheng Terence",
          "Ng"
        ],
        [
          "Yi Ren",
          "Leng"
        ]
      ],
      "title": "Data Augmentation, Missing Feature Mask and Kernel Classification for Through-the-Wall Acoustic Surveillance",
      "original": "0685",
      "page_count": 5,
      "order": 796,
      "p1": "3807",
      "pn": "3811",
      "abstract": [
        "This paper deals with sound event classification from poor quality\nsignals in the context of &#8220;through-the-wall&#8221; (TTW) surveillance.\nThe task is extremely challenging due to the high level of distortion\nand attenuation caused by complex sound propagation and modulation\neffect from signal acquisition. Another problem, facing in TTW surveillance,\nis the lack of comprehensive training data as the recording is much\nmore complicated than conventional approaches using audio microphones.\nTo address that challenge, we employ a recurrent neural network, particularly\nthe Long Short-Term Memory (LSTM) encoder, to transform conventional\nclean and noisy audio signals into TTW signals to augment additional\ntraining data. Furthermore, a novel missing feature mask kernel classification\nis developed to optimize the classification accuracy of TTW sound event\nclassification. Particularly, Wasserstein distance is calculated from\nreliable intersection regions between pair-wise sound image representations\nand embedded into a probabilistic distance Support Vector Machine (SVM)\nkernel to optimize the TTW data separation. The proposed missing feature\nmask kernel allows effective training with inhomogeneously distorted\ndata and the experimental results show promising results on TTW audio\nrecordings, outperforming several state-of-art methods.\n"
      ],
      "doi": "10.21437/Interspeech.2017-685"
    },
    "chang17_interspeech": {
      "authors": [
        [
          "Shuo-Yiin",
          "Chang"
        ],
        [
          "Bo",
          "Li"
        ],
        [
          "Tara N.",
          "Sainath"
        ],
        [
          "Gabor",
          "Simko"
        ],
        [
          "Carolina",
          "Parada"
        ]
      ],
      "title": "Endpoint Detection Using Grid Long Short-Term Memory Networks for Streaming Speech Recognition",
      "original": "0284",
      "page_count": 5,
      "order": 797,
      "p1": "3812",
      "pn": "3816",
      "abstract": [
        "The task of endpointing is to determine when the user has finished\nspeaking. This is important for interactive speech applications such\nas voice search and Google Home. In this paper, we propose a GLDNN-based\n(grid long short-term memory deep neural network) endpointer model\nand show that it provides significant improvements over a state-of-the-art\nCLDNN (convolutional, long short-term memory, deep neural network)\nmodel. Specifically, we replace the convolution layer in the CLDNN\nwith a grid LSTM layer that models both spectral and temporal variations\nthrough recurrent connections. Results show that the GLDNN achieves\n32% relative improvement in false alarm rate at a fixed false reject\nrate of 2%, and reduces median latency by 11%. We also include detailed\nexperiments investigating why grid LSTMs offer better performance than\nconvolution layers. Analysis reveals that the recurrent connection\nalong the frequency axis is an important factor that greatly contributes\nto the performance of grid LSTMs, especially in the presence of background\nnoise. Finally, we also show that multichannel input further increases\nrobustness to background speech. Overall, we achieve 16% (100 ms) endpointer\nlatency improvement relative to our previous best model on a Voice\nSearch Task.\n"
      ],
      "doi": "10.21437/Interspeech.2017-284"
    },
    "baby17_interspeech": {
      "authors": [
        [
          "Arun",
          "Baby"
        ],
        [
          "Jeena J.",
          "Prakash"
        ],
        [
          "Rupak",
          "Vignesh"
        ],
        [
          "Hema A.",
          "Murthy"
        ]
      ],
      "title": "Deep Learning Techniques in Tandem with Signal Processing Cues for Phonetic Segmentation for Text to Speech Synthesis in Indian Languages",
      "original": "0666",
      "page_count": 5,
      "order": 798,
      "p1": "3817",
      "pn": "3821",
      "abstract": [
        "Automatic detection of phoneme boundaries is an important sub-task\nin building speech processing applications, especially text-to-speech\nsynthesis (TTS) systems. The main drawback of the Gaussian mixture\nmodel - hidden Markov model (GMM-HMM) based forced-alignment is that\nthe phoneme boundaries are not explicitly modeled. In an earlier work,\nwe had proposed the use of signal processing cues in tandem with GMM-HMM\nbased forced alignment for boundary correction for building Indian\nlanguage TTS systems. In this paper, we capitalise on the ability of\nrobust acoustic modeling techniques such as deep neural networks (DNN)\nand convolutional deep neural networks (CNN) for acoustic modeling.\nThe GMM-HMM based forced alignment is replaced by DNN-HMM/CNN-HMM based\nforced alignment. Signal processing cues are used to correct the segment\nboundaries obtained using DNN-HMM/CNN-HMM segmentation. TTS systems\nbuilt using these boundaries show a relative improvement in synthesis\nquality.\n"
      ],
      "doi": "10.21437/Interspeech.2017-666"
    },
    "wang17m_interspeech": {
      "authors": [
        [
          "Yu-Hsuan",
          "Wang"
        ],
        [
          "Cheng-Tao",
          "Chung"
        ],
        [
          "Hung-Yi",
          "Lee"
        ]
      ],
      "title": "Gate Activation Signal Analysis for Gated Recurrent Neural Networks and its Correlation with Phoneme Boundaries",
      "original": "0877",
      "page_count": 5,
      "order": 799,
      "p1": "3822",
      "pn": "3826",
      "abstract": [
        "In this paper we analyze the gate activation signals inside the gated\nrecurrent neural networks, and find the temporal structure of such\nsignals is highly correlated with the phoneme boundaries. This correlation\nis further verified by a set of experiments for phoneme segmentation,\nin which better results compared to standard approaches were obtained.\n"
      ],
      "doi": "10.21437/Interspeech.2017-877"
    },
    "yin17_interspeech": {
      "authors": [
        [
          "Ruiqing",
          "Yin"
        ],
        [
          "Herv\u00e9",
          "Bredin"
        ],
        [
          "Claude",
          "Barras"
        ]
      ],
      "title": "Speaker Change Detection in Broadcast TV Using Bidirectional Long Short-Term Memory Networks",
      "original": "0065",
      "page_count": 5,
      "order": 800,
      "p1": "3827",
      "pn": "3831",
      "abstract": [
        "Speaker change detection is an important step in a speaker diarization\nsystem. It aims at finding speaker change points in the audio stream.\nIn this paper, it is treated as a sequence labeling task and addressed\nby Bidirectional long short term memory networks (Bi-LSTM). The system\nis trained and evaluated on the Broadcast TV subset from ETAPE database.\nThe result shows that the proposed model brings good improvement over\nconventional methods based on BIC and Gaussian Divergence. For instance,\nin comparison to Gaussian divergence, it produces speech turns that\nare 19.5% longer on average, with the same level of purity.\n"
      ],
      "doi": "10.21437/Interspeech.2017-65"
    },
    "do17c_interspeech": {
      "authors": [
        [
          "Cong-Thanh",
          "Do"
        ],
        [
          "Yannis",
          "Stylianou"
        ]
      ],
      "title": "Improved Automatic Speech Recognition Using Subband Temporal Envelope Features and Time-Delay Neural Network Denoising Autoencoder",
      "original": "1096",
      "page_count": 5,
      "order": 801,
      "p1": "3832",
      "pn": "3836",
      "abstract": [
        "This paper investigates the use of perceptually-motivated subband temporal\nenvelope (STE) features and time-delay neural network (TDNN) denoising\nautoencoder (DAE) to improve deep neural network (DNN)-based automatic\nspeech recognition (ASR). STEs are estimated by full-wave rectification\nand low-pass filtering of band-passed speech using a Gammatone filter-bank.\nTDNNs are used either as DAE or acoustic models. ASR experiments are\nperformed on Aurora-4 corpus. STE features provide 2.2% and 3.7% relative\nword error rate (WER) reduction compared to conventional log-mel filter-bank\n(FBANK) features when used in ASR systems using DNN and TDNN as acoustic\nmodels, respectively. Features enhanced by TDNN DAE are better recognized\nwith ASR system using DNN acoustic models than using TDNN acoustic\nmodels. Improved ASR performance is obtained when features enhanced\nby TDNN DAE are used in ASR system using DNN acoustic models. In this\nscenario, using STE features provides 9.8% relative WER reduction compared\nto when using FBANK features. \n"
      ],
      "doi": "10.21437/Interspeech.2017-1096"
    },
    "fujimoto17_interspeech": {
      "authors": [
        [
          "Masakiyo",
          "Fujimoto"
        ]
      ],
      "title": "Factored Deep Convolutional Neural Networks for Noise Robust Speech Recognition",
      "original": "0225",
      "page_count": 5,
      "order": 802,
      "p1": "3837",
      "pn": "3841",
      "abstract": [
        "In this paper, we present a framework of a factored deep convolutional\nneural network (CNN) learning for noise robust automatic speech recognition\n(ASR). Deep CNN architecture, which has attracted great attention in\nvarious research areas, has also been successfully applied to ASR.\nHowever, to ensure noise robustness, since merely introducing deep\nCNN architecture into the acoustic modeling of ASR is insufficient,\nwe introduce factored network architecture into deep CNN-based acoustic\nmodeling. The proposed factored deep CNN framework factors out feature\nenhancement, delta parameter learning, and hidden Markov model state\nclassification into three specific network blocks. By assigning specific\nroles to each block, the noise robustness of deep CNN-based acoustic\nmodels can be improved. With various comparative evaluations, we reveal\nthat the proposed method successfully improves ASR accuracies in noise\nenvironments.\n"
      ],
      "doi": "10.21437/Interspeech.2017-225"
    },
    "papadopoulos17b_interspeech": {
      "authors": [
        [
          "Pavlos",
          "Papadopoulos"
        ],
        [
          "Ruchir",
          "Travadi"
        ],
        [
          "Shrikanth S.",
          "Narayanan"
        ]
      ],
      "title": "Global SNR Estimation of Speech Signals for Unknown Noise Conditions Using Noise Adapted Non-Linear Regression",
      "original": "0230",
      "page_count": 5,
      "order": 803,
      "p1": "3842",
      "pn": "3846",
      "abstract": [
        "The performance of speech technologies deteriorates in the presence\nof noise. Additionally, we need these technologies to be able to operate\nacross a variety of noise levels and conditions. SNR estimation can\nguide the design and operation of such technologies or can be used\nas a pre-processing tool in database creation (e.g. identify/discard\nnoisy signals). We propose a new method to estimate the global SNR\nof a speech signal when prior information about the noise that corrupts\nthe signal, and speech boundaries within the signal, are not available.\nTo achieve this goal, we train a neural network that performs non-linear\nregression to estimate the SNR. We use energy ratios as features, as\nwell as ivectors to provide information about the noise that corrupts\nthe signal. We compare our method against others in the literature,\nusing the Mean Absolute Error (MAE) metric, and show that our method\noutperforms them consistently.\n"
      ],
      "doi": "10.21437/Interspeech.2017-230"
    },
    "ge17_interspeech": {
      "authors": [
        [
          "Fengpei",
          "Ge"
        ],
        [
          "Kehuang",
          "Li"
        ],
        [
          "Bo",
          "Wu"
        ],
        [
          "Sabato Marco",
          "Siniscalchi"
        ],
        [
          "Yonghong",
          "Yan"
        ],
        [
          "Chin-Hui",
          "Lee"
        ]
      ],
      "title": "Joint Training of Multi-Channel-Condition Dereverberation and Acoustic Modeling of Microphone Array Speech for Robust Distant Speech Recognition",
      "original": "0579",
      "page_count": 5,
      "order": 804,
      "p1": "3847",
      "pn": "3851",
      "abstract": [
        "We propose a novel data utilization strategy, called multi-channel-condition\nlearning, leveraging upon complementary information captured in microphone\narray speech to jointly train dereverberation and acoustic deep neural\nnetwork (DNN) models for robust distant speech recognition. Experimental\nresults, with a single automatic speech recognition (ASR) system, on\nthe REVERB2014 simulated evaluation data show that, on 1-channel testing,\nthe baseline joint training scheme attains a word error rate (WER)\nof 7.47%, reduced from 8.72% for separate training. The proposed multi-channel-condition\nlearning scheme has been experimented on different channel data combinations\nand usage showing many interesting implications. Finally, training\non all 8-channel data and with DNN-based language model rescoring,\na state-of-the-art WER of 4.05% is achieved. We anticipate an even\nlower WER when combining more top ASR systems.\n"
      ],
      "doi": "10.21437/Interspeech.2017-579"
    },
    "tran17c_interspeech": {
      "authors": [
        [
          "Dung T.",
          "Tran"
        ],
        [
          "Marc",
          "Delcroix"
        ],
        [
          "Atsunori",
          "Ogawa"
        ],
        [
          "Tomohiro",
          "Nakatani"
        ]
      ],
      "title": "Uncertainty Decoding with Adaptive Sampling for Noise Robust DNN-Based Acoustic Modeling",
      "original": "0793",
      "page_count": 5,
      "order": 805,
      "p1": "3852",
      "pn": "3856",
      "abstract": [
        "Although deep neural network (DNN) based acoustic models have obtained\nremarkable results, the automatic speech recognition (ASR) performance\nstill remains low in noise and reverberant conditions. To address this\nissue, a speech enhancement front-end is often used before recognition\nto reduce noise. However, the front-end cannot fully suppress noise\nand often introduces artifacts that are limiting the ASR performance\nimprovement. Uncertainty decoding has been proposed to better interconnect\nthe speech enhancement front-end and ASR back-end and mitigate the\nmismatch caused by residual noise and artifacts. By considering features\nas distributions instead of point estimates, the uncertainty decoding\napproach modifies the conventional decoding rules to account for the\nuncertainty emanating from the speech enhancement. Although the concept\nof uncertainty decoding has been investigated for DNN acoustic models\nrecently, finding efficient ways to incorporate distribution of the\nenhanced features within a DNN acoustic model still requires further\ninvestigations. In this paper, we propose to parameterize the distribution\nof the enhanced feature and estimate the parameters by backpropagation\nusing an unsupervised adaptation scheme. We demonstrate the effectiveness\nof the proposed approach on real audio data of the CHiME3 dataset.\n"
      ],
      "doi": "10.21437/Interspeech.2017-793"
    },
    "zhang17l_interspeech": {
      "authors": [
        [
          "Yu",
          "Zhang"
        ],
        [
          "Pengyuan",
          "Zhang"
        ],
        [
          "Yonghong",
          "Yan"
        ]
      ],
      "title": "Attention-Based LSTM with Multi-Task Learning for Distant Speech Recognition",
      "original": "0805",
      "page_count": 5,
      "order": 806,
      "p1": "3857",
      "pn": "3861",
      "abstract": [
        "Distant speech recognition is a highly challenging task due to background\nnoise, reverberation, and speech overlap. Recently, there has been\nan increasing focus on attention mechanism. In this paper, we explore\nthe attention mechanism embedded within the long short-term memory\n(LSTM) based acoustic model for large vocabulary distant speech recognition,\ntrained using speech recorded from a single distant microphone (SDM)\nand multiple distant microphones (MDM). Furthermore, multi-task learning\narchitecture is incorporated to improve robustness in which the network\nis trained to perform both a primary senone classification task and\na secondary feature enhancement task. Experiments were conducted on\nthe AMI meeting corpus. On average our model achieved 3.3% and 5.0%\nrelative improvements in word error rate (WER) over the LSTM baseline\nmodel in the SDM and MDM cases, respectively. In addition, the model\nprovided between a 2&#8211;4% absolute WER reduction compared to a\nconventional pipeline of independent processing stage on the MDM task.\n"
      ],
      "doi": "10.21437/Interspeech.2017-805"
    },
    "huang17h_interspeech": {
      "authors": [
        [
          "Hengguan",
          "Huang"
        ],
        [
          "Brian",
          "Mak"
        ]
      ],
      "title": "To Improve the Robustness of LSTM-RNN Acoustic Models Using Higher-Order Feedback from Multiple Histories",
      "original": "1315",
      "page_count": 5,
      "order": 807,
      "p1": "3862",
      "pn": "3866",
      "abstract": [
        "This paper investigates a novel multiple-history long short-term memory\n(MH-LSTM) RNN acoustic model to mitigate the robustness problem of\nnoisy outputs in the form of mis-labeled data and/or mis-alignments.\nConceptually, after an RNN is unfolded in time, the hidden units in\neach layer are re-arranged into ordered sub-layers with a master sub-layer\non top and a set of auxiliary sub-layers below it. Only the master\nsub-layer generates outputs for the next layer whereas the auxiliary\nsub-layers run in parallel with the master sub-layer but with increasing\ntime lags. Each sub-layer also receives higher-order feedback from\na fixed number of sub-layers below it. As a result, each sub-layer\nmaintains a different history of the input speech, and the ensemble\nof all the different histories lends itself to the model&#8217;s robustness.\nThe higher-order connections not only provide shorter feedback paths\nfor error signals to propagate to the farther preceding hidden states\nto better model the long-term memory, but also more feedback paths\nto each model parameter and smooth its update during training. Phoneme\nrecognition results on both real TIMIT data as well as synthetic TIMIT\ndata with noisy labels or alignments show that the new model outperforms\nthe conventional LSTM RNN model.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1315"
    },
    "kim17h_interspeech": {
      "authors": [
        [
          "Suyoun",
          "Kim"
        ],
        [
          "Ian",
          "Lane"
        ]
      ],
      "title": "End-to-End Speech Recognition with Auditory Attention for Multi-Microphone Distance Speech Recognition",
      "original": "1536",
      "page_count": 5,
      "order": 808,
      "p1": "3867",
      "pn": "3871",
      "abstract": [
        "End-to-End speech recognition is a recently proposed approach that\ndirectly transcribes input speech to text using a single model. End-to-End\nspeech recognition methods including Connectionist Temporal Classification\nand Attention-based Encoder Decoder Networks have been shown to obtain\nstate-of-the-art performance on a number of tasks and significantly\nsimplify the modeling, training and decoding procedures for speech\nrecognition. In this paper, we extend our prior work on End-to-End\nspeech recognition focusing on the effectiveness of these models in\nfar-field environments. Specifically, we propose introducing Auditory\nAttention to integrate input from multiple microphones directly within\nan End-to-End speech recognition model, leveraging the attention mechanism\nto dynamically tune the model&#8217;s attention to the most reliable\ninput sources. We evaluate our proposed model on the CHiME-4 task,\nand show substantial improvement compared to a model optimized for\na single microphone input.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1536"
    },
    "menon17_interspeech": {
      "authors": [
        [
          "Anjali",
          "Menon"
        ],
        [
          "Chanwoo",
          "Kim"
        ],
        [
          "Richard M.",
          "Stern"
        ]
      ],
      "title": "Robust Speech Recognition Based on Binaural Auditory Processing",
      "original": "1665",
      "page_count": 5,
      "order": 809,
      "p1": "3872",
      "pn": "3876",
      "abstract": [
        "This paper discusses a combination of techniques for improving speech\nrecognition accuracy in the presence of reverberation and spatially-separated\ninterfering sound sources. Interaural Time Delay (ITD), observed as\na consequence of the difference in arrival times of a sound to the\ntwo ears, is an important feature used by the human auditory system\nto reliably localize and separate sound sources. In addition, the &#8220;precedence\neffect&#8221; helps the auditory system differentiate between the direct\nsound and its subsequent reflections in reverberant environments. This\npaper uses a cross-correlation-based measure across the two channels\nof a binaural signal to isolate the target source by rejecting portions\nof the signal corresponding to larger ITDs. To overcome the effects\nof reverberation, the steady-state components of speech are suppressed,\neffectively boosting the onsets, so as to retain the direct sound and\nsuppress the reflections. Experimental results show a significant improvement\nin recognition accuracy using both these techniques. Cross-correlation-based\nprocessing and steady-state suppression are carried out separately,\nand the order in which these techniques are applied produces differences\nin the resulting recognition accuracy.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1665"
    },
    "caroselli17_interspeech": {
      "authors": [
        [
          "Joe",
          "Caroselli"
        ],
        [
          "Izhak",
          "Shafran"
        ],
        [
          "Arun",
          "Narayanan"
        ],
        [
          "Richard",
          "Rose"
        ]
      ],
      "title": "Adaptive Multichannel Dereverberation for Automatic Speech Recognition",
      "original": "1791",
      "page_count": 5,
      "order": 810,
      "p1": "3877",
      "pn": "3881",
      "abstract": [
        "Reverberation is known to degrade the performance of automatic speech\nrecognition (ASR) systems dramatically in far-field conditions. Adopting\nthe weighted prediction error (WPE) approach, we formulate an online\ndereverberation algorithm for a multi-microphone array. The key contributions\nof this paper are: (a) we demonstrate that dereverberation using WPE\nimproves performance even when the acoustic models are trained using\nmulti-style training (MTR) with noisy, reverberated speech; (b) we\nshow that the gains from WPE are preserved even in large and diverse\nreal-world data sets; (c) we propose an adaptive version for online\nmultichannel ASR tasks which gives similar gains as the non-causal\nversion; and (d) while the algorithm can just be applied for evaluation,\nwe show that also including dereverberation during training gives increased\nperformance gains. We also report how different parameter settings\nof the dereverberation algorithm impacts the ASR performance.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1791"
    },
    "zihlmann17_interspeech": {
      "authors": [
        [
          "Urban",
          "Zihlmann"
        ]
      ],
      "title": "The Effects of Real and Placebo Alcohol on Deaffrication",
      "original": "1579",
      "page_count": 5,
      "order": 811,
      "p1": "3882",
      "pn": "3886",
      "abstract": [
        "The more alcohol a person has consumed, the more mispronunciations\noccur. This study investigates how deaffrication surfaces in Bernese\nSwiss German when speakers are moderately intoxicated (0.05&#8211;0.08%\nVol.), whether these effects can be hidden, and whether a placebo effect\ninteracting with mispronunciation occurs. Five participants reading\na text were recorded as follows. In stage I, they read the text before\nand after drinking placebo alcohol, and finally again after being told\nto enunciate very clearly. 3&#8211;7 days later, the same experiment\nwas repeated with real alcohol. The recordings were then analysed with\n Praat. Despite interspeaker variation, the following generalisations\ncan be made. The most deaffrication occurs in the C_C context both\nwhen speakers are sober and inebriated; affricates in _#, V_C, and\nV_V position encounter more deaffrication in the alcohol stage; and\n/&#x361;t&#643;/ and &#x361;kx are deaffricated more when the speaker\nis intoxicated, with /&#x361;t&#643;/ being the most susceptible to\nmispronunciation. Moreover, when alcohol is consumed, more deaffrication\noccurs, which cannot consciously be controlled. Furthermore, a statistically\nsignificant difference between the pre- and the post-placebo-drinking\nexperiment could be found, which implies that a placebo effect takes\nplace. Nevertheless, the effects of real alcohol are considerably stronger.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1579"
    },
    "mcauliffe17b_interspeech": {
      "authors": [
        [
          "Michael",
          "McAuliffe"
        ],
        [
          "Elias",
          "Stengel-Eskin"
        ],
        [
          "Michaela",
          "Socolof"
        ],
        [
          "Morgan",
          "Sonderegger"
        ]
      ],
      "title": "Polyglot and Speech Corpus Tools: A System for Representing, Integrating, and Querying Speech Corpora",
      "original": "1390",
      "page_count": 5,
      "order": 812,
      "p1": "3887",
      "pn": "3891",
      "abstract": [
        "Speech datasets from many languages, styles, and sources exist in the\nworld, representing significant potential for scientific studies of\nspeech &#8212; particularly given structural similarities among all\nspeech datasets. However, studies using multiple speech corpora remain\ndifficult in practice, due to corpus size, complexity, and differing\nformats. We introduce open-source software for  unified corpus analysis:\nintegrating speech corpora and querying across them. Corpora are stored\nin a custom &#8216;polyglot persistence&#8217; scheme that combines\nthree sub-databases mirroring different data types: a Neo4j graph database\nto represent temporal annotation graph structure, and SQL and InfluxDB\ndatabases to represent meta- and acoustic data. This scheme abstracts\naway from the idiosyncratic formats of different speech corpora, while\nmirroring the structure of different data types improves speed and\nscalability. A Python API and a GUI both allow for: enriching the database\nwith positional, hierarchical, temporal, and signal measures (e.g.\nutterance boundaries, f0) that are useful for linguistic analysis;\nquerying the database using a simple query language; and exporting\nquery results to standard formats for further analysis. We describe\nthe software, summarize two case studies using it to examine effects\non pitch and duration across languages, and outline planned future\ndevelopment.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1390"
    },
    "hughes17b_interspeech": {
      "authors": [
        [
          "Vincent",
          "Hughes"
        ],
        [
          "Philip",
          "Harrison"
        ],
        [
          "Paul",
          "Foulkes"
        ],
        [
          "Peter",
          "French"
        ],
        [
          "Colleen",
          "Kavanagh"
        ],
        [
          "Eugenia San",
          "Segundo"
        ]
      ],
      "title": "Mapping Across Feature Spaces in Forensic Voice Comparison: The Contribution of Auditory-Based Voice Quality to (Semi-)Automatic System Testing",
      "original": "1508",
      "page_count": 5,
      "order": 813,
      "p1": "3892",
      "pn": "3896",
      "abstract": [
        "In forensic voice comparison, there is increasing focus on the integration\nof automatic and phonetic methods to improve the validity and reliability\nof voice evidence to the courts. In line with this, we present a comparison\nof long-term measures of the speech signal to assess the extent to\nwhich they capture complementary speaker-specific information. Likelihood\nratio-based testing was conducted using MFCCs and (linear and Mel-weighted)\nlong-term formant distributions (LTFDs). Fusing automatic and semi-automatic\nsystems yielded limited improvement in performance over the baseline\nMFCC system, indicating that these measures capture essentially the\nsame speaker-specific information. The output from the best performing\nsystem was used to evaluate the contribution of auditory-based analysis\nof supralaryngeal (filter) and laryngeal (source) voice quality in\nsystem testing. Results suggest that the problematic speakers for the\n(semi-)automatic system are, to some extent, predictable from their\nsupralaryngeal voice quality profiles, with the least distinctive speakers\nproducing the weakest evidence and most misclassifications. However,\nthe misclassified pairs were still easily differentiated via auditory\nanalysis. Laryngeal voice quality may thus be useful in resolving problematic\npairs for (semi-)automatic systems, potentially improving their overall\nperformance.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1508"
    },
    "arantes17_interspeech": {
      "authors": [
        [
          "Pablo",
          "Arantes"
        ],
        [
          "Anders",
          "Eriksson"
        ],
        [
          "Suska",
          "Gutzeit"
        ]
      ],
      "title": "Effect of Language, Speaking Style and Speaker on Long-Term F0 Estimation",
      "original": "0449",
      "page_count": 5,
      "order": 814,
      "p1": "3897",
      "pn": "3901",
      "abstract": [
        "In this study, we compared three long-term fundamental frequency estimates\n&#8212; mean, median and base value &#8212; with respect to how fast\nthey approach a stable value, as a function of language, speaking style\nand speaker. The base value concept was developed in search for an\nf<SUB>0</SUB> value which should be invariant under prosodic variation.\nIt has since also been tested in forensic phonetics as a possible speaker-specific\nf<SUB>0</SUB> value. Data used in this study &#8212; recorded speech\nby male and female speakers in seven languages and three speaking styles,\nspontaneous, phrase reading and word list reading &#8212; had been\nrecorded for a previous project. Average stabilisation times for the\nmean, median and base value are 9.76, 9.67 and 8.01 s. Base values\nstabilise significantly faster. Languages differ in both average and\nvariability of the stabilisation times. Values range from 7.14 to 11.41\n(mean), 7.5 to 11.33 (median) and 6.74 to 9.34 (base value). Spontaneous\nspeech yields the most variable stabilisation times for the three estimators\nin Italian and Swedish, for the median in French and Portuguese and\nbase value in German. Speakers within each language do not differ significantly\nin terms of stabilisation time variability for the three estimators.\n"
      ],
      "doi": "10.21437/Interspeech.2017-449"
    },
    "volin17_interspeech": {
      "authors": [
        [
          "Jan",
          "Vol\u00edn"
        ],
        [
          "Tereza",
          "Tykalov\u00e1"
        ],
        [
          "Tom\u00e1\u0161",
          "Bo\u0159il"
        ]
      ],
      "title": "Stability of Prosodic Characteristics Across Age and Gender Groups",
      "original": "1503",
      "page_count": 5,
      "order": 815,
      "p1": "3902",
      "pn": "3906",
      "abstract": [
        "The indexical function of speech prosody signals the membership of\na speaker in a social group. The factors of age and gender are relatively\neasy to establish but their reflection in speech characteristics can\nbe less straightforward as they interact with other social aspects.\nTherefore, diverse speaker communities should be investigated with\nthe aim of their subsequent comparison. Our study provides data for\nthe population of adult speakers of Czech &#8212; a West Slavic language\nof Central Europe. The sample consists of six age groups (20 to 80\nyears of age) with balanced representation of gender. The search for\nage and gender related attributes covered both global acoustic descriptors\nand linguistically informed prosodic feature extraction. Apart from\ncommonly used measures and methods we also exploited Legendre polynomials,\nk-means clustering and a newly designed Cumulative Slope Index (CSI).\nThe results specify general deceleration of articulation rate with\nage and lowering of F0 in aging Czech women, and reveal an increase\nin CSI of both F0 tracks and intensity curves with age. Furthermore,\nvarious melodic shapes were found to be distributed unequally across\nthe age groups.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1503"
    },
    "plantehebert17_interspeech": {
      "authors": [
        [
          "Julien",
          "Plante-H\u00e9bert"
        ],
        [
          "Victor J.",
          "Boucher"
        ],
        [
          "Boutheina",
          "Jemel"
        ]
      ],
      "title": "Electrophysiological Correlates of Familiar Voice Recognition",
      "original": "1392",
      "page_count": 4,
      "order": 816,
      "p1": "3907",
      "pn": "3910",
      "abstract": [
        "Our previous work using voice lineups has established that listeners\ncan recognize with near-perfect accuracy the voice of familiar individuals.\nIn a forensic perspective, however, there are limitations to the application\nof voice lineups in that some witnesses may not wish to recognize the\nfamiliar voice of a parent or close friend or else provide unreliable\nresponses. Considering this problem, the present study aimed to isolate\nthe electrophysiological markers of voice familiarity. We recorded\nthe evoked response potentials (ERPs) of 11 participants as they listened\nto a set of similar voices in varying utterances (standards of voice\nline ups were used in selecting voices). Within the presented set,\nonly one voice was familiar to the listener (the voice of a parent,\nclose friend, etc.). The ERPs showed a marked difference for heard\nfamiliar voices compared to an unfamiliar set. These are the first\nfindings of a neural marker of voice recognition based on voices that\nare actually familiar to a listener and which take into account utterances\nrather than isolated vowels. The present results thus indicate that\nprotocols of near-perfect voice recognition can be devised without\nusing behavioral responses.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1392"
    },
    "cooperleavitt17_interspeech": {
      "authors": [
        [
          "Jamison",
          "Cooper-Leavitt"
        ],
        [
          "Lori",
          "Lamel"
        ],
        [
          "Annie",
          "Rialland"
        ],
        [
          "Martine",
          "Adda-Decker"
        ],
        [
          "Gilles",
          "Adda"
        ]
      ],
      "title": "Developing an Embosi (Bantu C25) Speech Variant Dictionary to Model Vowel Elision and Morpheme Deletion",
      "original": "1280",
      "page_count": 5,
      "order": 817,
      "p1": "3911",
      "pn": "3915",
      "abstract": [
        "This paper investigates vowel elision and morpheme deletion in Embosi\n(Bantu C25), an under-resourced language spoken in the Republic of\nCongo. We propose that the observed morpheme deletion is morphological,\nand that vowel elision is phonological. The study focuses on vowel\nelision that occurs across word boundaries between the contact of long/short\nvowels (i.e. CV[long] # V[short].CV), and between the contact of short/short\nvowels (CV[short] # V[short].CV). Several different categories of morphemes\nare explored: (i) prepositions ( ya, mo), (ii) class-noun nominal prefixes\n( ba, etc.), (iii) singular subject pronouns ( ng&#225;, n&#596;, wa).\nFor example, the preposition,  ya, regularly deletes allowing for vowel\nelision if vowel contact occurs between the head of the noun phrase\nand the previous word. Phonetically motivated speech variants are proposed\nin the lexicon used for forced alignment (segmentation) enabling these\nphenomena to be quantified in the corpus so as to develop a dictionary\ncontaining relevant phonetic variants.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1280"
    },
    "murphy17_interspeech": {
      "authors": [
        [
          "Andy",
          "Murphy"
        ],
        [
          "Irena",
          "Yanushevskaya"
        ],
        [
          "Ailbhe N\u00ed",
          "Chasaide"
        ],
        [
          "Christer",
          "Gobl"
        ]
      ],
      "title": "R<SUB>d</SUB> as a Control Parameter to Explore Affective Correlates of the Tense-Lax Continuum",
      "original": "1448",
      "page_count": 5,
      "order": 818,
      "p1": "3916",
      "pn": "3920",
      "abstract": [
        "This study uses the R<SUB>d</SUB> glottal waveshape parameter to simulate\nthe phonatory tense-lax continuum and to explore its affective correlates\nin terms of activation and valence. Based on a natural utterance which\nwas inverse filtered and source-parameterised, a range of synthesized\nstimuli varying along the tense-lax continuum were generated using\nR<SUB>d</SUB> as a control parameter. Two additional stimuli were included,\nwhich were versions of the most lax stimuli with additional creak (lax-creaky\nvoice). In a listening test, participants chose an emotion from a set\nof affective labels and indicated its perceived strength. They also\nindicated the naturalness of the stimulus and their confidence in their\njudgment. Results showed that stimuli at the tense end of the range\nwere most frequently associated with  angry, at the lax end of the\nrange the association was with  sad, and in the intermediate range,\nthe association was with  content. Results also indicate, as was found\nin our earlier work, that a particular stimulus can be associated with\nmore than one affect. Overall these results show that R<SUB>d</SUB>\ncan be used as a single control parameter to generate variation along\nthe tense-lax continuum of phonation.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1448"
    },
    "barbosa17_interspeech": {
      "authors": [
        [
          "Pl\u00ednio A.",
          "Barbosa"
        ],
        [
          "Sandra",
          "Madureira"
        ],
        [
          "Philippe Boula de",
          "Mare\u00fcil"
        ]
      ],
      "title": "Cross-Linguistic Distinctions Between Professional and Non-Professional Speaking Styles",
      "original": "0007",
      "page_count": 5,
      "order": 819,
      "p1": "3921",
      "pn": "3925",
      "abstract": [
        "This work investigates acoustic and perceptual differences in four\nlanguage varieties by using a corpus of professional and non-professional\nspeaking styles. The professional stimuli are composed of excerpts\nof broadcast news and political discourses from six subjects in each\ncase. The non-professional stimuli are made up of recordings of 10\nsubjects who read a long story and narrated it subsequently. All this\nmaterial was obtained in four language varieties: Brazilian and European\nPortuguese, standard French and German. The corpus is balanced for\ngender. Eight melodic and intensity parameters were automatically obtained\nfrom excerpts of 10 to 20 seconds. We showed that 6 out of 8 parameters\npartially distinguish professional from non-professional style in the\nfour language varieties. Classification and discrimination tests carried\nout with 12 Brazilian listeners using delexicalised speech showed that\nthese subjects are able to distinguish professional style from non-professional\nstyle with about 2/3 of hits irrespective of language. In comparison,\nan automatic classification using an LDA model performed better in\nclassifying non-professional (96%) against professional styles, but\nnot in classifying professional (42%) against non-professional styles.\n"
      ],
      "doi": "10.21437/Interspeech.2017-7"
    },
    "gendrot17_interspeech": {
      "authors": [
        [
          "Cedric",
          "Gendrot"
        ]
      ],
      "title": "Perception and Production of Word-Final /&#x281;/ in French",
      "original": "0990",
      "page_count": 5,
      "order": 820,
      "p1": "3926",
      "pn": "3930",
      "abstract": [
        "Variability of (French) /&#x281;/ is a frequently studied phenomenon\nshowing that /&#x281;/ can have multiple realizations. In French, all\nthese studies were undertaken using small read corpora and we have\nreason to believe that these corpora don&#8217;t allow to look at the\nfull picture. Indeed factors such as local word frequency, as well\nas speech rate can have almost as much influence as phonemic context\nin the realization of /&#x281;/.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  According to Ohala&#8217;s\nAerodynamic Voicing principle, /&#x281;/ would tend to be either an\nunvoiced fricative or a voiced approximant. We chose to analyze word\nfinal /&#x281;/s as they tend to embrace the largest spectrum of variation.\nThe study realized here is two-fold: a perception study in a specific\nphonemic context, between /a/ and /l/, where /&#x281;/ is realized\nas an approximant, so as to better understand the parameters and their\nthresholds necessary for /&#x281;/ identification, and provide a measure\nof rhoticity.<br style=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  In a second step, keeping the rhoticity measurement in mind, we\nanalyzed the realizations of word final /&#x281;/s in two continuous\nspeech corpora and modelled the realization of /&#x281;/ using predictors\nsuch as diphone and digram frequency, phonemic context and speech rate.\n"
      ],
      "doi": "10.21437/Interspeech.2017-990"
    },
    "narendra17_interspeech": {
      "authors": [
        [
          "N.P.",
          "Narendra"
        ],
        [
          "Manu",
          "Airaksinen"
        ],
        [
          "Paavo",
          "Alku"
        ]
      ],
      "title": "Glottal Source Estimation from Coded Telephone Speech Using a Deep Neural Network",
      "original": "0882",
      "page_count": 5,
      "order": 821,
      "p1": "3931",
      "pn": "3935",
      "abstract": [
        "In speech analysis, the information about the glottal source is obtained\nfrom speech by using glottal inverse filtering (GIF). The accuracy\nof state-of-the-art GIF methods is sufficiently high when the input\nspeech signal is of high-quality (i.e., with little noise or reverberation).\nHowever, in realistic conditions, particularly when GIF is computed\nfrom coded telephone speech, the accuracy of GIF methods deteriorates\nseverely. To robustly estimate the glottal source under coded condition,\na deep neural network (DNN)-based method is proposed. The proposed\nmethod utilizes a DNN to map the speech features extracted from the\ncoded speech to the glottal flow waveform estimated from the corresponding\nclean speech. To generate the coded telephone speech, adaptive multi-rate\n(AMR) codec is utilized which is a widely used speech compression method.\nThe proposed glottal source estimation method is compared with two\nexisting GIF methods, closed phase covariance analysis (CP) and iterative\nadaptive inverse filtering (IAIF). The results indicate that the proposed\nDNN-based method is capable of estimating glottal flow waveforms from\ncoded telephone speech with a considerably better accuracy in comparison\nto CP and IAIF.\n"
      ],
      "doi": "10.21437/Interspeech.2017-882"
    },
    "christodoulides17_interspeech": {
      "authors": [
        [
          "George",
          "Christodoulides"
        ],
        [
          "Mathieu",
          "Avanzi"
        ],
        [
          "Anne Catherine",
          "Simon"
        ]
      ],
      "title": "Automatic Labelling of Prosodic Prominence, Phrasing and Disfluencies in French Speech by Simulating the Perception of Na&#239;ve and Expert Listeners",
      "original": "0971",
      "page_count": 5,
      "order": 822,
      "p1": "3936",
      "pn": "3940",
      "abstract": [
        "We explore the use of machine learning techniques (notably SVM classifiers\nand Conditional Random Fields) to automate the prosodic labelling of\nFrench speech, based on modelling and simulating the perception of\nprosodic events by na&#239;ve and expert listeners. The models are\nbased on previous work on the perception of syllabic prominence and\nhesitation-related disfluencies, and on an experiment on the real-time\nperception of prosodic boundaries. Expert and non-expert listeners\nannotated samples from three multi-genre corpora (CPROM, CPROM-PFC,\nLOCAS-F). Automatic prosodic annotation is approached as a sequence\nlabelling problem, drawing on multiple information sources (acoustic\nfeatures, lexical and shallow syntactic features) in accordance with\nthe experimental findings showing that listeners integrate all such\ninformation in their perception of prosodic segmentation and events.\nWe test combinations of features and machine learning methods, and\nwe compare the automatic labelling with expert annotation. The result\nof this study is a tool that automatically annotates prosodic events\nby simulating the perception of expert and na&#239;ve listeners.\n"
      ],
      "doi": "10.21437/Interspeech.2017-971"
    },
    "levit17_interspeech": {
      "authors": [
        [
          "Michael",
          "Levit"
        ],
        [
          "Yan",
          "Huang"
        ],
        [
          "Shuangyu",
          "Chang"
        ],
        [
          "Yifan",
          "Gong"
        ]
      ],
      "title": "Don&#8217;t Count on ASR to Transcribe for You: Breaking Bias with Two Crowds",
      "original": "0164",
      "page_count": 5,
      "order": 823,
      "p1": "3941",
      "pn": "3945",
      "abstract": [
        "A crowdsourcing approach for collecting high-quality speech transcriptions\nis presented. The approach addresses typical weakness of traditional\nsemi-supervised transcription strategies that show ASR hypotheses to\ntranscribers to help them cope with unclear or ambiguous audio and\nspeed up transcriptions. We explain how the traditional methods introduce\nbias into transcriptions that make it difficult to objectively measure\nsystem improvements against existing baselines, and suggest a two-stage\ncrowdsourcing alternative that, first, iteratively collects transcription\nhypotheses and, then, asks a different crowd to pick the best of them.\nWe show that this alternative not only outperforms the traditional\nmethod in a side-by-side comparison, but it also leads to ASR improvements\ndue to superior quality of acoustic and language models trained on\nthe transcribed data.\n"
      ],
      "doi": "10.21437/Interspeech.2017-164"
    },
    "airaksinen17_interspeech": {
      "authors": [
        [
          "Manu",
          "Airaksinen"
        ],
        [
          "Paavo",
          "Alku"
        ]
      ],
      "title": "Effects of Training Data Variety in Generating Glottal Pulses from Acoustic Features with DNNs",
      "original": "0363",
      "page_count": 5,
      "order": 824,
      "p1": "3946",
      "pn": "3950",
      "abstract": [
        "Glottal volume velocity waveform, the acoustical excitation of voiced\nspeech, cannot be acquired through direct measurements in normal production\nof continuous speech. Glottal inverse filtering (GIF), however, can\nbe used to estimate the glottal flow from recorded speech signals.\nUnfortunately, the usefulness of GIF algorithms is limited since they\nare sensitive to noise and call for high-quality recordings. Recently,\nefforts have been taken to expand the use of GIF by training deep neural\nnetworks (DNNs) to learn a statistical mapping between frame-level\nacoustic features and glottal pulses estimated by GIF. This framework\nhas been successfully utilized in statistical speech synthesis in the\nform of the GlottDNN vocoder which uses a DNN to generate glottal pulses\nto be used as the synthesizer&#8217;s excitation waveform. In this\nstudy, we investigate how the DNN-based generation of glottal pulses\nis affected by training data variety. The evaluation is done using\nboth objective measures as well as subjective listening tests of synthetic\nspeech. The results suggest that the performance of the glottal pulse\ngeneration with DNNs is affected particularly by how well the training\ncorpus suits GIF: processing low-pitched male speech and sustained\nphonations shows better performance than processing high-pitched female\nvoices or continuous speech.\n"
      ],
      "doi": "10.21437/Interspeech.2017-363"
    },
    "hantke17b_interspeech": {
      "authors": [
        [
          "Simone",
          "Hantke"
        ],
        [
          "Zixing",
          "Zhang"
        ],
        [
          "Bj\u00f6rn",
          "Schuller"
        ]
      ],
      "title": "Towards Intelligent Crowdsourcing for Audio Data Annotation: Integrating Active Learning in the Real World",
      "original": "0406",
      "page_count": 5,
      "order": 825,
      "p1": "3951",
      "pn": "3955",
      "abstract": [
        "In this contribution, we combine the advantages of traditional crowdsourcing\nwith contemporary machine learning algorithms with the aim of ultimately\nobtaining reliable training data for audio processing in a faster,\ncheaper and therefore more efficient manner than has been previously\npossible. We propose a novel crowdsourcing approach, which brings a\nsimulated active learning annotation scenario into a real world environment\ncreating an intelligent and gamified crowdsourcing platform for manual\naudio annotation. Our platform combines two active learning query strategies\nwith an internally calculated trustability score to efficiently reduce\nmanual labelling efforts. This reduction is achieved in a twofold manner:\nfirst our system automatically decides if an instance requires annotation;\nsecond, it dynamically decides, depending on the quality of previously\ngathered annotations, on exactly how many annotations are needed to\nreliably label an instance. Results presented indicate that our approach\ndrastically reduces the annotation load and is considerably more efficient\nthan conventional methods.\n"
      ],
      "doi": "10.21437/Interspeech.2017-406"
    },
    "henter17_interspeech": {
      "authors": [
        [
          "Gustav Eje",
          "Henter"
        ],
        [
          "Jaime",
          "Lorenzo-Trueba"
        ],
        [
          "Xin",
          "Wang"
        ],
        [
          "Junichi",
          "Yamagishi"
        ]
      ],
      "title": "Principles for Learning Controllable TTS from Annotated and Latent Variation",
      "original": "0171",
      "page_count": 5,
      "order": 826,
      "p1": "3956",
      "pn": "3960",
      "abstract": [
        "For building flexible and appealing high-quality speech synthesisers,\nit is desirable to be able to accommodate and reproduce fine variations\nin vocal expression present in natural speech. Synthesisers can enable\ncontrol over such output properties by adding adjustable control parameters\nin parallel to their text input. If not annotated in training data,\nthe values of these control inputs can be optimised jointly with the\nmodel parameters. We describe how this established method can be seen\nas approximate maximum likelihood and MAP inference in a latent variable\nmodel. This puts previous ideas of (learned) synthesiser inputs such\nas sentence-level control vectors on a more solid theoretical footing.\nWe furthermore extend the method by restricting the latent variables\nto orthogonal subspaces via a sparse prior. This enables us to learn\ndimensions of variation present also within classes in coarsely annotated\nspeech. As an example, we train an LSTM-based TTS system to learn nuances\nin emotional expression from a speech database annotated with seven\ndifferent acted emotions. Listening tests show that our proposal successfully\ncan synthesise speech with discernible differences in expression within\neach emotion, without compromising the recognisability of synthesised\nemotions compared to an identical system without learned nuances.\n"
      ],
      "doi": "10.21437/Interspeech.2017-171"
    },
    "takamichi17_interspeech": {
      "authors": [
        [
          "Shinnosuke",
          "Takamichi"
        ],
        [
          "Tomoki",
          "Koriyama"
        ],
        [
          "Hiroshi",
          "Saruwatari"
        ]
      ],
      "title": "Sampling-Based Speech Parameter Generation Using Moment-Matching Networks",
      "original": "0362",
      "page_count": 5,
      "order": 827,
      "p1": "3961",
      "pn": "3965",
      "abstract": [
        "This paper presents sampling-based speech parameter generation using\nmoment-matching networks for Deep Neural Network (DNN)-based speech\nsynthesis. Although people never produce exactly the same speech even\nif we try to express the same linguistic and para-linguistic information,\ntypical statistical speech synthesis produces completely the same speech,\ni.e., there is no inter-utterance variation in synthetic speech. To\ngive synthetic speech natural inter-utterance variation, this paper\nbuilds DNN acoustic models that make it possible to randomly sample\nspeech parameters. The DNNs are trained so that they make the moments\nof generated speech parameters close to those of natural speech parameters.\nSince the variation of speech parameters is compressed into a low-dimensional\nsimple prior noise vector, our algorithm has lower computation cost\nthan direct sampling of speech parameters. As the first step towards\ngenerating synthetic speech that has natural inter-utterance variation,\nthis paper investigates whether or not the proposed sampling-based\ngeneration deteriorates synthetic speech quality. In evaluation, we\ncompare speech quality of conventional maximum likelihood-based generation\nand proposed sampling-based generation. The result demonstrates the\nproposed generation causes no degradation in speech quality.\n"
      ],
      "doi": "10.21437/Interspeech.2017-362"
    },
    "pollet17_interspeech": {
      "authors": [
        [
          "Vincent",
          "Pollet"
        ],
        [
          "Enrico",
          "Zovato"
        ],
        [
          "Sufian",
          "Irhimeh"
        ],
        [
          "Pier",
          "Batzu"
        ]
      ],
      "title": "Unit Selection with Hierarchical Cascaded Long Short Term Memory Bidirectional Recurrent Neural Nets",
      "original": "0428",
      "page_count": 5,
      "order": 828,
      "p1": "3966",
      "pn": "3970",
      "abstract": [
        "Bidirectional recurrent neural nets have demonstrated state-of-the-art\nperformance for parametric speech synthesis. In this paper, we introduce\na top-down application of recurrent neural net models to unit-selection\nsynthesis. A hierarchical cascaded network graph predicts context phone\nduration, speech unit encoding and frame-level logF0 information that\nserves as targets for the search of units. The new approach is compared\nwith an existing state-of-art hybrid system that uses Hidden Markov\nModels as basis for the statistical unit search.\n"
      ],
      "doi": "10.21437/Interspeech.2017-428"
    },
    "cooper17_interspeech": {
      "authors": [
        [
          "Erica",
          "Cooper"
        ],
        [
          "Xinyue",
          "Wang"
        ],
        [
          "Alison",
          "Chang"
        ],
        [
          "Yocheved",
          "Levitan"
        ],
        [
          "Julia",
          "Hirschberg"
        ]
      ],
      "title": "Utterance Selection for Optimizing Intelligibility of TTS Voices Trained on ASR Data",
      "original": "0465",
      "page_count": 5,
      "order": 829,
      "p1": "3971",
      "pn": "3975",
      "abstract": [
        "This paper describes experiments in training HMM-based text-to-speech\n(TTS) voices on data collected for Automatic Speech Recognition (ASR)\ntraining. We compare a number of filtering techniques designed to identify\nthe best utterances from a noisy, multi-speaker corpus for training\nvoices, to exclude speech containing noise and to include speech close\nin nature to more traditionally-collected TTS corpora. We also evaluate\nthe use of automatic speech recognizers for intelligibility assessment\nin comparison with crowdsourcing methods. While the goal of this work\nis to develop natural-sounding and intelligible TTS voices in Low Resource\nLanguages (LRLs) rapidly and easily, without the expense of recording\ndata specifically for this purpose, we focus on English initially to\nidentify the best filtering techniques and evaluation methods. We find\nthat, when a large amount of data is available, selecting from the\ncorpus based on criteria such as standard deviation of f0, fast speaking\nrate, and hypo-articulation produces the most intelligible voices.\n"
      ],
      "doi": "10.21437/Interspeech.2017-465"
    },
    "rosenberg17_interspeech": {
      "authors": [
        [
          "Andrew",
          "Rosenberg"
        ],
        [
          "Bhuvana",
          "Ramabhadran"
        ]
      ],
      "title": "Bias and Statistical Significance in Evaluating Speech Synthesis with Mean Opinion Scores",
      "original": "0479",
      "page_count": 5,
      "order": 830,
      "p1": "3976",
      "pn": "3980",
      "abstract": [
        "Listening tests and Mean Opinion Scores (MOS) are the most commonly\nused techniques for the evaluation of speech synthesis quality and\nnaturalness. These are invaluable in the assessment of subjective qualities\nof machine generated stimuli. However, there are a number of challenges\nin understanding the MOS scores that come out of listening tests.<br\nstyle=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  Primarily, we advocate for the use of non-parametric statistical\ntests in the calculation of statistical significance when comparing\nlistening test results.<br style=\"mso-data-placement:same-cell;\" /><br\nstyle=\"mso-data-placement:same-cell;\" />\n  Additionally, based on the\nresults of 46 legacy listening tests, we measure the impact of two\nsources of bias. Bias introduced by individual participants and synthesized\ntext can a dramatic impact on observed MOS scores. For example, we\nfind that on average the mean difference between the highest and lowest\nscoring rater is over 2 MOS points (on a 5 point scale). From this\nobservation, we caution against using any statistical test without\nadjusting for this bias, and provide specific non-parametric recommendations.\n"
      ],
      "doi": "10.21437/Interspeech.2017-479"
    },
    "adiga17b_interspeech": {
      "authors": [
        [
          "Nagaraj",
          "Adiga"
        ],
        [
          "S.R. Mahadeva",
          "Prasanna"
        ]
      ],
      "title": "Phase Modeling Using Integrated Linear Prediction Residual for Statistical Parametric Speech Synthesis",
      "original": "0587",
      "page_count": 5,
      "order": 831,
      "p1": "3981",
      "pn": "3985",
      "abstract": [
        "The conventional statistical parametric speech synthesis (SPSS) focus\non characteristics of the magnitude spectrum of speech for speech synthesis\nby ignoring phase characteristics of speech. In this work, the role\nof phase information to improve the naturalness of synthetic speech\nis explored. The phase characteristics of excitation signal are estimated\nfrom the integrated linear prediction residual (ILPR) using an all-pass\n(AP) filter. The coefficients of the AP filter are estimated by minimizing\nan entropy based objective function from the cosine phase of the analytical\nsignal obtained from ILPR signal. The AP filter coefficients (APCs)\nderived from the AP filter are used as features for modeling phase\nin SPSS. During synthesis time, to generate the excitation signal,\nframe wise generated APCs are used to add the group delay to the impulse\nexcitation. The proposed method is compared with the group delay based\nphase excitation used in the STRAIGHT method. The experimental results\nshow that proposed phased modeling having a better perceptual synthesis\nquality when compared with the STRAIGHT method.\n"
      ],
      "doi": "10.21437/Interspeech.2017-587"
    },
    "gonzalez17_interspeech": {
      "authors": [
        [
          "Jose A.",
          "Gonzalez"
        ],
        [
          "Lam A.",
          "Cheah"
        ],
        [
          "Phil D.",
          "Green"
        ],
        [
          "James M.",
          "Gilbert"
        ],
        [
          "Stephen R.",
          "Ell"
        ],
        [
          "Roger K.",
          "Moore"
        ],
        [
          "Ed",
          "Holdsworth"
        ]
      ],
      "title": "Evaluation of a Silent Speech Interface Based on Magnetic Sensing and Deep Learning for a Phonetically Rich Vocabulary",
      "original": "0802",
      "page_count": 5,
      "order": 832,
      "p1": "3986",
      "pn": "3990",
      "abstract": [
        "To help people who have lost their voice following total laryngectomy,\nwe present a speech restoration system that produces audible speech\nfrom articulator movement. The speech articulators are monitored by\nsensing changes in magnetic field caused by movements of small magnets\nattached to the lips and tongue. Then, articulator movement is mapped\nto a sequence of speech parameter vectors using a transformation learned\nfrom simultaneous recordings of speech and articulatory data. In this\nwork, this transformation is performed using a type of recurrent neural\nnetwork (RNN) with fixed latency, which is suitable for real-time processing.\nThe system is evaluated on a phonetically-rich database with simultaneous\nrecordings of speech and articulatory data made by non-impaired subjects.\nExperimental results show that our RNN-based mapping obtains more accurate\nspeech reconstructions (evaluated using objective quality metrics and\na listening test) than articulatory-to-acoustic mappings using Gaussian\nmixture models (GMMs) or deep neural networks (DNNs). Moreover, our\nfixed-latency RNN architecture provides comparable performance to an\nutterance-level batch mapping using bidirectional RNNs (BiRNNs).\n"
      ],
      "doi": "10.21437/Interspeech.2017-802"
    },
    "greenwood17_interspeech": {
      "authors": [
        [
          "David",
          "Greenwood"
        ],
        [
          "Stephen",
          "Laycock"
        ],
        [
          "Iain",
          "Matthews"
        ]
      ],
      "title": "Predicting Head Pose from Speech with a Conditional Variational Autoencoder",
      "original": "0894",
      "page_count": 5,
      "order": 833,
      "p1": "3991",
      "pn": "3995",
      "abstract": [
        "Natural movement plays a significant role in realistic speech animation.\nNumerous studies have demonstrated the contribution visual cues make\nto the degree we, as human observers, find an animation acceptable.<br\nstyle=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  Rigid head motion is one visual mode that universally co-occurs\nwith speech, and so it is a reasonable strategy to seek a transformation\nfrom the speech mode to predict the head pose. Several previous authors\nhave shown that prediction is possible, but experiments are typically\nconfined to rigidly produced dialogue. Natural, expressive, emotive\nand prosodic speech exhibit motion patterns that are far more difficult\nto predict with considerable variation in expected head pose.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  Recently, Long Short\nTerm Memory (LSTM) networks have become an important tool for modelling\nspeech and natural language tasks. We employ Deep Bi-Directional LSTMs\n(BLSTM) capable of learning long-term structure in language, to model\nthe relationship that speech has with rigid head motion. We then extend\nour model by conditioning with prior motion. Finally, we introduce\na generative head motion model, conditioned on audio features using\na Conditional Variational Autoencoder (CVAE). Each approach mitigates\nthe problems of the one to many mapping that a speech to head pose\nmodel must accommodate.\n"
      ],
      "doi": "10.21437/Interspeech.2017-894"
    },
    "wester17_interspeech": {
      "authors": [
        [
          "Mirjam",
          "Wester"
        ],
        [
          "David A.",
          "Braude"
        ],
        [
          "Blaise",
          "Potard"
        ],
        [
          "Matthew P.",
          "Aylett"
        ],
        [
          "Francesca",
          "Shaw"
        ]
      ],
      "title": "Real-Time Reactive Speech Synthesis: Incorporating Interruptions",
      "original": "1250",
      "page_count": 5,
      "order": 834,
      "p1": "3996",
      "pn": "4000",
      "abstract": [
        "The ability to be interrupted and react in a realistic manner is a\nkey requirement for interactive speech interfaces. While previous systems\nhave long implemented techniques such as &#8216;barge in&#8217; where\nspeech output can be halted at word or phrase boundaries, less work\nhas explored how to mimic human speech output responses to real-time\nevents like interruptions which require a reaction from the system.\nUnlike previous work which has focused on incremental production, here\nwe explore a novel re-planning approach. The proposed system is versatile\nand offers a large range of possible ways to react. A focus group was\nused to evaluate the approach, where participants interacted with a\nsystem reading out a text. The system would react to audio interruptions,\neither with no reactions, passive reactions, or active negative reactions\n(i.e. getting increasingly irritated). Participants preferred a reactive\nsystem.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1250"
    },
    "blaauw17_interspeech": {
      "authors": [
        [
          "Merlijn",
          "Blaauw"
        ],
        [
          "Jordi",
          "Bonada"
        ]
      ],
      "title": "A Neural Parametric Singing Synthesizer",
      "original": "1420",
      "page_count": 5,
      "order": 835,
      "p1": "4001",
      "pn": "4005",
      "abstract": [
        "We present a new model for singing synthesis based on a modified version\nof the WaveNet architecture. Instead of modeling raw waveform, we model\nfeatures produced by a parametric vocoder that separates the influence\nof pitch and timbre. This allows conveniently modifying pitch to match\nany target melody, facilitates training on more modest dataset sizes,\nand significantly reduces training and generation times. Our model\nmakes frame-wise predictions using mixture density outputs rather than\ncategorical outputs in order to reduce the required parameter count.\nAs we found overfitting to be an issue with the relatively small datasets\nused in our experiments, we propose a method to regularize the model\nand make the autoregressive generation process more robust to prediction\nerrors. Using a simple multi-stream architecture, harmonic, aperiodic\nand voiced/unvoiced components can all be predicted in a coherent manner.\nWe compare our method to existing parametric statistical and state-of-the-art\nconcatenative methods using quantitative metrics and a listening test.\nWhile naive implementations of the autoregressive generation algorithm\ntend to be inefficient, using a smart algorithm we can greatly speed\nup the process and obtain a system that&#8217;s competitive in both\nspeed and quality.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1420"
    },
    "wang17n_interspeech": {
      "authors": [
        [
          "Yuxuan",
          "Wang"
        ],
        [
          "R.J.",
          "Skerry-Ryan"
        ],
        [
          "Daisy",
          "Stanton"
        ],
        [
          "Yonghui",
          "Wu"
        ],
        [
          "Ron J.",
          "Weiss"
        ],
        [
          "Navdeep",
          "Jaitly"
        ],
        [
          "Zongheng",
          "Yang"
        ],
        [
          "Ying",
          "Xiao"
        ],
        [
          "Zhifeng",
          "Chen"
        ],
        [
          "Samy",
          "Bengio"
        ],
        [
          "Quoc",
          "Le"
        ],
        [
          "Yannis",
          "Agiomyrgiannakis"
        ],
        [
          "Rob",
          "Clark"
        ],
        [
          "Rif A.",
          "Saurous"
        ]
      ],
      "title": "Tacotron: Towards End-to-End Speech Synthesis",
      "original": "1452",
      "page_count": 5,
      "order": 836,
      "p1": "4006",
      "pn": "4010",
      "abstract": [
        "A text-to-speech synthesis system typically consists of multiple stages,\nsuch as a text analysis frontend, an acoustic model and an audio synthesis\nmodule. Building these components often requires extensive domain expertise\nand may contain brittle design choices. In this paper, we present Tacotron,\nan end-to-end generative text-to-speech model that synthesizes speech\ndirectly from characters. Given &#60;text, audio&#62; pairs, the model\ncan be trained completely from scratch with random initialization.\nWe present several key techniques to make the sequence-to-sequence\nframework perform well for this challenging task. Tacotron achieves\na 3.82 subjective 5-scale mean opinion score on US English, outperforming\na production parametric system in terms of naturalness. In addition,\nsince Tacotron generates speech at the frame level, it&#8217;s substantially\nfaster than sample-level autoregressive methods.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1452"
    },
    "capes17_interspeech": {
      "authors": [
        [
          "Tim",
          "Capes"
        ],
        [
          "Paul",
          "Coles"
        ],
        [
          "Alistair",
          "Conkie"
        ],
        [
          "Ladan",
          "Golipour"
        ],
        [
          "Abie",
          "Hadjitarkhani"
        ],
        [
          "Qiong",
          "Hu"
        ],
        [
          "Nancy",
          "Huddleston"
        ],
        [
          "Melvyn",
          "Hunt"
        ],
        [
          "Jiangchuan",
          "Li"
        ],
        [
          "Matthias",
          "Neeracher"
        ],
        [
          "Kishore",
          "Prahallad"
        ],
        [
          "Tuomo",
          "Raitio"
        ],
        [
          "Ramya",
          "Rasipuram"
        ],
        [
          "Greg",
          "Townsend"
        ],
        [
          "Becci",
          "Williamson"
        ],
        [
          "David",
          "Winarsky"
        ],
        [
          "Zhizheng",
          "Wu"
        ],
        [
          "Hepeng",
          "Zhang"
        ]
      ],
      "title": "Siri On-Device Deep Learning-Guided Unit Selection Text-to-Speech System",
      "original": "1798",
      "page_count": 5,
      "order": 837,
      "p1": "4011",
      "pn": "4015",
      "abstract": [
        "This paper describes Apple&#8217;s hybrid unit selection speech synthesis\nsystem, which provides the voices for Siri with the requirement of\nnaturalness, personality and expressivity. It has been deployed into\nhundreds of millions of desktop and mobile devices (e.g. iPhone, iPad,\nMac, etc.) via iOS and macOS in multiple languages. The system is following\nthe classical unit selection framework with the advantage of using\ndeep learning techniques to boost the performance. In particular, deep\nand recurrent mixture density networks are used to predict the target\nand concatenation reference distributions for respective costs during\nunit selection. In this paper, we present an overview of the run-time\nTTS engine and the voice building process. We also describe various\ntechniques that enable on-device capability such as preselection optimization,\ncaching for low latency, and unit pruning for low footprint, as well\nas techniques that improve the naturalness and expressivity of the\nvoice such as the use of long units.\n"
      ],
      "doi": "10.21437/Interspeech.2017-1798"
    },
    "esch17_interspeech": {
      "authors": [
        [
          "Daan van",
          "Esch"
        ],
        [
          "Richard",
          "Sproat"
        ]
      ],
      "title": "An Expanded Taxonomy of Semiotic Classes for Text Normalization",
      "original": "0402",
      "page_count": 5,
      "order": 838,
      "p1": "4016",
      "pn": "4020",
      "abstract": [
        "We describe an expanded taxonomy of semiotic classes for text normalization,\nbuilding upon the work in [1]. We add a large number of categories\nof non-standard words (NSWs) that we believe a robust real-world text\nnormalization system will have to be able to process. Our new categories\nare based upon empirical findings encountered while building text normalization\nsystems across many languages, for both speech recognition and speech\nsynthesis purposes. We believe our new taxonomy is useful both for\nensuring high coverage when writing manual grammars, as well as for\neliciting training data to build machine learning-based text normalization\nsystems.\n"
      ],
      "doi": "10.21437/Interspeech.2017-402"
    },
    "nakashika17b_interspeech": {
      "authors": [
        [
          "Toru",
          "Nakashika"
        ],
        [
          "Shinji",
          "Takaki"
        ],
        [
          "Junichi",
          "Yamagishi"
        ]
      ],
      "title": "Complex-Valued Restricted Boltzmann Machine for Direct Learning of Frequency Spectra",
      "original": "0584",
      "page_count": 5,
      "order": 839,
      "p1": "4021",
      "pn": "4025",
      "abstract": [
        "In this paper, we propose a new energy-based probabilistic model where\na restricted Boltzmann machine (RBM) is extended to deal with complex-valued\nvisible units. The RBM that automatically learns the relationships\nbetween visible units and hidden units (but without connections in\nthe visible or the hidden units) has been widely used as a feature\nextractor, a generator, a classifier, pre-training of deep neural networks,\netc. However, all the conventional RBMs have assumed the visible units\nto be either binary-valued or real-valued, and therefore complex-valued\ndata cannot be fed to the RBM.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  In various applications,\nhowever, complex-valued data is frequently used such examples include\ncomplex spectra of speech, fMRI images, wireless signals, and acoustic\nintensity. For the direct learning of such the complex-valued data,\nwe define the new model called &#8220;complex-valued RBM (CRBM)&#8221;\nwhere the conditional probability of the complex-valued visible units\ngiven the hidden units forms a complex-Gaussian distribution. Another\nimportant characteristic of the CRBM is to have connections between\nreal and imaginary parts of each of the visible units unlike the conventional\nreal-valued RBM. Our experiments demonstrated that the proposed CRBM\ncan directly encode complex spectra of speech signals without decoupling\nimaginary number or phase from the complex-value data.\n"
      ],
      "doi": "10.21437/Interspeech.2017-584"
    },
    "zioko17_interspeech": {
      "authors": [
        [
          "Bartosz",
          "Zi\u00f3\u0142ko"
        ],
        [
          "Tomasz",
          "P\u0229dzim\u0105\u017c"
        ],
        [
          "Szymon",
          "Pa\u0142ka"
        ]
      ],
      "title": "Soundtracing for Realtime Speech Adjustment to Environmental Conditions in 3D Simulations",
      "original": "2002",
      "page_count": 2,
      "order": 840,
      "p1": "4026",
      "pn": "4027",
      "abstract": [
        "We present a 3D realtime audio engine which utilizes frustum tracing\nto create realistic audio auralization, modifying speech in architectural\nwalkthroughs. All audio effects are computed based on both the geometrical\n(e.g. walls, furniture) and acoustical scene properties (e.g. materials,\nair attenuation). The sound changes dynamically as we change the point\nof perception and sound sources. The engine can be configured to use\nas little as 10 percent of available processing power. Our demonstration\nwill be based on listening radio samples in rooms with similar shape,\nbut different acoustical properties. The described system is a component\nof a virtual reality trainer for firefighters using Oculus Rift. It\nallows to conduct dialogues with victims and to locate them based on\nsound cues.\n"
      ]
    },
    "arai17b_interspeech": {
      "authors": [
        [
          "Takayuki",
          "Arai"
        ]
      ],
      "title": "Vocal-Tract Model with Static Articulators: Lips, Teeth, Tongue, and More",
      "original": "2027",
      "page_count": 2,
      "order": 841,
      "p1": "4028",
      "pn": "4029",
      "abstract": [
        "Our physical models of the human vocal tract successfully demonstrate\ntheories such as the source-filter theory of speech production, mechanisms\nsuch as the relationship between vocal-tract configuration and vowel\nquality, and phenomena such as formant frequency estimation. Earlier\nmodels took one of two directions: either simplification, showing only\na few target themes, or diversification, simulating human articulation\nmore broadly. In this study, we have designed a static, hybrid model.\nEach model of this type produces one vowel. However, the model also\nsimulates the human articulators more broadly, including the lips,\nteeth, and tongue. The sagittal block is enclosed with transparent\nplates so that the inside of the vocal tract is visible from the outside.\nWe also colored the articulators to make them more easily identified.\nIn testing, we confirmed that the vocal-tract models can produce the\ntarget vowel. These models have great potential, with applications\nnot only in acoustics and phonetics education, but also pronunciation\ntraining in language learning and speech therapy in the clinical setting.\n"
      ]
    },
    "masudakatsuse17_interspeech": {
      "authors": [
        [
          "Ikuyo",
          "Masuda-Katsuse"
        ]
      ],
      "title": "Remote Articulation Test System Based on WebRTC",
      "original": "2038",
      "page_count": 2,
      "order": 842,
      "p1": "4030",
      "pn": "4031",
      "abstract": [
        "A remote articulation test system with multimedia communication has\nbeen developed in order that outside speech-language-hearing therapists\n(STs) can exam pronunciations of the students in special education\nclasses in regular elementary schools and give advice to their teachers.\nThe proposed system has video and voice communication and image transmission\nfunctions based on WebRTC. Using image transmission, the ST presents\npicture cards for the word test to the student and asks what is depicted.\nUsing video / voice communication, the ST confirms the student&#8217;s\nvoice and articulation movement. Compared to our previous system in\nwhich written words were presented, the proposed system enables a more\nformal and accurate articulation test.\n"
      ]
    },
    "bunnell17_interspeech": {
      "authors": [
        [
          "H. Timothy",
          "Bunnell"
        ],
        [
          "Jason",
          "Lilley"
        ],
        [
          "Kathleen",
          "McGrath"
        ]
      ],
      "title": "The ModelTalker Project: A Web-Based Voice Banking Pipeline for ALS/MND Patients",
      "original": "2054",
      "page_count": 2,
      "order": 843,
      "p1": "4032",
      "pn": "4033",
      "abstract": [
        "The Nemours ModelTalker supports  voice banking for users diagnosed\nwith ALS/MND and related neurodegenerative diseases. Users record up\nto 1600 sentences from which a synthetic voice is constructed. For\nthe past two years we have focused on extending and refining a web-based\nrecording tool to support this process. In this demonstration, we illustrate\nthe features of the web-based pipeline that guides patients through\nthe process of setting up to record at home, recording a standard speech\ninventory, adding custom recordings, and screening alternative versions\nof their voice and alternative synthesis parameter settings. Finally,\nwe summarize results from 352 individuals with a wide range of speaking\nability, who have recently used this voice banking pipeline. \n"
      ]
    },
    "heeringa17_interspeech": {
      "authors": [
        [
          "Wilbert",
          "Heeringa"
        ],
        [
          "Hans Van de",
          "Velde"
        ]
      ],
      "title": "Visible Vowels: A Tool for the Visualization of Vowel Variation",
      "original": "2055",
      "page_count": 2,
      "order": 844,
      "p1": "4034",
      "pn": "4035",
      "abstract": [
        "This paper presents Visible Vowels, a web app that visualizes variation\nin f0, formants and duration. It combines user friendliness with maximum\nfunctionality and flexibility, using a live plot view.\n"
      ]
    }
  },
  "sessions": [
    {
      "title": "ISCA Medal 2017 Ceremony",
      "papers": [
        "itakura17_interspeech"
      ]
    },
    {
      "title": "Special Session: Interspeech 2017 Automatic Speaker Verification Spoofing and Countermeasures Challenge 1",
      "papers": [
        "kinnunen17_interspeech",
        "font17_interspeech",
        "patil17_interspeech",
        "cai17_interspeech",
        "jelil17_interspeech",
        "witkowski17_interspeech",
        "wang17_interspeech"
      ]
    },
    {
      "title": "Special Session: Speech Technology for Code-Switching in Multilingual Communities",
      "papers": [
        "ylmaz17_interspeech",
        "ylmaz17b_interspeech",
        "ramanarayanan17_interspeech",
        "rallabandi17_interspeech",
        "chandu17_interspeech",
        "amazouz17_interspeech",
        "guzman17_interspeech",
        "westhuizen17_interspeech",
        "soto17_interspeech"
      ]
    },
    {
      "title": "Special Session: Interspeech 2017 Automatic Speaker Verification Spoofing and Countermeasures Challenge 2",
      "papers": [
        "lavrentyeva17_interspeech",
        "ji17_interspeech",
        "li17b_interspeech",
        "nagarsheth17_interspeech",
        "chen17_interspeech",
        "alluri17_interspeech"
      ]
    },
    {
      "title": "Conversational Telephone Speech Recognition",
      "papers": [
        "hartmann17_interspeech",
        "wong17_interspeech",
        "cui17_interspeech",
        "ma17_interspeech",
        "saon17_interspeech",
        "stolcke17_interspeech"
      ]
    },
    {
      "title": "Multimodal Paralinguistics",
      "papers": [
        "petukhova17_interspeech",
        "bone17_interspeech",
        "burmania17_interspeech",
        "fotedar17_interspeech",
        "huang17_interspeech",
        "dohen17_interspeech"
      ]
    },
    {
      "title": "Dereverberation, Echo Cancellation and Speech",
      "papers": [
        "guzewich17_interspeech",
        "bulling17_interspeech",
        "franzen17_interspeech",
        "wang17b_interspeech",
        "ayllon17_interspeech",
        "wu17_interspeech"
      ]
    },
    {
      "title": "Acoustic and Articulatory Phonetics",
      "papers": [
        "hermes17_interspeech",
        "elie17_interspeech",
        "frej17_interspeech",
        "turco17_interspeech",
        "jones17_interspeech",
        "dutta17_interspeech"
      ]
    },
    {
      "title": "Multimodal and Articulatory Synthesis",
      "papers": [
        "cabral17_interspeech",
        "gully17_interspeech",
        "maguer17_interspeech",
        "alexander17_interspeech",
        "mendelson17_interspeech",
        "cao17_interspeech"
      ]
    },
    {
      "title": "Neural Networks for Language Modeling",
      "papers": [
        "ma17b_interspeech",
        "oualil17_interspeech",
        "chen17b_interspeech",
        "huang17b_interspeech",
        "kurata17_interspeech",
        "benes17_interspeech"
      ]
    },
    {
      "title": "Pathological Speech and Language",
      "papers": [
        "poorjam17_interspeech",
        "le17_interspeech",
        "garcia17_interspeech",
        "chien17_interspeech",
        "pokorny17_interspeech",
        "vasquezcorrea17_interspeech"
      ]
    },
    {
      "title": "Speech Analysis and Representation 1",
      "papers": [
        "bai17_interspeech",
        "chen17c_interspeech",
        "vijayan17_interspeech",
        "meireles17_interspeech",
        "nataraj17_interspeech",
        "backstrom17_interspeech"
      ]
    },
    {
      "title": "Perception of Dialects and L2",
      "papers": [
        "ghosh17_interspeech",
        "jacewicz17_interspeech",
        "yoneyama17_interspeech",
        "maastricht17_interspeech",
        "takiguchi17_interspeech",
        "zhang17_interspeech"
      ]
    },
    {
      "title": "Far-field Speech Recognition",
      "papers": [
        "kim17_interspeech",
        "kinoshita17_interspeech",
        "ichikawa17_interspeech",
        "tu17_interspeech",
        "li17c_interspeech",
        "mirsamadi17_interspeech"
      ]
    },
    {
      "title": "Speech Analysis and Representation 2",
      "papers": [
        "morise17_interspeech",
        "loweimi17_interspeech",
        "stone17_interspeech",
        "kawahara17_interspeech",
        "kumar17_interspeech",
        "alradhi17_interspeech",
        "wu17b_interspeech",
        "sharma17_interspeech",
        "laine17_interspeech",
        "kroos17_interspeech"
      ]
    },
    {
      "title": "Speech and Audio Segmentation and Classification 2",
      "papers": [
        "dai17_interspeech",
        "khonglah17_interspeech",
        "guo17_interspeech",
        "xia17_interspeech",
        "jang17_interspeech",
        "sonowal17_interspeech",
        "ebbers17_interspeech",
        "zohrer17_interspeech",
        "mcauliffe17_interspeech",
        "meenakshi17_interspeech"
      ]
    },
    {
      "title": "Search, Computational Strategies and Language Modeling",
      "papers": [
        "williams17_interspeech",
        "zenkel17_interspeech",
        "hadian17_interspeech",
        "chorowski17_interspeech",
        "li17d_interspeech",
        "xiang17_interspeech",
        "chandrashekaran17_interspeech",
        "toyama17_interspeech",
        "pahuja17_interspeech",
        "shen17_interspeech",
        "moro17_interspeech"
      ]
    },
    {
      "title": "Speech Perception",
      "papers": [
        "wang17c_interspeech",
        "wang17d_interspeech",
        "ishida17_interspeech",
        "burchfield17_interspeech",
        "davis17_interspeech",
        "maslowski17_interspeech",
        "peres17_interspeech",
        "guevararukoz17_interspeech",
        "matsui17_interspeech",
        "lorenzotrueba17_interspeech",
        "niebuhr17_interspeech",
        "marques17_interspeech",
        "kim17b_interspeech"
      ]
    },
    {
      "title": "Speech Production and Perception",
      "papers": [
        "silva17_interspeech",
        "somandepalli17_interspeech",
        "asadiabadi17_interspeech",
        "ananthapadmanabha17_interspeech",
        "sorensen17_interspeech",
        "cao17b_interspeech",
        "franken17_interspeech",
        "peters17_interspeech",
        "lai17_interspeech",
        "ito17_interspeech",
        "renner17_interspeech",
        "kyaw17_interspeech",
        "mehta17_interspeech",
        "bandini17_interspeech",
        "sb17_interspeech",
        "romren17_interspeech"
      ]
    },
    {
      "title": "Multi-lingual Models and Adaptation for ASR",
      "papers": [
        "zhou17_interspeech",
        "siohan17_interspeech",
        "tong17_interspeech",
        "karafiat17_interspeech",
        "matassoni17_interspeech",
        "kim17c_interspeech",
        "do17_interspeech",
        "joy17_interspeech",
        "samarakoon17_interspeech",
        "fainberg17_interspeech"
      ]
    },
    {
      "title": "Prosody and Text Processing",
      "papers": [
        "sproat17_interspeech",
        "rendel17_interspeech",
        "ijima17_interspeech",
        "ni17_interspeech",
        "fukuoka17_interspeech",
        "huang17c_interspeech",
        "zheng17_interspeech",
        "chen17d_interspeech",
        "chen17e_interspeech",
        "ribeiro17_interspeech",
        "szekely17_interspeech"
      ]
    },
    {
      "title": "Show &amp; Tell 1",
      "papers": [
        "oktem17_interspeech",
        "vetchinnikova17_interspeech",
        "jochim17_interspeech",
        "warlaumont17_interspeech",
        "bell17_interspeech",
        "bhat17_interspeech"
      ]
    },
    {
      "title": "Show &amp; Tell 2",
      "papers": [
        "jaumardhakoun17_interspeech",
        "draxler17_interspeech",
        "salimbajevs17_interspeech",
        "park17_interspeech",
        "lennes17_interspeech",
        "suzuki17_interspeech"
      ]
    },
    {
      "title": "Keynote 1: James Allen",
      "papers": [
        "allen17_interspeech"
      ]
    },
    {
      "title": "Special Session: Speech and Human-Robot Interaction",
      "papers": [
        "stasak17_interspeech",
        "novoa17_interspeech",
        "turker17_interspeech",
        "baird17_interspeech",
        "oertel17_interspeech",
        "lancia17_interspeech"
      ]
    },
    {
      "title": "Special Session: Incremental Processing and Responsive Behaviour",
      "papers": [
        "delalez17_interspeech",
        "saryazdi17_interspeech",
        "ishi17_interspeech",
        "ruede17_interspeech",
        "raveh17_interspeech",
        "ebhotemhen17_interspeech"
      ]
    },
    {
      "title": "Special Session: Acoustic Manifestations of Social Characteristics",
      "papers": [
        "niebuhr17b_interspeech",
        "kouklia17_interspeech",
        "gallardo17_interspeech",
        "ishi17b_interspeech",
        "simpson17_interspeech",
        "schweitzer17_interspeech",
        "weirich17_interspeech",
        "soleraurena17_interspeech",
        "tatman17_interspeech"
      ]
    },
    {
      "title": "Neural Network Acoustic Models for ASR 1",
      "papers": [
        "prabhavalkar17_interspeech",
        "zeyer17_interspeech",
        "hori17_interspeech",
        "lu17_interspeech",
        "audhkhasi17_interspeech",
        "li17e_interspeech"
      ]
    },
    {
      "title": "Models of Speech Production",
      "papers": [
        "lucero17_interspeech",
        "sivaraman17_interspeech",
        "arai17_interspeech",
        "badino17_interspeech",
        "uchida17_interspeech",
        "sorensen17b_interspeech"
      ]
    },
    {
      "title": "Speaker Recognition",
      "papers": [
        "snyder17_interspeech",
        "villalba17_interspeech",
        "ranjan17_interspeech",
        "shon17_interspeech",
        "khosravani17_interspeech",
        "jorrin17_interspeech"
      ]
    },
    {
      "title": "Phonation and Voice Quality",
      "papers": [
        "aare17_interspeech",
        "yanushevskaya17_interspeech",
        "kalita17_interspeech",
        "mokhtari17_interspeech",
        "sheena17_interspeech",
        "nara17_interspeech"
      ]
    },
    {
      "title": "Speech Synthesis Prosody",
      "papers": [
        "wang17e_interspeech",
        "klimkov17_interspeech",
        "tanaka17_interspeech",
        "hojo17_interspeech",
        "malisz17_interspeech",
        "betz17_interspeech"
      ]
    },
    {
      "title": "Emotion Recognition",
      "papers": [
        "satt17_interspeech",
        "zhang17b_interspeech",
        "gideon17_interspeech",
        "parthasarathy17_interspeech",
        "le17b_interspeech",
        "kim17d_interspeech"
      ]
    },
    {
      "title": "WaveNet and Novel Paradigms",
      "papers": [
        "tamamori17_interspeech",
        "gu17_interspeech",
        "takaki17_interspeech",
        "ronanki17_interspeech",
        "kobayashi17_interspeech",
        "wan17_interspeech"
      ]
    },
    {
      "title": "Models of Speech Perception",
      "papers": [
        "kain17_interspeech",
        "irino17_interspeech",
        "bosch17_interspeech",
        "jahromi17_interspeech",
        "huber17_interspeech",
        "neufeld17_interspeech"
      ]
    },
    {
      "title": "Source Separation and Auditory Scene Analysis",
      "papers": [
        "wang17f_interspeech",
        "higuchi17_interspeech",
        "pirhosseinloo17_interspeech",
        "chien17b_interspeech",
        "andrei17_interspeech",
        "li17f_interspeech"
      ]
    },
    {
      "title": "Prosody: Tone and Intonation",
      "papers": [
        "quiroz17_interspeech",
        "simko17_interspeech",
        "ip17_interspeech",
        "zahner17_interspeech",
        "rognoni17_interspeech",
        "maekawa17_interspeech"
      ]
    },
    {
      "title": "Emotion Modeling",
      "papers": [
        "ma17c_interspeech",
        "sahu17_interspeech",
        "dang17_interspeech",
        "khorram17_interspeech",
        "chasaide17_interspeech",
        "neumann17_interspeech"
      ]
    },
    {
      "title": "Voice Conversion 1",
      "papers": [
        "miyoshi17_interspeech",
        "hsu17_interspeech",
        "hashimoto17_interspeech",
        "kaneko17_interspeech",
        "ardaillon17_interspeech",
        "mohammadi17_interspeech"
      ]
    },
    {
      "title": "Neural Network Acoustic Models for ASR 2",
      "papers": [
        "sak17_interspeech",
        "pundak17_interspeech",
        "ravanelli17_interspeech",
        "chien17c_interspeech",
        "ratajczak17_interspeech",
        "han17_interspeech"
      ]
    },
    {
      "title": "Speaker Recognition Evaluation",
      "papers": [
        "lee17_interspeech",
        "torrescarrasquillo17_interspeech",
        "colibro17_interspeech",
        "zhang17c_interspeech",
        "plchot17_interspeech",
        "sadjadi17_interspeech"
      ]
    },
    {
      "title": "Glottal Source Modeling",
      "papers": [
        "kawahara17b_interspeech",
        "lopez17_interspeech",
        "juvela17_interspeech",
        "sorin17_interspeech",
        "manriquez17_interspeech",
        "espic17_interspeech"
      ]
    },
    {
      "title": "Prosody: Rhythm, Stress, Quantity and Phrasing",
      "papers": [
        "kember17_interspeech",
        "hou17_interspeech",
        "athanasopoulou17_interspeech",
        "plug17_interspeech",
        "yang17_interspeech",
        "ewald17_interspeech"
      ]
    },
    {
      "title": "Speech Recognition for Language Learning",
      "papers": [
        "qian17_interspeech",
        "yue17_interspeech",
        "lee17b_interspeech",
        "arora17_interspeech",
        "proenca17_interspeech",
        "escuderomancebo17_interspeech"
      ]
    },
    {
      "title": "Stance, Credibility, and Deception",
      "papers": [
        "ward17_interspeech",
        "levow17_interspeech",
        "barriere17_interspeech",
        "luo17_interspeech",
        "schroder17_interspeech",
        "mendels17_interspeech"
      ]
    },
    {
      "title": "Short Utterances Speaker Recognition",
      "papers": [
        "swart17_interspeech",
        "dey17_interspeech",
        "zhang17d_interspeech",
        "yu17_interspeech",
        "wang17g_interspeech",
        "ma17d_interspeech",
        "zhong17_interspeech",
        "vestman17_interspeech",
        "bhattacharya17_interspeech",
        "park17b_interspeech",
        "lee17c_interspeech",
        "heo17_interspeech"
      ]
    },
    {
      "title": "Speaker Characterization and Recognition",
      "papers": [
        "chen17f_interspeech",
        "li17g_interspeech",
        "bousquet17_interspeech",
        "mccree17_interspeech",
        "borgstrom17_interspeech",
        "tan17_interspeech",
        "matejka17_interspeech",
        "silnova17_interspeech",
        "travadi17_interspeech",
        "rahman17_interspeech"
      ]
    },
    {
      "title": "Acoustic Models for ASR 1",
      "papers": [
        "cheng17_interspeech",
        "kim17e_interspeech",
        "tran17_interspeech",
        "karita17_interspeech",
        "ark17_interspeech",
        "wu17c_interspeech",
        "heck17_interspeech",
        "grosz17_interspeech",
        "grosz17b_interspeech"
      ]
    },
    {
      "title": "Acoustic Models for ASR 2",
      "papers": [
        "wang17h_interspeech",
        "takeda17_interspeech",
        "variani17_interspeech",
        "sim17_interspeech",
        "tuske17_interspeech",
        "tang17_interspeech"
      ]
    },
    {
      "title": "Dialog Modeling",
      "papers": [
        "masumura17_interspeech",
        "wodarczak17_interspeech",
        "heeman17_interspeech",
        "maier17_interspeech",
        "ishimoto17_interspeech",
        "liu17_interspeech",
        "inaguma17_interspeech",
        "rahimi17_interspeech",
        "reverdy17_interspeech",
        "crook17_interspeech",
        "ramanarayanan17b_interspeech",
        "ando17_interspeech",
        "ultes17_interspeech",
        "nakamura17_interspeech",
        "fatima17_interspeech"
      ]
    },
    {
      "title": "L1 and L2 Acquisition",
      "papers": [
        "elsner17_interspeech",
        "bohn17_interspeech",
        "kleber17_interspeech",
        "reidy17_interspeech",
        "xiao17_interspeech",
        "chen17g_interspeech",
        "wiener17_interspeech",
        "chen17h_interspeech",
        "luo17b_interspeech",
        "grigonyte17_interspeech",
        "hanulikova17_interspeech",
        "fernandez17_interspeech",
        "sjons17_interspeech",
        "zhang17e_interspeech",
        "marklund17_interspeech"
      ]
    },
    {
      "title": "Voice, Speech and Hearing Disorders",
      "papers": [
        "berisha17_interspeech",
        "castellana17_interspeech",
        "bandini17b_interspeech",
        "adiga17_interspeech",
        "k17_interspeech",
        "laaridh17_interspeech",
        "klumpp17_interspeech",
        "hlavnicka17_interspeech",
        "tu17b_interspeech",
        "vachhani17_interspeech",
        "lilley17_interspeech",
        "li17h_interspeech",
        "gu17b_interspeech"
      ]
    },
    {
      "title": "Source Separation and Voice Activity Detection",
      "papers": [
        "kumar17b_interspeech",
        "huang17d_interspeech",
        "pradhan17_interspeech",
        "gao17_interspeech",
        "xu17_interspeech",
        "guo17b_interspeech",
        "wang17i_interspeech",
        "shannon17_interspeech",
        "he17_interspeech",
        "zegers17_interspeech",
        "yang17b_interspeech",
        "karthik17_interspeech",
        "chen17i_interspeech",
        "tao17_interspeech",
        "maas17_interspeech",
        "kothapally17_interspeech"
      ]
    },
    {
      "title": "Speech-enhancement",
      "papers": [
        "wu17d_interspeech",
        "zhang17f_interspeech",
        "ogawa17_interspeech",
        "gelderblom17_interspeech",
        "koutsogiannaki17_interspeech",
        "hirsch17_interspeech",
        "rehr17_interspeech",
        "marxer17_interspeech",
        "park17c_interspeech",
        "li17i_interspeech",
        "websdale17_interspeech",
        "michelsanti17_interspeech",
        "qian17b_interspeech",
        "zhang17g_interspeech",
        "zorila17_interspeech"
      ]
    },
    {
      "title": "Show &amp; Tell 3",
      "papers": [
        "meermeier17_interspeech",
        "cernak17_interspeech",
        "lenarczyk17_interspeech",
        "chennupati17_interspeech",
        "stemmer17_interspeech",
        "tsuji17_interspeech"
      ]
    },
    {
      "title": "Show &amp; Tell 4",
      "papers": [
        "daniel17_interspeech",
        "milosevic17_interspeech",
        "hagerer17_interspeech",
        "jeon17_interspeech",
        "wood17_interspeech",
        "rouhe17_interspeech"
      ]
    },
    {
      "title": "Keynote 2: Catherine Pelachaud",
      "papers": [
        "pelachaud17_interspeech"
      ]
    },
    {
      "title": "Special Session: Digital Revolution for Under-resourced Languages 1",
      "papers": [
        "papadopoulos17_interspeech",
        "mihajlik17_interspeech",
        "watson17_interspeech",
        "feng17_interspeech",
        "das17_interspeech",
        "gutkin17_interspeech"
      ]
    },
    {
      "title": "Special Session: Data Collection, Transcription and Annotation Issues in Child Language Acquisition",
      "papers": [
        "hall17_interspeech",
        "schwarz17_interspeech",
        "casillas17_interspeech",
        "casillas17b_interspeech",
        "bergmann17_interspeech",
        "tsuji17b_interspeech"
      ]
    },
    {
      "title": "Special Session: Digital Revolution for Under-resourced Languages 2",
      "papers": [
        "chasaide17b_interspeech",
        "saeb17_interspeech",
        "malandrakis17_interspeech",
        "kocharov17_interspeech",
        "bhati17_interspeech",
        "gauthier17_interspeech",
        "glarner17_interspeech",
        "zhuang17_interspeech",
        "abraham17_interspeech",
        "abraham17b_interspeech",
        "helgadottir17_interspeech",
        "alumae17_interspeech",
        "gunason17_interspeech",
        "niekerk17_interspeech",
        "gutkin17b_interspeech",
        "mendelson17b_interspeech"
      ]
    },
    {
      "title": "Special Session: Computational Models in Child Language Acquisition",
      "papers": [
        "tong17b_interspeech",
        "larsen17_interspeech",
        "wiren17_interspeech",
        "marklund17b_interspeech",
        "strombergsson17_interspeech",
        "chaabouni17_interspeech"
      ]
    },
    {
      "title": "Special Session: Voice Attractiveness",
      "papers": [
        "obuchi17_interspeech",
        "bosker17_interspeech",
        "gallardo17b_interspeech",
        "trouvain17_interspeech",
        "schweitzer17b_interspeech",
        "novaktot17_interspeech",
        "michalsky17_interspeech",
        "jiao17_interspeech",
        "baumann17_interspeech"
      ]
    },
    {
      "title": "Speech Production and Physiology",
      "papers": [
        "signorello17_interspeech",
        "serrurier17_interspeech",
        "patil17b_interspeech",
        "tang17b_interspeech",
        "blaylock17_interspeech",
        "uezu17_interspeech"
      ]
    },
    {
      "title": "Speech and Harmonic Analysis",
      "papers": [
        "gangamohan17_interspeech",
        "hua17_interspeech",
        "dhiman17_interspeech",
        "miwa17_interspeech",
        "graf17_interspeech",
        "morise17b_interspeech"
      ]
    },
    {
      "title": "Dialog and Prosody",
      "papers": [
        "stehwien17_interspeech",
        "galvez17_interspeech",
        "zellers17_interspeech",
        "mukherjee17_interspeech",
        "luque17_interspeech",
        "brusco17_interspeech"
      ]
    },
    {
      "title": "Social Signals, Styles, and Interaction",
      "papers": [
        "egorow17_interspeech",
        "chen17j_interspeech",
        "lin17_interspeech",
        "brueckner17_interspeech",
        "gosztolya17_interspeech",
        "haider17_interspeech"
      ]
    },
    {
      "title": "Acoustic Model Adaptation",
      "papers": [
        "li17j_interspeech",
        "ahmad17_interspeech",
        "xie17_interspeech",
        "arsikere17_interspeech",
        "srinivasamurthy17_interspeech",
        "kim17f_interspeech"
      ]
    },
    {
      "title": "Cognition and Brain Studies",
      "papers": [
        "bosker17b_interspeech",
        "wang17j_interspeech",
        "verma17_interspeech",
        "watanabe17_interspeech",
        "rietmolen17_interspeech",
        "zhao17_interspeech"
      ]
    },
    {
      "title": "Noise Robust Speech Recognition",
      "papers": [
        "agrawal17_interspeech",
        "mimura17_interspeech",
        "yu17b_interspeech",
        "tachioka17_interspeech",
        "loweimi17b_interspeech",
        "king17_interspeech"
      ]
    },
    {
      "title": "Topic Spotting, Entity Extraction and Semantic Analysis",
      "papers": [
        "bapna17_interspeech",
        "georgiadou17_interspeech",
        "jannet17_interspeech",
        "dinarelli17_interspeech",
        "meng17_interspeech",
        "liu17b_interspeech"
      ]
    },
    {
      "title": "Dialog Systems",
      "papers": [
        "liu17c_interspeech",
        "cuayahuitl17_interspeech",
        "bayer17_interspeech",
        "akhtiamov17_interspeech",
        "ramanarayanan17c_interspeech",
        "kraljevski17_interspeech"
      ]
    },
    {
      "title": "Lexical and Pronunciation Modeling",
      "papers": [
        "milde17_interspeech",
        "zhang17h_interspeech",
        "shinozaki17_interspeech",
        "smit17_interspeech",
        "bruguier17_interspeech",
        "naaman17_interspeech"
      ]
    },
    {
      "title": "Language Recognition",
      "papers": [
        "gelly17_interspeech",
        "jin17_interspeech",
        "zhang17i_interspeech",
        "irtza17_interspeech",
        "qian17c_interspeech",
        "khurana17_interspeech"
      ]
    },
    {
      "title": "Speaker Database and Anti-spoofing",
      "papers": [
        "alluri17b_interspeech",
        "sailor17_interspeech",
        "suthokumar17_interspeech",
        "sarkar17_interspeech",
        "nagrani17_interspeech",
        "jones17b_interspeech"
      ]
    },
    {
      "title": "Speech Translation",
      "papers": [
        "weiss17_interspeech",
        "kano17_interspeech",
        "ruiz17_interspeech",
        "do17b_interspeech",
        "cho17_interspeech"
      ]
    },
    {
      "title": "Multi-channel Speech Enhancement",
      "papers": [
        "drude17_interspeech",
        "zmolikova17_interspeech",
        "pfeifenberger17_interspeech",
        "wood17b_interspeech",
        "ji17b_interspeech",
        "zhang17j_interspeech"
      ]
    },
    {
      "title": "Speech Recognition: Applications in Medical Practice",
      "papers": [
        "liu17d_interspeech",
        "ylmaz17c_interspeech",
        "smith17_interspeech",
        "joy17b_interspeech",
        "simantiraki17_interspeech",
        "sadeghian17_interspeech"
      ]
    },
    {
      "title": "Language models for ASR",
      "papers": [
        "biadsy17_interspeech",
        "deena17_interspeech",
        "singh17_interspeech",
        "chelba17_interspeech",
        "kumar17c_interspeech",
        "zhu17_interspeech"
      ]
    },
    {
      "title": "Speech Recognition: Technologies for New Applications and Paradigms",
      "papers": [
        "dimitriadis17_interspeech",
        "seshadri17_interspeech",
        "proenca17b_interspeech",
        "yoon17_interspeech",
        "li17k_interspeech",
        "tsujimura17_interspeech",
        "kim17g_interspeech",
        "knill17_interspeech",
        "yi17_interspeech",
        "pusateri17_interspeech",
        "chen17k_interspeech",
        "gale17_interspeech",
        "kaushik17_interspeech"
      ]
    },
    {
      "title": "Speaker and Language Recognition Applications",
      "papers": [
        "mclaren17_interspeech",
        "fernando17_interspeech",
        "shen17b_interspeech",
        "miguel17_interspeech",
        "yun17_interspeech",
        "vinals17_interspeech",
        "india17_interspeech",
        "gresse17_interspeech",
        "ajili17_interspeech",
        "solewicz17_interspeech",
        "liu17e_interspeech",
        "kumar17d_interspeech",
        "misra17_interspeech",
        "shon17b_interspeech"
      ]
    },
    {
      "title": "Spoken Document Processing",
      "papers": [
        "settle17_interspeech",
        "kaneko17b_interspeech",
        "khokhlov17_interspeech",
        "chen17l_interspeech",
        "tasaki17_interspeech",
        "lu17b_interspeech",
        "tsuchiya17_interspeech",
        "lopezotero17_interspeech",
        "kumar17e_interspeech",
        "tsunoo17_interspeech",
        "bouchekif17_interspeech",
        "bang17_interspeech",
        "svec17_interspeech"
      ]
    },
    {
      "title": "Speech Intelligibility",
      "papers": [
        "gallardo17c_interspeech",
        "botinhao17_interspeech",
        "yamamoto17_interspeech",
        "chen17m_interspeech",
        "ward17b_interspeech",
        "andersen17_interspeech",
        "spille17_interspeech"
      ]
    },
    {
      "title": "Articulatory and Acoustic Phonetics",
      "papers": [
        "sugai17_interspeech",
        "ying17_interspeech",
        "klingler17_interspeech",
        "issa17_interspeech",
        "brandt17_interspeech",
        "boril17_interspeech",
        "tabain17_interspeech",
        "gobl17_interspeech",
        "benus17_interspeech",
        "jochim17b_interspeech",
        "nellore17_interspeech",
        "ding17_interspeech",
        "schatz17_interspeech"
      ]
    },
    {
      "title": "Music and Audio Processing",
      "papers": [
        "lin17b_interspeech",
        "phan17_interspeech",
        "sandsten17_interspeech",
        "matousek17_interspeech",
        "qi17_interspeech",
        "jesus17_interspeech",
        "guan17_interspeech",
        "hyder17_interspeech",
        "feng17b_interspeech",
        "xu17b_interspeech",
        "pan17_interspeech",
        "chowdhury17_interspeech",
        "wang17k_interspeech",
        "mostafa17_interspeech",
        "sailor17b_interspeech",
        "soni17_interspeech"
      ]
    },
    {
      "title": "Disorders Related to Speech and Language",
      "papers": [
        "weiner17_interspeech",
        "gupta17_interspeech",
        "gillespie17_interspeech",
        "novotny17_interspeech",
        "hantke17_interspeech",
        "agurto17_interspeech",
        "mirheidari17_interspeech",
        "zhang17k_interspeech",
        "lopezotero17b_interspeech",
        "wankerl17_interspeech",
        "mundnich17_interspeech",
        "pettorino17_interspeech"
      ]
    },
    {
      "title": "Prosody",
      "papers": [
        "tu17c_interspeech",
        "sahkai17_interspeech",
        "hao17_interspeech",
        "michelas17_interspeech",
        "wagner17_interspeech",
        "kleinhans17_interspeech",
        "godoy17_interspeech",
        "kakouros17_interspeech",
        "kuang17_interspeech",
        "skarnitzl17_interspeech",
        "wagner17b_interspeech",
        "hsu17b_interspeech",
        "lundmark17_interspeech",
        "puga17_interspeech"
      ]
    },
    {
      "title": "Speaker States and Traits",
      "papers": [
        "paradacabaleiro17_interspeech",
        "gibson17_interspeech",
        "wortwein17_interspeech",
        "wu17e_interspeech",
        "vlasenko17_interspeech",
        "siddique17_interspeech",
        "arimoto17_interspeech",
        "fayet17_interspeech",
        "akira17_interspeech",
        "tseng17_interspeech",
        "nasir17_interspeech",
        "huang17e_interspeech"
      ]
    },
    {
      "title": "Language Understanding and Generation",
      "papers": [
        "sadamitsu17_interspeech",
        "sawada17_interspeech",
        "morchid17_interspeech",
        "korpusik17_interspeech",
        "parcollet17_interspeech",
        "simonnet17_interspeech",
        "ma17e_interspeech",
        "nayak17_interspeech",
        "riou17_interspeech",
        "martinezhinarejos17_interspeech",
        "morales17_interspeech",
        "huang17f_interspeech"
      ]
    },
    {
      "title": "Voice Conversion 2",
      "papers": [
        "hsu17c_interspeech",
        "nakashika17_interspeech",
        "aihara17_interspeech",
        "wu17f_interspeech",
        "tanaka17b_interspeech",
        "kaneko17c_interspeech",
        "bollepalli17_interspeech",
        "luo17c_interspeech",
        "doddipatla17_interspeech",
        "li17l_interspeech",
        "ramos17_interspeech"
      ]
    },
    {
      "title": "Show &amp; Tell 5",
      "papers": [
        "moore17_interspeech",
        "dominguez17_interspeech",
        "gruber17_interspeech",
        "juzova17_interspeech",
        "ghone17_interspeech",
        "karhila17_interspeech"
      ]
    },
    {
      "title": "Show &amp; Tell 6",
      "papers": [
        "larsson17_interspeech",
        "ahmed17_interspeech",
        "duckhorn17_interspeech",
        "winata17_interspeech",
        "alam17_interspeech"
      ]
    },
    {
      "title": "Keynote 3: Bj&#246;rn Lindblom",
      "papers": [
        "lindblom17_interspeech"
      ]
    },
    {
      "title": "Special Session: Interspeech 2017 Computational Paralinguistics ChallengE (ComParE) 1",
      "papers": [
        "schuller17_interspeech",
        "krajewski17_interspeech",
        "janott17_interspeech",
        "bergelson17_interspeech",
        "huckvale17_interspeech",
        "cai17b_interspeech",
        "wagner17c_interspeech",
        "suresh17_interspeech",
        "nwe17_interspeech"
      ]
    },
    {
      "title": "Special Session: State of the Art in Physics-based Voice Simulation",
      "papers": [
        "kitamura17_interspeech",
        "arnela17_interspeech",
        "vasudevan17_interspeech",
        "murtola17_interspeech",
        "degirmenci17_interspeech",
        "dabbaghchian17_interspeech"
      ]
    },
    {
      "title": "Special Session: Interspeech 2017 Computational Paralinguistics ChallengE (ComParE) 2",
      "papers": [
        "mv17_interspeech",
        "freitag17_interspeech",
        "amiriparian17_interspeech",
        "tavarez17_interspeech",
        "gosztolya17b_interspeech",
        "kaya17_interspeech",
        "steidl17_interspeech",
        "schuller17b_interspeech"
      ]
    },
    {
      "title": "Discriminative Training for ASR",
      "papers": [
        "toshniwal17_interspeech",
        "shannon17b_interspeech",
        "sainath17_interspeech",
        "meng17b_interspeech",
        "dighe17_interspeech",
        "yang17c_interspeech"
      ]
    },
    {
      "title": "Speaker Diarization",
      "papers": [
        "zajic17_interspeech",
        "jati17_interspeech",
        "lan17_interspeech",
        "cohen17_interspeech",
        "wisniewksi17_interspeech",
        "bredin17_interspeech"
      ]
    },
    {
      "title": "Spoken Term Detection",
      "papers": [
        "chen17n_interspeech",
        "trmal17_interspeech",
        "khokhlov17b_interspeech",
        "sun17_interspeech",
        "suzuki17b_interspeech",
        "gundogdu17_interspeech"
      ]
    },
    {
      "title": "Noise Reduction",
      "papers": [
        "samui17_interspeech",
        "huang17g_interspeech",
        "chen17o_interspeech",
        "yan17_interspeech",
        "pascual17_interspeech",
        "maiti17_interspeech"
      ]
    },
    {
      "title": "Speech Recognition: Multimodal Systems",
      "papers": [
        "stafylakis17_interspeech",
        "thangthai17_interspeech",
        "wand17_interspeech",
        "abdelaziz17_interspeech",
        "csapo17_interspeech",
        "kamper17_interspeech"
      ]
    },
    {
      "title": "Neural Network Acoustic Models for ASR 3",
      "papers": [
        "chien17d_interspeech",
        "vesely17_interspeech",
        "hou17b_interspeech",
        "fukuda17_interspeech",
        "prabhavalkar17b_interspeech",
        "soltau17_interspeech"
      ]
    },
    {
      "title": "Robust Speaker Recognition",
      "papers": [
        "guo17c_interspeech",
        "ranjan17b_interspeech",
        "mahto17_interspeech",
        "wang17l_interspeech",
        "alam17b_interspeech",
        "castan17_interspeech"
      ]
    },
    {
      "title": "Multimodal Resources and Annotation",
      "papers": [
        "abidi17_interspeech",
        "narwekar17_interspeech",
        "abdelaziz17b_interspeech",
        "howcroft17_interspeech",
        "mansikkaniemi17_interspeech",
        "abdo17_interspeech"
      ]
    },
    {
      "title": "Forensic Phonetics and Sociophonetic Varieties",
      "papers": [
        "hughes17_interspeech",
        "delvaux17_interspeech",
        "wu17g_interspeech",
        "duran17_interspeech",
        "kohtz17_interspeech",
        "gessinger17_interspeech"
      ]
    },
    {
      "title": "Speech and Audio Segmentation and Classification 1",
      "papers": [
        "ghaffarzadegan17_interspeech",
        "tran17b_interspeech",
        "chang17_interspeech",
        "baby17_interspeech",
        "wang17m_interspeech",
        "yin17_interspeech"
      ]
    },
    {
      "title": "Noise Robust and Far-field ASR",
      "papers": [
        "do17c_interspeech",
        "fujimoto17_interspeech",
        "papadopoulos17b_interspeech",
        "ge17_interspeech",
        "tran17c_interspeech",
        "zhang17l_interspeech",
        "huang17h_interspeech",
        "kim17h_interspeech",
        "menon17_interspeech",
        "caroselli17_interspeech"
      ]
    },
    {
      "title": "Styles, Varieties, Forensics and Tools",
      "papers": [
        "zihlmann17_interspeech",
        "mcauliffe17b_interspeech",
        "hughes17b_interspeech",
        "arantes17_interspeech",
        "volin17_interspeech",
        "plantehebert17_interspeech",
        "cooperleavitt17_interspeech",
        "murphy17_interspeech",
        "barbosa17_interspeech",
        "gendrot17_interspeech",
        "narendra17_interspeech",
        "christodoulides17_interspeech",
        "levit17_interspeech",
        "airaksinen17_interspeech",
        "hantke17b_interspeech"
      ]
    },
    {
      "title": "Speech Synthesis: Data, Evaluation, and Novel Paradigms",
      "papers": [
        "henter17_interspeech",
        "takamichi17_interspeech",
        "pollet17_interspeech",
        "cooper17_interspeech",
        "rosenberg17_interspeech",
        "adiga17b_interspeech",
        "gonzalez17_interspeech",
        "greenwood17_interspeech",
        "wester17_interspeech",
        "blaauw17_interspeech",
        "wang17n_interspeech",
        "capes17_interspeech",
        "esch17_interspeech",
        "nakashika17b_interspeech"
      ]
    },
    {
      "title": "Show &amp; Tell 7",
      "papers": [
        "zioko17_interspeech",
        "arai17b_interspeech",
        "masudakatsuse17_interspeech",
        "bunnell17_interspeech",
        "heeringa17_interspeech"
      ]
    }
  ],
  "doi": "10.21437/Interspeech.2017"
}
