{
 "title": "Workshop on Speech, Music and Mind (SMM 2018)",
 "location": "Hyderabad, India",
 "startDate": "01/9/2018",
 "endDate": "01/9/2018",
 "URL": "http://smmw.iiit.ac.in/",
 "chair": "Chairs: Venkata S Viraraghavan and Suryakanth V Gangashetty",
 "conf": "SMM",
 "year": "2018",
 "name": "smm_2018",
 "series": "SMM",
 "SIG": "",
 "title1": "Workshop on Speech, Music and Mind",
 "title2": "(SMM 2018)",
 "date": "1 September 2018",
 "booklet": "smm_2018.pdf",
 "papers": {
  "schuller18_smm": {
   "authors": [
    [
     "Björn",
     "Schuller"
    ]
   ],
   "title": "State of Mind Sensing from Speech: State of Matters and What Matters",
   "original": "schuller",
   "page_count": 0,
   "order": 1,
   "p1": "",
   "pn": "",
   "abstract": [
    "﻿The sensing of a plethora of speaker states and traits has entered a new era of \"in the wild processing\" largely empowered by deep learning. This includes the self-learning of feature representations by convolutional neural networks directly from the raw audio signal. In addition, examples of feature vectors or even raw audio material on the signal level itself can increasingly be generated by generative adversarial neural networks allowing to self-synthesise additional training material. Put together with memory-enhanced recurrent neural networks such as with long-short term memory or gated recurrent units, powerful architectures can be built to model a broad range of speaker characteristics - ideally side-by-side in a multi-target training framework to allow for exploitation of mutual dependencies. This puts a new question in optimal modeling to the fore: How should topologies of such networks best be shaped that serve different tasks in coupled ways able to self-learn representation and even imagine learning examples? Automatic Machine Learning offers solutions to this end enabling the self-optimisation of such network topologies by controller nets that iteratively shape child nets reinforced by the task performance. In this overview  on the state of mind sensing from speech, examples from the Interspeech Computational Paralinguistics Challenge series (ComParE) serve to illustrate the richness of speaker characteristics that can be assessed automatically at present. Further, these are used to exemplify the sketched latest development in the field from a technical viewpoint in terms of \"what matters“. This is further elaborated upon in an outlook on which are the crucial next steps to be taken to allow for human or super-human performance in challenging real-world conditions.\n"
   ]
  },
  "duggirala18_smm": {
   "authors": [
    [
     "Mayuri",
     "Duggirala"
    ]
   ],
   "title": "New directions in psychophysiology: A multidisciplinary perspective",
   "original": "duggirala",
   "page_count": 0,
   "order": 2,
   "p1": "",
   "pn": "",
   "abstract": [
    "﻿The talk presents an overview of the field of psychophysiology. It focuses on the current topics in the field of psychophysiology and presents areas of research, methods, and frameworks in the field as well as challenges in examining the physiological underpinnings of behavior. This is followed by a focus on research on wellbeing and the possible approaches in studying wellbeing in different contexts from a psychophysiological standpoint. Our current work in physiological aspects of behavior has focused on speech as a key source of behavioral data. In this context, our ongoing work in audio-based emotion detection will also be discussed. Avenues for the use of computational tools and techniques in the study of psychophysiology will also be explored."
   ]
  },
  "hegde18_smm": {
   "authors": [
    [
     "Shantala",
     "Hegde"
    ]
   ],
   "title": "Distinctive and overlapping neural correlates of music and speech",
   "original": "hegde",
   "page_count": 0,
   "order": 8,
   "p1": "",
   "pn": "",
   "abstract": [
    "﻿Music and Language are the ubiquitous phenomena and the two engages a host of cognitive processes. From an evolutionary viewpoint it is still debatable if music functioned as a spandrel to the evolution of music. Language share music-like features such as pitch, frequency-amplitude modulations timbre and rhythm. Neuromusicological research indicates that syntax and semantics (which are considered core features of language) are basic aspects of music.  The neural correlates of semantics and syntax in music overlap considerably with those involved in language perception, underscoring close links between music and language at a neural level. Scientific findings involving methods such as EEG/ERP and fMRI, so far indicate presence of neural separability between music and language, occurring with overlapping cognitive mechanisms and brain regions. This intimate cognitive and neural overlap has formed the basic principle for the possibility of music training and music based intervention to target language functions. There is mounting evidence that musical training benefits the neural encoding of speech. Music based interventions are being explored rigorously in neurorehabilitation of language functions."
   ]
  },
  "cabral18_smm": {
   "authors": [
    [
     "João",
     "Cabral"
    ]
   ],
   "title": "Acoustic cues of prosodic stability in the perceptual distinction between speech and singing",
   "original": "cabral",
   "page_count": 0,
   "order": 9,
   "p1": "",
   "pn": "",
   "abstract": [
    "﻿In this work, we try to understand how spoken and sung versions of the same text differ in terms of the variability in duration and pitch. People can perceptually distinguish speech from singing, especially if the audio is sufficiently long. However, we assume that there is both an intersection in the realisation of the two modalities, which makes more difficult to differentiate them sometimes, and clear differences between them other times. This raises the questions: Are speaking and singing completely different phenomena? Can we measure acoustic properties of these signals that demonstrate their differences or eventually their similarities? We have conducted different types of experiments in this project. One is based on stability measures of fundamental frequency (F0) and speech rate on spoken and sung versions of the same text (from Brazilian Portuguese popular songs). Another one is a perceptual study of the differences between the two types of recordings. Initial results were unexpected, especially the F0 stability which was not always significantly different between the two. However, later results were more supportive of the hypothesis that we can differentiate speech and singing in terms of acoustic properties, particularly, results that can supplant the initial one related to F0 stability."
   ]
  },
  "ramdinmawii18_smm": {
   "authors": [
    [
     "Esther",
     "Ramdinmawii"
    ],
    [
     "V. K.",
     "Mittal"
    ]
   ],
   "title": "Discriminating between High-Arousal and Low-Arousal Emotional States of Mind using Acoustic Analysis",
   "original": "4",
   "page_count": 5,
   "order": 3,
   "p1": 1,
   "pn": 5,
   "abstract": [
    "﻿Identification of emotions from human speech can be attempted by focusing upon three aspects of emotional speech: valence, arousal and dominance. In this paper, changes in the production characteristics of emotional speech are examined to discriminate between the high-arousal and low-arousal emotions, and amongst emotions within each of these categories. Basic emotions anger, happy and fear are examined in high-arousal, and neutral speech and sad emotion in low-arousal emotional speech. Discriminating changes are examined first in the excitation source characteristics, i.e., instantaneous fundamental frequency (F0) derived using the zero-frequency filtering (ZFF) method. Differences observed in the spectrograms are then validated by examining changes in the combined characteristics of the source and the vocal tract filter, i.e., strength of excitation (SoE), derived using ZFF method, and signal energy features. Emotions within each category are distinguished by examining changes in two scarcely explored discriminating features, namely, zero-crossing rate and the ratios amongst the spectral sub-band energies computed using short-time Fourier transform. Effectiveness of these features in discriminating emotions is validated using two emotion databases, Berlin EMODB (German) and IIT-KGP-SESC (Telugu). Proposed features exhibit highly encouraging results in discriminating these emotions. This study can be helpful towards automatic classification of emotions from speech."
   ],
   "doi": "10.21437/SMM.2018-1"
  },
  "d18_smm": {
   "authors": [
    [
     "Vinothkumar",
     "D"
    ],
    [
     "Mari Ganesh",
     "Kumar"
    ],
    [
     "Abhishek",
     "Kumar"
    ],
    [
     "Hitesh",
     "Gupta"
    ],
    [
     "Saranya M.",
     "S"
    ],
    [
     "Mriganka",
     "Sur"
    ],
    [
     "Hema A.",
     "Murthy"
    ]
   ],
   "title": "Task-Independent EEG based Subject Identification using Auditory Stimulus",
   "original": "7",
   "page_count": 5,
   "order": 10,
   "p1": 26,
   "pn": 30,
   "abstract": [
    "﻿Recent studies have shown that task-specific electroencephalography (EEG) can be used as a reliable biometric. This paper extends this study to task-independent EEG with auditory stimuli. Data collected from 40 subjects in response to various types of audio stimuli, using a 128 channel EEG system is presented to different classifiers, namely, k-nearest neighbor (k-NN), artificial neural network (ANN) and universal background model - Gaussian mixture model (UBM-GMM). It is observed that k-NN and ANN perform well when testing is performed intrasession, while UBM-GMM framework is more robust when testing is performed intersession. This can be attributed to the fact that the correspondence of the sensor locations across sessions is only approximate. It is also observed that EEG from parietal and temporal regions contain more subject information although the performance using all the 128 channel data is marginally better."
   ],
   "doi": "10.21437/SMM.2018-6"
  },
  "v18_smm": {
   "authors": [
    [
     "Vishnu Vidyadhara Raju",
     "V"
    ],
    [
     "Priyam",
     "Jain"
    ],
    [
     "Krishna",
     "Gurugubelli"
    ],
    [
     "Anil Kumar",
     "Vuppala"
    ]
   ],
   "title": "Emotional Speech Classifier Systems: For Sensitive Assistance to support Disabled Individuals",
   "original": "9",
   "page_count": 5,
   "order": 4,
   "p1": 6,
   "pn": 10,
   "abstract": [
    "﻿This paper provides the classification of emotionally annotated speech of mentally impaired people. The main problem encountered in the classification task is the class-imbalance. This imbalance is due to the availability of large number of speech samples for the neutral speech compared to other emotional speech. Different sampling methodologies are explored at the back-end to handle this class-imbalance problem. Mel-frequency cepstral coefficients (MFCCs) features are considered at the front-end, eep neural networks (DNNs) and gradient boosted decision trees (GBDT) are investigated at the back-end as classifiers. The experimental results obtained from the EmotAsS dataset have shown higher classification accuracy and Unweighted Average Recall (UAR) scores over the baseline system."
   ],
   "doi": "10.21437/SMM.2018-2"
  },
  "sarma18_smm": {
   "authors": [
    [
     "Biswajit Dev",
     "Sarma"
    ],
    [
     "Rohan Kumar",
     "Das"
    ],
    [
     "Abhishek",
     "Dey"
    ],
    [
     "Risto",
     "Haukioja"
    ]
   ],
   "title": "Analysis of Speech Emotions in Realistic Environments",
   "original": "10",
   "page_count": 5,
   "order": 5,
   "p1": 11,
   "pn": 15,
   "abstract": [
    "﻿The classification of emotional speech is a challenging task and it depends critically on the correctness of labeled data. Most of the databases used for research purposes are either acted or simulated. Annotation of such acted database is easier as the actor exaggerates the emotions. On the other hand, emotion labeling on real-world data is very difficult due to confusion among the emotion classes. Another problem in such scenario is the class imbalance, because most of the data is found to be neutral in realistic environment. In this study, we perform emotion labeling on realistic data in a customized manner using emotion priority and confidence level. The annotated speech corpus is then used for analysis and study. Percentage distribution of different emotion classes in the real-world data and the confusions between the emotions during labeling are presented."
   ],
   "doi": "10.21437/SMM.2018-3"
  },
  "gangamohan18_smm": {
   "authors": [
    [
     "P.",
     "Gangamohan"
    ],
    [
     "Suryakanth V",
     "Gangashetty"
    ],
    [
     "B.",
     "Yegnanarayana"
    ]
   ],
   "title": "Time-frequency spectral error for analysis of high arousal speech",
   "original": "11",
   "page_count": 5,
   "order": 6,
   "p1": 16,
   "pn": 20,
   "abstract": [
    "﻿High arousal speech is produced by speakers when they raise their loudness levels. There are deviations from neutral speech, especially in the excitation component of the speech production mechanism in the high arousal mode. In this study, a parameter, called the time-frequency spectral error (TFe) is derived using the single frequency filtering (SFF) spectrogram. It is used to characterize the high arousal regions in speech signals. The proposed parameter captures the fine temporal and spectral variations due to changes in the excitation source."
   ],
   "doi": "10.21437/SMM.2018-4"
  },
  "etienne18_smm": {
   "authors": [
    [
     "Caroline",
     "Etienne"
    ],
    [
     "Guillaume",
     "Fidanza"
    ],
    [
     "Andrei",
     "Petrovskii"
    ],
    [
     "Laurence",
     "Devillers"
    ],
    [
     "Benoit",
     "Schmauch"
    ]
   ],
   "title": "CNN+LSTM Architecture for Speech Emotion Recognition with Data Augmentation",
   "original": "12",
   "page_count": 5,
   "order": 7,
   "p1": 21,
   "pn": 25,
   "abstract": [
    "﻿In this work, we design a neural network for recognizing emotions in speech, using the IEMOCAP dataset. Following the latest advances in audio analysis, we use an architecture involving both convolutional layers, for extracting high-level features from raw spectrograms, and recurrent ones for aggregating long-term dependencies. We examine the techniques of data augmentation with vocal track length perturbation, layer-wise optimizer adjustment, batch normalization of recurrent layers and obtain highly competitive results of 64:5% for weighted accuracy and 61:7% for unweighted accuracy on four emotions."
   ],
   "doi": "10.21437/SMM.2018-5"
  },
  "viraraghavan18_smm": {
   "authors": [
    [
     "Venkata Subramanian",
     "Viraraghavan"
    ],
    [
     "Arpan",
     "Pal"
    ],
    [
     "Hema",
     "Murthy"
    ],
    [
     "R",
     "Aravind"
    ]
   ],
   "title": "A component-based approach to study the effect of Indian music on emotions",
   "original": "13",
   "page_count": 5,
   "order": 11,
   "p1": 31,
   "pn": 35,
   "abstract": [
    "﻿The emotional impact of Indian music on human listeners has been studied mainly with respect to ragas. Although this approach aligns with the traditional and musicological views, some studies show that raga-specific effects may not be consistent. In this paper, we propose an alternative method of study based on the components of Indian Classical Music, which may be viewed as consisting of constant-pitch notes (CPNs) providing the context, and transients, the detail. One hundred concert pieces in four ragas each in Carnatic music (CM) and Hindustani music (HM) are analyzed to show that the transients are, on average, longer than CPNs. Further, the defined scale of the raga is not always mirrored in the CPNs for CM. We also draw upon the result that CPNs and transients scale non-uniformly when changing the tempo of CM pieces. Based on the observations and previous results on the emotional impact of the major and minor scales in Western music, we propose that the effect of CPNs and transients should be analyzed separately. We present a preliminary experiment that brings outs related challenges."
   ],
   "doi": "10.21437/SMM.2018-7"
  }
 },
 "sessions": [
  {
   "title": "Keynote speech",
   "papers": [
    "schuller18_smm"
   ]
  },
  {
   "title": "Chair's address",
   "papers": [
    "duggirala18_smm"
   ]
  },
  {
   "title": "Detecting mental states from speech",
   "papers": [
    "ramdinmawii18_smm",
    "v18_smm",
    "sarma18_smm",
    "gangamohan18_smm",
    "etienne18_smm"
   ]
  },
  {
   "title": "Keynote speech",
   "papers": [
    "hegde18_smm"
   ]
  },
  {
   "title": "Chair's address",
   "papers": [
    "cabral18_smm"
   ]
  },
  {
   "title": "Effect of music and audio on mental states",
   "papers": [
    "d18_smm",
    "viraraghavan18_smm"
   ]
  }
 ],
 "doi": "10.21437/SMM.2018"
}