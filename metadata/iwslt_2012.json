{
 "title": "International Workshop on Spoken Language Translation (IWSLT 2012)",
 "location": "Hong Kong",
 "startDate": "6/12/2012",
 "endDate": "7/12/2012",
 "conf": "IWSLT",
 "year": "2012",
 "name": "iwslt_2012",
 "series": "IWSLT",
 "SIG": "",
 "title1": "International Workshop on Spoken Language Translation",
 "title2": "(IWSLT 2012)",
 "date": "6-7 December 2012",
 "booklet": "iwslt_2012.pdf",
 "papers": {
  "wutiwiwatchai12_iwslt": {
   "authors": [
    [
     "Chai",
     "Wutiwiwatchai"
    ]
   ],
   "title": "Toward universal network-based speech translation",
   "original": "sltc_008",
   "page_count": 1,
   "order": 1,
   "p1": "8",
   "pn": "",
   "abstract": [
    "The speech translation technology has been widely expected to play an important role in today global communication. This talk will address activities of a recently developed international consortium, called Universal Speech Translation Advanced Research (U-STAR), which composes 26 research organizations from 23 Asian and European countries. This largest research consortium has jointly developed a network-based speech translation service which supports translation among 23 languages and accepts up to 17 languages speech input. The service has been developed based on shared language resources in travel and sport domains. Users are able to access the service via a freely available iPhone application, namely VoiceTra4U-M. This talk will start by describing the initiation of the U-STAR consortium, followed by summarizing the development issues on both language resource and system engineering parts. Some statistics and analyses of the global usage during a few months field-testing after service launching will be revealed. Finally, challenging issues to improve the service accuracy and to extend the number of supported languages and translation domains will be discussed.\n",
    ""
   ]
  },
  "yu12_iwslt": {
   "authors": [
    [
     "Dong",
     "Yu"
    ]
   ],
   "title": "Who can understand your speech better - deep neural network or Gaussian mixture model?",
   "original": "sltc_009",
   "page_count": 1,
   "order": 2,
   "p1": "9",
   "pn": "",
   "abstract": [
    "Recently we have shown that the context-dependent deep neural network (DNN) hidden Markov model (CD-DNN-HMM) can do surprisingly well for large vocabulary speech recognition (LVSR) as demonstrated on several benchmark tasks. Since then, much work has been done to understand its potential and to further advance the state of the art. In this talk I will share some of these thoughts and introduce some of the recent progresses we have made.   In the talk, I will first briefly describe CD-DNN-HMM and bring some insights on why DNNs can do better than the shallow neural networks and Gaussian mixture models. My discussion will be based on the fact that DNN can be considered as a joint model of a complicated feature extractor and a log-linear model. I will then describe how some of the obstacles, such as training speed, decoding speed, sequence-level training, and adaptation, on adopting CD-DNN-HMMs can be removed thanks to recent advances. After that, I will show ways to further improve the DNN structures to achieve better recognition accuracy and to support new scenarios. I will conclude the talk by indicating that DNNs not only do better but also are simpler than GMMs.\n",
    ""
   ]
  },
  "isozaki12_iwslt": {
   "authors": [
    [
     "Hideki",
     "Isozaki"
    ]
   ],
   "title": "Head finalization: translation from SVO to SOV",
   "original": "sltc_010",
   "page_count": 1,
   "order": 3,
   "p1": "10",
   "pn": "",
   "abstract": [
    "Asian languages such as Japanese and Korean follow Subject-Object-Verb (SOV) word order, which is completely different from European languages such as English and French that follow Subject-Verb-Object word. The difference is not limited to the position of \"Object\" or the accusative case, and the former is also called head-final and the latter is also called head-initial. Because of the difference, phrasebased SMT between SVO and SOV does not work well. This talk introduces Head Finalization that reorders sentences into the head-final word order. According to the result of the NTCIR-9 workshop, Head Finalization was quite effective for English-to- Japanese patent translation.\n",
    ""
   ]
  },
  "federico12_iwslt": {
   "authors": [
    [
     "Marcello",
     "Federico"
    ],
    [
     "Mauro",
     "Cettolo"
    ],
    [
     "Luisa",
     "Bentivogli"
    ],
    [
     "Michael",
     "Paul"
    ],
    [
     "Sebastian",
     "Stüker"
    ]
   ],
   "title": "Overview of the IWSLT 2012 evaluation campaign",
   "original": "sltc_012",
   "page_count": 22,
   "order": 4,
   "p1": "12",
   "pn": "33",
   "abstract": [
    "We report on the ninth evaluation campaign organized by the IWSLT workshop. This year, the evaluation offered multi- ple tracks on lecture translation based on the TED corpus, and one track on dialog translation from Chinese to English based on the Olympic trilingual corpus. In particular, the TED tracks included a speech transcription track in English, a speech translation track from English to French, and text translation tracks from English to French and from Arabic to English. In addition to the official tracks, ten unofficial MT tracks were offered that required translating TED talks into English from either Chinese, Dutch, German, Polish, Por- tuguese (Brazilian), Romanian, Russian, Slovak, Slovene, or Turkish. 16 teams participated in the evaluation and sub- mitted a total of 48 primary runs. All runs were evaluated with objective metrics, while runs of the official translation tracks were also ranked by crowd-sourced judges. In par- ticular, subjective ranking for the TED task was performed on a progress test which permitted direct comparison of the results from this year against the best results from the 2011 round of the evaluation campaign.\n",
    ""
   ]
  },
  "yamamoto12_iwslt": {
   "authors": [
    [
     "Hitoshi",
     "Yamamoto"
    ],
    [
     "Youzheng",
     "Wu"
    ],
    [
     "Chien-Lin",
     "Huang"
    ],
    [
     "Xugang",
     "Lu"
    ],
    [
     "Paul R.",
     "Dixon"
    ],
    [
     "Shigeki",
     "Matsuda"
    ],
    [
     "Chiori",
     "Hori"
    ],
    [
     "Hideki",
     "Kashioka"
    ]
   ],
   "title": "The NICT ASR system for IWSLT2012",
   "original": "sltc_034",
   "page_count": 4,
   "order": 5,
   "p1": "34",
   "pn": "37",
   "abstract": [
    "This paper describes our automatic speech recognition (ASR) systemfor the IWSLT 2012 evaluation campaign. The target data of the campaign is selected from the TED talks, a collection of public speeches on a variety of topics spoken in English. Our ASR system is based on weighted finite-state transducers and exploits an combination of acoustic models for spontaneous speech, language models based on ngram and factored recurrent neural network trained with effectively selected corpora, and unsupervised topic adaptation framework utilizing ASR results. Accordingly, the system achieved 10.6% and 12.0% word error rate for the tst2011 and tst2012 evaluation set, respectively.\n",
    ""
   ]
  },
  "mediani12_iwslt": {
   "authors": [
    [
     "Mohammed",
     "Mediani"
    ],
    [
     "Yuqi",
     "Zhang"
    ],
    [
     "Thanh-Le",
     "Ha"
    ],
    [
     "Jan",
     "Niehues"
    ],
    [
     "Eunah",
     "Cho"
    ],
    [
     "Teresa",
     "Herrmann"
    ],
    [
     "Rainer",
     "Kärgel"
    ],
    [
     "Alex",
     "Waibel"
    ]
   ],
   "title": "The KIT translation systems for IWSLT 2012",
   "original": "sltc_038",
   "page_count": 8,
   "order": 6,
   "p1": "38",
   "pn": "45",
   "abstract": [
    "In this paper, we present the KIT systems participating in the English-French TED Translation tasks in the framework of the IWSLT 2012 machine translation evaluation. We also present several additional experiments on the English- German, English-Chinese and English-Arabic translation pairs.   Our system is a phrase-based statistical machine translation system, extended with many additional models which were proven to enhance the translation quality. For instance, it uses the part-of-speech (POS)-based reordering, translation and language model adaptation, bilingual language model, word-cluster language model, discriminative word lexica (DWL), and continuous space language model.   In addition to this, the system incorporates special steps in the preprocessing and in the post-processing step. In the preprocessing the noisy corpora are filtered by removing the noisy sentence pairs, whereas in the postprocessing the agreement between a noun and its surrounding words in the French translation is corrected based on POS tags with morphological information.   Our system deals with speech transcription input by removing case information and punctuation except periods from the text translation model.\n",
    ""
   ]
  },
  "hasler12_iwslt": {
   "authors": [
    [
     "Eva",
     "Hasler"
    ],
    [
     "Peter",
     "Bell"
    ],
    [
     "Arnab",
     "Ghoshal"
    ],
    [
     "Barry",
     "Haddow"
    ],
    [
     "Philipp",
     "Koehn"
    ],
    [
     "Fergus",
     "McInnes"
    ],
    [
     "Steve",
     "Renals"
    ],
    [
     "Pawel",
     "Swietojanski"
    ]
   ],
   "title": "The UEDIN systems for the IWSLT 2012 evaluation",
   "original": "sltc_046",
   "page_count": 8,
   "order": 7,
   "p1": "46",
   "pn": "53",
   "abstract": [
    "This paper describes the University of Edinburgh (UEDIN) systems for the IWSLT 2012 Evaluation. We participated in the ASR (English), MT (English-French, German-English) and SLT (English-French) tracks.\n",
    ""
   ]
  },
  "neubig12_iwslt": {
   "authors": [
    [
     "Graham",
     "Neubig"
    ],
    [
     "Kevin",
     "Duh"
    ],
    [
     "Masaya",
     "Ogushi"
    ],
    [
     "Takatomo",
     "Kano"
    ],
    [
     "Tetsuo",
     "Kiso"
    ],
    [
     "Sakriani",
     "Sakti"
    ],
    [
     "Tomoki",
     "Toda"
    ],
    [
     "Satoshi",
     "Nakamura"
    ]
   ],
   "title": "The NAIST machine translation system for IWSLT2012",
   "original": "sltc_054",
   "page_count": 7,
   "order": 8,
   "p1": "54",
   "pn": "60",
   "abstract": [
    "This paper describes the NAIST statistical machine translation system for the IWSLT2012 Evaluation Campaign. We participated in all TED Talk tasks, for a total of 11 languagepairs. For all tasks, we use the Moses phrase-based decoder and its experiment management system as a common base for building translation systems. The focus of our work is on performing a comprehensive comparison of a multitude of existing techniques for the TED task, exploring issues such as out-of-domain data filtering, minimum Bayes risk decoding, MERT vs. PRO tuning, word alignment combination, and morphology.\n",
    ""
   ]
  },
  "ruiz12_iwslt": {
   "authors": [
    [
     "Nicholas",
     "Ruiz"
    ],
    [
     "Arianna",
     "Bisazza"
    ],
    [
     "Roldano",
     "Cattoni"
    ],
    [
     "Marcello",
     "Federico"
    ]
   ],
   "title": "FBK’s machine translation systems for IWSLT 2012’s TED lectures",
   "original": "sltc_061",
   "page_count": 8,
   "order": 9,
   "p1": "61",
   "pn": "68",
   "abstract": [
    "This paper reports on FBK’s Machine Translation (MT) submissions at the IWSLT 2012 Evaluation on the TED talk translation tasks. We participated in the English-French and the Arabic-, Dutch-, German-, and Turkish-English translation tasks. Several improvements are reported over our last year baselines. In addition to using fill-up combinations of phrase-tables for domain adaptation, we explore the use of corpora filtering based on cross-entropy to produce concise and accurate translation and language models. We describe challenges encountered in under-resourced languages (Turkish) and language-specific preprocessing needs.\n",
    ""
   ]
  },
  "peitz12_iwslt": {
   "authors": [
    [
     "Stephan",
     "Peitz"
    ],
    [
     "Saab",
     "Mansour"
    ],
    [
     "Markus",
     "Freitag"
    ],
    [
     "Minwei",
     "Feng"
    ],
    [
     "Matthias",
     "Huck"
    ],
    [
     "Joern",
     "Wuebker"
    ],
    [
     "Malte",
     "Nuhn"
    ],
    [
     "Markus",
     "Nußbaum-Thom"
    ],
    [
     "Hermann",
     "Ney"
    ]
   ],
   "title": "The RWTH Aachen speech recognition and machine translation system for IWSLT 2012",
   "original": "sltc_069",
   "page_count": 8,
   "order": 10,
   "p1": "69",
   "pn": "76",
   "abstract": [
    "In this paper, the automatic speech recognition (ASR) and statistical machine translation (SMT) systems of RWTH Aachen University developed for the evaluation campaign of the International Workshop on Spoken Language Translation (IWSLT) 2012 are presented. We participated in the ASR (English), MT (English-French, Arabic-English, Chinese- English, German-English) and SLT (English-French) tracks. For the MT track both hierarchical and phrase-based SMT decoders are applied. A number of different techniques are evaluated in the MT and SLT tracks, including domain adaptation via data selection, translation model interpolation, phrase training for hierarchical and phrase-based systems, additional reordering model, word class language model, various Arabic and Chinese segmentation methods, postprocessing of speech recognition output with an SMT system, and system combination. By application of these methods we can show considerable improvements over the respective baseline systems.\n",
    ""
   ]
  },
  "zhu12_iwslt": {
   "authors": [
    [
     "Xiaoning",
     "Zhu"
    ],
    [
     "Yiming",
     "Cui"
    ],
    [
     "Conghui",
     "Zhu"
    ],
    [
     "Tiejun",
     "Zhao"
    ],
    [
     "Hailong",
     "Cao"
    ]
   ],
   "title": "The HIT-LTRC machine translation system for IWSLT 2012",
   "original": "sltc_077",
   "page_count": 4,
   "order": 11,
   "p1": "77",
   "pn": "80",
   "abstract": [
    "In this paper, we describe HIT-LTRC's participation in the IWSLT 2012 evaluation campaign. In this year, we took part in the Olympics Task which required the participants to translate Chinese to English with limited data.   Our system is based on Moses[1], which is an open source machine translation system. We mainly used the phrase-based models to carry out our experiments, and factored-based models were also performed in comparison. All the involved tools are freely available.   In the evaluation campaign, we focus on data selection, phrase extraction method comparison and phrase table combination.\n",
    ""
   ]
  },
  "falavigna12_iwslt": {
   "authors": [
    [
     "Daniele",
     "Falavigna"
    ],
    [
     "Roberto",
     "Gretter"
    ],
    [
     "Fabio",
     "Brugnara"
    ],
    [
     "Diego",
     "Giuliani"
    ]
   ],
   "title": "FBK@IWSLT 2012 - ASR track",
   "original": "sltc_081",
   "page_count": 6,
   "order": 12,
   "p1": "81",
   "pn": "86",
   "abstract": [
    "This paper reports on the participation of FBK at the IWSLT2012 evaluation campaign on automatic speech recognition: namely in the English ASR track. Both primary and contrastive submissions have been sent for evaluation.   The ASR system features acoustic models trained on a portion of the TED talk recordings that was automatically selected according to the fidelity of the provided transcriptions. Three decoding steps are performed interleaved by acoustic feature normalization and acoustic model adaptation.   A final rescoring step, based on the usage of an interpolated language model, is applied to word graphs generated in the third decoding step. For the primary submission, language models entering the interpolation are trained on both out-of-domain and in-domain text data, instead the contrastive submission uses both \"general purpose\" and auxiliary language models trained only on out-of-domain text data. Despite this fact, similar performance are obtained with the two submissions.\n",
    ""
   ]
  },
  "saam12_iwslt": {
   "authors": [
    [
     "Christian",
     "Saam"
    ],
    [
     "Christian",
     "Mohr"
    ],
    [
     "Kevin",
     "Kilgour"
    ],
    [
     "Michael",
     "Heck"
    ],
    [
     "Matthias",
     "Sperber"
    ],
    [
     "Keigo",
     "Kubo"
    ],
    [
     "Sebastian",
     "Stüker"
    ],
    [
     "Sakriani",
     "Sakti"
    ],
    [
     "Graham",
     "Neubig"
    ],
    [
     "Tomoki",
     "Toda"
    ],
    [
     "Satoshi",
     "Nakamura"
    ],
    [
     "Alex",
     "Waibel"
    ]
   ],
   "title": "The 2012 KIT and KIT-NAIST English ASR systems for the IWSLT evaluation",
   "original": "sltc_087",
   "page_count": 4,
   "order": 13,
   "p1": "87",
   "pn": "90",
   "abstract": [
    "This paper describes our English Speech-to-Text (STT) systems for the 2012 IWSLT TED ASR track evaluation. The systems consist of 10 subsystems that are combinations of different front-ends, e.g. MVDR based and MFCC based ones, and two different phone sets. The outputs of the subsystems are combined via confusion network combination. Decoding is done in two stages, where the systems of the second stage are adapted in an unsupervised manner on the combination of the first stage outputs using VTLN, MLLR, and cMLLR.\n",
    "Index Terms: speech recognition, IWSLT, TED talks, evaluation system, system development\n",
    ""
   ]
  },
  "heck12_iwslt": {
   "authors": [
    [
     "Michael",
     "Heck"
    ],
    [
     "Keigo",
     "Kubo"
    ],
    [
     "Matthias",
     "Sperber"
    ],
    [
     "Sakriani",
     "Sakti"
    ],
    [
     "Sebastian",
     "Stüker"
    ],
    [
     "Christian",
     "Saam"
    ],
    [
     "Kevin",
     "Kilgour"
    ],
    [
     "Christian",
     "Mohr"
    ],
    [
     "Graham",
     "Neubig"
    ],
    [
     "Tomoki",
     "Toda"
    ],
    [
     "Satoshi",
     "Nakamura"
    ],
    [
     "Alex",
     "Waibel"
    ]
   ],
   "title": "The KIT-NAIST (contrastive) English ASR system for IWSLT 2012",
   "original": "sltc_091",
   "page_count": 5,
   "order": 14,
   "p1": "91",
   "pn": "95",
   "abstract": [
    "This paper describes the KIT-NAIST (Contrastive) English speech recognition system for the IWSLT 2012 Evaluation Campaign. In particular, we participated in the ASR track of the IWSLT TED task. The system was developed by Karlsruhe Institute of Technology (KIT) and Nara Institute of Science and Technology (NAIST) teams in collaboration within the interACT project. We employ single system decoding with fully continuous and semi-continuousmodels, as well as a three-stage, multipass system combination framework built with the Janus Recognition Toolkit. On the IWSLT 2010 test set our single system introduced in this work achieves a WER of 17.6%, and our final combination achieves a WER of 14.4%.\n",
    ""
   ]
  },
  "chu12_iwslt": {
   "authors": [
    [
     "Chenhui",
     "Chu"
    ],
    [
     "Toshiaki",
     "Nakazawa"
    ],
    [
     "Sadao",
     "Kurohashi"
    ]
   ],
   "title": "EBMT system of kyoto university in OLYMPICS task at IWSLT 2012",
   "original": "sltc_096",
   "page_count": 6,
   "order": 15,
   "p1": "96",
   "pn": "101",
   "abstract": [
    "This paper describes the EBMT system of Kyoto University that participated in the OLYMPICS task at IWSLT 2012. When translating very different language pairs such as Chinese-English, it is very important to handle sentences in tree structures to overcome the difference. Many recent studies incorporate tree structures in some parts of translation process, but not all the way from model training (alignment) to decoding. Our system is a fully tree-based translation system where we use the Bayesian phrase alignment model on dependency trees and example-based translation. To improve the translation quality, we conduct some special processing for the IWSLT 2012 OLYMPICS task, including sub-sentence splitting, non-parallel sentence filtering, adoption of an optimized Chinese segmenter and rule-based decoding constraints.\n",
    ""
   ]
  },
  "besacier12_iwslt": {
   "authors": [
    [
     "Laurent",
     "Besacier"
    ],
    [
     "Benjamin",
     "Lecouteux"
    ],
    [
     "Marwen",
     "Azouzi"
    ],
    [
     "Luong Ngoc",
     "Quang"
    ]
   ],
   "title": "The LIG English to French machine translation system for IWSLT 2012",
   "original": "sltc_102",
   "page_count": 7,
   "order": 16,
   "p1": "102",
   "pn": "108",
   "abstract": [
    "This paper presents the LIG participation to the E-F MT task of IWSLT 2012. The primary system proposed made a large improvement (more than 3 point of BLEU on tst2010 set) compared to our last year participation. Part of this improvment was due to the use of an extraction from the Gigaword corpus. We also propose a preliminary adaptation of the driven decoding concept for machine translation. This method allows an efficient combination of machine translation systems, by rescoring the log-linear model at the N-best list level according to auxiliary systems: the basis technique is essentially guiding the search using one or previous system outputs. The results show that the approach allows a significant improvement in BLEU score using Google translate to guide our own SMT system. We also try to use a confidence measure as an additional log-linear feature but we could not get any improvment with this technique.\n",
    ""
   ]
  },
  "drexler12_iwslt": {
   "authors": [
    [
     "Jennifer",
     "Drexler"
    ],
    [
     "Wade",
     "Shen"
    ],
    [
     "",
     "Terry"
    ],
    [
     "Tim",
     "Anderson"
    ],
    [
     "Raymond",
     "Slyh"
    ],
    [
     "Brian",
     "Ore"
    ],
    [
     "Eric",
     "Hansen"
    ]
   ],
   "title": "The MIT-LL/AFRL IWSLT-2012 MT system",
   "original": "sltc_109",
   "page_count": 8,
   "order": 17,
   "p1": "109",
   "pn": "116",
   "abstract": [
    "This paper describes the MIT-LL/AFRL statistical MT system and the improvements that were developed during the IWSLT 2012 evaluation campaign. As part of these efforts, we experimented with a number of extensions to the standard phrase-based model that improve performance on the Arabic to English and English to French TED-talk translation task. We also applied our existing ASR system to the TED-talk lecture ASR task, and combined our ASR and MT systems for the TED-talk SLT task.   We discuss the architecture of the MIT-LL/AFRL MT system, improvements over our 2011 system, and experiments we ran during the IWSLT-2012 evaluation. Specifically, we focus on 1) cross-domain translation using MAP adaptation, 2) cross-entropy filtering of MT training data, and 3) improved Arabic morphology for MT preprocessing\n",
    ""
   ]
  },
  "shimizu12_iwslt": {
   "authors": [
    [
     "Hiroaki",
     "Shimizu"
    ],
    [
     "Masao",
     "Utiyama"
    ],
    [
     "Eiichiro",
     "Sumita"
    ],
    [
     "Satoshi",
     "Nakamura"
    ]
   ],
   "title": "Minimum Bayes-Risk decoding extended with similar examples: NAIST-NICT at IWSLT 2012",
   "original": "sltc_117",
   "page_count": 4,
   "order": 18,
   "p1": "117",
   "pn": "120",
   "abstract": [
    "This paper describes our methods used in the NAIST-NICT submission to the International Workshop on Spoken Language Translation (IWSLT) 2012 evaluation campaign. In particular, we propose two extensions to minimum bayesrisk decoding which reduces a expected loss.\n",
    ""
   ]
  },
  "finch12_iwslt": {
   "authors": [
    [
     "Andrew",
     "Finch"
    ],
    [
     "Ohnmar",
     "Htun"
    ],
    [
     "Eiichiro",
     "Sumita"
    ]
   ],
   "title": "The NICT translation system for IWSLT 2012",
   "original": "sltc_121",
   "page_count": 5,
   "order": 19,
   "p1": "121",
   "pn": "125",
   "abstract": [
    "This paper describes NTCT's participation in the IWSLT 2012 evaluation campaign forthe TED speech translation Russian-English shared-task. Our approach was based on a phrase-based statistical machine translation system that was augmented by using transliteration mining techniques. The basic premise behind our approach was to try to use sub-word-level alignments to guide the word-level alignment process used to learn the phrase-table. We did this by first mining a corpus of Russian-English transliterations pairs and cognates from a set of interlanguage link titles from Wikipedia. This corpus was then used to build a many-to-many nonparametric Dayesian bilingual alignment model that could be used to identify the occurrence of transliterations and cognates in the training corpus itself. Alignment counts for these mined pairs were increased in the training corpus to increase the likelihood that these pairs would align in training. Our experiments on the test sets from the 2010 and 2011 shared tasks, showed that an improvement in BLEU score can be gained in translation performance by encouraging the alignment of cognates and transliterations during word alignment.\n",
    ""
   ]
  },
  "marasek12_iwslt": {
   "authors": [
    [
     "Krzysztof",
     "Marasek"
    ]
   ],
   "title": "TED Polish-to-English translation system for the IWSLT 2012",
   "original": "sltc_126",
   "page_count": 4,
   "order": 20,
   "p1": "126",
   "pn": "129",
   "abstract": [
    "This paper presents efforts in preparation of the Polish-to- English SMT system for the TED lectures domain that is to be evaluated during the IWSLT 2012 Conference. Our attempts cover systems which use stems and morphological information on Polish words (using two different tools) and stems and POS.\n",
    ""
   ]
  },
  "na12_iwslt": {
   "authors": [
    [
     "Hwidong",
     "Na"
    ],
    [
     "Jong-Hyeok",
     "Lee"
    ]
   ],
   "title": "Forest-to-string translation using binarized dependency forest for IWSLT 2012 OLYMPICS task",
   "original": "sltc_130",
   "page_count": 6,
   "order": 21,
   "p1": "130",
   "pn": "135",
   "abstract": [
    "We participated in the OLYMPICS task in IWSLT 2012 and submitted two formal runs using a forest-to-string translation system. Our primary run achieved better translation quality than our contrastive run, but worse than a phrase-based and a hierarchical system using Moses.\n",
    ""
   ]
  },
  "dumitrescu12_iwslt": {
   "authors": [
    [
     "Ştefan Daniel",
     "Dumitrescu"
    ],
    [
     "Radu",
     "Ion"
    ],
    [
     "Dan",
     "Ştefǎnescu"
    ],
    [
     "Tiberiu",
     "Boroş"
    ],
    [
     "Dan",
     "Tufiş"
    ]
   ],
   "title": "Romanian to English automatic MT experiments at IWSLT12 (system description paper)",
   "original": "sltc_136",
   "page_count": 8,
   "order": 22,
   "p1": "136",
   "pn": "143",
   "abstract": [
    "The paper presents the system developed by RACAI for the ISWLT 2012 competition, TED task, MT track, Romanian to English translation. We describe the starting baseline phrasebased SMT system, the experiments conducted to adapt the language and translation models and our post-translation cascading system designed to improve the translation without external resources. We further present our attempts at creating a better controlled decoder than the open-source Moses system offers.\n",
    ""
   ]
  },
  "mermer12_iwslt": {
   "authors": [
    [
     "Coşkun",
     "Mermer"
    ],
    [
     "Hamza",
     "Kaya"
    ],
    [
     "İlknur Durgar",
     "El-Kahlout"
    ],
    [
     "Mehmet Uğur",
     "Doğan"
    ]
   ],
   "title": "The TÜBİTAK statistical machine translation system for IWSLT 2012",
   "original": "sltc_144",
   "page_count": 5,
   "order": 23,
   "p1": "144",
   "pn": "148",
   "abstract": [
    "We describe the TÜBİTAK submission to the IWSLT 2012 Evaluation Campaign. Our system development focused on utilizing Bayesian alignment methods such as variational Bayes and Gibbs sampling in addition to the standard GIZA++ alignments. The submitted tracks are the Arabic- English and Turkish-English TED Talks translation tasks.\n",
    ""
   ]
  },
  "prasad12_iwslt": {
   "authors": [
    [
     "Rohit",
     "Prasad"
    ],
    [
     "Rohit",
     "Kumar"
    ],
    [
     "Sankaranarayanan",
     "Ananthakrishnan"
    ],
    [
     "Wei",
     "Chen"
    ],
    [
     "Sanjika",
     "Hewavitharana"
    ],
    [
     "Matthew",
     "Roy"
    ],
    [
     "Frederick",
     "Choi"
    ],
    [
     "Aaron",
     "Challenner"
    ],
    [
     "Enoch",
     "Kan"
    ],
    [
     "Arvind",
     "Neelakantan"
    ],
    [
     "Prem",
     "Natarajan"
    ]
   ],
   "title": "Active error detection and resolution for speech-to-speech translation",
   "original": "sltc_150",
   "page_count": 8,
   "order": 24,
   "p1": "150",
   "pn": "157",
   "abstract": [
    "We describe a novel two-way speech-to-speech (S2S) translation system that actively detects a wide variety of common error types and resolves them through user-friendly dialog with the user(s). We present algorithms for detecting out-of-vocabulary (OOV) named entities and terms, sense ambiguities, homophones, idioms, ill-formed input, etc. and discuss novel, interactive strategies for recovering from such errors. We also describe our approach for prioritizing different error types and an extensible architecture for implementing these decisions. We demonstrate the efficacy of our system by presenting analysis on live interactions in the English-to-Iraqi Arabic direction that are designed to invoke different error types for spoken language translation. Our analysis shows that the system can successfully resolve 47% of the errors, resulting in a dramatic improvement in the transfer of problematic concepts.\n",
    ""
   ]
  },
  "kano12_iwslt": {
   "authors": [
    [
     "Takatomo",
     "Kano"
    ],
    [
     "Sakriani",
     "Sakti"
    ],
    [
     "Shinnosuke",
     "Takamichi"
    ],
    [
     "Graham",
     "Neubig"
    ],
    [
     "Tomoki",
     "Toda"
    ],
    [
     "Satoshi",
     "Nakamura"
    ]
   ],
   "title": "A method for translation of paralinguistic information",
   "original": "sltc_158",
   "page_count": 6,
   "order": 25,
   "p1": "158",
   "pn": "163",
   "abstract": [
    "This paper is concerned with speech-to-speech translation that is sensitive to paralinguistic information. From the many different possible paralinguistic features to handle, in this paper we chose duration and power as a first step, proposing a method that can translate these features from input speech to the output speech in continuous space. This is done in a simple and language-independent fashion by training a regression model that maps source language duration and power information into the target language. We evaluate the proposed method on a digit translation task and show that paralinguistic information in input speech appears in output speech, and that this information can be used by target language speakers to detect emphasis.\n",
    ""
   ]
  },
  "niehues12_iwslt": {
   "authors": [
    [
     "Jan",
     "Niehues"
    ],
    [
     "Alex",
     "Waibel"
    ]
   ],
   "title": "Continuous space language models using restricted Boltzmann machines",
   "original": "sltc_164",
   "page_count": 7,
   "order": 26,
   "p1": "164",
   "pn": "170",
   "abstract": [
    "We present a novel approach for continuous space language models in statistical machine translation by using Restricted Boltzmann Machines (RBMs). The probability of an n-gram is calculated by the free energy of the RBM instead of a feedforward neural net. Therefore, the calculation is much faster and can be integrated into the translation process instead of using the language model only in a re-ranking step.   Furthermore, it is straightforward to introduce additional word factors into the language model. We observed a faster convergence in training if we include automatically generated word classes as an additional word factor.   We evaluated the RBM-based language model on the German to English and English to French translation task of TED lectures. Instead of replacing the conventional n-grambased language model, we trained the RBM-based language model on the more important but smaller in-domain data and combined them in a log-linear way. With this approach we could show improvements of about half a BLEU point on the translation task.\n",
    ""
   ]
  },
  "falavigna12b_iwslt": {
   "authors": [
    [
     "Daniele",
     "Falavigna"
    ],
    [
     "Roberto",
     "Gretter"
    ]
   ],
   "title": "Focusing language models for automatic speech recognition",
   "original": "sltc_171",
   "page_count": 8,
   "order": 27,
   "p1": "171",
   "pn": "178",
   "abstract": [
    "This paper describes a method for selecting text data from a corpus with the aim of training auxiliary Language Models (LMs) for an Automatic Speech Recognition (ASR) system. A novel similarity score function is proposed, which allows to score each document belonging to the corpus in order to select those with the highest scores for training auxiliary LMs which are linearly interpolated with the baseline one. The similarity score function makes use of \"similarity models\" built from the automatic transcriptions furnished by earlier stages of the ASR system, while the documents selected for training auxiliary LMs are drawn from the same set of data used to train the baseline LM used in the ASR system. In this way, the resulting interpolated LMs are \"focused\" towards the output of the recognizer itself.   The approach allows to improve word error rate, measured on a task of spontaneous speech, of about 3% relative. It is important to note that a similar improvement has been obtained using an \"in-domain\" set of texts data not contained in the sources used to train the baseline LM.   In addition, we compared the proposed similarity score function with two other ones based on perplexity (PP) and on TFxIDF (Term Frequency x Inverse Document Frequency) vector space model. The proposed approach provides about the same performance as that based on TFxIDF model but requires both lower computation and occupation memory.\n",
    ""
   ]
  },
  "koehn12_iwslt": {
   "authors": [
    [
     "Philipp",
     "Koehn"
    ]
   ],
   "title": "Simulating human judgment in machine translation evaluation campaigns",
   "original": "sltc_179",
   "page_count": 6,
   "order": 28,
   "p1": "179",
   "pn": "184",
   "abstract": [
    "We present a Monte Carlo model to simulate human judgments in machine translation evaluation campaigns, such as WMT or IWSLT. We use the model to compare different ranking methods and to give guidance on the number of judgments that need to be collected to obtain sufficiently significant distinctions between systems.\n",
    ""
   ]
  },
  "aransa12_iwslt": {
   "authors": [
    [
     "Walid",
     "Aransa"
    ],
    [
     "Holger",
     "Schwenk"
    ],
    [
     "Loic",
     "Barrault"
    ]
   ],
   "title": "Semi-supervised transliteration mining from parallel and comparable corpora",
   "original": "sltc_185",
   "page_count": 8,
   "order": 29,
   "p1": "185",
   "pn": "192",
   "abstract": [
    "Transliteration is the process of writing a word (mainly proper noun) from one language in the alphabet of another language. This process requires mapping the pronunciation of the word from the source language to the closest possible pronunciation in the target language. In this paper we introduce a new semi-supervised transliteration mining method for parallel and comparable corpora. The method is mainly based on a new suggested Three Levels of Similarity (TLS) scores to extract the transliteration pairs. The first level calculates the similarity of of all vowel letters and consonants letters. The second level calculates the similarity of long vowels and vowel letters at beginning and end position of the words and consonants letters. The third level calculates the similarity consonants letters only.   We applied our method on Arabic-English parallel and comparable corpora. We evaluated the extracted transliteration pairs using a statistical based transliteration system. This system is built using letters instead or words as tokens. The transliteration system achieves an accuracy of 0.50 and a mean F-score 0.8958 when trained on transliteration pairs extracted from a parallel corpus. The accuracy is 0.30 and the mean F-score 0.84 when we used instead a comparable corpus to automatically extract the transliteration pairs. This shows that the proposed semi-supervised transliteration mining algorithm is effective and can be applied to other language pairs. We also evaluated two segmentation techniques and reported the impact on the transliteration performance.\n",
    ""
   ]
  },
  "mansour12_iwslt": {
   "authors": [
    [
     "Saab",
     "Mansour"
    ],
    [
     "Hermann",
     "Ney"
    ]
   ],
   "title": "A simple and effective weighted phrase extraction for machine translation adaptation",
   "original": "sltc_193",
   "page_count": 8,
   "order": 30,
   "p1": "193",
   "pn": "200",
   "abstract": [
    "The task of domain-adaptation attempts to exploit data mainly drawn from one domain (e.g. news) to maximize the performance on the test domain (e.g. weblogs). In previous work, weighting the training instances was used for filtering dissimilar data. We extend this by incorporating the weights directly into the standard phrase training procedure of statistical machine translation (SMT). This allows the SMT system to make the decision whether to use a phrase translation pair or not, a more methodological way than discarding phrase pairs completely when using filtering. Furthermore, we suggest a combined filtering and weighting procedure to achieve better results while reducing the phrase table size. The proposed methods are evaluated in the context of Arabicto- English translation on various conditions, where significant improvements are reported when using the suggested weighted phrase training. The weighting method also improves over filtering, and the combined filtering and weighting is better than a standalone filtering method. Finally, we experiment with mixture modeling, where additional improvements are reported when using weighted phrase extraction over a variety of baselines.\n",
    ""
   ]
  },
  "axelrod12_iwslt": {
   "authors": [
    [
     "Amittai",
     "Axelrod"
    ],
    [
     "QingJun",
     "Li"
    ],
    [
     "William D.",
     "Lewis"
    ]
   ],
   "title": "Applications of data selection via cross-entropy difference for real-world statistical machine translation",
   "original": "sltc_201",
   "page_count": 8,
   "order": 31,
   "p1": "201",
   "pn": "208",
   "abstract": [
    "We broaden the application of data selection methods for domain adaptation to a larger number of languages, data, and decoders than shown in previous work, and explore comparable applications for both monolingual and bilingual crossentropy difference methods. We compare domain adapted systems against very large general-purpose systems for the same languages, and do so without a bias to a particular direction. We present results against real-world generalpurpose systems tuned on domain-specific data, which are substantially harder to beat than standard research baseline systems. We show better performance for nearly all domain adapted systems, despite the fact that the domainadapted systems are trained on a fraction of the content of their general domain counterparts. The high performance of these methods suggest applicability to a wide variety of contexts, particularly in scenarios where only small supplies of unambiguously domain-specific data are available, yet it is believed that additional similar data is included in larger heterogenous-content general-domain corpora.\n",
    ""
   ]
  },
  "tu12_iwslt": {
   "authors": [
    [
     "Mei",
     "Tu"
    ],
    [
     "Yu",
     "Zhou"
    ],
    [
     "Chengqing",
     "Zong"
    ]
   ],
   "title": "A universal approach to translating numerical and time expressions",
   "original": "sltc_209",
   "page_count": 8,
   "order": 32,
   "p1": "209",
   "pn": "216",
   "abstract": [
    "Although statistical machine translation (SMT) has made great progress since it came into being, the translation of numerical and time expressions is still far from satisfactory. Generally speaking, numbers are likely to be out-of-vocabulary (OOV) words due to their non-exhaustive characteristics even when the size of training data is very large, so it is difficult to obtain accurate translation results for the infinite set of numbers only depending on traditional statistical methods. We propose a language-independent framework to recognize and translate numbers more precisely by using a rule-based method. Through designing operators, we succeed to make rules educible and totally separate from codes, thus, we can extend rules to various language-pairs without re-coding, which contributes a lot to the efficient development of an SMT system with good portability. We classify numbers and time expressions into seven types, which are Arabic number, cardinal numbers, ordinal numbers, date, time of day, day of week and figures. A greedy algorithm is developed to deal with rule conflicts. Experiments have shown that our approach can significantly improve the translation performance.\n",
    ""
   ]
  },
  "kolkhorst12_iwslt": {
   "authors": [
    [
     "Henrich",
     "Kolkhorst"
    ],
    [
     "Kevin",
     "Kilgour"
    ],
    [
     "Sebastian",
     "Stüker"
    ],
    [
     "Alex",
     "Waibel"
    ]
   ],
   "title": "Evaluation of interactive user corrections for lecture transcription",
   "original": "sltc_217",
   "page_count": 5,
   "order": 33,
   "p1": "217",
   "pn": "221",
   "abstract": [
    "In this work, we present and evaluate the usage of an interactive web interface for browsing and correcting lecture transcripts. An experiment performed with potential users without transcription experience provides us with a set of example corrections.   On German lecture data, user corrections greatly improve the comprehensibility of the transcripts, yet only reduce the WER to 22%. The precision of user edits is relatively low at 77% and errors in inflection, case and compounds were rarely corrected. Nevertheless, characteristic lecture data errors, such as highly specific terms, were typically corrected, providing valuable additional information.\n",
    "Index Terms: speech recognition, user study, transcript correction, lectures\n",
    ""
   ]
  },
  "wu12_iwslt": {
   "authors": [
    [
     "Youzheng",
     "Wu"
    ],
    [
     "Hitoshi",
     "Yamamoto"
    ],
    [
     "Xugang",
     "Lu"
    ],
    [
     "Shigeki",
     "Matsuda"
    ],
    [
     "Chiori",
     "Hori"
    ],
    [
     "Hideki",
     "Kashioka"
    ]
   ],
   "title": "Factored recurrent neural network language model in TED lecture transcription",
   "original": "sltc_222",
   "page_count": 7,
   "order": 34,
   "p1": "222",
   "pn": "228",
   "abstract": [
    "In this study, we extend recurrent neural network-based language models (RNNLMs) by explicitly integrating morphological and syntactic factors (or features). Our proposed RNNLM is called a factored RNNLM that is expected to enhance RNNLMs. A number of experiments are carried out on top of state-of-the-art LVCSR system that show the factored RNNLM improves the performance measured by perplexity and word error rate. In the IWSLT TED test data sets, absolute word error rate reductions over RNNLM and n-gram LM are 0.4~0.8 points.\n",
    ""
   ]
  },
  "blain12_iwslt": {
   "authors": [
    [
     "Frédéric",
     "Blain"
    ],
    [
     "Holger",
     "Schwenk"
    ],
    [
     "Jean",
     "Senellart"
    ]
   ],
   "title": "Incremental adaptation using translation information and post-editing analysis",
   "original": "sltc_229",
   "page_count": 8,
   "order": 35,
   "p1": "229",
   "pn": "236",
   "abstract": [
    "It is well known that statistical machine translation systems perform best when they are adapted to the task. In this paper we propose new methods to quickly perform incremental adaptation without the need to obtain word-by-word alignments from GIZA or similar tools. The main idea is to use an automatic translation as pivot to infer alignments between the source sentence and the reference translation, or user correction. We compared our approach to the standard method to perform incremental re-training. We achieve similar results in the BLEU score using less computational resources. Fast retraining is particularly interesting when we want to almost instantly integrate user feed-back, for instance in a post-editing context or machine translation assisted CAT tool. We also explore several methods to combine the translation models.\n",
    ""
   ]
  },
  "khadivi12_iwslt": {
   "authors": [
    [
     "Shahram",
     "Khadivi"
    ],
    [
     "Zeinab",
     "Vakil"
    ]
   ],
   "title": "Interactive-predictive speech-enabled computer-assisted translation",
   "original": "sltc_237",
   "page_count": 7,
   "order": 36,
   "p1": "237",
   "pn": "243",
   "abstract": [
    "In this paper, we study the incorporation of statistical machine translation models to automatic speech recognition models in the framework of computer-assisted translation. The system is given a source language text to be translated and it shows the source text to the human translator to translate it orally. The system captures the user speech which is the dictation of the target language sentence. Then, the human translator uses an interactive-predictive process to correct the system generated errors. We show the efficiency of this method by higher human productivity gain compared to the baseline systems: pure ASR system and integrated ASR and MT systems.\n",
    ""
   ]
  },
  "ruiz12b_iwslt": {
   "authors": [
    [
     "Nicholas",
     "Ruiz"
    ],
    [
     "Marcello",
     "Federico"
    ]
   ],
   "title": "MDI adaptation for the lazy: avoiding normalization in LM adaptation for lecture translation",
   "original": "sltc_244",
   "page_count": 8,
   "order": 37,
   "p1": "244",
   "pn": "251",
   "abstract": [
    "This paper provides a fast alternative to Minimum Discrimination Information-based language model adaptation for statistical machine translation. We provide an alternative to computing a normalization term that requires computing full model probabilities (including back-off probabilities) for all n-grams. Rather than re-estimating an entire language model, our Lazy MDI approach leverages a smoothed unigram ratio between an adaptation text and the background language model to scale only the n-gram probabilities corresponding to translation options gathered by the SMT decoder. The effects of the unigram ratio are scaled by adding an additional feature weight to the log-linear discriminative model. We present results on the IWSLT 2012 TED talk translation task and show that Lazy MDI provides comparable language model adaptation performance to classic MDI.\n",
    ""
   ]
  },
  "cho12_iwslt": {
   "authors": [
    [
     "Eunah",
     "Cho"
    ],
    [
     "Jan",
     "Niehues"
    ],
    [
     "Alex",
     "Waibel"
    ]
   ],
   "title": "Segmentation and punctuation prediction in speech language translation using a monolingual translation system",
   "original": "sltc_252",
   "page_count": 8,
   "order": 38,
   "p1": "252",
   "pn": "259",
   "abstract": [
    "In spoken language translation (SLT), finding proper segmentation and reconstructing punctuation marks are not only significant but also challenging tasks. In this paper we present our recent work on speech translation quality analysis for German-English by improving sentence segmentation and punctuation.   From oracle experiments, we show an upper bound of translation quality if we had human-generated segmentation and punctuation on the output stream of speech recognition systems. In our oracle experiments we gain 1.78 BLEU points of improvements on the lecture test set. We build a monolingual translation system from German to German implementing segmentation and punctuation prediction as a machine translation task. Using the monolingual translation system we get an improvement of 1.53 BLEU points on the lecture test set, which is a comparable performance against the upper bound drawn by the oracle experiments.\n",
    ""
   ]
  },
  "feng12_iwslt": {
   "authors": [
    [
     "Minwei",
     "Feng"
    ],
    [
     "Jan-Thorsten",
     "Peter"
    ],
    [
     "Hermann",
     "Ney"
    ]
   ],
   "title": "Sequence labeling-based reordering model for phrase-based SMT",
   "original": "sltc_260",
   "page_count": 8,
   "order": 39,
   "p1": "260",
   "pn": "267",
   "abstract": [
    "For current statistical machine translation system, reordering is still a major problem for language pairs like Chinese- English, where the source and target language have significant word order differences. In this paper, we propose a novel reordering model based on sequence labeling techniques. Our model converts the reordering problem into a sequence labeling problem, i.e. a tagging task. For the given source sentence, we assign each source token a label which contains the reordering information for that token. We also design an unaligned word tag so that the unaligned word phenomenon is automatically implanted in the proposed model. Our reordering model is conditioned on the whole source sentence. Hence it is able to catch the long dependency in the source sentence. Although the learning on large scale task requests notably amounts of computational resources, the decoder makes use of the tagging information as soft constraints. Therefore, the training procedure of our model is computationally expensive for large task while in the test phase (during translation) our model is very efficient. We carried out experiments on five Chinese-English NIST tasks trained with BOLT data. Results show that our model improves the baseline system by 1.32 BLEU 1.53 TER on average.\n",
    ""
   ]
  },
  "hasler12b_iwslt": {
   "authors": [
    [
     "Eva",
     "Hasler"
    ],
    [
     "Barry",
     "Haddow"
    ],
    [
     "Philipp",
     "Koehn"
    ]
   ],
   "title": "Sparse lexicalised features and topic adaptation for SMT",
   "original": "sltc_268",
   "page_count": 8,
   "order": 40,
   "p1": "268",
   "pn": "275",
   "abstract": [
    "We present a new approach to domain adaptation for SMT that enriches standard phrase-based models with lexicalised word and phrase pair features to help the model select appropriate translations for the target domain (TED talks). In addition, we show how source-side sentence-level topics can be incorporated to make the features differentiate between more fine-grained topics within the target domain (topic adaptation). We compare tuning our sparse features on a development set versus on the entire in-domain corpus and introduce a new method of porting them to larger mixed-domain models. Experimental results show that our features improve performance over a MIRA baseline and that in some cases we can get additional improvements with topic features. We evaluate our methods on two language pairs, English-French and German-English, showing promising results.\n",
    ""
   ]
  },
  "peitz12b_iwslt": {
   "authors": [
    [
     "Stephan",
     "Peitz"
    ],
    [
     "Simon",
     "Wiesler"
    ],
    [
     "Markus",
     "Nußbaum-Thom"
    ],
    [
     "Hermann",
     "Ney"
    ]
   ],
   "title": "Spoken language translation using automatically transcribed text in training",
   "original": "sltc_276",
   "page_count": 8,
   "order": 41,
   "p1": "276",
   "pn": "283",
   "abstract": [
    "In spoken language translation a machine translation system takes speech as input and translates it into another language. A standard machine translation system is trained on written language data and expects written language as input. In this paper we propose an approach to close the gap between the output of automatic speech recognition and the input of machine translation by training the translation system on automatically transcribed speech. In our experiments we show improvements of up to 0.9 BLEU points on the IWSLT 2012 English-to-French speech translation task.\n",
    ""
   ]
  },
  "potet12_iwslt": {
   "authors": [
    [
     "Marion",
     "Potet"
    ],
    [
     "Laurent",
     "Besacier"
    ],
    [
     "Hervé",
     "Blanchon"
    ],
    [
     "Marwen",
     "Azouzi"
    ]
   ],
   "title": "Towards a better understanding of statistical post-edition usefulness",
   "original": "sltc_284",
   "page_count": 8,
   "order": 42,
   "p1": "284",
   "pn": "291",
   "abstract": [
    "We describe several experiments to better understand the usefulness of statistical post-edition (SPE) to improve phrasebased statistical MT (PBMT) systems raw outputs. Whatever the size of the training corpus, we show that SPE systems trained on general domain data offers no breakthrough to our baseline general domain PBMT system. However, using manually post-edited system outputs to train the SPE led to a slight improvement in the translations quality compared with the use of professional reference translations. We also show that SPE is far more effective for domain adaptation, mainly because it recovers a lot of specific terms unknown to our general PBMT system. Finally, we compare two domain adaptation techniques, post-editing a general domain PBMT system vs building a new domain-adapted PBMT system with two different techniques, and show that the latter outperforms the first one. Yet, when the PBMT is a “black box”, SPE trained with post-edited system outputs remains an interesting option for domain adaptation.\n",
    ""
   ]
  },
  "gong12_iwslt": {
   "authors": [
    [
     "Li",
     "Gong"
    ],
    [
     "Aurélien",
     "Max"
    ],
    [
     "François",
     "Yvon"
    ]
   ],
   "title": "Towards contextual adaptation for any-text translation",
   "original": "sltc_292",
   "page_count": 8,
   "order": 43,
   "p1": "292",
   "pn": "299",
   "abstract": [
    "Adaptation for Machine Translation has been studied in a variety of ways, using an ideal scenario where the training data can be split into ”out-of-domain” and ”in-domain” corpora, on which the adaptation is based. In this paper, we consider a more realistic setting which does not assume the availability of any kind of ”in-domain” data, hence the name ”any-text translation”. In this context, we present a new approach to contextually adapt a translation model onthe- fly, and present several experimental results where this approach outperforms conventionaly trained baselines. We also present a document-level contrastive evaluation whose results can be easily interpreted, even by non-specialists.\n",
    ""
   ]
  }
 },
 "sessions": [
  {
   "title": "Keynote Papers",
   "papers": [
    "wutiwiwatchai12_iwslt",
    "yu12_iwslt",
    "isozaki12_iwslt"
   ]
  },
  {
   "title": "The IWSLT 2012 Evaluation Campaign",
   "papers": [
    "federico12_iwslt",
    "yamamoto12_iwslt",
    "mediani12_iwslt",
    "hasler12_iwslt",
    "neubig12_iwslt",
    "ruiz12_iwslt",
    "peitz12_iwslt",
    "zhu12_iwslt",
    "falavigna12_iwslt",
    "saam12_iwslt",
    "heck12_iwslt",
    "chu12_iwslt",
    "besacier12_iwslt",
    "drexler12_iwslt",
    "shimizu12_iwslt",
    "finch12_iwslt",
    "marasek12_iwslt",
    "na12_iwslt",
    "dumitrescu12_iwslt",
    "mermer12_iwslt"
   ]
  },
  {
   "title": "Technical Papers",
   "papers": [
    "prasad12_iwslt",
    "kano12_iwslt",
    "niehues12_iwslt",
    "falavigna12b_iwslt",
    "koehn12_iwslt",
    "aransa12_iwslt",
    "mansour12_iwslt",
    "axelrod12_iwslt",
    "tu12_iwslt",
    "kolkhorst12_iwslt",
    "wu12_iwslt",
    "blain12_iwslt",
    "khadivi12_iwslt",
    "ruiz12b_iwslt",
    "cho12_iwslt",
    "feng12_iwslt",
    "hasler12b_iwslt",
    "peitz12b_iwslt",
    "potet12_iwslt",
    "gong12_iwslt"
   ]
  }
 ]
}