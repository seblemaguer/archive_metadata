{
 "title": "Speech and Language Technology in Education (SLaTE 2013)",
 "location": "Grenoble, France",
 "startDate": "30/8/2013",
 "endDate": "1/9/2013",
 "conf": "SLaTE",
 "year": "2013",
 "name": "slate_2013",
 "series": "SLaTE",
 "SIG": "SLaTE",
 "title1": "Speech and Language Technology in Education",
 "title2": "(SLaTE 2013)",
 "date": "30 August - 1 September 2013",
 "booklet": "slate_2013.pdf",
 "papers": {
  "litman13_slate": {
   "authors": [
    [
     "Diane J.",
     "Litman"
    ]
   ],
   "title": "Enhancing the effectiveness of spoken dialogue for STEM education",
   "original": "sl13_013",
   "page_count": 3,
   "order": 1,
   "p1": "13",
   "pn": "15",
   "abstract": [
    "This talk will illustrate some of the opportunities and challenges in applying speech and language processing to two types of STEM (Science, Technology, Engineering, and Mathematics) dialogue applications: 1) one-on-one physics tutoring, where students engage in dialogues with either a computer or human tutor, and 2) engineering design, where students engage in multi-party dialogue to complete a group project. I will first present results illustrating that relationships exists between student learning and both student affect, as well as lexical/prosodic entrainment between conversational partners. I will then illustrate our use of such findings to build better educational dialogue systems.\n",
    "Index Terms: spoken dialogue systems, intelligent tutoring, multi-party dialogue, affective systems, lexical and prosodic entrainment\n",
    ""
   ],
   "doi": "10.21437/SLaTE.2013-1"
  },
  "colpaert13_slate": {
   "authors": [
    [
     "Jozef",
     "Colpaert"
    ]
   ],
   "title": "The role and shape of speech technologies in well-designed language learning environments",
   "original": "sl13_016",
   "page_count": 4,
   "order": 2,
   "p1": "16",
   "pn": "19",
   "abstract": [
    "No technology carries an inherent, direct, measurable and generalizable effect on learning. Nor does speech technology. Its assumed added value is not a starting point for design, but a hypothesis resulting from design. Role and shape of speech technologies are simply logical and natural consequences of the specification of an optimal language learning environment for a specific context. This ecological paradigm shift will be illustrated by a discussion of the design concept behind a project on pronunciation training. Challenges for CALL research on this topic will be briefly discussed. This presentation will conclude with some considerations on Educational Engineering and how this approach can be recognized as a novel research method.\n",
    "Index Terms: pronunciation training, courseware design, learning environments\n",
    ""
   ],
   "doi": "10.21437/SLaTE.2013-2"
  },
  "beckman13_slate": {
   "authors": [
    [
     "Mary E.",
     "Beckman"
    ]
   ],
   "title": "Enriched technology-enabled annotation and analyses of child speech",
   "original": "sl13_020",
   "page_count": 4,
   "order": 3,
   "p1": "20",
   "pn": "23",
   "abstract": [
    "This talk will review a range of studies illustrating the ways in which speech technology has enabled richer analyses of corpora of children's speech and non-speech vocalizations. One set of studies involves VLAM, an articulatory synthesis system that models the transfer functions of infant- and child-proportioned vocal tracts. VLAM has been used to evaluate cross-language differences in the early emergence of contrasting vowel categories. Another set of studies involves the paidologos corpus of utterances elicited in a picture-prompted word-repetition task from 2- through 5-year-old child speakers of a variety of languages. Tools for tagging the corpus on multiple tiers allow researchers to extract target sounds for spectral analysis as well as for calculating transcribed accuracy rates. The tag sets also can be used to extract stimuli for perception experiments, yielding naive-listener responses that can then become another layer of tags.\n",
    "Index Terms: child speech corpora, articulatory synthesis, perceptual evaluation of child productions\n",
    ""
   ],
   "doi": "10.21437/SLaTE.2013-3"
  },
  "bernstein13_slate": {
   "authors": [
    [
     "Jared",
     "Bernstein"
    ],
    [
     "Ognjen",
     "Todic"
    ],
    [
     "Kayla",
     "Neumeyer"
    ],
    [
     "Katharyn",
     "Schultz"
    ],
    [
     "Liang",
     "Zhao"
    ]
   ],
   "title": "Young children's performance on self-administered ipad language activities",
   "original": "sl13_024",
   "page_count": 2,
   "order": 4,
   "p1": "24",
   "pn": "25",
   "abstract": [
    "Twenty-nine different English language activities were implemented on a touchtablet computer. Some activities were focused on a single skill (e.g. reading or speaking), while others involved several integrated skills (e.g. listening and writing). Materials were presented in several modalities, including speech only, speech with figure, silent video, text, speech with text, and speech with text and figure. Response modalities included speech, typing, touch, dragging screen objects, selecting and/or arranging words, and drawing figures. Various Test-like sequences of 24 to 45 activities were presented to 784 children, 53% from non-English speaking homes. Analysis of over 28,000 responses to self-administered activities indicates that most activities can be successfully modeled by a single short video example. By age 7 or 8 years, nearly all children will respond meaningfully to about 95% of these specific activities. Examples of these activities and child responses are presented.\n",
    "Index Terms: Language, assessment, touch tablet, four skills, ELL, children, modeling, modality.\n",
    ""
   ]
  },
  "claus13_slate": {
   "authors": [
    [
     "Felix",
     "Claus"
    ],
    [
     "Hamurabi",
     "Gamboa Rosales"
    ],
    [
     "Rico",
     "Petrick"
    ],
    [
     "Horst-Udo",
     "Hain"
    ],
    [
     "Rüdiger",
     "Hoffmann"
    ]
   ],
   "title": "A survey about ASR for children",
   "original": "sl13_026",
   "page_count": 5,
   "order": 5,
   "p1": "26",
   "pn": "30",
   "abstract": [
    "This paper is intended to survey the state of the art of automatic speech recognition (ASR) for children's speech. Investigating ASR for children is a current trend in research. Therefore databases of children's speech are needed for training and testing of ASR systems. In the first part of this paper the most relevant databases of children's speech are described. There are less speech data of children available than of adults and speech of preschool children is even more rarely available.   In the second part of this paper the common techniques for recognizing children's speech are summarized. Most investigations about children's ASR focus on the acoustic model. The common methods are described and approaches regarding the lexical and speech model are mentioned subsequently.   In an extensive literature research we collected papers investigating ASR for children. Several studies have been carried out investigating children's ASR. Due to the lack of data from preschool children only a few investigations for this age group have been accomplished. This is illustrated by presenting a statistic on the age of the children in past studies.\n",
    "Index Terms: children's speech, preschool children's speech, ASR for children, child computer interaction, statistics on children's speech, children's speech corpora\n",
    ""
   ],
   "doi": "10.21437/SLaTE.2013-4"
  },
  "hamalainen13_slate": {
   "authors": [
    [
     "Annika",
     "Hämäläinen"
    ],
    [
     "Fernando Miguel",
     "Pinto"
    ],
    [
     "Silvia",
     "Rodrigues"
    ],
    [
     "Ana",
     "Júdice"
    ],
    [
     "Sandra Morgado",
     "Silva"
    ],
    [
     "António",
     "Calado"
    ],
    [
     "Miguel Sales",
     "Dias"
    ]
   ],
   "title": "A multimodal educational game for 3-10-year-old children: collecting and automatically recognising European Portuguese children’s speech",
   "original": "sl13_031",
   "page_count": 6,
   "order": 6,
   "p1": "31",
   "pn": "36",
   "abstract": [
    "Speech interfaces have tremendous potential in education. In this paper, we present our work in the Contents for Next Generation Networks project, an ongoing Portuguese industry-academia collaboration developing a multimodal educational game aimed at improving the physical coordination and the basic mathematical and musical skills of 3-10- year-old children. We focus on our work in the area of children's speech recognition: designing, collecting, transcribing and annotating a 21-hour corpus of prompted European Portuguese children's speech, as well as our first experiments with different acoustic modelling approaches. Our speech recognition results suggest that training children's speech models from scratch is a more promising approach than retraining adult speech models using children's speech when a sufficient amount of training data is available from the targeted age group. This finding also holds for adult female speech models retrained using children's speech. As compared with a baseline recogniser comprising gender-dependent adult speech models, the best-performing children's speech models that we have trained so far – genderindependent cross-word triphones trained with 17.5 hours of speech from 3-10-year-old children – resulted in a 45-percent (relative) decrease in word error rate in a task expecting isolated cardinal numbers, sequences of cardinal numbers or musical notes as speech input.\n",
    "Index Terms: acoustic modelling, ASR, child-computer interaction, corpus, educational game, European Portuguese\n",
    ""
   ],
   "doi": "10.21437/SLaTE.2013-5"
  },
  "su13_slate": {
   "authors": [
    [
     "Pei-hao",
     "Su"
    ],
    [
     "Tien-han",
     "Yu"
    ],
    [
     "Ya-Yunn",
     "Su"
    ],
    [
     "Lin-shan",
     "Lee"
    ]
   ],
   "title": "A cloud-based personalized recursive dialogue game system for computer-assisted language learning",
   "original": "sl13_037",
   "page_count": 6,
   "order": 7,
   "p1": "37",
   "pn": "42",
   "abstract": [
    "In this paper we present the design and experimental results of a cloud-based personalized recursive dialogue game system for computer-assisted language learning. A number of tree-structured sub-dialogues are used sequentially and recursively as the script for the game. The dialogue policy at each dialogue turn is optimized to offer the most appropriate training sentence for every individual learner considering the learning status, such that the learner can have the scores for all selected pronunciation units exceeding a pre-defined threshold in minimum number of turns. The policy is modeled as a Markov Decision Process (MDP) with high-dimensional continuous state space and trained with a huge number of simulated learners generated from a corpus of real learner data. A real cloud-based system is implemented and the experimental results demonstrate promising outcomes.\n",
    "Index Terms: Computer-Assisted Language Learning, Dialogue Game, Continuous State Markov Decision Process, Fitted Value Iteration, Gaussian Mixture Model\n",
    ""
   ],
   "doi": "10.21437/SLaTE.2013-6"
  },
  "davis13_slate": {
   "authors": [
    [
     "Elizabeth M.",
     "Davis"
    ],
    [
     "Oscar",
     "Saz"
    ],
    [
     "Maxine",
     "Eskenazi"
    ]
   ],
   "title": "POLLI: a handheld-based aid for non-native student presentations",
   "original": "sl13_043",
   "page_count": 5,
   "order": 8,
   "p1": "43",
   "pn": "47",
   "abstract": [
    "Language learning is finally leaving the classroom for the real world. This usually takes the form of a dedicated implementation on a handheld device. In this paper we describe POLLI, an app that helps non-native students prepare scientific presentations.\n",
    "Index Terms: handheld applications, pronunciation skills\n",
    ""
   ],
   "doi": "10.21437/SLaTE.2013-7"
  },
  "strik13_slate": {
   "authors": [
    [
     "Helmer",
     "Strik"
    ],
    [
     "Polina",
     "Drozdova"
    ],
    [
     "Catia",
     "Cucchiarini"
    ]
   ],
   "title": "GOBL: games online for basic language learning",
   "original": "sl13_048",
   "page_count": 6,
   "order": 9,
   "p1": "48",
   "pn": "53",
   "abstract": [
    "In the GOBL project we develop and test small web-based mini games for loweducated and disadvantaged beginning learners of Dutch, English, and French. An innovative aspect of this project is that we incorporate language and speech technology to practice speaking skills. The present paper explains the notion of mini games employed in the project and the advantages of their use with respect to the target group. We then present the first results of the project concerning pedagogical and user requirements, based on the literature and user-based research. We then introduce our plans for the immediate future.\n",
    "Index Terms: serious gaming, speech and language technology, second language acquisition\n",
    ""
   ],
   "doi": "10.21437/SLaTE.2013-8"
  },
  "cai13_slate": {
   "authors": [
    [
     "Carrie J.",
     "Cai"
    ],
    [
     "Robert C.",
     "Miller"
    ],
    [
     "Stephanie",
     "Seneff"
    ]
   ],
   "title": "Enhancing speech recognition in fast-paced educational games using contextual cues",
   "original": "sl13_054",
   "page_count": 6,
   "order": 10,
   "p1": "54",
   "pn": "59",
   "abstract": [
    "Arcade-style games like Tetris and Pacman are often difficult to adapt for educational purposes because their fast-paced intensity and keystroke-heavy nature leave little room for simultaneous practice of other skills. Incorporating spoken language technology could make it possible for players to learn as they play, keeping up with game speed through multimodal interaction. To date, however, it remains exceedingly difficult to augment fastpaced games with speech interaction because the frustrating effect of recognition errors highly compromises entertainment. In this paper, we design a modified version of Tetris with speech recognition to help students practice and remember word-picture mappings. Using utterances collected from learners interacting with the speech-enabled Tetris game, we present and evaluate several techniques for leveraging contextual cues to increase recognition accuracy in fast-paced game environments.\n",
    "Index Terms: speech recognition, education, serious games, user interfaces\n",
    ""
   ],
   "doi": "10.21437/SLaTE.2013-9"
  },
  "vries13_slate": {
   "authors": [
    [
     "Bart Penning de",
     "Vries"
    ],
    [
     "Stephen",
     "Bodnar"
    ],
    [
     "Catia",
     "Cucchiarini"
    ],
    [
     "Helmer",
     "Strik"
    ],
    [
     "Roeland van",
     "Hout"
    ]
   ],
   "title": "Spoken grammar practice in an ASR-based CALL system",
   "original": "sl13_060",
   "page_count": 6,
   "order": 11,
   "p1": "60",
   "pn": "65",
   "abstract": [
    "In this paper we present a computer assisted language learning (CALL) system that is developed to practice grammar in spoken language. To enable this, the system uses Automatic Speech Recognition (ASR) to process the L2 learner's responses. We investigate the possibility of providing corrective feedback (CF) on learner errors, and compare that with self-monitored language learning through output practice. In this paper we present the comparison of the two conditions 1) one group of learners received oral practice and CF on spoken performance, and 2) the other group received oral practice and no CF on spoken performance. We found that our system is successful for L2 speaking practice. The main finding is that both groups show learning after treatment. Between the groups, we did not find a learning difference, but the groups' sessions proceeded differently. Additionally we found that the CF group was more positive about the system than the NO CF group.\n",
    "Index Terms: second language acquisition, corrective feedback, speech recognition, CALL\n",
    ""
   ],
   "doi": "10.21437/SLaTE.2013-10"
  },
  "bodnar13_slate": {
   "authors": [
    [
     "Stephen",
     "Bodnar"
    ],
    [
     "Bart Penning de",
     "Vries"
    ],
    [
     "Catia",
     "Cucchiarini"
    ],
    [
     "Helmer",
     "Strik"
    ],
    [
     "Roeland van",
     "Hout"
    ]
   ],
   "title": "Learners' situated motivation in oral grammar practice with an ASR-enabled CALL system",
   "original": "sl13_066",
   "page_count": 6,
   "order": 12,
   "p1": "66",
   "pn": "71",
   "abstract": [
    "Advances in speech recognition (ASR) technology have resulted in computer applications that provide compelling forms of speaking practice to learners of a second language (L2). Evaluation of such applications typically does not include an analysis of how learners' situated motivational states fluctuate during language practice. In connection with recent developments in L2 motivation theory, this paper investigates situated learner motivation in practice with an ASR-enabled system. An experiment was conducted in which our system provided oral grammar practice for Dutch L2 under two learning conditions: with and without corrective feedback. We report on learners' motivational experiences by triangulating an analysis of 1) motivational trajectories from a periodic motivation questionnaire, 2) post-practice reflective questionnaires, and 3) behavioural log data recorded by the system during practice. Our analysis shows that learners maintained positive attitudes towards the system throughout practice and became increasingly confident over time.\n",
    "Index Terms: CALL, motivation, ASR, corrective feedback, grammar\n",
    ""
   ],
   "doi": "10.21437/SLaTE.2013-11"
  },
  "lee13_slate": {
   "authors": [
    [
     "Kyusong",
     "Lee"
    ],
    [
     "Soo-Ok",
     "Kweon"
    ],
    [
     "Hae-Ri",
     "Kim"
    ],
    [
     "Gary Geunbae",
     "Lee"
    ]
   ],
   "title": "Filtering-based automatic cloze test generation",
   "original": "sl13_072",
   "page_count": 5,
   "order": 13,
   "p1": "72",
   "pn": "76",
   "abstract": [
    "We propose a method to generate high-quality cloze test questions using a computational approach. Previous methods for automatic cloze test generation have contained some problems; specifically, there can be multiple correct answers. We found that approximately 50% of the generated answers have such errors with previous methods, which requires human post-editing was necessary in previous research. We propose an N-gram filtering method that can detect the answer to a given question. We compare the errors of the generated questions before and after applying the filtering methods. We found that our filtering method can select quality distractors by reducing errors in the generated questions. Moreover, when we generate cloze tests using semantic similarity, non-native speakers are very hard to answer the questions.\n",
    "Index Terms: Cloze Test Generation, Sentence Completion Task, Vocabulary Question Generation\n",
    ""
   ],
   "doi": "10.21437/SLaTE.2013-12"
  },
  "rayner13_slate": {
   "authors": [
    [
     "Manny",
     "Rayner"
    ],
    [
     "Nikos",
     "Tsourakis"
    ]
   ],
   "title": "Methodological issues in evaluating a spoken CALL game: can crowdsourcing help us perform controlled experiments?",
   "original": "sl13_077",
   "page_count": 6,
   "order": 14,
   "p1": "77",
   "pn": "82",
   "abstract": [
    "We summarise a series of experiments we have carried out over the last three years on CALL-SLT, a speech-enabled web-based CALL game for learning and improving fluency in domain language, focussing on the methodological aspects. In particular, we argue that our previous evaluations have been systematically flawed due to the lack of a control group. We present a detailed description of our most recent evaluation, where 130 subjects, recruited using crowdsourcing methods, followed a short course in basic French over a period of one week, with 24 subjects completing the course. About a third of the subjects (half of the ones that finished) were assigned to a control group who used a version of the system with speech recognition feedback disabled; subjects in both groups demonstrated significant improvements in language skills over the duration of the experiment, but the improvements were significantly larger for the non-control subjects. We argue in conclusion that this type of experiment opens up interesting new ways to attack the difficult problem of performing controlled experiments with CALL applications.\n",
    "Index Terms: CALL, speech recognition, evaluation, methodology, crowdsourcing\n",
    ""
   ],
   "doi": "10.21437/SLaTE.2013-13"
  },
  "bang13_slate": {
   "authors": [
    [
     "Jeesoo",
     "Bang"
    ],
    [
     "Sechun",
     "Kang"
    ],
    [
     "Gary Geunbae",
     "Lee"
    ]
   ],
   "title": "An automatic feedback system for English speaking integrating pronunciation and prosody assessments",
   "original": "sl13_083",
   "page_count": 7,
   "order": 15,
   "p1": "83",
   "pn": "89",
   "abstract": [
    "We have proposed a computer-assisted language learning (CALL) system, called Postech English Speaking Assessment and Assistant (PESAA) for non-native English learners, especially Koreans, to improve their overall language skills. PESAA is an automatic feedback system for speaking English that integrates both pronunciation and prosody assessments. The system has three error-feedback modules: the pronunciation, rhythm, and phrase break error-feedback modules. PESAA generates scores on each of three modules as well as a combined score. The pronunciation assessment gives feedback that is based on comparing canonical and actual phoneme alignment results. The rhythm and phrase break assessments give feedback that is based on comparing predictions with detected results. English learners can use PESAA to practice pronunciations, rhythm and phrase breaks by themselves. We evaluated PESAA in three different ways: accuracy, correlation of the system's assessment with human assessments, and user satisfaction from the expected learning effectiveness and user interface. The evaluation showed that PESAA could work as a CALL system well, with good accuracy and positive learning results for the users.\n",
    "Index Terms: computer-assisted language learning, CALL, language assessment, error feedback\n",
    ""
   ],
   "doi": "10.21437/SLaTE.2013-14"
  },
  "miyakoda13_slate": {
   "authors": [
    [
     "Haruko",
     "Miyakoda"
    ]
   ],
   "title": "Visual approach to speech sounds",
   "original": "sl13_090",
   "page_count": 4,
   "order": 16,
   "p1": "90",
   "pn": "93",
   "abstract": [
    "Many people often struggle to master the pronunciation of foreign languages without much success. One of the reasons why L2 learners are not successful is because teaching pronunciation in the classroom is usually marginalized. With the advent of computers, this problem may partially have been overcome, due to the fact that many different types of systems and software for autonomous learning have been developed, allowing learners to improve their pronunciation skills outside the classroom. However, there are few, if any, systems and software that can present a form of visual feedback that allows learners to actually understand what their problems are.   In this paper, we present the auditory-visual pronunciation system that we have developed. One of the key features of this system is that it employs easy-to-understand visuals of the speech organ that can be seen from different angles. In addition, the internal organs can also be presented by changing the mode to transparent. Furthermore, movement of the speech organs can freely be adjusted by the instructors so that the learner's movements (especially the deviant) can be highlighted by comparing them with those of the model samples.\n",
    "Index Terms: pronunciation system, speech organs, visual feedback\n",
    ""
   ],
   "doi": "10.21437/SLaTE.2013-15"
  },
  "hirano13_slate": {
   "authors": [
    [
     "Hiroko",
     "Hirano"
    ],
    [
     "Ibuki",
     "Nakamura"
    ],
    [
     "Nobuaki",
     "Minematsu"
    ],
    [
     "Masayuki",
     "Suzuki"
    ],
    [
     "Chieko",
     "Nakagawa"
    ],
    [
     "Noriko",
     "Nakamura"
    ],
    [
     "Yukinori",
     "Tagawa"
    ],
    [
     "Keikichi",
     "Hirose"
    ],
    [
     "Hiroya",
     "Hashimoto"
    ]
   ],
   "title": "OJAD: a free online accent and intonation dictionary for teachers and learners of Japanese",
   "original": "sl13_094",
   "page_count": 1,
   "order": 17,
   "p1": "94",
   "pn": "",
   "abstract": [
    "We developed the very first online and free framework for teaching and learning Japanese prosody including word accent and phrase intonation. This framework is called OJAD (Online Japanese Accent Dictionary), which provides three functions. Subjective assessment by teachers shows very high pedagogical effectiveness of the framework.\n",
    "Index Terms: language education, Japanese prosody, accent sandhi, OJAD, TTS synthesizer, assessment experiment\n",
    ""
   ]
  },
  "delmonte13_slate": {
   "authors": [
    [
     "Rodolfo",
     "Delmonte"
    ],
    [
     "Ciprian",
     "Bacalu"
    ]
   ],
   "title": "SPARSAR: a system for Poetry Automatic Rhythm and Style AnalyzeR",
   "original": "sl13_095",
   "page_count": 1,
   "order": 18,
   "p1": "95",
   "pn": "",
   "abstract": [
    "Any poem can be characterized by its rhythm which is also revealing of the poet's peculiar style. In turn, the poem's rhythm is based mainly on two elements: meter, that is distribution of stressed and unstressed syllables in the verse, presence of rhyming and other poetic devices like alliteration, assonance, consonance, enjambements, etc. which contribute to poetic form at stanza level.   Traditionally, poetic meter is visualized by a sequence of signs, typically a straight line is used to indicate vowels of stressed syllabes and a half circle is positioned on vowels of unstressed ones. We also agree with this view, however, we would like to be more specific on the notion of rhythm that we intend to purport.   In our view, a prosodic acoustic view needs to be implemented as well, if any precise definition of rhythm and style is the goal. Syllables are not just any combination of sounds, and their internal structure is fundamental to the nature of the poetic rhythm that will ensue. This is partly amenable to the use and exploitation of poetic devices, which we also intend to highlight in our system. But what is paramount in our description of rhythm, is the use of the acoustic parameter of duration. In our demo we will show haw poems can be chacterized by the use of rhythmic and stylistic features in a very revelatory manner, by comparing metrically similar poems of the same poet.\n",
    ""
   ]
  },
  "cucchiarini13_slate": {
   "authors": [
    [
     "Catia",
     "Cucchiarini"
    ],
    [
     "Ineke van de",
     "Craats"
    ],
    [
     "Jan",
     "Deutekom"
    ],
    [
     "Helmer",
     "Strik"
    ]
   ],
   "title": "The digital instructor for literacy learning",
   "original": "sl13_096",
   "page_count": 6,
   "order": 19,
   "p1": "96",
   "pn": "101",
   "abstract": [
    "The DigLIn project aims at providing concrete solutions for adult literacy students by developing and testing L2 literacy acquisition material in four different languages and by employing Automatic Speech Recognition (ASR) to analyse the learner's read speech output and provide feedback. We develop the technology and design sample exercises for different languages (Dutch, English, German and Finnish) and test them in literacy classes in adult education centres with adult L2 learners.   Existing language learning material for loweducated second language learners developed at Friesland College (the digital sources of the FC Sprint2) is augmented with an ASR module capable of recognizing what the learners say, of diagnosing possible errors in reading aloud or pronunciation and of providing practice and feedback in learning to read aloud in the L2.\n",
    "Index Terms: adult literacy learning, language and speech technology, second language acquisition\n",
    ""
   ],
   "doi": "10.21437/SLaTE.2013-16"
  },
  "vries13b_slate": {
   "authors": [
    [
     "Nic J. de",
     "Vries"
    ],
    [
     "Febe de",
     "Wet"
    ]
   ],
   "title": "Off-line mobile-assisted vocabulary training for the developing world",
   "original": "sl13_102",
   "page_count": 2,
   "order": 20,
   "p1": "102",
   "pn": "103",
   "abstract": [
    "Mobile-assisted language learning applications (MALL) has significant potential in the developing world where access to teachers and classrooms are real barriers to learning.   This demonstration of an Android-based mobile language learning application is used to teach vocabulary and employs off-line Automatic Speech Recognition (ASR) and Text-to-speech (TTS) technologies with custom-built language and acoustic models, incorporating the key design criteria outlined in the article.\n",
    "Index Terms: Computer-assisted language learning (CALL), Mobile-assisted language learning (MALL), Automatic Speech Recognition (ASR), Text-to-speech (TTS) synthesis, Android, on-device ASR and TTS, game-based learning, developing world\n",
    ""
   ]
  },
  "su13b_slate": {
   "authors": [
    [
     "Pei-hao",
     "Su"
    ],
    [
     "Tien-han",
     "Yu"
    ],
    [
     "Ya-Yunn",
     "Su"
    ],
    [
     "Lin-shan",
     "Lee"
    ]
   ],
   "title": "NTU Chinese 2.0: a personalized recursive dialogue game for computer-assisted learning of Mandarin Chinese",
   "original": "sl13_104",
   "page_count": 1,
   "order": 21,
   "p1": "104",
   "pn": "",
   "abstract": [
    "We present and demonstrate a cloud-based personalized dialogue game for computer-assisted learning of Mandarin Chinese. A sequence of tree-structured sub-dialogues in restaurant scenario are linked recursively and used as the script for the game. Based on NTU Chinese, a Mandarin Chinese pronunciation evaluation software (http://chinese.ntu.edu.tw/), the user can get immediate evaluation on pronunciation, pitch, timing and emphasis and corresponding corrective feedback on each syllable as well as on sentence level for each utterance produced. The system policy is optimized to offer personalized dialogue path planning for each individual learner such that more practice opportunities are given along the dialogue path to poorly produced pronunciation units. When using the system, the learner can practice the sub-dialogues in either sequential or random order; at each dialogue turn, the learner also can choose to pronounce an arbitrary candidate sentence or following the recommended sentences by the system policy. Following the system recommendation along the sub-dialogues sequentially offers the fastest learning though. The above evaluation and learning records are displayed and stored in personal profile.   The system framework is modeled as a Markov Decision Process (MDP) with highdimensional continuous state space considering the learning status of the learner. The dialogue policy is trained using a huge number of simulated learners generated from a corpus recorded by 278 real Mandarin Chinese learners from 36 countries with various mother tongues. The detailed principles of this system are presented in a companion paper also submitted to SLaTe 2013 [1]. This is a joint work with the International Chinese Language Program of National Taiwan University.\n",
    "",
    "",
    "P.-H. Su, T.-H. Yu, Y.-Y. Su, and L.-S. Lee, “A cloud-based personalized recursive dialogue game system for computer-assisted language learning,” this volume.\n",
    ""
   ]
  },
  "harbusch13_slate": {
   "authors": [
    [
     "Karin",
     "Harbusch"
    ],
    [
     "Johannes",
     "Härtel"
    ],
    [
     "Christel-Joy",
     "Cameran"
    ]
   ],
   "title": "COMPASS III: teaching L2 grammar graphically on a tablet computer",
   "original": "sl13_105",
   "page_count": 1,
   "order": 22,
   "p1": "105",
   "pn": "",
   "abstract": [
    "We demonstrate a prototype of the tablet-based L2 grammar teaching system COMPASS III. COMPASS stands for COMbina-torial and Paraphrastic Assembly of Sentence Structure; for a de-scription of the underlying computational-linguistic software, see [1]. COMPASS invites the student to construct sentences by composing syntactic trees out of lexically anchored “treelets” via the graphical drag & drop user interface provided by tablet and touchscreen. After each move (i.e. each attempt to combine two treelets, or to reorder a branch), the system's natural-language generator computes all possible grammatically wellformed sentences entailed by the attempted tree. COMPASS provides positive feedback if the student-composed tree belongs to the well-formed set, and negative feedback otherwise. In the latter case, COMPASS may propose alternatives based on a comparison between the student-composed tree and its own well-formed trees (informative feedback on demand). As system feedback may explicitly refer to grammar rules, the learner needs to have elementary syntactic knowledge. COMPASS III targets L2 learners of German with high-school level understanding of word classes and grammatical functions. The user interface allows the student to select words and to move (parts of) trees around through finger or stylus gestures. No typing is required. COMPASS III focuses on word order and case morphology—difficult topics in L2 German.   The grammar formalism underlying COMPASS is Performance Grammar, which assumes separate rules for the hierarchical structures of a sentence and the linear order of its constituents. This split allows the student to break sentence construction exercises into relatively small parts. For instance, the learner can select a word and inflect it according to the intended grammatical function without having to worry about the linear position of the constituent in the sentence under construction. At any time during this “scaffolded” sentence construction process, the tree built so far remains visible on the screen, ready to be expanded by attach-ing additional words/treelets; any earlier decision can be undone and corrected.\n",
    "",
    "",
    "Harbusch, K. and Kempen, G. “Automatic online writing support for L2 learners of German through output monitoring by a natural-language paraphrase generator”, in M. Levy, Blin, F., Bradin Siskin, C. and Takeuchi, O. [Eds], WORLDCALL, New York: Routledge, 2011, pp. 128-143.\n",
    ""
   ]
  },
  "ahmed13_slate": {
   "authors": [
    [
     "Imran",
     "Ahmed"
    ],
    [
     "Meghna",
     "Pandharipande"
    ],
    [
     "Sunil Kumar",
     "Kopparapu"
    ]
   ],
   "title": "A suite of mobile applications to assist speaking at right speed",
   "original": "sl13_106",
   "page_count": 3,
   "order": 23,
   "p1": "106",
   "pn": "108",
   "abstract": [
    "One of the prominent reason for ineffective communication in call center telephone conversations is primarily due to the manner in which the voice agents speak and not necessarily due to what they speak. Speaking rate, a non-linguistic aspect of speech, is a critical factor affecting intelligibility and comprehension of speech in general and specifically in call center telephone conversations. There have been attempts to monitor speaking rate of agents in a call center setup. However, there has been a paradigm shift to the conventional call centers with companies opting for home-agents. In this model the agents, unlike in the current call center setup, can operate virtually from anywhere using their mobile phones. In this paper, we present SpeakRite - a suite of mobile applications that can assist home-agents. While one of the components of SpeakRite analyzes the speaking rate during an ongoing telephone conversation and provides a real time feedback to assist the speaker modify his speaking rate, there are several other components that allow the home-agent to assess their speaking rate and learn to speak right themselves.\n",
    "Index Terms: speaking rate, speech rate, mobile application, home-agents\n",
    ""
   ],
   "doi": "10.21437/SLaTE.2013-17"
  },
  "pongkittiphan13_slate": {
   "authors": [
    [
     "Teeraphon",
     "Pongkittiphan"
    ],
    [
     "Nobuaki",
     "Minematsu"
    ],
    [
     "Takehiko",
     "Makino"
    ],
    [
     "Keikichi",
     "Hirose"
    ]
   ],
   "title": "Automatic detection of the words that will become unintelligible through Japanese accented pronunciation of English",
   "original": "sl13_109",
   "page_count": 3,
   "order": 24,
   "p1": "109",
   "pn": "111",
   "abstract": [
    "This study examines automatic detection of the words that will be unintelligible if they are spoken by Japanese speakers of English. In our previous study, 800 English utterances spoken by Japanese speakers, which contained 6,063 words, were presented to 173 American listeners and correct perception rate was obtained for each spoken word. By using the results, in this study, we define the words that will be very unintelligible through Japanese accented English pronunciation and also define the words that will be rather unintelligible. Then, by using Classification And Regression Tree (CART) with linguistic features and lexical features only, we examine automatic detection of these words. After that, we introduce an additional feature derived by considering phonological and phonotactic differences between Japanese and English. This additional feature is found to be very effective and our proposed method can detect very unintelligible words and rather unintelligible words automatically with F1-scores of 65.44 and 70.45 [%], respectively.\n",
    "Index Terms: speech intelligibility, second language learning, foreign accent, ERJ database, CART\n",
    ""
   ],
   "doi": "10.21437/SLaTE.2013-18"
  },
  "rasmussen13_slate": {
   "authors": [
    [
     "Morten Højfeldt",
     "Rasmussen"
    ],
    [
     "Zheng-Hua",
     "Tan"
    ]
   ],
   "title": "Fusing eye-gaze and speech recognition for tracking in an automatic reading tutor – a step in the right direction?",
   "original": "sl13_112",
   "page_count": 4,
   "order": 25,
   "p1": "112",
   "pn": "115",
   "abstract": [
    "In this paper we present a novel approach for automatically tracking the reading progress using a combination of eye-gaze tracking and speech recognition. The two are fused by first generating word probabilities based on eye-gaze information and then using these probabilities to augment the language model probabilities during speech recognition. Experimental results on a small dataset show that the tracking error rate of the system using only speech recognition is 37.9% whereas the tracking error rate for the system that incorporates eye-gaze tracking into the speech recognizer is 35.8%.\n",
    "Index Terms: automatic reading tutor, eye-gaze tracking, speech recognition\n",
    ""
   ],
   "doi": "10.21437/SLaTE.2013-19"
  },
  "rayner13b_slate": {
   "authors": [
    [
     "Manny",
     "Rayner"
    ],
    [
     "Nikos",
     "Tsourakis"
    ]
   ],
   "title": "A speech-based internet game for beginner students of English",
   "original": "sl13_201",
   "page_count": 0,
   "order": 26,
   "p1": "(abstract)",
   "pn": "",
   "abstract": [
    "We demonstrate a web-enabled serious game, based on the CALL-SLT platform, which is intended to help German-speaking beginner students of English improve their generative and auditory competence. Empirically, English is a language which Germanspeakers find easy to learn; the main challenge for the student is not so much grammar or phonology, but rather acquiring the experience needed to build up confidence. The game, designed to address this need, offers a short course of eight interactive lessons using a combined vocabulary of about 450 words. It was developed in collaboration with a secondary school teacher, with the content taken from a commonly used textbook. The system is gamified to increase student motivation, using common game elements such as badges.   The game is structured as a series of short dialogues between the student and the machine, where the student is encouraged to use simple language in practical contexts like booking a hotel room or ordering a meal in a restaurant. At each turn, the system starts by playing a short video file in English (“How may I help you?”), and simultaneously displays a piece of text in German indicating to the student how they should reply (frag: Zimmer für 2 Nächte). The student gives a spoken response; they can usually respond in several different ways to each prompt (“A room for 2 nights please”/ “Could I have a room for 2 nights” / “I would like a room for 2 nights” …). If the student is uncertain how to respond, they can ask for a help example, which is presented in both written and spoken form. After the student answers, the system performs speech recognition, machine translation and matching, and either accepts or rejects. Depending on the result and the defined lesson structure, it can either go to a new step or repeat the current one; there are multiple dialogue paths. The student gains badges by completing lessons and achieving a minimum score dependent on the difficulty of the badge.\n",
    ""
   ]
  },
  "hueber13_slate": {
   "authors": [
    [
     "Thomas",
     "Hueber"
    ],
    [
     "Gérard",
     "Bailly"
    ],
    [
     "Pierre",
     "Badin"
    ],
    [
     "Frédéric",
     "Elisei"
    ]
   ],
   "title": "Vizart3d – real-time system of visual articulatory feedback",
   "original": "sl13_202",
   "page_count": 0,
   "order": 27,
   "p1": "(abstract)",
   "pn": "",
   "abstract": [
    "We present recent developments on our system of visual articulatory feedback, named Vizartd3D, which is based on the 3D orofacial clone developed at GIPSA-lab. This system aims at providing any speaker with a feedback on his/her own articulation, especially by displaying the tongue movements. Application areas are computer-assisted pronunciation training (phonetic correction) for second-language learning and speech rehabilitation. In our system, the orofacial clone is animated in real-time from the speech audio signal, using a statistical mapping method based on multiple Gaussian mixture regressions (GMR). This approach combines in the same framework a spectral mapping step with an acousticarticulatory inversion step. All the modules of the system are implemented in the real-time environment Max/MSP (acoustic analysis, mapping and 3D rendering). More information is available at http://www.gipsa-lab.fr/projet/vizart3D/.\n",
    ""
   ]
  },
  "patil13_slate": {
   "authors": [
    [
     "Vaishali",
     "Patil"
    ],
    [
     "Preeti",
     "Rao"
    ]
   ],
   "title": "Automatic pronunciation feedback for phonemic aspiration",
   "original": "sl13_116",
   "page_count": 6,
   "order": 28,
   "p1": "116",
   "pn": "121",
   "abstract": [
    "The computer-assisted learning of spoken language is closely tied to automatic speech recognition (ASR) technology which, as is well known, is challenging with non-native speech. By focusing on specific phonological differences between the target and source languages of non-native speakers, pronunciation assessment can be made more reliable. Aspiration, an important phonemic attribute in plosives of Indo-Aryan languages such as Hindi, Marathi and Gujarati, is rarely found in the world's languages. The improper production of the aspiration contrast is thus often the most important cue to non-native accents of spoken Hindi. A system for the detection of phonemic aspiration in unvoiced and voiced stops based on discriminative acoustic features is shown to be effective for rating nonnative accents and providing reliable phoneme-level feedback.\n",
    "Index Terms: computer-assisted language learning, pronunciation scoring, non-native accent, phonemic aspiration\n",
    ""
   ],
   "doi": "10.21437/SLaTE.2013-20"
  },
  "lee13b_slate": {
   "authors": [
    [
     "Ann",
     "Lee"
    ],
    [
     "James",
     "Glass"
    ]
   ],
   "title": "Pronunciation assessment via a comparison-based system",
   "original": "sl13_122",
   "page_count": 5,
   "order": 29,
   "p1": "122",
   "pn": "126",
   "abstract": [
    "In this paper, we present preliminary results on applying a comparison-based framework to the task of pronunciation scoring. The comparison-based system works by aligning a student's utterance with a teacher's utterance via dynamic time warping (DTW). Features that describe the degree of mis-alignment are extracted from the aligned path and the distance matrix. We focus on a dataset in Levantine Arabic, a low-resource language for which there is not enough automatic speech recognition (ASR) capability available. Three different speech representations are investigated: MFCCs, Gaussian posteriorgrams, and English phoneme state posteriorgrams decoded on Levantine data. Experimental results show that the system can improve both correlation and mean squared error between machine predicted scores and human ratings compared to a template-based system.\n",
    "Index Terms: pronunciation scoring, dynamic time warping, posteriorgrams\n",
    ""
   ],
   "doi": "10.21437/SLaTE.2013-21"
  },
  "wang13_slate": {
   "authors": [
    [
     "Hao",
     "Wang"
    ],
    [
     "Xiaojun",
     "Qian"
    ],
    [
     "Helen",
     "Meng"
    ]
   ],
   "title": "Predicting gradation of L2 English mispronunciations using crowdsourced ratings and phonological rules",
   "original": "sl13_127",
   "page_count": 5,
   "order": 30,
   "p1": "127",
   "pn": "131",
   "abstract": [
    "Pedagogically, CAPT systems can be improved by giving effective feedback based on the severity of pronunciation errors. We obtained perceptual gradation of L2 English mispronunciations through crowdsourcing, and conducted quality control utilizing the WorkerRank algorithm to refine the collected results and reach a reliable consensus on the ratings of word mispronunciations. This paper presents our work on modeling the relationship between the phonetic mispronunciations and the actual word ratings. Based on phonological rules representing phonetic mispronunciation productions, we propose two approaches to predict the gradation of word mispronunciations. Reasonable correlation and agreement are found between the human-labeled and machine-predicted gradations for both approaches, which imply that the use of phonological rules in word-level mispronunciation gradation prediction is promising.\n",
    "Index Terms: CAPT, crowdsourcing, mispronunciation gradation\n",
    ""
   ],
   "doi": "10.21437/SLaTE.2013-22"
  },
  "bang13b_slate": {
   "authors": [
    [
     "Jeesoo",
     "Bang"
    ],
    [
     "Gary Geunbae",
     "Lee"
    ]
   ],
   "title": "Determining sentence pronunciation difficulty for non-native speakers",
   "original": "sl13_132",
   "page_count": 5,
   "order": 31,
   "p1": "132",
   "pn": "136",
   "abstract": [
    "This paper investigates the features that determine the sentence pronunciation difficulty for non-native speakers. We selected three types of features: length, word frequency, and phonemes that Korean speakers generally replace with other phonemes. Support vector machines and a multiple linear regression model were used to determine the pronunciation difficulty of given sentences, and the results were measured with a five-fold cross validation. We demonstrated that these features could determine sentence pronunciation difficulty with an accuracy and a correlation coefficient sufficient for computer-assisted pronunciation training (CAPT) systems. The combination of all three feature types had the highest accuracy and correlation coefficient in determining sentence pronunciation difficulty. For single features, the length-based feature type was the most accurate in determining sentence pronunciation difficulty. The phoneme-specific feature type also had high accuracy. Length, phoneme, and word features can be used to guide the automatic choice of sentences for CAPT systems that depend on users' proficiency levels.\n",
    "Index Terms: sentence level decision, pronunciation level, pronunciation difficulty feature, CAPT sentence level\n",
    ""
   ],
   "doi": "10.21437/SLaTE.2013-23"
  },
  "xiong13_slate": {
   "authors": [
    [
     "Wenting",
     "Xiong"
    ],
    [
     "Keelan",
     "Evanini"
    ],
    [
     "Klaus",
     "Zechner"
    ],
    [
     "Lei",
     "Chen"
    ]
   ],
   "title": "Automated content scoring of spoken responses containing multiple parts with factual information",
   "original": "sl13_137",
   "page_count": 6,
   "order": 32,
   "p1": "137",
   "pn": "142",
   "abstract": [
    "This paper presents approaches to automated content scoring of spoken language test responses from non-native speakers of English which contain multiple parts addressing factual information that the test taker has previously heard via auditory stimulus materials. While previous work relating to content scoring of spontaneous, unpredictable speech has focused only on entire responses and on general topic matching approaches, such as content vector analysis, the specific nature of spoken responses in our data requires response segmentation and extraction of features that indicate the relevance and correctness of the facts contained in the different parts of the response. Our best content features, based on similarity with key facts and concepts, achieve correlations of r=0.615 (for speech recognition output) and r=0.637 (using human transcriptions) with expert human rater scores. Furthermore, we show that these content features outperform traditional vector space based features. Finally, we demonstrate that the performance of a scoring model based on a combination of features developed previously and some of the newly designed content features improves significantly from r=0.624 to r=0.664 on an unseen evaluation set when using speech recognition output.\n",
    "Index Terms: spoken language assessment, automated scoring, content appropriateness\n",
    ""
   ],
   "doi": "10.21437/SLaTE.2013-24"
  },
  "rongna13_slate": {
   "authors": [
    [
     "A.",
     "Rongna"
    ],
    [
     "Ryoko",
     "Hayashi"
    ],
    [
     "Tatsuya",
     "Kitamura"
    ]
   ],
   "title": "Naturalness on Japanese pronunciation before and after shadowing training and prosody modified stimuli",
   "original": "sl13_143",
   "page_count": 4,
   "order": 33,
   "p1": "143",
   "pn": "146",
   "abstract": [
    "This study attempts to investigate the change of naturalness impression for the Japanese utterance by Japanese as foreign language learners (JFL) before and after shadowing and repeating trainings and to discuss the crucial prosodic cue for the naturalness judgment.   The speech of 8 JFL learners before and after pronunciation training was used, and their durational pattern, pitch pattern, or both of them were replaced with those of the model speech. 52 Japanese native speakers (JNS) assessed the naturalness of these stimuli. The results showed JNS judge the duration AND F0 modified stimuli most natural. In addition, the shadowing trained group tended to get higher scores than the repeating trained group after training. The acoustical analysis of speech material showed a difference of moraic structure and pitch accent between the shadowing and repeating group.\n",
    "Index Terms: Japanese as a foreign language learners, naturalness, shadowing, repeating, prosodic modified stimuli\n",
    ""
   ],
   "doi": "10.21437/SLaTE.2013-25"
  },
  "mixdorff13_slate": {
   "authors": [
    [
     "Hansjörg",
     "Mixdorff"
    ],
    [
     "Murray J.",
     "Munro"
    ]
   ],
   "title": "Quantifying and evaluating the impact of prosodic differences of foreign-accented English",
   "original": "sl13_147",
   "page_count": 6,
   "order": 34,
   "p1": "147",
   "pn": "152",
   "abstract": [
    "The identification and correction of prosodic deviations still poses a significant challenge for computer-aided language learning. With this ultimate goal in mind, the current study compares utterances by Cantonese speakers of Canadian English with those of native English subjects through both acoustic analysis and perceptual evaluation. We aim to find measurable prosodic differences accounting for the perceptual results. Our outcomes indicate, inter alia, that unstressed syllables are relatively longer compared to stressed ones in the Cantonese corpus than in the Canadian English corpus. Furthermore, the correlations of syllabic durations in utterances of one and the same sentence are much higher for Canadian English subjects than for Cantonese speakers. The latter use a similar range of F0, but produce more and longer pitch-accents than Canadian English speakers. In a perception study we found that applying native durations together with F0 contours to the foreign-accented speech led to significantly improved listener judgments of prosodic goodness. Adjustments to duration alone also tended to yield better ratings, though the effect was not statistically significant. When durations of native English utterances were adjusted to those of Cantonese speakers, significant decrements in ratings were observed.\n",
    "Index Terms: foreign accent, prosodic analysis, perception tests\n",
    ""
   ],
   "doi": "10.21437/SLaTE.2013-26"
  },
  "mixdorff13b_slate": {
   "authors": [
    [
     "Hansjörg",
     "Mixdorff"
    ],
    [
     "Hamurabi",
     "Gamboa Rosales"
    ]
   ],
   "title": "Prosodic chunking of German as a foreign language",
   "original": "sl13_153",
   "page_count": 6,
   "order": 35,
   "p1": "153",
   "pn": "158",
   "abstract": [
    "This study concerns the perception of boundaries and accented syllables by native German subjects as compared to foreign non-speakers and learners of the language at different proficiency levels. To this effect six-syllable sequences excised from a context of three poly-syllabic words of German were presented to participants who had to select the syllables they perceived as accented, as well as the locations of word boundaries. Results show that German native subjects perform well at the word boundary task, but mark correctly less than two thirds of accented syllables. Chinese and Mexican non-learners still detect a considerable number of word boundaries and accented syllables. Learners of German show improvement at the task with growing experience though they often pick legal subword units that do not necessarily form a plausible sequence. Correlation analysis of factors for syllable and boundary selection performed for non-learners and German subjects – as expected – shows considerably different behaviours. Whereas the boundary location does not influence the Germans' decision on the accent location, Chinese and Mexican non-learners show a preference to mark an accent when the syllable is followed by a word boundary. We also found that the acoustic properties of the syllables had a larger impact on the non-learners' decisions since they could not operate on linguistic knowledge of German.\n",
    "Index Terms: Prominence, accent and boundary perception, L2 learning\n",
    ""
   ],
   "doi": "10.21437/SLaTE.2013-27"
  },
  "lai13_slate": {
   "authors": [
    [
     "Catherine",
     "Lai"
    ],
    [
     "Keelan",
     "Evanini"
    ],
    [
     "Klaus",
     "Zechner"
    ]
   ],
   "title": "Applying rhythm metrics to non-native spontaneous speech",
   "original": "sl13_159",
   "page_count": 5,
   "order": 36,
   "p1": "159",
   "pn": "163",
   "abstract": [
    "This study investigates a variety of rhythm metrics on two corpora of non-native spontaneous speech and compares the non-native distributions to values from a corpus of native speech. Several of the metrics are shown to differentiate well between native and nonnative speakers and to also have moderate correlations with English proficiency scores that were assigned to the non-native speech. The metric that had the highest correlation with English proficiency scores (apart from speaking rate) was rPVIsyl (the raw Pairwise Variability Index for syllables), with r=-0.43.\n",
    "Index Terms: Rhythm metrics, non-native speech, fluency\n",
    ""
   ],
   "doi": "10.21437/SLaTE.2013-28"
  },
  "tseng13_slate": {
   "authors": [
    [
     "Chiu-yu",
     "Tseng"
    ],
    [
     "Chao-yu",
     "Su"
    ],
    [
     "Tanya",
     "Visceglia"
    ]
   ],
   "title": "Underdifferentiation of English lexical stress contrasts by L2 taiwan speakers",
   "original": "sl13_164",
   "page_count": 4,
   "order": 37,
   "p1": "164",
   "pn": "167",
   "abstract": [
    "Learning the stress patterns of English words presents a challenge for L1 speakers from syllable-timed and/or tone languages. Realization of stress contrasts in previous studies has been measured in a variety of ways. This study adapts and extends PVI, a method generally used to measure duration as a property of speech rhythm, to compare F0 and amplitude contrasts across L1 and L2 production of stressed and unstressed syllables in English multisyllabic words. L1 North American English and L1 Taiwan-Mandarin English speech data were extracted from the AESOP-ILAS corpus. Results of acoustic analysis show that overall, stress contrasts were realized most robustly by L1 English speakers. A general pattern of contrast underdifferentiation was found in L2 speakers with respect to F0, duration and intensity, with the most striking difference found in F0. These results corroborate our earlier findings on L1 Mandarin speakers' production of on-focus/post-focus contrasts in their realization of English narrow focus. Taken together, these results demonstrate that underdifferentiation of prosodic contrasts at both the lexical and phrase levels is a major prosodic feature of Taiwan English; future research will determine whether it can also be found in the L2 English of other syllable-timed or tone language speakers.\n",
    "Index Terms: L2 English, lexical stress, Taiwan Mandarin, underdifferentiation\n",
    ""
   ],
   "doi": "10.21437/SLaTE.2013-29"
  },
  "carranza13_slate": {
   "authors": [
    [
     "Mario",
     "Carranza"
    ]
   ],
   "title": "Intermediate phonetic realizations in a Japanese accented L2 Spanish corpus",
   "original": "sl13_168",
   "page_count": 4,
   "order": 38,
   "p1": "168",
   "pn": "171",
   "abstract": [
    "This paper addresses the issue of manual transcription of non native speech in an attempt to establish rule-based strategies for labelling intermediate realizations. The problems of transcribing non canonical realizations of L2 sounds which present shared features of the target (Spanish) and the source language (Japanese) will be considered. We introduce a Japanese accented non native L2 Spanish corpus, and exemplify the use of decision trees in manual transcriptions as a systematic method for dealing with ambiguous realizations. This approach could help a potential error detection system to detect both canonical and erroneous realizations, contributing to the development of CAPT tools.\n",
    "Index Terms: non native speech transcription, ASR, CAPT, L2 Spanish, L1 Japanese, non native spoken corpus\n",
    ""
   ],
   "doi": "10.21437/SLaTE.2013-30"
  },
  "koreman13_slate": {
   "authors": [
    [
     "Jacques",
     "Koreman"
    ],
    [
     "Preben",
     "Wik"
    ],
    [
     "Olaf",
     "Husby"
    ],
    [
     "Egil",
     "Albertsen"
    ]
   ],
   "title": "Universal contrastive analysis as a learning principle in CAPT",
   "original": "sl13_172",
   "page_count": 6,
   "order": 39,
   "p1": "172",
   "pn": "177",
   "abstract": [
    "Universal contrastive analysis as a learning principle in CAPT Cross-linguistic comparison is good starting point for CAPT. For segmental contrasts, the IPA categories with their claim to phonemic universality enable a structure which ensures communicative effectiveness for learners who manage to perceive and pronounce all phoneme categories in an L2. The CALST system implements contrastive analysis in two types of exercises for phonetic and abstract listening, where users can train the perception of sounds which are not part of the L1 inventory. Since substitutions for unfamiliar sounds depend on L1, the selection of sound contrasts which are trained in the exercises should also depend on L1. We shall argue for a pragmatic approach to the selection of exercises.\n",
    "Index Terms: computer-assisted pronunciation training, CAPT, CALST, sound contrasts, differential substitution\n",
    ""
   ],
   "doi": "10.21437/SLaTE.2013-31"
  },
  "short13_slate": {
   "authors": [
    [
     "Greg",
     "Short"
    ],
    [
     "Keikichi",
     "Hirose"
    ],
    [
     "Nobuaki",
     "Minematsu"
    ]
   ],
   "title": "Automatic recognition of vowel length in Japanese for a CALL system motivated by perceptual experiments",
   "original": "sl13_178",
   "page_count": 6,
   "order": 40,
   "p1": "178",
   "pn": "183",
   "abstract": [
    "Acquisition of the Japanese vowel length contrast can be problematic for nonnative speakers. For these speakers, a CALL system which can automatically recognize whether a particular vowel is being pronounced as long or short could be of great benefit for pointing out their errors and issuing corrective feedback. However, a method that can adequately do this has not been proposed yet. Vowel length recognition is made difficult because the vowel length distinction is dependent on the surrounding vowel durations. HMMs, the standard way of recognizing this distinction, do not make use of this information. Other methods have been proposed to recognize this, but they do not appear viable unless knowledge about the length of other vowels is present. Thus, to derive a method, we conduct perceptual experiments to analyze the mechanism of vowel length perception. From this analysis, we develop an automatic recognition algorithm for vowel length that uses support vector machines (SVMs). We tested this method on a speaking rate corpus, native speech, and non-native speech. The method produced recognition results that are overall superior to HMMs and also more robust against speaking rate differences with an average of a 0.83 correct recognition rate for 3 datasets. The error and non-error classification rates on nonnative speech for this are 0.86 and 0.84 respectively recognition rate for 3 datasets. The error and non-error classification rates on non-native speech for this are 0.86 and 0.84 respectively.\n",
    "Index Terms: Japanese, vowel length, recognition, speaking rate, perception, CALL\n",
    ""
   ],
   "doi": "10.21437/SLaTE.2013-32"
  },
  "shen13_slate": {
   "authors": [
    [
     "Han-Ping",
     "Shen"
    ],
    [
     "Nobuaki",
     "Minematsu"
    ],
    [
     "Takehiko",
     "Makino"
    ],
    [
     "Steven H.",
     "Weinberger"
    ],
    [
     "Teeraphon",
     "Pongkittiphan"
    ],
    [
     "Chung-Hsien",
     "Wu"
    ]
   ],
   "title": "Speaker-based accented English clustering using a world English archive",
   "original": "sl13_184",
   "page_count": 5,
   "order": 41,
   "p1": "184",
   "pn": "188",
   "abstract": [
    "English is the only language available for global communication. Due to the influence of speakers' mother tongue, however, those from different regions often have different accents in their pronunciation of English. The ultimate goal of our project is automatic creation of a global pronunciation map of World Englishes on an individual basis, for speakers to use to locate similar English pronunciations. Creating the map mathematically requires a matrix of pronunciation distances among all the speakers considered. Our previous study proposed a good algorithm for that purpose [1], where, using reference pronunciation distances calculated from labeled data, a pronunciation distance predictor was trained and built for unlabeled data. Due to space limit in [1], the procedure for calculating reference distances was not described in detail. Then in this paper, detailed descriptions are given and 498 world-wide native and non-native speakers in the Speech Accent Archive are clustered using the reference distances. Results show high accentual validity of the reference interspeaker distances.\n",
    "",
    "",
    "H.-P. Shen, N. Miiiematsu. S. H. Weinberger. T. Makino. J. Novak. T. Pongkittiphan. C.-H. Wu. \"Speaker-based pronunciation clustering of World Englishes based on pronunciation structure analysis.\" IEICE Technical Report. SP2012-116. pp.7-12 (2013-2)\n",
    "",
    "",
    "Index Terms: World Englishes, IPA transcription, DTW, Speech Accent Archive, phonetic pronunciation clustering\n",
    ""
   ],
   "doi": "10.21437/SLaTE.2013-33"
  },
  "hong13_slate": {
   "authors": [
    [
     "Hyejin",
     "Hong"
    ],
    [
     "Sunhee",
     "Kim"
    ],
    [
     "Minhwa",
     "Chung"
    ]
   ],
   "title": "A corpus-based analysis of Korean segments produced by Japanese learners",
   "original": "sl13_189",
   "page_count": 4,
   "order": 42,
   "p1": "189",
   "pn": "192",
   "abstract": [
    "This paper examines variations of Korean segments produced by Japanese learners of Korean. For corpus-based statistical analysis, we have used Korean read speech corpus produced by Japanese learners. Contrastive analysis of the target language and the source language is performed to provide information for interpreting the results of corpus analysis. Segmental variations are analyzed by aligning canonical phonetic transcriptions with auditory phonetic transcriptions of the corpus. The results show that (1) Japanese learners tend to demonstrate substitutions due to differences in the phonetic systems of the two languages; and (2) they are likely to omit a consonant or insert a vowel to deal with the different syllable structures. These results with detailed statistical data are useful for designing a computerassisted pronunciation training and assessment system for Japanese learners of Korean.\n",
    "Index Terms: Korean language education, Japanese learners, segmental variations, computer-assisted language learning\n",
    ""
   ],
   "doi": "10.21437/SLaTE.2013-34"
  }
 },
 "sessions": [
  {
   "title": "Keynotes",
   "papers": [
    "litman13_slate",
    "colpaert13_slate",
    "beckman13_slate"
   ]
  },
  {
   "title": "Children's Education / Children ASR",
   "papers": [
    "bernstein13_slate",
    "claus13_slate",
    "hamalainen13_slate"
   ]
  },
  {
   "title": "Computer_Aided Language Learning (CALL) 1, 2",
   "papers": [
    "su13_slate",
    "davis13_slate",
    "strik13_slate",
    "cai13_slate",
    "vries13_slate",
    "bodnar13_slate",
    "lee13_slate",
    "rayner13_slate"
   ]
  },
  {
   "title": "Demonstration of Applications and Posters",
   "papers": [
    "bang13_slate",
    "miyakoda13_slate",
    "hirano13_slate",
    "delmonte13_slate",
    "cucchiarini13_slate",
    "vries13b_slate",
    "su13b_slate",
    "harbusch13_slate",
    "ahmed13_slate",
    "pongkittiphan13_slate",
    "rasmussen13_slate",
    "rayner13b_slate",
    "hueber13_slate"
   ]
  },
  {
   "title": "Gradation / Evaluation 1, 2",
   "papers": [
    "patil13_slate",
    "lee13b_slate",
    "wang13_slate",
    "bang13b_slate",
    "xiong13_slate"
   ]
  },
  {
   "title": "Prosody",
   "papers": [
    "rongna13_slate",
    "mixdorff13_slate",
    "mixdorff13b_slate",
    "lai13_slate"
   ]
  },
  {
   "title": "Phonetics / Phonology",
   "papers": [
    "tseng13_slate",
    "carranza13_slate",
    "koreman13_slate",
    "short13_slate",
    "shen13_slate",
    "hong13_slate"
   ]
  }
 ],
 "doi": "10.21437/SLaTE.2013"
}