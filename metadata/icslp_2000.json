{
 "title": "6th International Conference on Spoken Language Processing (ICSLP 2000)",
 "location": "Beijing, China",
 "startDate": "16/10/2000",
 "endDate": "20/10/2000",
 "chair": "General Chair: Dinghua Guan",
 "conf": "ICSLP",
 "year": "2000",
 "name": "icslp_2000",
 "series": "ICSLP",
 "SIG": "",
 "title1": "6th International Conference on Spoken Language Processing",
 "title2": "(ICSLP 2000)",
 "date": "16-20 October 2000",
 "booklet": "icslp_2000.pdf",
 "papers": {
  "liljencrants00_icslp": {
   "authors": [
    [
     "Johan",
     "Liljencrants"
    ],
    [
     "Gunnar",
     "Fant"
    ],
    [
     "Anita",
     "Kruckenberg"
    ]
   ],
   "title": "Subglottal pressure and prosody in Swedish",
   "original": "i00_1001",
   "page_count": 4,
   "order": 1,
   "p1": "vol. 1, 1-4",
   "pn": "",
   "abstract": [
    "One part of our study is concerned with modelling of the respiratory system. A phonetically trained speaker uttered a sequence of similar sentences differing in stress patterns. Suband supraglottal pressures and oral flow were measured and from these data stylized articulatory trajectories representing glottal and oral constrictions were derived. A simplified mass, spring, resistance equivalent circuit of the pulmonary system was adopted for an analysis-by-synthesis derivation of underlying pulmonary forces. These amount to simple binary onset and offset commands for breath group initiation and offsets and a set of superimposed gaussian shaped pressure pulses for modelling stress patterns. Glottal and supraglottal articulations impose a passively induced fine structure on a Psub contour. This production oriented modelling supports the findings in speech analysis.\n",
    "A second object of our study was the reading of a one minute long passage from a novel with the respiratory pressures included in the recording. A special listening test was carried our to grade the relative prominence of each syllable. We thus had the opportunity to add both production and perception data to the speech analysis. Of special interest was studies of the covariation of subglottal pressure with F0 and their joint contribution to speech intensity measures. A general finding was that the subglottal pressure plays a role even at moderate stress level. It usually builds up to a maximum at the left boundary of a stressed syllable and decays with a rate that is positively correlated to the perceived prominence.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-1"
  },
  "honda00_icslp": {
   "authors": [
    [
     "Kiyoshi",
     "Honda"
    ],
    [
     "Shinobu",
     "Masaki"
    ],
    [
     "Yasuhiro",
     "Shimada"
    ]
   ],
   "title": "Observation of laryngeal control for voicing and pitch change by magnetic resonance imaging technique",
   "original": "i00_1005",
   "page_count": 4,
   "order": 2,
   "p1": "vol. 1, 5-8",
   "pn": "",
   "abstract": [
    "Phonatory control by laryngeal movement is discussed by summarizing our two previous studies regarding the role of vertical larynx movement based on magnetic resonance imaging (MRI) observation. The first study focuses on a fundamental frequency (F0) control mechanism involving vertical larynx movement. Mid-sagittal images were recorded during vowel productions with a descending musical scale. Successive display of the images demonstrated larynx lowering along forward convexity of the cervical spine. This movement produces a rotation of the cricoid cartilage to shorten the vocal folds because the posterior plate of this cartilage moves almost parallel to the spinal convexity. This F0 lowering mechanism by larynx lowering provides a possible physiological account for the phenomenon that low tones tend to be associated with low larynx positions. The second study deals with a voicing control mechanism for consonant production. Mid-sagittal images were recorded by motion imaging techinique during repetitions of CVCV words such as [tata] and [tada]. The measurement of cricoid cartilage position indicated that larynx position was lower in voiced stop [d] than in voiceless stop [t]. Since larynx lowering during voiced obstruents permits airflow from the glottis with a closed vocal tract, it accounts for a mechanism of consonat voicing. This finding along with the result of the first study accounts for acoustic features of CV syllables such that the vowels tend to have lower F0 and formant frequencies when they follow voiced consonants.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-2"
  },
  "fujisaki00_icslp": {
   "authors": [
    [
     "Hiroya",
     "Fujisaki"
    ],
    [
     "Ryou",
     "Tomana"
    ],
    [
     "Shuichi",
     "Narusawa"
    ],
    [
     "Sumio",
     "Ohno"
    ],
    [
     "Changfu",
     "Wang"
    ]
   ],
   "title": "Physiological mechanisms for fundamental frequency control in standard Chinese",
   "original": "i00_1009",
   "page_count": 4,
   "order": 3,
   "p1": "vol. 1, 9-12",
   "pn": "",
   "abstract": [
    "This paper first presents the physiological and physical properties of the vocal fold and the laryngeal structure that support a model for the generation process of F0 contour with global components and positive local components. It then takes up Standard Chinese as an example of languages that use both positive and negative local components to express tones, and explains the mechanism involving extrinsic laryngeal muscles that is responsible for the generation of negative local components.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-3"
  },
  "carre00_icslp": {
   "authors": [
    [
     "René",
     "Carré"
    ]
   ],
   "title": "On vocal tract asymmetry/symmetry",
   "original": "i00_1013",
   "page_count": 4,
   "order": 4,
   "p1": "vol. 1, 13-16",
   "pn": "",
   "abstract": [
    "To obtain the maximal formant frequency variations for minimal area function deformations, the deformations of the acoustic tube must be anti-symmetrical in case of a closed-open tube (anti-symmetrical structure) and symmetrical in case of a closed-closed tube (symmetrical structure). It means that, in the first case, a front constriction is associated with a back cavity and vice-versa, in the second case, a central constriction is associated with 2 lateral cavities. Thus, in the first case, the optimal solutions are obtained when the length of the pharynx cavity is equal to the length of the mouth cavity. The vowel triangle, the main places of articulation for vowels and consonants are automatically derived from this deductive approach. Nevertheless, several questions are arisen:\n",
    "1) Which strategy can be used to pass from the anti-symmetrical to the symmetrical configuration and vice-versa?\n",
    "2) The optimal acoustic solutions correspond to the male speaker production. Thus, how to explain the configurations with small pharynx cavity such as female or child vocal tract?\n",
    "3) How to simulate vowel production with advance tongue root (ATR) ?\n",
    "Results of simulation for these situations show how to deform an uniform tube to produce different formant variations. The linearity and orthogonality of the relation between the area function deformation and the resulting acoustic F-pattern are pointed out.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-4"
  },
  "engwall00_icslp": {
   "authors": [
    [
     "Olov",
     "Engwall"
    ]
   ],
   "title": "Are static MRI measurements representative of dynamic speech? results from a comparative study using MRI, EPG and EMA",
   "original": "i00_1017",
   "page_count": 4,
   "order": 5,
   "p1": "vol. 1, 17-20",
   "pn": "",
   "abstract": [
    "As the acquisition times of static MRI have diminished, the focus of MRI studies has shifted from isolated phonemes to e.g. vowel-consonant sequences. The articulations are hence somewhat closer to running speech and measurements of contextual influence can be made. The acquisition in the vast majority of the MRI studies still requires artificially sustained articulations, however, and it is hence not evident that the measurements are representative of the subjects normal speech. In order to assess the influence of the artificial sustaining, results from two coarticulatory studies, using static MRI and EMA-EPG, respectively, are compared.\n",
    "The differences between the two studies are substantial concerning jaw position, lip protrusion and tongue contours. However, rather than showing non-representative articulations, the articulations in the MRI data are judged to represent a case of hyperarticulated speech, and should still be valid if considered as that.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-5"
  },
  "lu00_icslp": {
   "authors": [
    [
     "Shinan",
     "Lu"
    ],
    [
     "Lin",
     "He"
    ],
    [
     "Yufang",
     "Yang"
    ],
    [
     "Jianfen",
     "Cao"
    ]
   ],
   "title": "Prosodic control in Chinese TTS system",
   "original": "i00_1021",
   "page_count": 4,
   "order": 6,
   "p1": "vol. 1, 21-24",
   "pn": "",
   "abstract": [
    "In this paper, the prosodic control strategy is discussed under the collectivity of Chinese TTS system design. A four level (syllable, prosodic word, prosodic phrase and sentence) pitch modification and multiplicative duration model are suggested. Although the prototype of models was formed in 1994, the subsequent results of concerned research based on large speech databases are also represented, which effectively advance to perfect the prosody control mode of the Chinese TTS system.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-6"
  },
  "gao00_icslp": {
   "authors": [
    [
     "Yuqing",
     "Gao"
    ],
    [
     "Raimo",
     "Bakis"
    ],
    [
     "Jing",
     "Huang"
    ],
    [
     "Bing",
     "Xiang"
    ]
   ],
   "title": "Multistage coarticulation model combining articulatory, formant and cepstral features",
   "original": "i00_1025",
   "page_count": 4,
   "order": 7,
   "p1": "vol. 1, 25-28",
   "pn": "",
   "abstract": [
    "We describe a multi-stage speech production model containing a linear, phoneme-independent coarticulation filter, followed by a nonlinear component. The latter generates two cepstra which are then additively combined: one corresponding to a relatively smooth background spectrum, and the other representing three formant-like spectral peaks. A neural net is used for both parts, but the second part also utilizes a hard-coded function that generates exactly three spectral peaks. A unified model of training, adaptation, and decoding is developed, each operation di\u000bering only with respect to prior probability distributions. Prior probabilities can be introduced at each stage of the model, providing a flexible framework for utilizing both specific and general prior knowledge. We demonstrate the use of this model for speech synthesis as well as recognition.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-7"
  },
  "fujimura00_icslp": {
   "authors": [
    [
     "Osamu",
     "Fujimura"
    ]
   ],
   "title": "Rhythmic organization and signal characteristics of speech",
   "original": "i00_1029",
   "page_count": 7,
   "order": 8,
   "p1": "vol. 1, 29-35",
   "pn": "",
   "abstract": [
    "The Converter-Distributor (C/D) Model, a generative theory of phonetic implementation, describes an utterance as a linear string of syllables with intervening boundaries. Its base component includes phonetic status contours for voicing, tonal, and vocalic gestures. Consonantal elemental gestures, as stored impulse responses, are excited by the syllable pulse and superimposed onto the base function. A magnitude-modulated syllable-boundary pulse train constitutes a skeletal representation of the rhythmic organization of the utterance. All the temporal characteristics of the speech signal are computed based on the input specifications for each syllable by phonological features and the metrical structure, numerically augmented by prominence enhancement specified for the discourse situation, along with system parameter settings for the particular speaker in each discourse. Segmental durations in the acoustic signal vary according to syllable magnitude, not uniformly among consonants and vowels. The C/D model predicts complex patterns of such prosodic effects on segmental duration as a function of fixed threshold values for relating abstract gestures to observable durations of acoustic signals. (Supported in part by NSF and ATR)\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-8"
  },
  "ohman00_icslp": {
   "authors": [
    [
     "Sven E. G.",
     "Öhman"
    ]
   ],
   "title": "Oral culture in the 21st century: the case of speech processing",
   "original": "i00_1036",
   "page_count": 6,
   "order": 9,
   "p1": "vol. 1, 36-41",
   "pn": "",
   "abstract": [
    "In the 20th Century modern scientific research into speech and spoken language was launched by the pioneering efforts of men like Gunnar Fant, Ken Stevens, Al Liberman, Frank Cooper, and others. As this research progressed it had gradually to shake off the straight-jacket of traditional grammatical-linguistic conceptions, and to develop entirely fresh views. This paper will spell out some of the conceptual differences and discuss their probable consequences for the future in some detail.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-9"
  },
  "jiang00_icslp": {
   "authors": [
    [
     "Jintao",
     "Jiang"
    ],
    [
     "Abeer",
     "Alwan"
    ],
    [
     "Lynne E.",
     "Bernstein"
    ],
    [
     "Patricia",
     "Keating"
    ],
    [
     "Ed",
     "Auer"
    ]
   ],
   "title": "On the correlation between facial movements, tongue movements and speech acoustics",
   "original": "i00_1042",
   "page_count": 4,
   "order": 10,
   "p1": "vol. 1, 42-45",
   "pn": "",
   "abstract": [
    "This study is a first step in a large-scale study that aims at quantifying the relationship between external facial movements, tongue movements, and the acoustics of speech sounds. The database analyzed consisted of 69 CV syllables spoken by two males and two females; each utterance was repeated four times. A Qualysis (optical motion capture system) and an EMA (electromagnetic midsaggital articulography) system were used to characterize facial and tongue movements, respectively. Acoustic features were represented by linear spectral pairs (LSP). To quantify the correlation between them, a multilinear regression technique was applied. The results were analyzed in terms of vowel context, place of articulation, and individual articulatory (EMA or Optical) or acoustic (LSP) channel.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-10"
  },
  "whiteside00_icslp": {
   "authors": [
    [
     "S. P.",
     "Whiteside"
    ],
    [
     "E.",
     "Rixon"
    ]
   ],
   "title": "Coarticulation patterns in identical twins: an acoustic case study",
   "original": "i00_1046",
   "page_count": 4,
   "order": 11,
   "p1": "vol. 1, 46-49",
   "pn": "",
   "abstract": [
    "This study reports on an investigation into the coarticulation patterns of a set of young adult male monozygotic (identical) twins, based on a speech corpus of monosyllabic words. The monosyllabic words were of the structure CVC, and were chosen to represent a variety of consonantal and vowel contexts. Coarticulation patterns were investigated by deriving F2 locus equations and examining patterns of formant frequency changes (excursions) of the first three formant frequencies (F1 to F3 in Hz) by the place of articulation of the initial consonants. Rates of formant frequency change were also examined. Results indicated that there were both similarities and differences in the twins' coarticulation patterns. For example, both twins displayed similar coarticulation patterns across bilabial, alveolar, velar and glottal places of articulation, which was supported by similar F2 locus equation functions for both twins. In contrast, there were some inter-twin differences in formant frequency changes, and rates of formant frequency changes. The details of these results are presented and discussed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-11"
  },
  "hanna00_icslp": {
   "authors": [
    [
     "Philip",
     "Hanna"
    ],
    [
     "Darryl",
     "Stewart"
    ],
    [
     "Ji",
     "Ming"
    ],
    [
     "F. Jack",
     "Smith"
    ]
   ],
   "title": "Improved lexicon formation through removal of co-articulation and acoustic recognition errors",
   "original": "i00_1050",
   "page_count": 4,
   "order": 12,
   "p1": "vol. 1, 50-53",
   "pn": "",
   "abstract": [
    "It is becoming increasingly more necessary that speech recognition systems contain an accurate lexicon, consisting of likely word pronunciations that actually occur within a given domain. Given the increasing size of speech databases, it would appear that data driven approaches are best suited to derive such pronunciations. Presently, however, such an approach often introduces implausible pronunciations, resulting in a higher degree of confusability within the decoder. In this paper, we outline a novel data driven approach which aims to improve the quality of extracted word pronunciations through the removal of co-articulation effects and acoustic model misclassifications from the speech data. A number of selection constraints are additionally employed to exclude any improbable pronunciation alternatives. Initial experiments have shown that the approach does indeed provide plausible pronunciation alternatives without introducing improbable pronunciations.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-12"
  },
  "lindstrom00_icslp": {
   "authors": [
    [
     "Anders",
     "Lindström"
    ],
    [
     "Anna",
     "Kasaty"
    ]
   ],
   "title": "A two-level approach to the handling of foreign items in Swedish speech technology applications",
   "original": "i00_1054",
   "page_count": 4,
   "order": 13,
   "p1": "vol. 1, 54-57",
   "pn": "",
   "abstract": [
    "The functional criteria of a lexical component for Swedish speech recognition and speech synthesis systems are defined and extended with demands regarding proper treatment of foreign features on several linguistic levels. A corpus of Stockholm Swedish, judged to be representative of a variety spoken by younger people, is studied, and necessary extensions on the phonological and morphological level are suggested and implemented in a two-level framework - PC-Kimmo. This \"extended\" two-level description of Swedish is tested on some examples of English loan words and phrases drawn from the corpus.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-13"
  },
  "den00_icslp": {
   "authors": [
    [
     "Yasuharu",
     "Den"
    ],
    [
     "Herbert H.",
     "Clark"
    ]
   ],
   "title": "Word repetitions in Japanese spontaneous speech",
   "original": "i00_1058",
   "page_count": 4,
   "order": 14,
   "p1": "vol. 1, 58-61",
   "pn": "",
   "abstract": [
    "This paper examines several hypotheses based on a strategic view of word repetitions in English. We test whether these hypotheses also apply to Japanese with its fundamentally different syntax. Analyses of 10 task-oriented Japanese dialogues reveal two effects. First, pauses are more frequent before and just after a word at a suspension of the speech than after a repetition of that word. Second, the first token of the repeated word is abnormally prolonged. These results support the strategic view of repetitions. Speakers often suspend speaking after making a preliminary commitment to a constituent, but they prefer to produce that constituent with a continuous delivery. These findings suggest the generality of these strategies across languages.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-14"
  },
  "jongman00_icslp": {
   "authors": [
    [
     "Allard",
     "Jongman"
    ],
    [
     "Corinne B.",
     "Moore"
    ]
   ],
   "title": "The role of language experience in speaker and rate normalization processes",
   "original": "i00_1062",
   "page_count": 4,
   "order": 15,
   "p1": "vol. 1, 62-65",
   "pn": "",
   "abstract": [
    "This study explores the extent to which listeners are sensitive to variations in context when listening to Mandarin tones. Specifically, the effects of speaker F0 and speaking rate are evaluated on the perception of a Tone 2-Tone 3 continuum that varied either along a spectral parameter, a temporal parameter, or both. In addition, two groups of listeners were tested, Chinese and American. Results showed that both listener groups compensate for variations in both F0 and speaking rate. However, Chinese and American listeners did not weigh the acoustic cues in the same manner. Results suggest that language background aids in disambiguating phonemic contrasts for Mandarin listeners, but that for English listeners the normalization effects are a consequence of acoustic discriminability. Limitations on perceptual resources allow English listeners to attend to extrinsic information only when intrinsic acoustic differences become more perceptually salient.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-15"
  },
  "muller00_icslp": {
   "authors": [
    [
     "Achim F.",
     "Müller"
    ],
    [
     "Jianhua",
     "Tao"
    ],
    [
     "Rüdiger",
     "Hoffmann"
    ]
   ],
   "title": "Data-driven importance analysis of linguistic and phonetic information",
   "original": "i00_1066",
   "page_count": 4,
   "order": 16,
   "p1": "vol. 1, 66-69",
   "pn": "",
   "abstract": [
    "In this paper the weight decay concept known from neural network theory is applied to the two modules involved in prosody generation within our text-to-speech system Papageno. Both modules are based on neural networks (NN). Preprocessing layers are inserted connected to the inputs of specialized NN architectures via diagonal weight matrices. The weight decay concept is applied to the weights of these diagonal weight matrices. This allows an importance analysis of the used input parameters in the context of the ised NN architectures.\n",
    "In the symbolic prosody module the importabnce for phrase break prediction of part-of-speech (POS) tags could be evaluated. Further, the necessary length of a POS context window could be analyzed and optimized.\n",
    "For f0 generation for Mandarin language an importance analysis of the phonological information could be performed. The importance analysis led to an optimized input feature set, reducing the squared error of the used NN architecture.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-16"
  },
  "fujisaki00b_icslp": {
   "authors": [
    [
     "Hiroya",
     "Fujisaki"
    ],
    [
     "Katsuhiko",
     "Shirai"
    ],
    [
     "Shuji",
     "Doshita"
    ],
    [
     "Seiichi",
     "Nakagawa"
    ],
    [
     "Keikichi",
     "Hirose"
    ],
    [
     "Shuichi",
     "Itahashi"
    ],
    [
     "Tatsuya",
     "Kawahara"
    ],
    [
     "Sumio",
     "Ohno"
    ],
    [
     "Hideaki",
     "Kikuchi"
    ],
    [
     "Kenji",
     "Abe"
    ],
    [
     "Shinya",
     "Kiriyama"
    ]
   ],
   "title": "Overview of an intelligent system for information retrieval based on human-machine dialogue through spoken language",
   "original": "i00_1070",
   "page_count": 4,
   "order": 17,
   "p1": "vol. 1, 70-73",
   "pn": "",
   "abstract": [
    "This paper presents an intelligent system for information retrieval based on human-machine dialogue through spoken language with novel features such as use of key concepts, unknown word processing, dialogue management through user and system modeling, and automatic acquisition of knowledge to adapt the system to individual users. It then describes an experimental system constructed to implement these features and to demonstrate their feasibility.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-17"
  },
  "yang00_icslp": {
   "authors": [
    [
     "Li-chiung",
     "Yang"
    ]
   ],
   "title": "The expression and recognition of emotions through prosody",
   "original": "i00_1074",
   "page_count": 4,
   "order": 18,
   "p1": "vol. 1, 74-77",
   "pn": "",
   "abstract": [
    "Emotion is an integral component of human speech, and prosody is the principle conveyer of the speaker's state. In this study we show how specific emotional states are expressed in the prosody of spontaneous speech. The significance of prosodic meaning to communicating judgements, attitudes, and the cognitive state of the speaker makes it essential to emotion-intention tracking and to natural-sounding synthesis systems.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-18"
  },
  "swerts00_icslp": {
   "authors": [
    [
     "Marc",
     "Swerts"
    ],
    [
     "Miki",
     "Taniguchi"
    ],
    [
     "Yasuhiro",
     "Katagiri"
    ]
   ],
   "title": "Prosodic marking of information status in tokyo Japanese",
   "original": "i00_1078",
   "page_count": 4,
   "order": 19,
   "p1": "vol. 1, 78-81",
   "pn": "",
   "abstract": [
    "This paper reports the results of a study on the prosodic marking of information status, i.e., whether information is new or given in the discourse, in Tokyo Japanese noun phrases (NPs). Previous investigations have indicated that such discoursal information is re- flected in accent distribution in Dutch NPs, and in degree of accentuation in Italian NPs. An acoustic analysis of Japanese data, in which accent is lexically determined, reveals that information status, both for accented and unaccented words, is mainly reflected in pitch range, with higher range used to signal new information, and lower range used to signal given information. In addition to the effect of discourse, there is downstep, which lowers the pitch of words following an accented word inside a prosodic phrase. A perception study shows that this pitch range variation has limited cue value for listeners about the information status of particular words.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-19"
  },
  "wrede00_icslp": {
   "authors": [
    [
     "Britta",
     "Wrede"
    ],
    [
     "Gernot A.",
     "Fink"
    ],
    [
     "Gerhard",
     "Sagerer"
    ]
   ],
   "title": "Influence of duration on static and dynamic properties of German vowels in spontaneous speech",
   "original": "i00_1082",
   "page_count": 4,
   "order": 20,
   "p1": "vol. 1, 82-85",
   "pn": "",
   "abstract": [
    "Changes in speech rate severely affect the performance of continuous speech recognition systems. In order to better understand the underlying effects of speech rate changes an analysis was carried out on the influence of duration on the spectral properties of vowels in a large German corpus of spontaneous speech. The results show a strong centralisation effect og the vowel formant frequencies due to shorter duration while the formant movements are only slightly affected. The data suggest that the movement velocity is not changed in vowels with a limited duration. As the means of the on- and offset frequencies also remain stable only the middle part of the vowels are affected by the centralisation effect. These results are discussed in the light of the modelling of varying speech rate in automatic speech recognition systems.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-20"
  },
  "zheng00_icslp": {
   "authors": [
    [
     "Bo",
     "Zheng"
    ],
    [
     "Bei",
     "Wang"
    ],
    [
     "Yufang",
     "Yang"
    ],
    [
     "Shinan",
     "Lu"
    ],
    [
     "Jianfen",
     "Cao"
    ]
   ],
   "title": "The regular accent in Chinese sentences",
   "original": "i00_1086",
   "page_count": 4,
   "order": 21,
   "p1": "vol. 1, 86-89",
   "pn": "",
   "abstract": [
    "The regular focus accent is determined by the syntactic structure of a sentence. In general, the distribution of the regular focus accent is regular in simple sentences. Some linguists had proposed some rules on the distribution of regular focus accent, but no experimental test yet.\n",
    "To improve the naturalness of the TTS system, it is very important to prove the rules of focus distribution and apply them in TTS system.\n",
    "The research verified rules of the distribution of the regular accent, and investigated the stressed degree to the different kinds of accents - regular accent and emphatic accent by using psycho-phonetic experiment. The results support all of the rules. The results show that the two kinds of accents are perceptually different in emphatic degree, and that the stressed degree of regular accent is situated between that of emphatic accent and that of non-accent. The acoustic cues of regular accent and emphatic accent are very similar.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-21"
  },
  "mella00_icslp": {
   "authors": [
    [
     "Odile",
     "Mella"
    ],
    [
     "Dominique",
     "Fohr"
    ],
    [
     "Laurent",
     "Martin"
    ],
    [
     "Andreas",
     "Carlen"
    ]
   ],
   "title": "A tool for the synchronization of speech and mouth shapes: LIPS",
   "original": "i00_1090",
   "page_count": 4,
   "order": 22,
   "p1": "vol. 1, 90-93",
   "pn": "",
   "abstract": [
    "This paper presents a new approach to improve the phoneme-based lipsync process. The lipsync process is a module in the animation production pipelines of 2D and 3D cartoons. It consists in generating the mouth positions of a cartoon character from dialogue recorded by an actor. The result is a sequence of time markers which indicate the series of mouth shapes to be drawn. We propose to speed up the lipsync process using tools coming from the field of automatic speech recognition. We describe the LIPS tool (LIPS Logiciel Interactif de PostSynchronisation: lipsync Interactive Software) and the generation of the acoustic models required by the tool and we present results obtained on two cartoons. Using the LIPS tool for the post-synchronization of 52 minutes of cartoon reduced the step of post-synchronization from 14 days to 3 days.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-22"
  },
  "kurdi00_icslp": {
   "authors": [
    [
     "Mohamed-Zakaria",
     "Kurdi"
    ]
   ],
   "title": "Semantic tree unification grammar: a new formalism for spoken language processing",
   "original": "i00_1094",
   "page_count": 4,
   "order": 23,
   "p1": "vol. 1, 94-97",
   "pn": "",
   "abstract": [
    "In this paper we present the Semantic Tree Unification Grammar (STUG) which is a new formalism for parsing spoken language.\n",
    "The main motivation of this formalism is the combination of the robustness and simplicity of the classical semantic grammar to the deepness of the traditional syntactic formalisms.\n",
    "The key properties of STUG are: the direct linearization of the semantic structure, an economical feature structure and the simplicity of the grammar writing and modification. STUG was implemented in the OASIS system which is a system of partial parsing of spontaneous spoken language. The results of our evaluation are encouraging.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-23"
  },
  "kurematsu00_icslp": {
   "authors": [
    [
     "Akira",
     "Kurematsu"
    ],
    [
     "Yousuke",
     "Shionoya"
    ]
   ],
   "title": "Identification of utterance intention in Japanese spontaneous spoken dialogue by use of prosody and keyword information",
   "original": "i00_1098",
   "page_count": 4,
   "order": 24,
   "p1": "vol. 1, 98-101",
   "pn": "",
   "abstract": [
    "This paper describes the study on the identification of utterance intention in Japanese spontaneous dialogue. The procedure of tagging the dialog act which was labeled by hand was evaluated by the analysis of the prosodic information and keyword recognition for the dialogues of scheduling and travel arrangement domains. It was shown that the integration of prosody and keywords relevant to illocutional force type is effective for dialog tagging of Japanese spontaneous dialogue.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-24"
  },
  "abdou00_icslp": {
   "authors": [
    [
     "Sherif",
     "Abdou"
    ],
    [
     "Michael",
     "Scordilis"
    ]
   ],
   "title": "Improved speech understanding using dialogue expectation in sentence parsing",
   "original": "i00_1102",
   "page_count": 4,
   "order": 25,
   "p1": "vol. 1, 102-105",
   "pn": "",
   "abstract": [
    "In dialogue systems, speech recognition errors force the user to repeat information resulting in more turns, lower dialogue efficiency and maybe complete failure. Higher level information such as history, expectation, discourse knowledge and pragmatics can improve performance but are hard to quantify and effectively include in the recognition process. In this paper, dialogue expectation is used to improve recognition. By permitting the dialogue manager to guide the interaction it is possible to track the dialogue state and thus estimate the expected semantic content of the users response. The parser is allowed to process a large number of sentences provided by the decoder. Expectation is used as an effective criterion for selecting among competing hypotheses. This approach was tested with a simple flight reservation task and results show improvement in concept recognition without adding significant computation.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-25"
  },
  "meng00_icslp": {
   "authors": [
    [
     "Helen M.",
     "Meng"
    ],
    [
     "Carmen",
     "Wai"
    ],
    [
     "Roberto",
     "Pieraccini"
    ]
   ],
   "title": "The use of belief networks for mixed-initiative dialog modeling",
   "original": "i00_1106",
   "page_count": 4,
   "order": 26,
   "p1": "vol. 1, 106-109",
   "pn": "",
   "abstract": [
    "This paper describes the use of Belief Networks for mixedinitiative dialog modeling within the context of the CU FOREX system [1]. CU FOREX is a bilingual hotline for real-time foreign exchange inquiries. Presently, it supports two separate interaction modalities: a direct dialog (DD) interaction, which is systeminitiated for novice users; as well as natural language (NLS) shortcuts, which is user-initiated for expert users. In this work, we propose to use Belief Networks (BNs) to automatically govern the model transitions for mixed-initiative interactions. Furthermore, we hope that our approach can reduce the amount of handcrafting involved in the development of current mixedinitiative dialog models, to ease portability across different applications domains.\n",
    "",
    "",
    "Meng, H., S. Lee and C. Wai, CU FOREX: A bilingual spoken dialog system for the foreign exchange domain, Proceedings of ICASSP-2000.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-26"
  },
  "mctear00_icslp": {
   "authors": [
    [
     "Michael F.",
     "McTear"
    ],
    [
     "Susan",
     "Allen"
    ],
    [
     "Laura",
     "Clatworthy"
    ],
    [
     "Noelle",
     "Ellison"
    ],
    [
     "Colin",
     "Lavelle"
    ],
    [
     "Helen",
     "McCaffery"
    ]
   ],
   "title": "Integrating flexibility into a structured dialogue model: some design considerations",
   "original": "i00_1110",
   "page_count": 4,
   "order": 27,
   "p1": "vol. 1, 110-113",
   "pn": "",
   "abstract": [
    "Structured dialogue models are the most commonly used dialogue models in commercial systems, particularly as they are relatively easy to design and re-use. The current paper reports on a study that examined the feasibility of combining more flexible dialogue control with a structured dialogue model. Several systems were built using the RAD (Rapid Application Developer) component of the CSLU toolkit, augmented with the Phoenix natural language parsing system and a dialogue manager that used a representation of the systems information state to determine the systems next question or action. Results indicated that with an optimized continuous speech recognizer a dialogue permitting flexible input can be concluded efficiently and successfully, while in cases of degraded recognition the recovery strategies and more structured dialogue control enhance the likelihood of a successful transaction. The paper discusses a number of design issues that support developers in making structured dialogue models more flexible.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-27"
  },
  "niimi00_icslp": {
   "authors": [
    [
     "Yasuhisa",
     "Niimi"
    ],
    [
     "Tomoki",
     "Oku"
    ],
    [
     "Takuya",
     "Nishimoto"
    ],
    [
     "Masahiro",
     "Araki"
    ]
   ],
   "title": "A task-independent dialogue controller based on the extended frame-driven method",
   "original": "i00_1114",
   "page_count": 4,
   "order": 28,
   "p1": "vol. 1, 114-117",
   "pn": "",
   "abstract": [
    "This paper presents a task-independent dialogue con- trol scheme, which is an extension of the frame-driven method. A spoken dialogue system, SDSKIT-3, which is based on this scheme, has been developed for the tourist information service and has demonstrated it can manage problems which are encountered in a spo- ken dialogue system. The scheme is transported to another task, the guide for personal computer buy- ers. Fifty dialogues were collected and used to design the topic frames for the new task and to test the new system. Simulated dialogues controlled by the newly created topic frames have proven the proposed scheme can be task-independent within a kind of tasks.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-28"
  },
  "xu00_icslp": {
   "authors": [
    [
     "Wei",
     "Xu"
    ],
    [
     "Alex",
     "Rudnicky"
    ]
   ],
   "title": "Language modeling for dialog system",
   "original": "i00_1118",
   "page_count": 4,
   "order": 29,
   "p1": "vol. 1, 118-121",
   "pn": "",
   "abstract": [
    "Language modeling for speech recognizer in dialog systems can take two forms. Human input can be constrained through a directed dialog, allowing the decoder to use a state-specific language model to improve recognition accuracy. Mixedinitiative systems allow for human input that while domainspecific might not be state-specific. Nevertheless, for the most part human input to a mixed-initiative system is predictable, particularly when given information about the immediately preceding system prompt. The work reported in this paper addresses the problem of balancing state-specific and general language modeling in a mixed-initiative dialog system. By incorporating dialog state adaptation of the language model, we have reduced the recognition error rate by 11.5%.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-29"
  },
  "georgila00_icslp": {
   "authors": [
    [
     "Kallirroi",
     "Georgila"
    ],
    [
     "Nikos",
     "Fanotakis"
    ],
    [
     "George",
     "Kokkinakis"
    ]
   ],
   "title": "Building stochastic language model networks based on simultaneous word/phrase clustering",
   "original": "i00_1122",
   "page_count": 4,
   "order": 30,
   "p1": "vol. 1, 122-125",
   "pn": "",
   "abstract": [
    "In this paper we present a novel method for creating stochastic networks for language modelling in spoken dialogue systems. This is accomplished by taking a set of sentences (created manually, derived from simulation experiments, from using the system itself, the application grammar, or a combination of these methods), and training a Hidden Markov Model (HMM), which incorporates all the information about the structure of these sentences. Our technique has the great advantage that during the creation of the HMM, classes containing words or phrases with semantic-syntactic similarities are formed automatically. After all the training data has been used the final HMM is transformed to a stochastic network. The states and observations of the HMM correspond to the word/phrase classes and words/phrases respectively. The nodes of the stochastic network are the word/phrase classes and the arcs are the state transition probabilities of the HMM. The observation probabilities of the HMM correspond to the probabilities within the classes of the stochastic network. Our method has been tested using data from an Interactive telephone-based Directory Assistance Services system and a call-routing spoken dialogue system and has shown the expected advantages.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-30"
  },
  "yang00b_icslp": {
   "authors": [
    [
     "Li-chiung",
     "Yang"
    ],
    [
     "Richard",
     "Esposito"
    ]
   ],
   "title": "Prosody and topic structuring in spoken dialogue",
   "original": "i00_1126",
   "page_count": 4,
   "order": 31,
   "p1": "vol. 1, 126-129",
   "pn": "",
   "abstract": [
    "Prosody is critical in conveying topic coherence and the salience of information in speech. In this study we propose that the overall coherence is brought about through pitch level structuring of phrases at both the local level of hierarchical phrase unit positioning and the global level of pitch baseline rise and fall as climax and resolution. Our results show that prosody has critical importance in conversation, and is crucial for topic segmentation, topic tracking, and information extraction.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-31"
  },
  "maes00_icslp": {
   "authors": [
    [
     "Stéphane H.",
     "Maes"
    ]
   ],
   "title": "Elements of conversational computing - a paradigm shift",
   "original": "i00_1130",
   "page_count": 4,
   "order": 32,
   "p1": "vol. 1, 130-133",
   "pn": "",
   "abstract": [
    "The new decade will be characterized by the availability of multiple access devices that enable ubiquitous access to information. The ability to access and transform information via a multiplicity of appliances, each designed to suit the user's specific usage environment, necessarily means that these interactions will exploit all available input and output modalities to maximize the band-width of man-machine communication.\n",
    "This paper introduces the concept of conversational computing: a set of new fundamental principles of computing that the new generation of computers will need to follow in order to satisfy the new demand that information can be accessed and manipulated anywhere, at any time and through any device. A feat not yet achievable with today's access devices and channels.\n",
    "We address the challenges of designing user interfaces that work across these multiplicity of information appliances Amongst the key issues to be addressed are the user's ability to interact in parallel with the same information via a multiplicity of appliances and user interfaces, and the need to present a unified, synchronized view of information across the various appliances that the user deploys to interact with information. We achieve such synchronized interactions and views by adopting the well-known Model, View, Controller (MVC) design paradigm and adapting it to conversational interactions.\n",
    "Following these key principles, we will discusses principles, functions and services to be supported by CUI applications or platforms. In particular, we introduce a new programming model, \"programming by interaction\", as corner stone of new CUI application development environments. This will demonstrate by example the implications that conversational computing can be expected to have on the IT world.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-32"
  },
  "muller00b_icslp": {
   "authors": [
    [
     "Ludek",
     "Müller"
    ],
    [
     "Filip",
     "Jurcicek"
    ],
    [
     "Lubos",
     "Smidl"
    ]
   ],
   "title": "Rejection and key-phrase spottin techniques using a mumble model in a czech telephone dialog system",
   "original": "i00_1134",
   "page_count": 4,
   "order": 33,
   "p1": "vol. 1, 134-137",
   "pn": "",
   "abstract": [
    "This paper describes an implementation of a mumble model in a real-time Czech telephone dialog system, and its contribution to speech recognition. A short overview of the Czech telephone dialog system with a special focus on its speech recognition module is given. Furthermore, the structure of a mumble model is described. A new rejection technique based on a local time comparison between a mumbles score and a words score, evaluated by a Viterbi search is explained. A key-phrase spotting technique using a mumble model is shown. The both techniques have been evaluated with a Czech telephone database. The results show the 25.1% equal error rate of the rejection technique and 15.5% equal error rate of the key-phrase spotting technique.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-33"
  },
  "paek00_icslp": {
   "authors": [
    [
     "Tim",
     "Paek"
    ],
    [
     "Eric",
     "Horvitz"
    ],
    [
     "Eric",
     "Ringger"
    ]
   ],
   "title": "Continuous listening for unconstrained spoken dialog",
   "original": "i00_1138",
   "page_count": 4,
   "order": 34,
   "p1": "vol. 1, 138-141",
   "pn": "",
   "abstract": [
    "A major hindrance to rendering spoken dialog systems capable of ongoing, continuous listening without requiring a push-totalk device is the problem of distinguishing speech which is intended for the system from that which is overheard. We present a decision-theoretic approach to this problem that exploits Bayesian models of spoken dialog at four levels of analysis within a domain-independent, multi-modal computational architecture called Quartet. We applied Quartet to the task of navigating PowerPoint slide shows during a spoken presentation in a prototype system called Presenter. We describe the runtime behavior of Presenter as well as the results of an experimental study comparing the performance of Presenter to human subjects in discriminating arbitrarily formed spoken requests for slide navigation during a recorded lecture.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-34"
  },
  "shriver00_icslp": {
   "authors": [
    [
     "Stefanie",
     "Shriver"
    ],
    [
     "Alan W.",
     "Black"
    ],
    [
     "Ronald",
     "Rosenfeld"
    ]
   ],
   "title": "Audio signals in speech interfaces",
   "original": "i00_1142",
   "page_count": 4,
   "order": 35,
   "p1": "vol. 1, 142-145",
   "pn": "",
   "abstract": [
    "This paper discusses a variety of types of non-lexical signals such as beeps, prosodic variation and speaker style changes, and we consider four cases in which such signals might be used to good effect. We discuss the results of user tests to determine if specific types of non-lexical signals are better in some situations than in others, and we discuss the advantages and disadvantages of using such signals.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-35"
  },
  "boda00_icslp": {
   "authors": [
    [
     "Péter Pál",
     "Boda"
    ]
   ],
   "title": "Visualisation of spoken dialogues",
   "original": "i00_1146",
   "page_count": 4,
   "order": 36,
   "p1": "vol. 1, 146-149",
   "pn": "",
   "abstract": [
    "Evaluation of spoken dialogue systems is in the main interest of the application developer. During the whole development cycle reliable and straightforwardly interpretable measures are needed in order to identify weak points of the user interface and the actual implementation. This paper proposes two issues. First, a visualisation method is introduced in order to follow what exactly happens in spoken dialogues during the interaction and thus giving means for system developers to explore dialogue paths explicitly in a statistical sense. The formalism of the visualisation process is explained and examples are given for different interaction types. The second issue is the introduction of a measure derived directly from the visualisation method. This measure is a sort of combination of traditional success rate and average number of system-user turns. The measure provides means for system developers to compare the average system performance to a pre-defined interaction described by a dialogue path. Example is given with a speaker-independent name dialling application.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-36"
  },
  "zajicek00_icslp": {
   "authors": [
    [
     "Mary",
     "Zajicek"
    ]
   ],
   "title": "The construction of speech output to support elderly visually impaired users starting to use the internet",
   "original": "i00_1150",
   "page_count": 4,
   "order": 37,
   "p1": "vol. 1, 150-153",
   "pn": "",
   "abstract": [
    "This paper is concerned with the construction of dialogues for speech output with large text, in a Web browser for elderly visually impaired people. Studies made in our research group, show that elderly visually impaired users experience great difficulty in getting going on the Internet when they have no previous experience in computing [6]. Age Associated Memory Impairment (AAMI) is a major problem for these users [2]. This coupled with their visual impairment demands a new dialogue interface modality.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-37"
  },
  "takagi00_icslp": {
   "authors": [
    [
     "Kazuyuki",
     "Takagi"
    ],
    [
     "Rei",
     "Oguro"
    ],
    [
     "Kazuhiko",
     "Ozeki"
    ]
   ],
   "title": "Effects of word string language models on noisy broadcast news speech recognition",
   "original": "i00_1154",
   "page_count": 4,
   "order": 38,
   "p1": "vol. 1, 154-157",
   "pn": "",
   "abstract": [
    "In this paper, we present the results that our n-gram based word string language model, combined with speaker and noise adaptation of the acoustic model, improves recognition performance of noisy broadcast news speech. The focus was brought into a remedy against recognition errors of short words. The word string language models based on POS and n-gram fre- quency reduced deletion errors by 17%, insertion errors by 20%, and substitution errors by 3% in Japanese TV broadcast news speech recognition.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-38"
  },
  "luo00_icslp": {
   "authors": [
    [
     "Xiaoqiang",
     "Luo"
    ],
    [
     "Martin",
     "Franz"
    ]
   ],
   "title": "Semantic tokenization of verbalized numbers in language modeling",
   "original": "i00_1158",
   "page_count": 4,
   "order": 39,
   "p1": "vol. 1, 158-161",
   "pn": "",
   "abstract": [
    "In spoken dialog systems, number strings frequently carry crucial information such as DATE, TIME, and PRICE. Yet numbers are inherently difficult to recognize, partly because reliable statistics for training a language model is hard to obtain. In this paper, we take the advantage of the fact that dialog systems perform some form of semantic parsing. We use this parsing information to distinguish between the occurrences of number expressions in various semantic roles, as for example between the word \"one\" in \"one oclock\", \"sunday june one\" and \"another one\" to improve the performance of the language model and thus reduce the error rate. We process number expressions with the same spelling, but different semantics, as separate language model tokens. We have tested this approach in a speech recognition system used as a part of a dialog system for the Air Travel domain. In a controlled experiment, the proposed technique yields a healthy 9.75% relative (overall) word error reduction on a test set of 689 sentences, collected using a live telephony Air Travel system.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-39"
  },
  "kato00_icslp": {
   "authors": [
    [
     "Kazuomi",
     "Kato"
    ],
    [
     "Hiroaki",
     "Nanjo"
    ],
    [
     "Tatsuya",
     "Kawahara"
    ]
   ],
   "title": "Automatic transcription of lecture speech using topic-independent language modeling",
   "original": "i00_1162",
   "page_count": 4,
   "order": 40,
   "p1": "vol. 1, 162-165",
   "pn": "",
   "abstract": [
    "We approach lecture speech recognition with a topic-independent language model and its adaptation. As lecture speech has its characteristic style that is different from newspapers and conversations, dedicated language modeling is needed. The problem is that, although lectures have many keywords specific to the topic and fields, available corpus of each domain is limited in size. Thus, we introduce topic-independent modeling with a vocabulary selection mechanism based on a mutual information criterion. It realizes better coverage and accuracy with small complexity than the conventional word frequency-based method. This baseline model is adapted to specific lectures using preprint texts. We have tried automatic transcription of oral presentations and achieved a word error rate of 23.6% on the average.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-40"
  },
  "guillen00_icslp": {
   "authors": [
    [
     "Rocio",
     "Guillén"
    ],
    [
     "Randal",
     "Erman"
    ]
   ],
   "title": "Extending grammars based on similar-word recognition",
   "original": "i00_1166",
   "page_count": 4,
   "order": 41,
   "p1": "vol. 1, 166-169",
   "pn": "",
   "abstract": [
    "Pronunciation variation is extremely widespread and one of the reasons for recognition errors. In this paper we explore how similar-recognized-words can be used to construct or expand more accurate grammars in a specific domain. The domain that serves as framework for this research is the assessment of depression. Assessment of depression is done via a system that verbally administers a discrete choice questionnaire over the telephone. Several experiments carried out have shown that in spite of the limited number of valid responses, responses uttered by the same speaker may be both correctly or incorrectly recognized. Analysis of the responses incorrectly recognized has provided the elements to formulate new grammar rules. To test the grammars thus expanded a new set of experiments was completed and the results obtained are presented and discussed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-41"
  },
  "whittaker00_icslp": {
   "authors": [
    [
     "E. W. D.",
     "Whittaker"
    ],
    [
     "P. C.",
     "Woodland"
    ]
   ],
   "title": "Particle-based language modelling",
   "original": "i00_1170",
   "page_count": 4,
   "order": 42,
   "p1": "vol. 1, 170-173",
   "pn": "",
   "abstract": [
    "This paper investigates the use of particle (sub-word) N-grams for language modelling. One linguistics-based and two datadriven algorithms are presented and evaluated in terms of perplexity for Russian and English. Interpolating word trigram and particle 6-gram models gives up to a 7.5% perplexity reduction over the baseline word trigram model for Russian. Lattice rescoring experiments are also performed on 1997 DARPA Hub4 evaluation lattices where the interpolated model gives a 0.4% absolute reduction in word error rate over the baseline word trigram model.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-42"
  },
  "choi00_icslp": {
   "authors": [
    [
     "W. N.",
     "Choi"
    ],
    [
     "Y. W.",
     "Wong"
    ],
    [
     "Tan",
     "Lee"
    ],
    [
     "P. C.",
     "Ching"
    ]
   ],
   "title": "Lexical tree decoding with a class-based language model for Chinese speech recognition",
   "original": "i00_1174",
   "page_count": 4,
   "order": 43,
   "p1": "vol. 1, 174-177",
   "pn": "",
   "abstract": [
    "This paper presents a method to integrate the class bigram language model effectively to the lexical tree decoder. The method reduces the memory requirement and search effort in comparison with the conventional lexical tree search with word bigram language model. The decoder is based on a time-synchronous beam search, using cross-word triphone acoustic model. To demonstrate its effectiveness, the algorithm is tested with a stock query task. Experimental results show that the lexical tree decoder based on a class bigram can reduce the search space by 11.8%. By using class-bigram look-ahead, the memory cost for storing the look-ahead probability can also achieve a saving of 73% without degradation in recognition accuracy.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-43"
  },
  "visweswariah00_icslp": {
   "authors": [
    [
     "K.",
     "Visweswariah"
    ],
    [
     "H.",
     "Printz"
    ],
    [
     "M.",
     "Picheny"
    ]
   ],
   "title": "Impact of bucketing on performance of linearly interpolated language models",
   "original": "i00_1178",
   "page_count": 4,
   "order": 44,
   "p1": "vol. 1, 178-181",
   "pn": "",
   "abstract": [
    "N-gram models are used to model language in various applications. For large vocabularies, even a very large corpus is insu\u000ecient to estimate a raw ratio-of-counts trigram model. One common way to overcome this problem is by linear interpolation of the trigram model with lower order models. The interpolation weights can be varied as a function of the current history, to reflect the confidence we have in the estimates of various orders. Since the number of histories is large we cannot hope to estimate a set of weights for each history. Thus sets of histories are tied together and the same weights are used for all histories within the set. In this paper we study the e\u000bect of the algorithm used to tie together the various histories. We report word error rate (WER) results on a large-vocabulary speech recognition task.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-44"
  },
  "zhang00_icslp": {
   "authors": [
    [
     "Shuwu",
     "Zhang"
    ],
    [
     "Hirofami",
     "Yamamoto"
    ],
    [
     "Yoshinori",
     "Sagisaka"
    ]
   ],
   "title": "An embedded knowledge integration for hybrid language modelling",
   "original": "i00_1182",
   "page_count": 4,
   "order": 45,
   "p1": "vol. 1, 182-195",
   "pn": "",
   "abstract": [
    "This paper describes an embedded architectnre to couple utilizable language knowledge and innovative language models, as well as modeling approaches, for intensive language modeling in speech recognition. In this embedded mechanism, three innovative language modeling approaches at different levels, i.e composite N-gram, distance-related unit association maximnm entropy (DU-AME), and linkgram have different functions to extend the definitions of basic language units, favorably improve the underlying model instead of conventional N-grams, and provide effective combination with longer history syntactic link dependency knowledge, respectively.\n",
    "In this three-level hybrid language modeling, each lower level modeling serves the higher level modeling(s). The results in each level are well utilized or embedded in the higher level(s). These models can be trained level by level. Accordingly, some prospective language constraints can finally be embedded in a well-organized hybrid model.\n",
    "Experimental data based on the embedded modeling show that the hybrid model reduces WER 14.5% compared with the conventional word-based bigram model. As a result, it can be expected to improve the conventional statistical language modeling.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-45"
  },
  "galescu00_icslp": {
   "authors": [
    [
     "Lucian",
     "Galescu"
    ],
    [
     "James",
     "Allen"
    ]
   ],
   "title": "Hierarchical statistical language models: experiments on in-domain adaptation",
   "original": "i00_1186",
   "page_count": 4,
   "order": 46,
   "p1": "vol. 1, 186-189",
   "pn": "",
   "abstract": [
    "We introduce a hierarchical statistical language model, represented as a collection of local models plus a general sentence model. We provide an example that mixes a trigram general model and a PFSA local model for the class of decimal numbers, described in terms of sub-word units (graphemes). This model practically extends the vocabulary of the overall model to an infinite size, but still has better performance compared to a word-based model.\n",
    "Using in-domain language model adaptation experiments, we show that local models can encode enough linguistic information, if well trained, that they may be ported to new language models without re-estimation.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-46"
  },
  "yamamoto00_icslp": {
   "authors": [
    [
     "Hirofumi",
     "Yamamoto"
    ],
    [
     "Kouichi",
     "Tanigaki"
    ],
    [
     "Yoshinori",
     "Sagisaka"
    ]
   ],
   "title": "A language model for conversational speech recognition using information designed for speech translation",
   "original": "i00_1190",
   "page_count": 4,
   "order": 47,
   "p1": "vol. 1, 190-193",
   "pn": "",
   "abstract": [
    "In this paper, a new language model is proposed for speech recognition in conversational speech translation. In conversation, speech strongly depends on the previous utterance of the other participant. Applying this dependency in language modeling, we can reduce the speech recognition error rate. To this end, we propose the following new language model where the content of the previous utterance is expressed by an interlingual representation which is widely used in the spoken language translation research group C-star. The proposed method reduces word error rate by 6% (from 14.7% to 13.9%), confirming our expectations.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-47"
  },
  "carpenter00_icslp": {
   "authors": [
    [
     "Bob",
     "Carpenter"
    ],
    [
     "Sol",
     "Lerner"
    ],
    [
     "Roberto",
     "Pieraccini"
    ]
   ],
   "title": "Optimizing BNF grammars through source transformations",
   "original": "i00_1194",
   "page_count": 4,
   "order": 48,
   "p1": "vol. 1, 194-197",
   "pn": "",
   "abstract": [
    "In this paper we explore the efficiency of various ways of expressing the form and meaning of natural language utterances as context-free grammars. We concentrate on the top-down parsing strategy employed in SpeechWorks 6.5, a strategy common to many systems. As with other search-based parsers, the key to efficiency is to limit the uncertainty of the parser at any given stage by reducing non-determinism in the grammar. Here we study the effects of different expressions of the same grammar in terms of efficiency. We also describe a methodology for transforming a source grammar into a more efficient expression of the same forms and meanings.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-48"
  },
  "wu00_icslp": {
   "authors": [
    [
     "Jian",
     "Wu"
    ],
    [
     "Fang",
     "Zheng"
    ]
   ],
   "title": "On enhancing katz-smoothing based back-off language model",
   "original": "i00_1198",
   "page_count": 5,
   "order": 49,
   "p1": "vol. 1, 198-201",
   "pn": "",
   "abstract": [
    "Though the statistical language modeling plays an important role in speech recognition, there are still problems that are difficult to be solved such as the sparseness of training data. Generally, two kinds of smoothing approaches, namely the back-off model and the interpolated model, have been proposed to solve the problem of the impreciseness of language models caused by the sparseness of training data. By expanding the idea of interpolation model to Katz-smoothing based re-estimation of the seen word pairs, a back-off model based modified method is proposed, referred to as the enhanced Katz smoothing with deleted interpolation (EKSWDI). A uniform expression and two simplified versions for this modified model are also given. Experiments on a Chinese pinyin-to-character conversion system and the perplexity measure show that the proposed model has a better performance than the Katz smoothing method does.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-49"
  },
  "xu00b_icslp": {
   "authors": [
    [
     "Wei",
     "Xu"
    ],
    [
     "Alex",
     "Rudnicky"
    ]
   ],
   "title": "Can artificial neural networks learn language models?",
   "original": "i00_1202",
   "page_count": 4,
   "order": 50,
   "p1": "vol. 1, 202-205",
   "pn": "",
   "abstract": [
    "Currently, N-gram models are the most common and widely used models for statistical language modeling. In this paper, we investigated an alternative way to build language models, i.e., using artificial neural networks to learn the language model. Our experiment result shows that the neural network can learn a language model that has performance even better than standard statistical methods.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-50"
  },
  "savova00_icslp": {
   "authors": [
    [
     "Guergana",
     "Savova"
    ],
    [
     "Michael",
     "Schonwetter"
    ],
    [
     "Sergey",
     "Pakhomov"
    ]
   ],
   "title": "Improving language model perplexity and recognition accuracy for medical dictations via within-domain interpolation with literal and semi-literal corpora",
   "original": "i00_1206",
   "page_count": 4,
   "order": 51,
   "p1": "vol. 1, 206-209",
   "pn": "",
   "abstract": [
    "We propose a technique for improving language modeling for automated speech recognition of medical dictations by interpolating finished text (25M words) with small humangenerated literal or/and machine-generated semiliteral corpora. By building and testing interpolated (ILM) with literal (LILM), semiliteral (SILM) and partial (PILM) corpora, we show that both perplexity and recognition results improve significantly with LILM and SILM; the two yielding very close results.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-51"
  },
  "weilhammer00_icslp": {
   "authors": [
    [
     "Karl",
     "Weilhammer"
    ],
    [
     "Günther",
     "Ruske"
    ]
   ],
   "title": "Placing structuring elements in a word sequence for generating new statistical language models",
   "original": "i00_1210",
   "page_count": 4,
   "order": 52,
   "p1": "vol. 1, 210-213",
   "pn": "",
   "abstract": [
    "Class based n-gram language models have been applied successfully in speech technology. We will present an automatic method to improve n-gram language models by distributing structural elements in a new way in word sequences. Our algorithm works on textual data consisting of two different kinds of text elements, namely words and structural elements. The order of words will not be changed during the iterations. Only structural elements can be inserted or deleted by the algorithm between any two items in the data. Thus unseen n-grams will be interpolated by n-grams containing structural elements. We give a detailed description of the algorithm and present first results of a system trained on a small corpus.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-52"
  },
  "esteve00_icslp": {
   "authors": [
    [
     "Yannick",
     "Estève"
    ],
    [
     "Frédéric",
     "Béchet"
    ],
    [
     "Renato de",
     "Mori"
    ]
   ],
   "title": "Dynamic selection of language models in a dialogue system",
   "original": "i00_1214",
   "page_count": 4,
   "order": 53,
   "p1": "vol. 1, 214-217",
   "pn": "",
   "abstract": [
    "This paper describes a method for building statistical Language Models (LMs) dedicated to specific dialogue situations. The architecture of the speech recognition system proposed uses several LMs. The first stage of this system, consists of producing a word-lattice from a given sentence uttered by a speaker. A general LM calculates a sentence-hypothesis. Then, in a second stage, the system chooses a specialized LM according to the word-lattice and the previous hypothesis. Another decoding process is performed using this specialized LM in order to produce a new sentence- hypothesis. Finally, a decision-module processes these two hypotheses in order to assign three confidence levels to the sentence-hypothesis produced. These confidence levels can be used by the dialogue manager in order to improve the dialogue, by asking a confirmation to the speaker when a sentence is labeled ambiguous.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-53"
  },
  "johnsen00_icslp": {
   "authors": [
    [
     "Magne H.",
     "Johnsen"
    ],
    [
     "Trym",
     "Holter"
    ],
    [
     "Torbjørn",
     "Svendsen"
    ],
    [
     "Erik",
     "Harborg"
    ]
   ],
   "title": "Stochastic modeling of semantic content for use IN a spoken dialogue system",
   "original": "i00_1218",
   "page_count": 4,
   "order": 54,
   "p1": "vol. 1, 218-221",
   "pn": "",
   "abstract": [
    "A key issue in a spoken dialogue system is the successful semantic interpretation of the output from the speech recognizer. Extracting the semantic concepts, i.e. the meaningful phrases, of an utterance is traditionally performed using rule based methods. In this paper we describe a statistical framework for modeling (and decoding) semantic concepts based on discrete hidden Markov models (DHMMs). Each semantic concept class is modeled as a multi-state DHMM, where the observations are the recognized words. The proposed decoding procedure is capable of parsing an utterance into a sequence of phrases, each belonging to a different concept class. The phrase sequence will correspond to a concept segmentation and class identification, whilst the semantic entities constituting each phrase contain the semantic value.\n",
    "The algorithm has been tested on a dialogue system for bus route information in Norwegian. The results confirm the applicability of the procedure. Semantically relevant concepts in input inquiries could be identified with 6.9% error rate on the sentence level. The corresponding segmentation error rate was 8.6% when concept segmentation information was available during training. Without this information, i.e. if the training was performed in an embedded mode, the segmentation error rate increased to 23.5%.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-54"
  },
  "takara00_icslp": {
   "authors": [
    [
     "Tomio",
     "Takara"
    ],
    [
     "Eiji",
     "Nagaki"
    ]
   ],
   "title": "Spoken word recognition using the artificial evolution of a set of vocabulary",
   "original": "i00_1222",
   "page_count": 4,
   "order": 55,
   "p1": "vol. 1, 222-225",
   "pn": "",
   "abstract": [
    "Hidden Markov models (HMMs) are widely used for automatic speech recognition. However, there is a problem still unresolved, i.e. how to design the optimal structure of the HMM. As an answer to this problem, we proposed the application of a genetic algorithm (GA) to search out such an optimal structure, and we showed this method to be effective for isolated word recognition. In these applications, the evolutions occurred at each word class independently. However, many isolated word recognition systems are performed using a set of vocabulary. Therefore, the artificial evolution using the vocabulary set is thought to be more e\u000bective. In this paper, we propose the spoken word recognition using the artificial evolution of a set of vocabulary.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-55"
  },
  "horvitz00_icslp": {
   "authors": [
    [
     "Eric",
     "Horvitz"
    ],
    [
     "Tim",
     "Paek"
    ]
   ],
   "title": "Deeplistener: harnessing expected utility to guide clarification dialog in spoken language systems",
   "original": "i00_1226",
   "page_count": 4,
   "order": 56,
   "p1": "vol. 1, 226-229",
   "pn": "",
   "abstract": [
    "We describe research on endowing spoken language systems with the ability to consider the cost of misrecognition, and using that knowledge to guide clarification dialog about a users intentions. Our approach relies on coupling utility-directed policies for dialog with the ongoing Bayesian fusion of evidence obtained from multiple utterances recognized during an interaction. After describing the methodology, we review the operation of a prototype system called DeepListener. DeepListener considers evidence gathered about utterances over time to make decisions about the optimal dialog strategy or realworld action to take given uncertainties about a users intentions and the costs and benefits of different outcomes.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-56"
  },
  "deng00_icslp": {
   "authors": [
    [
     "Yunbin",
     "Deng"
    ],
    [
     "Bo",
     "Xu"
    ],
    [
     "Taiyi",
     "Huang"
    ]
   ],
   "title": "Chinese spoken language understanding across domain",
   "original": "i00_1230",
   "page_count": 4,
   "order": 57,
   "p1": "vol. 1, 230-233",
   "pn": "",
   "abstract": [
    "A robust parsing model for spontaneous Chinese based on semantic constituent spotting and concept assembling model (SCAM) had been successfully developed in our \"LOADSTAR\" dialog system. It is a travel information accessing system and the SCAM is rule based. Considering the domain portability, a statistical model for spoken language understanding is adopted. The statistical spoken language understanding model is developed in the domain of hotel reservation. Then the statistical model was ported to the domain of travel information accessing within four weeks.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-57"
  },
  "martin00_icslp": {
   "authors": [
    [
     "Sven C.",
     "Martin"
    ],
    [
     "Andreas",
     "Kellner"
    ],
    [
     "Thomas",
     "Portele"
    ]
   ],
   "title": "Interpolation of stochastic grammar and word bigram models in natural language understanding",
   "original": "i00_1234",
   "page_count": 4,
   "order": 58,
   "p1": "vol. 1, 234-237",
   "pn": "",
   "abstract": [
    "The paper shows the e\u000bects of combining a stochastic grammar with a word bigram language model by log-linear interpolation. It is divided into three main parts: The first part derives the stochastic grammar model and gives a sound theoretical motivation to incorporate word dependencies such as bigrams. The second part describes two different algorithmic approaches to the combination of both models by log-linear interpolation. The third part reports attribute error rate (AER) results measured on the Philips corpus of train time table inquiries that show a reduction of up to 9% relative.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-58"
  },
  "kogure00_icslp": {
   "authors": [
    [
     "Satoru",
     "Kogure"
    ],
    [
     "Seiichi",
     "Nakagawa"
    ]
   ],
   "title": "A portable development tool for spoken dialogue systems",
   "original": "i00_1238",
   "page_count": 4,
   "order": 59,
   "p1": "vol. 1, 238-241",
   "pn": "",
   "abstract": [
    "Speech recognition and language processing technologies have recently been improved, and speech recognition systems and dialogue systems are now in practical use. Nevertheless, not only the fundamental techniques but also the techniques for improving portability and expansibility of the systems need to be developed further. Whereas we need many user utterances to construct N-gram based language models for speech recognition. It is difficult to construct the language model for a new task/domain because many utterances cannot be collected easily. So, we propose an efficient method that is able to coustruct a language model, preparing at least only several hundreds utterances, We designed a domain-independent platform for developing any spoken dialogue systems for retrieving information from a database, and used the platform to build a literature retrieval system. When a system is implemented, the various items in the dictionary for understanding can be semi-automatically created by preparing the databases, dialogue samples and so forth.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-59"
  },
  "lin00_icslp": {
   "authors": [
    [
     "Yi-Chung",
     "Lin"
    ],
    [
     "Huei-Ming",
     "Wang"
    ]
   ],
   "title": "Error-tolerant language understanding for spoken dialogue systems",
   "original": "i00_1242",
   "page_count": 4,
   "order": 60,
   "p1": "vol. 1, 242-245",
   "pn": "",
   "abstract": [
    "The problem of speech recognition error is still the major obstacle to making spoken dialog systems be widely used in our daily lives. In this paper, we proposed an error-tolerant language understanding model to improve the robustness of dialogue systems by detecting and recovering the errors arising from speech recognition. The basic idea of our approach is using example sentences to provide the clues for detecting and recovering errors. The procedure of detection and recovery is guided by a unified scoring function which consults not only the scores from the speech recognizer and concept parser but also the scores of concept-bigram and edit operations, such as deleting, inserting and substituting concepts. Experimental results show that the proposed model effectively reduces the impact of speech recognition errors. The precision and recall rates of semantic slot are improved by 40.3% and 25.4% respectively.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-60"
  },
  "ito00_icslp": {
   "authors": [
    [
     "Akinori",
     "Ito"
    ],
    [
     "Chiori",
     "Hori"
    ],
    [
     "Masaharu",
     "Katoh"
    ],
    [
     "Masaki",
     "Kohda"
    ]
   ],
   "title": "Language modeling by stochastic dependency grammar for Japanese speech recognition",
   "original": "i00_1246",
   "page_count": 4,
   "order": 61,
   "p1": "vol. 1, 246-249",
   "pn": "",
   "abstract": [
    "This paper describes a language modeling technique using a kind of stochastic context free grammar (stochastic dependency grammar, SDG). In this work, two improvements are done upon the general CFG based SCFG model. The first improvement is to use a restricted grammar instead of general CFG. The dependency grammar used here is a restricted CFG that expresses modification between two words or phrases. The derivation probabilities are estimated by inside-outside algorithm. The computational complexity of the estimation is reduced from O(N3L3) to O(N2L3), where N and L means the number of nonterminals and length of a sentence respectively. Second, word grouping is introduced for further reduction of the estimation time. The basic idea is that regular grammar is applied within a group and CFG is used to express intergroup relationship. To achieve the idea, a new algorithm is introduced. When a group have two words in average, the learning time becomes about one-eighth. Two experiments were carried out to investigate the performance of the proposed model. In the first experiment, various kinds of SCFGs were compared using perplexity. From the result, it was found that the proposed model havemuch lower PP than the original model. As for the training speed, restricted grammar made training process twenty times faster, and the word grouping made it eight times faster. In the second experiment, the proposed model was used as a language model of LVCSR. The result showed that the proposed model was as good as bigram and trigram, and that the combination of trigram and the proposed model achieved further improvement of WER.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-61"
  },
  "zhang00b_icslp": {
   "authors": [
    [
     "Ruiqiang",
     "Zhang"
    ],
    [
     "Ezra",
     "Black"
    ],
    [
     "Andrew",
     "Finch"
    ],
    [
     "Yoshinori",
     "Sagisaka"
    ]
   ],
   "title": "A tagger-aided language model with a stack decoder",
   "original": "i00_1250",
   "page_count": 4,
   "order": 62,
   "p1": "vol. 1, 250-253",
   "pn": "",
   "abstract": [
    "This contribution of this paper is to investigate the utility of exploiting words and predicted detailed semantic tags in the long history to enhance a standard trigram language model. The paper builds on earlier work in the field that also used words and tags in the long history, but offers a cleaner, and ultimately much more accurate system by integrating the application of these new features directly into the decoding algorithm. The features used in our models are derived using a set of complex questions about the tags and words in the history, written by a linguist. Maximum entropy modelling techniques are then used to com- bine these features with a standard trigram language model. We evaluate the technique in terms of word error rate, on Wall Street Journal test data.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-62"
  },
  "hirschberg00_icslp": {
   "authors": [
    [
     "Julia",
     "Hirschberg"
    ],
    [
     "Diane",
     "Litman"
    ],
    [
     "Marc",
     "Swerts"
    ]
   ],
   "title": "Generalizing prosodic prediction of speech recognition errors",
   "original": "i00_1254",
   "page_count": 4,
   "order": 63,
   "p1": "vol. 1, 254-257",
   "pn": "",
   "abstract": [
    "Since users of spoken dialogue systems have difficulty correcting system misconceptions, it is important for automatic speech recognition (ASR) systems to know when their best hypothesis is incorrect. We compare results of previous experiments which showed that prosody improves the detection of ASR errors to experiments with a new system and new domain, the W99 conference registration system. Our new results again show that prosodic features can improve prediction of ASR misrecognitions over the use of other standard techniques for ASR rejection.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-63"
  },
  "bellegarda00_icslp": {
   "authors": [
    [
     "Jerome R.",
     "Bellegarda"
    ],
    [
     "Kim E. A.",
     "Silverman"
    ]
   ],
   "title": "Toward unconstrained command and control: data-driven semantic inference",
   "original": "i00_1258",
   "page_count": 4,
   "order": 64,
   "p1": "vol. 1, 258-261",
   "pn": "",
   "abstract": [
    "Command and control tasks are typically approached using a context-free grammar as language model. While ensuring a good system performance, this imposes a rigid framework on users, by implicitly forcing them to conform to a pre-defined interaction structure. This paper introduces the concept of data-driven semantic inference, which in principle allows for any word constructs in command/query formulation. Each unconstrained word string is automatically mapped onto the intended action through a semantic classification against the set of supported actions. The underlying (latent semantic analysis) framework relies on co-occurrences between words and commands, as observed in a training corpus. A suitable extension can also handle commands that are ambiguous at the word level. Experiments conducted on a desktop command and control task involving 113 di\u000berent actions show a classification error rate of 1.7%.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-64"
  },
  "hanazawa00_icslp": {
   "authors": [
    [
     "Ken",
     "Hanazawa"
    ],
    [
     "Shinsuke",
     "Sakai"
    ]
   ],
   "title": "Continuous speech recognition with parse filtering",
   "original": "i00_1262",
   "page_count": 4,
   "order": 65,
   "p1": "vol. 1, 262-265",
   "pn": "",
   "abstract": [
    "We propose \"parse-filtering\", a new approach to continuous speech recognition. With it, word sequence hypotheses generated on the basis of N-gram language models are verified by grammar-based parsing during the search for the best-scoring hypothesis, and unparsable hypotheses are filtered out immediately as the search proceeds. Experimental results show that this method yields a higher sentence accuracy than can be achieved with a trigram language model alone. Error reductions are, respectively, 10.0% for word error rate and 12.3% for sentence error rate.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-65"
  },
  "addadecker00_icslp": {
   "authors": [
    [
     "Martine",
     "Adda-Decker"
    ],
    [
     "Gilles",
     "Adda"
    ],
    [
     "Lori",
     "Lamel"
    ]
   ],
   "title": "Investigating text normalization and pronunciation variants for German broadcast transcription",
   "original": "i00_1266",
   "page_count": 4,
   "order": 66,
   "p1": "vol. 1, 266-269",
   "pn": "",
   "abstract": [
    "In this paper we describe our ongoing work concerning lexical modeling in the LIMSI broadcast transcription system for German. Lexical decomposition is investigated with a twofold goal: lexical coverage optimization and improved letter-to-sound conversion. A set of about 450 decompounding rules, developed using statistics from a 300M word corpus, reduces the OOV rate from 4.5% to 4.0% on a 30k development text set. Adding partial inflection stripping, the OOV rate drops to 2.9%. For letterto- sound conversion, decompounding reduces cross-lexeme ambiguities and thus contributes to more consistent pronunciation dictionaries. Another point of interest concerns reduced pronunciation modeling. Word error rates, measured on 1.3 hours of ARTE TV broadcast, vary between 18 and 24% depending on the show and the system configuration. Our experiments indicate that using reduced pronunciations slightly decreases word error rates.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-66"
  },
  "wester00_icslp": {
   "authors": [
    [
     "Mirjam",
     "Wester"
    ],
    [
     "Eric",
     "Fosler-Lussier"
    ]
   ],
   "title": "A comparison of data-derived and knowledge-based modeling of pronunciation variation",
   "original": "i00_1270",
   "page_count": 5,
   "order": 67,
   "p1": "vol. 1, 270-273",
   "pn": "",
   "abstract": [
    "This paper focuses on modeling pronunciation variation in two different ways: data-derived and knowledge-based. The knowledge-based approach consists of using phonological rules to generate variants. The data-derived approach consists of performing phone recognition, followed by various pruning and smoothing methods to alleviate some of the errors in the phone recognition. Using phonological rules led to a small improvement in WER; whereas, using a data-derived approach in which the phone recognition was smoothed using simple decision trees (d-trees) prior to lexicon generation led to a significant improvement compared to the baseline. Furthermore, we found that 10% of variants generated by the phonological rules were also found using phone recognition, and this increased to 23% when the phone recognition output was smoothed by using d-trees. In addition, we propose a metric to measure confusability in the lexicon and we found that employing this confusion metric to prune variants results in roughly the same improvement as using the d-tree method.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-67"
  },
  "kessens00_icslp": {
   "authors": [
    [
     "Judith M.",
     "Kessens"
    ],
    [
     "Helmer",
     "Strik"
    ],
    [
     "Catia",
     "Cucchiarini"
    ]
   ],
   "title": "A bottom-up method for obtaining information about pronunciation variation",
   "original": "i00_1274",
   "page_count": 4,
   "order": 68,
   "p1": "vol. 1, 274-277",
   "pn": "",
   "abstract": [
    "In this paper a Bottom-Up (BU) method of obtaining information about pronunciation variation is proposed. BU transcriptions (Tbu) were obtained by letting a CSR decide for each phone whether it was deleted or not. The Tbu were compared to transcriptions obtained automatically with a Top- Down method, and the agreement appeared to be very high. Subsequently, the Tbu were aligned with canonical reference transcriptions (Tref) and on the basis of this alignment, deletion rules were derived. The BU rules were employed to generate variants which were used in recognition experiments. The results of these recognition experiments show that the information about pronunciation variation obtained using the BU method can be used to improve recognition performance.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-68"
  },
  "zhang00c_icslp": {
   "authors": [
    [
     "Jiyong",
     "Zhang"
    ],
    [
     "Fang",
     "Zheng"
    ],
    [
     "Mingxing",
     "Xu"
    ],
    [
     "Ditang",
     "Fang"
    ]
   ],
   "title": "Semi-continuous segmental probability modeling for continuous speech recognition",
   "original": "i00_1278",
   "page_count": 5,
   "order": 69,
   "p1": "vol. 1, 278-281",
   "pn": "",
   "abstract": [
    "In this paper the design of semi-continuous segmental probability models (SCSPMs) in large vocabulary continuous speech recognition is presented. The tied Gaussian densities are trained using data from all states of all utterances while the mixture weights are estimated using data from the state being trained individually. The SCSPMs tie all the densities of all states from all Speech Recognition Units (SRUs) to form a shared pdf codebook, thus the number of Gaussian densities is greatly reduced. Several pruning methods are reviewed and then a new pruning criterion is proposed in order to reduce the number of tied mixture Gaussian densities while there is only a small subset of mixture Gaussian densities with larger tying weights. Our preliminary experiments show that the SCSPM incorporated with the pruning techniques can lessen the size of model storage and speed up the system with little degradation in the accuracy compared to the prior continuous model.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-69"
  },
  "antoniou00_icslp": {
   "authors": [
    [
     "Christos A.",
     "Antoniou"
    ],
    [
     "T. Jeff",
     "Reynolds"
    ]
   ],
   "title": "Acoustic modelling using modular/ensemble combinations of heterogeneous neural networks",
   "original": "i00_1282",
   "page_count": 4,
   "order": 70,
   "p1": "vol. 1, 282-285",
   "pn": "",
   "abstract": [
    "We have been investigating for some time the use of modular/ensemble neural networks to model phones, a commonly chosen acoustic unit for speech. We have demonstrated the advantage of using separately trained MLPs to estimate each phone's probability, posterior on a sequence of feature vectors representing the expression of the phone over some window in time. In this paper we show how MLPs trained on different feature vectors, derived from different pre-processing techniques, may be combined to produce better estimates of phone posteriors and hence lower word error rates. We also show how calculated broad-class posterior probabilities may be used to provide contextual information to train further nets. The combination of these techniques results in significant improvements for phone classification and word error rates on the TIMIT corpus.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-70"
  },
  "hon00_icslp": {
   "authors": [
    [
     "Hsiao-Wuen",
     "Hon"
    ],
    [
     "Shankar",
     "Kumar"
    ],
    [
     "Kuansan",
     "Wang"
    ]
   ],
   "title": "Unifying HMM and phone-pair segment models",
   "original": "i00_1286",
   "page_count": 4,
   "order": 71,
   "p1": "vol. 1, 286-289",
   "pn": "",
   "abstract": [
    "It is well known that HMM is ineffective in modeling the dynamics of speech due to the piecewise stationary and the independent observation assumptions. In this paper, we propose an analytically tractable framework in which the two modeling techniques are combined to reach a jointly optimal decision in both training and recognition. The combination is achieved by coupling the hidden processes from the HMM and the segment model. To take the full advantage of the segmental approach, phone-pair units are used as the basic acoustic units for segment models. In addition, we construct context-dependent phone-pair units to account for acoustic variations in context. The superior quality of phone-pair segment models contributes to an 8.2% reduction in error rates on the WSJ dictation task.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-71"
  },
  "li00_icslp": {
   "authors": [
    [
     "Ming",
     "Li"
    ],
    [
     "Tiecheng",
     "Yu"
    ]
   ],
   "title": "Multi-group mixture weight HMM",
   "original": "i00_1290",
   "page_count": 3,
   "order": 72,
   "p1": "vol. 1, 290-292",
   "pn": "",
   "abstract": [
    "This paper presents a new modeling method of the continuous density Hidden Markov Model. As we know, speech signal is characterized by a hidden state sequence and each state is described by the mixture of weighted Gaussian density functions. Usually if we want to describe speech signal more precisely, we need to use more Gaussian functions for each state. But it will increase the computation significantly. On the other hand, the weight of each Gaussian component is the statistical average of Gaussian component probabilities for the whole training data. So it just can depict the average characteristics of speech signal. For some speech signal these weights are not proper in fact. Therefore, we propose Multi-group Mixture Weight HMM to solve this problem. In this kind of HMM, each state has several groups of mixture weight for the Gaussian components and it only needs very little additional computation. In our experiments, it achieved 12% reduction for errors.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-72"
  },
  "kitazoe00_icslp": {
   "authors": [
    [
     "Tetsuro",
     "Kitazoe"
    ],
    [
     "Tomoyuki",
     "Ichiki"
    ],
    [
     "Makoto",
     "Funamori"
    ]
   ],
   "title": "Application of pattern recognition neural network model to hearing system for continuous speech",
   "original": "i00_1293",
   "page_count": 4,
   "order": 73,
   "p1": "vol. 1, 293-296",
   "pn": "",
   "abstract": [
    "The two or three layered networks 2LNN, 3LNN which originate from stereovision neural network are applied to speech recognition. To accommodate sequential data flow, we consider a window to which new acoustic data enter and from which final neural activities are output. Inside the window recurrent neural network develops neural activity toward a stable point. The process is called Winner-Take-All(WTA) with cooperation and competition. The resulting neural activities clearly showed recognition of a continuous speech of a word. The string of phonemes obtained is compared with reference words by using dynamical programming method. The resulting recognition rate amounts to 96.7% for 100 words spoken by 9 male speakers, which is compared to 97.9% by hidden markov model (HMM) with three states and single gaussian distribution. The present results which are close to those of HMM seem noticeable because the architecture of the neural network is very simple and parameters in the neural net equations are small numbered and always fixed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-73"
  },
  "smith00_icslp": {
   "authors": [
    [
     "Nathan",
     "Smith"
    ],
    [
     "Mahesan",
     "Niranjan"
    ]
   ],
   "title": "Data-dependent kernels in svm classification of speech patterns",
   "original": "i00_1297",
   "page_count": 4,
   "order": 74,
   "p1": "vol. 1, 297-300",
   "pn": "",
   "abstract": [
    "Support Vector Machines (SVMs) have recently proved to be powerful pattern classification tools with a strong connection to statistical learning theory. One of the hurdles to using SVMs in speech recognition, and a crucial aspect of SVM design in general, is the choice of the kernel function for non-separable data, and the setting of its parameters. This is often based on experience or a potentially costly search. This paper gives some experimental justification for the Fisher kernels proposed in [1]; kernels are obtained and their extra regularisation and use of labelled and unlabelled data discussed. Fisher kernels are derived from generative probability models of the data, and are a first step to implementing kernels for variable length sequences.\n",
    "",
    "",
    "T. Jaakkola and D. Haussler. Exploiting generative models in discriminative classifiers. In M.S. Kearns, S.A. Solla, and D.A. Cohn, editors, Advances in Neural Information Processing Systems 11. MIT Press, 1999.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-74"
  },
  "umesh00_icslp": {
   "authors": [
    [
     "S.",
     "Umesh"
    ],
    [
     "Richard C.",
     "Rose"
    ],
    [
     "S.",
     "Parthasarathy"
    ]
   ],
   "title": "Exploiting frequency-scaling invariance properties of the scale transform for automatic speech recognition",
   "original": "i00_1301",
   "page_count": 4,
   "order": 75,
   "p1": "vol. 1, 301-304",
   "pn": "",
   "abstract": [
    "An experimental study of the application of scale-transform to improve the performance of speaker independent continuous speech recognition, is presented in this paper. Three major results are described. First, a comparison was made between the scale-transform based magnitude cepstrum coeÆcients (STCC) and mel-scale filter bank cepstrum coefficients (MFCC) on a telephone based connected digit recognition task. It was shown that the STCC can obtain a performance that is close to that of the MFCC. Second, a simple frequency-normalization procedure was applied to the scale-transform representation that improved performance on the connected digit recognition task with respect to theMFCC. Finally, in a more controlled experimental setting using the TIMIT database, it was shown that the application of phone-specific frequency warpings improved phone classification performance over using a single speaker-specific warping. This last result may have general implications for all frequency warping based speaker normalization procedures.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-75"
  },
  "fujimoto00_icslp": {
   "authors": [
    [
     "Masahiro",
     "Fujimoto"
    ],
    [
     "Jun",
     "Ogata"
    ],
    [
     "Yasuo",
     "Ariki"
    ]
   ],
   "title": "Large vocabulary continuous speech recognition under real environments using adaptive sub-band spectral subtraction",
   "original": "i00_1305",
   "page_count": 4,
   "order": 76,
   "p1": "vol. 1, 305-308",
   "pn": "",
   "abstract": [
    "In this study, we propose an Adaptive Sub-Band Spectral Subtraction (ASBSS) method which can vary noise subtraction rate according to SNR in frequency bands at each frame. In the conventional Spectral Subtraction(SS), speech spectral is estimated by adjusting noise subtraction rate according to SNR. In general, SNR is defined and computed as the average over all the input speech signal. However, even if the noise is stationary, SNR varies according to speech energy. Therefore the subtraction rate of noise spectral should be adjusted according to the segmental SNR. This method is called Adaptive SS(ASS). Considering difference of spectral features such as vowel and consonant, the subtraction rate of noise spectral should be adjusted according to the sub-band SNR. This idea leads to the ASBSS method we propose in this paper. In order to evaluate the proposed method, we carried out Large Vocabulary Continuous Speech Recognition experiments and compared the results by our method with the conventional method in word accuracy.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-76"
  },
  "gu00_icslp": {
   "authors": [
    [
     "Liang",
     "Gu"
    ],
    [
     "Kenneth",
     "Rose"
    ]
   ],
   "title": "Perceptual harmonic cepstral coefficients as the front-end for speech recognition",
   "original": "i00_1309",
   "page_count": 4,
   "order": 77,
   "p1": "vol. 1, 309-312",
   "pn": "",
   "abstract": [
    "Perceptual harmonic cepstral coefficients (PHCC) are proposed as features to extract for speech recognition. Pitch estimation and classification into voiced, unvoiced, and transitional speech are performed by a spectro-temporal auto-correlation technique. A peak picking algorithm is then employed to precisely locate pitch harmonics. A weighting function, which depends on the classification and the pitch harmonics, is applied to the power spectrum and ensures accurate representation of the voiced speech spectral envelope. The harmonics weighted power spectrum undergoes mel-scaled band-pass filtering, and the logenergy of the filters output is discrete cosine transformed to produce cepstral coefficients. For perceptual considerations, within-filter cubic-root amplitude compression is applied to reduce amplitude variation without compromise of the gain invariance properties. Experiments show substantial recognition gains of PHCC over MFCC, with 48% and 15% error rate reduction for the Mandarin digit database and E-set, respectively.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-77"
  },
  "tam00_icslp": {
   "authors": [
    [
     "Yik-Cheung",
     "Tam"
    ],
    [
     "Brian",
     "Mak"
    ]
   ],
   "title": "Optimization of sub-band weights using simulated noisy speech in multi-band speech recognition",
   "original": "i00_1313",
   "page_count": 4,
   "order": 78,
   "p1": "vol. 1, 313-316",
   "pn": "",
   "abstract": [
    "Recently multi-band speech recognition has been proposed to improve robustness under environmental noises. One important issue is how to combine decisions from individual sub-band recognizers to arrive at a final decision. Under the hidden Markov modeling (HMM) framework, one common approach is combining sub-band likelihoods linearly in an optimal manner so that the more reliable sub-bands are emphasized and the corrupted sub-bands are de-emphasized. In our experience, estimating the weights from clean speech is not e\u000bective as the weights are not optimal under noisy environments. In this paper, we derive the optimal weights from simulated noisy speech using discriminative training method with minimum classification errors (MCE) or maximum mutual information (MMI) as the cost function. The methods are evaluated on recognition of isolated TI digits. Compared with full-band recognition with noises at an SNR of 0dB, multi-band recognition with MCE-derived weights reduces word errors by 45.9% on a tone noise, and an average of 17.9% on three real noises. MCE-derived weights and MMI-derived weights have similar performance, and are much better than weights derived from other means.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-78"
  },
  "faltlhauser00_icslp": {
   "authors": [
    [
     "Robert",
     "Faltlhauser"
    ],
    [
     "Thilo",
     "Pfau"
    ],
    [
     "Günther",
     "Ruske"
    ]
   ],
   "title": "On the use of speaking rate as a generalized feature to improve decision trees",
   "original": "i00_1317",
   "page_count": 4,
   "order": 79,
   "p1": "vol. 1, 317-320",
   "pn": "",
   "abstract": [
    "Decision trees are probably the most common way for gen- erating models for phonemes in their phonetic context. In this paper we investigated several ways how speaking rate information can be integrated in the decision tree process. We basically focused on two approaches: on the one hand a speaking rate feature included in the decision tree itself and on the other hand a pruning approach for creating individual model sets. Recently, some papers have come up with the idea to include a gender feature already in the decision process. In our paper we went a step further and wanted to see whether speaking rate can be a fruitful extension to decision trees. Experiments have shown that the introduction of speaking rate leads to improvements in combination with a general gender feature. Further experi- ments with di\u000berent pruning strategies aimed at creating adequate model sets for di\u000berent speaking rate categories.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-79"
  },
  "toyama00_icslp": {
   "authors": [
    [
     "Jun",
     "Toyama"
    ],
    [
     "Masaru",
     "Shimbo"
    ]
   ],
   "title": "Syllable recognition using glides based on a non-linear transformation",
   "original": "i00_1321",
   "page_count": 4,
   "order": 80,
   "p1": "vol. 1, 321-324",
   "pn": "",
   "abstract": [
    "The steady part of a phoneme becomes short or is sometimes even lost as its utterance speed increases or in natural conversation. Thus, a robust recognition model can be constructed by focusing on the glide passage parts in speech instead of on the phonemic stationary parts. On the other hand, the trajectories in feature space are generally curved. If an increase in likelihood corresponds to an increase in the degree of recognition accuracy, it is desirable that the curve-like trajectories are transformed into straight lines. In addition, an increase in the degree of recognition accuracy can be expected since degeneration of the feature space can be avoided by imposing the constraint that the distance between the centers of distributions is large. The results of computational evaluation tests on CV syllable glides for 50 ms showed syllable recognition rate of 87.1%.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-80"
  },
  "sonmez00_icslp": {
   "authors": [
    [
     "Kemal",
     "Sönmez"
    ],
    [
     "Madelaine",
     "Plauché"
    ],
    [
     "Elizabeth",
     "Shriberg"
    ],
    [
     "Horacio",
     "Franco"
    ]
   ],
   "title": "Consonant discrimination in elicited and spontaneous speech: a case for signal-adaptive front ends in ASR",
   "original": "i00_1325",
   "page_count": 4,
   "order": 81,
   "p1": "vol. 1, 325-328",
   "pn": "",
   "abstract": [
    "The constant frame length in typical ASR front ends is too long to capture transient phenomena in speech, such as stop bursts. However, current HMM systems have consistently outperformed systems based solely on non-uniform units. This work investigates an approach to \"add back\" such transient information to a speech recognizer, without losing the robustness of the standard acoustic models. We demonstrate a set of phonetically-motivated acoustic features that discriminate a preliminary test set of highly ambiguous voiceless stops in CV contexts. The features are automatically computed from data that had been hand-marked for consonant burst location and voicing onset (extension to automatic marking is also proposed). Two corpora are processed using a parallel set of features: conversational speech over the telephone (Switchboard), and a corpus of carefully elicited speech. The latter provides an upper bound on discrimination, and allows for comparison of feature usage across speaking style. We explore data-driven approaches to obtaining variable-length time-localized features compatible with an HMM statistical framework. We also suggest techniques for extension to automatic annotation of burst location, for computation of features at such points, and for augmentation of an HMM system with the added information.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-81"
  },
  "daoudi00_icslp": {
   "authors": [
    [
     "Khalid",
     "Daoudi"
    ],
    [
     "Dominique",
     "Fohr"
    ],
    [
     "Christophe",
     "Antoine"
    ]
   ],
   "title": "A new approach for multi-band speech recognition based on probabilistic graphical models",
   "original": "i00_1329",
   "page_count": 4,
   "order": 82,
   "p1": "vol. 1, 329-332",
   "pn": "",
   "abstract": [
    "In this paper, we introduce a new approach for multi-band speech recognition which allows interaction between sub-bands and does not require a recombination step. Moreover, this approach is a natural generalization of the HMMs paradigm and leads to fast learning and recognition algorithms.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-82"
  },
  "glotin00_icslp": {
   "authors": [
    [
     "Hervé",
     "Glotin"
    ],
    [
     "Frédéric",
     "Berthommier"
    ]
   ],
   "title": "Test of several external posterior weighting functions for multiband full combination ASR",
   "original": "i00_1333",
   "page_count": 4,
   "order": 83,
   "p1": "vol. 1, 333-336",
   "pn": "",
   "abstract": [
    "Information about speech reliability can be extracted and then integrated in a recogniser by various means. The full combination (FC) approach allows the weight- ing of the posterior values estimated locally in the time frequency representation, according a speech reliability measure. Since most of the speech segments are voiced, we use a method exploiting the harmonicity of speech tos derive these weights. We test this method together with the direct integration of the a priori SNR. Then, we run speech recognition with di\u000berent kind of weighting functions. The weights are continuous or binary values. This corresponds to a soft or to a hard decision function about the speech reliability, which is derived from an observable harmonicity index. Using a binary decision process, the e\u000bect is, for each time frame, to collapse the set of combinations of sub-bands into a single com- bination. On the other hand, we substitute empirical values to these terms, including functions of the a priori SNR, which are continuous or discrete, but not based on a probabilistic estimation. We establish the average scores in % WER for a panel of noises at di\u000berent levels, stationary or not, narrow-band or wide-band. All these functions are found to be sub-optimal comparatively to the constant weighting, but a robustness of the FC for narrow-band noises is observed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-83"
  },
  "okada00_icslp": {
   "authors": [
    [
     "Kanji",
     "Okada"
    ],
    [
     "Takayuki",
     "Arai"
    ],
    [
     "Noburu",
     "Kanederu"
    ],
    [
     "Yasunori",
     "Momomura"
    ],
    [
     "Yuji",
     "Murahara"
    ]
   ],
   "title": "Using the modulation wavelet transform for feature extraction in automatic speech recognition",
   "original": "i00_1337",
   "page_count": 4,
   "order": 84,
   "p1": "vol. 1, 337-340",
   "pn": "",
   "abstract": [
    "In this paper, we examine robust feature extraction methods for automatic speech recognition (ASR) in noise-distorted environments. Several perceptual experiments have shown that the range between 1 and 16 Hz of modulation frequency band is important for human speech recognition. Furthermore it has been reported the same modulation frequency band is important for ASR. Combining the coefficients of multi-resolutional Fourier transform to split the important modulation frequency band for ASR into several bands especially increased recognition performance. Combining coefficients of a multi-resolutional Fourier transform corresponds to a wavelet transform. To test the effectiveness and efficiency of the wavelet transform we, therefore, applied the wavelet transform to recognition experiments. This approach yielded an average of 3% increase in recognition accuracy compared to the standard approach using mel-frequency cepstral coefficients (MFCC) in several noise-distorted environments.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-84"
  },
  "zhu00_icslp": {
   "authors": [
    [
     "Qifeng",
     "Zhu"
    ],
    [
     "Abeer",
     "Alwan"
    ]
   ],
   "title": "AM-demodulation of speech spectra and its application io noise robust speech recognition",
   "original": "i00_1341",
   "page_count": 5,
   "order": 85,
   "p1": "vol. 1, 341-344",
   "pn": "",
   "abstract": [
    "In this paper, a novel algorithm that resembles amplitude demodulation in the frequency domain is introduced, and its application to automatic speech recognition (ASR) is studied. Speech production can be regarded as a result of amplitude modulation (AM) with the source (excitation) spectrum being the carrier and the vocal tract transfer function (VTTF) being the modulating signal. From this point of view, the VTTF can be recovered by amplitude demodulation. Amplitude demodulation of the speech spectrum is achieved by a novel nonlinear technique, which effectively performs envelope detection by using amplitudes of the harmonics and discarding inter-harmonic valleys. The technique is noise robust since frequency bands of low energy are discarded. The same principle is used to reshape the detected envelope. The algorithm is then used to construct an ASR feature extraction module. It is shown that this technique achieves superior performance to MFCCs in the presence of additive noise. Recognition accuracy is further improved if peak isolation is also performed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-85"
  },
  "hagen00_icslp": {
   "authors": [
    [
     "Astrid",
     "Hagen"
    ],
    [
     "Andrew",
     "Morris"
    ]
   ],
   "title": "Comparison of HMM experts with MLP experts in the full combination multi-band approach to robust ASR",
   "original": "i00_1345",
   "page_count": 4,
   "order": 86,
   "p1": "vol. 1, 345-348",
   "pn": "",
   "abstract": [
    "In this paper we apply the Full Combination (FC) multi-band approach, which has originally been introduced in the framework of posterior-based HMM/ANN (Hidden Markov Model/Artificial Neural Network) hybrid systems, to systems in which the ANN (or Multilayer Perceptron (MLP)) is itself replaced by a Multi Gaussian HMM (MGM). Both systems represent the most widely used statistical models for robust ASR (automatic speech recognition). It is shown how the FC formula for the likelihood-based MGMs can easily be derived from the posterior-based approach by simply applying Bayes Rule. The experiments show that the Full Combination multi-band system with MGM experts performs better, in all noise conditions tested, than the simple sum and product rules which are normally used. As compared to the baseline full-band system, the FC system shows increased robustness mainly on band-limited noise. The goal of this article is not a performance comparison between Multilayer Perceptrons and Multi Gaussian Models but between the theory of the two approaches, posterior-based vs. likelihood-based FC approach, so results are only given for the MGMs.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-86"
  },
  "hagen00b_icslp": {
   "authors": [
    [
     "Astrid",
     "Hagen"
    ],
    [
     "Hervé",
     "Bourlard"
    ]
   ],
   "title": "Using multiple time scales in the framework of multi-stream speech recognition",
   "original": "i00_1349",
   "page_count": 4,
   "order": 87,
   "p1": "vol. 1, 349-352",
   "pn": "",
   "abstract": [
    "In this paper, we present a new approach to incorporating multiple time scale information as independent streams in multi-stream processing. To illustrate the procedure, we take two different sets of multiple time scale features. In the first system, these are features extracted over variable sized windows of three and five times the original window size. In the second system, we take as separate input streams the commonly used difference features, i.e. the first and second order derivatives of the instantaneous features. In the same way, any other kinds of multiple time scale features could be employed. The approach is embedded in the recently introduced \"full combination\" approach to multi-stream processing in which, the phoneme probabilities from all possible combinations of streams are combined in a weighted sum. As an extension of this approach we have found that replacing the sum of probabilities by their product, in the same \"all wise\" context, can result in higher robustness. Capturing different information in each stream, and with the longer time scale features being more robust to noise, the multiple time scale multi-stream system gained a significant performance improvement in both clean speech and in real-environmental noise.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-87"
  },
  "yu00_icslp": {
   "authors": [
    [
     "Hua",
     "Yu"
    ],
    [
     "Alex",
     "Waibel"
    ]
   ],
   "title": "Streamlining the front end of a speech recognizer",
   "original": "i00_1353",
   "page_count": 4,
   "order": 88,
   "p1": "vol. 1, 353-356",
   "pn": "",
   "abstract": [
    "In this paper we seek to streamline various operations within the front end of a speech recognizer, both to reduce unnecessary computation and to simplify the conceptual framework. First, a novel view of the front end in terms of linear transformations is presented. Then we study the invariance property of recognition performance with respect to linear transformations (LT) at the front end. Analysis reveals that several LT steps can be consolidated into a single LT, which effectively eliminates the Discrete Cosine Transform (DCT) step, part of the traditional MFCC (Mel-Frequency Cepstral Coefficient) front end. Moreover, a highly simplified, data-driven front-end scheme is proposed as a direct generalization of this idea. The new setup has no Mel-scale filtering, another part of the MFCC front end. Experimental results show a 5% relative improvement on the Broadcast News task.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-88"
  },
  "raj00_icslp": {
   "authors": [
    [
     "Bhiksha",
     "Raj"
    ],
    [
     "Michael L.",
     "Seltzer"
    ],
    [
     "Richard M.",
     "Stern"
    ]
   ],
   "title": "Reconstruction of damaged spectrographic features for robust speech recognition",
   "original": "i00_1357",
   "page_count": 4,
   "order": 89,
   "p1": "vol. 1, 357-360",
   "pn": "",
   "abstract": [
    "We present two missing-feature based algorithms that recover noise-corrupted regions of spectrographic representations of speech for noise-robust speech recognition. These algorithms modify the incoming feature vector without any changes to the speech recognition system, in contrast to previously-described approaches. The first approach clusters the feature vectors representing clean speech. Missing data are recovered by estimating the spectral cluster in each analysis frame based on the uncorrupted feature values. The second approach uses MAP procedures to estimate the values of missing data elements based on their correlations with the features that are present. Both methods take into account bounds on the clean spectrogram implied by the noisy spectrogram. Large improvements in recognition accuracy are observed when these methods are used on speech corrupted by non-stationary noise when the locations of the corrupt regions of the spectrogram are known. We also present a new method of estimating the locations of corrupt regions in spectrograms that treats the problem of identifying these regions as one of Bayesian classification. This method, when used along with the best method to reconstruct them, results in recognition accuracies comparable with the best previous data compensation algorithm on speech corrupted by white noise. It also provides significant improvement on speech corrupted by music when the global SNR of the corrupted signal is known a priori.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-89"
  },
  "sturm00_icslp": {
   "authors": [
    [
     "Janienke",
     "Sturm"
    ],
    [
     "Hans",
     "Kamperman"
    ],
    [
     "Lou",
     "Boves"
    ],
    [
     "Els den",
     "Os"
    ]
   ],
   "title": "Impact of speaking style and speaking task on acoustic models",
   "original": "i00_1361",
   "page_count": 4,
   "order": 90,
   "p1": "vol. 1, 361-364",
   "pn": "",
   "abstract": [
    "The loss in performance caused by mismatch between train and test material suggests a need for task specific acoustic models, especially for highly demanding tasks. However, since the training of these models is extremely expensive, general purpose models are more attractive. In this paper we address the impact of mismatch in speaking style and task. We trained three sets of acoustic models on data from different tasks, involving both read and extemporaneous speech. The average utterance length in the training corpora varied between 10.5 and 1.2 words. The models were tested on matched as well on very different tasks. The results suggest that general purpose models trained from short utterances are to be preferred in most spoken dialog systems. However, these models might not perform adequately in dictation tasks.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-90"
  },
  "kadambe00_icslp": {
   "authors": [
    [
     "Shubha",
     "Kadambe"
    ],
    [
     "Ron",
     "Burns"
    ]
   ],
   "title": "Encoded speech recognition accuracy improvement in adverse environments by enhancing formant spectral bands",
   "original": "i00_1365",
   "page_count": 4,
   "order": 91,
   "p1": "vol. 1, 365-368",
   "pn": "",
   "abstract": [
    "Spoken dialogue information retrieval applications are the future trend for mobile users in automobiles, on cellular phones, etc. Due to the limitation of resources in these platforms, it may be advantageous to extract speech features, and compress and transmit them to a central hub where the computation intensive tasks such as speech recognition and speech understanding, etc. can be performed. Generally, the speech recognition accuracy degrades when the decoded speech signal (that is obtained after re-synthesizing the signal from the compressed features) is used. In addition, the background noise that is present in the above mentioned mobile systems will reduce the recognition accuracy. Therefore, in order to improve the recognition accuracy it is essential to extract robust features that can jointly optimize compression and recognition. In this paper, we describe a technique that improves the recognition accuracy of noisy encoded speech signals by performing spectral correction and spectral formant band enhancement before synthesizing the speech signal from the compressed features. We have conducted experiments on 1831 telephone speech utterances from 1831 speakers. We added (a) the invehicle noise recorded from a Volvo car moving on an asphalt road at 134 kmph, (b) the factory noise recorded in a factory and (c) the speech (babble) noise recorded in a cafeteria to these utterances at various signal-to-noise ratios (SNR). Our experimental results indicate recognition accuracy improvement up to 10% at 0 dB SNR.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-91"
  },
  "barker00_icslp": {
   "authors": [
    [
     "Jon",
     "Barker"
    ],
    [
     "Ljubomir",
     "Josifovski"
    ],
    [
     "Martin",
     "Cooke"
    ],
    [
     "Phil",
     "Green"
    ]
   ],
   "title": "Soft decisions in missing data techniques for robust automatic speech recognition",
   "original": "i00_1373",
   "page_count": 4,
   "order": 92,
   "p1": "vol. 1, 373-376",
   "pn": "",
   "abstract": [
    "In previous work we have developed the theory and demonstrated the promise of the Missing Data approach to robust Automatic Speech Recognition. This technique is based on hard decisions as to whether each time-frequency \"pixel\" is either reliable or unreliable. In this paper we replace these discrete decisions with soft estimates of the probability that each \"pixel\" is reliable. We adapt the probability calculation to use these estimates as weighting factors for the complementary reliable/unreliable interpretations for each feature vector component. Experiments using the TIDigits connected digit recognition task demonstrate that this technique a\u000bords significant performance improvements at low SNRs.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-92"
  },
  "liu00_icslp": {
   "authors": [
    [
     "Jian",
     "Liu"
    ],
    [
     "Tiecheng",
     "Yu"
    ]
   ],
   "title": "New tone recognition methods for Chinese continuous speech",
   "original": "i00_1377",
   "page_count": 4,
   "order": 93,
   "p1": "vol. 1, 377-380",
   "pn": "",
   "abstract": [
    "in this paper, we present two tone recognition methods for Chinese continuous speech and the way of integrating methods with the HMM-based speech recognition system. The first method we call it Gaussian Mixture Tone Models (GMTM) method, the second is tone Clustering-Mapping (CM) method. Both of them are statistic methods. At the end of the paper, we will also provide the comparison result of these two methods with the tone classification accuracy tested on the Chinese continuous speech database of national \"863 project.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-93"
  },
  "zhang00d_icslp": {
   "authors": [
    [
     "Bo",
     "Zhang"
    ],
    [
     "Gang",
     "Peng"
    ],
    [
     "William S.-Y.",
     "Wang"
    ]
   ],
   "title": "Reliable bands guided similarity measure for noise-robust speech recognition",
   "original": "i00_1381",
   "page_count": 4,
   "order": 94,
   "p1": "vol. 1, 381-384",
   "pn": "",
   "abstract": [
    "Under noisy conditions, due to the redundancy of speech signal, there are some spectral bands (Reliable Bands) whose local SNRs are high enough to be used effectively by a recognizer. A novel, phonetically motivated Reliable Bands Guided similarity measure (RBG measure) is proposed in this study. It has the following features. Firstly, for reference spectrum, frequency bands which have larger absolute energy or sharper spectral peaks are marked as reliable bands. They are to be given more weight than the other bands in the definition of the RBG measure. Secondly, within each reliable band, similarity between formant positions and formant shapes of test spectrum and reference spectrum is explicitly modelled. Lastly, the measure can automatically emphasize spectral bands whose amplitudes change abruptly, which normally contain more reliable dynamic features of the speech signal. Both the RBG measure and the Parallel Model Combination (PMC) method are tested on a speaker-independent, continuous Mandarin digit string recognition task, under 15 noisy conditions. Noises are drawn from the NOISEX92 database. The RBG measure shows an average 4.22% word accuracy score below the PMC method above 0 dB. However, it outperforms the PMC method by 8.82% at 0 dB. More importantly,the RBG measure does not rely on accurate background noise modeling, which is a difficult task in itself.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-94"
  },
  "nitta00_icslp": {
   "authors": [
    [
     "Tsuneo",
     "Nitta"
    ],
    [
     "Masashi",
     "Takigawa"
    ],
    [
     "Takashi",
     "Fukuda"
    ]
   ],
   "title": "A novel feature extraction using multiple acoustic feature planes for HMM-based speech recognition",
   "original": "i00_1385",
   "page_count": 5,
   "order": 95,
   "p1": "vol. 1, 385-388",
   "pn": "",
   "abstract": [
    "This paper describes an attempt to extract multiple peripheral features of a point x(ti,fj) on a timespectrum (TS) pattern by observing n´n neighborhoods of the point, and to incorporate these peripheral features (MPFPs: multiple peripheral feature planes) into the feature extractor of a speech recognition system together with MFCC parameters. Two types of peripheral feature extractor, MPFP-KL and MPFP-LR, are proposed. MPFP-KL adopts the orthogonal bases extracted directly from speech data by using KLT of 7x7 blocks on TS patterns. In MPFP-LR, the upper two primal bases are selected and simplified in the form of Δt-operator and Δfoperator obtained by linear regression calculation. MPFP-KL and MPFP-LR show significant improvements in comparison with the standard MFCC feature extractor in experiments with the HMM-based ASR system.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-95"
  },
  "zheng00b_icslp": {
   "authors": [
    [
     "Fang",
     "Zheng"
    ],
    [
     "Guoliang",
     "Zhang"
    ]
   ],
   "title": "Integrating the energy information into MFCC",
   "original": "i00_1389",
   "page_count": 4,
   "order": 96,
   "p1": "vol. 1, 389-392",
   "pn": "",
   "abstract": [
    "The Mel-Frequency Cepstrum Coefficients (MFCC) is a widely used set of feature used in automatic speech recognition systems introduced in 1980 by Davis and Mermelstein [1]. In this traditional implementation, the 0th coefficient is excluded for the reason it is somewhat unreliable. In this paper, we analyze this term and find that it can be regarded as the generalized frequency band energy (FBE) and is hence useful, resulting in the FBE-MFCC. We also propose a better analysis, called the auto-regressive analysis, on the frame energy, which performs better than its 1st and/or 2nd order differential derivatives. Experiments show that, the FBE-MFCC and the frame energy with their corresponding auto-regressive analysis coefficients form the better combination reducing the syllable error rate (SER) by 10.0% across a giant speech database, compared to the traditional MFCC with its corresponding autoregressive analysis coefficients.\n",
    "",
    "",
    "Davis, S.B. and Mermelstein, P. (1980), Comparison of parametric representation for monosyllabic word recognition in continuously spoken sentences, IEEE Trans. on ASSP, Aug. 1980.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-96"
  },
  "farooq00_icslp": {
   "authors": [
    [
     "Omar",
     "Farooq"
    ],
    [
     "Sekharjit",
     "Datta"
    ]
   ],
   "title": "Speaker independent phoneme recognition by MLP using wavelet features",
   "original": "i00_1393",
   "page_count": 4,
   "order": 97,
   "p1": "vol. 1, 393-396",
   "pn": "",
   "abstract": [
    "Feature extraction is one of the most important tasks in speech recognition system. Most of the speech recognition systems use Short Time Fourier Transform (STFT) for the derivation of features from the spoken utterances. In this paper we try to exploit the higher time-frequency resolution property of Discrete Wavelet Transform (DWT) for extraction of speaker independent features. The features are extracted every 8ms to account for the faster changes in the phoneme. These features are then used to train a Multi-Layer Perceptron (MLP) classifier for the recognition of phonemes.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-97"
  },
  "couvreur00_icslp": {
   "authors": [
    [
     "Laurent",
     "Couvreur"
    ],
    [
     "Christophe",
     "Couvreur"
    ],
    [
     "Christophe",
     "Ris"
    ]
   ],
   "title": "A corpus-based approach for robust ASR in reverberant environments",
   "original": "i00_1397",
   "page_count": 4,
   "order": 98,
   "p1": "vol. 1, 397-400",
   "pn": "",
   "abstract": [
    "In this paper, we discuss the use of artificial room reverberation to increase the performance of automatic speech recognition (ASR) systems in reverberant enclosures. Our approach consists in training acoustic models on artificially reverberated speech material. In order to obtain the desired reverberated speech training database, we propose to use a reverberating filter whose impulse response is designed to match two high-level acoustic properties of the target reverberant operating environment, namely the early-to-late energy ratio and the reverberation time. Speech recognition experiments in simulated reverberant environments show that recognizers trained on speech reverberated with the proposed method outperform systems trained on clean speech, even when channel normalization methods like CMS and logRASTA-PLP are used. The extension of our approach to multi-style training is also considered.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-98"
  },
  "bazzi00_icslp": {
   "authors": [
    [
     "Issam",
     "Bazzi"
    ],
    [
     "James R.",
     "Glass"
    ]
   ],
   "title": "Modeling out-of-vocabulary words for robust speech recognition",
   "original": "i00_1401",
   "page_count": 4,
   "order": 99,
   "p1": "vol. 1, 401-404",
   "pn": "",
   "abstract": [
    "In this paper we present an approach for modeling and recognizing out-of-vocabulary (OOV) words in a single stage recognizer. A word-based recognizer is augmented with an extra OOV word model, which enables the OOV word to be predicted by a wordbased language model. The OOV model itself is phone-based, so that an OOV word can be realized as an arbitrary sequence of phones. A phone bigram is used to provide phonotactic constraints within the OOV model. A recognizer with this configuration can recognize words in the original vocabulary as well as any potential new words of arbitrary pronunciation. In our preliminary investigation of this framework, we have evaluated the recognizer on a weather information domain with one test set containing only in-vocabulary (IV) data, and another containing OOV words. On the IV test set, the recognizer had an OOV insertion rate of only 1.3%, and degraded the baseline WER from 10.4% to 10.7%. On the OOV test set, the recognizer was able to detect nearly half of the OOV words (47% detection rate).\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-99"
  },
  "gajic00_icslp": {
   "authors": [
    [
     "Bojana",
     "Gajic"
    ],
    [
     "Richard C.",
     "Rose"
    ]
   ],
   "title": "Hidden Markov model environmental compensation for automatic speech recognition on hand-held mobile devices",
   "original": "i00_1405",
   "page_count": 4,
   "order": 100,
   "p1": "vol. 1, 405-408",
   "pn": "",
   "abstract": [
    "This paper is concerned with applying hidden Markov model compensation techniques for improving the performance of automatic speech recognition (ASR) based services on hand-held mobile devices. The implementation and evaluation of an ASR based task for a mobile, hand-held device is presented, along with a set of compensation techniques that are used to compensate speaker independent hidden Markov models with respect to environmental and transducer variability. A technique for combined environment/ transducer compensation is shown to significantly reduce the effects of environmental mismatch. The overall performance degradation with respect to clean conditions was reduced from 41.7 percent to 10.4 percent for speech spoken through a farfield microphone in an office environment, and from 79.2 percent to 39.8 percent for the same transducer in a noisy cafeteria envionment.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-100"
  },
  "morris00_icslp": {
   "authors": [
    [
     "Andrew C.",
     "Morris"
    ],
    [
     "Ljubomir",
     "Josifovski"
    ],
    [
     "Hervé",
     "Bourlard"
    ],
    [
     "Martin",
     "Cooke"
    ],
    [
     "Phil",
     "Green"
    ]
   ],
   "title": "A neural network for classification with incomplete data: application to robust ASR",
   "original": "i00_1409",
   "page_count": 4,
   "order": 101,
   "p1": "vol. 1, 409-412",
   "pn": "",
   "abstract": [
    "There are many situations in data classification where the data vector to be classified is partially corrupted, or otherwise incomplete. In this case the optimal estimate for each class probability output, for any given set of missing data components, can be obtained by calculating its expected value. However, this means that classifiers whose expected outputs do not have a closed form expression in terms of the original function parameters, such as the commonly used multi-layer perceptron (MLP), cannot be used for classification with missing data. No classifier can compete with the performance of an MLP on complete data unless it is discriminatively trained. In this paper we present a particular form of RBF classifier which can be discriminatively trained and whose expected outputs are a simple function of the original classifier parameters, even though the output unit function is non-linear. This provides us with an incomplete data classifier network (IDCN) which combines the discriminative classification performance normally associated with artificial neural networks, with the ability to deal gracefully with missing data. We describe two ways in which this IDCN can be applied to robust automatic speech recognition (ASR), depending on whether or not the position of missing data is known. We compare the performance of one of these models with an existing system for ASR with missing data.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-101"
  },
  "matsuda00_icslp": {
   "authors": [
    [
     "Shigeki",
     "Matsuda"
    ],
    [
     "Mitsuru",
     "Nakai"
    ],
    [
     "Hiroshi",
     "Shimodaira"
    ],
    [
     "Shigeki",
     "Sagayama"
    ]
   ],
   "title": "Feature-dependent allophone clustering",
   "original": "i00_1413",
   "page_count": 4,
   "order": 102,
   "p1": "vol. 1, 413-416",
   "pn": "",
   "abstract": [
    "We propose a novel method for clustering allophones called Feature-Dependent Allophone Clustering (FD-AC) that determines feature-dependent HMM topology automatically. Existing methods for allophone clustering are based on parameter sharing between the allophone models that resemble each other in behaviors of feature vector sequences. However, all the features of the vector sequences may not necessarily have a common allophone clustering structures It is considered that the vector sequences can be better modeled by allocating the optimal allophone clustering structure to each feature. In this paper, we propose Feature-Dependent Successive State Splitting (FD-SSS) as an implementation of FD-AC. In speaker-dependent continuous phoneme recognition experiments, HMMs created by FD-SSS reduced the error rates by about 10% compared with the conventional HMMs that have a common allophone clustering structure for all the features.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-102"
  },
  "yang00c_icslp": {
   "authors": [
    [
     "Qian",
     "Yang"
    ],
    [
     "Jean-Pierre",
     "Martens"
    ]
   ],
   "title": "Data-driven lexical modeling of pronunciation variations for ASR",
   "original": "i00_1417",
   "page_count": 4,
   "order": 103,
   "p1": "vol. 1, 417-420",
   "pn": "",
   "abstract": [
    "In this paper a method for the automatic construction of a lexicon with multiple entries per word is described. The basic idea is to transform a reference word transcription by means of stochastic pronunciation rules that can be learned automatically. This approach already proved its potential (Cremelie & Martens, 1999), and is now brought to a much higher level of performance. Relative reductions of the word error rate (WER) of 20 % (open vocabulary) to 45 % (closed vocabulary) are now within reach.\n",
    "",
    "",
    "",
    "",
    "N. Cremelie, J.P. Martens. \"In search for better pronunciation models for speech recognition,\" Speech Communication 29, 115-136, 1999.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-103"
  },
  "tran00_icslp": {
   "authors": [
    [
     "Dat",
     "Tran"
    ],
    [
     "Michael",
     "Wagner"
    ]
   ],
   "title": "Fuzzy entropy hidden Markov models for speech recognition",
   "original": "i00_1421",
   "page_count": 4,
   "order": 104,
   "p1": "vol. 1, 421-424",
   "pn": "",
   "abstract": [
    "A new fuzzy technique called fuzzy entropy (FE) clustering is proposed and applied to hidden Markov models (HMMs) for speech recognition. FE-HMMs, both discrete and con- tinuous, are proposed in this paper. Experimental results in speech recognition show good results for FE models com- pared with fuzzy C-means and conventional models.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-104"
  },
  "quillen00_icslp": {
   "authors": [
    [
     "Carl",
     "Quillen"
    ]
   ],
   "title": "Adjacent node continuous-state HMMs",
   "original": "i00_1425",
   "page_count": 4,
   "order": 105,
   "p1": "vol. 1, 425-428",
   "pn": "",
   "abstract": [
    "This paper explores properties of a family of Continuous state hidden Markov models (CSHMMs) that are proposed for use in acoustic modeling. These models can be viewed as applying a smoothing to ordinary HMMs in order to make estimates of transition and observation probabilities more robust by sharing data between adjacent state nodes. They may be trained by EM so that all parameters properly reflect the applied smoothing. The amount of smoothing may be trained as well, and the model reverts to the ordinary HMM in the limit as the smoothing parameters reach zero. Thus this technique may be employed selectively only in areas in the model where training data is sparse. This paper formulates EM-training for one variant of these models, and explores their performance when applied to constructing ergodic CSHMM models of speech and to a phoneme recognition task on the same data. The ergodic CSHMM did not improve performance over the HMM, but the phoneme CSHMMs model the data with higher likelihood than the equivalent HMMs, and have superior recognition accuracy.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-105"
  },
  "sturm00b_icslp": {
   "authors": [
    [
     "Janienke",
     "Sturm"
    ],
    [
     "Eric",
     "Sanders"
    ]
   ],
   "title": "Modelling phonetic context using head-body-tail models for connected digit recognition",
   "original": "i00_1429",
   "page_count": 4,
   "order": 106,
   "p1": "vol. 1, 429-432",
   "pn": "",
   "abstract": [
    "Both whole word modelling and context modelling have proven to improve recognition performance for connected digit strings. In this paper we will show that word boundary variation can be effectively modelled by applying the Head-Body-Tail (HBT) method as proposed by Chou et al in [1] and also applied by Gandhi in [2]. Each digit is split into three parts, representing the beginning, middle and end of a word. The middle part - the body - is assumed to be context-independent, whereas the first part - the head - and the last part - the tail - incorporate information about the preceding or subsequent digit. The results we obtained with HBT-modelling are compared with results obtained with whole-word models (WWMs) [3] and with the results obtained with HBT-models reported in [2]. It is shown that using HBT models a relative improvement over contextindependent WWMs of 28% on string level can be reached.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-106"
  },
  "bazzi00b_icslp": {
   "authors": [
    [
     "Issam",
     "Bazzi"
    ],
    [
     "Dina",
     "Katabi"
    ]
   ],
   "title": "Using support vector machines for spoken digit recognition",
   "original": "i00_1433",
   "page_count": 4,
   "order": 107,
   "p1": "vol. 1, 433-436",
   "pn": "",
   "abstract": [
    "Recently Support Vector Machines (SVM) have emerged as a pattern classifier that can deal with a large feature space and a small data set. In this paper, we look at using SVMs for automatic speech recognition. We focus on a fairly simple speech recognition task: recognizing isolated spoken digits in English. The approach relies solely on the acoustic signal and does not utilize any phonological rules or language constraints in the recognition process. For each digit, a 420-dimension feature vector is extracted. The feature vector is derived from the Mel-Frequency cepstral coefficients of the speech signal. The 420 features are then reduced to a smaller number using principal component analysis (PCA). To perform N-way classification for the ten digits using the standard 2-class SVM classifiers, we examine scoring and voting classification schemes. The best performance is obtained with an N-way 1-versus-9 SVM classifier with a Gaussian kernel of a variance of 4 and using the first 45 PCA features. The accuracy of this classifier is 94.9%.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-107"
  },
  "sun00_icslp": {
   "authors": [
    [
     "Jiping",
     "Sun"
    ],
    [
     "Xing",
     "Jing"
    ],
    [
     "Li",
     "Deng"
    ]
   ],
   "title": "Data-driven model construction for continuous speech recognition using overlapping articulatory features",
   "original": "i00_1437",
   "page_count": 4,
   "order": 108,
   "p1": "vol. 1, 437-440",
   "pn": "",
   "abstract": [
    "A new, data-driven approach to deriving overlapping articulatory-feature based HMMs for speech recognition is presented in this paper. This approach uses speech data from University of Wisconsin's Microbeam X-ray Speech Production Database. Regression tree models were created for constructing HMMs. Use of actual articulatory data improves upon our previous rule-based feature overlapping system. The regression trees allow construction of the HMM topology for an arbitrary utterance given its phonetic transcription and some prosodic information. Experimental results in ASR show preliminary success of this approach.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-108"
  },
  "vasilache00_icslp": {
   "authors": [
    [
     "Marcel",
     "Vasilache"
    ]
   ],
   "title": "Speech recognition using HMMs with quantized parameters",
   "original": "i00_1441",
   "page_count": 4,
   "order": 109,
   "p1": "vol. 1, 441-444",
   "pn": "",
   "abstract": [
    "In this paper we describe the structure and examine the performance of a recognition engine based on hidden Markov models (HMMs) with quantized parameters (qHMM). The main goal of qHMMs is to enable a low complexity implementation without sacrificing the classification performance. In the tests with a whole word digit dialler engine and a phoneme based isolated word recognizer we managed to preserve the performance of unquantized HMMs with qHMMs having as little as 5 bit for a mean component and 3 bit for a variance component.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-109"
  },
  "qi00_icslp": {
   "authors": [
    [
     "Yingyong",
     "Qi"
    ],
    [
     "Jack",
     "Xin"
    ]
   ],
   "title": "A perception and PDE based nonlinear transformation for processing spoken words",
   "original": "i00_1445",
   "page_count": 4,
   "order": 110,
   "p1": "vol. 1, 445-448",
   "pn": "",
   "abstract": [
    "A perception and PDE (partial di\u000berential equa- tion) based nonlinear transformation is proposed to process spoken words in noisy environment. The transformation was designed to reduce noise through time adaptation and spectral enhancement by evolving a focusing quadratic fourth order nonlinear PDE (the Cahn-Hillard equation). Constant, low SNR signals were adaptively decreased to reduce noise. Once the noise levels were reduced, the evolution of focusing PDE was used to enhance the spectral peaks, and further reduce noise interference. Numerical results on noisy spoken words indicated that the transformed spectral pattern of the spoken words was insensitive to nosie. The spectral distances between noisy words and original words were significantly reduced after the transformation.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-110"
  },
  "blasig00_icslp": {
   "authors": [
    [
     "Reinhard",
     "Blasig"
    ],
    [
     "Georg",
     "Rose"
    ],
    [
     "Carsten",
     "Meyer"
    ]
   ],
   "title": "Training of isolated word recognizers with continuous speech",
   "original": "i00_1449",
   "page_count": 4,
   "order": 111,
   "p1": "vol. 1, 449-452",
   "pn": "",
   "abstract": [
    "Is it possible to use out-of-domain acoustic training data to improve a speech recognizer's performance on a specific, independent application? In our experiments, we use Wallstreet Journal (WSJ) data to train a recognizer, which is adapted and evaluated in the Phonebook domain. Apart from their common language (US English), the two corpora di\u000ber in many important respects: microphone vs. telephone channel, continuous speech vs. isolated words, mismatch in speaking rate.\n",
    "This paper deals with two questions. First, starting from the WSJ-trained recognizer, how much adaptation data (taken from the Phonebook training corpus) is necessary to achieve a reasonable recognition performance in spite of the high degree of mismatch? Second, is it possible to improve the recognition performance of a Phonebook-trained baseline acoustic model by using additional out-of-domain training data? The paper describes the adaptation and normalization techniques used to bridge the mismatch between the two corpora.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-111"
  },
  "tseng00_icslp": {
   "authors": [
    [
     "Shu-Chuan",
     "Tseng"
    ]
   ],
   "title": "Repair patterns in spontaneous Chinese dialogs: morphemes, words, and phrases",
   "original": "i00_1453",
   "page_count": 4,
   "order": 112,
   "p1": "vol. 1, 453-456",
   "pn": "",
   "abstract": [
    "Applying experimental and empirical results of linguistic analyses on Chinese speech repairs, this paper presents a new line of research on the relationship of constituent boundaries and speech repairs in spoken Mandarin Chinese. A judgement experiment was carried out to look for particularity of Chinese repairs, so that preliminary results can be obtained. Based on the results, regular repair patterns were determined in relation to three constituent levels: morpheme, word, and phrase. Furthermore, a detailed corpus analysis on Chinese spontaneous dialogs shows that these three constituent units in spoken Chinese influence the production of speech repairs in different ways. The interrelationship of the constituent boundaries and the repair patterns is empirically illustrated. This leads to the conclusion that research on speech repairs not only helps language understanding systems in the way that they can cope with spontaneous speech phenomena. This kind of research also provides empirical cues to the construction of linguistic theories.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-112"
  },
  "dang00_icslp": {
   "authors": [
    [
     "Jianwu",
     "Dang"
    ],
    [
     "Kiyoshi",
     "Honda"
    ]
   ],
   "title": "Improvement of a physiological articulatory model for synthesis of vowel sequences",
   "original": "i00_1457",
   "page_count": 5,
   "order": 113,
   "p1": "vol. 1, 457-460",
   "pn": "",
   "abstract": [
    "A 3D physiological articulatory model has been constructed based on volumetric MRI data obtained from a male speaker. The model is driven by muscles according to a target-dependent activation pattern. In this study, we improved dynamic characteristics of the model to produce higher sound quality for vowel sequences. Dynamic characteristics of articulatory organs were investigated using X-ray microbeam data for vowel sequences and vowel-consonant-vowel (VCV) sequences for 11 Japanese speakers. It was found that the velocity of the tongue tip is about 60% faster in transition of vowel-to-consonant than that of vowel-to-vowel, while the velocities of the tongue dorsum and jaw were independent of the sequences. Reaction time, from maximal acceleration to maximal velocity, of the articulators is about 40% shorter in vowel-to-consonant transitions than in vowel-to-vowel transitions. To apply the improved model for speech analysis, articulatory targets were estimated for the vowels in vowel sequences using AbS method, and used to generate the vocal tract shapes for vowel sequences. The vocal tract shapes and synthetic sounds were compared with speech sound and articulatory data from the target speaker. The results showed that our model demonstrates plausible dynamic characteristics of articulatory movement in producing vowel sequences. The simulation error was about 2.5% for the formants, and 0.2 cm for the observation points of the vocal tract.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-113"
  },
  "motoki00_icslp": {
   "authors": [
    [
     "Kunitoshi",
     "Motoki"
    ],
    [
     "Xavier",
     "Pelorson"
    ],
    [
     "Pierre",
     "Badin"
    ],
    [
     "Hiroki",
     "Matsuzaki"
    ]
   ],
   "title": "Computation of 3-d vocal tract acoustics based on mode-matching technique",
   "original": "i00_1461",
   "page_count": 4,
   "order": 114,
   "p1": "vol. 1, 461-464",
   "pn": "",
   "abstract": [
    "A model for calculating the acoustic characteristics of 3-D vocal tract configuration is presented. A mode-matching technique and an impedance transformation are used to calculate the higher-order modes in this model where both propagative and evanescent higher-order modes are considered. A cascaded structure of rectangular acoustic tubes is introduced as an approximation of the real vocal tract geometry. The number of higher-order modes in each tube can be selected independently, which significantly decreases the instability of computation caused by the evanescent higher-order modes. Preliminary calculation results are discussed from the view point of the influences of the offset connection of tubes.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-114"
  },
  "menard00_icslp": {
   "authors": [
    [
     "Lucie",
     "Ménard"
    ],
    [
     "Louis-Jean",
     "Boë"
    ]
   ],
   "title": "Exploring vowel production strategies from infant to adult by means of articulatory inversion of formant data",
   "original": "i00_1465",
   "page_count": 4,
   "order": 115,
   "p1": "vol. 1, 465-468",
   "pn": "",
   "abstract": [
    "It is well known that the adults vocal tract is not a uniform scaled up version of a child vocal tract. Considering these morphological differences, what are the articulatory strategies used by the speaker throughout growth to produce the same vowels? Our previous simulation study [1] predicts that a speaker with a newborn-like vocal tract would employ a fronting articulation compared to an adult male, in order to produce the same acoustic targets. In this paper, we extend our simulations with the VLAM model to a 4-year-old and a 10-year-old children, a 16-year-old boy and an adult man. Articulatory positions, for each growth stage, for the 4 vowels [i], [y], [u], and [a] have been determined using a formant-to-articulatory inversion method. Analysis of real data is finally presented.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-115"
  },
  "smith00b_icslp": {
   "authors": [
    [
     "Gavin",
     "Smith"
    ],
    [
     "Tony",
     "Robinson"
    ]
   ],
   "title": "Segmentation of a speech waveform according to glottal open and closed phases using an autoregressive-HMM",
   "original": "i00_1469",
   "page_count": 4,
   "order": 116,
   "p1": "vol. 1, 469-472",
   "pn": "",
   "abstract": [
    "This paper presents an algorithm to segment speech according to glottal open and closed phases using the time waveform alone. Based on this, pitch, jitter and closed to open glottal ratios can be computed. Segmentation is achieved by identifying spectral changepoints at the sub-pitch period timescale. Changepoints are identified using a 3-state autoregressive hidden Markov model (AR-HMM) operating on the time waveform, with the Liljencrants-Fant (LF) glottal model as a theoretical basis. Model parameters and optimal state sequence are determined re- spectively using the expectation-maximisation (EM) algorithm and a bounded state duration (BSD) Viterbi algo- rithm. Experiments on synthetic speech give encouraging glottal segmentation for modal, fry and breathy voice types. Experiments on real speech obtained from TIMIT give meaningful segmentations also.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-116"
  },
  "orr00_icslp": {
   "authors": [
    [
     "Rosemary",
     "Orr"
    ],
    [
     "Bert",
     "Cranen"
    ],
    [
     "Felix de",
     "Jong"
    ],
    [
     "Lou",
     "Boves"
    ]
   ],
   "title": "Comparison of inverse filtering of the flow signal and microphone signal",
   "original": "i00_1473",
   "page_count": 5,
   "order": 117,
   "p1": "vol. 1, 473-476",
   "pn": "",
   "abstract": [
    "This study looks at two ways of extracting a glottal waveform from recorded speech. One way is to inverse filter the flow at the mouth. Another is to inverse filter the microphone signal. Theoretically, the microphone signal is considered to be the equivalent of a first order differentiation of the flow signal recorded at the lips.\n",
    "Recording the oral airflow is more complicated than the recording of a microphone signal, as it requires the use of a mask, with constant adjustments during the recording. Recording of the microphone signal is more straightforward for the experimenter and less intrusive for the subject. If the two inverse filtering procedures can be shown to produce similar glottal flow waveforms for both types of recorded speech, this would support the use of only the microphone signal for those types of glottal flow analysis where the DC component of the flow is not essetial, making voice source analysis applicable in less specialised situations.\n",
    "In this study, we used recordings of microphone signal and recordings of oral flow to compare the results of inverse filtering. A group of twenty subjects produced repetitions of the utterance /pae/. We recorded oral flow, EGG, and the microphone signal. The flow and microphone signals were analysed using an automatic inverse filtering program and values for parameters which are extracted from the source wave are compared.\n",
    "The results were not as similar as expected, although in some respects, they correlated well. This may be due to experimental design, the degree of insight of the subject into the voicing task, and the fact that the speech material used for the comparison was not identical.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-117"
  },
  "iseli00_icslp": {
   "authors": [
    [
     "Markus R.",
     "Iseli"
    ],
    [
     "Abeer",
     "Alwan"
    ]
   ],
   "title": "Inter- and intra-speaker variability of glottal flow derivative using the LF model",
   "original": "i00_1477",
   "page_count": 4,
   "order": 118,
   "p1": "vol. 1, 477-480",
   "pn": "",
   "abstract": [
    "The vowels /a, i, u/ spoken by American English talkers with non-pathological voices are described by means of voice source model parameters using the Liljencrants-Fant (LF) model. The sampling frequency of the data is 8 kHz which matches approximately telephone bandwidth. After inverse filtering, trends of voice source characteristics depending on the LF parameters are analyzed and compared to literature and listening results.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-118"
  },
  "blache00_icslp": {
   "authors": [
    [
     "Philippe",
     "Blache"
    ],
    [
     "Daniel",
     "Hirst"
    ]
   ],
   "title": "Multi-level annotation for spoken language corpora",
   "original": "i00_1481",
   "page_count": 4,
   "order": 119,
   "p1": "vol. 1, 481-484",
   "pn": "",
   "abstract": [
    "The constitution of multi-level databases integrating, for example, both prosodic and morphosyntactic levels of representation presents a number of problems, some specific to the individual domains, and others concerning the integration of the two domains. It is argued that the formalism of annotation graphs provides an adequate solution to these problems, which can be implemented in an XML representation. It is further argued that a generic query language, DQL, currently being developed, will provide a satisfactory framework both for querying and for manipulating documents of this type.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-119"
  },
  "li00b_icslp": {
   "authors": [
    [
     "Aijun",
     "Li"
    ],
    [
     "Fang",
     "Zheng"
    ],
    [
     "William",
     "Byrne"
    ],
    [
     "Pascale",
     "Fung"
    ],
    [
     "Terri",
     "Kamm"
    ],
    [
     "Yi",
     "Liu"
    ],
    [
     "Zhanjiang",
     "Song"
    ],
    [
     "Umar",
     "Ruhi"
    ],
    [
     "Veera",
     "Venkataramani"
    ],
    [
     "XiaoXia",
     "Chen"
    ]
   ],
   "title": "CASS: a phonetically transcribed corpus of mandarin spontaneous speech",
   "original": "i00_1485",
   "page_count": 4,
   "order": 120,
   "p1": "vol. 1, 485-488",
   "pn": "",
   "abstract": [
    "A collection of Chinese spoken language has been collected and phonetically annotated to capture spontaneous speech and language effects. The Chinese Annotated Spontaneous Speech (CASS) corpus contains phonetically transcribed spontaneous speech. This corpus was created to begin to collect samples of most of the phonetic variations in Mandarin spontaneous speech due to pronunciation effects, including allophonic changes, phoneme reduction, phoneme deletion and insertion, as well as duration changes. It is intended for use in pronunciation modeling for improved automatic speech recognition and will be used at the 2000 Johns Hopkins University Language Engineering Workshop by the project on Pronunciation Modeling ofMandarin Casual Speech.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-120"
  },
  "yamamoto00b_icslp": {
   "authors": [
    [
     "Kazuhide",
     "Yamamoto"
    ],
    [
     "Eiichiro",
     "Sumita"
    ]
   ],
   "title": "Multiple decision-tree strategy for input-error robustness: a simulation of tree combinations",
   "original": "i00_1489",
   "page_count": 4,
   "order": 121,
   "p1": "vol. 1, 489-492",
   "pn": "",
   "abstract": [
    "This paper illustrates the characteristics of the multiple decision-tree (MDT) model, which we proposed in a previous work. MDT is an extension of the decision-tree model and is proposed for its robustness against input uncertainty. We present simulation results to show that the MDT model is task-independent and outperforms both the conventional decision-tree model and the majority model against noisy inputs.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-121"
  },
  "chen00_icslp": {
   "authors": [
    [
     "Zheng",
     "Chen"
    ],
    [
     "Kai-Fu",
     "Lee"
    ],
    [
     "Ming-jing",
     "Li"
    ]
   ],
   "title": "Discriminative training on language model",
   "original": "i00_1493",
   "page_count": 4,
   "order": 122,
   "p1": "vol. 1, 493-496",
   "pn": "",
   "abstract": [
    "Statistical language models have been successfully applied to a lot of problems, including speech recognition, handwriting, Chinese pinyin-input etc. In recognition, a statistical language model, such as the trigram model, is used to predict the probabilities of hypothesized word sequences. The traditional method that relies on distribution estimation is sub-optimal when the assumed distribution form is inapplicable, and that \"optimality\" in distribution estimation does not automatically translate into \"optimality\" in classifier design. This paper proposed a discriminative training method to minimize the error rate of recognizer rather than estimate the distribution of training data. Furthermore, lexicon is also optimized to minimize the error rate of the decoder through discriminative training. Compared to the traditional LM building method, our system achieves a 5%-25% reduction in error rate.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-122"
  },
  "gao00b_icslp": {
   "authors": [
    [
     "Jianfeng",
     "Gao"
    ],
    [
     "Mingjing",
     "Li"
    ],
    [
     "Kai-Fu",
     "Lee"
    ]
   ],
   "title": "N-gram distribution based language model adaptation",
   "original": "i00_1497",
   "page_count": 4,
   "order": 123,
   "p1": "vol. 1, 497-500",
   "pn": "",
   "abstract": [
    "This paper presents two techniques for language model (LM) adaptation. The first aims to build a more general LM. We propose a distribution-based pruning of n-gram LMs, where we prune n-grams that are likely to be infrequent in a new document. Experimental results show that the distribution-based pruning method performed up to 9% (word perplexity reduction) better than conventional cutoff methods. Moreover, the pruning method results in a more general ngram backoff model, in spite of the domain, style, or temporal bias in the training data.\n",
    "The second aims to build a more task-specific LM. We propose an n-gram distribution adaptation method for LM training. Given a large set of out-of-task training data, called training set, and a small set of task-specific training data, called seed set, we adapt the LM towards the task by adjusting the n-gram distribution in the training set to that in the seed set. Experimental results show non-trivial improvements over conventional methods.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-123"
  },
  "palou00_icslp": {
   "authors": [
    [
     "Francisco",
     "Palou"
    ],
    [
     "P.",
     "Bravetti"
    ],
    [
     "O.",
     "Emam"
    ],
    [
     "V.",
     "Fischer"
    ],
    [
     "Eric",
     "Janke"
    ]
   ],
   "title": "Towards a common phone alphabet for multilingual speech recognition",
   "original": "i00_1501",
   "page_count": 3,
   "order": 124,
   "p1": "vol. 1, 501-504",
   "pn": "",
   "abstract": [
    "New automatic speech recognition applications, mainly for small and medium vocabulary sizes, demand the capability of recognizing speech in several languages simultaneously. We have started exploring the possibility of building acoustic models that integrate multiple languages (up to seven in the initial stage), using speech transcriptions based on a common phoneme alphabet across all the languages. To reach a common alphabet, we start from the previously existing alphabets for each one of the seven languages. We first proceed to simplify some of them, partially following SAMPA transcription guidelines, and then to merge phones present in several languages that correspond to the same IPA symbol. We study and compare two variants of the common phoneme alphabet. The first of these two alphabets is closer to the starting ones, and includes the use of diphthong phones for English and German, and long-vowel phones for Arabic, English, and German. The second one avoids the long-vowel and diphthong phone models, and also the stressed vowel models. We present and discuss the results of decoding large vocabulary dictation tests, comparing the two alphabet variants, and also the multilingual decoding results with the corresponding monolingual acoustic models.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-124"
  },
  "belvin00_icslp": {
   "authors": [
    [
     "Robert",
     "Belvin"
    ],
    [
     "Ron",
     "Burns"
    ],
    [
     "Cheryl",
     "Hein"
    ]
   ],
   "title": "What²s next: a case study in the multidimensionality of a dialog system",
   "original": "i00_1504",
   "page_count": 4,
   "order": 125,
   "p1": "vol. 1, 504-507",
   "pn": "",
   "abstract": [
    "In this paper we argue that conversational dialog systems must model a multidimensional space defined by syntactico-semantic, circumstantial, and interactional dimensions. We take as a testcase the interpretation of a very simple query type within a conversational navigation system that we have implemented. We outline the decision logic this system follows in interpreting next-turn and numbered turn queries, and discuss some of the interesting results that have come about from user testing of the prototype.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-125"
  },
  "higashida00_icslp": {
   "authors": [
    [
     "Masanobu",
     "Higashida"
    ],
    [
     "Kumiko",
     "Ohmori"
    ]
   ],
   "title": "A new dialogue control method based on human listening process to construct an interface for ascertaining a user²s inputs",
   "original": "i00_1508",
   "page_count": 5,
   "order": 126,
   "p1": "vol. 1, 508-511",
   "pn": "",
   "abstract": [
    "This paper describes a new dialogue control method that utilizes new recognition processes called \"Presupposition-type Recognition\" and \"Pretense-type Recognition\" that we propose based on human dialogue analysis. This method provides users with stress-free voice input through real-time responses, comprising a naturally controlled dialogue to obtain information in order to winnow candidates comprehensively.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-126"
  },
  "wang00_icslp": {
   "authors": [
    [
     "XianFang",
     "Wang"
    ],
    [
     "LiMin",
     "Du"
    ]
   ],
   "title": "Spoken language understanding in a Chinese spoken dialogue system engine",
   "original": "i00_1512",
   "page_count": 4,
   "order": 127,
   "p1": "vol. 1, 512-515",
   "pn": "",
   "abstract": [
    "In this paper, we introduce the method of spoken language understanding in the Linguistic Processor of a Chinese Spoken Dialogue System Engine. The Linguistic Processor is conceived for the development of simple, robust nature language interfaces to different applications. It is designed for extracting meaning representation from Chinese spoken utterances. Its strategy is to apply grammatical constraints at the phrase level rather than on the entire sentence.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-127"
  },
  "dharanipragada00_icslp": {
   "authors": [
    [
     "Satya",
     "Dharanipragada"
    ],
    [
     "Martin",
     "Franz"
    ],
    [
     "J. Scott",
     "McCarley"
    ],
    [
     "K.",
     "Papineni"
    ],
    [
     "Salim",
     "Roukos"
    ],
    [
     "T.",
     "Ward"
    ],
    [
     "W.-J.",
     "Zhu"
    ]
   ],
   "title": "Statistical methods for topic segmentation",
   "original": "i00_1516",
   "page_count": 4,
   "order": 128,
   "p1": "vol. 1, 516-519",
   "pn": "",
   "abstract": [
    "Automatic Topic Segmentation is an important technology for multimedia archival and retrieval systems. In this paper we present an algorithm for topic segmentation which uses a combination of machine learning, statistical natural lan- guage processing, and information retrieval techniques. The performance of this algorithm is measured by considering the misses and false alarms on a manually segmented corpus. We present our results on the widely used TDT2 and TDT3 cor- pora provided by NIST. Most of the techniques described are independent of the source language. We demonstrate this by applying the algorithm on both the English and Mandarin TDT3 corpora with only minor changes.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-128"
  },
  "chen00b_icslp": {
   "authors": [
    [
     "Berlin",
     "Chen"
    ],
    [
     "Hsin-min",
     "Wang"
    ],
    [
     "Lin-shan",
     "Lee"
    ]
   ],
   "title": "Retrieval of mandarin broadcast news using spoken queries",
   "original": "i00_1520",
   "page_count": 5,
   "order": 129,
   "p1": "vol. 1, 520-523",
   "pn": "",
   "abstract": [
    "Considering the monosyllabic structure of the Chinese language, a whole class of indexing features for retrieval of Mandarin broadcast news using syllable-level statistical characteristics has been previously investigated. This paper presents the improvements achieved over the previous results. The major differences are: (1) Multi-scale character- and word-level indexing terms have been integrated with the syllable-level information. (2) Information cues from the contemporary newswire text corpus have been used to create more accurate syllable indexing terms. (3) Automatic document expansion, blind relevance feedback, and query expansion via the term association matrix have been applied in retrieval. With all these schemes, the average precision can be improved from 55.46% to 71.29%.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-129"
  },
  "hansen00_icslp": {
   "authors": [
    [
     "John H. L.",
     "Hansen"
    ],
    [
     "Jay",
     "Plucienkowski"
    ],
    [
     "Stephen",
     "Gallant"
    ],
    [
     "Bryan",
     "Pellom"
    ],
    [
     "Wayne",
     "Ward"
    ]
   ],
   "title": "CU-move: robust speech processing for in-vehicle speech systems",
   "original": "i00_1524",
   "page_count": 5,
   "order": 130,
   "p1": "vol. 1, 524-527",
   "pn": "",
   "abstract": [
    "In this paper, we present our recent work in the formulation of a new in-vehicle interactive system for route planning and navigation. The novel aspects presented include the formulation of a new microphone array and multi-channel noise suppression front-end, corpus development for speech and acoustic vehicle conditions, environmental classification for changing in-vehicle noise conditions, and a back-end dialog navigation information retrieval sub-system connected to the WWW. While previous attempts at in-vehicle speech systems have generally focused on isolated command words to set radio frequencies, temperature control, etc. The CUMove system is focused on natural conversational interaction between the user and in-vehicle system. Since previous studies in speech recognition have shown significant loses in performance when speakers are under task or emotional stress, it is important to develop conversational systems that minimize operator stress for the driver. In addition to discussing the overall CU-Move system, we report on specific research accomplishments in microphone array/beamforming development, environmental noise characterization, and our prototype dialogue system which is based on the MIT Galaxy-II Hub architecture. We also discuss CU-Move corpus development issues.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-130"
  },
  "kim00_icslp": {
   "authors": [
    [
     "Ji-Hwan",
     "Kim"
    ],
    [
     "Philip C.",
     "Woodland"
    ]
   ],
   "title": "A rule-based named entity recognition system for speech input",
   "original": "i00_1528",
   "page_count": 4,
   "order": 131,
   "p1": "vol. 1, 528-531",
   "pn": "",
   "abstract": [
    "In this paper, we propose a rule based (transformation based) named entity recognition system which uses the Brill rule inference approach. To measure its performance, we compare the performance of the rule-based system and IdentiFinder, one of the most successful stochastic systems. In the baseline case (no punctuation and no capitalisation), both systems show almost equal performance. They also have similar performance in the case of additional infor- mation such as punctuation, capitalisation and name lists. The performance of both systems degrade linearly with added speech recognition errors, and their rates of degrada- tion are almost equal. These results show that automatic rule inference is a viable alternative to the HMM-based approach to named entity recognition, but it retains the advantages of a rule-based approach.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-131"
  },
  "sadigh00_icslp": {
   "authors": [
    [
     "Mohammad Reza",
     "Sadigh"
    ],
    [
     "Hamid",
     "Sheikhzadeh"
    ],
    [
     "M. R.",
     "Jahangir"
    ],
    [
     "Arash",
     "Farzan"
    ]
   ],
   "title": "A rule-based approach to farsi language text-to-phoneme conversion",
   "original": "i00_1532",
   "page_count": 4,
   "order": 132,
   "p1": "vol. 1, 532-535",
   "pn": "",
   "abstract": [
    "A conversion from orthographic (written) form to a phonetic transcription is the first stage in a text-to-speech system. In this study, algorithms are presented to facilitate the text-to-phoneme (TTP) conversion for the Farsi language. Using a lexicon of about 15000 base morphemes, word formation rules are investigated and implemented. Moreover, a word segmentation of the written sentence has to be done prior to any phonetic transcription of the text. Due to special form of Farsi orthography, the word segmentation process is a complicated one. To solve the problem, a fast and on-line algorithm and a more complicated off-line algorithm are presented. The overall performance of the TTP conversion is evaluated to be more than 90%.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-132"
  },
  "jongman00b_icslp": {
   "authors": [
    [
     "Allard",
     "Jongman"
    ],
    [
     "Yue",
     "Wang"
    ],
    [
     "Joan",
     "Sereno"
    ]
   ],
   "title": "Acoustic and perceptual properties of English fricatives",
   "original": "i00_1536",
   "page_count": 5,
   "order": 133,
   "p1": "vol. 1, 536-539",
   "pn": "",
   "abstract": [
    "This study presents a comparative analysis of acoustic and perceptual cues to place of articulation in English fricatives. Acoustic parameters investigated include spectral peak location, spectral moments, locus equations, F2 onset, overall noise amplitude, relative amplitude, and fricative noise duration. Perception data were collected for isolated fricatives and fricativevowel syllables. The present study investigates which acoustic parameters can account for the perceptual data. Relating the acoustic and perception data by means of regression analysis suggests that spectral peak location, noise amplitude, and relative amplitude are the primary cues to perception of place of articulation in English fricatives.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-133"
  },
  "shattuckhufnagel00_icslp": {
   "authors": [
    [
     "Stefanie",
     "Shattuck-Hufnagel"
    ],
    [
     "Nanette",
     "Veilleux"
    ]
   ],
   "title": "The special phonological characteristics of monosyllabic function words in English",
   "original": "i00_1540",
   "page_count": 4,
   "order": 134,
   "p1": "vol. 1, 540-543",
   "pn": "",
   "abstract": [
    "Monosyllabic Function Words of American English, such as articles (e.g. the, a), pronouns (him, them) and conjunctions (and, or), are notorious for their pronunciation variability in continuous speech. This study explores one potential correlate of this variation: the phonological character of Function Word forms. The Brown Corpus of 1 million words of written text, with each word token labeled for part of speech, provides a quasi-comprehensive listing and categorization of the Function Words of English, making it possible to compare the phonological characteristics of Function Words with those of a substantial sample of Content Words. Results show that Function Words are more likely to begin with a vowel than Content Words are; in addition, when an onset consonant is specified, it is less likely to be a stop consonant for a Function Word than for a Content Word. A set of words whose categorization is uncertain, like quite, such all, many etc, show intermediate values on these two phonological dimensions. The differences are consistent with the hypothesis that Function Words are phonologically weaker than Content Words, perhaps contributing to their susceptibility to severe phonetic modification in continuous speech.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-134"
  },
  "lopezdeipina00_icslp": {
   "authors": [
    [
     "Miren Karmele",
     "López de Ipiña"
    ],
    [
     "Inés",
     "Torres"
    ],
    [
     "Lourdes",
     "Oñederra"
    ],
    [
     "Amparo",
     "Varona"
    ],
    [
     "Luis Javier",
     "Rodríguez"
    ]
   ],
   "title": "Selection of sublexical units for continuous speech recognition of basque",
   "original": "i00_1544",
   "page_count": 4,
   "order": 135,
   "p1": "vol. 1, 544-547",
   "pn": "",
   "abstract": [
    "This paper describes the work carried out to select the most suitable set of Sublexical Units for Continuous Speech Recognition of Basque. Even if there are several dialects in Basque, only one of them has been used to choose the preliminary set of sounds. Bearing in mind this aim, a wide experimentation has been carried out to select Context Independent Phone-Like Units. Then, in order to obtain robust acoustic models for the language, the units have been evaluated with most of the dialectal variants of Basque. Finally, Decision-Trees based Context Dependent Sublexical Units are selected. For building the trees the classical methodology of Bahl and the efficient Growing and Pruning algorithm have been used.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-135"
  },
  "plauche00_icslp": {
   "authors": [
    [
     "Madelaine C.",
     "Plauché"
    ],
    [
     "Kemal",
     "Sönmez"
    ]
   ],
   "title": "Machine learning techniques for the identification of cues for stop place",
   "original": "i00_1548",
   "page_count": 5,
   "order": 136,
   "p1": "vol. 1, 548-551",
   "pn": "",
   "abstract": [
    "This paper is situated in a long line of phonetic studies that seek to determine and qualify the acoustic cues humans use to identify stop place. The present study draws from a database of 1500 CV tokens of American English and their values for the acoustic features thought to be cues for stop place identification, including (1) VOT, (2) energy of the burst and release, (3) spectrum at the burst, and (4) formant transitions into the following vowel. Decision trees are used to determine the relative invariance of these acoustic features, which indicates their potential to serve as useful cues for listeners cross-contextually. Decision trees thus allow the evaluation of vocalic effects on this hierarchy of features for the purpose of guiding classic perceptual confusion studies.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-136"
  },
  "widera00_icslp": {
   "authors": [
    [
     "Christina",
     "Widera"
    ]
   ],
   "title": "Strategies of vowel reduction - a speaker-dependent phenomenon",
   "original": "i00_1552",
   "page_count": 4,
   "order": 137,
   "p1": "vol. 1, 552-555",
   "pn": "",
   "abstract": [
    "In natural speech, a lot of inter- and intra-subject variation in the realisation of vowels is found. Perception experiments show that listeners can discriminate speaker-independent levels of vowel reduction. The question is whether all speakers used the same acoustic cues for signalling reduction levels. The acoustic realisation of reduction levels of the vowels [u:], [i:], and [a:] of three speakers were tested for their separability and their predictability using linear discriminant analysis, support vector machines, and artificial neural networks. The results indicate a speaker-dependent realisation of vowel reduction.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-137"
  },
  "fox00_icslp": {
   "authors": [
    [
     "Michelle A.",
     "Fox"
    ]
   ],
   "title": "Syllable-final /s/ lenition in the LDC's callhome Spanish corpus",
   "original": "i00_1556",
   "page_count": 4,
   "order": 138,
   "p1": "vol. 1, 556-559",
   "pn": "",
   "abstract": [
    "This paper describes a data corpus which is being made available through the Linguistic Data Consortium (LDC) that codes lenition of syllable-final /s/ in Latin American Spanish in the LDCs CallHome Spanish corpus. This lenition is a process whereby the /s/ may be aspirated (pronounced [h]) or deleted altogether. Since syllable-final /s/ is frequent in Spanish, lenition has a great effect on overall pronunciation. While previous data collected on syllable-final /s/ lenition has been dialect-specific, the CallHome Spanish corpus contains speech from many speakers of different dialects and provides the type of speech data needed to adequately model the phenomenon. Data analysis indicates that over 40% of instances of syllablefinal /s/ are deleted. The deletion rate is extremely speakerand dialect-dependent and also depends on linguistic factors such as phonetic environment and grammatical status of the /s/, as well as the identity of the individual word.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-138"
  },
  "kurematsu00b_icslp": {
   "authors": [
    [
     "Akira",
     "Kurematsu"
    ],
    [
     "Takeaki",
     "Nakazaki"
    ]
   ],
   "title": "Meaning extraction based on frame representation for Japanese spoken dialogue",
   "original": "i00_1560",
   "page_count": 5,
   "order": 139,
   "p1": "vol. 1, 560-563",
   "pn": "",
   "abstract": [
    "This paper describes the issue of meaning extraction based on frame representation for spoken dialogue. The framework of semantic lexicon and the structure of semantic caseframe which focuses on the keywords in phrase is described. The results of the preliminary experiments of meaning extraction using spontaneous speech in the scheduling task and the document retrieval task are shown.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-139"
  },
  "caspers00_icslp": {
   "authors": [
    [
     "Johanneke",
     "Caspers"
    ]
   ],
   "title": "Pitch accents, boundary tones and turn-taking in dutch map task dialogues",
   "original": "i00_1565",
   "page_count": 4,
   "order": 140,
   "p1": "vol. 1, 565-568",
   "pn": "",
   "abstract": [
    "The present paper reports on an investigation of Dutch Map Task dialogues, looking for specific melodic turn-keeping cues. The materials were divided into so-called Inter Pausal Units (cf. [1]), and all IPU boundaries were labeled as instances of either turn-keeping or turn-changing. The pitch accents and boundary tones preceding the IPU boundaries were labeled in the ToDI system [2]. Results indicate one particular melodic configuration that seems typically associated with turn-keeping: an accent-lending rise followed by level high pitch (H* %). But also the level boundary tone (%) in itself and - to a lesser extent - the rising pitch accent (H*) seem to function as melodic turn-keeping cues.\n",
    "",
    "",
    "Koiso, H., Horiuchi, Y., Tutiya, S., Ichikawa, A., and Den, Y., \"An analysis of turn-taking and backchannels based on prosodic and syntactic features in Japanese Map Task dialogs\", Language and Speech 41, 295-321, 1998.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-140"
  },
  "yamashita00_icslp": {
   "authors": [
    [
     "Yoichi",
     "Yamashita"
    ],
    [
     "Michiyo",
     "Murai"
    ]
   ],
   "title": "An annotation scheme of spoken dialogues with topic break indexes",
   "original": "i00_1569",
   "page_count": 4,
   "order": 141,
   "p1": "vol. 1, 569-572",
   "pn": "",
   "abstract": [
    "This paper proposes a scheme of annotating spoken dialogues with discourse level information in terms of the discourse segment. Dialogues are coded with topic break index (TBI), which indicates the degree of topic break between the discourse segments, instead of marking a beginning and an ending utterances of the segment. TBI is graded by two levels, 1 and 2, and TBI=2 indicates a large change of the topic. Two methods are tried for assigning a TBI value for segment boundaries. In the method-I, the coder directly describes TBI according to the di\u000berence of contents between the adjacent segments. In the method-II, the coder classifies relative change of the topic break between the adjacent segments into three categories. Then, the relative changes are automatically converted into TBIs by extraction of local maximum change of the topic break. Two annotation methods are evaluated with the agreement score and the relation to prosodic parameters.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-141"
  },
  "veilleux00_icslp": {
   "authors": [
    [
     "Nanette",
     "Veilleux"
    ]
   ],
   "title": "Application of the centering framework in spontaneous dialogues",
   "original": "i00_1573",
   "page_count": 4,
   "order": 142,
   "p1": "vol. 1, 573-576",
   "pn": "",
   "abstract": [
    "Spontaneous speech poses problems for automatic systems. While many investigators are making progress in recognition and dialogue processing, spontaneous speech also raises interesting problems for a deeper level of discourse modeling. Here, the discourse segmentation (Grosz and Sidner, 1986) and the centering frameworks (Grosz, Joshi and Weinstein, 1995) are used to track the evolution of local and global attentional states in a spontaneous speech dialogue. Several issues characteristic of this corpus are discussed: the informality of sentence structure, the use of pronouns, and the role of prosodic cues. Empirical analysis indicates that the most useful domain for assigning discourse centers is the clause in spontaneous speech, rather than the utterance. Speakers appear to have little constraint on inserting references to discourse participants, but these assume center status only when used as active agents in the discourse. Furthermore, pitch accents can be used to promote the salience of discourse entities, allowing prominent entities to become backward-looking centers in subsequent clauses.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-142"
  },
  "mori00_icslp": {
   "authors": [
    [
     "Hiroki",
     "Mori"
    ],
    [
     "Hideki",
     "Kasuya"
    ]
   ],
   "title": "Automatic lexicon generation and dialogue modeling for spontaneous speech",
   "original": "i00_1577",
   "page_count": 4,
   "order": 143,
   "p1": "vol. 1, 577-580",
   "pn": "",
   "abstract": [
    "This paper describes novel framework for dialogue modeling based on a superword model, a superset of word n-gram. This has a remarkable advantage, because only transcribed text is needed to obtain the model, and no word dictionary is needed. In this paper, it is shown that the expressions specific to dialogue speech are extracted automatically from the transcriptions of spoken dialogue corpora by applying the acquisition method of the superword model. From experimental results based on a Japanese spoken dialogue database which consists of 42 sessions from 6 different tasks, it has been found that the proposed language modeling method has an ability to acquire task-independent lexical entry characteristic of dialogue speech, and many lexical entries are found to be relevant to discourse structures.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-143"
  },
  "wolters00_icslp": {
   "authors": [
    [
     "Maria",
     "Wolters"
    ],
    [
     "Hansjörg",
     "Mixdorff"
    ]
   ],
   "title": "Evaluating radio news intonation - autosegmental versus superpositional modelling",
   "original": "i00_1581",
   "page_count": 4,
   "order": 144,
   "p1": "vol. 1, 581-584",
   "pn": "",
   "abstract": [
    "This study examines prosodic correlates of the givenness of discourse entities in German radio news speech. The material comes from the Stuttgart Radio News Corpus. Both GToBI intonation labels and a Fujisaki-style parametrization of the intonation contour were examined. We find strong word-class specific accentuation defaults; the influence of entity status is rather small and varies with word class. However, there are strong influences of newness on phrasing. The results of autosegmental and superpositional approaches complement each other nicely.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-144"
  },
  "falavigna00_icslp": {
   "authors": [
    [
     "Daniele",
     "Falavigna"
    ],
    [
     "Roberto",
     "Gretter"
    ],
    [
     "Marco",
     "Orlandi"
    ]
   ],
   "title": "A mixed language model for a dialogue system over ihe telephone",
   "original": "i00_1585",
   "page_count": 4,
   "order": 145,
   "p1": "vol. 1, 585-588",
   "pn": "",
   "abstract": [
    "This paper describes the work, under way in ITC-irst, on spoken dialogue technology over the telephone. Several activities have been carried out in order to improve either the flexibility and usage facility of ITC-irst voice recogniser and to efficiently afford difficult tasks arising when the system has to cope with large domains. At present, the dialogue technology is sufficiently robust for handling information access in restricted domains (e.g. train timetable inquiry). Furthermore, the portability of the technology towards different domains is guaranteed by an application independent architecture and by easy to use development tools and interfaces. To port the technology on larger domains (e.g. requests of tourism information) several improvements have still to be done, especially in the language models. These latter ones must be capable of handling spoken interactions aimed at retrieving information contained in large and variable databases. Hence techniques and tools for efficiently adapting language models need to be developed. Finally, language generation methods for giving the retrieved information to the callers in a useful way need to be studied.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-145"
  },
  "bell00_icslp": {
   "authors": [
    [
     "Linda",
     "Bell"
    ],
    [
     "Joakim",
     "Gustafson"
    ]
   ],
   "title": "Positive and negative user feedback in a spoken dialogue corpus",
   "original": "i00_1589",
   "page_count": 4,
   "order": 146,
   "p1": "vol. 1, 589-592",
   "pn": "",
   "abstract": [
    "This paper examines feedback strategies in a Swedish corpus of multimodal humancomputer interaction. The aim of the study is to investigate how users provide positive and negative feedback to a dialogue system and to discuss the function of these utterances in the dialogues. User feedback in the AdApt corpus was labeled and analyzed, and its distribution in the dialogues is discussed. The question of whether it is possible to utilize user feedback in future systems is considered. More specifically, we discuss how error handling in human-computer dialogue might be improved through greater knowledge of user feedback strategies. In the present corpus, almost all subjects used positive or negative feedback at least once during their interaction with the system. Our results indicate that some types of feedback more often occur in certain positions in the dialogue. Another observation is that there appear to be great individual variations in feedback strategies, so that certain subjects give feedback at almost every turn while others rarely or never respond to a spoken dialogue system in this manner. Finally, we discuss how feedback could be used to prevent problems in human-computer dialogue.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-146"
  },
  "cutler00_icslp": {
   "authors": [
    [
     "Anne",
     "Cutler"
    ],
    [
     "Mariëtte",
     "Koster"
    ]
   ],
   "title": "Stress and lexical activation in dutch",
   "original": "i00_1593",
   "page_count": 4,
   "order": 147,
   "p1": "vol. 1, 593-596",
   "pn": "",
   "abstract": [
    "Dutch listeners were slower to make judgements about the semantic relatedness between a spoken target word (e.g. atLEET, 'athlete') and a previously presented visual prime word (e.g. SPORT 'sport') when the spoken word was mis-stressed. The adverse effect of mis-stressing confirms the role of stress information in lexical recognition in Dutch. However, although the erroneous stress pattern was always initially compatible with a competing word (e.g. ATlas, 'atlas'), mis-stressed words did not produced high false alarm rates in unrelated pairs (e.g. SPORT - atLAS). This suggests that stress information did not completely rule out segmentally matching but suprasegmentally mismatching words, a finding consistent with spoken-word recognition models involving multiple activation and inter-word competition.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-147"
  },
  "eldin00_icslp": {
   "authors": [
    [
     "Safa Nasser",
     "Eldin"
    ],
    [
     "Hanna Abdel",
     "Nour"
    ],
    [
     "Rajouani",
     "Abdenbi"
    ]
   ],
   "title": "Automatic modeling and implementation of intonation for the arabic language in TTS systems",
   "original": "i00_1597",
   "page_count": 5,
   "order": 148,
   "p1": "vol. 1, 597-600",
   "pn": "",
   "abstract": [
    "This paper proposes a set of rules for the automatic generation of F0 contours for modern standard Arabic (MSA) affirmative and interrogative sentences. The objective is to finalize a model for the automatic processing of the intonative pattern in different TTS systems (e.g. synthesis by formants using Klatt synthesizer and synthesis by diphones, using a synthesizer based on PSOLA algorithm). The approach is based on the perceptive description of the Arabic intonation by extracting the pertinent variation of the intonative curve using a suitable stylization.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-148"
  },
  "gadde00_icslp": {
   "authors": [
    [
     "Venkata Ramana Rao",
     "Gadde"
    ]
   ],
   "title": "Modeling word durations",
   "original": "i00_1601",
   "page_count": 4,
   "order": 149,
   "p1": "vol. 1, 601-604",
   "pn": "",
   "abstract": [
    "We describe a new method of modeling duration at word level. These duration models are easily trained from the acoustic training data and can be used to rescore N-best lists of recognition hypotheses. The models capture some of the well known durational effects such as prepausal lengthening. They incorporate a simple back off mechanism to handle unseen words during rescoring. Experiments with various large vocabulary conversational speech recognition (LVCSR) evaluation sets showed consistent improvements of 0.7-1.0% in word error rate (WER).\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-149"
  },
  "venditti00_icslp": {
   "authors": [
    [
     "Jennifer J.",
     "Venditti"
    ],
    [
     "Jan P. H. van",
     "Santen"
    ]
   ],
   "title": "Japanese intonation synthesis using superposition and linear alignment models",
   "original": "i00_1605",
   "page_count": 4,
   "order": 150,
   "p1": "vol. 1, 605-608",
   "pn": "",
   "abstract": [
    "This paper outlines a new approach to Tokyo Japanese intonation synthesis, in which the F0 contour of an utterance is generated using the superposition of multi-level phrase curves and lexical accent curves, coupled with linear alignment models which determine the precise alignment of the curves with the segmental material. We first discuss the construction of a phrase curve used to model the prosodic domain termed the 'UA-group' (defined below), and describe the alignment of this curve with the syllabic structure of the utterance. Then, we describe a separate accent curve, carrying independent prominence specification, which is added to this UA-curve in the case of accented phrases. The alignment of the accent curve with the segments is determined by linear alignment models.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-150"
  },
  "minowa00_icslp": {
   "authors": [
    [
     "Toshimitsu",
     "Minowa"
    ],
    [
     "Ryo",
     "Mochizuki"
    ],
    [
     "Hirofumi",
     "Nishimura"
    ]
   ],
   "title": "Improving the naturalness of synthetic speech by utilizing the prosody of natural speech",
   "original": "i00_1609",
   "page_count": 5,
   "order": 151,
   "p1": "vol. 1, 609-612",
   "pn": "",
   "abstract": [
    "The quality of synthetic speech is greatly improved if a prosody of natural speech is adopted instead of a rule based prosody. In order to apply this effect to an arbitrary word synthesis, the authors proposed a new prosody control method. According to the result of a listening test, it was shown that rhythm could be independently controlled from pitch and power whereas pitch and power should be dependently controlled. Therefore, it seems that pitch and power control method should be derived from the same speech. However, in a embedded types of practical arbitrary word synthesis, the amount of memory is so limited that there is little room for redundant data. So, authors systematically derived the prosody (pitch interval and pitch waveform amplitude) of continuously uttered mono syllable speech which cover most Japanese accent types. Syllables are chosen in accordance with the categories of Japanese consonants which are classified by manner and place of articulation. By this method, the naturalness of the synthetic speech achieved almost the same preference score with the one that copied the prosody of natural speech.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-151"
  },
  "chen00c_icslp": {
   "authors": [
    [
     "Sin-Horng",
     "Chen"
    ],
    [
     "Chen-Chung",
     "Ho"
    ]
   ],
   "title": "A hybrid statistical/RNN approach to prosody synthesis for taiwanese TTS",
   "original": "i00_1613",
   "page_count": 4,
   "order": 152,
   "p1": "vol. 1, 613-616",
   "pn": "",
   "abstract": [
    "In this paper a hybrid approach which incorporates statistical modeling of prosodic parameters into recurrent neural network (RNN)-based prosody synthesis for Min-Nan speech (Taiwanese) is proposed. It takes syllable as the basic synthesis unit and constructs statistical models for syllable-initial duration, syllable-final duration, inter-syllable pause duration, pitch contour of syllabIe, and log-energy level of syllabloe. In the training, it normalizes prosodic parameters by these statistical models and uses the results to train an RNN prosody synthesizer. In synthesis, it denormalizes the RNN outputs by the same statistical models to generate all prosodic parameters required by the TTS system. The advantage of the approach can be justified as to relieve the RNN prosody synthesizer of some affecting factors via taking care them by using the statistical models.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-152"
  },
  "minematsu00_icslp": {
   "authors": [
    [
     "Nobuaki",
     "Minematsu"
    ],
    [
     "Yukiko",
     "Fujisawa"
    ],
    [
     "Seiichi",
     "Nakagawa"
    ]
   ],
   "title": "Performance comparison among HMM, DTW, and human abilities in terms of identifying stress patterns of word utterances",
   "original": "i00_1617",
   "page_count": 4,
   "order": 153,
   "p1": "vol. 1, 617-620",
   "pn": "",
   "abstract": [
    "We have been focusing on applying speech technologies to pronunciation learning. In our previous study [1], a stressed syllable detector was implemented by using stressed syllable HMMs and unstressed ones. And using the detector internally, several systems were implemented [2]. However, their development did not necessarily require the use of HMMs as an acoustic modeling method. In this paper, an HMM-based method, a DTW-based method, and a human strategy only with visual inspection were compared in terms of their performance in judging whether two utterances of a word have the same stress pattern, e.g. r´ecord and rec´ord. Here, one utterance was given by a Japanese learner and the other one was done by a native speaker. Experiments showed that HMMs gave us the higher performance than DTW and even human strategies. This result strongly supports the use of HMMs as an acoustic modeling method in the stressed syllable detector development.\n",
    "s N. Minematsu et al., \"Automatic detection of accent in English words spoken by Japanese students,\" Proc. EUROSPEECH97, pp.701-704 (1997). N. Minematsu et al., \"Prosodic evaluation of English words spoken by Japanese based upon estimating their pronunciation habits,\" Proc. ICSP, pp.439-444 (1999)\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-153"
  },
  "montero00_icslp": {
   "authors": [
    [
     "Juan Manuel",
     "Montero"
    ],
    [
     "Ricardo",
     "Córdoba"
    ],
    [
     "José A.",
     "Vallejo"
    ],
    [
     "Juana",
     "Gutiérrez-Arriola"
    ],
    [
     "Emilia",
     "Enríquez"
    ],
    [
     "Juan Manuel",
     "Pardo"
    ]
   ],
   "title": "Restricted-domain female-voice synthesis in Spanish: from database design to ANN prosodic modeling",
   "original": "i00_1621",
   "page_count": 4,
   "order": 154,
   "p1": "vol. 1, 621-624",
   "pn": "",
   "abstract": [
    "In this paper, we describe the development of a female voice in a Restricted-Domain Speech Synthesis System for Spanish. For the design of the database, we have used a greedyalgorithm approach that focus not only on covering a set of target phonemes, but also on mimicking the histogram of prosodic features from a larger database. For modeling the prosody, both duration and F0, we have used two Multi-Layer Perceptrons, based on our previous experience in unrestricted-domain modeling. The error normalised by the deviation is always below 0.7.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-154"
  },
  "fernandezsalgado00_icslp": {
   "authors": [
    [
     "Xavier",
     "Fernández-Salgado"
    ],
    [
     "Eduardo R.",
     "Banga"
    ]
   ],
   "title": "A hierarchical intonation model for synthesising F0 contours in galician language",
   "original": "i00_1625",
   "page_count": 4,
   "order": 155,
   "p1": "vol. 1, 625-628",
   "pn": "",
   "abstract": [
    "In this contribution we propose a hierarchical intonation model for synthesising f0 contours with application to text-to-speech synthesis in Galician language. This model makes use of the implicit knowledge that resides in a database of natural f0 contours obtained from a read corpus. The novelty of this method lies on the way the f0 contour is generated. First, no phonological description in terms of a sequence of tones is needed prior to f0 generation. The phrasing obtained from previous stages of the TTS system is enough for this task. Second, the final f0 contour is built through several steps that assign patterns at the phonic group level (intonational phrase), the tonic group level and the segmental level following a hierarchical method. The proposed algorithm guarantees a coherent concatenation of the patterns that belong to different levels, and it seems to work properly as a general intonation model for a wide range of sentence modalities.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-155"
  },
  "applebaum00_icslp": {
   "authors": [
    [
     "Ted H.",
     "Applebaum"
    ],
    [
     "Nick",
     "Kibre"
    ],
    [
     "Steve",
     "Pearson"
    ]
   ],
   "title": "Features for F0 contour prediction",
   "original": "i00_1629",
   "page_count": 4,
   "order": 156,
   "p1": "vol. 1, 629-632",
   "pn": "",
   "abstract": [
    "Decision trees based on features derived from text analysis have previously been used to predict the input parameters of models of F0 contour for text-to-speech synthesis. Yet it is not known which features contribute most to the success of the prediction. This paper quantifies the dependence of the predicted F0 contour on each of several input features derived from the text.\n",
    "Parameters for the Tilt intonation model of F0 contour were predicted by decision trees trained on 6 simple features or 17 features derived from a rule-based front end. To evaluate the contribution of each input feature, F0 prediction error measures were first compared within a group of predictors where each predictor considered only a single input feature, and then within a second group of predictors where each predictor ignored one of the input features.\n",
    "F0 prediction error was measured on a new speaker by RMS deviation, mean absolute deviation and correlation. Similar trends were observed for each error measure.\n",
    "The features observed to most strongly affect F0 prediction were \"position in the word of the following syllable\", \"percent of the way through a breath group\", \"presence of prosodic boundary at the end of the syllable\" and \"stress of the current syllable\". These features are defined over different time scales and demonstrate how a local model of F0 contour can capture global properties.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-156"
  },
  "gu00b_icslp": {
   "authors": [
    [
     "Zhenglai",
     "Gu"
    ],
    [
     "Hiroki",
     "Mori"
    ],
    [
     "Hideki",
     "Kasuya"
    ]
   ],
   "title": "Prosodic variation of focused syllables of disyllabic word in Mandarin Chinese",
   "original": "i00_1633",
   "page_count": 4,
   "order": 157,
   "p1": "vol. 1, 633-636",
   "pn": "",
   "abstract": [
    "This paper describes a study on how disyllabic on-focus words under a certain context condition affects the variations of prosodic correlates of syllables in terms of different focal positions in Mandarin Chinese. The study shows that: (1) there is no direct link between semantic structure of disyllabic on-focus words and the prosodic correlates, but an indirect link through the mediation of focus positions; (2) the prosodic correlates of the second syllable are more variable than those of the first syllable. Specifically, there is a significant asymmetry of vowel duration and fundamental frequency (F0) range between the pre-focus and post-focus syllables.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-157"
  },
  "chu00_icslp": {
   "authors": [
    [
     "Stephen M.",
     "Chu"
    ],
    [
     "Thomas S.",
     "Huang"
    ]
   ],
   "title": "Automatic head gesture learning and synthesis from prosodic cues",
   "original": "i00_1637",
   "page_count": 4,
   "order": 158,
   "p1": "vol. 1, 637-640",
   "pn": "",
   "abstract": [
    "We present a novel approach to automatically learn and synthesize head gestures using prosodic features extracted from acoustic speech signals. A minimum entropy hidden Markov model is employed to learn the 3-D head-motion of a speaker. The result is a generative model that is compact and highly predictive. The model is further exploited to synchronize the head-motion with a set of continuous prosodic observations and gather the correspondence between the two by sharing its state machine. In synthesis, the prosodic features are used as the cue signal to drive the generative model so that 3-D head gestures can be inferred.\n",
    "A tracking algorithm based on the Bézier volume deformation model is implemented to track the head-motion. To evaluate the performance of the proposed system, we compare the true head-motion with the prosody-inferred motion. The prosody to head-motion mapping acquired through learning is subsequently applied to animate a talking head. Very convincing head-gestures are produced when novel prosodic cues of the same speaker are presented.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-158"
  },
  "vainio00_icslp": {
   "authors": [
    [
     "Martti",
     "Vainio"
    ],
    [
     "Toomas",
     "Altosaar"
    ],
    [
     "Stefan",
     "Werner"
    ]
   ],
   "title": "Measuring the importance of morphological information for finnish speech synthesis",
   "original": "i00_1641",
   "page_count": 4,
   "order": 159,
   "p1": "vol. 1, 641-644",
   "pn": "",
   "abstract": [
    "The basic assumption in intonation models and perhaps generally in prosody models is, that part-of-speech information is of paramount importance for predicting the actual values for the prosodic parameters; be they pitch, segmental duration or loudness. We have studied whether morphological knowledge, in addition to part-of-speech and functional information, is of any help in predicting prosody in a morphologically rich language such as Finnish. Our research concerns Finnish prosody with respect to pitch and segmental duration. The basic methodology we employ is based on artificial neural networks. It is a continuation of our earlier studies on prosody where we investigated the problem of generating values for prosodic parameters for text-to-speech synthesis. The basic methodology we employ is based on standard multi-layer feed-forward networks that are trained with backpropagation. The results we have obtained show that there are certain advantages in adding morphological knowledge to the network input. Apart from part-of-speech information, there are certain cases where morphological features seem to affect the outcome of both pitch and segmental durations. This behavior can be expected in a morphologically rich language.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-159"
  },
  "jokisch00_icslp": {
   "authors": [
    [
     "Oliver",
     "Jokisch"
    ],
    [
     "Hansjörg",
     "Mixdorff"
    ],
    [
     "Hans",
     "Kruschke"
    ],
    [
     "Ulrich",
     "Kordon"
    ]
   ],
   "title": "Learning the parameters of quantitative prosody models",
   "original": "i00_1645",
   "page_count": 4,
   "order": 160,
   "p1": "vol. 1, 645-648",
   "pn": "",
   "abstract": [
    "The article introduces a novel hybrid data driven and rule based approach for the prosody control in a TTS system, which combines the advantages of well-balanced, quantitative models with the flexible training of derived model parameters. Instancing the training of Fujisaki intonation parameters for German (MFGI) the article describes the hybrid data driven and rule based architecture HYDRA, the speech database, the extraction of the model parameters and the neural network (NN) training of these parameters. Preliminary results using the hybrid intonation model are presented. A hybrid neural network and rule based, quantitative model can be easily parameterized and adapted e.g. for multilingual applications, but has a higher complexity and requires the automatic extraction of the model parameters from a speech database.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-160"
  },
  "narusawa00_icslp": {
   "authors": [
    [
     "Shuichi",
     "Narusawa"
    ],
    [
     "Hiroya",
     "Fujisaki"
    ],
    [
     "Sumio",
     "Ohno"
    ]
   ],
   "title": "A method for automatic extraction of parameters of the fundamental frequency contour",
   "original": "i00_1649",
   "page_count": 4,
   "order": 161,
   "p1": "vol. 1, 649-652",
   "pn": "",
   "abstract": [
    "The process of generating an F0 contour has been modeled quite accurately in mathematical terms by Fujisaki and his coworkers, but the derivation of the underlying commands from an observed F0 contour is an inverse problem that cannot be solved analytically. Although it can be solved by successive approximation, a good first-order approximation is necessary to guarantee an efficient and accurate search for the optimum solution. The present paper describes a method for pre-processing an observed F0 contour to obtain a smooth contour, from which a good first-order approximation can be analytically obtained. Experimental results show that correct extraction rates for the accent and the phrase commands are about 90% and 79%, respectively.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-161"
  },
  "kitazoe00b_icslp": {
   "authors": [
    [
     "Tetsuro",
     "Kitazoe"
    ],
    [
     "Sung-Ill",
     "Kim"
    ],
    [
     "Yasunari",
     "Yoshitomi"
    ],
    [
     "Tatsuhiko",
     "Ikeda"
    ]
   ],
   "title": "Recognition of emotional states using voice, face image and thermal image of face",
   "original": "i00_1653",
   "page_count": 4,
   "order": 162,
   "p1": "vol. 1, 653-656",
   "pn": "",
   "abstract": [
    "A new integration method is presented to recognize the emotional expressions. We attempted to use both voices and facial expressions. For voices, we use such prosodic parameters as pitch signals, energy, and their derivatives, which are trained by Hidden Markov Model (HMM) for recognition. For facial expressions, we use feature parameters from thermal images in addition to visible images, which are trained by neural networks (NN) for recognition. The thermal images are observed by infrared ray which is not influenced by lighting conditions. The total recognition rates show better performance than each performance rate obtained from isolated experiment. The results are compared with the recognition by human questionnaire.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-162"
  },
  "watanuki00_icslp": {
   "authors": [
    [
     "Keiko",
     "Watanuki"
    ],
    [
     "Susumu",
     "Seki"
    ],
    [
     "Hideo",
     "Miyoshi"
    ]
   ],
   "title": "Turn taking and multimodal information in two-people dialog",
   "original": "i00_1657",
   "page_count": 4,
   "order": 163,
   "p1": "vol. 1, 657-660",
   "pn": "",
   "abstract": [
    "This research focuses on the relationship between turntaking and intensity of motions (head, hand and upper body motions). We examine five spontaneous dialogues conducted in Japanese, which were collected using video and audio recording equipment, and optical motion sensing device. We show first that the motions appear more in speech duration than in pause. Next, we present some data showing that motions of chest and hand are more relevant to turn hold, and that motions of head do not have a signal function for differentiating the turn hold form the turn change.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-163"
  },
  "abutalebi00_icslp": {
   "authors": [
    [
     "Hamid Reza",
     "Abutalebi"
    ],
    [
     "Mahmood",
     "Bijankhan"
    ]
   ],
   "title": "Implementation of a text-to-speech system for farsi language",
   "original": "i00_1661",
   "page_count": 4,
   "order": 164,
   "p1": "vol. 1, 661-664",
   "pn": "",
   "abstract": [
    "In this research, a Text-To-Speech system for Farsi language has been implemented. The proposed synthesizer concatenates Farsi syllables in a TD-PSOLA manner. This paper is mainly concentrated on investigation about pitch variations in Farsi sentences and presentation of some novel rules for modeling these variations. Based on the location of stressed syllable, we obtain a primary pitch curve for each word. Using prosodic grouping and sentence type effects, the final pitch contour can be determined. High intelligibility and acceptable naturalness of the synthesized speech have been confirmed by subjective listening tests.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-164"
  },
  "huber00_icslp": {
   "authors": [
    [
     "Richard",
     "Huber"
    ],
    [
     "Anton",
     "Batliner"
    ],
    [
     "Jan",
     "Buckow"
    ],
    [
     "Elmar",
     "Nöth"
    ],
    [
     "Volker",
     "Warnke"
    ],
    [
     "Heinrich",
     "Niemann"
    ]
   ],
   "title": "Recognition of emotion in a realistic dialogue scenario",
   "original": "i00_1665",
   "page_count": 4,
   "order": 165,
   "p1": "vol. 1, 665-668",
   "pn": "",
   "abstract": [
    "Nowadays modern automatic dialogue systems are able to understand complex sentences instead of only a few commands like Stop or No. In a call-center, such a system should be able to determine in a critical phase of the dialogue if the call should be passed over to a human operator. Such a critical phase can be indicated by the customer's vocal expression. Other studies prooved that it is possible to distinguish between anger and neutral speech with prosodic features alone. Subjects in these studies were mostly people acting or simulating emotions like anger. In this paper we use data from a so-called Wizard of Oz (WoZ) scenario to get more realistic data instead of simulated anger. As shown below, the classification rate for the two classes \"emotion\" (class E) and \"neutral\" (class :E) is signiftcantly worse for these more realistic data. Furthermore the classification results are heavily speaker dependent. Prosody alone might thus not be sufficient and has to be supplemented by the use of other knowledge sources such as the detection of repetitions, reformulations, swear words, and dialogue acts.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-165"
  },
  "barry00_icslp": {
   "authors": [
    [
     "Johanna",
     "Barry"
    ],
    [
     "Peter",
     "Blamey"
    ],
    [
     "Kathy",
     "Lee"
    ],
    [
     "Dilys",
     "Cheung"
    ]
   ],
   "title": "Differentiation in tone production in cantonese-speaking hearing-impaired children",
   "original": "i00_1669",
   "page_count": 4,
   "order": 166,
   "p1": "vol. 1, 669-672",
   "pn": "",
   "abstract": [
    "This paper describes results from a pilot study investigating the process of tone development and differentiation in three hearing-impaired children who received a 22-electrode Cochlear Implant (CI). The hypothesis to be tested is that subsequent to receiving an implant, the children will begin to acquire a tonal inventory and will acquire a full tonal inventory more rapidly than they acquire a full vowel inventory. The hypothesis was tested by plotting F0 offset versus F0 onset for production of three tones (high level, (T1), high-rising (T2) and low-falling (T4)) pre- and post-implant. Results were compared with phonetic transcriptions of the same materials. Contrary to expectation, rate of acquisition of the tonal inventory is found to be slower than the rate of development of the vowel inventory for these children. The two children who acquired specific tones in the time frame of the study acquired them in the order, T1 before T2 while none of the children acquired T4.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-166"
  },
  "zundert00_icslp": {
   "authors": [
    [
     "Martine van",
     "Zundert"
    ],
    [
     "Jacques",
     "Terken"
    ]
   ],
   "title": "Learning effects for phonetic properties of synthetic speech",
   "original": "i00_1673",
   "page_count": 4,
   "order": 167,
   "p1": "vol. 1, 673-676",
   "pn": "",
   "abstract": [
    "We address the question of what is learned while listening to synthetic speech produced by means of diphone-based synthesis. In standard diphone-based speech synthesis, the diphone database contains a single token for each phoneme transition. Learning may occur at different levels: listeners may learn the mapping between acoustic properties of particular diphones and their phonemic labelling; or they may learn phoneme models; or they may learn realisations of phonological features. Predictions of the different hypotheses are tested in an experiment in which we determine the improvement in intelligibility as a result of training for a specially constructed set of stimuli. The results force us to reject the hypothesis that listeners learn realisations of phonological features. However, they do not exclude the possibility that subjects learn phoneme models, although the results supporting this hypothesis do not reach significance.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-167"
  },
  "tomokiyo00_icslp": {
   "authors": [
    [
     "Laura Mayfield",
     "Tomokiyo"
    ],
    [
     "Le",
     "Wang"
    ],
    [
     "Maxine",
     "Eskenazi"
    ]
   ],
   "title": "An empirical study of the effectiveness of speech-recognition-based pronunciation training",
   "original": "i00_1677",
   "page_count": 4,
   "order": 168,
   "p1": "vol. 1, 677-680",
   "pn": "",
   "abstract": [
    "We have tested a fixed version of the Fluency foreign language pronunciation trainer to determine whether users actually improve their pronunciation through its use. The Fluency system uses the CMU SPHINX II speech recognizer to pinpoint pronunciation errors and then gives suggestions as to how to correct them. Users in the experiment we will describe in this paper were to learn to pronounce the voiced and voiceless interdental fricatives /Θ/ and /δ/, as in \"thin\" and \"that.\" The choice of these two phones allowed us to attract users from several native languages. In this paper, we present the results of this study, fully describing the test situation and drawing conclusions about Fluency as a testbed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-168"
  },
  "deroo00_icslp": {
   "authors": [
    [
     "Olivier",
     "Deroo"
    ],
    [
     "Christophe",
     "Ris"
    ],
    [
     "Sofie",
     "Gielen"
    ],
    [
     "Johan",
     "Vanparys"
    ]
   ],
   "title": "Automatic detection of mispronounced phonemes for language learning tools",
   "original": "i00_1681",
   "page_count": 4,
   "order": 169,
   "p1": "vol. 1, 681-684",
   "pn": "",
   "abstract": [
    "Automatic Speech Recognition (ASR) can be very useful in language learning tools in order to correct mistakes in the pronunciation of foreign words by non-native speakers. Most of the systems integrating ASR proposed on the market are just rejecting or accepting whole words or whole sentences. In this paper, we propose a method to identify the pronunciation errors at the phoneme level. Indeed, mistakes are often predictable and concern a particular subset of phonemes not present in the mother language of the speaker. We describe two different approaches based on the Hybrid HMM/ANN technology. The methodology for the training of the recognizer is discussed, and we describe a new approach where a mixed database is used to train a speech recognition system able to detect pronunciation errors at the phoneme level. Preliminary but promising results have been obtained on the DEMOSTHENES database.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-169"
  },
  "escalona00_icslp": {
   "authors": [
    [
     "Horacio Meza",
     "Escalona"
    ],
    [
     "Ingrid",
     "Kirschning"
    ],
    [
     "Ofelia Cervantes",
     "Villagómez"
    ]
   ],
   "title": "Estimation of duration models for phonemes in m exican speech synthesis",
   "original": "i00_1685",
   "page_count": 4,
   "order": 170,
   "p1": "vol. 1, 685-688",
   "pn": "",
   "abstract": [
    "Voice is the most used communication media. For this reason, the voice is the natural media in the human-computer interaction and it is more used nowadays. A person does not need specialized training to use a computer when using voice as media. It is of general interest the study of the speech and even for trying to construct machines or systems to produce speech automatically.\n",
    "This work shows the results of the conducted measures to obtain duration models that can get up in a Spanish synthesis system like Festival. In addition, The characteristics of a corpora used in the experimentation are described too. This work shows the results obtained by experimenting with isolated phonemes, as well as phonemes with left context, right context and both. Finally, there is a discussion on the results that illustrate the improvements obtained in the synthesis system using the new models of duration and some other works to future also set out.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-170"
  },
  "wu00b_icslp": {
   "authors": [
    [
     "Xiaoru",
     "Wu"
    ],
    [
     "Renhua",
     "Wang"
    ],
    [
     "Guoping",
     "Hu"
    ]
   ],
   "title": "Special text processing based external descriptor rule",
   "original": "i00_1689",
   "page_count": 4,
   "order": 171,
   "p1": "vol. 1, 689-692",
   "pn": "",
   "abstract": [
    "In this paper, a special text processing method is presented, which integrates external descriptor rule and model matching. The letter-to-sound rules for a given kind of special mark can be described by some describable features which can be extracted from this kind of special mark, and then a corresponding model can be built and then the model parameter can be stored into a external form. Since the features can be extracted and described conveniently, a new model can be built easily for improving the ability to analyze special text.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-171"
  },
  "yu00b_icslp": {
   "authors": [
    [
     "Zhenli",
     "Yu"
    ],
    [
     "Shangcui",
     "Zeng"
    ]
   ],
   "title": "Articulatory synthesis using a vocal-tract model of variable length",
   "original": "i00_1693",
   "page_count": 4,
   "order": 172,
   "p1": "vol. 1, 693-696",
   "pn": "",
   "abstract": [
    "A method of articulatory synthesis using a vocal-tract model with variable length is proposed. The vocal-tract length is derived prior to the unique determination of vocal-tract area parameters incorporated with a codebook that maps formants to vocal-tract length is used. A two-dimensional interpolation function for irregularly spaced data is conducted to confine vocal-tract length in the first and second formant plain to generate the codebook. The reflection-type line analog (RTLA) model of articulatory synthesis is employed to generate speech sound. Variable vocal-tract length yields the issue of discretetime implementation with multi-rate sampling of the RTLA model. Multi-rate sampling conversion is conducted.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-172"
  },
  "boulademareuil00_icslp": {
   "authors": [
    [
     "Philippe",
     "Boula de Mareüil"
    ]
   ],
   "title": "Linguistic-prosodic processing for text-to-speech synthesis in italian",
   "original": "i00_1697",
   "page_count": 4,
   "order": 173,
   "p1": "vol. 1, 697-700",
   "pn": "",
   "abstract": [
    "The linguistic-prosodic processing applied to text-to-speech synthesis in Italian is described. It proceeds in 5 steps: tokenisation and normalisation of abbreviations, numbers, etc.; part-of-speech tagging, based on function words, terminations and contextual heuristics; shallow parsing, based on a chunk grammar; grapheme-to-phoneme conversion, lexical stress assignment and syllabification by an expert system, particular attention being paid to loanwords and proper names; at least, prosody generation. The last three steps are emphasised.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-173"
  },
  "eichner00_icslp": {
   "authors": [
    [
     "Matthias",
     "Eichner"
    ],
    [
     "Matthias",
     "Wolff"
    ],
    [
     "Rüdiger",
     "Hoffmann"
    ]
   ],
   "title": "A unified approach for speech synthesis and speech recognition using stochastic Markov graphs",
   "original": "i00_1701",
   "page_count": 6,
   "order": 174,
   "p1": "vol. 1, 701-704",
   "pn": "",
   "abstract": [
    "With the progress of speech synthesis towards the development of complete TTS systems, the databases of speech synthesizers obtain more and more similarity with databases of speech recognizers. This offers new possibilities in combining systems for speech synthesis and recognition. In a new project, we are developing a speech dialogue system with the synthesis and recognition components using unified databases. In this paper, we describe the aim of the project, the framework of the system and first results at the acoustic and the word level.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-174"
  },
  "breen00_icslp": {
   "authors": [
    [
     "Andrew",
     "Breen"
    ],
    [
     "James",
     "Salter"
    ]
   ],
   "title": "Using F0 within a phonologically motivated method of unit selection",
   "original": "i00_1705",
   "page_count": 4,
   "order": 175,
   "p1": "vol. 1, 705-708",
   "pn": "",
   "abstract": [
    "The current generation of concatenative speech synthesis systems rely on the selection of appropriate pre-recorded speech units from a repository of sounds. This process, commonly referred to as unit selection, is a critical step in the production of natural sounding speech. However the process of unit selection is only as good as the labelling strategy used and the quality and style of the recordings. Simply stated, the unit selection process cannot select that which isn't labelled or recorded. These units once selected must be seamlessly concatenated and prosodically modified to reflect the desired rhythm and intonation. Traditionally this has been viewed as a signal-processing step. The most popular algorithms are based on (Pitch Synchronous Overlap Add) PSOLA or Harmonic plus noise (HMN) models, each has its strengths and weaknesses. Some researchers are of the opinion that signal processing should be kept to a minimum, as a result they have concentrated on building systems with extremely large databases of recorded speech. Unit selection within such systems has a vast amount of data from which to select an appropriate unit. As such, selected sounds tend to be close to the desired phonemic and prosodic contexts, the result of which is that little post selection signal processing is required. However such approaches have a number of practical and commercial disadvantages. Practically, large databases are difficult to record, annotate and manage while a large program footprint is commercially impractical for a number of applications.\n",
    "The Laureate Text to Speech system, originally developed at BT Adastral Park, differs from the approaches discussed above in that it does not use any acoustic properties of the speech signal in the unit selection process. Instead, it relies solely on a rich phonological representation in the labels associated with the speech data. However, such an approach does not take into account limitations in the post unit selection signal processing. The method described in this paper extends the basic Laureate philosophy to include sensitivity to the method of signal processing used within the system. In this technique, a systematic approach to the addition of speech data is adopted, which enables the developer to trade off quality against computational load and storage. The paper describes how multiple copies of the repository of speech data used in the unit selection process, recorded at different fundamental frequencies, may be used within the selection process, and how these multiple recordings are used to automatically control the degree of signal processing applied to the selected units.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-175"
  },
  "blouin00_icslp": {
   "authors": [
    [
     "Christophe J.",
     "Blouin"
    ],
    [
     "Paul C.",
     "Bagshaw"
    ]
   ],
   "title": "Analysis of the degradation of French vowels induced by the TD-PSOLA algorithm, in text-to-speech context",
   "original": "i00_1709",
   "page_count": 4,
   "order": 176,
   "p1": "vol. 1, 709-712",
   "pn": "",
   "abstract": [
    "In concatenative speech synthesis systems, synthetic speech is obtained by concatenating acoustic units selected from a database of natural speech. The duration and fundamental frequency (F0) of the selected units are usually different from those requested by a prosodic model, and so some prosodic modification must be applied to the units in order to obtain the desired target. TD-PSOLA is an effective and widely used prosodic modification algorithm, but its use can degrade the perceived quality of the synthetic speech signal. This paper focuses on the evaluation of the degradation of French vowels and determines the influence of several parameters through an analysis of variance. The results show that vowels divide into two groups, based on their first formant frequency (F1). Finally, a modification cost function representative of the degradation is derived from the investigation.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-176"
  },
  "janicki00_icslp": {
   "authors": [
    [
     "Artur",
     "Janicki"
    ]
   ],
   "title": "Automatic construction of acoustic inventory for the concatenative speech synthesis for polish",
   "original": "i00_1713",
   "page_count": 4,
   "order": 177,
   "p1": "vol. 1, 713-716",
   "pn": "",
   "abstract": [
    "This paper describes the automatic construction of the acoustic inventory for the purpose of development of the speech synthesis system for the Polish language. Very efficient algorithm of speech segmentation is proposed, also original process of reduction is described. Results of classification using LBG algorithm and brief description of created inventory are presented.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-177"
  },
  "hirschfeld00_icslp": {
   "authors": [
    [
     "Diane",
     "Hirschfeld"
    ],
    [
     "Matthias",
     "Wolff"
    ]
   ],
   "title": "Universal and multilingual unit selection for DRESS",
   "original": "i00_1717",
   "page_count": 4,
   "order": 178,
   "p1": "vol. 1, 717-720",
   "pn": "",
   "abstract": [
    "oiuzoiuzoiuzoiuz\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-178"
  },
  "pan00_icslp": {
   "authors": [
    [
     "Davis",
     "Pan"
    ],
    [
     "Brian",
     "Heng"
    ],
    [
     "Shiufun",
     "Cheung"
    ],
    [
     "Ed",
     "Chang"
    ]
   ],
   "title": "Improving speech synthesis for high intelligibility under adverse conditions",
   "original": "i00_1721",
   "page_count": 4,
   "order": 179,
   "p1": "vol. 1, 721-724",
   "pn": "",
   "abstract": [
    "We investigate methods of improving the intelligibility of synthetic speech under noisy or low-fidelity acoustic conditions. Techniques explored improve speech in a natural manner, such that training wont be required for the user to understand the enhanced speech. While the improvements are natural in this respect, the changes arent limited to creating only speech that is achievable by a human vocal tract. Modifications fall into three broad classes: increasing phoneme amplitude, altering spectral shape, and lengthening phoneme duration. Listening tests conducted in noisy and noise-free conditions demonstrate significant improvements to intelligibility for most of the subject phonemes.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-179"
  },
  "nishizawa00_icslp": {
   "authors": [
    [
     "Nobuyuki",
     "Nishizawa"
    ],
    [
     "Nobuaki",
     "Minematsu"
    ],
    [
     "Keikichi",
     "Hirose"
    ]
   ],
   "title": "Development of a formant-based analysis-synthesis system and generation of high quality liquid sounds of Japanese",
   "original": "i00_1725",
   "page_count": 4,
   "order": 180,
   "p1": "vol. 1, 725-728",
   "pn": "",
   "abstract": [
    "Although flexible control of acoustic features is possible in formant-based speech synthesizers, their development requires precise estimation of parameters related to vocal tract and source. This requirement is difficult to satisfy and often results in limiting quality of the synthesized speech. The difficulty is derived from the fact that estimation of the parameters is a nonlinear problem. Therefore, the completely automatic estimation of the parameters is quite difficult and some approximations or manual modifications of parameters with a priori knowledge are required in the development. In this study, mainly to make the estimation more efficient and/or to assist developers doing the manual modifications of parameters, a formant-based analysissynthesis system is build. The system introduces pitchsynchronous acoustic analysis to reduce fluctuation of the estimated parameters. Experiments show that quality of synthetic speech of Japanese /r/ sounds is significantly improved by using the proposed system.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-180"
  },
  "jokisch00b_icslp": {
   "authors": [
    [
     "Oliver",
     "Jokisch"
    ],
    [
     "Matthias",
     "Eichner"
    ]
   ],
   "title": "Synthesizing and evaluating an artificial language: klingon",
   "original": "i00_1729",
   "page_count": 4,
   "order": 181,
   "p1": "vol. 1, 729-732",
   "pn": "",
   "abstract": [
    "The synthesis of an artificial language can provide some interesting extensions for the evaluation of text-to-speech (TTS) systems. For the alternative evaluation of the TTS system DRESS a new module for the artificial language Klingon has been developed. The linguistic and phonetic structure of Klingon can be modeled mainly by rules, with less exceptions. This contribution introduces the multilingual features of the TTS system DRESS, the fundamentals of Klingon, the synthesis approach using an allophone inventory and the integration in the TTS system. Some examples are presented and the evaluation method is shortly discussed. Preliminary listening results validate the TTS approach of synthesizing an artificial language like Klingon and indicate the applicability for evaluation procedures. For a practical use, the speech database for the Klingon synthesis needs to be enlarged.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-181"
  },
  "olinsky00_icslp": {
   "authors": [
    [
     "Craig",
     "Olinsky"
    ],
    [
     "Alan W.",
     "Black"
    ]
   ],
   "title": "Non-standard word and homograph resolution for asian language text analysis",
   "original": "i00_1733",
   "page_count": 4,
   "order": 182,
   "p1": "vol. 1, 733-736",
   "pn": "",
   "abstract": [
    "In this paper we present a general model for text analysis of Asian languages (Chinese and Japanese). That is a method for mapping strings of characters to strings of identified trivially pronounceable words. This work is based on the English Non- Standard Word analysis model suitably augmented to deal with both the lack of spaces between words in Japanese and Chinese and addressing the issues of homographs. Results are present for the sub-components of the process.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-182"
  },
  "sen00_icslp": {
   "authors": [
    [
     "Zhang",
     "Sen"
    ],
    [
     "Katsuhiko",
     "Shirai"
    ]
   ],
   "title": "Re-estimation of LPC coefficients in the sense of l&inf; criterion",
   "original": "i00_1737",
   "page_count": 4,
   "order": 183,
   "p1": "vol. 1, 737-740",
   "pn": "",
   "abstract": [
    "Now the generally used approaches such as auto-correlation method and covariance method for estimating LPC coefficients are to solve a set of linear equations by using of Levinson-Durbin recursion or lattice formulations, but the LPC coefficients computed are best only in the sense of L2 criterion. For speech processing, L&inf; criterion is a more suitable measure metric. The idea of our approach is: from the initial values of LPC coefficients, the residual errors could be reduced step by step by using Least Squares process iteratively until the LPC coefficients are approximately best in the sense of L&inf; criterion. Furthermore, this approach could be applied to other problems for estimating some parameters.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-183"
  },
  "jung00_icslp": {
   "authors": [
    [
     "Sung-Kyo",
     "Jung"
    ],
    [
     "Yong-Soo",
     "Choi"
    ],
    [
     "Young-Cheol",
     "Park"
    ],
    [
     "Dae-Hee",
     "Youn"
    ]
   ],
   "title": "An efficient codebook search algorithm for EVRC",
   "original": "i00_1741",
   "page_count": 4,
   "order": 184,
   "p1": "vol. 1, 741-744",
   "pn": "",
   "abstract": [
    "In this paper, a fast implementation algorithm for EVRC encoder is presented. Among the routines of CELP-type speech coders, the fixed codebook search is the most computationally demanding. The fast algorithm developed in this study mainly concerns the fixed codebook search procedure. For efficient codebook search, two efficient schemes are developed: limiting the number of possible combination of pulse positions, and using a reduced-tap FIR filter instead of the impulse response of the weighted synthesis filter. The proposed EVRC algorithm was implemented using a fixed-point DSP, and the new EVRC algorithm was subjectively evaluated. It was found that the new FCB search routine could be implemented with only 24.3 % of the original one. Informal subjective tests confirmed that the speech quality of the proposed algorithm was indistinguishable from the original EVRC.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-184"
  },
  "kim00b_icslp": {
   "authors": [
    [
     "Jong-Kuk",
     "Kim"
    ],
    [
     "Jeong-Jin",
     "Kim"
    ],
    [
     "Myung-Jin",
     "Bae"
    ]
   ],
   "title": "The reduction of the search time by the pre-determination of the grid bit in the g.723.1 MP-MLQ",
   "original": "i00_1745",
   "page_count": 5,
   "order": 185,
   "p1": "vol. 1, 745-749",
   "pn": "",
   "abstract": [
    "In general CELP type vocoders provide good speech quality around 4.8kbps. Among them, G.723.1 developed for Internet Phone and videoconferencing includes two vocoders, 5.3kbps ACELP and 6.3kbps MP-MLQ. Since 6.3kbps MP-MLQ requires large amount of computation for the fixed codebook search, it is difficult to realize real time processing for the Internet-based environment. In order to improve the problem this paper proposes the new method that reduces the processing time up to about 50% of codebook search time. We first decide the gr id bit, and then search the codebook. Grid bit is selected by comparison between the DC-removed original speech and synthesized speech, which is synthesized with only even or odd pulses of target vector.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-185"
  },
  "moller00_icslp": {
   "authors": [
    [
     "Sebastian",
     "Möller"
    ],
    [
     "Hervé",
     "Bourlard"
    ]
   ],
   "title": "Real-time telephone transmission simulation for speech recognizer and dialogue system evaluation and improvement",
   "original": "i00_1750",
   "page_count": 4,
   "order": 186,
   "p1": "vol. 1, 750-753",
   "pn": "",
   "abstract": [
    "Recognizer performance in telephone-based spoken dialogue systems may be strongly affected by the transmission channel. In order to investigate the impact of different parts of the transmission channel in more detail, a simulation model is presented. It implements all transmission characteristics of modern telephone networks, based on instrumentally measurable values as they are used by network planners. The simulation shows real-time capability and runs on a programmable DSP-based hardware. It can be used for a systematic investigation of recognizer performance as a function of transmission channel degradations, for producing training material with specified transmission characteristics, or for estimating the impact of transmission impairments on dialogue flow and system usability. The impact of transmission channel characteristics on the performance of a speech recognizer integrated in an interactive voice server is analyzed in more detail. It turns out that specific transmission characteristics may lead to a recognition degradation which otherwise would not have been expected from the standard training material. An outlook is given on future extensions of the simulation model, in order to better cover effects of mobile and IP-based telephone systems.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-186"
  },
  "chengalvarayan00_icslp": {
   "authors": [
    [
     "Rathinavelu",
     "Chengalvarayan"
    ],
    [
     "David L.",
     "Thomson"
    ]
   ],
   "title": "HMM-based echo and announcement modeling approaches for noise suppression avoiding the problem of false triggers",
   "original": "i00_1754",
   "page_count": 4,
   "order": 187,
   "p1": "vol. 1, 754-757",
   "pn": "",
   "abstract": [
    "In the past, we proposed an HMM-based residual echo modeling technique that proved to be effective in eliminating false triggering and enhanced the recognition of valid keyword speech. The narrow training strategy based on single recorded prompt made the echo model a better discriminator, though it has the possible drawback of requiring retraining if the prompt changes. To overcome this intrinsic problem, we consider building a general echo model that is trained over many sentences from varied voices. The experimental results show that the multiple echo models in conjunction with suitable filler model to represent the extraneous speech not only provide good recognition accuracy but also yield better out-of-vocabulary rejection.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-187"
  },
  "chen00d_icslp": {
   "authors": [
    [
     "Fangxin",
     "Chen"
    ]
   ],
   "title": "Speaker information enhancement",
   "original": "i00_1758",
   "page_count": 4,
   "order": 188,
   "p1": "vol. 1, 758-761",
   "pn": "",
   "abstract": [
    "This study consists of two experiments*. Experiment A investigates speaker-information distribution in the parametric domain. Experiment B compares different weighting strategies for speaker-information enhancement. The results indicate that weightings based on speaker-information distribution in the parametric domain yield better speaker recognition performance. how it statistically affects the separation of this speaker from the rest speakers in the database. The degree of its effect can be used as an indicator of the amount of speaker-information coded in that particular MFCC coefficient.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-188"
  },
  "dolfing00_icslp": {
   "authors": [
    [
     "Hans",
     "Dolfing"
    ]
   ],
   "title": "Exhaustive search for lower-bound error-rates in vocal tract length normalization",
   "original": "i00_1762",
   "page_count": 4,
   "order": 189,
   "p1": "vol. 1, 762-765",
   "pn": "",
   "abstract": [
    "In the context of large-vocabulary, continuous speech recognition, we address the problem of speaker normalization. In particular, we address the main drawback of many vocal tract length normalization (VTLN) studies and explore the relation between achieved and potential error-rate reduction. In other words, we investigate the correlations between the estimated and optimal warping factors. In addition, we compare the merits of maximum-likelihood VTLN and fast VTLN with the best possible, optimal error-rate for every approach. The experimental results include achieved error-rate reductions of 13.5% and a potential error-rate reduction of about 20%. We show that maximum-likelihood VTLN achieves 90% of the potential, speaker-based, error-rate reduction.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-189"
  },
  "macho00_icslp": {
   "authors": [
    [
     "Dusan",
     "Macho"
    ],
    [
     "Climent",
     "Nadeu"
    ]
   ],
   "title": "Use of voicing information to improve the robustness of the spectral parameter set",
   "original": "i00_1766",
   "page_count": 4,
   "order": 190,
   "p1": "vol. 1, 766-769",
   "pn": "",
   "abstract": [
    "Speech recognition systems that operate in real world environments have to be robust against additive noises. In this work, a technique that uses a voicing-dependent exponent in the computation of the filter-bank parameters to improve their robustness to additive noise is presented. Speech recognition experiments with the Aurora 1.0 database and recognition setup are reported to show the potential of this new technique.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-190"
  },
  "yao00_icslp": {
   "authors": [
    [
     "Kaisheng",
     "Yao"
    ],
    [
     "Bertram E.",
     "Shi"
    ],
    [
     "Satoshi",
     "Nakamura"
    ],
    [
     "Zhigang",
     "Cao"
    ]
   ],
   "title": "Residual noise compensation by a sequential EM algorithm for robust speech recognition in nonstationary noise",
   "original": "i00_1770",
   "page_count": 4,
   "order": 191,
   "p1": "vol. 1, 770-773",
   "pn": "",
   "abstract": [
    "We model noise as a stationary component plus a time varying residual. The stationary part is estimated off-line and compensated using Log-Add noise compensation. The time varying residual is estimated and compensated using a sequential EM algorithm. The residual noise compensation proceeds in parallel with the recognition process. Experimental results demonstrate that the proposed algorithm improves the recognition performance not only in highly nonstationary noise but also in slow-varying noise, compared with Log-Add noise compensation alone.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-191"
  },
  "ye00_icslp": {
   "authors": [
    [
     "Hui",
     "Ye"
    ],
    [
     "Pascale",
     "Fung"
    ],
    [
     "Taiyi",
     "Huang"
    ]
   ],
   "title": "Principal mixture speaker adaptation for improved continuous speech recognition",
   "original": "i00_1774",
   "page_count": 4,
   "order": 192,
   "p1": "vol. 1, 774-777",
   "pn": "",
   "abstract": [
    "Nowadays, almost all speaker-independent (SI) speech recognition systems use CDHMM with multivariate mixture Gaussian as observation density to cover speaker variabilities. It has been shown that given sufficient training data, the more mixtures are used in the HMM observation density, the better the systems perform. However, acoustic HMM with more Gaussian densities is more complex and slows down recognition speed. Another efficient way to handle speaker variation is to use speaker adaptation (SA). Yet, even though speaker adaptation of full multivariate mixture Gaussian densities can increase recognition accuracy, it does not improve recognition speed. In this paper, we introduce a principal mixture speaker adaptation method which reduces HMM complexity by choosing only the principle mixtures corresponding to a particular speakers characteristics. We show that our method both improves recognition accuracy by 31.8% when compared to SI models, and reduces recognition speed by 30%, when compared to full mixture SA models.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-192"
  },
  "altosaar00_icslp": {
   "authors": [
    [
     "Toomas",
     "Altosaar"
    ],
    [
     "Martti",
     "Vainio"
    ]
   ],
   "title": "Reduced impedance mismatch in speech database access",
   "original": "i00_1778",
   "page_count": 4,
   "order": 193,
   "p1": "vol. 1, 778-781",
   "pn": "",
   "abstract": [
    "Database access can be seen as a series of communicating systems each contributing to the overall efficiency of the query. This paper investigates the effect that impedance mismatch has on speech database access: the efficiency of database transactions between the corpus and the querying application(s). A system where impedance mismatch has been reduced through an object-oriented paradigm is presented and several query examples are shown. The paper suggests that for applications requiring high rates of database access for data with many expressed relationships a tightly coupled object-oriented system extending from the corpus to the end-user application is required.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-193"
  },
  "tian00_icslp": {
   "authors": [
    [
     "Jiapeng",
     "Tian"
    ],
    [
     "Jouji",
     "Miwa"
    ]
   ],
   "title": "Internet training system for listening and pronunciation of Chinese stop consonants",
   "original": "i00_1782",
   "page_count": 4,
   "order": 194,
   "p1": "vol. 1, 782-785",
   "pn": "",
   "abstract": [
    "This paper describes a system for Chinese pronunciation and listening training on the Internet. The target Chinese stop consonants /b/, /p/, /d/, /t/ were chosen for this experiment.\n",
    "In the pronunciation component, a dynamic low-pass filtered power (DLP) was used as an acoustic feature to discriminate between aspirated sounds and unaspirated sounds. With the sounds uttered by Chinese native speak- ers, we obtained a discrimination accuracy of 98.37% and 96.71% for aspirated consonants and unaspirated conso- nants respectively. Then we applied the pronunciation sys- tem to the sounds uttered by Japanese speakers who are learning Chinese as a second language and obtained an ac- curacy of discrimination. At last, a score is fed back to trainee according to the accuracy of his pronunciation and at the same time, the system instructs the trainee to im- prove his/her pronunciation.\n",
    "In the listening component, in contrast to traditional methods, our system can not only let the trainee listen to the sound, but can also give him a feedback which will show whether or not his judgment about what he has heard is correct. The listening training system was constructed in Java language so that it can be used on any platform.\n",
    "We will introduce the background and basic principles of the system, and give some examples of its use in language education.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-194"
  },
  "ishi00_icslp": {
   "authors": [
    [
     "Carlos Toshinori",
     "Ishi"
    ],
    [
     "Keikichi",
     "Hirose"
    ],
    [
     "Nobuaki",
     "Minematsu"
    ]
   ],
   "title": "Identification of Japanese double-mora phonemes considering speaking rate for the use in CALL systems",
   "original": "i00_1786",
   "page_count": 5,
   "order": 195,
   "p1": "vol. 1, 786-790",
   "pn": "",
   "abstract": [
    "Effect of speaking rate on phone duration was investigated to improve a Computer-Aided Language Learning (CALL) system that teaches Japanese double-mora phoneme pronunciation [1]. The influence of the speaking rate was formulated as a linear regression line for each phone, based on the analysis results of a male speakers non-sense word utterances. The formulations were used to develop a method to estimate the speaking rate of the utterance over two hypothesis (presence and absence of double-mora phoneme), and decide which of the hypothesis is the more suitable. Experiments indicated that good identification between singleand double-mora phonemes was possible irrespective of the speaking rate.\n",
    "",
    "",
    "Kawai and Hirose, \"Teaching the pronunciation of Japanese double-mora phonemes using speech recognition technology,\" Speech Communication, Vol.30, Nos.2-3, pp.131-143, (2000).\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-195"
  },
  "patterson00_icslp": {
   "authors": [
    [
     "Roy D.",
     "Patterson"
    ],
    [
     "Stefan",
     "Uppenkamp"
    ],
    [
     "Dennis",
     "Norris"
    ],
    [
     "William",
     "Marslen-Wilson"
    ],
    [
     "Ingrid",
     "Johnsrude"
    ],
    [
     "Emma",
     "Williams"
    ]
   ],
   "title": "Phonological processing in the auditory system: a new class of stimuli and advances in fmri techniques",
   "original": "i00_2001",
   "page_count": 4,
   "order": 196,
   "p1": "vol. 2, 1-4",
   "pn": "",
   "abstract": [
    "It is commonly assumed that, in the cochlea and the brainstem, the auditory system processes speech sounds without differentiating them from any other sounds. At some stage, however, it must treat speech and non-speech sounds differently. In broad terms, the purpose of this paper is to consider where this speech specific processing begins in the auditory pathway. Specifically, the paper is concerned with extrapolating the concepts of an auditory model to the point where we can define matched sets of speech and non-speech sounds that can be used in a brain-imaging experiment to delimit where phonological processing of vowel sounds begins in the auditory system. Pilot results suggest that phonological processing of vowels may begin just outside auditory cortex in Brodmann area 21.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-196"
  },
  "tatsumi00_icslp": {
   "authors": [
    [
     "Itaru F.",
     "Tatsumi"
    ],
    [
     "Michio",
     "Senda"
    ],
    [
     "Kenji",
     "Ishii"
    ],
    [
     "Masahiro",
     "Mishina"
    ],
    [
     "Masashi",
     "Oyama"
    ],
    [
     "Hinako",
     "Toyama"
    ],
    [
     "Keiichi",
     "Oda"
    ],
    [
     "Masayuki",
     "Tanaka"
    ],
    [
     "Yasuyuki",
     "Gondo"
    ]
   ],
   "title": "Brain regions responsible for word retrieval, speech production and deficient word fluency in elderly people: a PET activation study",
   "original": "i00_2005",
   "page_count": 6,
   "order": 197,
   "p1": "vol. 2, 5-10",
   "pn": "",
   "abstract": [
    "A PET (positron emission tomography) activation study using word-fluency tasks was administered to identify the network for word retrieval and speech production, and to explore mechanisms for word finding difficulty observed in elderly people. It was found that the left anterior insula, as opposed to Brocas area, left premotor area, the anterior cingulate gyrus, thalamus, basal ganglia, midbrain and cerebellum constitute the network for speech production. In semantic processing for word retrieval of proper names, the left anterior temporal lobe and frontal pole (BA 10) were activated. As for the animate and inanimate name retrieval, and the syllable fluency the same areas, viz., the left inferoposterior temporal lobe (BA 37) and left inferior frontal lobe (Brocas area), were activated.\n",
    "In elderly people an activated area was generally small, and those regions activated in young people were sometimes inactive. However, an great, stable and diffuse activation was also observed. The latter may reflect an effort to compensate for deficient word retrieval. These are possible sources of word finding difficulty in the elderly.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-197"
  },
  "alku00_icslp": {
   "authors": [
    [
     "Paavo",
     "Alku"
    ],
    [
     "Hannu",
     "Tiitinen"
    ],
    [
     "Kalle J.",
     "Palomäki"
    ],
    [
     "Päivi",
     "Sivonen"
    ]
   ],
   "title": "MEG-measurements of brain activity reveal the link between human speech production and perception",
   "original": "i00_2011",
   "page_count": 4,
   "order": 198,
   "p1": "vol. 2, 11-14",
   "pn": "",
   "abstract": [
    "Whether human speech perception depends on a biologically based link between production and perception or whether it is best characterised as a series of acoustic, phonetic, and semantic transformations has remained an unresolved issue. We addressed this question via the use of objective brain research methods combined with advanced stimulus production methodology. We removed the contribution of the periodic glottal excitation, produced by the vocal folds in the human larynx, from vowel stimuli and found that magnetic responses generated in the auditory cortex respond to this removal. The amplitude of the main deflection of the magnetic responses, N1m, decreased even though the formant settings, intensity, and duration of the stimuli were identical. Hence, because human brain activity attenuates if vowel stimuli are \"distorted\" by the removal of their naturally occurring periodic excitation we conclude that speech production and perception mechanisms in the human cortex are fundamentally interrelated.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-198"
  },
  "patterson00b_icslp": {
   "authors": [
    [
     "Karalyn",
     "Patterson"
    ],
    [
     "Matthew A. Lambon",
     "Ralph"
    ],
    [
     "Helen",
     "Bird"
    ],
    [
     "John R.",
     "Hodges"
    ],
    [
     "James L.",
     "McClelland"
    ]
   ],
   "title": "Normal and impaired processing in quasi-regular domains of language: the case of English past-tense verbs",
   "original": "i00_2015",
   "page_count": 4,
   "order": 199,
   "p1": "vol. 2, 15-19",
   "pn": "",
   "abstract": [
    "In most if not all languages, there are domains in which the relationship between one form of a word and another can be described as quasi-regular [1]. This means that, across the whole relevant vocabulary, there is substantial but imperfect consistency in the nature of the transformation linking the two forms. Quasi-regularity may apply between the same word forms in different modalities, as in the spoken and written versions of words, and also between different morphological forms of the same word root. In many languages, some aspects of verb morphology have this feature. The majority of English verbs, for example, are transformed to past tense by simply adding -ed to the present tense form (e.g., talk -> talked). The remainder, which include some of the most commonly used verbs in the language, form their past tenses in an atypical way (e.g., speak -> spoke; think -> thought; have -> had).\n",
    "According to one widely held theory about language, the nature of processing for regular (and novel) vs. irregular transformations differs so fundamentally as to require two completely separate mechanisms. In the realm of the English past-tense, this theory has been most thoroughly described by Pinker [2]. An alternative view, most recently developed by Joanisse & Seidenberg [3], proposes that all types of past-tense transformation are achieved by a single distributed, constraint-satisfaction process recruiting activation of the phonological and semantic representations of words.\n",
    "Among various forms of empirical evidence pertinent to this theoretical debate, attention has recently turned to the performance of neurological patients with acquired language impairments. One popular paradigm assesses generation of the past-tense in a sentence frame preceded by the present tense of the same verb (Ùcoday I speak to my friend; yesterday I ___ to my friend¡¨). Previous evidence is reviewed and new evidence presented here for a double dissociation in this task, with some types of aphasic patients achieving greater success on regular and novel verbs whereas others show an advantage for irregular verbs. Although the dual-mechanism theory predicts this double dissociation, such a pattern of results can also be well explained by a single constraint-satisfaction process. The crucial proposal here is that disruption to semantic representations has a disproportionate impact on the processing of irregular forms, and that disruption to phonological processing is more detrimental to success with regular forms. The semantic and phonological capabilities of patients with the two sides of the dissociation therefore become salient evidence in this debate. Additional features of the evidence, such as the nature of the patients' errors, seem to favour the constraint-satisfaction approach.\n",
    "s Plaut, D.C., McClelland, J.L. Seidenberg, M.S., Patterson, K. (1996). Understanding normal and impaired word reading: Computational principles in quasi-regular domains. Psychological Review, 103, 56-115. Pinker, S. (1999). Words and Rules: The Ingredients of Language. London: Weidenfeld & Nicolson. Joanisse, M.F. & Seidenberg, M.S.. (1999). Impairments in verb morphology after brain injury: A connectionist model. Proceedings of the National Academy of Sciences, 96, 7592-7597.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-199"
  },
  "martin00b_icslp": {
   "authors": [
    [
     "Nadine",
     "Martin"
    ],
    [
     "Eleanor M.",
     "Saffran"
    ],
    [
     "Gary S.",
     "Dell"
    ],
    [
     "Myrna F.",
     "Schwartz"
    ],
    [
     "Prahlad",
     "Gupta"
    ]
   ],
   "title": "Neuropsychological and computational evidence for a model of lexical processing, verbal short-term memory and learning",
   "original": "i00_2020",
   "page_count": 7,
   "order": 200,
   "p1": "vol. 2, 20-25",
   "pn": "",
   "abstract": [
    "Almost without exception, acquired language disorders resulting from focal brain injury are accompanied by impairments of verbal short-term memory (STM) and verbal learning. Moreover, disturbances in verbal STM are generally associated with language dysfunction, in some cases mild and not disruptive to most language activities. The co-occurrence of language and mnestic deficits in the aphasic population affords the opportunity to examine their relationships. Here we review evidence from aphasia supporting the view that verbal STM processes are inextricably linked with the lexical processing system (as opposed to drawing solely on an independent shortterm memory system). These studies are discussed as a foundation for the development of a new model of word processing, verbal STM and learning. An inherent assumption of this model is that spreading activation processes which maintain activation of linguistic representations when generating a single word also contribute to the support of multiple word representations in verbal STM.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-200"
  },
  "fushimi00_icslp": {
   "authors": [
    [
     "Takao",
     "Fushimi"
    ],
    [
     "Mutsuo",
     "Ijuin"
    ],
    [
     "Naoko",
     "Sakuma"
    ],
    [
     "Masayuki",
     "Tanaka"
    ],
    [
     "Tadahisa",
     "Kondo"
    ],
    [
     "Shigeaki",
     "Amano"
    ],
    [
     "Karalyn",
     "Patterson"
    ],
    [
     "Itaru F.",
     "Tatsumi"
    ]
   ],
   "title": "Normal and impaired reading of Japanese kanji and kana",
   "original": "i00_2026",
   "page_count": 6,
   "order": 201,
   "p1": "vol. 2, 26-31",
   "pn": "",
   "abstract": [
    "Two kinds of scripts are used in the written forms of Japanese words: morphographic kanji and phonographic kana. Whereas each kana character invariably represents a single pronunciation, the majority of kanji characters have two or more legitimate pronunciations, with one appropriate to the character in any given word. Furthermore, each kanji character has meaning while a kana character does not. On the basis of these and other differences between kanji and kana, some traditional views assume that, in reading aloud, kanji is processed by a semantic/lexical system while kana is processed by a phonological/rule system.\n",
    "We review accumulating evidence from our research that argues against these traditional views. (1) In reading aloud twocharacter kanji words, normal readers are slower on lowfrequency words with statistically atypical character-sound correspondences than either high-frequency words or words with statistically typical correspondences. (2) Normal readers are easily capable of reading aloud two-character kanji nonwords. (3) Normal readers are slower on low-imageability words than high-imageability words, but the imageability effect emerges only for low-familiarity kanji words with atypical character-sound correspondences. (4) Although Japanese surface dyslexia has been described as a selective reading disorder on kanji words, recently reported cases reveal good kanji performance for high-frequency words and words with statistically typical correspondences, despite a profound deficit on low-frequency words with atypical character-sound correspondences.\n",
    "(5) In reading aloud kana nonwords, normal readers are faster on pseudohomophones (orthographic nonwords with a familiar phonological pattern, created by transcribing kanji words into kana strings) than nonwords not homophonic with any words, but this significant advantage emerges only when the pseudohomophones share their pronunciations with high-imageability words. (6) Although Japanese phonological dyslexia has been considered a selective reading disorder on kana nonwords, a recently reported case showed good performance on pseudohomophones with the identical pronunciation to high-familiarity and high-imageability words, despite a profound deficit on nonhomophonic nonwords.\n",
    "These data suggest that phonology of both kanji and kana strings is computed directly from orthography, with additional reliance on semantics when the direct computation is inefficient.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-201"
  },
  "ijuin00_icslp": {
   "authors": [
    [
     "Mutsuo",
     "Ijuin"
    ],
    [
     "Takao",
     "Fushimi"
    ],
    [
     "Karalyn",
     "Patterson"
    ],
    [
     "Naoko",
     "Sakuma"
    ],
    [
     "Masayuki",
     "Tanaka"
    ],
    [
     "Itaru",
     "Tatsumi"
    ],
    [
     "Tadahisa",
     "Kondo"
    ],
    [
     "Shigeaki",
     "Amano"
    ]
   ],
   "title": "A connectionist approach to naming disorders of Japanese in dyslexic patients",
   "original": "i00_2032",
   "page_count": 6,
   "order": 202,
   "p1": "vol. 2, 32-37",
   "pn": "",
   "abstract": [
    "The triangle model is a computational model for lexical processing which computes word orthography, phonology and semantics using the architecture of a parallel distributed processing network. The computation takes the form of interactions among neuron-like processing units. In the present research, a Japanese triangle model computed phonology directly from orthography for both Kanji and Kana strings, with additional input to phonology from a component representing putative word semantics. This model successfully simulated certain effects seen in the performance of Japanese skilled readers. Moreover, different types of damage to the model reproduced data on both the surface and phonological forms of acquired dyslexia.\n",
    "After damage to the semantic component, the models reading performance remained good for Kana words, Kana nonwords, and Kanji words with consistent character-sound correspondences, but was significantly impaired on Kanji words with atypical correspondences: this simulates surface dyslexia. After damage to the phonological component of the model, the networks performance remained good for both Kanji and Kana words but was impaired on Kana nonwords: this simulates phonological dyslexia.\n",
    "These results are basically comparable to those of previous models developed for English, and thus demonstrate that the same computational principles of the triangle model can be applied to alphabetic and non-alphabetic writing systems. Mechanisms and properties of the model for Japanese are discussed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-202"
  },
  "wydell00_icslp": {
   "authors": [
    [
     "Taeko N.",
     "Wydell"
    ],
    [
     "Takako",
     "Shinkai"
    ]
   ],
   "title": "Impaired pronunciations of kanji words by Japanese CVA patients",
   "original": "i00_2038",
   "page_count": 4,
   "order": 203,
   "p1": "vol. 2, 38-41",
   "pn": "",
   "abstract": [
    "We report contrasting oral reading impairments for Kanji (Japanese logographic or morphograpnic) words in two Japanese neurological patients with CVA. Through neurological battery tests one was identified as a surface dyslexic, while the other was identified as a phonological dyslexic. A Kanji-word oral reading test revealed that the Legitimate Alternative Reading Component (LARC) errors, analogous to regularisation errors in English, with the inconsistent and exception words, were observed only with the surface dyslexic patient. In contrast, the phonological dyslexic patient made fewer errors, which were mainly word substitution errors. It was concluded that the different types of acquired dyslexia identified in English appear to have universal application. Of particular interest was that surface dyslexia during oral reading of Kanji words was observed in a Japanese CVA patient. Hitherto surface dyslexia in Japanese has been described by the dissociation between a better oral reading performance for Kanji and a poor performance for Kana.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-203"
  },
  "uno00_icslp": {
   "authors": [
    [
     "Akira",
     "Uno"
    ],
    [
     "M.",
     "Kaneko"
    ],
    [
     "N.",
     "Haruhara"
    ],
    [
     "M.",
     "Kaga"
    ]
   ],
   "title": "Disability of phonological versus visual information processes in Japanese dyslexic children",
   "original": "i00_2042",
   "page_count": 3,
   "order": 204,
   "p1": "vol. 2, 42-44",
   "pn": "",
   "abstract": [
    "We analyzed phonological and visual information processes of four Japanese dyslexic children between 7- 10 years of age. None of them had disability in word retrieval and/or auditory memory. Two of them manifested lower score in phonological awareness tests while the others showed normal score. All children showed subnormal score in a visual cognitive test, a visuo-spacial constructional test, and a visual long-term memory test. 90 % of their errors were formal ones. As results, these four Japanese dyslexic children are thought to have disabilities in visual information processes and are thought to be of a different type from those reported as phonological dyslexia.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-204"
  },
  "zhou00_icslp": {
   "authors": [
    [
     "Xiaolin",
     "Zhou"
    ],
    [
     "Yanxuan",
     "Qu"
    ]
   ],
   "title": "Lexical tone in the spoken word recognition of Chinese",
   "original": "i00_2045",
   "page_count": 6,
   "order": 205,
   "p1": "vol. 2, 45-50",
   "pn": "",
   "abstract": [
    "Constraints of lexical tones on semantic activation in spoken word recognition of Chinese were investigated in three crossmodal priming lexical decision experiments. In Experiments 1 and 2, disyllabic compound words that shared the same segmental templates but differed in lexical tones (e.g., tiao4 yue4 vs. tiao2 yue1, jump vs. treaty, numbers indicating tone types) were used as auditory primes while words that were semantically related to one of the pairs (e.g., ben1 pao3, run) were visually presented for lexical decision. The semantic primes and the tone-mismatch primes differed in the tones of either the first, the second, or both syllables. In Experiment 3, nonword tone-mismatch primes were created by changing the first or the second tones of semantic primes. The similarity between the original tones and the resulting tones was also manipulated. The appearance of significant priming effects for the tone-different primes depended on lexical competition environment, the global and local goodness of fit between input tones and underlying tones, and the constituent position of tone alternation. The findings are discussed in terms of how tonal information is represented in the mental lexicon, how tonal information in speech input is mapped onto the lexicon, and how tonal constraints on lexical access and semantic activation is influenced by competition environment.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-205"
  },
  "zhou00b_icslp": {
   "authors": [
    [
     "Xiaolin",
     "Zhou"
    ],
    [
     "Jie",
     "Zhuang"
    ]
   ],
   "title": "Lexical tone in the speech production of Chinese words",
   "original": "i00_2051",
   "page_count": 4,
   "order": 206,
   "p1": "vol. 2, 51-54",
   "pn": "",
   "abstract": [
    "Picture and word naming tasks were used to investigate the activation of tonal information in the speech production of Chinese words. A picture was presented first, followed by a Chinese character superimposing on the picture. This character shared either the segmental template and the tone, or only the segmental template, or no phonological properties with the name of the picture. Subjects were asked to name the picture (Experiment 1) or the character (Experiment 2). The SOA between presentation of the picture and presentation of the character was manipulated. In Experiment 1, the naming of pictures at the SOA of 57 ms and 200 ms was facilitated when the names of the pictures shared phonological properties with the characters. But the effect for tone activation was lager at the short SOA than at the longer SOA while the effect for segmental activation was stable across SOAs, indicating different time courses for the activation of tonal information and segmental information in speech production. In Experiment 2, at the both the short and the long SOAs, character naming was facilitated when the characters shared both segmental and tonal information with the picture names. Character naming tended to be inhibitory when the characters shared only the segmental templates with the picture names. These results suggested that lexical tones are activated very early in the speech production of Chinese words and the mismatch of tonal information in primed picture naming or character naming can effectively reduce or neutralize segmental facilitation.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-206"
  },
  "hu00_icslp": {
   "authors": [
    [
     "Yu",
     "Hu"
    ],
    [
     "Qin-Feng",
     "Liu"
    ],
    [
     "Ren-Hua",
     "Wang"
    ]
   ],
   "title": "Prosody generation in Chinese synthesis using the template of quantified prosodic unit and base intonation contour",
   "original": "i00_2055",
   "page_count": 4,
   "order": 207,
   "p1": "vol. 2, 55-58",
   "pn": "",
   "abstract": [
    "This paper presents a prosody generation method for Chinese mandarin using the template of quantified prosodic unit and base intonation contour. This method uses the prosodic feature picked-up from the syllables in the prosody words by rule as the base unit, and integrates the prosody rules in the prosody words of Chinese mandarin and base intonation contour to achieve the prosody contours with high naturalness. Experiment shows that the prosody contour generated by this method lively represent s the agility of the prosody of Chinese mandarin, and improves the expressive force of the synthesized speech.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-207"
  },
  "chen00e_icslp": {
   "authors": [
    [
     "Yiqiang",
     "Chen"
    ],
    [
     "Wen",
     "Gao"
    ],
    [
     "Tingshao",
     "Zhu"
    ],
    [
     "Jiyong",
     "Ma"
    ]
   ],
   "title": "Multi-strategy data mining on Mandarin prosodic patterns",
   "original": "i00_2059",
   "page_count": 4,
   "order": 208,
   "p1": "vol. 2, 59-62",
   "pn": "",
   "abstract": [
    "Mandarin prosodic models are very important in speech research and synthesis, which mainly describes the variation of pitch. The models that are now being used in most Chinese Text-To-Speech systems are constructed by expert, qualitatively and with low precision. In this paper, we propose a Multi-strategy Data Mining framework to extract prosodic patterns from actual large Mandarin speech database to improve the naturalness and intelligibility of synthesized speech. In data preprocessing, typical prosody models are found by clustering analysis, and Rough Set is employed for feature selection. ANN and Decision tree are trained respectively. The prediction result of ANN and Decision Tree are integrated to generate fundamental frequency and energy contours. The experimental results showed that synthesized prosodic features quite resembled their original counterparts for most syllables.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-208"
  },
  "verhelst00_icslp": {
   "authors": [
    [
     "Werner",
     "Verhelst"
    ],
    [
     "Dirk van",
     "Compernolle"
    ],
    [
     "Patrick",
     "Wambacq"
    ]
   ],
   "title": "A unified view on synchronized overlap-add methods for prosodic modifications of speech",
   "original": "i00_2063",
   "page_count": 4,
   "order": 209,
   "p1": "vol. 2, 63-66",
   "pn": "",
   "abstract": [
    "We present several synchronized overlap-add methods for prosodic modification of speech in a unifying framework. The discussion starts by exploring time-scale modification based on the short-time Fourier transform and proceeds to review an iterative phase reconstruction method for time-scaled magnitude spectrograms. The search for a good initial phase estiniate leads to waveform.synchronized overlap add methods for time-scaling of speech, in particular SOLA and WSOLA. Pitch modification is made possible when pitch-synchronized overlap-addition is applied to wideband short-time Fourier transforms, as is the case with current techniques like PSOLA, PIOLA, and MBROLA. All these prosodic modification methods essentially perform some form of automatic editing of the speech waveform in the time domain. Therefore, they can be computationally efficient and at the same time maintain a natural sound quality.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-209"
  },
  "shih00_icslp": {
   "authors": [
    [
     "Chilin",
     "Shih"
    ],
    [
     "Greg P.",
     "Kochanski"
    ]
   ],
   "title": "Chinese tone modeling with stem-ML",
   "original": "i00_2067",
   "page_count": 4,
   "order": 210,
   "p1": "vol. 2, 67-70",
   "pn": "",
   "abstract": [
    "This paper models tonal variations with Stem-ML tags. Surface tone shapes often deviate from their expected canonical shapes in natural sentences, presenting a challenging case to tone modeling. In this study we employed a subset of Stem-ML tags which incorporated information of lexical tones and linguistically motivated prosodic strength of the syllable. The tags successfully captured the \"distorted\" tone shapes and produced contextually appropriate surface variations.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-210"
  },
  "wightman00_icslp": {
   "authors": [
    [
     "Colin W.",
     "Wightman"
    ],
    [
     "Ann K.",
     "Syrdal"
    ],
    [
     "Georg",
     "Stemmer"
    ],
    [
     "Alistair",
     "Conkie"
    ],
    [
     "Mark",
     "Beutnagel"
    ]
   ],
   "title": "Perceptually based automatic prosody labeling and prosodically enriched unit selection improve concatenative text-to-speech synthesis",
   "original": "i00_2071",
   "page_count": 4,
   "order": 211,
   "p1": "vol. 2, 71-74",
   "pn": "",
   "abstract": [
    "Prosody is an important factor in the quality of text-to-speech (TTS) synthesis. Typically, acoustic parameters such as f0 and duration are the only variables related to prosody that are used to determine unit selection. Our study explored adding the explicit use of linguistically and perceptually motivated prosodic categories in unit selection-based TTS. One of our goals was to automate the process of prosodically labeling our TTS inventory. However, reliability among labelers for some ToBI (Tones and Break Indices) categories was too low for successful training of an automatic prosody recognizer. We developed a prosody labeling system simpler and more robust than standard EToBI (English ToBI). This \"ToBI Lite\" system was used successfully for automatic labeling of the acoustic inventory and in prosodically enriched unit selection. A formal listening test was conducted to compare subjective quality ratings for several variations of the AT&T unit selection concatenative TTS system that differed only in their method of prosodic labeling of the inventory or their use of prosody for unit selection. The use of simple prosodic categories in unit selection significantly improved ratings, and automatic prosodic labeling resulted in higher ratings than manual labeling.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-211"
  },
  "muller00c_icslp": {
   "authors": [
    [
     "Achim F.",
     "Müller"
    ],
    [
     "Jianhua",
     "Tao"
    ],
    [
     "Rüdiger",
     "Hoffmann"
    ]
   ],
   "title": "Data-driven importance analysis of linguistic and phonetic information",
   "original": "i00_2075",
   "page_count": 4,
   "order": 212,
   "p1": "vol. 2, 75-78",
   "pn": "",
   "abstract": [
    "In this paper the weight decay concept known from neural network theory is applied to the two modules involved in prosody generation within our text-to-speech system Papageno. Both modules are based on neural networks (NN). Preprocessing layers are inserted connected to the inputs of specialized NN architectures via diagonal weight matrices. The weight decay concept is applied to the weights of these diagonal weight matrices. This allows an importance analysis of the used input parameters in the context of the used NN architectures.\n",
    "In the symbolic prosody module the importance for phrase break prediction of part-of-speech (POS) tags could be evaluated Further, the necessary length of a POS context window could be analyzed and optimized.\n",
    "For fO-generation for Mandarin language an importance analysis of the phonological information could be performed. The importance analysis led to an optimized input feature set reducing the squared error of the used NN architecture.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-212"
  },
  "li00c_icslp": {
   "authors": [
    [
     "Zhiqiang",
     "Li"
    ],
    [
     "Degif Petros",
     "Banksira"
    ]
   ],
   "title": "Tonal structure of yes-no question intonation in chaha",
   "original": "i00_2079",
   "page_count": 4,
   "order": 213,
   "p1": "vol. 2, 79-82",
   "pn": "",
   "abstract": [
    "This paper examines the tonal structure of yes-no questions in Chaha, using the autosegmental-metrical model of intonational phonology. Yes-no questions can be formed in two ways: lengthening the final syllable or attach the question suffix to the final word. They exhibit different F0 patterns at the end of the utterance. We propose that the same underlying tonal structure is realized differently in two types of yes-no questions. In addition to the initial F0 peak, which is present in both declaratives and questions, the bitonal pitch accent HL is associated with the penultimate syllable of the last word in yesno questions. The realization of the complex edge tone LH% is different. In the case of lengthening, only L of LH% is associated with the lengthened final syllable, and the floating H% triggers upstep of the preceding tone, leading to a mid-level F0 plateau on the final syllable. In the case of question suffix attachment, L and H of LH% are realized on the final syllable and the attached question suffix respectively, leading to a final rise.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-213"
  },
  "wang00b_icslp": {
   "authors": [
    [
     "Chao",
     "Wang"
    ],
    [
     "Stephanie",
     "Seneff"
    ]
   ],
   "title": "Improved tone recognition by normalizing for coarticulation and intonation effects",
   "original": "i00_2083",
   "page_count": 4,
   "order": 214,
   "p1": "vol. 2, 83-86",
   "pn": "",
   "abstract": [
    "We have previously demonstrated that tone modeling improved speech recognition on a digit corpus [1]. In this work, we further improve tone recognition by normalizing for both tone coarticulation and intonation effects. The tone classification errors on continuous digit strings were reduced by 26.1% from the baseline, when the effects of F0 downdrift, phrase boundary and tone coarticulation were normalized. We also applied the same approach to conversational speech from the YINHE domain [2], and obtained similar improvements. The word error rate on spontaneous YINHE data was reduced by 16.5% when a simple fourtone model was applied to resort recognizer 10-best outputs.\n",
    "s C. Wang and S. Seneff, \"A study of tones and tempo in continuous Mandarin digit strings and their application in telephone quality speech recognition,\" in Proc. ICSLP98, Sydney, Australia, pp. 635-638, 1998. C. Wang, J. Glass, H. Meng, J. Polifroni, S. Seneff and V. Zue, \"Yinhe: A Mandarin Chinese version of the Galaxy system,\" in Eurospeech 97, Rhodes, Greece, pp. 351-354, 1997.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-214"
  },
  "zhang00e_icslp": {
   "authors": [
    [
     "Jin-Song",
     "Zhang"
    ],
    [
     "Satoshi",
     "Nakamura"
    ],
    [
     "Keikichi",
     "Hirose"
    ]
   ],
   "title": "Discriminating Chinese lexical tones by anchoring F0 features",
   "original": "i00_2087",
   "page_count": 4,
   "order": 215,
   "p1": "vol. 2, 87-90",
   "pn": "",
   "abstract": [
    "In this paper we present some new features, called as anchoring FO features including left-context-dependent and right-context-dependent ones, for discriminating Chinese lexical tones of continuous speech. The features are calculated based on a hypothesis that auditory tone perception possibly depends on the shift of the judgement boundary of tone targets by the anchoring effects from the neighboring tone targets. Statistical distribution analyses on continuous speech data by 20 speakers (10 males and 10 females) showed that the proposed anchoring FO features are statistically efficient for tone discrimination, and the left-context-dependent FO anchoring feature is more efficient than the FO height for discriminating the pitch value of tone onset. Moreover, higher efficiency of the left-context-dependent anchoring features than the right-context-dependent ones indicates that carry-over effect play a more important role than the anticipation effect for tone discrimination.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-215"
  },
  "gussenhoven00_icslp": {
   "authors": [
    [
     "Carlos",
     "Gussenhoven"
    ],
    [
     "Aoju",
     "Chen"
    ]
   ],
   "title": "Universal and language-specific effects in the perception of question intonation",
   "original": "i00_2091",
   "page_count": 4,
   "order": 216,
   "p1": "vol. 2, 91-94",
   "pn": "",
   "abstract": [
    "Three groups of monolingual listeners, with Standard Chinese, Dutch and Hungarian as their native language, judged pairs of trisyllabic stimuli which differed only in their pitch pattern. The segmental structure of the stimuli was made up by the experimenters and presented to subjects as being taken from a little-known language spoken on a South Pacific island. Pitch patterns consisted of a single rise-fall located on or near the second syllable. By and large, listeners selected the stimulus with the higher peak, the later peak, and the higher end rise as the one that signalled a question, regardless of language group. This result is argued to reflect innate, non-linguistic knowledge of the meaning of pitch variation, notably Ohalas Frequency Code. A significant difference between groups is explained as due to the influence of the mother tongue.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-216"
  },
  "tseng00b_icslp": {
   "authors": [
    [
     "Chiu-Yu",
     "Tseng"
    ],
    [
     "Da-De",
     "Chen"
    ]
   ],
   "title": "The interplay and interaction between prosody and syntax: evidence from Mandarin Chinese",
   "original": "i00_2095",
   "page_count": 3,
   "order": 217,
   "p1": "vol. 2, 95-97",
   "pn": "",
   "abstract": [
    "This paper aims to (1.) quantify possible correlation between syntactic structure and prosodic manifestation in Mandarin Chinese, (2.) explore to what extent such correlation could be predicted by syntactic structures and what may go beyond these correlations, and (3.) increase more operational characteristics to Mandarin prosodic structures.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-217"
  },
  "mixdorff00_icslp": {
   "authors": [
    [
     "Hansjörg",
     "Mixdorff"
    ],
    [
     "Hiroya",
     "Fujisaki"
    ]
   ],
   "title": "A quantitative description of German prosody offering symbolic labels as a by-product",
   "original": "i00_2098",
   "page_count": 4,
   "order": 218,
   "p1": "vol. 2, 98-101",
   "pn": "",
   "abstract": [
    "The prosodic quality of a text-to-speech system is important for the intellegibility and perceived naturalness of synthetic speech. In earlier works the author developed a linguistically motivated model of German intonation based on the quantitative Fujisaki model of the production process of F0. The current paper compares results yielded by automatic Fujisaki modeling with a GToBI-style anotation. On the accent level, a good correlation between tone labels and accent commands can be observed. On the phrase level, most level 3 and 4 break index boundaries are aligned with phrase commands whereas lower level boundaries are presumably marked with durational cues. Subsequently a regression model of syllable duration is introduced which permits to decompose the measured duration contour into an extrinsic and an intrinsic component.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-218"
  },
  "rosenfeld00_icslp": {
   "authors": [
    [
     "Roni",
     "Rosenfeld"
    ],
    [
     "Xiaojin",
     "Zhu"
    ],
    [
     "Arthur",
     "Toth"
    ],
    [
     "Stefanie",
     "Shriver"
    ],
    [
     "Kevin",
     "Lenzo"
    ],
    [
     "Alan W.",
     "Black"
    ]
   ],
   "title": "Towards a universal speech interface",
   "original": "i00_2102",
   "page_count": 4,
   "order": 219,
   "p1": "vol. 2, 102-105",
   "pn": "",
   "abstract": [
    "We discuss our ongoing attempt to design and evaluate universal human-machine speech-based interfaces. We describe one such initial design suitable for database retrieval applications, and discuss its implementation in a movie information application prototype. Initial user studies provided encouraging results regarding the usability of the design, as well as suggest some questions for further investigation.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-219"
  },
  "russell00_icslp": {
   "authors": [
    [
     "Dale",
     "Russell"
    ]
   ],
   "title": "A domain model centered approach to spoken language dialog systems",
   "original": "i00_2106",
   "page_count": 4,
   "order": 220,
   "p1": "vol. 2, 106-109",
   "pn": "",
   "abstract": [
    "This paper presents a view of Spoken Language Dialog Systems in which a Domain Model is the unifying feature, providing a common representation of knowledge about the domain of application that is shared by many components of the system. The information in the Domain Model is used in different ways by the grammar and parser, the speech recognizer, the dialog manager, and the back end interface. Sharing a common knowledge base allows these components to operate together seamlessly, while maintaining modularity.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-220"
  },
  "fafiotte00_icslp": {
   "authors": [
    [
     "Georges",
     "Fafiotte"
    ],
    [
     "Jian-She",
     "Zhai"
    ]
   ],
   "title": "From multilingual multimodal spoken language acquisition towards on-line assistance to intermittent human interpreting: SIM*, a versatile environment for SLP",
   "original": "i00_2110",
   "page_count": 4,
   "order": 221,
   "p1": "vol. 2, 110-113",
   "pn": "",
   "abstract": [
    "We present and discuss SIM*, a versatile multiplatform simulation environment for Speech MT. Based on a Wizard of Oz scheme, it is firstly intended for supporting and collecting multimodal bilingual spontaneous spoken dialogues through the Internet, in order to later build annotated multimodal multilingual speech corpora, on taskoriented subdomains. Current prototyping investigates a symmetrical paradigm, where the system would provide two distant monolingual speakers, and especially their distant human intermittent interpreter, with automatic lexical and terminological aids derived from Speech MT active functions, through the network. Future developments to be explored are the assistance to corpus annotation and the capture of multimodal and linguistic events.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-221"
  },
  "denecke00_icslp": {
   "authors": [
    [
     "Matthias",
     "Denecke"
    ]
   ],
   "title": "Informational characterization of dialogue states",
   "original": "i00_2114",
   "page_count": 4,
   "order": 222,
   "p1": "vol. 2, 114-117",
   "pn": "",
   "abstract": [
    "We introduce multidimensional feature structures as a generalization of standard slot/filler representations commonly employed in spoken language dialogue systems. Nodes in multidimensional feature structures contain an n>/i> dimensional vector of values instead of one single filler element. The additional elements serve to represent, among other information, confidence measures of speech recognizers or the number of times a filler has been queried. We demonstrate the application of multidimensional feature structures to spoken dialogue systems. We show that unification based dialogue processing can be retained as long as the elements of the fillers are drawn from partially ordered sets. The dialogue manager employs a variant of constraint logic programming for representing dialogue strategies and update rules. The constraint logic program partitions the space of possible dialogue states in sets of states that are equivalent for the dialogue strategy.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-222"
  },
  "abe00_icslp": {
   "authors": [
    [
     "Kenji",
     "Abe"
    ],
    [
     "Kazushige",
     "Kurokawa"
    ],
    [
     "Kazunari",
     "Taketa"
    ],
    [
     "Sumio",
     "Ohno"
    ],
    [
     "Hiroya",
     "Fujisaki"
    ]
   ],
   "title": "A new method for dialogue management in an intelligent system for information retrieval",
   "original": "i00_2118",
   "page_count": 4,
   "order": 223,
   "p1": "vol. 2, 118-121",
   "pn": "",
   "abstract": [
    "This paper proposes a new method for dialogue management to be used in an intelligent system for information retrieval. The user and the system are modeled as two separate finitestate automata exchanging information through dialogue, and the system manages the dialogue efficiently by estimating the internal states of the user model. These models are constructed on the basis of analysis of a corpus of simulated dialogues, and the proposed method is evaluated from the point of view of perplexity reduction of language models.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-223"
  },
  "levin00_icslp": {
   "authors": [
    [
     "Esther",
     "Levin"
    ],
    [
     "Shrikanth",
     "Narayanan"
    ],
    [
     "Roberto",
     "Pieraccini"
    ],
    [
     "Konstantin",
     "Biatov"
    ],
    [
     "E.",
     "Bocchieri"
    ],
    [
     "Giuseppe Di",
     "Fabbrizio"
    ],
    [
     "Wieland",
     "Eckert"
    ],
    [
     "S.",
     "Lee"
    ],
    [
     "A.",
     "Pokrovsky"
    ],
    [
     "Mazin",
     "Rahim"
    ],
    [
     "P.",
     "Ruscitti"
    ],
    [
     "M.",
     "Walker"
    ]
   ],
   "title": "The AT&t-DARPA communicator mixed-initiative spoken dialog system",
   "original": "i00_2122",
   "page_count": 4,
   "order": 224,
   "p1": "vol. 2, 122-125",
   "pn": "",
   "abstract": [
    "The design and implementation of the AT&T Communicator mixed-initiative spoken dialog system is described. The Communicator project, sponsored by DARPA and launched in 1999, is a multi-year multi-site project on advanced spoken dialog systems research. The main focus of this paper is on issues related to the design of mixed-initiative systems. In addition to describing our architecture and implementation of the complex travel task, the paper reports on some preliminary evaluation results.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-224"
  },
  "bangalore00_icslp": {
   "authors": [
    [
     "Srinivas",
     "Bangalore"
    ],
    [
     "Michael",
     "Johnston"
    ]
   ],
   "title": "Integrating multimodal language processing with speech recognition",
   "original": "i00_2126",
   "page_count": 4,
   "order": 225,
   "p1": "vol. 2, 126-129",
   "pn": "",
   "abstract": [
    "One of the critical challenges facing next-generation human-computer interfaces concerns the development of effective language processing techniques for utterances distributed over multiple input modes such as speech, touch, and gesture. Finite-state models for parsing, understanding, and integration of multimodal input are efficient, enable tight coupling of multimodal language processing with speech recognition, and provide a general probabilistic framework for multimodal ambiguity resolution. We describe an experiment that demonstrates the effectiveness of tight coupling of multimodal language processing in improving speech recognition performance with clean speech and with different levels of background noise. Our approach yields an average 23% relative sentence error reduction on clean speech.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-225"
  },
  "rudnicky00_icslp": {
   "authors": [
    [
     "Alexander I.",
     "Rudnicky"
    ],
    [
     "Christina",
     "Bennett"
    ],
    [
     "Alan W.",
     "Black"
    ],
    [
     "Ananlada",
     "Chotimongkol"
    ],
    [
     "Kevin",
     "Lenzo"
    ],
    [
     "Alice",
     "Oh"
    ],
    [
     "Rita",
     "Singh"
    ]
   ],
   "title": "Task and domain specific modelling in the Carnegie Mellon communicator system",
   "original": "i00_2130",
   "page_count": 4,
   "order": 226,
   "p1": "vol. 2, 130-134",
   "pn": "",
   "abstract": [
    "The Carnegie Mellon Communicator is a telephone-based dialog system that supports planning in a travel domain. The implementation of such a system requires two complimentary components, an architecture capable of managing interaction and the task, as well as a knowledge base that captures the speech, language and task characteristics specific to the domain. Given a suitable architecture, the principal effort in development in taken up in the acquisition and processing of a domain knowledge base. This paper describes a variety of techniques we have applied to modeling in acoustic, language, task, generation and synthesis components of the system.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-226"
  },
  "gustafson00_icslp": {
   "authors": [
    [
     "Joakim",
     "Gustafson"
    ],
    [
     "Linda",
     "Bell"
    ],
    [
     "Jonas",
     "Beskow"
    ],
    [
     "Johan",
     "Boye"
    ],
    [
     "Rolf",
     "Carlson"
    ],
    [
     "Jens",
     "Edlund"
    ],
    [
     "Björn",
     "Granström"
    ],
    [
     "David",
     "House"
    ],
    [
     "Mats",
     "Wirén"
    ]
   ],
   "title": "Adapt - a multimodal conversational dialogue system in an apartment domain",
   "original": "i00_2134",
   "page_count": 4,
   "order": 227,
   "p1": "vol. 2, 134-137",
   "pn": "",
   "abstract": [
    "A general overview of the AdApt project and the research that is performed within the project is presented. In this project various aspects of human-computer interaction in a multimodal conversational dialogue systems are investigated. The project will also include studies on the integration of user/system/dialogue dependent speech recognition and multimodal speech synthesis. A domain in which multimodal interaction is highly useful has been chosen, namely, finding available apartments in Stockholm. A Wizard-of-Oz data collection within this domain is also described.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-227"
  },
  "wang00c_icslp": {
   "authors": [
    [
     "Kuansan",
     "Wang"
    ]
   ],
   "title": "Implementation of a multimodal dialog system using extended markup languages",
   "original": "i00_2138",
   "page_count": 4,
   "order": 228,
   "p1": "vol. 2, 138-141",
   "pn": "",
   "abstract": [
    "In this paper, we describe an implementation of a plan-based multimodal dialog system using the extensible markup language (XML). The dialog manager receives semantic objects representing the users utterance at the end of each users turn. We define a semantic markup language (SML), based on XML, to describe these semantic objects. Following the principles of XML Schema, we define the schema of SML in another XML called semantic definition language (SDL). In addition to supporting many discourse and dialog features, SDL is also designed to represent the domain knowledge via the application schema and the hierarchy of the semantic objects. We show that, with a thoughtful design in SDL, SML can be expressive enough that the behavior of a dialog planner can be fully specified in the extensible stylesheet language (XSL), a standardized language with a logical programming model that is most popular for implementing intelligent systems.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-228"
  },
  "seneff00_icslp": {
   "authors": [
    [
     "Stephanie",
     "Seneff"
    ],
    [
     "Chian",
     "Chuu"
    ],
    [
     "D. Scott",
     "Cyphers"
    ]
   ],
   "title": "ORION: from on-line interaction to off-line delegation",
   "original": "i00_2142",
   "page_count": 4,
   "order": 229,
   "p1": "vol. 2, 142-145",
   "pn": "",
   "abstract": [
    "This paper introduces ORION, a conversational system that performs off-line tasks and initiates later contact with a user at a prenegotiated time. Orion has two major episodes of activity: the enrollment of new tasks and the execution of pending tasks. The task manager periodically checks the pending tasks and updates their status, sending off requests to other servers for information and possibly launching a phone call when a particular task has reached its trigger time. A separate user interface engages in a dialogue with a user to enroll new tasks and/or update existing tasks. ORION is still in an early stage of its development cycle, but it has introduced several interesting new research issues, such as continuous state maintenance and contact verification.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-229"
  },
  "duan00_icslp": {
   "authors": [
    [
     "Lei",
     "Duan"
    ],
    [
     "Alexander",
     "Franz"
    ],
    [
     "Keiko",
     "Horiguchi"
    ]
   ],
   "title": "Practical spoken language translation using compiled feature structure grammars",
   "original": "i00_2146",
   "page_count": 4,
   "order": 230,
   "p1": "vol. 2, 146-149",
   "pn": "",
   "abstract": [
    "Practical work on spoken language translation must pursue two types of efficiency: computational efficiency, and \"language engineering\" efficiency. This paper describes the design, implementation, and evaluation of the GPL-based framework for spoken language translation that addresses both of these goals. In this framework, computational grammars are written in GPL, an easy-to-use imperative programming language that allows the direct expression of linguistic algorithms in terms of rewrite-grammars with feature structure tests and manipulations. Computational efficiency is achieved with the GPL compiler, which converts GPL grammars into efficient C routines, and with the GPL runtime environment, which provides services for linguistic representations, manipulation, and memory management. An evaluation of an English-Japanese spoken language translation system based on GPL shows that it is linguistically powerful, yet only requires reasonable computational resources.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-230"
  },
  "meng00b_icslp": {
   "authors": [
    [
     "Helen",
     "Meng"
    ],
    [
     "Shuk Fong",
     "Chan"
    ],
    [
     "Yee Fong",
     "Wong"
    ],
    [
     "Tien Ying",
     "Fung"
    ],
    [
     "Wai Ching",
     "Tsui"
    ],
    [
     "Tin Hang",
     "Lo"
    ],
    [
     "Cheong Chat",
     "Chan"
    ],
    [
     "Ke",
     "Chen"
    ],
    [
     "Lan",
     "Wang"
    ],
    [
     "Ting Yao",
     "Wu"
    ],
    [
     "Xiaolong",
     "Li"
    ],
    [
     "Tan",
     "Lee"
    ],
    [
     "Wing Nin",
     "Choi"
    ],
    [
     "Yiu Wing",
     "Wong"
    ],
    [
     "P. C.",
     "Ching"
    ],
    [
     "Huisheng",
     "Chi"
    ]
   ],
   "title": "ISIS: A multilingual spoken dialog system developed with CORBA and KQML agents",
   "original": "i00_2150",
   "page_count": 4,
   "order": 231,
   "p1": "vol. 2, 150-153",
   "pn": "",
   "abstract": [
    "ISIS, which abbreviates Intelligent Speech for Information Systems, is a trilingual spoken dialog system (SDS) for the financial domain. It handles two dialects of Chinese (Cantonese and Putonghua), as well as English ¨ the predominant languages in our region. The system supports spoken language queries regarding stock market information and simulated personal portfolios. Real-time information is retrieved directly from a dedicated Reuters satellite feed. ISIS provides a system test-bed for our work in multilingual speech recognition and generation, speaker authentication, language understanding and dialog modeling. Furthermore, ISIS supports our initial explorations in: (i) CORBA's interoperability and scalability for SDS development; in conjunction with (ii) asynchronous humancomputer interaction by delegation to KQML software agents.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-231"
  },
  "hirasawa00_icslp": {
   "authors": [
    [
     "Jun-Ichi",
     "Hirasawa"
    ],
    [
     "Noboru",
     "Miyazaki"
    ],
    [
     "Mikio",
     "Nakano"
    ],
    [
     "Kiyoaki",
     "Aikawa"
    ]
   ],
   "title": "New feature parameters for detecting misunderstandings in a spoken dialogue system",
   "original": "i00_2154",
   "page_count": 4,
   "order": 232,
   "p1": "vol. 2, 154-157",
   "pn": "",
   "abstract": [
    "This paper describes new feature parameters for detecting misunderstandings in a spoken dialogue system. Although recognition errors cannot be completely avoided with current speech recognition techniques, a spoken dialogue system could be a good human-machine interface if it could automatically detect and recover from its own misunderstandings during natural interaction between it and a user. For this purpose, we collected user responses to system confirmations with/without the system misunderstandings using the wizard of OZ method so that we could analyze the differences in the characteristics of user responses following correct/incorrect confirmations. The experimental results demonstrate that the content and duration of the user responses are good feature parameters for detecting the system's misunderstandings.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-232"
  },
  "mokhtari00_icslp": {
   "authors": [
    [
     "Parham",
     "Mokhtari"
    ],
    [
     "Frantz",
     "Clermont"
    ],
    [
     "Kazuyo",
     "Tanaka"
    ]
   ],
   "title": "Toward an acoustic-articulatory model of inter-speaker variability",
   "original": "i00_2158",
   "page_count": 4,
   "order": 233,
   "p1": "vol. 2, 158-161",
   "pn": "",
   "abstract": [
    "In this paper we propose a more complete model of inter-speaker variability, which accounts quantitatively for structural differences of the vocal tract (VT) and for learned differences in articulatory setting and in phoneme-specific strategy. This tripartite modelling is applied to a dataset of VT area functions estimated by acoustic-to-articulatory mapping, from formants measured in the steady-states of 5 vowels recorded by 5 adult, male speakers of Japanese. Principal component analyses of each of those physical sources of inter-speaker variation then yield eigenmodes and their variances, which together define an acoustic-articulatory, functional model of inter-speaker variability in vowel production.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-233"
  },
  "perrier00_icslp": {
   "authors": [
    [
     "Pascal",
     "Perrier"
    ],
    [
     "Joseph",
     "Perkell"
    ],
    [
     "Yohan",
     "Payan"
    ],
    [
     "Majid",
     "Zandipour"
    ],
    [
     "Frank",
     "Guenther"
    ],
    [
     "Ali",
     "Khalighi"
    ]
   ],
   "title": "Degrees of freedom of tongue movements in speech may be constrained by biomechanics",
   "original": "i00_2162",
   "page_count": 4,
   "order": 234,
   "p1": "vol. 2, 162-165",
   "pn": "",
   "abstract": [
    "A number of studies carried out on different languages have found that tongue movements in speech are made along two primary degrees of freedom (d.f.s): the high-front to low-back axis and the high-back to low-front axis. We explore the hypothesis that these two main d.f.s could find their origins in the physical properties of the vocal tract. A large set of tongue shapes was generated with a biomechanical tongue model using a Monte-Carlo method to thoroughly sample the muscle control space. The resulting shapes were analyzed with PCA. The first two factors explain 84% of the variance, and they are similar to the two experimentally observed d.f.s. This finding suggests that the d.f.s. are not speech-specific, and that speech takes advantage of biomechanically based tongue properties to form different sounds.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-234"
  },
  "vaxelaire00_icslp": {
   "authors": [
    [
     "Béatrice",
     "Vaxelaire"
    ],
    [
     "Rudolph",
     "Sock"
    ],
    [
     "Pascal",
     "Perrier"
    ]
   ],
   "title": "Gestural overlap, place of articulation and speech rate - an x-ray investigation",
   "original": "i00_2166",
   "page_count": 4,
   "order": 235,
   "p1": "vol. 2, 166-169",
   "pn": "",
   "abstract": [
    "This investigation deals with the production of consonant sequences in French, with particular focus on overlap of labial, apical, tongue-dorsum and velar gestures. X-ray and acoustic data are obtained for two speakers, at two speaking rates, normal-conversational and fast. Speech rate is varied in order to explore gestural overlapping possibilities, when places of articulation differ between plosives. Results show that: anticipatory coarticulation is not systematically predominant; anticipatory or perseverative coarticulation may or may not occur depending on similarity/dissimilarity in place of articulation between contiguous consonants; vowels are less resistant than consonants to velar coarticulation; speech rate facilitates gestural overlap. Findings are explained in terms of biomechanical and viable linguistic constraints.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-235"
  },
  "honda00b_icslp": {
   "authors": [
    [
     "Masaaki",
     "Honda"
    ],
    [
     "Akinori",
     "Fujino"
    ]
   ],
   "title": "Articulatory compensation and adaptation for unexpected palate shape perturbation",
   "original": "i00_2170",
   "page_count": 4,
   "order": 236,
   "p1": "vol. 2, 170-173",
   "pn": "",
   "abstract": [
    "This paper describes compensatory articulatory behavior in response to an unexpected perturbation of the oral cavity. An artificial palate, whose thickness can be changed during speech, was constructed to provide downward (increase of thickness) and upward (decrease of thickness) perturbation on the palate shape. The compensatory articulation during the utterance of repeated syllables, which contain fricative /R / and stopfricative /tR /, was recorded with an electromagnetic articulographic system. An EMG recording of tongue muscles was also made to examine the speech control mechanism of the immediate compensation of the tongue. The compensatory behavior was examined for both unmasked and masked audiofeedback conditions. The immediate compensation of the tongue to the unexpected perturbation occured within 100 ms after the perturbation for both audio-feedback conditions. It was, however, often incomplete and an overshoot of the tongue caused speech errors in the first occurrence of the fricative. The speech error disappeared and complete compensation was achieved in the successive occurrence of the phoneme during the same utterance. The time course in achieving the complete compensation was dependent on the audio-feedback condition. The masked audio-feedback condition needed a longer interval for the complete compensation.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-236"
  },
  "niikawa00_icslp": {
   "authors": [
    [
     "Takuya",
     "Niikawa"
    ],
    [
     "Masafumi",
     "Matsumura"
    ],
    [
     "Takashi",
     "Tachimura"
    ],
    [
     "Takeshi",
     "Wada"
    ]
   ],
   "title": "Modeling of a speech production system based on MRI measurement of three-dimensional vocal tract shapes during fricative consonant phonation",
   "original": "i00_2174",
   "page_count": 4,
   "order": 237,
   "p1": "vol. 2, 174-177",
   "pn": "",
   "abstract": [
    "This study, based on the measurement of three-dimensional (3-D) vocal tract shapes during fricative consonant phonation, presents a realistic modeling of a human speech production system.\n",
    "The 3-D shapes of a vocal tract and a dental crown were measured using Magnetic Resonance Imaging (MRI). A male subject was asked to produce the fricatives /s/ and /6/ while wearing a dental crown plate that contained a contrast medium for MRI processing. 3-D MR images of the vocal tract for each sound were obtained while the subjectís tongue was kept still. The 3-D shapes and area functions of the vocal tract corresponding to respective sounds were computed using a gray level interpolation technique to form serial sections. The measured results suggest that there are individual differences in speech production and vocal tract shapes.\n",
    "The airflow involved in the production of the fricatives /s/ and /6/ was estimated in the 3-D vocal tract using the Finite Element Method (FEM). The shapes of the 3-D vocal tracts for the fricatives were reconstructed from the coronal MR images. The behavior of the airflow was determined from the vector diagram of the flow rate.\n",
    "In this study, the vocal tract model with cascading circular tubes is called the VT model. A new acoustic model for the phonation of fricatives was proposed based on the VT model in which the sound source was a noise. Synthesized sounds of the Japanese fricatives /s/ and /6/ were generated using this model. An auditory test demonstrated that the generated sounds were intelligible.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-237"
  },
  "ouni00_icslp": {
   "authors": [
    [
     "Slim",
     "Ouni"
    ],
    [
     "Yves",
     "Laprie"
    ]
   ],
   "title": "Improving acoustic-to-articulatory inversion by using hypercube codebooks",
   "original": "i00_2178",
   "page_count": 4,
   "order": 238,
   "p1": "vol. 2, 178-181",
   "pn": "",
   "abstract": [
    "This paper presents an articulatory codebook construction method which gives a good space coverage with a limited number of points. We represent the articulatory space by hypercubes. Therefore the articulatory space is decomposed into regions represented by few number of points. We de- scribe our interpolation method to retrieve points from the hypercube and the inversion method based on the same inter- polation method. The advantage of the codebook and the in- version method is its robustness with respect to articulatory- to-acoustic mapping non-linearity problems.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-238"
  },
  "hamza00_icslp": {
   "authors": [
    [
     "Wael M.",
     "Hamza"
    ],
    [
     "Mohsen A.",
     "Rashwan"
    ]
   ],
   "title": "Concatenative arabic speech synthesis using large speech database",
   "original": "i00_2182",
   "page_count": 4,
   "order": 239,
   "p1": "vol. 2, 182-185",
   "pn": "",
   "abstract": [
    "Speech synthesis has got a lot of research interest as it represents an important part in a complete text-to-speech system. In this paper, an Arabic speech synthesis system has been proposed. The proposed system belongs to the family of concatenative speech synthesis systems that use large speech database. The concatenation unit inventory has been automatically constructed from a pre-recorded one hour of speech using context dependent HMM. A unified way to the unit selection has been introduced to enable the use of any type of concatenation units. The introduction of the context cost in the unit selection algorithm makes it easy to use longer and non-uniform units using the same framework. Context cost is represented as the distance between leafs of the context clustering trees that have been grown during the HMM acoustic modeling. Selected unit occurrences have been time and/or pitch scaled to match the required target. This operation is done using an adapted version of sinusoidal model. This version is referred to as Pitch- Synchronous All-Harmonic model. The resulting system has been evaluated using two types of evaluation tests. A word error of 10.3 % has been achieved in a DRT-like test while 3.8 score has been recorded in a subjective MOS-like test. These results show that the proposed system can be used as a front-end synthesizer of a complete Arabic text-to-speech system.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-239"
  },
  "chen00f_icslp": {
   "authors": [
    [
     "Dong",
     "Chen"
    ],
    [
     "Jingming",
     "Kuang"
    ],
    [
     "Yan",
     "Zhang"
    ]
   ],
   "title": "A new speech classifier based on Yinyang compensatory soft computing theory",
   "original": "i00_2186",
   "page_count": 4,
   "order": 240,
   "p1": "vol. 2, 186-189",
   "pn": "",
   "abstract": [
    "In future mobile communication system, it is very important to expand the limited radio resources through exploiting the silence compression technology In this paper, a new speech classifier based on YinYang Compensatory soft computing theory is described. The classifier is used to divide the noisy speech signal input to the mobile phone into the active speech (including voiced, unvoiced and transition) and inactive speech (including silence or noise) during conversation. The novel speech classifier algorithm is discussed and verified at the GSM discontinuous transmission platform with an adaptive multi-rate speech codec. We found the accuracy of the classifier is approximately 98% in clean background acoustic environment. Moreover, we analyzed its classification accuracy in eight types of noise environment with varying signal-to noise ratio.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-240"
  },
  "moller00b_icslp": {
   "authors": [
    [
     "Sebastian",
     "Möller"
    ],
    [
     "Ute",
     "Jekosch"
    ],
    [
     "Alexander",
     "Raake"
    ]
   ],
   "title": "New models predicting conversational effects of telephone transmission on speech communication quality",
   "original": "i00_2190",
   "page_count": 4,
   "order": 241,
   "p1": "vol. 2, 190-193",
   "pn": "",
   "abstract": [
    "In the planning process of telephone networks, it is important to have estimates of the speech communication quality which can be obtained with specific network configurations, even before the network has been set up. For this aim, network planning models have been developed. They base quality predictions on instrumentally measurable input parameters of the transmission chain, and take conversational factors into account. In this paper, the most prominent and complete model, the so-called E-model, is presented in more detail. The underlying principles are discussed, and it is shown how new speech processing equipment (e.g. codecs) can be included in the model. Comparisons of model quality predictions to auditory test data show how conversational aspects are covered by the model, and where modifications can improve quality prediction output. An outlook identifies the parameters where work is still needed in order to use network planning models to reliably predict conversational impacts in modern network scenarios, and to estimate the influence of the transmission channel on dialogue system quality.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-241"
  },
  "li00d_icslp": {
   "authors": [
    [
     "Jinyu",
     "Li"
    ],
    [
     "Xin",
     "Luo"
    ],
    [
     "Ren-Hua",
     "Wang"
    ]
   ],
   "title": "A novel search algorithm for LSF VQ",
   "original": "i00_2194",
   "page_count": 4,
   "order": 242,
   "p1": "vol. 2, 194-197",
   "pn": "",
   "abstract": [
    "Department of Electronic Engineering and Information Science, University of Science and Technology of China, Hefei, China Because classical fast vector quantization(VQ) algorithms can't error(1): function not defined be used in the LSF vector quantizers that use varying weighted Euclidean distance, a novel fast VQ search algorithm - CRVQ-CS (Constrained Range Vector Quantization based on Component Searching) is presented in this paper. The CRVQ-CS algorithm works well with the varying weighted Euclidean distance and yields the same result as full search VQ with reduced computational complexity does. Although the CRVQ-CS algorithm is proposed for VQ using varying weighted Euclidean distance measure, it is also suitable for VQ using simple Euclidean distance measure.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-242"
  },
  "maes00b_icslp": {
   "authors": [
    [
     "Stéphane H.",
     "Maes"
    ],
    [
     "Dan",
     "Chazan"
    ],
    [
     "Gilad",
     "Cohen"
    ],
    [
     "Ron",
     "Hoory"
    ]
   ],
   "title": "Conversational networking: conversational protocols for transport, coding, and control",
   "original": "i00_2198",
   "page_count": 4,
   "order": 243,
   "p1": "vol. 2, 198-201",
   "pn": "",
   "abstract": [
    "With the increased deployment of conversational systems, new technical challenges and limitations appear. Client devices do not have the processing power and memory requirements to perform the necessary recognition and presentation tasks. Also, networks do not have enough bandwidth to rapidly exchange the data files needed by the conversational engines. Assuming that these two problems are solved, vendors and service providers often do not wish to exchange such data files which may be considered as intellectual, business logic and technology crown jewels. Distributed architectures solve these issues, if implemented in appropriately managed networks to guarantee quality of service for each active dialog. In this paper, we introduce some aspects of conversational networking: distributed speech recognition (DSR), distributed conversational architecture and conversational protocols for transport, coding and control. These are building blocks of a conversational networking solutions and distributed multi-modal browsers.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-243"
  },
  "ohmura00_icslp": {
   "authors": [
    [
     "Hiroshi",
     "Ohmura"
    ],
    [
     "Akira",
     "Sasou"
    ],
    [
     "Kazuyo",
     "Tanaka"
    ]
   ],
   "title": "A low bit rate speech coding method using a formant-articulatory parameter nomogram",
   "original": "i00_2202",
   "page_count": 4,
   "order": 244,
   "p1": "vol. 2, 202-205",
   "pn": "",
   "abstract": [
    "In this paper, we propose a new method for low bit rate speech coding using a nomogram that is a pair of codebooks representing the functional relationship between formant frequencies and articulatory parameters. Significant features of our approach are 1) using the codebooks derived theoretically from the computation using a stylized vocal tract model and 2) independent coding by separating frequency information from the amplitude in a speech segment. From these features, the method is also characterized by little dependency upon speech databases and/or languages in the acoustic domain, so that it has a potential to construct a more flexible rule-based speech synthesis system. We have conducted articulatory encode-decode experiments with the bit rate range from 3.2kbps to 1.6kbps using speech samples in ASJ and TIMIT speech databases and confirmed that good quality speech synthesis is achieved with improvements on the bit allocation scheme and a frame sampling method.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-244"
  },
  "li00e_icslp": {
   "authors": [
    [
     "Ning",
     "Li"
    ],
    [
     "Derek J.",
     "Molyneux"
    ],
    [
     "Meau Shin",
     "Ho"
    ],
    [
     "B. M. G.",
     "Cheetham"
    ]
   ],
   "title": "Variable bit-rate sinusoidal transform coding using variable order spectral estimation",
   "original": "i00_2206",
   "page_count": 4,
   "order": 245,
   "p1": "vol. 2, 206-209",
   "pn": "",
   "abstract": [
    "Sinusoidal transform coding (STC) is known to be capable of producing good communication quality speech coded at bitrates below 4kb/s. Discrete all-pole modelling (DAP) is an alternative spectral estimation method which can be more accurate than the conventional linear prediction (LP) analysis normally used by STC. In the quest to achieve the highest possible speech quality at lower and lower average bit-rates in variable bit-rate coding schemes, more and more effort must be made to investigate ways of varying the number of parameters according to the characteristics of each speech frame. This paper considers the advantage to be gained by varying the all-pole model order according to the discrete Itakura-Saito (IS) distance measure used in DAP. A significant reduction is achieved in the average number of parameters to be quantised compared to the fixed order model while the speech quality remains the same.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-245"
  },
  "choi00b_icslp": {
   "authors": [
    [
     "Yong-Soo",
     "Choi"
    ],
    [
     "Sueng-Kyun",
     "Ryu"
    ],
    [
     "Young-Cheol",
     "Park"
    ],
    [
     "Dae-Hee",
     "Youn"
    ]
   ],
   "title": "Efficient harmonic-CELP based hybrid coding of speech at low bit rates",
   "original": "i00_2210",
   "page_count": 5,
   "order": 246,
   "p1": "vol. 2, 210-213",
   "pn": "",
   "abstract": [
    "This paper presents an efficient Harmonic-CELP hybrid coder at 2.4 kbps utilizing the well-known characteristics of the Harmonic and CELP coders. According to frame voicing decision, the proposed hybrid coder switches the RP-VSELP coder as a fast CELP in case of unvoiced, or an improved Harmonic coder in case of voiced. The proposed Harmonic-CELP hybrid coder has several features as follows: fast CELP coding, fast harmonic estimation, variable dimension harmonic vector quantization, perceptual weighting including Bark frequency resolution, fast harmonic synthesis, and naturalness control by band voicing. To demonstrate the performance of the proposed hybrid coder, a 2.4 kbps coder has been implemented and compared with 5.3 kbps ACELP and 4.4 kbps IMBE as reference coders. From results of subjective tests, the proposed hybrid coder showed good quality at about half rates of the reference coders.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-246"
  },
  "jensen00_icslp": {
   "authors": [
    [
     "Jesper",
     "Jensen"
    ],
    [
     "John H. L.",
     "Hansen"
    ]
   ],
   "title": "Speech enhancement based on a constrained sinusoidal model",
   "original": "i00_2214",
   "page_count": 4,
   "order": 247,
   "p1": "vol. 2, 214-217",
   "pn": "",
   "abstract": [
    "In this study we propose an algorithm for enhancement of speech degraded by additive broad-band noise. The algorithm represents speech using a sinusoidal model, where model parameters are estimated iteratively. In order to ensure speech-like characteristics observed in clean speech, the model parameters are restricted to satisfy certain smoothness constraints. The algorithm is evaluated using speech signals degraded by additive white Gaussian noise. Results from both objective and subjective evaluations show considerable improvement over traditional spectral subtraction and Wiener filtering based schemes. In particular, in a subjective AB preference test, where enhanced signals were encoded/decoded with the G729 speech codec, the proposed scheme was preferred over the traditional schemes in more than 5 out of 6 cases for input SNRs ranging from 5-20 dB.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-247"
  },
  "park00_icslp": {
   "authors": [
    [
     "Sang-Wook",
     "Park"
    ],
    [
     "Seung-Kyun",
     "Ryu"
    ],
    [
     "Young-Cheol",
     "Park"
    ],
    [
     "Dae-Hee",
     "Youn"
    ]
   ],
   "title": "A bark coherence function for perceived speech quality estimation",
   "original": "i00_2218",
   "page_count": 4,
   "order": 248,
   "p1": "vol. 2, 218-221",
   "pn": "",
   "abstract": [
    "A new methodology for perceptual quality measure is presented. The new method defines the bark coherence function (BCF) as a new cognition module. False prediction errors are occasionally observed in previously developed perceptual measures when they are applied to the end-to-end speech quality measurement of communication systems. Those errors are mainly caused by the linear distortion of the analogue interface of the system being evaluated. The BCF itself normalizes those effects of linear filtering, so that it is ideal for the speech quality assessment of mobile communication systems. In addition, the proposed scheme does not require the local as well as global scaling, so that it is robust to the difference between the original and received speech levels. To evaluate the performance of the new perceptual model, the regression analysis was performed with CDMA digital cellular, CDMA personal communication service (PCS) and speech codecs. The correlation coefficients computed using the BCF showed noticeable improvements over the PSQM that is recommended by ITU-T. Robustness of the BCF to various conditions was also tested.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-248"
  },
  "kiang00_icslp": {
   "authors": [
    [
     "Jinyu",
     "Kiang"
    ],
    [
     "Kun",
     "Deng"
    ],
    [
     "Ronghuai",
     "Huang"
    ]
   ],
   "title": "A high-efficiency scheme for secure speech transmission using spatiotemporal chaos synchronization",
   "original": "i00_2222",
   "page_count": 4,
   "order": 249,
   "p1": "vol. 2, 222-225",
   "pn": "",
   "abstract": [
    "A high-efficiency scheme for secure speech transmision via computer networks is proposed. Two one-way coupled map lattice (OCOML) systems driven by a common driving sequence are synchronized. The chaotic signals of the space units of OCOML at the transmitter serve as the encryption keys and those at the receiver as the decryption keys. Each driving sample can drive the OCOML to generate many keys at the same time. The communication efficiency is enhanced significantly. An example of speech transmission is demonstrated.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-249"
  },
  "rodriguezlinares00_icslp": {
   "authors": [
    [
     "Leandro",
     "Rodríguez Liñares"
    ],
    [
     "Carmen",
     "García Mateo"
    ]
   ],
   "title": "Application of speaker authentication technology to a telephone dialogue system",
   "original": "i00_2226",
   "page_count": 4,
   "order": 250,
   "p1": "vol. 2, 226-229",
   "pn": "",
   "abstract": [
    "TelCorreo is a dialogue system that allows users to manage their e-mail accounts using the telephone. Regarding speech technology, TelCorreo is a highly demanding application, since it needs of state-of-art text-to-speech system, speech recognizer and speaker authentifier. The main features of TelCorreo are described in [1] and [2]. In this paper, we will concentrate on the speaker authentication module.\n",
    "Presently, there are very few speaker recognition systems working in real-world applications. The reason is that the performance achieved by most of the techniques proposed in the literature dramatically drops for out-of-lab conditions. Realworld conditions raise new issues to be considered and a compromise between performance and feasibility must be achieved.\n",
    "We studied the problem of how to increase the performance of a speaker authentifier system by combining the outputs of an utterance verifier and a speaker verifier (the former recognizes the actual verbal content of the speech, while the latter uses speaker voice by itself as a way of verifying the identity).\n",
    "We have developed a speaker authentifier module suitable to be integrated in a real-world dialogue system. In this paper, we describe the main relevant aspects of this module. The rest of this paper is structured as follows. Section 1 introduces TelCorreo and its speaker authentication module. Section 2 explains the module design from the users' point of view, while section 3 presents TelCorreoDB: an experimental frame adequate for testing the speaker authentication technology used in TelCorreo. Section 4 presents the implementation details. Finally, some conclusions and guidelines for further work are included.\n",
    "s L. Rodríguez-Liñares et al.: \"TelCorreo: A bilingual e-mail client over the telephone\". Accepted for publication in TSD'2000 Proceedings, Brno, Czech Republic (2000). In http://www.gts.tsc.uvigo.es/telcorreo a description of the system TelCorreo can be found.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-250"
  },
  "dutat00_icslp": {
   "authors": [
    [
     "Michel",
     "Dutat"
    ],
    [
     "Ivan",
     "Magrin-Chagnolleau"
    ],
    [
     "Frédéric",
     "Bimbot"
    ]
   ],
   "title": "Language recognition using time-frequency principal component analysis and acoustic modeling",
   "original": "i00_2230",
   "page_count": 4,
   "order": 251,
   "p1": "vol. 2, 230-233",
   "pn": "",
   "abstract": [
    "Time-Frequency Principal Component (TFPC) is a speech parameterization technique based on a principal component analysis applied to acoustic feature parameters augmented by their time context. In this paper, we investigate on the performance of TFPC in the framework of automatic language recognition. In our experiments, identification rate is improved compared to the use of the conventional cepstral coefficients augmented by their Δ coefficients.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-251"
  },
  "tanprasert00_icslp": {
   "authors": [
    [
     "Chularat",
     "Tanprasert"
    ],
    [
     "Varin",
     "Achariyakulporn"
    ]
   ],
   "title": "Comparative study of GMM, DTW, and ANN on Thai speaker identification system",
   "original": "i00_2234",
   "page_count": 4,
   "order": 252,
   "p1": "vol. 2, 234-237",
   "pn": "",
   "abstract": [
    "This paper proposes a new investigation on Gaussian mixture model (GMM) by comparing it with some preliminary experiments on multilayered perceptron network (MLP) with backpropagation learning algorithm (BKP) and dynamic time warping (DTW) techniques on Thai text-dependent speaker identification system. Three major identification engines are conducted on 50 speakers with isolated digits 0-9. Training and testing utterances were recorded over a five week duration. Furthermore, three well-known speech features, namely linear predictive coding derived cepstrum (LPCC), postfiltered ceptrum (PFL), and Mel frequency cepstral coefficient (MFCC) were evaluated. From our previous experiments, the MFCC has given the highest identification rates on DTW and MLP. Therefore, GMM with MFCC feature was experimented and attained 87.54% average identification accuracy, as opposed to 86.74% of DTW and 82.34% of MLP. The results are the same with top-3 concatenated digits, the average identification rates are 99%, 98.70 %, and 97.30% for GMM, DTW, and MLP, respectively.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-252"
  },
  "schwardt00_icslp": {
   "authors": [
    [
     "Ludwig",
     "Schwardt"
    ],
    [
     "Johan du",
     "Preez"
    ]
   ],
   "title": "Efficient mixed-order hidden Markov model inference",
   "original": "i00_2238",
   "page_count": 4,
   "order": 253,
   "p1": "vol. 2, 238-241",
   "pn": "",
   "abstract": [
    "Recent studies have shown that high-order hidden Markov models (HMMs) are feasible and useful for spoken language processing. This paper extends the fixed-order versions to ergodic mixedorder HMMs, which allow the modelling of variable-length contexts with significantly less parameters. A novel training procedure automatically infers the number of states and the topology of the HMM from the training set, based on information-theoretic criteria. This is done by incorporating only high-order contexts with sufficient support in the data. The mixed-order training algorithm is faster than fixed-order methods, with similar classifi- cation performance in language identification tasks.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-253"
  },
  "thyes00_icslp": {
   "authors": [
    [
     "Olivier",
     "Thyes"
    ],
    [
     "Roland",
     "Kuhn"
    ],
    [
     "Patrick",
     "Nguyen"
    ],
    [
     "Jean-Claude",
     "Junqua"
    ]
   ],
   "title": "Speaker identification and verification using eigenvoices",
   "original": "i00_2242",
   "page_count": 4,
   "order": 254,
   "p1": "vol. 2, 242-245",
   "pn": "",
   "abstract": [
    "Gaussian Mixture Models (GMMs) have been successfully applied to the tasks of speaker ID and verification when a large amount of enrolment data is available to characterize client speakers. However, there are many applications where it is unreasonable to expect clients to spend this much time training the system. Thus, we have been exploring the performance of various methods when only a sparse amount of enrolment data is available. Under such conditions, the performance of GMMs deteriorates drastically. A possible solution is the \"eigenvoice\" approach, in which client and test speaker models are confined to a low-dimensional linear subspace obtained previously from a different set of training data. One advantage of the approach is that it does away with the need for impostor models for speaker verification.\n",
    "After giving a detailed description of the eigenvoice approach, the paper compares the performance of conventional GMMs on speaker ID and verification with that of GMMs obtained by means of the eigenvoice approach. Experimental results are presented to show that conventional GMMs perform better if there are abundant enrolment data, while eigenvoice GMMs perform better if enrolment data are sparse. The paper also gives experimental results for the case where the eigenspace is trained on one database (TIMIT), but client enrolment and testing involve another (YOHO). For this case, we show that performance improves if an environment adaptation technique is applied to the eigenspace. Finally, we discuss priorities for future work.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-254"
  },
  "surendran00_icslp": {
   "authors": [
    [
     "Arun C.",
     "Surendran"
    ],
    [
     "Chin-Hui",
     "Lee"
    ]
   ],
   "title": "A priori threshold selection for fixed vocabulary speaker verification systems",
   "original": "i00_2246",
   "page_count": 4,
   "order": 255,
   "p1": "vol. 2, 246-249",
   "pn": "",
   "abstract": [
    "A priori threshold selection is an important problem in practical speaker verification (SV) systems. Most earlier empirical methods for estimating prior thresholds assume availability of data from impostors i.e. speakers saying the same phrase as the desired speaker. While this is true for most databases available for research, it is not so in practice. In this paper,we present a novel prior threshold selection mechanism that uses the distributions of the \"units\" of the pass phrase rather than the entire phrase. Hence the new approach does not need imposter data and provides flexibility and adaptability to the system. We demonstrate the effectiveness of our new idea in a connected-digit based SV system.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-255"
  },
  "jin00_icslp": {
   "authors": [
    [
     "Qin",
     "Jin"
    ],
    [
     "Alex",
     "Waibel"
    ]
   ],
   "title": "Application of LDA to speaker recognition",
   "original": "i00_2250",
   "page_count": 4,
   "order": 256,
   "p1": "vol. 2, 250-253",
   "pn": "",
   "abstract": [
    "The speaker recognition task falls under the general problem of pattern classification. Speaker recognition as a pattern classification problem, its ultimate objective is design of a system that classifies the vector of features in different classes by partitioning the feature space into optimal speaker discriminative space. Linear Discriminant Analysis (LDA) is a feature extraction method that provides a linear transformation of n-dimensional feature vectors (or samples) into mdimensional space (m < n), so that samples belonging to the same class are close together but samples from different classes are far apart from each other. In this paper we discuss the issue of the application of LDA to our Gaussian Mixture Model (GMM) based speaker identification task. Applying LDA improved the identification performance.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-256"
  },
  "schwardt00b_icslp": {
   "authors": [
    [
     "Ludwig",
     "Schwardt"
    ],
    [
     "Johan du",
     "Preez"
    ]
   ],
   "title": "Automatic language identification using mixed-order HMMs and untranscribed corpora",
   "original": "i00_2254",
   "page_count": 4,
   "order": 257,
   "p1": "vol. 2, 254-257",
   "pn": "",
   "abstract": [
    "The state-of-the-art language identification (LID) systems are based on phone recognisers and n-gram language models, which require the use of transcribed speech databases for training. An alternate solution to the LID problem directly applies mixedorder hidden Markov models (HMMs) to untranscribed speech. The competitive performance of these mixed-order HMMs on the NIST 1996 evaluation set is very promising, considering the ease of implementation and many possible improvements. This validates a novel mixed-order HMM training procedure and extends previous results obtained with high-order HMMs to take advantage of larger datasets.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-257"
  },
  "lindberg00_icslp": {
   "authors": [
    [
     "Johan",
     "Lindberg"
    ],
    [
     "Mats",
     "Blomberg"
    ]
   ],
   "title": "On the potential threat of using large speech corpora for impostor selection in speaker verification",
   "original": "i00_2258",
   "page_count": 4,
   "order": 258,
   "p1": "vol. 2, 258-261",
   "pn": "",
   "abstract": [
    "In order to evaluate the risk in SV systems one should take into account the possible perpetrator who knows whom he/she is attacking. This paper thus evaluated if a large speech corpus can be used for selecting impostors to use against a speaker verification (SV) system. We tested the possibility by selecting the most similar speakers from a large corpus and then using the recordings in this corpus for impostor attempts against clients in an SV system. Speech samples of the clients uttering words not used in the SV service was used in order to search the large corpus for similar speakers. Recordings of utterances used in the SV service was then collected from these similar speakers and used for impostor attempts. Our conclusion is that this scenario is a threat that needs to be considered by providers of SV systems.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-258"
  },
  "ortegagarcia00_icslp": {
   "authors": [
    [
     "J.",
     "Ortega-Garcia"
    ],
    [
     "J. G.",
     "Rodriguez"
    ],
    [
     "D. T.",
     "Merino"
    ]
   ],
   "title": "Phonetic consistency in Spanish for pin-based speaker verification system",
   "original": "i00_2262",
   "page_count": 4,
   "order": 259,
   "p1": "vol. 2, 262-265",
   "pn": "",
   "abstract": [
    "The use of uttered Personal Identification Numbers (PIN) is a well-suited approach for person identification through voice in real applications. In this paper, speaker verification with short 4-digit strings, in a pragmatic perspective where very few utterances for training are available, is accomplished. The problem here arises due to the small quantity of voice available in short PIN utterances. Furthermore, it has to be taken into account the specificity of Spanish in this task, as digit strings are not uttered in a isolated digit-by-digit basis, but mentally grouped without constraints, and read as whole figures, with varying groups for different utterances of the same PIN. This specific factor induces high dependency on the phonetic contents of the PIN, and complicates considerably the design of text-dependent systems. Considering this, a textindependent GMM speaker verification system, including nearest reference speaker and universal background model score normalization, together with CMN channel compensation, has been evaluated over a specific PIN database, where different training conditions (phonetic dependent/independent) are tested.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-259"
  },
  "liu00b_icslp": {
   "authors": [
    [
     "Zhimin",
     "Liu"
    ],
    [
     "Xihong",
     "Wu"
    ],
    [
     "Bin",
     "Zhen"
    ],
    [
     "Huisheng",
     "Chi"
    ]
   ],
   "title": "An auditory feature extraction method based on forward-masking and its application in robust speaker identification and speech recognition",
   "original": "i00_2266",
   "page_count": 4,
   "order": 260,
   "p1": "vol. 2, 266-269",
   "pn": "",
   "abstract": [
    "This article presents a new auditory feature extraction method, which considers the forwardmasking mechanism of auditory nerves and feasible in practice. Two features based on this method are extracted: FMFRC (forward masking firing-rate cepstrum) and FMSRC (forward masking synchronized rate cepstrum). Isolate-word speech recognition and text-dependent speaker identification experiments based on TI46 are conducted. The experiment results show that the new auditory features has comparable performance with MFCC under clean environment but far better noise-resistant property than MFCC in both tasks.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-260"
  },
  "peters00_icslp": {
   "authors": [
    [
     "S. Douglas",
     "Peters"
    ],
    [
     "Matthieu",
     "Hébert"
    ],
    [
     "Daniel",
     "Boies"
    ]
   ],
   "title": "Transition-oriented hidden Markov models for speaker verification",
   "original": "i00_2270",
   "page_count": 4,
   "order": 261,
   "p1": "vol. 2, 270-273",
   "pn": "",
   "abstract": [
    "In this article, we present a novel mechanism by which more precise voiceprints can be constructed in a typical text-dependent speaker verification system based on a continuous density hidden Markov model (HMM). Typical voiceprints (speaker-dependent HMMs) are first trained using a subscriber's enrollment data. The resulting models are then restructured to permit a modeling of sub-state behavior. At first, the restructured models are functionally equivalent to the conventional voiceprint. Sub-state parameters are then estimated by the re-application of the enrollment data. The resulting speaker-dependent models provide improved speaker verification performance relative to the models with the original topology.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-261"
  },
  "tsoi00_icslp": {
   "authors": [
    [
     "Pang Kuen",
     "Tsoi"
    ],
    [
     "Pascale",
     "Fung"
    ]
   ],
   "title": "An LLR-based technique for frame selection for GMM-based text-independent speaker identification",
   "original": "i00_2274",
   "page_count": 4,
   "order": 262,
   "p1": "vol. 2, 274-277",
   "pn": "",
   "abstract": [
    "In speaker recognition systems, frame selection, which aims at determining which frame is useful and which is not and selecting useful frames from the test utterance, can be utilized to increase recognition accuracy. In this paper, we present a new approach for frame selection using Log Likelihood Ratio (LLR), which is based on the idea that if a frame contains speaker information, the Log likelihood Score of the corresponding speaker model will be much larger than that of its competing model. As a result, for each frame we can calculate the Log Likelihood Ratio (LLR) between the largest score and the second largest score in different speaker models and take it as a reference: Those frames with a small LLR can be rejected and those with a large LLR can be kept. This algorithm is implemented based on a GMM-based text-independent speaker identification system. We compare the algorithm with another frame selection approach based on Jensen Difference (ID). Experiment shows that the approach using SD reduces the error by about 39,34%, while our approach using LLR reduces the error by about 46.32%.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-262"
  },
  "ma00_icslp": {
   "authors": [
    [
     "Jiyong",
     "Ma"
    ],
    [
     "Wen",
     "Gao"
    ]
   ],
   "title": "Robust speaker recognition based on high order cumulant",
   "original": "i00_2278",
   "page_count": 4,
   "order": 263,
   "p1": "vol. 2, 278-281",
   "pn": "",
   "abstract": [
    "LP-derived cepstral coefficients are sensitive to additive noise in speech signal. In this paper, an approach to extracting speech feature based on the high-order cumulant is proposed to depress the effect of additive noise in speech signal. The performance of this approach is evaluated using a text-prompt speaker verification system. Experimental results show that this approach is effective to increase the robustness of the speaker recognition system.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-263"
  },
  "si00_icslp": {
   "authors": [
    [
     "Luo",
     "Si"
    ],
    [
     "Qi Xiu",
     "Hu"
    ]
   ],
   "title": "Two-stage speaker identification system based on VQ and NBDGMM",
   "original": "i00_2282",
   "page_count": 4,
   "order": 264,
   "p1": "vol. 2, 282-285",
   "pn": "",
   "abstract": [
    "In this paper, a new speaker identification system is presented. The system can be divided into two subsystems, one close-set speaker identification system and one speaker verification system. The VQ model is used in the close-set speaker identification system and a new method called NBDGMM (Normalization Based on Difference of GMM) is introduced. Experiments have been done to prove that this speaker identification system is efficient.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-264"
  },
  "mariethoz00_icslp": {
   "authors": [
    [
     "Johnny",
     "Mariethoz"
    ],
    [
     "Johan",
     "Lindberg"
    ],
    [
     "Frédéric",
     "Bimbot"
    ]
   ],
   "title": "A MAP approach, with synchronous decoding and unit-based normalization for text-dependent speaker verification",
   "original": "i00_2286",
   "page_count": 4,
   "order": 265,
   "p1": "vol. 2, 286-289",
   "pn": "",
   "abstract": [
    "This paper presents an overview of some of the research tracks in text-dependent speaker verification followed by the Picasso Project (Research Work Package). We focus successively on the training algorithm (based on a MAP approach), the state-sequence decoding procedure (by Synchronous Alignment) and the score normalization (unit-based z-normalization). On short utterances of 1 to 5 syllables (Polyvar command words), we obtain an error rate of 5.6 % with 2 training sessions, which can be brought down to 3.3% with 3 additional sessions in incremental trainmg mode.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-265"
  },
  "pan00b_icslp": {
   "authors": [
    [
     "Zhibin",
     "Pan"
    ],
    [
     "Koji",
     "Kotani"
    ],
    [
     "Tadahiro",
     "Ohmi"
    ]
   ],
   "title": "A fast search method of speaker identification for large population using pre-selection and hierarchical matching",
   "original": "i00_2290",
   "page_count": 4,
   "order": 266,
   "p1": "vol. 2, 290-293",
   "pn": "",
   "abstract": [
    "Performance of search during matching phase in a speaker identification system realized through vector quantization (VQ) is investigated in this paper. Voice of each person is recorded in a office room with personal computers. LPC-cepstrum is selected as feature vector. In order to gain higher success rate of identification, it is necessary to use larger size codebook for each person. Consequently, it is extremely time-consuming to do full matching directly with all of registered larger size codebooks for large population to determine who the current speaker is. To fulfill identification at a tolerable speed, trade-off between success rate and search time is unavoidable usually.\n",
    "In this paper, a fast search method is proposed that is based on pre-selection and hierarchical matching so as to eliminate a large amount of impossible candidates before doing final fine matching with larger size codebooks meanwhile keep the success rate not degraded. Pre -selection is achieved using divergences between unknown input speech and all registered codebooks. Hierarchical matching is implemented using smaller size codebook and larger size codebook corresponding to each person respectively. With this method, for a population of 290, time required for identification can be reduced to about 13% while memory occupation is increased by just 6% when baselines are taken as those using larger size codebooks in conventional way. Success rate is not degraded as 94% comparing to that acquired in conventional way as well.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-266"
  },
  "wang00d_icslp": {
   "authors": [
    [
     "Lan",
     "Wang"
    ],
    [
     "Ke",
     "Chen"
    ],
    [
     "Huisheng",
     "Chi"
    ]
   ],
   "title": "Optimal fusion of diverse feature sets for speaker identification: an alternative method",
   "original": "i00_2294",
   "page_count": 4,
   "order": 267,
   "p1": "vol. 2, 294-297",
   "pn": "",
   "abstract": [
    "For speaker identification, a robust and effective feature extraction method is necessary. But in the current circumstance, there exists no perfect feature that could optimally characterize physiological difference among speakers regardless of personal variation. A soft competition scheme for optimal fusion of diverse feature sets is applied to speaker identification in order to achieve the improved performance. Based on a linear combination scheme, diverse feature vectors are used together while the winning feature vector through soft competition plays more important role in the representation. The simulations on KING corpus show that this alternative method could yield good performance for speaker identification.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-267"
  },
  "chaudhari00_icslp": {
   "authors": [
    [
     "Upendra V.",
     "Chaudhari"
    ],
    [
     "Jiri",
     "Navrátil"
    ],
    [
     "Stéphane H.",
     "Maes"
    ],
    [
     "Ramesh",
     "Gopinath"
    ]
   ],
   "title": "Transformation enhanced multi-grained modeling for text-independent speaker recognition",
   "original": "i00_2298",
   "page_count": 4,
   "order": 268,
   "p1": "vol. 2, 298-301",
   "pn": "",
   "abstract": [
    "We describe our formulation of transformation enhanced data modeling used to develop a multi-grained data analysis approach to text independent speaker recognition. The broad goal is to address difficulties caused by sparse training and test data. First, our development of maximum likelihood transformation based recognition with diagonally constrained Gaussian mixture models is detailed. We give results to show its robustness to decreasing training data. Then using the these models as building blocks, a multigrained model structure is developed. For this, the training data must be labeled, e.g. with an HMM based phone labeler. A graduated phone class structure is then used to train the speaker model at various levels of detail. This structure is a tree with the root node containing all the phones. Subsequent levels partition the phones into increasingly finer grained linguistic classes. We demonstrate the effectiveness of the modeling with identification and verification experiments.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-268"
  },
  "masuko00_icslp": {
   "authors": [
    [
     "Takashi",
     "Masuko"
    ],
    [
     "Keiichi",
     "Tokuda"
    ],
    [
     "Takao",
     "Kobayashi"
    ]
   ],
   "title": "Imposture using synthetic speech against speaker verification based on spectrum and pitch",
   "original": "i00_2302",
   "page_count": 4,
   "order": 269,
   "p1": "vol. 2, 302-305",
   "pn": "",
   "abstract": [
    "This paper describes security of speaker verification systems against imposture using synthetic speech. We propose a text-prompted speaker verification technique which utilizes pitch information in addition to spectral information, and investigate whether synthetic speech is rejected. Experimental results show that pitch information is not necessarily useful for rejection of synthetic speech, and it is required to develop techniques to discriminate synthetic speech from natural speech.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-269"
  },
  "parveen00_icslp": {
   "authors": [
    [
     "Shahla",
     "Parveen"
    ],
    [
     "Abdul",
     "Qadeer"
    ],
    [
     "Phil",
     "Green"
    ]
   ],
   "title": "Speaker recognition with recurrent neural networks",
   "original": "i00_2306",
   "page_count": 4,
   "order": 270,
   "p1": "vol. 2, 306-309",
   "pn": "",
   "abstract": [
    "We report on the application of recurrent neural nets in a openset text-dependent speaker identification task. The motivation for applying recurrent neural nets to this domain is to find out if their ability to take short-term spectral features but yet respond to long-term temporal events is advantageous for speaker identification.\n",
    "We use a feedforward net architecture adapted from that introduced by Robinson et.al. We introduce a fully-connected hidden layer between the input and state nodes and the output. We show that this hidden layer makes the learning of complex classification tasks more efficient. Training uses back propagation through time. There is one output unit per speaker, with the training targets corresponding to speaker identity.\n",
    "For 12 speakers (a mixture of male and female) we obtain a true acceptance rate 100% with a false acceptance rate 4%. For 16 speakers these figures are 94% and 7% respectively. We also investigate the sensitivity of identification accuracy to environmental factors (signal level, change of microphone and band limitation), choice of acoustic vectors (FFT, LPC or Cepstral), distribution of speakers in the training database, inclusion of fundamental frequency. FFT features plus fundamental frequency give the best results.\n",
    "This performance is shown to compare favorably with studies reported on similar tasks with Hidden Markov Model technique.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-270"
  },
  "itoh00_icslp": {
   "authors": [
    [
     "Yoshiroh",
     "Itoh"
    ],
    [
     "Jun",
     "Toyama"
    ],
    [
     "Masaru",
     "Shimbo"
    ]
   ],
   "title": "Speaker feature extraction from pitch information based on spectral subtraction for speaker identification",
   "original": "i00_2310",
   "page_count": 4,
   "order": 271,
   "p1": "vol. 2, 310-313",
   "pn": "",
   "abstract": [
    "Robust speaker feature extraction under noise conditions is an important issue for application of a speaker recognition system. It is well known that LPC cepstrum, which expresses the spectral envelope, is e\u000bective for speaker recognition. This implies that the spectral rough structure is e\u000bective for speaker recognition. However, LPC cepstrum is a noise-sensitive feature. On the other hand, spectral subtraction is an effective speech enhancement method under stationary noise conditions. In this study, we developed a new method for feature extraction based on spectral subtraction and noise robust spectral rough structures, and we evaluated the e\u000bectiveness of the feature extraction method in speaker identification experiments.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-271"
  },
  "tsai00_icslp": {
   "authors": [
    [
     "Wei-Ho",
     "Tsai"
    ],
    [
     "Chiwei",
     "Che"
    ],
    [
     "Wen-Whei",
     "Chang"
    ]
   ],
   "title": "Text-independent speaker identification using Gaussian mixture bigram models",
   "original": "i00_2314",
   "page_count": 4,
   "order": 272,
   "p1": "vol. 2, 314-317",
   "pn": "",
   "abstract": [
    "In this paper, a novel speaker modeling technique based on Gaussian mixture bigram model (GMBM) is introduced and evaluated for text-independent speaker identification (speaker-ID). GMBM is a stochastic framework that explores the context or time dependency of continuous observations from an information source. In view of the fact that speech features are correlated between successive frames, we attempt to investigate if speaker-ID can be aided by modeling the spectral correlation in speech through the usage of GMBMs. The proposed method was evaluated on a 100-speaker speech database. Experimental results demonstrated that the error rate of speaker-ID could be greatly reduced by using GMBMs, compared to the conventional speaker-ID technique based on Gaussian mixture models (GMMs).\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-272"
  },
  "ezzaidi00_icslp": {
   "authors": [
    [
     "Hassan",
     "Ezzaidi"
    ],
    [
     "Jean",
     "Rouat"
    ]
   ],
   "title": "Comparison of MFCC and pitch synchronous AM, FM parameters for speaker identification",
   "original": "i00_2318",
   "page_count": 4,
   "order": 273,
   "p1": "vol. 2, 318-321",
   "pn": "",
   "abstract": [
    "We study robust pitch synchronous parameters that are derived from envelope and instantaneous frequencies estimated via a bank of cochlear filters. Closed set Speaker Identification experiments are performed on the SPIDRE corpus with matched and mismatched handsets conditions. The recognizer is based on a hybrid Linear Vector Quantization and Single Layer Perceptron (LVQSLP). Experiments are reported with different codebook sizes. In mismatched condition, the Mel Frequency Cepstral Coefficients (MFCC) yield slightly better rating (68%) than Envelope (58%) and Instantaneous Frequency (65%) parameters when used independently. When the MFCC based recognizer is used in conjunction with the envelope based recognizer, the recognition rate increases to 80%. We also report identification rates based on two classes: women and men. In another experiment, listeners were asked to discriminate speakers on a subset of ten females. We discuss their performance. We also discuss the potential of the approach and of judicious combination of the parameters to improve Speaker Identification Systems.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-273"
  },
  "faundezzanu00_icslp": {
   "authors": [
    [
     "Marcos",
     "Faúndez-Zanu"
    ],
    [
     "Adam",
     "Slupinski"
    ]
   ],
   "title": "Speaker verification in mismatch training and testing conditions",
   "original": "i00_2322",
   "page_count": 7,
   "order": 274,
   "p1": "vol. 2, 322-325",
   "pn": "",
   "abstract": [
    "This paper presents an exhaustive study about the robustness of several parameterizations, with a new database specially acquired for the purpose of a speaker recognition application. This database includes the following variations: different recording sessions (including telephonic and microphonic recordings), recording rooms, and languages (it has been obtained from a bilingual set of speakers). This study has been performed with covariance matrices in a text independent speaker verification application. It reveals that the combination of several parameterizations can improve the robustness in all the scenarios.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-274"
  },
  "uchibe00_icslp": {
   "authors": [
    [
     "Toshiaki",
     "Uchibe"
    ],
    [
     "Shingo",
     "Kuroiwa"
    ],
    [
     "Norio",
     "Higuchi"
    ]
   ],
   "title": "Determination of threshold for speaker verification using speaker adaptation gain in likelihood during training",
   "original": "i00_2326",
   "page_count": 5,
   "order": 275,
   "p1": "vol. 2, 326-329",
   "pn": "",
   "abstract": [
    "This paper describes methods to determine thresholds for speaker verification. Setting an appropriate threshold a priori is difficult because likelihood verification covers a wide range and the appropriate threshold for each speaker is different. We propose new methods to determine the speaker verification threshold depending on the \"adaptation degree\" for each speaker. We use the gain in likelihood during the adaptive training process from speaker-independent models as the \"adaptation degree\" and determine the threshold by its linear function. We evaluate the proposed methods in text-prompted speaker verification experiments using connected digit speech to show that the estimated coefficients of the linear function are relatively constant regardless of the amount of training data and that thresholds set by our proposed methods are stable and reliable. Consequently, use of our new methods improves verification performance and reduces the error rate by 30 percent.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-275"
  },
  "liu00c_icslp": {
   "authors": [
    [
     "Mingkuan",
     "Liu"
    ],
    [
     "Bo",
     "Xu"
    ]
   ],
   "title": "Accent-specific Mandarin adaptation based on pronunciation modeling technology",
   "original": "i00_2330",
   "page_count": 4,
   "order": 276,
   "p1": "vol. 2, 330-333",
   "pn": "",
   "abstract": [
    "An accent adaptation approach using pronunciation variation modeling technology for Mandarin accent was proposed in this paper. As Chinese language is monosyllabic, the syllable pronunciation variation dictionary (SPVD) was built to depict the characteristics of accent. Firstly, the pronunciation modeling technology was utilized to get the context-independent and contextdependent accent-specific syllable confusion matrix according to the acoustic recognition results (pin-yin stream). Then the accentspecific Chinese SPVD was constructed from this confusion matrix. Finally, N-Best acoustic recognition candidates were rescored with the help of SPVD. To curtail the necessary adaptation data size for context-dependent SPVD, we divided the syllable context into several groups. The experiment results show that pronunciation variation modeling technology is an effective method for Mandarin accent adaptation, and the context grouping strategy can reduce the adapting speech data effectively while keep the same satisfactory performance.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-276"
  },
  "lee00_icslp": {
   "authors": [
    [
     "Hyun Bok",
     "Lee"
    ]
   ],
   "title": "In search of paralinguistic features",
   "original": "i00_2334",
   "page_count": 7,
   "order": 277,
   "p1": "vol. 2, 334-340",
   "pn": "",
   "abstract": [
    "The aim of this paper is twofold: 1) to examine the nature, scope and function of the paralinguistic features on the basis of a survey of the previous studies, and 2) to make an attempt to search and exemplify paralinguistic features operating at various linguistic and non-linguistic units in several languages. A special emphasis will be placed on the paralinguistic functions of phonetic elements including segmental a s well as prosodic features. The term \"Paralinguistic feature\" has been reexamined and its scope broadened in this paper so that it can accommodate any phonetic features, whether intentional or not on the part of the speaker, that can perform the paralinguistic function of contributing and modifying the basic meaning of words, etc. In view of the growing importance of paralinguistic features, a new term \"Paraphonetic feature\" is suggested to include exclusive ly vocal or phonetic media operating as paralinguistic features in human communica tion. Therefore paraphonetic features include not only prosodic features such as pitch, intensity, duration, speech rhythm and intonation but also segmental elements such as vowels, consonants, voice quality and airstream mechanism involved.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-277"
  },
  "fant00_icslp": {
   "authors": [
    [
     "Gunnar",
     "Fant"
    ],
    [
     "Anita",
     "Kruckenberg"
    ]
   ],
   "title": "A prominence based model of Swedish intonation",
   "original": "i00_2341",
   "page_count": 4,
   "order": 278,
   "p1": "vol. 2, 341-344",
   "pn": "",
   "abstract": [
    "Our study is concerned with the modelling of intonation in Swedish, primarily prose reading. It is basically a superposition model with accentuation patterns added to prosodic base contours with prescribed onset, offset and declination. Boundary conditions at a juncture are related to sentence type and pause duration. There are several novel features of our approach. It is based on analysis of F0 data on a semitone scale of each syllable of a text read by five subjects of which two females. A sampling convention inspired by the canonical accent 1 and accent 2 typology of Bruce [1] is adopted. Inter-speaker variations are reduced by frequency and time normalisation, thereby bringing out common elements as well as individual global traits. The extent of F0 modulation of accent patterns are modelled as a function of the continuously scaled syllable and word prominence parameter Rs which we have introduced in earlier studies of stress and accentuation. Additional modifications with respect to the relative location of a word in a clause are introduced. Differences between speaker average and predicted data within a sentence are usually but not always small and will be systematically studied in order to derive rules for assimilation and syntactically motivated grouping. Our study contributes to detailed insights in the realisation of accent 1 and accent 2 patterns.\n",
    "",
    "",
    "Bruce, G. Swedish Word Accents in Sentence Perspective. Lund: Gleerup, 1977.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-278"
  },
  "kasuya00_icslp": {
   "authors": [
    [
     "Hideki",
     "Kasuya"
    ],
    [
     "Masanori",
     "Yoshizawa"
    ],
    [
     "Kikuo",
     "Maekawa"
    ]
   ],
   "title": "Roles of voice source dynamics as a conveyer of paralinguistic features",
   "original": "i00_2345",
   "page_count": 4,
   "order": 279,
   "p1": "vol. 2, 345-348",
   "pn": "",
   "abstract": [
    "This paper focuses on the role of frequency spectral tilt (TL) of the voicing source waveform, the glottal turbulent noise and fluctuations of the source spectrum on period-to-period basis in the cognition of paralinguistic features as in suspicious and disappointed renditions. The TL is closely related to duration of the return phase of the differentiated glottal waveform. Perceptual experiments have shown that the larger the TL, the lower the degree of suspicion and existence of the source spectral fluctuations increases that degree over the entire range of the TL. In the case of disappointment, the larger the TL, the higher the degree of disappointment and inclusion of the glottal turbulent noise emphasizes disappointment when the TL is small.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-279"
  },
  "maekawa00_icslp": {
   "authors": [
    [
     "Kikuo",
     "Maekawa"
    ],
    [
     "Takayuki",
     "Kagomiya"
    ]
   ],
   "title": "Influence of paralinguistic information on segmental articulation",
   "original": "i00_2349",
   "page_count": 4,
   "order": 280,
   "p1": "vol. 2, 349-352",
   "pn": "",
   "abstract": [
    "Influence of paralinguistic information upon articulatory gesture was examined using EMMA record of tongue, jaw and lips. The focus was upon the contrast between \"admiration\" and \"suspicion.\" It was found that previously reported contrast in F2 is related to the horizontal displacement of the tongue body and lip aperture control. These articulatory characteristics were observed not only in vowels but also in consonant portions. This suggests that the effect of paralanguage upon speech production is a manipulation of voice-quality rather than the control of individual segments.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-280"
  },
  "ohno00_icslp": {
   "authors": [
    [
     "Sumio",
     "Ohno"
    ],
    [
     "Yoshimitsu",
     "Sugiyama"
    ],
    [
     "Hiroya",
     "Fujisaki"
    ]
   ],
   "title": "Analysis and modeling of the effect of paralinguistic information upon the local speech rate",
   "original": "i00_2353",
   "page_count": 4,
   "order": 281,
   "p1": "vol. 2, 353-356",
   "pn": "",
   "abstract": [
    "The local speech rate is one of the important prosodic parameters for representing both linguistic and paralinguistic information. The present paper presents a command-response model for the process of speech rate control, and applies it to the analysis of the effects of emphasis on the local speech rate in English utterances. It shows that the local speech rate as a function of time can be approximated by the smoothed response of a second-order linear system for speech rate control to a set of underlying commands, whose timings and amplitudes serve as quantitative parameters for representing paralinguistic information. Together with the command-response model for F0 contour generation, it provides a unified approach to the quantitative analysis of paralinguistic information in prosody.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-281"
  },
  "cao00_icslp": {
   "authors": [
    [
     "Jianfen",
     "Cao"
    ]
   ],
   "title": "Rhythm of spoken Chinese - linguistic and paralinguistic evidences -",
   "original": "i00_2357",
   "page_count": 4,
   "order": 282,
   "p1": "vol. 2, 357-360",
   "pn": "",
   "abstract": [
    "This paper discusses rhythmic characteristics of Standard Chinese. The study is based on relevant acoustic-phonetic investigations, including some paralinguistic analyses. The preliminary result leads to the considerations that (1) there is no evidence to regard Chinese as a syllable-timed language; (2) speech rhythm may be more related to regular occurrence and variation of certain prosodic phenomena at particular positions, in stead of isochronal recurrence of any speech unit.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-282"
  },
  "eda00_icslp": {
   "authors": [
    [
     "Sanae",
     "Eda"
    ]
   ],
   "title": "Identification and discrimination of syntactically and pragmatically contrasting intonation patterns by native and non-native speakers of standard Japanese",
   "original": "i00_2361",
   "page_count": 4,
   "order": 283,
   "p1": "vol. 2, 361-364",
   "pn": "",
   "abstract": [
    "This paper reports the results of an empirical study which examined the discrimination and identification of prosodic variations by native and non-native speakers of Japanese. The data support the idea that recognizing prosodic cues is easier than associating the correct interpretations with them. This result is in keeping with first language acquisition studies showing that children acquire accurate production of prosodic effects well before they acquire adult like competence in pragmatic use.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-283"
  },
  "erickson00_icslp": {
   "authors": [
    [
     "Donna",
     "Erickson"
    ],
    [
     "Arthur",
     "Abramson"
    ],
    [
     "Kikuo",
     "Maekawa"
    ],
    [
     "Tokihiko",
     "Kaburagi"
    ]
   ],
   "title": "Articulatory characteristics of emotional utterances in spoken English",
   "original": "i00_2365",
   "page_count": 4,
   "order": 284,
   "p1": "vol. 2, 365-368",
   "pn": "",
   "abstract": [
    "Acoustic and articulatory properties of emotional utterances in English were examined using articulatory (EMA) recordings of speech elicited from two speakers of American English. The speakers produced 10 to 12 repetitions of the sentence \"Thats wonderful,\" using several different intonational patterns and types of paralinguistic information. Perception tests showed that listeners could perceive the emotions intended by the speakers. Furthermore, F0, formant frequencies, jaw and tongue dorsum position changed as a function of the particular emotion. Initial analysis suggests that the emotion \"anger\" may involve more jaw lowering, \"suspicion,\" a raising of the tongue, and \"admiration,\" a lowering of the tongue.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-284"
  },
  "hirose00_icslp": {
   "authors": [
    [
     "Keikichi",
     "Hirose"
    ],
    [
     "Nobuaki",
     "Minematsu"
    ],
    [
     "Hiromichi",
     "Kawanami"
    ]
   ],
   "title": "Analytical and perceptual study on the role of acoustic features in realizing emotional speech",
   "original": "i00_2369",
   "page_count": 4,
   "order": 285,
   "p1": "vol. 2, 369-372",
   "pn": "",
   "abstract": [
    "Investigation was conducted on how prosodic features of emotional speech changed depending on emotion levels. The analysis results on fundamental frequency (F0) contours and speech rates implied that humans have several ways to express emotions and use them rather randomly. Investigation was also conducted on what acoustic features were important to express emotions. Perceptual experiments using synthetic speech with copied acoustic features of target speech indicated importance of the segmental features other than the prosodic features. Especially, a high importance was observed in the case of happiness.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-285"
  },
  "mozziconacci00_icslp": {
   "authors": [
    [
     "Sylvie J. L.",
     "Mozziconacci"
    ],
    [
     "Dik J.",
     "Hermes"
    ]
   ],
   "title": "Expression of emotion and attitude through temporal speech variations",
   "original": "i00_2373",
   "page_count": 6,
   "order": 286,
   "p1": "vol. 2, 373-378",
   "pn": "",
   "abstract": [
    "The present study investigates temporal characteristics of speech conveying emotion and attitude. First, a production study was conducted at the global level of the whole utterance, determining the production values of \"global speech rate\" for each emotion and attitude involved in the study. A perception study was also carried out at this global level, seeking perceptually optimal \"global speech rate\" values for conveying each emotion and attitude. Perception and production values were compared. Second, as more local information inside utterances might be specific to particular emotions or attitudes, the study went a step further with the analysis of \"local speech rate\" relative to neutrality, considering accented and nonaccented speech ½segments separately. The perceptual relevance of variations in \"local speech rate relative to neutrality\" was then tested in a perception experiment. Both global and local variations appeared to be relevant for conveying emotions and attitudes in speech.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-286"
  },
  "scherer00_icslp": {
   "authors": [
    [
     "Klaus R.",
     "Scherer"
    ]
   ],
   "title": "A cross-cultural investigation of emotion inferences from voice and speech: implications for speech technology",
   "original": "i00_2379",
   "page_count": 4,
   "order": 287,
   "p1": "vol. 2, 379-382",
   "pn": "",
   "abstract": [
    "Recent years have seen increasing efforts to improve speech technology tools such as speaker verification, speech recognition, and speech synthesis by taking voice and speech variations due to speaker emotion or attitudes into account. Given the global marketing of speech technology products, it is of vital importance to establish whether the vocal changes produced by emotional and attitudinal factors are universal or vary over cultures and/or languages. The answer to this question determines whether similar algorithms can be used to factor out (or produce) emotional variations across all languages and cultures. This contribution describes the first large-scale effort to obtain empirical data on this issue by studying emotion recognition from voice in nine countries on three different continents.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-287"
  },
  "kang00_icslp": {
   "authors": [
    [
     "Bong-Seok",
     "Kang"
    ],
    [
     "Chul-Hee",
     "Han"
    ],
    [
     "Sang-Tae",
     "Lee"
    ],
    [
     "Dae-Hee",
     "Youn"
    ],
    [
     "Chungyong",
     "Lee"
    ]
   ],
   "title": "Speaker dependent emotion recognition using speech signals",
   "original": "i00_2383",
   "page_count": 4,
   "order": 288,
   "p1": "vol. 2, 383-386",
   "pn": "",
   "abstract": [
    "This paper examines three algorithms to recognize speakers emotion using the speech signals. Target emotions are happiness, sadness, anger, fear, boredom and neutral state. MLB (Maximum-Likelihood Bayes), NN (Nearest Neighbor) and HMM (Hidden Markov Model) algorithms are used as the pattern matching techniques. In all cases, pitch and energy are used as the features. The feature vectors for MLB and NN are composed of pitch mean, pitch standard deviation, energy mean, energy standard deviation, etc. For HMM, vectors of delta pitch with delta-delta pitch and delta energy with delta-delta energy are used. A corpus of emotional speech data was recorded and the subjective evaluation of the data was performed by 23 untrained listeners. The subjective recognition result was 56% and was compared with the classifiers recognition rates. MLB, NN, and HMM classifiers achieved recognition rates of 68.9%, 69.3%, and 89.1%, respectively, for the speaker dependent and context-independent classification.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-288"
  },
  "morais00_icslp": {
   "authors": [
    [
     "Edmilson S.",
     "Morais"
    ],
    [
     "Paul",
     "Taylor"
    ],
    [
     "Fábio",
     "Violaro"
    ]
   ],
   "title": "Concatenative text-to-speech synthesis based on prototype waveform interpolation (a time frequency approach)",
   "original": "i00_2387",
   "page_count": 4,
   "order": 289,
   "p1": "vol. 2, 387-390",
   "pn": "",
   "abstract": [
    "This paper presents some preliminary methods to apply the Time- Frequency Interpolation technique - TFI [3] to concatenative text-to-speech synthesis. The TFI technique described here is a pitch-synchronous time-frequency approach of the well known Prototype-Waveform Interpolation technique - PWI [2]. The basic concepts of representing the speech signal in the Time-Frequency Domain as well as techniques to perform Time-Scale and Pitch- Scale modifications are described. Using the flexibility of TFI technique to perform spectral smothing, a method was developed to minimize the spectral mismatch at the boundaries of the Synthesis-Units - SUs. The proposed system was evaluated using SUs (Diphones) and prosodic modifications generated by the Festival system [1]. An informal subjective test was performed, between the proposed TFI system and the standard TD-PSOLA system, highligthing the superior quality of the proposed system in comparasion with TD-PSOLA.\n",
    "s A. Black, P. Taylor, R. Caley. The Festival Speech Synthesis. Avaliable at http://www.cstr.ed.ac.uk/projects/festival.html, 4(5), Sept. 1996. B. Kleijn, K. Paliwal, eds. Speech Coding and Synthesis. Elsevier, Amsterdam, 1998. Y. Shoham. High-quality Speech Coding at 2.4 to 4.0 kbps Based on Time-Frequency Intepolation. IEEE Proc. ICASSP 93, II.167-170, April, 1993.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-289"
  },
  "wang00e_icslp": {
   "authors": [
    [
     "Ren-Hua",
     "Wang"
    ],
    [
     "Zhongke",
     "Ma"
    ],
    [
     "Wei",
     "Li"
    ],
    [
     "Donglai",
     "Zhu"
    ]
   ],
   "title": "A corpus-based Chinese speech synthesis with contextual dependent unit selection",
   "original": "i00_2391",
   "page_count": 4,
   "order": 290,
   "p1": "vol. 2, 391-394",
   "pn": "",
   "abstract": [
    "This paper describes the realization of a corpus-based Chinese speech synthesis system, including the corpus design and unit selection procedure. The system selects the synthesis unit according to context similarity between target unit and candidate unit. Neither prosody parameter prediction nor prosody feature modification is needed. The informal test shows that the synthesized speech is quite natural, and the speaking style of original speaker is preserved because units are all from the speakers utterances.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-290"
  },
  "coorman00_icslp": {
   "authors": [
    [
     "Geert",
     "Coorman"
    ],
    [
     "Justin",
     "Fackrell"
    ],
    [
     "Peter",
     "Rutten"
    ],
    [
     "Bert Van",
     "Coile"
    ]
   ],
   "title": "Segment selection in the L&h Realspeak laboratory TTS system",
   "original": "i00_2395",
   "page_count": 5,
   "order": 291,
   "p1": "vol. 2, 395-398",
   "pn": "",
   "abstract": [
    "The L&H RealSpeak Laboratory TTS (RSLab) system is a corpus based speech synthesis system comprising components that deal with linguistic processing, prosody prediction, segment selection, concatenation and modification. In this paper we focus on the segment selection process. During segment selection, the units in a large database of speech are scored with a cost according to their prosodic/phonetic mismatch with the target description of the utterance to be synthesized. This prosodic/phonetic cost is computed on the basis of a combination of symbolic and numeric features. The candidate units from the speech database are then evaluated for the ease with which they can be concatenated. A dynamic programming algorithm, using additive costs, is used to find the optimal path of candidates to represent the spoken utterance. The chosen segments are then concatenated in the time domain to yield a smooth-sounding speech signal, with natural-sounding prosody. One of the keys to the success of the segment selection component is the context dependent choice of cost functions, and the method of combining the costs from the various features. The RSLab system makes use of a family of complex cost functions that allows linguistic and perceptual knowledge to be incorporated in the segment selection process.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-291"
  },
  "lyu00_icslp": {
   "authors": [
    [
     "Ren-yuan",
     "Lyu"
    ],
    [
     "Zhen-hong",
     "Fu"
    ],
    [
     "Yuang-chin",
     "Chiang"
    ],
    [
     "Hui-mei",
     "Liu"
    ]
   ],
   "title": "A Taiwanese (min-nan) text-to-speech (TTS) system based on automatically generated synthetic units",
   "original": "i00_2399",
   "page_count": 4,
   "order": 292,
   "p1": "vol. 2, 399-402",
   "pn": "",
   "abstract": [
    "A Taiwanese (Min-nan) Text-to-Speech (TTS) system has been constructed in this paper based on automatically generated synthetic units by considering several specific phonetic and linguistic characteristics of Taiwanese. Some basic facts about Taiwanese useful in a TTS system is summarized, including the issues of tone sandhi, the writen format and the others. Three functional modules, namely a text analysis module, a prosody module, and a waveform synthesis modules is described sequentially. The synthetic units in the waveform synthesis module come from 2 sources, i.e., (1) a set of isolated-uttered tonal syllables and (2) a set of designed continuous speech corpus. A HMM-based large vocabulary Taiwanese speech recognizer is used to do the forced alignment for the speech corpus. A 85.17% segmentation consistency rate within 20 ms can be achieved.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-292"
  },
  "yamada00_icslp": {
   "authors": [
    [
     "Masayuki",
     "Yamada"
    ],
    [
     "Yasuo",
     "Okutani"
    ],
    [
     "Toshiaki",
     "Fukada"
    ],
    [
     "Takashi",
     "Aso"
    ],
    [
     "Yasuhiro",
     "Komori"
    ]
   ],
   "title": "Puretalk: a high quality Japanese text-to-speech system",
   "original": "i00_2403",
   "page_count": 4,
   "order": 293,
   "p1": "vol. 2, 403-406",
   "pn": "",
   "abstract": [
    "This paper describes a high quality Japanese text to speech (TTS) system, PureTalk. This system is similar to the conventional diphone-based TTS using PSOLA except that PureTalk employs the following novel tech- niques which enable to produce more intelligible and nat- ural-sounding speech: 1) two-stage duration modeling based on a linear regression technique, 2) F0 contour modeling using polynomial segment models, 3) sophisti- cated waveform unit selection, and 4) e\u000ecient waveform compression designed for TTS system. The result of the subjective hearing test shows that PureTalk achieves high quality under practical computation and memory requirement.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-293"
  },
  "law00_icslp": {
   "authors": [
    [
     "Ka Man",
     "Law"
    ],
    [
     "Tan",
     "Lee"
    ]
   ],
   "title": "Using cross-syllable units for Cantonese speech synthesis",
   "original": "i00_2407",
   "page_count": 4,
   "order": 294,
   "p1": "vol. 2, 407-410",
   "pn": "",
   "abstract": [
    "Monosyllables have been widely accepted as the basic units for concatenative speech synthesis of Chinese dialects. However, concatenating individual syllables is not adequate to produce highly natural synthetic speech because of the improper coupling at syllable boundaries. This paper describes a preliminary research of using cross-syllable units for Cantonese speech synthesis. The acoustic inventory contains 1725 cross-syllable units, which are excised from properly selected and recorded carrier words. TD-PSOLA is employed for prosodic modification of synthetic speech. The results of subjective listening tests reveal that the proposed use of cross-syllable units has potential in producing highly natural synthetic speech, although the currently achieved performance is only fair. Substantial improvement is anticipated with better smoothing technique for waveform concatenation and greater coverage of context-dependent variation of the acoustic units.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-294"
  },
  "black00_icslp": {
   "authors": [
    [
     "Alan W.",
     "Black"
    ],
    [
     "Kevin A.",
     "Lenzo"
    ]
   ],
   "title": "Limited domain synthesis",
   "original": "i00_2411",
   "page_count": 4,
   "order": 295,
   "p1": "vol. 2, 411-414",
   "pn": "",
   "abstract": [
    "This work presents a reliable and efficient method for building limited domain speech synthesis voices. By constructing databases close to the targeted domain of the speech application, unit selection synthesis techniques can be used to reliably give very high quality synthesis within domain. In addition to a high quality result we include the techniques and processes required to build such voices often allowing new voices in limited but quite complex domains such as dialog systems to be created in under a week. The full tools, documentation examples etc are available for free at http://festvox.org.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-295"
  },
  "nakatani00_icslp": {
   "authors": [
    [
     "Christine H.",
     "Nakatani"
    ],
    [
     "Jennifer",
     "Chu-Carroll"
    ]
   ],
   "title": "Coupling dialogue and prosody computation in spoken dialogue generation",
   "original": "i00_2415",
   "page_count": 5,
   "order": 296,
   "p1": "vol. 2, 415-418",
   "pn": "",
   "abstract": [
    "We introduce a concept-to-speech (CTS) system that generates prosodic structure compositionally, in a spoken dialogue agent architecture. Representations from the semantic interpretation, task modeling, and dialogue strategy selection components drive the computation of accentuation, pitch accent type selection, and choice of melodic contour, respectively. These principled couplings of dialogue and prosody computation extend both the theory and practice of concept-to-speech generation.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-296"
  },
  "takara00b_icslp": {
   "authors": [
    [
     "Tomio",
     "Takara"
    ],
    [
     "Kazuto",
     "Izumi"
    ],
    [
     "Keiichi",
     "Funaki"
    ]
   ],
   "title": "A study on the pitch pattern of a singing voice synthesis system based on the cepstral method",
   "original": "i00_2419",
   "page_count": 4,
   "order": 297,
   "p1": "vol. 2, 419-422",
   "pn": "",
   "abstract": [
    "We synthesize singing voice by rule based on cepstral method. Higher accuracy of analysis and synthesis is required to synthesize singing voice, comparing to rule-based speech synthesis. In this paper, we propose a method of analysis and synthesis with high accuracy. Also, we express pitch patterns minutely by curves that close to natural pitch by using this method. We apply Fujisaki model and add vibrato to the pitch pattern.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-297"
  },
  "pearson00_icslp": {
   "authors": [
    [
     "Steve",
     "Pearson"
    ],
    [
     "Roland",
     "Kuhn"
    ],
    [
     "Steven",
     "Fincke"
    ],
    [
     "Nick",
     "Kibre"
    ]
   ],
   "title": "Automatic methods for lexical stress assignment and syllabification",
   "original": "i00_2423",
   "page_count": 5,
   "order": 298,
   "p1": "vol. 2, 423-426",
   "pn": "",
   "abstract": [
    "Improvements in automatic lexical stress assignment and syllabification can increase the quality of text-to-speech synthesis as well as decrease the memory requirements for dictionaries. Several methods were evaluated. Machine-learning based methods are preferred since they easily adapt to multiple languages. For stress prediction, encouraging results were obtain by combining a decision tree approach with an algorithm that uses global (word level) statistical data derived from the training dictionary. For syllable boundary prediction, algorithms that learn syllable level statistics from the training dictionary perform very well, and can be implemented as a post-process after prediction of phoneme transcription and stress.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-298"
  },
  "goubanova00_icslp": {
   "authors": [
    [
     "Olga",
     "Goubanova"
    ],
    [
     "Paul",
     "Taylor"
    ]
   ],
   "title": "Using bayesian belief networks for model duration in text-to-speech systems",
   "original": "i00_2427",
   "page_count": 4,
   "order": 299,
   "p1": "vol. 2, 427-430",
   "pn": "",
   "abstract": [
    "The problems of database imbalance and factor interaction make modelling of segment duration in text-to-speech systems a challenging task. We therefore propose a probabilistic Bayesian belief network (BN) approach to tackle data sparsity and factor interaction problems. The belief network approach makes good estimations in cases of missed or incomplete data. Also, it captures factor interaction in a concise way of causal relationships among the nodes in a directed acyclic (DAG) graph. Furthermore, a belief network approach allows a significant reduction of the number of parameters to be estimated. In our work, we model segment duration as a hybrid Bayesian network consisting of discrete and continuous nodes; each node in the network represents a linguistic factor that affects segmental duration. The interaction between the factors is represented as conditional dependence relations in the graphical model. We contrasted the results of belief network model with those of sums of products model and classification and regression tree (CART) model. We trained and tested all three models on the same data. Our new model significantly outperforms CART: the belief network achieves a RMS error of 5 milliseconds compared with 20 ms from CART. The SoP model also produces an error of 9 ms, and hence our new model isnt any worse in terms of final performance. However, we think our model has many other advantages compared to SoP, for instance it is much easier to configure and experiment with new features. This should make it easier to adapt to new languages.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-299"
  },
  "hirschfeld00b_icslp": {
   "authors": [
    [
     "Diane",
     "Hirschfeld"
    ]
   ],
   "title": "Comparing static and dynamic features for segmental cost function calculation in concatenative speech synthesis",
   "original": "i00_2435",
   "page_count": 4,
   "order": 300,
   "p1": "vol. 2, 435-438",
   "pn": "",
   "abstract": [
    "The evaluation of spectral fit at the concatenation point is esseiltial for the generation of smooth and natural sounding speech in concatenative speech synthesis. Spectral features of a speech unit are mainly influenced by its segmental structure and its segmental context. Duration plays a key role as well. In order to concatenate units without audible signal discontinuities, a distance feature is needed for the evaluation of the concatenation quality which adequately models the named influences.\n",
    "This paper presents a new distance feature for use in the unit selection of a concatenative synthesiser - the articulation function. This feature is able to model coarticulation processes and their acoustical results adequately. The modelling power of this feature will be compared to symbol-based segmental features conventionally used in cost function evaluation.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-300"
  },
  "jain00_icslp": {
   "authors": [
    [
     "Pratibha",
     "Jain"
    ],
    [
     "Hynek",
     "Hermansky"
    ]
   ],
   "title": "Temporal patterns of critical-band spectrum for text-to-speech",
   "original": "i00_2439",
   "page_count": 3,
   "order": 301,
   "p1": "vol. 2, 439-441",
   "pn": "",
   "abstract": [
    "The means of the long temporal trajectories of loga- rithmic critical band energies in a vicinity of individ- ual phoneme show distinct patterns (TRAPs Fig 1) in each critical band for di\u000berent phonemes. These temporal patterns were successfully used in Automatic Speech Recognition [1]. By using the fact that they not only contain spectral evolution but also the average co-articulation of the phonemes, we examine to what extent they capture information about sound units by synthesizing speech from them.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-301"
  },
  "choi00c_icslp": {
   "authors": [
    [
     "Eric H. C.",
     "Choi"
    ],
    [
     "Jianming",
     "Song"
    ]
   ],
   "title": "Successive cohort selection (SCS) for text-independent speaker verification",
   "original": "i00_2442",
   "page_count": 4,
   "order": 302,
   "p1": "vol. 2, 442-445",
   "pn": "",
   "abstract": [
    "A novel cohort selection method, namely, successive cohort selection (SCS) is presented in this paper for text-independent speaker verification. The proposed method computes distance between two models directly and it selects new cohort member based on both the claimed speaker model and the existing cohort members. In addition to this new cohort selection method, we also propose a new score measure to be used in verification stage. This new score measure makes use of the absolute score of an utterance to weigh its normalized score in order to further improve the accuracy of a verification. Experimental results on the YOHO speech corpus have revealed that the proposed cohort selection method together with the new score measure reduce the equal-error-rate (EER) of a textindependent system by four times. With only a cohort size of four, the EER of the system is reduced to 0.72%.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-302"
  },
  "tran00b_icslp": {
   "authors": [
    [
     "Dat",
     "Tran"
    ],
    [
     "Michael",
     "Wagner"
    ]
   ],
   "title": "Fuzzy normalisation methods for speaker verification",
   "original": "i00_2446",
   "page_count": 4,
   "order": 303,
   "p1": "vol. 2, 446-449",
   "pn": "",
   "abstract": [
    "This paper proposes normalisation methods based on fuzzy set theory for speaker verification. A claimed speaker's score used to accept or reject this speaker is viewed as a fuzzy membership function. We propose two scores: the fuzzy entropy and fuzzy C-means membership functions. Moreover, a likelihood transformation is considered to obtain a general approach and, based on this, five more fuzzy scores are proposed. Finally, a noise clustering method is applied to the current and proposed methods, reducing the equal error rate in all cases. Experiments performed on the ANDOSL and YOHO speech corpora show better results for all proposed methods.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-303"
  },
  "gu00c_icslp": {
   "authors": [
    [
     "Yong",
     "Gu"
    ],
    [
     "Hans",
     "Jongebloed"
    ],
    [
     "Dorota",
     "Iskra"
    ],
    [
     "Els den",
     "Os"
    ],
    [
     "Lou",
     "Boves"
    ]
   ],
   "title": "Speaker verification in operational environments - monitoring for improved service operation",
   "original": "i00_2450",
   "page_count": 4,
   "order": 304,
   "p1": "vol. 2, 450-453",
   "pn": "",
   "abstract": [
    "There are very few, if any, published accounts of practical experience with Speaker Verification as a means to provide secure access to telematics services. Yet, there is no reason to expect that Speaker Verification is very different from speech recognition, for which many deployed services have shown the need for close and intensive on-line monitoring during the time when the service becomes operational. In this paper we present our experience with a monitoring scheme for Speaker Verification during the field test of a financial investment game. Many of the issues that were monitored were suggested by our experience with a semi-operational service, viz. free access to Directory Assistance for visually impaired. A newly developed enrolment procedure, that can flag potentially weak speaker models, is an essential part of the monitoring procedure.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-304"
  },
  "heck00_icslp": {
   "authors": [
    [
     "Larry P.",
     "Heck"
    ],
    [
     "Nikki",
     "Mirghafori"
    ]
   ],
   "title": "On-line unsupervised adaptation in speaker verification",
   "original": "i00_2454",
   "page_count": 4,
   "order": 305,
   "p1": "vol. 2, 454-457",
   "pn": "",
   "abstract": [
    "This paper presents a new approach to on-line unsupervised adaptation in speaker verification. The approach extends previous work by (1) improving performance on the enrollment handset- type when adapting on a different handset-type (e.g., improving performance on cellular when adapting on a landline office phone), (2) accomplishing this cross channel improvement without increasing the size of the speaker model after adaptation, (3) employing a count-based, parameter-dependent smoothing algorithm that emphasizes the use of mean parameters in the speaker models until sufficient adaptation data are present to accurately estimate variances, and (4) developing a new confidence-based adaptation update weight which minimizes the corrupting effects on the speaker models from impostor attacks. Experimental results were completed on a gender-balanced database of Japanese digits with 5222 speaker models across mixed channel conditions (landline and cellular). After adaptations on 8 separate phone calls with a single 8-digit utterance per call and a 12.5% impostor attack rate, the EER was reduced by 61% (rel.) using the new unsupervised adaptation approach. This compares favorably to the (optimal) 84% reduction in EER resulting from supervised adaptation.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-305"
  },
  "sivakumaran00_icslp": {
   "authors": [
    [
     "P.",
     "Sivakumaran"
    ],
    [
     "A. M.",
     "Ariyaeeinia"
    ],
    [
     "Jill A.",
     "Hewitt"
    ]
   ],
   "title": "Multiple sub-band systems for speaker verification",
   "original": "i00_2458",
   "page_count": 4,
   "order": 306,
   "p1": "vol. 2, 458-461",
   "pn": "",
   "abstract": [
    "It is well known that the spectral information contained in the full-band cepstral parameters within a certain range is highly useful for speaker discrimination. However, this information cannot be fully represented by using the cepstral parameters generated in a single sub-band-system. This paper focuses on methods to tackle this deficiency of the sub-band cepstrum in the context of text-dependent speaker verification. In particular, it focuses on a technique based on the collective use of the cepstral vectors genorated from a set of different sub-band systems. The paper also addresses the procedure for an effective incorporation of this technique into the hidden Markov model (HMM) framework. Furthermore, the implementation issues are discussed and details of the experimental evaluation are presented.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-306"
  },
  "liu00d_icslp": {
   "authors": [
    [
     "Xiaoxing",
     "Liu"
    ],
    [
     "Baosheng",
     "Yuan"
    ],
    [
     "Yonghong",
     "Yan"
    ]
   ],
   "title": "An orthogonal GMM based speaker verification system",
   "original": "i00_2462",
   "page_count": 4,
   "order": 307,
   "p1": "vol. 2, 462-465",
   "pn": "",
   "abstract": [
    "This paper describes a new speaker verification system based on orthogonal Gaussian mixture modeling (GMM) techniques combined with maximum a posteriori (MAP) adaptation. In most of the GMM based speaker verification systems, the variance of each component is constrained to be diagonal for its computational simplicity. However, this approximation inevitably introduces performance degradation. To reduce this deleterious effect, we combine the orthogonal transform and MAP adaptation to build our system. The transform matrix is shared according to which mixture of the Speaker Independent (SI) model the current mixture component is adapted from. This gives significant system performance improvement at the cost of introducing only a small additional computational load.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-307"
  },
  "jin00b_icslp": {
   "authors": [
    [
     "Qin",
     "Jin"
    ],
    [
     "Alex",
     "Waibel"
    ]
   ],
   "title": "A naive de-lambing method for speaker identification",
   "original": "i00_2466",
   "page_count": 4,
   "order": 308,
   "p1": "vol. 2, 466-469",
   "pn": "",
   "abstract": [
    "This paper addresses the issue of close-set text-independent speaker identification from speech samples recorded over telephone. We have known that the speaker identification performance variability can be attributed to many factors. One major factor is the inherent differences in the recognizability of different speakers. In speaker recognition systems such differences are characterized by the use of animal names for different types of speakers. In this paper we use lambs to refer to those speakers who are particularly easy to imitate in our close-set text-independent speaker identification system. That is, other speakers are much more likely to be recognized as these lamb speakers when they cannot be correctly recognized. Lambs adversely affect our close-set text-independent speaker identification performance a lot. In this paper we describe a naive de-lambing method to deal with these lamb speakers so as to improve our system performance.\n",
    "The speech data of our close-set speaker identification system is from the NIST 1999 Speaker Recognition Evaluation. Our experiments were conducted on 230 male speakers. We tried both testing from same telephone channels and sessions with training and different telephone channels and sessions with training for each speaker. Combined, the method developed in this paper result in a 15% relative improvement on the close-set 45-second training 10-second testing condition.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-308"
  },
  "reynolds00_icslp": {
   "authors": [
    [
     "Douglas A.",
     "Reynolds"
    ],
    [
     "R. Bob",
     "Dunn"
    ],
    [
     "Jack L.",
     "McLaughlin"
    ]
   ],
   "title": "The lincoln speaker recognition system: NIST eval2000",
   "original": "i00_2470",
   "page_count": 4,
   "order": 309,
   "p1": "vol. 2, 470-473",
   "pn": "",
   "abstract": [
    "This paper presents an overview of the Lincoln Laboratory systems fielded for the 20006 NIST speaker recognition evaluation (SREOO). In addition to the standard one-speaker detection tasks, this year's evaluation, as in 1999, included multi-speaker spokes dealing with detection, tracking and segmentation. The design approach for the Lincoln system in SREOO was to develop a set of core one-speaker detection and multi-speaker clustering tools that could be applied to all the tasks. This paper will describe these core systems, how they are applied to the SREO0 tasks and the results they produce. Additionally, a new channel normalization technique known as handset-dependent test-score norm (HTnorm) is introduced.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-309"
  },
  "rosenberg00_icslp": {
   "authors": [
    [
     "Aaron E.",
     "Rosenberg"
    ],
    [
     "S.",
     "Parthasarathy"
    ],
    [
     "Julia",
     "Hirschberg"
    ],
    [
     "Stephen",
     "Whittaker"
    ]
   ],
   "title": "Foldering voicemail messages by caller using text independent speaker recognition",
   "original": "i00_2474",
   "page_count": 4,
   "order": 310,
   "p1": "vol. 2, 474-478",
   "pn": "",
   "abstract": [
    "The ability to automatically scan voicemail messages for content and caller identity cues would be a useful service. This paper describes a system which automatically files voicemail messages into caller folders using text independent speaker recognition techniques. Callers are represented by Gaussian mixture models (GMM's). The speech for an incoming message is processed and scored against caller models created for a subscriber. A message whose matching score exceeds a threshold is filed in the matching caller folder; otherwise it is tagged as \"unknown\". The subscriber has the ability to listen to an \"unknown\" message and file it in the proper folder, if it exists, or create a new folder, if it does not. Such subscriber labelled messages are used to train and adapt caller models. The system has been evaluated on a database of voicemail messages collected at AT&T Labs. A set of 20 callers from this database is designated as \"ingroup\". Each of these callers has recorded at least 20 messages totalling 10 or more minutes in duration. A distinct set of 220 messages, each from a di\u000berent caller, are designated as \"outgroup\". Representative performance figures with threshold parameters set to ensure that outgroup acceptance is low compared with ingroup rejection are the following. The average ingroup message rejection rate is 11.0% and the average ingroup message confusion rate (matching the wrong caller) is 1.0%, while the average outgroup message accept rate is 2.7%.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-310"
  },
  "montacie00_icslp": {
   "authors": [
    [
     "Claude",
     "Montacié"
    ],
    [
     "Marie-José",
     "Caraty"
    ]
   ],
   "title": "Structural framework for combining speaker recognition methods",
   "original": "i00_2479",
   "page_count": 5,
   "order": 311,
   "p1": "vol. 2, 479-482",
   "pn": "",
   "abstract": [
    "The paper describes a structural framework for the design of a speaker recognition system based on multiple models. This combination is not only at the recognition level, but also at a joint training of the models. This unified training of the models uses a common structure : a decomposition tree of the set of data of normalization speakers. For the experiments, the Gaussian Mixture Model and the Auto-Regressive Vectorial Model are the two models we have selected to test the structural framework of the speaker verification scoring combination. This approach has been tested on a subset of the 30\"-NIST97 Speaker Recognition Evaluation corpus. The list of the files of this subset (i.e., normalization, training and test) can be found at http://www-apa.lip6.fr/PAROLE/ICSLP2000/.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-311"
  },
  "andrews00_icslp": {
   "authors": [
    [
     "Walter D.",
     "Andrews"
    ],
    [
     "Joseph P.",
     "Campbell"
    ],
    [
     "Douglas A.",
     "Reynolds"
    ]
   ],
   "title": "Bootstrapping for speaker recognition",
   "original": "i00_2483",
   "page_count": 4,
   "order": 312,
   "p1": "vol. 2, 483-486",
   "pn": "",
   "abstract": [
    "The technique known as bootstrapping or resampling has been used effectively in the field of statistics to obtain good estimates of statistics from only a small set of observations. In this paper we explore the use of this powerful technique to aid in improving the performance of a GMM-UBM text-independent speaker recognition system. We apply the bootstrap to the training process in the generation of speaker models for the GMM-UBM system. We also aggregate the outputs of the bootstraps multiple speaker models in our bagging system. Speaker recognition results of our bootstrap and bagging systems are presented on NIST corpora.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-312"
  },
  "zhen00_icslp": {
   "authors": [
    [
     "Bin",
     "Zhen"
    ],
    [
     "Xihong",
     "Wu"
    ],
    [
     "Zhimin",
     "Liu"
    ],
    [
     "Huisheng",
     "Chi"
    ]
   ],
   "title": "On the importance of components of the MFCC in speech and speaker recognition",
   "original": "i00_2487",
   "page_count": 4,
   "order": 313,
   "p1": "vol. 2, 487-490",
   "pn": "",
   "abstract": [
    "In this paper, we analyzed the relative importance of components of MFCC for both speech recognition and speaker recognition using DTW recognizer in various noise environments. For English digit and under the Euclidean distance definition, the experiment results show cepstral components from C2 to C16 contain the most useful speaker information, while C0 and C1 are usually harm to speaker recognition. Cepstral terms from C1 to C12 are found to contain the most useful speech information. In both tasks, the additive noise decreases the relative importance of low MFCC terms faster than that of the middle and high MFCC terms, and the decrement depends on the speech SNR. The channel distortion will deteriorate low terms more than the middle and high MFCC terms in both tasks, also.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-313"
  },
  "quatieri00_icslp": {
   "authors": [
    [
     "Thomas F.",
     "Quatieri"
    ],
    [
     "R. Bob",
     "Dunn"
    ],
    [
     "Douglas A.",
     "Reynolds"
    ]
   ],
   "title": "On the influence of rate, pitch, and spectrum on automatic speaker recognition performance",
   "original": "i00_2491",
   "page_count": 4,
   "order": 314,
   "p1": "vol. 2, 491-494",
   "pn": "",
   "abstract": [
    "In this paper we study of the influence of speech articulation rate, pitch, and spectrum on a GMVI-based automatic speaker recognition system [2]. Using the high-quality Sinusoidal transformation system [1], these factors are varied in a controlled manner and the effect on recognition performance evaluated. In general, there was found a larger loss in performance using modified speech for female than for male speakers due to greater feature dependence on spectral fine structure with increasing pitch. An important observation in this study is that certain transformations can dramatically alter the aural speaker identifiability of test data with little change in automatic recognition performance, particularly for male speakers. In addition, the influence of these rate, pitch, and spectral factors on recognition performance is important in order to understand the vulnerabilities of speaker recognition systems to speech modified for gaining false acceptance. For this purpose, we also investigate performance behavior when imposter (or target) speech alone is modified.\n",
    "s T.F. Quatieri and R.J. McAulay, \"Shape-Invariant Time-Scale and Pitch Modification of Speech\", IEEE Trans. Acoustics, Speech, and Signal Proccsstng, vol.40, no.3, pp. 497-510, March 1992 D.A. Reynolds, T.F. Quatieri, and R.B. Dunn, \"Speaker verification using adapted Gaussian mixture models\", Digital Signal Processing, Special Issue: NIST 1999 Speaker Recognition Workshop, Academic Press, vol.10, no.1-3, pp.19-41, January/April/July 2000.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-314"
  },
  "teunen00_icslp": {
   "authors": [
    [
     "Remco",
     "Teunen"
    ],
    [
     "Ben",
     "Shahshahani"
    ],
    [
     "Larry",
     "Heck"
    ]
   ],
   "title": "A model-based transformational approach to robust speaker recognition",
   "original": "i00_2495",
   "page_count": 4,
   "order": 315,
   "p1": "vol. 2, 495-498",
   "pn": "",
   "abstract": [
    "A novel statistical modeling and compensation method for robust speaker recognition is presented. The method specifically addresses the degradation in speaker verification performance due to the mismatch in channels (e.g., telephone handsets) between enrollment and testing sessions. In mismatched conditions, the new approach uses speaker-independent channel transformations to synthesize a speaker model that corresponds to the channel of the testing session. Effectively verification is always performed in matched channel conditions. Results on the 1998 NIST Speaker Recognition Evaluation corpus show that the new approach yields performance that matches the best reported results. Specifically, our approach yields similar improvements (19.9% reduction in EER compared to CMN alone) as the HNORM score-based compensation method, but with a fraction of the training time.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-315"
  },
  "millerockhuizen00_icslp": {
   "authors": [
    [
     "Amanda",
     "Miller-Ockhuizen"
    ],
    [
     "Bonny E.",
     "Sands"
    ]
   ],
   "title": "Contrastive lateral clicks and variation in click types",
   "original": "i00_2499",
   "page_count": 4,
   "order": 316,
   "p1": "vol. 2, 499-502",
   "pn": "",
   "abstract": [
    "We report on a new click type found in Mangetti Dune !Xung (M.D. !Xung) - a forward released denti-alveolar lateral click, transcribed [ ||| ]. We present acoustic data showing that ||| is distinct from the \"typical\" lateral (post-) alveolar ||, and the central (post-) alveolar !, a conclusion supported by synchronic and diachronic analyses of Northern Khoisan [1, 2].\n",
    "We use Adaptive Dispersion [3] to account for acoustic differences seen between || in !Xung and Ju|hoansi, a related language without the ||| click.\n",
    "The ! click is distinct from both ||| and || lateral clicks in rise time till peak intensity [4, 5], burst duration and peak burst frequency, while ||| and || differ primarily in terms of rise time and peak intensity.\n",
    "s\n",
    "Miller-Ockhuizen, Amanda and Bonny E. Sands. (1999). \"!Kung as a linguistic construct\". Language and Communication 19/4. June 1999. pp. 401-413.\n",
    "Sands, Bonny E. and Amanda Miller-Ockhuizen. (2000). \"Comparative Evidence for New Click Types in Northern Khoisan\". Paper presented at the 74th Annual Meeting of the Linguistics Society of America. Chicago, Illinois. Lindblom, Björn. (1990). \"Models of phonetic variation and selection.\" PERILUS (Phonetic Experimental Research, Institute of Linguistics , University of Stockholm) 11: 65-100. Ladefoged, P. and A. Traill. (1984).\"Linguistic Phonetic descriptions of clicks.\" Language 60: 1-20. Ladefoged, P. and A. Traill. (1994). \"Clicks and their accompaniments.\" Journal of Phonetics 22: 33-64.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-316"
  },
  "matsui00_icslp": {
   "authors": [
    [
     "Tomoko",
     "Matsui"
    ],
    [
     "Masaki",
     "Naito"
    ],
    [
     "Yoshinori",
     "Sagisaka"
    ],
    [
     "Kozo",
     "Okuda"
    ],
    [
     "Satoshi",
     "Nakamura"
    ]
   ],
   "title": "Analysis of acoustic models trained on a large-scale Japanese speech database",
   "original": "i00_2503",
   "page_count": 4,
   "order": 317,
   "p1": "vol. 2, 503-506",
   "pn": "",
   "abstract": [
    "This paper investigates the performance of speaker-independent (SI) acoustic hidden Markov models (HMMs) trained with a huge Japanese speech database, and discusses the e\u000eciency and task-independency involved. The database consists of read and spontaneous speech uttered by 3,771 speakers. The speech involves wide distributions with respect to region and age to capture the Japanese speech characteristics as best as possible. Recognition experiments using the spontaneous speech show that task-independent acoustic models can be created when training data with a huge number of speakers is available.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-317"
  },
  "bijankhan00_icslp": {
   "authors": [
    [
     "Mahmood",
     "Bijankhan"
    ]
   ],
   "title": "Farsi vowel compensatory lengthening: an experimental approach",
   "original": "i00_2507",
   "page_count": 4,
   "order": 318,
   "p1": "vol. 2, 507-510",
   "pn": "",
   "abstract": [
    "This paper addresses a question concerning phonological operations involved standard Farsi vowel Compensatory Lengthening (CL) due to glottal consonant deletion in syllabic coda. Specifically, I question two current views related to CL process. One view suggests that glottal allophonic weakening is compensated by vowel lengthening. Another view suggests that glottal consonant is deleted, but vacated mora is filled by preceding vowel. Based on an experimental approach, I claim that Farsi CL is a gradient process in which different magnitude of glottal gesture variation is accompanied by F0 decreasing of vowel onset and offset and lengthening of the vowel. Evidence for this claim comes from the results of two experiments. For experiment 1, ten sentences having ten CaC words were made. /a/ duration was doubled while each synthetic CaaC word supposed to be a phonetic counterpart of another CaCC lexical word. Ten other sentences were generated by such CaaC words. Ten subjects listened to these twenty sentences. Subjects lexical ambiguity to understand CaC and CaCC words was 37.8%. For experiment 2, subjects colloquially uttered those twenty sentences with CaC and CaCC words. I found that, in addition to significant vowel lengthening for colloquial CaCC , /a/ onset and offset F0 was significantly decreased, while different magnitude of glottal gesture was realized in speech signal from a weak through complete deletion of glottals. This finding can be used to improve Farsi ASR and TTS systems.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-318"
  },
  "wang00f_icslp": {
   "authors": [
    [
     "Yue",
     "Wang"
    ],
    [
     "Joan A.",
     "Sereno"
    ],
    [
     "Allard",
     "Jongman"
    ],
    [
     "Joy",
     "Hirsch"
    ]
   ],
   "title": "Cortical reorganization associated with the acquisition of Mandarin tones by american learners: an FMRI study",
   "original": "i00_2511",
   "page_count": 4,
   "order": 319,
   "p1": "vol. 2, 511-514",
   "pn": "",
   "abstract": [
    "This study employed functional magnetic resonance imaging (fMRI) to investigate cortical effects of learning Mandarin tones by native speakers of American English. Six American learners were scanned while listening to Mandarin tones. The learners then participated in a two-week Mandarin tone training program, after which they were scanned again. Regions of brain activation in the perception of tones before and after training were compared. Results showed significant cortical change in activation areas in the temporal-lobe languagesensitive regions (Wernickes area), and in the frontal-lobe language-sensitive regions (Brocas area) in the left hemisphere, as well as the homologous areas in the right hemisphere. These results are discussed in terms of changing cortical representations in language learning.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-319"
  },
  "whiteside00b_icslp": {
   "authors": [
    [
     "S. P.",
     "Whiteside"
    ],
    [
     "R. A.",
     "Varley"
    ],
    [
     "T.",
     "Phillips"
    ],
    [
     "H.",
     "Garety"
    ]
   ],
   "title": "The production of real and non-words in adult stutterers and non-stutterers: an acoustic study",
   "original": "i00_2515",
   "page_count": 4,
   "order": 320,
   "p1": "vol. 2, 515-518",
   "pn": "",
   "abstract": [
    "Dual-route models of speech production suggest that highfrequency forms are encoded in different ways from lowfrequency and novel forms [1, 2, 3]. There is some evidence to suggest that lexical status may have some influence on stuttering behaviour, with higher incidences of stuttering being observed for low frequency words [4, 5]. This paper reports on a study, which investigated the production of high and low frequency words, and non-words in two groups of young adult male speakers. The first, comprised a group of speakers with stuttering (N=4), and the second, speakers without stuttering (N=5). Three repetitions were elicited from each subject via a repetition task and recorded onto DAT. These data were subsequently digitised at a sampling rate of 20 kHz. Two parameters were measured (in milliseconds) from the digitised samples using a KAY Elemetrics Computerised Speech Lab: 1) Response latencies (time taken to respond to a stimulus); and 2) word durations (duration of utterance). We report here on the first repetition of the high/low frequency and non-words.\n",
    "The results are presented and discussed with reference to implications for the production of frequent and novel forms, motor speech learning, and how motor speech disorders such as stuttering can shed some light on speech encoding mechanisms.\n",
    "s Levelt, W. J. M. and Wheeldon, L. \"Do speakers have access to a mental syllabary?\", Cognition, 50, 1994, 239- 269. Varley, R. A. and Whiteside, S. P. \"What is the underlying impairment in acquired apraxia of speech?\", Aphasiology. In Press. Whiteside, S. P. and Varley, R. A. \"A new conceptualisation of apraxia of speech: a synthesis of evidence\", Cortex, 34, 1998, 221-231. Hubbard, CP, Prins, D. \"Word familiarity, syllabic stress pattern, and stuttering\", Journal of Speech and Hearing Research, 37, 1994, 564-571. Peters, H. F. M., Hulstijn, W. and Van Lieshout, P. H. H. M. \"Recent developments in speech motor research into stuttering\", Folia Phoniatrica et Logopaedica, 52, 2000, 103-119\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-320"
  },
  "shimizu00_icslp": {
   "authors": [
    [
     "Masaaki",
     "Shimizu"
    ],
    [
     "Masatake",
     "Dantsuji"
    ]
   ],
   "title": "A new proposal of laryngeal features for the tonal system of Vietnamese",
   "original": "i00_2519",
   "page_count": 4,
   "order": 321,
   "p1": "vol. 2, 519-522",
   "pn": "",
   "abstract": [
    "We have examined a tonal system of modern Hanoi dialect of Vietnamese. This has been described for a long time as a pitch contour prominent system with secondary phonatory variations. The present study aims to propose certain laryngeal features, [tense] and [glottal constriction], in addition to the former prosodic feature, [high], to cover the overall distinction within the system. Some acoustical and laryngographical analyses were made in order to present phonetic evidence for our proposal.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-321"
  },
  "zhang00f_icslp": {
   "authors": [
    [
     "Hong",
     "Zhang"
    ],
    [
     "Bo",
     "Xu"
    ],
    [
     "Taiyi",
     "Huang"
    ]
   ],
   "title": "How to choose training set for language modeling",
   "original": "i00_2523",
   "page_count": 4,
   "order": 322,
   "p1": "vol. 2, 523-526",
   "pn": "",
   "abstract": [
    "This paper investigates the problem of choosing the training set for language modeling in large vocabulary continuous speech recognition system. From our investigation, we find that the language style is more important than the domain in language modeling. Keeping the similarity of language style, extending of domain is not harmful. On the contrast, under this condition, the expanding size of the training set will improve the quality of the language model. Diversity of language styles in the training set will result in the degradation of the language model. The analysis of the correlation between CER and evaluation measures of language model indicates that under condition of same domain, same language style and whole model without cutoff, the perplexity correlates with CER strongly. Otherwise this correlation will be weakened. Another evaluation measure in our investigation, the Ngram hitting rate performs similarly to that of perplexity. To the back-off trigram model, the bigram hitting rate correlates stronger to CER than the trigram-hitting rate, which is meaningful to the size reduction of language model.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-322"
  },
  "cosi00_icslp": {
   "authors": [
    [
     "Piero",
     "Cosi"
    ],
    [
     "John-Paul",
     "Hosom"
    ]
   ],
   "title": "High performance \"general purpose\" phonetic recognition for Italian",
   "original": "i00_2527",
   "page_count": 4,
   "order": 323,
   "p1": "vol. 2, 527-530",
   "pn": "",
   "abstract": [
    "The development of a speaker independent \"general purpose\" phonetic recognizer for Italian is described. The CSLU Toolkit was used to develop and implement the system. The recognizer, based on a frame-based hybrid HMM/ANN architecture trained on context-dependent categories to account for coarticulatory variation, recognizes 38 different phonemes (not including silence or closures), and can distinguish between stressed and unstressed vowels as well as open and closed vowels. The APASCI corpus, containing nearly 2500 sentences read by 100 speakers, where the sentences have been designed to maximize the number of phonemes occurring in different contexts, was used for training and testing. As of the time of this writing, a phoneme-level accuracy of 82.90% on the development set and of 80.53% on the test set has been obtained. This level of accuracy is much greater than on a similar English-language corpus (with state-of-the-art performance of slightly better than 70%) and it represents the best performance obtained so far on this corpus.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-323"
  },
  "lopezdeipina00b_icslp": {
   "authors": [
    [
     "Miren Karmele",
     "López de Ipiña"
    ],
    [
     "Inés",
     "Torres"
    ],
    [
     "Lourdes",
     "Oñederra"
    ],
    [
     "Amparo",
     "Varona"
    ],
    [
     "N.",
     "Ezeiza"
    ],
    [
     "M.",
     "Peñagarikano"
    ],
    [
     "M.",
     "Hernandez"
    ],
    [
     "Luis Javier",
     "Rodriguez"
    ]
   ],
   "title": "First approach to the selection of lexical units for continuous speech recognition of Basque",
   "original": "i00_2531",
   "page_count": 4,
   "order": 324,
   "p1": "vol. 2, 531-534",
   "pn": "",
   "abstract": [
    "The selection of appropriated Lexical Units is an important issue in the Language Model (LM) generation. Word has been used classically as unit in most of the Continuous Speech Recognition systems. However, during the last years proposals of non-word units have begun to appear. Since Basque is an agglutinative language with a certain structure inside the word, the nonword units could be an adequate option. In this work, a statistical analysis of the morphological structure of Basque has been carried out. This analysis shows a slight increment of the rates of confusion in Continuous Speech Recognition Systems due to the great increment of acoustically similar and short units. Finally several proposals of Lexical Units are analyzed to deal with the problem.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-324"
  },
  "gowjr00_icslp": {
   "authors": [
    [
     "David W.",
     "Gow Jr."
    ]
   ],
   "title": "Assimilation, ambiguity, and the feature parsing problem",
   "original": "i00_2535",
   "page_count": 1,
   "order": 325,
   "p1": "vol. 2, 535-538",
   "pn": "",
   "abstract": [
    "Productive phonological processes including English coronal place assimilation appear to neutralize some lexical contrasts, and thus pose problems for spoken word recognition. The current work explores two questions: (1) can strong spontaneous assimilation create lexical ambiguity, and (2) how do listeners resolve potential lexical ambiguity. Two form priming experiments explored lexical activation created by strongly assimilated, potentially ambiguous prime items. In the first experiment listeners showed selective priming for the item corresponding to the underlying form of the probe item despite its surface similarity to another word. The second experiment replicated the first, but with post-assimilation context replaced by silence. The loss of this context lead to parallel access of items corresponding to both the underlying and apparent surface forms of the prime. It is suggested that assimilated coronals simultaneously encode coronal and non-coronal place and that listeners disambiguate conflicting place cues by associating coronality with the final segment of the assimilated item, and non-coronality with the subsequent segment.\n",
    ""
   ]
  },
  "kajarakar00_icslp": {
   "authors": [
    [
     "Sachin S.",
     "Kajarakar"
    ],
    [
     "Hynek",
     "Hermansky"
    ]
   ],
   "title": "Optimization of units for continuous-digit recognition task",
   "original": "i00_2539",
   "page_count": 4,
   "order": 326,
   "p1": "vol. 2, 539-542",
   "pn": "",
   "abstract": [
    "The choice of units, sub-word or whole-word, is generally based on the size of the vocabulary and the amount of training data. In this work, we have introduced new constraints on the units: 1) they should contain sufficient statistics of the features and 2) they should contain sufficient statistics of the vocabulary. This led to minimization of two cost functions, first based on the confusion between the features and the units and the second based on the confusion between the units and the words. We minimized first cost function by forming broad phone classes that were less confusing among themselves than the phones. The second cost function was minimized by coding the word-specific phone sequences. On the continuous digit recognition task, the broad classes performed worse than the phones. The word-specific phone sequences however significantly improved the performance over both the phones and the whole-word units. In this paper we discuss the new constraints, our specific implementation of the cost functions, and the corresponding recognition performance.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-325"
  },
  "vasilescu00_icslp": {
   "authors": [
    [
     "Ioana",
     "Vasilescu"
    ],
    [
     "Francois",
     "Pellegrino"
    ],
    [
     "Jean-Marie",
     "Hombert"
    ]
   ],
   "title": "Perceptual features for the identification of Romance languages",
   "original": "i00_2543",
   "page_count": 4,
   "order": 327,
   "p1": "vol. 2, 543-546",
   "pn": "",
   "abstract": [
    "This paper deals with perceptual identification and differentiation of five Romance languages, namely French, Italian, Spanish, Portuguese and Romanian. Previous studies have investigated human capability to identify spoken samples in unknown languages after a relatively brief exposure. Moreover, they have shown that subjects use perceptual categories and adapt foreign categories to their own categories in language differentiation. Accordingly, we conduct an analysis to determine which perceptual categories are salient in Romance languages identification and discrimination. Four different sets of listeners are tested. Each set consists in speakers of a different mother tongue (French, Romanian, Japanese and American English native speakers). Results reveal that identification scores are a function of the previous exposure of the listeners to the languages. Moreover, the strategies of discrimination among languages are mother tongue dependent and several potential features emerge that may be relevant in automatic language identification.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-326"
  },
  "behne00_icslp": {
   "authors": [
    [
     "Dawn M.",
     "Behne"
    ],
    [
     "Peter E.",
     "Czigler"
    ],
    [
     "Kirk P. H.",
     "Sullivan"
    ]
   ],
   "title": "Perception of Swedish vowel quantity: tracing late stages of development",
   "original": "i00_2547",
   "page_count": 4,
   "order": 328,
   "p1": "vol. 2, 547-550",
   "pn": "",
   "abstract": [
    "A distinction in vowel quantity is typically realized acoustically by vowel duration. Research on the perception of Swedish vowel quantity by adult native speakers supports this. It further suggests that when the duration of a vowel is relatively long (due, e.g., to inherent duration), listeners may also make use of vowel spectra to distinguish vowel quantities. The current project investigates the perceptual cues used to distinguish vowel quantities in late stages of language development. Of particular interest is whether pre-adult listeners use spectral cues to identify the quantity of vowels which have a relatively long inherent duration. Results are compared with the findings for Swedish adults and discussed in terms of the perceptual role of vowel duration and spectra as cues for vowel quantity.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-327"
  },
  "chotimongkol00_icslp": {
   "authors": [
    [
     "Ananlada",
     "Chotimongkol"
    ],
    [
     "Alan W.",
     "Black"
    ]
   ],
   "title": "Statistically trained orthographic to sound models for Thai",
   "original": "i00_2551",
   "page_count": 4,
   "order": 329,
   "p1": "vol. 2, 551-554",
   "pn": "",
   "abstract": [
    "Many languages have a non-obvious, but not unrelated, relationship between orthography and pronunciation. Traditional methods for automatic conversion from letters to phones involve hand-crafted letter-to-sound rules, but these require care and expertise to develop. This paper presents a letter-to-sound rule system for Thai, that is trained automatically from lexicons. A statistical model, decision trees, is used to predict phones from letters. Letters mappping to multi-phones are used to solve the problem of implicit vowels and final consonants propagation and pre- and post-processing techniques are used to handle the inversion of initial consonants and vowels. For tone prediction, hand-crafted rules are used instead since there is no ambiguation if the phonological composition is known. Combining the n-gram of phone model with the decision trees, we can achieve 68.76% word accuracy which is better than 65.15% word accuracy in the rule-based approach.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-328"
  },
  "fon00_icslp": {
   "authors": [
    [
     "Janice",
     "Fon"
    ],
    [
     "Keith",
     "Johnson"
    ]
   ],
   "title": "Speech timing patterning as an indicator of discourse and syntactic boundaries",
   "original": "i00_2555",
   "page_count": 4,
   "order": 330,
   "p1": "vol. 2, 555-558",
   "pn": "",
   "abstract": [
    "Although the perceptual reality of speech rhythm has not been unambiguously observed in acoustic correlates, speech has always been considered as rhythmic one way or another. This study looks at speech rhythm in spontaneous speech and its relationship to discourse and syntactic units. Two four-frame comic strips were used to elicit speech, and syllable onsets were measured from waveform/spectrogram displays. Inter-speaker variability due to different speaking styles is eliminated by zscore normalization. The monologues were transcribed and segmented into Discourse Segment Units according to a discourse structure model [1]. Syntactic units such as clauses and phrases were also identified. Results showed that isochrony is generally preserved until the pre-boundary syllable. Preboundary syllables are characterized by a significant lengthening effect, which differs by boundary types. Lengthening before discourse units was longer than that tied syntactic units such as clauses or phrases.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-329"
  },
  "arvaniti00_icslp": {
   "authors": [
    [
     "Amalia",
     "Arvaniti"
    ],
    [
     "Georgios",
     "Tserdanelis"
    ]
   ],
   "title": "On the phonetics of geminates: evidence from Cypriot Greek",
   "original": "i00_2559",
   "page_count": 4,
   "order": 331,
   "p1": "vol. 2, 559-562",
   "pn": "",
   "abstract": [
    "This paper examines the acoustic correlates of the geminate consonants of Cypriot Greek. Several measurements were obtained, including target segment duration, preceding vowel duration and quality, RMS for the geminates themselves, and voice quality differences in their production. It was found that for the segments whose duration can be most easily prolonged (i.e. /l/, nasals and fricatives) the most robust and consistent cue to gemination is length. In the case of stops, affricates and /r/, on the other hand, there are additional cues in the manner of articulation of the segments, with /r/ turning into a trill, and the stops and the affricate becoming aspirated. However, preliminary results from the other cues investigated suggest that gemination does not have a consistent effect on them either within or across speakers.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-330"
  },
  "ouden00_icslp": {
   "authors": [
    [
     "Hanny den",
     "Ouden"
    ],
    [
     "Carel van",
     "Wijk"
    ],
    [
     "Marc",
     "Swerts"
    ]
   ],
   "title": "A simple procedure to clarify the relation between text and prosody",
   "original": "i00_2563",
   "page_count": 5,
   "order": 332,
   "p1": "vol. 2, 563-566",
   "pn": "",
   "abstract": [
    "Four texts originally broadcasted on radio, were presented to 52 participants in a paper-and-pencil task: they were instructed to assign a linear structure by placing slashes between segments, and a hierarchical structure by underscoring and crossing out segments. Strength of paragraph boundary had a regular relation with pause duration and F0 maximum. Hierarchical position tended to be marked in two ways: centrality by pause duration and redundancy by F0 maximum. Speech rate was independent of text structure.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-331"
  },
  "tsukada00_icslp": {
   "authors": [
    [
     "Kimiko",
     "Tsukada"
    ]
   ],
   "title": "Effects of consonantal voicing on English diphthongs: a comparison of L1 and L2 production",
   "original": "i00_2567",
   "page_count": 5,
   "order": 333,
   "p1": "vol. 2, 567-570",
   "pn": "",
   "abstract": [
    "An acoustic comparison was made between native and non-native English diphthongs. We focused on the extent to which diphthong duration varied due to post-vocalic voicing. Talkers recorded three types of diphthongs /aI, eI, oU/ in /CVC/, /CVnC/ and /CVC./ words in which the second C contrasted in voicing. The major difference between the talker groups was found for the diphthongs in /CVC/ and /CVnC/ words for which the voicing effect was larger in native than in non-native English. Two groups also differed clearly in the duration of the nasal in /CVnC/. While native talkers lengthened both diphthongs and nasals before /d/, non-native talkers lengthened only diphthongs in this position. In /CVC./, there was a greater similarity between the two groups for both the mean and variation of diphthong duration.\n",
    "The data in this study indicate that transferring the L1 Japanese durational patterns to English would result in a positive transfer for /CVC./, but a negative one for /CVC/. L2 learners need to acquire the knowledge and skills to pronounce the diphthong with distinct production strategies according to the syllable type in order to approximate to the phonetic norms of the native English speaker.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-332"
  },
  "ward00_icslp": {
   "authors": [
    [
     "Nigel",
     "Ward"
    ]
   ],
   "title": "The challenge of non-lexical speech sounds",
   "original": "i00_2571",
   "page_count": 4,
   "order": 334,
   "p1": "vol. 2, 571-574",
   "pn": "",
   "abstract": [
    "Non-lexical speech sounds (conversational grunts), such as uh-huh, un-hn, mm, and oh, are common in English. In human dialogs these sounds are important in conversation control and for conveying attitudes. Spoken dialog systems may make use of these sounds to achieve concise, smooth, relaxed interactions. Doing so is, however, a challenge, because most algorithms used in spoken language processing were devised for words, but grunts are different from words both phonetically and semantically. For example, the phonetic inventory is different, superimposition of phonemes occurs, the set of conversational grunts is productive rather than finite, and the meanings are compositional and involve sound-symbolism.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-333"
  },
  "elimam00_icslp": {
   "authors": [
    [
     "Yousif A.",
     "El-Imam"
    ]
   ],
   "title": "A method to synthesize Arabic from short phonetic",
   "original": "i00_2575",
   "page_count": 4,
   "order": 335,
   "p1": "vol. 2, 575-578",
   "pn": "",
   "abstract": [
    "A system that uses short phonetic clusters, speech segments, or synthesis units to synthesize standard Arabic (SA) is described. The clusters are derived from the Arabic syllables. Basic and phonetic variants of the synthesis units are defined after qualitative and quantitative analyses of the language phonetics. A speech database of the synthesis units and their phonetic variations is created and the units are tested to control their segmental quality. A computer-based TTS system is developed using the method. Speech is synthesized by waveform concatenation. The intelligibility of synthesized speech is assessed by a standard intelligibility test method that is adapted to suit the Arabic phonetic characteristics.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-334"
  },
  "schramm00_icslp": {
   "authors": [
    [
     "Mauricio C.",
     "Schramm"
    ],
    [
     "Luis Felipe R.",
     "Freitas"
    ],
    [
     "Adriano",
     "Zanuz"
    ],
    [
     "Dante",
     "Barone"
    ]
   ],
   "title": "A brazilian portuguese language corpus development",
   "original": "i00_2579",
   "page_count": 4,
   "order": 336,
   "p1": "vol. 2, 579-582",
   "pn": "",
   "abstract": [
    "This article presents the techniques that are being used for the creation of a database related to the Brazilian Portuguese language. This database is composed of a collection of recorded voices, from different speakers and different regions of Brazil. The collected voices contain varied phonetic and phonologic information. The applications of this database are diverse, including synthesis and recognition systems and data for linguistic studies.\n",
    "The corpus is composed of read sentences in Brazilian Portuguese, similar to sentences found in the TIMIT corpus, as well as answers to questions such as the speakers name, address, telephone number, ZIP code, and other information. The data were recorded at 44 kHz with a direct connection from the microphone to the sound card. The corpus contains information from about 200 speakers, although future development efforts will expand the corpus size to 1000 speakers. The paper covers in some detail the protocol used to design this corpus and the methods of data collection.\n",
    "An HMM/ANN-hybrid continuous digits recognizer developed using a small subset of this corpus has 96.18% word-level accuracy and 78.95% sentence level accuracy. This recognizer was trained on 48 files, developed using 11 files, and tested on 19 files, with an average of 5 digits per file. A total of 103 context-dependent categories were used in training. A generalpurpose recognizer capable of recognizing arbitrary words is currently under development.\n",
    "This article is within the context of the Spoltech Project that is a project on computational linguistic research. It aims to create, develop and improve the technologies of speech synthesis and recognition. This interdisciplinary project is composed of researchers, teachers and students of the Instituto de Informática and Instituto de Letras (Language and Literature College) of the Universidade Federal do Rio Grande do Sul, the Departamento de Informática of the Universidade de Caxias do Sul, CSLR/CU (University of Colorado, Boulder) and CSLU/OGI (Oregon Graduate Institute).\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-335"
  },
  "colin00_icslp": {
   "authors": [
    [
     "C.",
     "Colin"
    ],
    [
     "Monique",
     "Radeau"
    ],
    [
     "Didier",
     "Demolin"
    ],
    [
     "A.",
     "Soquet"
    ]
   ],
   "title": "Visual lipreading of voicing for French stop consonants",
   "original": "i00_2583",
   "page_count": 5,
   "order": 337,
   "p1": "vol. 2, 583-586",
   "pn": "",
   "abstract": [
    "This study examined whether visually presented bilabials consonants are better identified than velars in a CV (C = consonant; V = vowel) or VCV context. We also investigated whether voiced and voiceless consonants sharing a same place and manner of articulation could be differentiated from each other with visual cues only. Although it is generally assumed that voicing is mainly mediated by the auditory modality, one cannot discard the possibility that the production of a voiced stop consonant produces a pattern of facial cues that could be detectable visually. Two pairs of stop consonants (/b/-/p/ and /g/-/k/) were articulated by a man and by a woman speaker in two syllabic contexts (CV monosyllables or VCV bisyllables). The bisyllables were uttered according to three speaking rates: slow, medial and fast. The materials were edited on a videotape and presented on a TV screen without sound. After each trial, participants had to choose between several written possibilities what they had perceived. Percentage of correct identifications reached 42% on average for the four consonants. Errors mostly consisted in voicing confusions (37%). Place of articulation confusions occurred in only 8% of the cases. Correct identifications were more numerous for bilabials than for velars but more particularly for monosyllables. Voiced consonants were better identidified than voiceless in both syllabic contexts, but especially for velars. This suggests that some voicing distinction is possible on the basis of visual cues.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-336"
  },
  "chen00g_icslp": {
   "authors": [
    [
     "Yang",
     "Chen"
    ],
    [
     "Michael",
     "Robb"
    ]
   ],
   "title": "Acoustic features of vowel production in Mandarin speakers of English",
   "original": "i00_2587",
   "page_count": 4,
   "order": 338,
   "p1": "vol. 2, 587-590",
   "pn": "",
   "abstract": [
    "English vowel productions were acoustically examined in a group of native Mandarin speakers. The first and second formant frequencies (F1 & F2) of 11 English vowels were examined in the syllable-level productions of 40 Mandarin speakers compared to 40 American English speakers. Results of the comparative acoustic analysis indicated that the Mandarin speakers differed significantly from the American English speakers in their production of several English vowels. Both male and female Mandarin speakers were also found to exhibit a compact vowel space (F1-F2) and less acoustic diversity compared to the native American English speakers. The results were discussed with regard to the impact of L1 (Mandarin) on L2 (English), the modification of articulatory pattern, and the attunement of motor skills for Mandarin speakers during L2 learning.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-337"
  },
  "belvin00b_icslp": {
   "authors": [
    [
     "Robert",
     "Belvin"
    ],
    [
     "Ron",
     "Burns"
    ],
    [
     "Cheryl",
     "Hein"
    ]
   ],
   "title": "Spoken language navigation systems for drivers",
   "original": "i00_2591",
   "page_count": 4,
   "order": 339,
   "p1": "vol. 2, 591-594",
   "pn": "",
   "abstract": [
    "We are developing and implementing driver interfaces to navigation systems. We discuss some of the design issues in developing these systems. Our implementation, based on the Galaxy- II architecture, is described in detail. We discuss both our current system under evaluation, which does not recognize street names, as well as our enhanced system, under parallel development, which does recognize street names. Results of our preliminary evaluations are presented.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-338"
  },
  "chen00h_icslp": {
   "authors": [
    [
     "Fang",
     "Chen"
    ],
    [
     "Baozong",
     "Yuan"
    ]
   ],
   "title": "An approach to intelligent Chinese dialogue system",
   "original": "i00_2595",
   "page_count": 4,
   "order": 340,
   "p1": "vol. 2, 595-598",
   "pn": "",
   "abstract": [
    "A human-machine natural interactive system, the Intelligent Chinese Dialogue System, is introduced in the paper. In the proposed system, users can ask questions and retrieve right answers in natural speech. The system consists of five modules, i.e., dialogue processor, task processor, natural language interface, user model and knowledgebase. It has the capabilities in dialogue management, table-based knowledge learning of task domain and linguistics domain, and speech generation. It can analyze users input in natural language, generate Chinese natural sentences, then to synthesize them as speech output. It is in a domain specific structure, while not designed for a specific domain. Users can edit the tables for knowledge learning, and create their own applications in different domains. The system architecture and the realization procedures are discussed in detail in the paper.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-339"
  },
  "wang00g_icslp": {
   "authors": [
    [
     "Huei-Ming",
     "Wang"
    ],
    [
     "Yi-Chung",
     "Lin"
    ]
   ],
   "title": "Goal-oriented table-driven design for dialogue manager",
   "original": "i00_2599",
   "page_count": 4,
   "order": 341,
   "p1": "vol. 2, 599-602",
   "pn": "",
   "abstract": [
    "Spoken dialogue systems have strong potentiality in becoming the main stream of next generation man/machine interface. To meet this future, spoken dialogue systems must not only be portable to support various applications, but also be scalable to provide capable dialogue to handle sophisticated services. In this paper, we proposed a goal-oriented table-driven approach to design a portable and scalable dialogue manager. In this approach, the dialogue strategy is modeled by a set of tables. Each table in the set handles a simple sub-goal and the tables are organized hierarchically according to a predefined flow to achieve a particular goal. The data sharing property of the proposed approach makes tables become reusable and, consequently, greatly relieves the efforts of scaling-up an old application or developing a new one. In addition, the mixinitiative dialogue management is pursued by incorporating both the system-initiated and the user-initiated table switching mechanisms.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-340"
  },
  "potamianos00_icslp": {
   "authors": [
    [
     "Alexandros",
     "Potamianos"
    ],
    [
     "Egbert",
     "Ammicht"
    ],
    [
     "Hong-Kwang J.",
     "Kuo"
    ]
   ],
   "title": "Dialogue management in the Bell Labs communicator system",
   "original": "i00_2603",
   "page_count": 4,
   "order": 342,
   "p1": "vol. 2, 603-606",
   "pn": "",
   "abstract": [
    "This paper describes a dialogue manager and its interaction with semantics and context tracking in a spoken dialogue system developed for general information retrieval and transaction applications. The dialogue system supports the following basic functionality: electronic form filling, database query, result navigation, attribute-value pair referencing, and value and reference resolution. General data structures and algorithms for representing and resolving ambiguity in a spoken dialogue system and a parsimonious parameterization for all application-dependent semantic and dialogue information are proposed. Dialogue management algorithms examine the semantics and dialogue state and adapt to the users needs and task necessities. These algorithms are applied to a travel reservation application developed under the auspices of the DARPA Communicator project. The proposed algorithms are application-independent and facilitate ease of developing new spoken dialogue systems by changing only the semantics encoded in the prototype tree and the domain-dependent templates used by such components as the parser and the prompt generator.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-341"
  },
  "han00_icslp": {
   "authors": [
    [
     "Jiang",
     "Han"
    ],
    [
     "Yong",
     "Wang"
    ]
   ],
   "title": "Dialogue management based on a hierarchical task structure",
   "original": "i00_2607",
   "page_count": 4,
   "order": 343,
   "p1": "vol. 2, 607-610",
   "pn": "",
   "abstract": [
    "In conventional form-based dialogue manager, the task structure of a given task domain is built on E-form or flat slot structure, which some useful structure information of the task domain has been lost. This paper describes a hierarchical task structure model that is domain independent at least for a class of task domains. Based on the task structure model, a rule based dialogue management, which has some domain independent \"conversational skill\", is presented.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-342"
  },
  "caspers00b_icslp": {
   "authors": [
    [
     "Johanneke",
     "Caspers"
    ]
   ],
   "title": "Melodic characteristics of backchannels in Dutch map task dialogues",
   "original": "i00_2611",
   "page_count": 4,
   "order": 344,
   "p1": "vol. 2, 611-614",
   "pn": "",
   "abstract": [
    "In natural conversation backchannels (short optional utterances like uh-huh or yes) are used to indicate to the current speaker that the current listener understands so far and that the speaker may continue. The question posed in the present paper is whether backchannels distinguish themselves melodically from lexically identical utterances that have a different function (e.g., the answer to a question). In a corpus of Dutch Map Task dialogues the melodic configurations realized on all backchannels and all lexically identical non-backchannels were transcribed using ToDI [1]. Comparison of the two groups of data reveal a clear tendency for backchannels to be marked by a non-prominence lending drop in pitch followed by a high boundary tone (69%), whereas the non-backchannels generally carry a pitch accent (61%).\n",
    "",
    "",
    "Gussenhoven, C., Rietveld, T. and Terken, J., \"ToDI, Transcription of Dutch Intonation\", http://lands.let.kun.nl/todi/, 1999.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-343"
  },
  "swerts00b_icslp": {
   "authors": [
    [
     "Marc",
     "Swerts"
    ],
    [
     "Diane",
     "Litman"
    ],
    [
     "Julia",
     "Hirschberg"
    ]
   ],
   "title": "Corrections in spoken dialogue systems",
   "original": "i00_2615",
   "page_count": 4,
   "order": 345,
   "p1": "vol. 2, 615-618",
   "pn": "",
   "abstract": [
    "This study analyzes user corrections of system errors in the TOOT spoken dialogue system. We find that corrections differ from noncorrections prosodically, in ways consistent with hyperarticulated speech, although many corrections are not hyperarticulated. Yet both are misrecognized more frequently than non-corrections - though no more likely to be rejected by the system. Corrections more distant from the error they correct tend to exhibit greater prosodic differences, and also to be recognized more poorly. System dialogue strategy affects users choice of correction type, suggesting that strategy-specific methods of detecting or coaching users on corrections may be useful. Strategies that produce longer tasks but fewer misrecognitions and subsequent corrections are preferred by users.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-344"
  },
  "fry00_icslp": {
   "authors": [
    [
     "John",
     "Fry"
    ]
   ],
   "title": "F0 correlates of topic and subject in spontaneous Japanese speech",
   "original": "i00_2619",
   "page_count": 4,
   "order": 346,
   "p1": "vol. 2, 619-622",
   "pn": "",
   "abstract": [
    "This paper examines F0 correlates of morphologically marked grammatical functions, in particular topic and subject, in spontaneous Japanese speech. Our data consist of F0 measurements of 7,106 nouns in the CallHome Japanese corpus of telephone conversations [4]. We find that topics exhibit higher peak F0 than subjects, contradicting information-structure accounts which predict that topics, which refer to old information, should be less prominent. However, we suggest that the style and genre of speech is an important factor in this regard.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-345"
  },
  "tomokiyo00b_icslp": {
   "authors": [
    [
     "Mutsuko",
     "Tomokiyo"
    ],
    [
     "Solange",
     "Hollard"
    ]
   ],
   "title": "Specification of communicative acts of utterances based on dialogue corpus analysis",
   "original": "i00_2623",
   "page_count": 5,
   "order": 347,
   "p1": "vol. 2, 623-627",
   "pn": "",
   "abstract": [
    "The aim of this paper is to present a specification of speaker's communicative acts of utterances to be carried out gradually by using contextual and cotexual informations.\n",
    "Communicative act is defined as a communicative goal or aim which can be expressed in language L by a distinctive set of conventional cue patterns in specified discourse contexts. In our case, the communicative acts are classified into 28 sorts. The approach based on speech act theory to discourse analysis is considered generally as an efficacious method for topic tracking, ellipsis recovery, anaphora solution, etc. in machine translation of oral dialogues or analysis/generation of utterances on man-machine systems. However, the method requires that speaker's communicative act is interpreted and determined in the process of the analysis. Here is produced an ambiguity problem ; for example, the utterance A could be analysed as either an instruct, or an action-request in automatic analysis, while it is interpreted as an instruct according to human reading in a context given.\n",
    "A: Prenez le bus 3 et descendez a l'arrêt Gustave Rivet, ... (you wanna take the bus 3 and get out at bus stop Gustave Rivet)\n",
    "Aiming at a solution of the ambiguity, we observed task-oriented dialogue corpus, to clarify elements which are presumed to realise the speaker's communicative act of the utterance.\n",
    "There are two different types of elements involved in the communicative act ; contexual elements and cotextual elements. The contexual elements contain surface cue patterns, grammatical and linguistic aspects of utterances, precedent or/and next utterance, connectors, etc.\n",
    "The cotexual elements contain the speaker and hearer, turn taking, conversation stages (open conversation, starting of main topic, close conversation), the relation between speaker's knowledge in the domain and vocabularies used in the conversation.\n",
    "So, we formalised these elements to use as disambiguation informations of the specification of the communicative acts. Finally, we make experiments on the disambiguation using the contextual and cotexual informations.\n",
    "In this paper, we pick up the ambiguity between \"Yes\" and \"Acknowledge\", \"Action-request\" and \"Instruction\", and \"Yn-question\" and \"wh-question\", and show an evaluation concerning them.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-346"
  },
  "noguchi00_icslp": {
   "authors": [
    [
     "Hiroaki",
     "Noguchi"
    ],
    [
     "Yasuhiro",
     "Katagiri"
    ],
    [
     "Yasuharu",
     "Den"
    ]
   ],
   "title": "An experimental verification of the prosodic/lexical effects on the occurrence of backchannels",
   "original": "i00_2628",
   "page_count": 4,
   "order": 348,
   "p1": "vol. 2, 628-631",
   "pn": "",
   "abstract": [
    "Japanese backchannel utterances have recently been studied to incorporate their conversational and social functions into spoken dialogue interfaces. Most of the studies employ corpus-based methodologies to elucidate conditions under which backchannel utterances occur. We propose, in this paper, an experimental method to identify backchannel-contexts through controlled manipulation of local prosody of conversational utterances by incorporating speech synthesis technologies. We found that backchannel- facilitating contexts have to be characterized both in terms of the lexical features and the local prosody, whereas backchannel- suppressing contexts can be partially identified in terms of the phrase-end prosody.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-347"
  },
  "sato00_icslp": {
   "authors": [
    [
     "Tsutomu",
     "Sato"
    ],
    [
     "John A.",
     "Maidment"
    ]
   ],
   "title": "The acoustic characteristics of Japanese identical vowel sequences in connected speech",
   "original": "i00_2632",
   "page_count": 4,
   "order": 349,
   "p1": "vol. 2, 632-635",
   "pn": "",
   "abstract": [
    "It has been pointed out that a word which contains an identical vowel sequence may not be distinguished, especially in faster speech, from a word which has a long vowel and phonologically the same mora-count. However, this indication has been made without any objective evidence and it is argued that a quantitative study is needed to investigate the acoustic characteristics of identical vowel sequence in comparison with those of a long vowel. This paper reports the results of production experiments for this purpose done by 10 native speakers of Japanese. The results will be discussed from the viewpoints of glottal stop insertion between an identical vowel sequence and durational ratio comparison. More specifically, the actual number of glottal stop insertions on citation and sentence levels will be clearly shown and the acoustic features of the glottal stop such as its duration will be investigated. In addition, the ratios of the whole token, consonant + vowel(s) and the vowel part of identical vowel sequence examples will be made clear and compared with those of long vowel examples in both citation and sentence reading forms.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-348"
  },
  "narayanan00_icslp": {
   "authors": [
    [
     "Shrikanth",
     "Narayanan"
    ],
    [
     "Giuseppe Di",
     "Fabbrizio"
    ],
    [
     "C.",
     "Kamm"
    ],
    [
     "James",
     "Hubbell"
    ],
    [
     "B.",
     "Buntschuh"
    ],
    [
     "P.",
     "Ruscitti"
    ],
    [
     "Jerry H.",
     "Wright"
    ]
   ],
   "title": "Effects of dialog initiative and multi-modal presentation strategies on large directory information access",
   "original": "i00_2636",
   "page_count": 5,
   "order": 350,
   "p1": "vol. 2, 636-639",
   "pn": "",
   "abstract": [
    "This paper compares the effects of three different dialog initiative strategies (system initiative, mixed initiative and user initiative) on system performance and user acceptance on a large directory information access task. We used a personnel directory query application that could be accessed from a voice-only (telephony) and a multi-modal (kiosk) interface. Although the user initiative condition resulted in a lower proportion of in-grammar utterances, no significant effects of dialog initiative were observed for concept accuracy, perceived task completion, ease of use or user satisfaction. Dialogs were significantly shorter with the kiosk interface than with the telephony interface, and users preferred the kiosk interface and found it easier to use.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-349"
  },
  "thompson00_icslp": {
   "authors": [
    [
     "William",
     "Thompson"
    ],
    [
     "Harry",
     "Bliss"
    ]
   ],
   "title": "A declarative framework for building compositional dialog modules",
   "original": "i00_2640",
   "page_count": 4,
   "order": 351,
   "p1": "vol. 2, 640-643",
   "pn": "",
   "abstract": [
    "Rapid development of spoken language dialog applications requires a domain-portable dialog framework and the ability to re-use dialogs which have already been created. Satisfying this goal should not come at the expense of dialog flexibility and naturalness. Current frameworks which enable dialog re-use typically do so by linking dialogs sequentially in a finite-state graph, producing a rigid dialog flow. This paper describes a framework for specifying dialogs that both enables re-use and maintains flexibility and naturalness. This is accomplished through nesting frame descriptions. The structure of a dialog is compositionally determined both by its own top-level frame description and by the frame description of the dialogs which are nested within it.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-350"
  },
  "wang00h_icslp": {
   "authors": [
    [
     "Kuansan",
     "Wang"
    ]
   ],
   "title": "A plan-based dialog system with probabilistic inferences",
   "original": "i00_2644",
   "page_count": 4,
   "order": 352,
   "p1": "vol. 2, 644-647",
   "pn": "",
   "abstract": [
    "In this paper, we present a dialog system that extends the plan-based approach with two features. Instead of Boolean inference, we include into the system the probabilistic measures from the front-end speech and language processes. As a result, rules can be activated and facts gathered based on statistical confidence measures. We also introduce the notion of entity types to classify the rules and facts. The entity types, derived from the schema of the knowledge base, assist the semantic evaluation process by indicating which rules and facts are interoperable. The semantic evaluation and dialog planning can therefore be better insulated among tasks, and be encapsulated into reusable components. To assess the feasibility and discover the areas for improvements for this framework, we launched the project DR. WHO, in which we strive to develop a speech interface for a personal information management application. We describe the current progress in the discourse area in this paper.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-351"
  },
  "komatani00_icslp": {
   "authors": [
    [
     "Kazunori",
     "Komatani"
    ],
    [
     "Tatsuya",
     "Kawahara"
    ]
   ],
   "title": "Generating effective confirmation and guidance using two-level confidence measures for dialogue systems",
   "original": "i00_2648",
   "page_count": 4,
   "order": 353,
   "p1": "vol. 2, 648",
   "pn": "",
   "abstract": [
    "We present a method to generate effective confirmation and guidance using concept-level confidence measures (CM) derived from speech recognizer output in order to handle speech recognition errors. We define two conceptlevel CM, which are on content-words and on semanticattributes, using 10-best outputs of the speech recognizer and parsing with phrase-level grammars. Content-word CM is useful for selecting plausible interpretations. Less confident interpretations are given to confirmation process, and non-confident ones are rejected. The strategy improved the interpretation accuracy by 11.5%. Moreover, the semantic-attribute CM is used to estimate users intention and generates system-initiative guidances even when successful interpretation is not obtained.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-352"
  },
  "strom00_icslp": {
   "authors": [
    [
     "Nikko",
     "Ström"
    ],
    [
     "Stephanie",
     "Seneff"
    ]
   ],
   "title": "Intelligent barge-in in conversational systems",
   "original": "i00_2652",
   "page_count": 4,
   "order": 354,
   "p1": "vol. 2, 652-655",
   "pn": "",
   "abstract": [
    "In this paper we present novel solutions to problems related to barge-in in telephony-based conversational systems. In particular we address recovery from falsely detected barge-in events and a method for signaling to the user that barge-in is disallowed at a particular dialogue state. The mechanisms and signals used to manage turn taking are similar to those in human-human conversation, which makes them easy to understand for users without explanation or prior training.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-353"
  },
  "breen00b_icslp": {
   "authors": [
    [
     "Andrew",
     "Breen"
    ],
    [
     "Barry",
     "Eggleton"
    ],
    [
     "Gavin",
     "Churcher"
    ],
    [
     "Paul",
     "Deans"
    ],
    [
     "Simon",
     "Downey"
    ]
   ],
   "title": "A system for the research into multi-modal man-machine communication within a virtual environment",
   "original": "i00_2656",
   "page_count": 4,
   "order": 355,
   "p1": "vol. 2, 656-659",
   "pn": "",
   "abstract": [
    "This paper reports on work currently under development jointly at the University of East Anglia, School of Information systems and BT. The aim of the work is to design and develop advanced demonstrators which are capable of holding natural multi-modal discourse with a human interlocutor. In addition, these demonstrators will investigate such interactions within a computer generated immersive environment in which both the computer agents and human users have a virtual presence. The paper presents a design for a distributed architecture to achieve this goal and brief introductions to many of the system components.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-354"
  },
  "brugnara00_icslp": {
   "authors": [
    [
     "Fabio",
     "Brugnara"
    ],
    [
     "Mauro",
     "Cettolo"
    ],
    [
     "Marcello",
     "Federico"
    ],
    [
     "Diego",
     "Giuliani"
    ]
   ],
   "title": "Advances in automatic transcription of Italian broadcast news",
   "original": "i00_2660",
   "page_count": 4,
   "order": 356,
   "p1": "vol. 2, 660-663",
   "pn": "",
   "abstract": [
    "This paper presents some recent improvements in automatic transcription of Italian broadcast news obtained at ITC-irst.\n",
    "A first preliminary activity was carried out in order to develop a suitable speech corpus for the Italian language. The resulting corpus, formed by recordings covering 30 hours of radio news, was exploited for developing a baseline system for transcription of broadcast news. The system performs in different stages: acoustic segmentation and classification, speaker clustering, acoustic model adaptation and speech decoding. Major recent advances allowing performance improvement concern with speech segmentation and clustering, acoustic modeling, acoustic model adaptation and the language model.\n",
    "The transcription system features a 14.3% word error rate on planned studio speech and 18.7% on the whole test set formed by recordings of radio broadcast news. When applied to a test set formed by recordings of television broadcast news, the system features 16.5% word error rate on planned studio speech and 23.2% by considering the whole test set.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-355"
  },
  "chuang00_icslp": {
   "authors": [
    [
     "Shui-Lung",
     "Chuang"
    ],
    [
     "Hsiao-Tieh",
     "Pu"
    ],
    [
     "Wen-Hsiang",
     "Lu"
    ],
    [
     "Lee-Feng",
     "Chien"
    ]
   ],
   "title": "Live thesaurus construction for interactive voice-based web search",
   "original": "i00_2664",
   "page_count": 4,
   "order": 357,
   "p1": "vol. 2, 664-667",
   "pn": "",
   "abstract": [
    "Since Web users queries are often too short, an accurate and interactive speech interface is believed very helpful, especially for WAP-based Web search engines. To provide high accurate speech recognition and effective interactive search, a rigid and live web thesaurus that contains users' search terms plus a set of relations between their associated terms is highly in demand. The purpose of this paper is intended to present a log-based approach for live thesaurus construction. Based on the live thesaurus, certain kinds of users' information behaviors could be characterized and a more effective voice-based search engine could be developed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-356"
  },
  "suzuki00_icslp": {
   "authors": [
    [
     "Yoshimi",
     "Suzuki"
    ],
    [
     "Fumiyo",
     "Fukumoto"
    ],
    [
     "Yoshihiro",
     "Sekiguchi"
    ]
   ],
   "title": "Selecting TV news stories and newswire articles related to a target article of newswire using SVM",
   "original": "i00_2668",
   "page_count": 4,
   "order": 358,
   "p1": "vol. 2, 668-671",
   "pn": "",
   "abstract": [
    "This paper describes a method for selecting TV news stories and newswire articles related to a target article of newswire by using a machine learning technique called SVM (Support Vector Machines) We used selected antecedents of overt pronouns, compound nouns in the experiments. The results of experiments showed that the use of antecedents of overt pronouns and compound nouns for SVM is effective. And SVM is more effective than term weighting methods such as word density, TF*IDF, χ2, a method based on entropy or a method based on standard deviation for event detection.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-357"
  },
  "ng00_icslp": {
   "authors": [
    [
     "Kenney",
     "Ng"
    ]
   ],
   "title": "Towards an integrated approach for spoken document retrieval",
   "original": "i00_2672",
   "page_count": 4,
   "order": 359,
   "p1": "vol. 2, 672-675",
   "pn": "",
   "abstract": [
    "This paper presents a novel approach to spoken document retrieval where the speech recognition and information retrieval components are more tightly integrated. This is done by developing new recognizer and retrieval models where the interface between the two components is better matched and the component goals are consistent with the overall goal of the combined system. Experiments on radio news data show that the integrated approach can improve performance by 28% over a baseline system.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-358"
  },
  "logan00_icslp": {
   "authors": [
    [
     "Beth",
     "Logan"
    ],
    [
     "Pedro",
     "Moreno"
    ],
    [
     "Jean-Manuel van",
     "Thong"
    ],
    [
     "Ed",
     "Whittaker"
    ]
   ],
   "title": "An experimental study of an audio indexing system for the web",
   "original": "i00_2676",
   "page_count": 4,
   "order": 360,
   "p1": "vol. 2, 676-679",
   "pn": "",
   "abstract": [
    "We have developed a speech recognition based audio search engine for indexing spoken documents found on the World Wide Web. Our site (http://www.compaq.com/speechbot) indexes around 20 news and talk radio shows covering a wide range of topics, speaking styles and acoustic conditions from a selection of public Web sites with multimedia archives. In this paper, we describe our system and its performance, focusing on the speech recognition and retrieval aspects. We describe our training procedure in some detail and report our historical error rate since the site launch. We also investigate the impact of Out Of Vocabulary (OOV) words. Finally we report the results of retrieval experiments which demonstrate that our system can index effectively.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-359"
  },
  "jin00c_icslp": {
   "authors": [
    [
     "Rong",
     "Jin"
    ],
    [
     "Alex G.",
     "Hauptmann"
    ]
   ],
   "title": "Title generation for spoken broadcast news using a training corpus",
   "original": "i00_2680",
   "page_count": 5,
   "order": 361,
   "p1": "vol. 2, 680-683",
   "pn": "",
   "abstract": [
    "The problem of title generation involves finding the essence of a document and expressing it in only a few words. The results of a query to the Informedia Digital Video Library are summarized through an automatically generated title for each retrieved news story. When the document is errorful, as with speech-recognized broadcast news stories, the title creation challenge becomes even greater. We implemented a set of title word selection strategies and evaluated them on an independent test corpus of 579 broadcast news documents, comparing manual transcription results to automatically recognized speech using the CMU Sphinx speech recognition system with a 64000-word broadcast news language model. Using a training collection of 21190 transcribed broadcast news stories, we trained several systems to produce appropriate title words, i.e. Naïve Bayesian approach with full vocabulary, Naïve Bayesian approach with limited vocabulary, nearest neighbor approach and extractive approach. The F1 results shows that the nearest neighbor approach is a quick and easy way of generating good titles for speech recognized documents (F1 = 15.2%), while a Nave Bayesian approach with limited vocabulary also does well on our F1 measure (F1 = 21.6%), which ignores word order in the titles. Overall, the results show that title generation for speech recognized news documents is possible at a level approaching the accuracy of titles generated for perfect text transcriptions. One surprising phenomenon is that extractive approach performances slightly better for speech recognized documents than for manual transcripts.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-360"
  },
  "weber00_icslp": {
   "authors": [
    [
     "Manfred",
     "Weber"
    ],
    [
     "Thomas",
     "Kemp"
    ]
   ],
   "title": "Evaluating different information retrieval algorithms on real-world data",
   "original": "i00_2684",
   "page_count": 4,
   "order": 362,
   "p1": "vol. 2, 684-687",
   "pn": "",
   "abstract": [
    "More and more data is produced in the form of videos, which are opaque to textual queries. To allow searching in video data collections, two problems have to be solved: The automatic generation of a searchable index, and the effective search in the automatically produced and therefore imperfect index. The ISL View4You system is a prototype of a video indexing and retrieval system which both generates the index and provides a search engine to access it. An end to end evaluation was carried out using real-world data and queries from naive subjects. From the results it can be concluded, errors of the overall system are not due to the index generation, but are introduced by the information retrieval engine (the search). Therefore, the focus of this paper is a comparison of two di\u000berent search algorithms, LSI (latent semantic indexing) and Okapi (a flavor of the traditional classic vector model approach). The evaluation is carried out on the automatically produced index on a relatively small database, which allows for full manual relevance judgement.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-361"
  },
  "koumpis00_icslp": {
   "authors": [
    [
     "Konstantinos",
     "Koumpis"
    ],
    [
     "Steve",
     "Renals"
    ]
   ],
   "title": "Transcription and summarization of voicemail speech",
   "original": "i00_2688",
   "page_count": 4,
   "order": 363,
   "p1": "vol. 2, 688-691",
   "pn": "",
   "abstract": [
    "This paper describes the development of a system to transcribe and summarize voicemail messages. The results of the research we present are two-fold. First, a hybrid connectionist approach to the Voicemail transcription task shows that competitive performance can be achieved using a context-independent system with fewer parameters than those based on mixtures of Gaussian likelihoods. Second, an effective and robust combination of statistical with prior knowledge sources for term weighting is used to extract information from the decoders output in order to deliver summaries to the message recipients via a GSM Short Message Service (SMS) gateway.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-362"
  },
  "tsai00b_icslp": {
   "authors": [
    [
     "W. C.",
     "Tsai"
    ],
    [
     "Y. C.",
     "Chu"
    ]
   ],
   "title": "Robust rejection for embedded systems",
   "original": "i00_2692",
   "page_count": 5,
   "order": 364,
   "p1": "vol. 2, 692-695",
   "pn": "",
   "abstract": [
    "This paper compares three low cost rejection-modeling approaches and their combination under the framework of a voice control task. Whole word acoustic models were created with in-vocabulary training data. Rejection parameters were adjusted with validation data to find optimal rejection performance for out-of-vocabulary commands within a 3% error rate limit for in-vocabulary commands. The rejection performance was then measured with in-vocabulary and out-of-vocabulary test data. The total error rate reduces from the baseline 69.4% to 10.2% for the combined approach. The impact of rejection parameters on validation data can be generalized to test data and rejection performance remains stable when only part of the vocabulary is activated during testing, indicating robustness of these approaches across different applications.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-363"
  },
  "oviatt00_icslp": {
   "authors": [
    [
     "Sharon",
     "Oviatt"
    ]
   ],
   "title": "Multimodal signal processing in naturalistic noisy environments",
   "original": "i00_2696",
   "page_count": 4,
   "order": 365,
   "p1": "vol. 2, 696-699",
   "pn": "",
   "abstract": [
    "When a system must process spoken language in natural environments that involve different types and levels of noise, the problem of supporting robust recognition is a very difficult one. In the present studies, over 2,600 multimodal utterances were collected during both mobile and stationary use of a multimodal pen/voice system. The results confirmed that multimodal signal processing supports significantly improved robustness over spoken language processing alone, with the largest improvement during mobile use. The multimodal architecture decreased the spoken language error rate by 19-35%. In addition, data collected on a command-by-command basis while users were mobile emphasized the adverse impact of users Lombard adaptation on system processing, even when a noise-canceling microphone was used. Implications of these findings are discussed for improving the reliability and stability of spoken language processing in mobile environments.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-364"
  },
  "chai00_icslp": {
   "authors": [
    [
     "Joyce",
     "Chai"
    ],
    [
     "Sylvie",
     "Levesque"
    ],
    [
     "Margorzata",
     "Budzikowska"
    ],
    [
     "Veronika",
     "Horvath"
    ],
    [
     "Nanda",
     "Kambhatla"
    ],
    [
     "Nicolas",
     "Nicolov"
    ],
    [
     "Wlodek",
     "Zadrozny"
    ]
   ],
   "title": "A multi-modal dialog system for business transactions",
   "original": "i00_2700",
   "page_count": 4,
   "order": 366,
   "p1": "vol. 2, 700-703",
   "pn": "",
   "abstract": [
    "This paper presents a general framework for conversational agents for business applications that supports multi-channel, multi-modal interactions through the use of a channel and modality independent Dialog Move Markup Language. In particular, we describe a prototype system as an instantiation of a general dialog architecture that supports web-based interaction through a combination of modalities such as natural language dialog and user interface components. User studies have revealed that the prototype system has enhanced user experience in an online shopping environment by significantly reducing the length of interactions in terms of time and the number of clicks. Furthermore, the success in extending the general architecture to a prototype system demonstrates the applicability and potentiality of such framework in business applications.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-365"
  },
  "han00b_icslp": {
   "authors": [
    [
     "Jiang",
     "Han"
    ],
    [
     "Yonghong",
     "Yan"
    ],
    [
     "Zhiwei",
     "Lin"
    ],
    [
     "Yong",
     "Wang"
    ],
    [
     "Jian",
     "Liu"
    ],
    [
     "Danjun",
     "Liu"
    ],
    [
     "Zhihui",
     "Wang"
    ]
   ],
   "title": "Office message center - a spoken dialogue system",
   "original": "i00_2704",
   "page_count": 3,
   "order": 367,
   "p1": "vol. 2, 704-706",
   "pn": "",
   "abstract": [
    "This paper describes the experience gained from the structuring of a spoken dialogue system and its key components during the design and development of a telephony based office message center, it integrates auto-attendant, email accessing, meeting scheduling capabilities through spoken dialogue interface. A building block dialogue toolkit has been designed based on these experiences and efficiently applied to the office message center with the capabilities of automatic call routine, meeting scheduling and email access. This paper also introduces some of the key components constructed for the spoken dialogue system: ASR engine and robust semantic parser, dialogue management module with consideration of domain portability, configurable template based sentence generation, and text to speech strategy.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-366"
  },
  "miyazaki00_icslp": {
   "authors": [
    [
     "Noboru",
     "Miyazaki"
    ],
    [
     "Jun-ichi",
     "Hirasawa"
    ],
    [
     "Mikio",
     "Nakano"
    ],
    [
     "Kiyoaki",
     "Aikawa"
    ]
   ],
   "title": "A new method for understanding sequences of utterances by multiple speakers",
   "original": "i00_2707",
   "page_count": 4,
   "order": 368,
   "p1": "vol. 2, 707-710",
   "pn": "",
   "abstract": [
    "This paper presents a new method for understanding utterances in spoken dialogue. The method is characterized by an understanding rule that accepts an utterance sequence in which each utterance has its own speaker identifier. This new idea enables the speech understanding system to understand a dialogue composed of sequences of short phrases each possibly uttered by different speakers. The method is also applied to a spoken dialogue system by regarding the dialogue system itself as one of the speakers. We give a formal description of the new dialogue understanding method. Sample behavior of a spoken dialogue system with our new method is also presentend followed by an evaluation in terms of the amount of dialogue description. The evaluation shows our method reduces the amount of understanding state description.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-367"
  },
  "kikuchi00_icslp": {
   "authors": [
    [
     "Hideaki",
     "Kikuchi"
    ],
    [
     "Katsuhiko",
     "Shirai"
    ]
   ],
   "title": "Improvement of dialogue efficiency by dialogue control model according to performance of processes",
   "original": "i00_2711",
   "page_count": 4,
   "order": 369,
   "p1": "vol. 2, 711-714",
   "pn": "",
   "abstract": [
    "In this research, we aim at the establishment of a method of controlling dialogues in accordance with the changes of performance in computation. So that the system takes the most suitable dialogue strategy according to performance in computation, it needs to calculate evaluation functions modeled with performance in computation in the inside all time. In this paper, we clarify dialogue efficiency is the most important evaluation function in controlling task-oriented dialogues, and model dialogue control centered on that evaluation function with computation time of systems processes. By the users dialogue experiment, we con- firmed that the proposed model is effective for improvement of dialogue efficiency.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-368"
  },
  "wang00i_icslp": {
   "authors": [
    [
     "C.",
     "Wang"
    ],
    [
     "D. Scott",
     "Cyphers"
    ],
    [
     "Xiaolong",
     "Mou"
    ],
    [
     "Joseph",
     "Polifroni"
    ],
    [
     "Stephanie",
     "Seneff"
    ],
    [
     "J.",
     "Yi"
    ],
    [
     "Victor",
     "Zue"
    ]
   ],
   "title": "MUXING: a telephone-access Mandarin conversational system",
   "original": "i00_2715",
   "page_count": 4,
   "order": 370,
   "p1": "vol. 2, 715-718",
   "pn": "",
   "abstract": [
    "MUXING is a telephone-based conversational system that allows users to access weather information in Mandarin Chinese over the telephone. Although MUXING utilizes the same architecture as well as most of the same human language technology components as its English predecessor, JUPITER, some modifi- cations to the system were necessary to account for differences between English andMandarin Chinese. In addition, the weather database needed to be modified to reflect regions of greater interest to potential Chinese users. This paper describes our system development effort, paying particular attention toMandarinspecific changes to the original JUPITER system.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-369"
  },
  "turunen00_icslp": {
   "authors": [
    [
     "Markku",
     "Turunen"
    ],
    [
     "Jaakko",
     "Hakulinen"
    ]
   ],
   "title": "Jaspis - a framework for multilingual adaptive speech applications",
   "original": "i00_2719",
   "page_count": 4,
   "order": 371,
   "p1": "vol. 2, 719-722",
   "pn": "",
   "abstract": [
    "We introduce Jaspis, an open framework for adaptive speech applications. Jaspis is designed to support distributed, highly context-sensitive applications that adapt to a user and an environment. Jaspis is especially designed for multilingual applications. In this paper we introduce the fundamental principles of the Jaspis framework, including shared information management, evaluator-based agent frameworks (for dialogue management and speech outputs) and input agent based communication management. A multilingual speech e-mail client, Postimies (Mailman), is described as an example application. Jaspis is written in Java and will soon be freely available for researchers and application developers.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-370"
  },
  "pellom00_icslp": {
   "authors": [
    [
     "Bryan",
     "Pellom"
    ],
    [
     "Wayne",
     "Ward"
    ],
    [
     "Sameer",
     "Pradhan"
    ]
   ],
   "title": "The CU communicator: an architecture for dialogue systems",
   "original": "i00_2723",
   "page_count": 5,
   "order": 372,
   "p1": "vol. 2, 723-726",
   "pn": "",
   "abstract": [
    "This paper presents our recent work towards development of the University of Colorado (CU) Communicator, an interactive dialogue system for airline, hotel and rental car information. The CU Communicator integrates speech recognition, synthesis and natural language understanding technologies using the DARPA Hub Architecture to allow users to converse with an automated travel agent. During a typical telephonebased interaction, users can retrieve up-to-date travel information such as flight schedules, pricing, along with hotel and rental car availability. The CU Communicator has been under development since April of 1999 and represents our test-bed system for developing robust human-computer interactions where reusability and dialogue system portability serve as two main goals of our work. This paper will focus on describing our recent system improvements and will present an analysis of dialogues received and lessons learned.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-371"
  },
  "bilici00_icslp": {
   "authors": [
    [
     "Vildan",
     "Bilici"
    ],
    [
     "Emiel",
     "Krahmer"
    ],
    [
     "Saskia te",
     "Riele"
    ],
    [
     "Raymond",
     "Veldhuis"
    ]
   ],
   "title": "Preferred modalities in dialogue systems",
   "original": "i00_2727",
   "page_count": 4,
   "order": 373,
   "p1": "vol. 2, 727-730",
   "pn": "",
   "abstract": [
    "This research describes which modalities are preferred in particular contexts when interacting with a multi-modal dialogue system. The trade-off between three factors is investigated: (i) speech recognition performance, (ii) efficiency of input modality and (iii) the systems output modality. Four versions were developed of a multimodal examinator to be used in elementary school. The versions differed in recognition performance (perfect vs. realistic) and output modality (speech or text). In all systems, subjects could provide input via speaking or typing. Answer length in characters was used as a measure of efficiency. Results show that both speech recognition performance and efficiency have a strong impact on preferred modalities. No effect was found of the systems output modality.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-372"
  },
  "bechet00_icslp": {
   "authors": [
    [
     "Fréderic",
     "Béchet"
    ],
    [
     "Elisabeth den",
     "Os"
    ],
    [
     "Lou",
     "Boves"
    ],
    [
     "Jürgen",
     "Sienel"
    ]
   ],
   "title": "Introduction to the IST-HLT project speech-driven multimodal automatic directory assistance (SMADA)",
   "original": "i00_2731",
   "page_count": 4,
   "order": 374,
   "p1": "vol. 2, 731-734",
   "pn": "",
   "abstract": [
    "This paper introduces the IST-HLT project SMADA. This project started in January 2000 and it will run for three years. We present information on the topics to be addressed in the project; in addition some preliminary results are given. Information is given on functional specifications and technical and Human Factors evaluations of Automatic Directory Assistance systems in four European countries, on the technology issues taken into account (e.g., automatic derivation of pronunciation variants and grammars, combination of multiple decoders, confidence measures), on multimodal access to Directory Assistance and in relation to this on Distributed Speech Recognition.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-373"
  },
  "mao00_icslp": {
   "authors": [
    [
     "Crusoe",
     "Mao"
    ],
    [
     "Tony",
     "Tuo"
    ],
    [
     "Danjun",
     "Liu"
    ]
   ],
   "title": "Using HPSG to represent multi-modal grammar in multi-modal dialogue",
   "original": "i00_2735",
   "page_count": 3,
   "order": 375,
   "p1": "vol. 2, 735-738",
   "pn": "",
   "abstract": [
    "In order to realize their full potential, multimodal systems need to support not just synchronized integration of multiple input modalities, but also a consistent easy-of-using interface to isolate integration strategies from application ad hoc manner. As the range of multi-modal utterances supported is extended, type of input modalities are increasing, utterances being supported from individual modalities are turning to more complicated, it becomes essential to provide a well-understood and generally applicable common meaning representation for multi-modal utterances. This paper presents a fully formalized declarative statement of multi-modal grammar, the expression we use for the grammar representation draws on unification-based approaches to syntax and semantics, such as head-driven phrase structure grammar (HPSG). The works presented here show that our approach supports parsing and interpretation of natural human input distributed across the spatial, temporal, and acoustic dimensions. Integration strategies are stated in a high level HPSG based representation supporting rapid prototyping and iterative development of multi-modal systems.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-374"
  },
  "dohsaka00_icslp": {
   "authors": [
    [
     "Kohji",
     "Dohsaka"
    ],
    [
     "Norihito",
     "Yasuda"
    ],
    [
     "Noboru",
     "Miyazaki"
    ],
    [
     "Mikio",
     "Nakano"
    ],
    [
     "Kiyoaki",
     "Aikawa"
    ]
   ],
   "title": "An efficient dialogue control method under system²s limited knowledge",
   "original": "i00_2739",
   "page_count": 4,
   "order": 376,
   "p1": "vol. 2, 739-742",
   "pn": "",
   "abstract": [
    "This paper presents a novel method that controls a dialogue between a spoken dialogue system and a user efficiently so that the system responds as helpfully as possible within the limits of its knowledge. Due to speech recognition errors, a system and user must engage in a \"confirmation dialogue\" to clarify a users request. Although a confirmation dialogue is unavoidable, it should be as concise as possible. Previous methods do not sufficiently allow for the effect of the limits of the systems knowledge on the efficiency of dialogue. The result is unnecessarily long dialogues to confirm a users request minutely even if the request is beyond the systems knowledge. This paper describes a method that controls a dialogue efficiently so as to avoid an unnecessary confirmation dialogue and presents a computational efficiency criterion for dialogue control within the limits of the systems knowledge.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-375"
  },
  "cheng00_icslp": {
   "authors": [
    [
     "Ying",
     "Cheng"
    ],
    [
     "Anurag",
     "Gupta"
    ],
    [
     "Raymond",
     "Lee"
    ]
   ],
   "title": "A distributed spoken user interface based on open agent architecture (OAA)",
   "original": "i00_2743",
   "page_count": 5,
   "order": 377,
   "p1": "vol. 2, 743-746",
   "pn": "",
   "abstract": [
    "The paper describes a spoken user interface that can be distributed over a network and that allows the user to have voice access to various functions such as user authentication, command controls in web browser and personal or public information retrieval. Integration of the user interface system is accomplished by using the Open Agent Architecture. A detailed description of the distributed speech recognizer used in the interface is also provided.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-376"
  },
  "chu00b_icslp": {
   "authors": [
    [
     "Stephen M.",
     "Chu"
    ],
    [
     "Thomas S.",
     "Huang"
    ]
   ],
   "title": "Bimodal speech recognition using coupled hidden Markov models",
   "original": "i00_2747",
   "page_count": 4,
   "order": 378,
   "p1": "vol. 2, 747-750",
   "pn": "",
   "abstract": [
    "In this paper we present a bimodal speech recognition system in which the audio and visual modalities are modeled and integrated using coupled hidden Markov models (CHMMs). CHMMs are probabilistic inference graphs that have hidden Markov models as sub-graphs. Chains in the corresponding inference graph are coupled through matrices of conditional probabilities modeling temporal influences between their hidden state variables. The coupling probabilities are both cross chain and cross time. The later is essential for allowing temporal influences between chains, which is important in modeling bimodal speech. Our bimodal speech recognition system employs a two-chain CHMM, with one chain being associated with the acoustic observations, the other with the visual features. A deterministic approximation for maximum a posteriori (MAP) estimation is used to enable fast classification and parameter estimation. We evaluated the system on a speaker independent connected-digit task. Comparing with an acoustic-only ASR system trained using only the audio channel of the same database, the bimodal system consistently demonstrates improved noise robustness at all SNRs. We further compare the CHMM system reported in this paper with our earlier bimodal speech recognition system in which the two modalities are fused by concatenating the audio and visual features. The recognition results clearly show the advantages of the CHMM framework in the context of bimodal speech recognition.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-377"
  },
  "ma00b_icslp": {
   "authors": [
    [
     "Jiyong",
     "Ma"
    ],
    [
     "Wen",
     "Gao"
    ]
   ],
   "title": "A parallel multi-stream model for sign language recognition",
   "original": "i00_2751",
   "page_count": 4,
   "order": 379,
   "p1": "vol. 2, 751-754",
   "pn": "",
   "abstract": [
    "In this paper, the sub-units in each stream are used and embedded in the multi-stream model. In this framework, sign language recognition system was implemented and evaluated. Experiments were carried out for 5177 Chinese signs. The real time isolated recognition rate is 95.1%. For continuous sign recognition, the word correct rate is 91.8%. This has shown that parallel multi-stream model is powerful for sign language recognition considering the problem of scalability.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-378"
  },
  "reveret00_icslp": {
   "authors": [
    [
     "Lionel",
     "Revéret"
    ],
    [
     "Gérard",
     "Bailly"
    ],
    [
     "Pierre",
     "Badin"
    ]
   ],
   "title": "MOTHER: a new generation of talking heads providing a flexible articulatory control for video-realistic speech animation",
   "original": "i00_2755",
   "page_count": 4,
   "order": 380,
   "p1": "vol. 2, 755-758",
   "pn": "",
   "abstract": [
    "This article presents the first version of a talking head, called MOTHER (MOrphable Talking Head for Enhanced Reality), based on an articulatory model describing the degrees-offreedom of visible (lips, cheeks ...) but also partially or indirectly visible (jaw, tongue ...) speech articulators. Skin details are rendered using texture mapping/blending techniques. We illustrate here the flexibility of such an articulatory control of video-realistic speaking faces by first demonstrating its ability in tracking facial movements by an optical-to-articulatory inversion using an analysis-by-synthesis technique. The stability and reliability of the results allow the automatic inversion of large video sequences. Inversion results are here used to build automatically a coarticulation model for the generation of facial movements from text. It improves the previous Text-To- AudioVisual-Speech (TTAVS) synthesizer developed at the ICP both in terms of the accuracy and realism.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-379"
  },
  "minnis00_icslp": {
   "authors": [
    [
     "Steve",
     "Minnis"
    ],
    [
     "Andrew",
     "Breen"
    ]
   ],
   "title": "Modeling visual coarticulation in synthetic talking heads using a lip motion unit inventory with concatenative synthesis",
   "original": "i00_2759",
   "page_count": 4,
   "order": 381,
   "p1": "vol. 2, 759-762",
   "pn": "",
   "abstract": [
    "The shape and synchronization of the lip movement with speech seems to be one of the important factors in the acceptability of a synthetic persona, particularly as synthetic beings approach human photo-realism. Most of us cannot lipread nor easily identify a sound by lip-shape alone, but we can readily detect whether the lip movements of a synthetic talking head are acceptable or not. This is true even when the viewer/listener is a considerable distance from the speaker. In addition, experiments have shown that visible synthetic speech is important in augmenting audible synthetic speech, in terms of ease of understandability and recognition accuracy. This is particularly true in noisy conditions where the audio signal is degraded [1]. Synthesizing the right lip movements for talking heads is therefore an important task in achieving a high degree of naturalness, as well as for potential applications where they provide assistance to hearing impaired individuals.\n",
    "One of the major challenges, in speech synthesis as well as lip-motion synthesis is in the modelling of coarticulation. Coarticulation is the influence on the articulation of a speech segment of the preceding (backward/retentive coarticulation) and following speech segments (forward/anticipatory coarticulation). Coarticulation effects in speech have been shown to effect speech sounds up to 6 segments away [2].\n",
    "Various techniques have been used to model visual coarticulation, all of which make assumptions about the degree of forward and backward influences and the way in which these are modeled - from simple additive influences to complex mathematical models. Usually these models are physiologically grounded; for example the speed at which mouth shape muscles can react may be one important factor. However, rule based models are by their very nature complex, since the physiology of the visible articulation musculature is also complex.\n",
    "Rather than explicitly modelling this face physiology, we present a data-driven method where the dynamics of the facial musculature is captured in synchronization with the acoustic data. This approach is an improvement on other data-driven techniques [3,4] as it allows us to model visual coarticulatory effects as an extension of a concatenative speech synthesis unit selection process. Concatenative synthesis relies on the ability to extract appropriate contextual (hence capturing coarticulatory effects) N-phone units of speech which are then concatenated and deformed based on linguistic criteria - for example if stress or appropriate pitch change and duration changes are required for intonation. Our hypothesis is that these linguistic criteria are also applicable to the visual lipsynthesis in a similar way. This paper investigates how the visual unit selection process is realized.\n",
    "s Cohen, M. M. and Massaro, D. W. Modeling Coarticulation in Synthetic Visual Speech, in Thalmann, N.M and Thalmann, D. (Eds.) Models and Techniques in Computer Animation, pp. 131-156, Tokyo-Springer, 1993. Kent, R.D. and Minifie, F.D. Coarticulation in Recent Speech production Models, Journal of Phonetics, 5, 115-133, 1977. Breen, A. P., Bowers, E., Welsh, W. An Investigation into the Generation of Mouth Shapes for a Talking Head, ICSLP '96, October, 1996. Breen, A.P, Gloaguen, O., Stern, P., A Fast Method of Producing Talking Head Mouth Shapes from Real Speech., Proc. ICSLP '98, November, 1998.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-380"
  },
  "wu00c_icslp": {
   "authors": [
    [
     "Hua",
     "Wu"
    ],
    [
     "Taiyi",
     "Huang"
    ],
    [
     "Bo",
     "Xu"
    ]
   ],
   "title": "A generation system for Chinese texts",
   "original": "i00_2763",
   "page_count": 4,
   "order": 382,
   "p1": "vol. 2, 763-767",
   "pn": "",
   "abstract": [
    "A domain-independent, reusable, general text generation system for Chinese is presented in this paper. This system combines the template method and generation technology in a single formalism, which enables the system to maintain both flexibility and efficiency. At the same time, in order to maintain the generators independence of application domains, An upper model is designed to interface the different application and the general generation system, which enables the systems adaptability to different application domains. The upper model is a kind of semantic hierarchy. It is organized according to the semantic relationship between predicates and their arguments, nouns and their modifiers. Tests show that the generation system embodies good adaptability to different application domains and good performances.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-381"
  },
  "seneff00b_icslp": {
   "authors": [
    [
     "Stephanie",
     "Seneff"
    ],
    [
     "Joseph",
     "Polifroni"
    ]
   ],
   "title": "Formal and natural language generation in the Mercury conversational system",
   "original": "i00_2767",
   "page_count": 4,
   "order": 383,
   "p1": "vol. 2, 767-770",
   "pn": "",
   "abstract": [
    "This paper describes the generation component of our MERCURY flight reservation conversational system. Generation makes use of the GENESIS-II generation server, which represents a signifi- cant redesign from its predecessor, GENESIS. While the main focus is on response generation, we also discuss a variety of other generation needs that are fulfilled by GENESIS-II. These include a paraphrase of the user query back into English and into a flattened electronic form, the paraphrase of the electronic form into a database query language, and the conversion of flight tables into HTML format.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-382"
  },
  "saito00_icslp": {
   "authors": [
    [
     "Takashi",
     "Saito"
    ],
    [
     "Masaharu",
     "Sakamoto"
    ]
   ],
   "title": "A method of creating a new speaker²s voicefont in a text-to-speech system",
   "original": "i00_2771",
   "page_count": 4,
   "order": 384,
   "p1": "vol. 2, 771-774",
   "pn": "",
   "abstract": [
    "This paper presents a method of creating a new speakers voice database (VoiceFont) by which the voice of the donor speaker can be synthesized for mimicking in a text-to-speech system. A VoiceFont creation system, \"VoiceFont Builder\", is developed to make the creation process easier and more effective than current systems. The voice feature extraction applied in the system is a simple but powerful method that makes the most of the target speech synthesizer. Using a VoiceFont obtained, we conducted experiments on F0 contour generation in view of reproducing that of the donor speakers voice.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-383"
  },
  "huang00_icslp": {
   "authors": [
    [
     "Jun",
     "Huang"
    ],
    [
     "Stephen",
     "Levinson"
    ],
    [
     "Mark",
     "Hasegawa-Johnson"
    ]
   ],
   "title": "Signal approximation in Hilbert space and its application on articulatory speech synthesis",
   "original": "i00_2775",
   "page_count": 4,
   "order": 385,
   "p1": "vol. 2, 775-778",
   "pn": "",
   "abstract": [
    "In this paper, we apply signal approximation theory to estimate the articulatory trajectory for a nonlinear speech synthesizer. First, we analyze the L2 error bounds of interpolation and LS approximation in the Hilbert space framework. Second, we use two signal approximation techniques, specifically, cubic spline interpolation and the LS approximation to estimate the trajectory of articulatory parameters given the static articulatory parameters of discrete phonemes. In our articulatory speech synthesizer, we assume that the sound propagation inside the human vocal tract is a three-dimensional non-plane wave propagation inside a viscous fluid described by the governing Navier-Stoke equations. Finally, we present some experimental results of the estimated articulatory trajectories of English diphthongs and the synthesized phonemes.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-384"
  },
  "minematsu00b_icslp": {
   "authors": [
    [
     "Nobuaki",
     "Minematsu"
    ],
    [
     "Seiichi",
     "Nakagawa"
    ]
   ],
   "title": "Quality improvement of PSOLA analysis-synthesis using partial zero-phase conversion",
   "original": "i00_2779",
   "page_count": 4,
   "order": 386,
   "p1": "vol. 2, 779-782",
   "pn": "",
   "abstract": [
    "This paper discusses two issues of the quality improvement of F0 modified speech based upon PSOLA analysissynthesis. Previous studies[1][2] pointed out that the location of a window of PSOLA influences the quality of synthesized speech and one of them claimed that the center of a window should be located at a pitch pulse in source waveforms. However, pitch pulse detection sometimes fails due to undesired acoustic events. In this paper, several methods are experimentally examined to reduce pitch pulse detection errors. Even when the detection is done correctly, F0 modified re-synthesized speech sometimes causes \"echoes\" in the re-arranged waveforms. This is mainly caused by a pitch pulse with small sharpness or by that with two relatively high pulses, not pitch pulses, before and after it. To suppress the echoes with little loss of naturalness, partial zero/ð-phase conversion is proposed here. Experiments show the high validity of the proposed methods in improving the quality of re-synthesized speech.\n",
    "s H. Kawai et al., \"A study of a text-to-speech system based on waveform splicing,\" Technical report of IEICE, SP93-9, pp.49-54 (1993, in Japanese). Y. Arai et al., \"A study on the optimal window position to extract pitch waveforms,\" Technical report of IEICE, SP95-8, pp.53-59 (1995, in Japanese).\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-385"
  },
  "lindgren00_icslp": {
   "authors": [
    [
     "Hanna",
     "Lindgren"
    ],
    [
     "Jessica",
     "Granberg"
    ]
   ],
   "title": "A machine learning approach to Swedish word pronunciation",
   "original": "i00_2783",
   "page_count": 4,
   "order": 387,
   "p1": "vol. 2, 783-786",
   "pn": "",
   "abstract": [
    "This study focuses on word pronunciation in Text-to-Speech systems for Swedish. The purpose is to investigate whether machine learning techniques match knowledge-based systems in Swedish word pronunciation. The experiments show a maximum grapheme accuracy of just over 97%, and word accuracies from 67.0% for word pronunciation excluding stress assignment, which compares favourably to existing knowledgebased state-of-the-art systems.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-386"
  },
  "ohtsuka00_icslp": {
   "authors": [
    [
     "Takahiro",
     "Ohtsuka"
    ],
    [
     "Hideki",
     "Kasuya"
    ]
   ],
   "title": "An improved speech analysis-synthesis algorithm based on the autoregressive with exogenous input speech production model",
   "original": "i00_2787",
   "page_count": 4,
   "order": 388,
   "p1": "vol. 2, 787-790",
   "pn": "",
   "abstract": [
    "Ding et al. have explored a novel pitch-synchronous speech analysis-synthesis method[1] based on an auto-regressive with exogenous input (ARX) speech production model. This method makes an automatic estimation of the vocal tract (formant) and voice source parameters from a speech utterance. This method, however, has suffered deficiencies in the analysis of a high-pitch voice and the introduction of click sounds in the transition between vocalic and weak voiced consonantal segments. This paper proposes an improved ARX method in order to solve the problems mentioned above. Perceptual comparison experiments have shown that quality of re-synthesized speech by the proposed method is higher than that by a well-known cepstral method.\n",
    "",
    "",
    "Ding, W., Kasuya, H., and Adachi, S. Simultaneous estimation of vocal tract and voice source parameters based on an ARX model. IEEE Trans. Inf. & Syst., E78-D, 738-743, 1995.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-387"
  },
  "yuo00_icslp": {
   "authors": [
    [
     "Kuo-Hwei",
     "Yuo"
    ],
    [
     "Tai-Hwei",
     "Hwang"
    ],
    [
     "Hsiao-Chuan",
     "Wang"
    ]
   ],
   "title": "Combination of temporal trajectory filtering and projection measure for robust speaker identification",
   "original": "i00_2791",
   "page_count": 4,
   "order": 389,
   "p1": "vol. 2, 791-794",
   "pn": "",
   "abstract": [
    "This paper presents a method that combines the techniques of temporal trajectory filtering and projection measure for robust speaker identification. The proposed robust feature, called Relative Autocorrelation Sequence Mel-scale Frequency Cepstral Coefficients (RAS-MFCC), is derived based on filtering the temporal trajectories of short-time one-sided autocorrelation sequences. This filtering process can minimize the effect of additive noise in the noisy speech. Since the norm of RAS-MFCC shrinks due to noise corruption, the projection measure (PM) technique, which is effective in dealing with the norm shrinkage of cepstrum, can be applied for the distance measure of RAS-MFCCs. The combination of these two techniques is then applied to a task of speaker identification of 100 speakers. Our experiment shows that the use of RASMFCC feature achieves significant improvement in identification rate as comparing with the use of MFCC. The combination of RAS-MFCC feature with PM technique can further improve the recognition accuracy.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-388"
  },
  "zhao00_icslp": {
   "authors": [
    [
     "Yunxin",
     "Zhao"
    ],
    [
     "Xiao",
     "Zhang"
    ],
    [
     "Xiaodong",
     "He"
    ],
    [
     "Laura",
     "Schopp"
    ]
   ],
   "title": "A combined adaptive and decision tree based speech separation technique for telemedicine applications",
   "original": "i00_2795",
   "page_count": 4,
   "order": 390,
   "p1": "vol. 2, 795-798",
   "pn": "",
   "abstract": [
    "We present a novel technique for separation of doctor and patients speech in conversations over a telemedicine network. The mixed speech signals acquired at doctors site is first broken into single talkers speech segments and background by using thresholds of energy and duration. The speech segments are then identified as spoken by doctor or patient in two steps. In the first step, Gaussian mixture models (GMM) of doctor and patient are used, where the doctors model is obtained from his/her training speech, and the patients model is initialized by a general speaker model and then adapted by the patients speech. In the second step, a decision tree that uses contextual and confidence features is applied to refine the identification results. Preliminary experiments were performed on three data sets collected in telemedicine. Without adaptation and decision tree, error rates at the segment-level and frame-level were 25.44% and 16.53%, respectively. With adaptation, segment and frame error rates were reduced to 13.11% and 7.85%, and with decision tree, the error rates were further reduced to 10.48% and 6.73%, respectively.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-389"
  },
  "bellot00_icslp": {
   "authors": [
    [
     "Olivier",
     "Bellot"
    ],
    [
     "Driss",
     "Matrouf"
    ],
    [
     "Teva",
     "Merlin"
    ],
    [
     "Jean-François",
     "Bonastre"
    ]
   ],
   "title": "Additive and convolutional noises compensation for speaker recognition",
   "original": "i00_2799",
   "page_count": 4,
   "order": 391,
   "p1": "vol. 2, 799-802",
   "pn": "",
   "abstract": [
    "It is well known that the performances of speaker identification systems degrade rapidly as the mismatch between training and test conditions increases. In this work we present a noise compensation technique whose goal is to minimize the effects of such mismatch, so as to obtain an identification accuracy as close as possible to that obtained under matched conditions. To reduce this mismatch, the adopted approach compensates the speaker model parameters using the noise present in the test data, and compensates the test data frames using the noise present in the training data. The test and the training data (for different speakers) are assumed to come from different and unknown microphones and acoustic environments.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-390"
  },
  "beaugendre00_icslp": {
   "authors": [
    [
     "Frédéric",
     "Beaugendre"
    ],
    [
     "Tom",
     "Claes"
    ],
    [
     "Hugo van",
     "Hamme"
    ]
   ],
   "title": "Dialect adaptation for Mandarin Chinese speech recognition",
   "original": "i00_2803",
   "page_count": 4,
   "order": 392,
   "p1": "vol. 2, 803-806",
   "pn": "",
   "abstract": [
    "Many local or regional dialects exist in China. In case of mismatch between the dialect used to train the system and the dialect of the user, poor recognition accuracy is obtained. In this paper, we therefore investigate the development of a dialectspecific recognition system in Mandarin Chinese using standard adaptation techniques: a speaker-independent (SI) model trained on a source dialect (Beijing dialect) is adapted using a limited set of speech data of a target dialect (Taiwanese dialect).\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-391"
  },
  "scherer00b_icslp": {
   "authors": [
    [
     "Klaus R.",
     "Scherer"
    ],
    [
     "Tom",
     "Johnstone"
    ],
    [
     "Gudrun",
     "Klasmeyer"
    ],
    [
     "Thomas",
     "Bänziger"
    ]
   ],
   "title": "Can automatic speaker verification be improved by training the algorithms on emotional speech?",
   "original": "i00_2807",
   "page_count": 4,
   "order": 393,
   "p1": "vol. 2, 807-810",
   "pn": "",
   "abstract": [
    "The ongoing work described in this contribution attempts to demonstrate the need to train ASV algorithms on emotional speech, in addition to neutral speech, in order to achieve more robust results in real life verification situations. A computerized induction program with 6 different tasks, producing different types of stressful or emotional speaker states, was developed, pretested, and used to record French, German, and English speaking participants. For a subset of these speakers, physiological data were obtained to determine the degree of physiological arousal produced by the emotion inductions and to determine the correlation between physiological responses and voice production as revealed in acoustic parameters. In collaboration with a commercial ASV provider (Ensigma Ltd.), a standard verification procedure was applied to this speech material. This paper reports the first set of preliminary analyses for the subset of 30 German speakers. It is concluded that an evaluation of the promise of training ASV material on emotional speech requires in-depth analyses of the individual differences in vocal reactivity and further exploration of the link between acoustic changes under stress or emotion and verification results.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-392"
  },
  "wang00j_icslp": {
   "authors": [
    [
     "Zhong-Hua",
     "Wang"
    ],
    [
     "Cheng",
     "Wu"
    ],
    [
     "David",
     "Lubensky"
    ]
   ],
   "title": "New distance measures for text-independent speaker identification",
   "original": "i00_2811",
   "page_count": 4,
   "order": 394,
   "p1": "vol. 2, 811-814",
   "pn": "",
   "abstract": [
    "Distance measures [1][2][3] based on the covariance matrix of feature vectors were applied to text-independent speaker verification and identification. However, some of them do not satisfy the symmetric property which is fundamental to a distance measure. In this paper, we propose several symmetric distance measures based on the covariance matrix of feature vectors, and then construct some advanced measures using the data fusion method [4]. These new distance measures have good mathematic properties and impose little overhead in calculation. We apply these distance measures to text-independent speaker identification and handset detection. A new robust technique is developed for crosshandset speaker identification, and find that compensating the second order statistics is important when dealing with the mismatch caused by different handsets. The experiment uses the cb1 and cb2 data in the LLHDB corpus [5] for same-handset and cross-handset speaker identification test. We find that the use of delta cepstra decreases the speaker identification error rate by as much as 38%. Data fusion technique could further decrease the error rate by 11%. Applying these distance measures to 2-handset detection problem, the error rate is 12%. Using our new robust technique, the cross-handset speaker identification error rate is could be decreased by around 17%.\n",
    "s H. Gish, \"Robust discrimination in automatic speaker identification\", Proc. ICASSP 1991, Vol. 1, pp. 289- 292. F. Bimbot and L. Mathan, \"Second-order statistical measures for text-independent speaker identification\", ECSA workshop on automatic speaker recognition, identification and verification, 1994, pp. 51-54. S. Johnson, \"Speaker tracking\", Mphil thesis, University of Cambridge, 1997. K. R. Farrell, \"Discriminatory measures for speaker recognition\", Proceedings of Neural Networks for Signal Processing, 1995, and references therein. D. A. Reynolds, \"HTIMIT and LLHDB: Speech corpora for the study of handset transducer effects\", ICASSP, pp. 1535-1538, May 1997, Munich, Germany.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-393"
  },
  "zhao00b_icslp": {
   "authors": [
    [
     "Fengguang",
     "Zhao"
    ],
    [
     "Prabhu",
     "Raghavan"
    ],
    [
     "Sunil K.",
     "Gupta"
    ],
    [
     "Ziyi",
     "Lu"
    ],
    [
     "Wentao",
     "Gu"
    ],
    [
     "Wentao",
     "Gu"
    ]
   ],
   "title": "Automatic speech recognition in Mandarin for embedded platforms",
   "original": "i00_2815",
   "page_count": 7,
   "order": 395,
   "p1": "vol. 2, 815-818",
   "pn": "",
   "abstract": [
    "In this paper, we describe a real-time automatic speech recognition system for Mandarin for low-cost embedded platforms using fixed-point digital signal processors. The hands-free, speaker-independent speech recognition system employs 41 mono-phone models for representing the sounds in Mandarin Chinese and 11 whole-word models for connected digit recognition. The system achieves greater than 98% recognition accuracy on our hands-free test database of 46 distinct command phrases. The system achieves 95.9% digit accuracy on a 14 speaker, hands-free, connected digit recognition database. The analysis of the results shows that for speakers without dialect, the digit recognition accuracy is almost 98%. We present a detailed analysis of the digit recognition results and propose further improvements. A realtime platform based upon Lucents DSP1627 fixed-point digital signal processor has been developed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-394"
  },
  "li00f_icslp": {
   "authors": [
    [
     "Husheng",
     "Li"
    ],
    [
     "Jia",
     "Liu"
    ],
    [
     "Runsheng",
     "Liu"
    ]
   ],
   "title": "Confidence measure based unsupervised speaker adaptation",
   "original": "i00_2819",
   "page_count": 4,
   "order": 396,
   "p1": "vol. 2, 819-822",
   "pn": "",
   "abstract": [
    "Unsupervised adaptation is the most convenient mode for the user of a speech recognition system. However the performance of unsupervised adaptation is worse than that of the supervised mode because of the recognition errors. This paper introduces a kind of word-lattice based confidence measure to evaluate the reliability of the recognition result and discard the uncertain parts from the adaptation speech. Experiments demonstrate that the confidence can improve the performance of unsupervised adaptation considerably.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-395"
  },
  "maciasguarasa00_icslp": {
   "authors": [
    [
     "Javier",
     "Macías-Guarasa"
    ],
    [
     "Javier",
     "Ferreiros"
    ],
    [
     "José",
     "Colás"
    ],
    [
     "A.",
     "Gallardo-Antolín"
    ],
    [
     "Juan Manuel",
     "Pardo"
    ]
   ],
   "title": "Improved variable preselection list length estimation using NNs in a large vocabulary telephone speech recognition system",
   "original": "i00_2823",
   "page_count": 4,
   "order": 397,
   "p1": "vol. 2, 823-826",
   "pn": "",
   "abstract": [
    "In very large vocabulary hypothesis-verification systems, the fine acoustic matcher is usually the most time consuming, so that the main concern is reducing the preselection list length as much as possible. Traditionally, these systems use a too high fixed preselection list length, increasing computational demands over the really needed.\n",
    "The idea we are proposing is estimating a different preselection list length for every utterance, so that we can lower the average computational effort needed for the recognition process. As we will show, its even possible that the resulting system outperforms the fixed length one in error rate, even when reducing computational cost.\n",
    "This paper presents a detailed study on a NN based approach to variable preselection list length estimation. The main achievement has been a relative decrease in error rate of up to 40%, while getting a relative decrease in average preselection list length of up to 31%.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-396"
  },
  "gallardoantolin00_icslp": {
   "authors": [
    [
     "Ascensión",
     "Gallardo-Antolín"
    ],
    [
     "Javier",
     "Ferreiros"
    ],
    [
     "Javier",
     "Macías-Guarasa"
    ],
    [
     "R. de",
     "Córdoba"
    ],
    [
     "Juan Manuel",
     "Pardo"
    ]
   ],
   "title": "Incorporating multiple-HMM acoustic modeling in a modular large vocabulary speech recognition system in telephone environment",
   "original": "i00_2827",
   "page_count": 4,
   "order": 398,
   "p1": "vol. 2, 827-830",
   "pn": "",
   "abstract": [
    "The use of multiple acoustic models has reported great improvements when facing speaker independent difficult tasks. In this paper, we are applying this strategy to a flexible, large vocabulary, speaker-independent, isolated-word hypothesis generation system in a telephone environment with vocabularies up to 10000 words. The new problem addressed here is how to efficiently integrate the multiple model scheme in the system, as due to its bottom-up approach (phonetic string generation followed by a lexical access process), multiple possibilities arise (apart from the alternatives in the training stage), and its not clear what combination would achieve the best results. In the paper, full details on every alternative are shown, along with results showing actual improvements in the system.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-397"
  },
  "suontausta00_icslp": {
   "authors": [
    [
     "Janne",
     "Suontausta"
    ],
    [
     "Juha",
     "Häkkinen"
    ]
   ],
   "title": "Decision tree based text-to-phoneme mapping for speech recognition",
   "original": "i00_2831",
   "page_count": 4,
   "order": 399,
   "p1": "vol. 2, 831-834",
   "pn": "",
   "abstract": [
    "In many embedded speech recognition systems, the phonetic transcriptions of the vocabulary items, i.e., the lexicons, cannot be stored to the device beforehand. A text-to-phoneme mapping functionality is hence needed to create the transcriptions from plain text. Several approaches have been evaluated in the literature. In this paper, a decision tree based text-to-phoneme mapping is studied. A decision tree is trained for each letter according to information theoretic criteria on a pronunciation dictionary that contains the phoneme transcriptions for a large number of words. Context information is utilized to create the mapping. In our experiments, the mapping was constructed on the Carnegie Mellon pronunciation dictionary [1]. The phoneme accuracy of the most effective mapping was 99% on the training set and 91% on the test set of the pronunciation dictionary. The mapping was also implemented in a speaker independent isolated word recognition system. The recognition rates in the clean and in the car noise test environment were close to the baseline recognition rates obtained with the correct transcriptions, when the training lexicon contained the test vocabulary. When the test vocabulary differed significantly from the training vocabulary, the mapping performed below our expectations.\n",
    "",
    "",
    "Weide, R.L., Carnegie Mellon Pronouncing Dictionary, Release 0.4, http://www.cs.cmu.edu.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-398"
  },
  "meunier00_icslp": {
   "authors": [
    [
     "Jeff",
     "Meunier"
    ]
   ],
   "title": "Reduced traceback matrix storage for small footprint model alignment",
   "original": "i00_2835",
   "page_count": 4,
   "order": 400,
   "p1": "vol. 2, 835-838",
   "pn": "",
   "abstract": [
    "This paper describes two alternative techniques for storage of path traceback information during model alignment. By taking advantage of the restricted state transitions in the standard leftright hidden Markov model (HMM), traceback information can be stored as a set of state dwell counts. Compared to the traditional method of storing intermediate path information for every frame, these in-place techniques can provide significant memory savings in a small footprint embedded system. The first technique organizes the dwell counts in a \"triangular array\" and specifies a set of update operations performed during alignment. This direct storage technique allows for easy access of the path information once alignment is complete. The second technique encodes the dwell counts in base-D numeric values. By doing this, the array update procedure reduces to a single calculation and does not require multiple memory copies. Both techniques can be done in-place and require around N2/2 memory words instead of the NT memory words required for conventional storage (where N is the number of HMM states and T is the maximum number of frames in a modeled utterance).\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-399"
  },
  "vair00_icslp": {
   "authors": [
    [
     "Claudio",
     "Vair"
    ],
    [
     "Luciano",
     "Fissore"
    ],
    [
     "Pietro",
     "Laface"
    ]
   ],
   "title": "Dynamic adaptation of vocabulary independent HMMs to an application environment",
   "original": "i00_2839",
   "page_count": 4,
   "order": 401,
   "p1": "vol. 2, 839-842",
   "pn": "",
   "abstract": [
    "The paper presents a software architecture allowing to collect, select, and exploit speech data from a specific application field to dynamically generate Hidden Markov Models tailored to that application environment and vocabulary. The framework we are interested in is, therefore, an already operational voice activated service that allows to collect directly from the field a large amount of speech data. We propose a procedure for data selection and for incremental training of the units using a strategy of model selection. Several tests are presented for a train timetable information system, and for a Directory Assistance application with a very large vocabulary of city names showing that significant improvements can be obtained with respect to the laboratory models, keeping the old models and transcribing only the most frequent words in terms of the new units, incrementally trained from the field data.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-400"
  },
  "gemello00_icslp": {
   "authors": [
    [
     "Roberto",
     "Gemello"
    ],
    [
     "Loreta",
     "Moisa"
    ],
    [
     "Pietro",
     "Laface"
    ]
   ],
   "title": "Synergy of spectral and perceptual features in multi-source connectionist speech recognition",
   "original": "i00_2843",
   "page_count": 4,
   "order": 402,
   "p1": "vol. 2, 843-846",
   "pn": "",
   "abstract": [
    "The combined use of different set of features extracted from the speech signal with different processing algorithms is a promising approach to improve speech recognition performances. Artificial Neural Networks are well suited to this task since they are able to use directly multiple heterogeneous input features to estimate a near optimal combination of them for classification, without being constrained by a priori assumptions on the stochastic independence of the input sources. This work shows how we have taken advantage of these characteristics of Neural Networks to improve the recognition accuracy of our systems. In particular, three set of input features have been considered as sources in this work: Mel based Cepstral Coefficients derived from the FFT spectrum, RASTA-PLP Cepstral Coefficients, and a set of features that describe the dynamics of the FFT power spectrum along the frequency dimension, instead of the usual time dimension. The experimental results confirm the usefulness of the proposed approach of feature integration that leads to a significant error reduction both on isolated and continuous speech recognition tasks on a large telephone speech test set.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-401"
  },
  "hariharan00_icslp": {
   "authors": [
    [
     "Ramalingam",
     "Hariharan"
    ],
    [
     "Olli",
     "Viikki"
    ]
   ],
   "title": "High performance connected digit recognition through gender-dependent acoustic modelling and vocal tract length normalisation",
   "original": "i00_2847",
   "page_count": 4,
   "order": 403,
   "p1": "vol. 2, 847-850",
   "pn": "",
   "abstract": [
    "Large inter-speaker variability of speech is one of the major sources which degrade the performance of state-of-the-art speech recognition systems. During the recent years, several methods, including gender-dependent acoustic modelling and vocal tract length normalisation, have been developed to reduce this variability. In this paper, we first investigate these two methods individually and propose how they should be implemented in real-world speech recognition systems. Secondly, we show that by combining these two techniques, it is possible to further reduce the error rate in a connected digit recognition task under a realistic car noise environment. Experimental results justify the use of the combined approach. A 44.1% decrease in string error rate was observed when the performance of the joint system was compared to the genderindependent baseline system. The results were also better than that obtained when using these techniques individually.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-402"
  },
  "eide00_icslp": {
   "authors": [
    [
     "Ellen",
     "Eide"
    ],
    [
     "Benoît",
     "Maison"
    ],
    [
     "D.",
     "Kanevsky"
    ],
    [
     "P.",
     "Olsen"
    ],
    [
     "S.",
     "Chen"
    ],
    [
     "L.",
     "Mangu"
    ],
    [
     "M.",
     "Gales"
    ],
    [
     "Miroslav",
     "Novak"
    ],
    [
     "Ramesh",
     "Gopinath"
    ]
   ],
   "title": "Transcription of broadcast news with a time constraint: IBMs 10xRT HUB4 system",
   "original": "i00_2851",
   "page_count": 4,
   "order": 404,
   "p1": "vol. 2, 851-854",
   "pn": "",
   "abstract": [
    "We describe a system which automatically transcribes broadcast news in less than 10 times real-time. We detail the system architecture of this system, which was used by IBM in the 1999 HUB4 10xRT evaluation, and show that the performance of this system is over 20 percent more accurate at the same speed than the system we used in the 1998 evaluation. Furthermore, we have closed the gap in word recognition accuracy between an unlimited resource system and this which runs in under 10 times real time from 45 percent to 14 percent.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-403"
  },
  "zweig00_icslp": {
   "authors": [
    [
     "Geoffrey",
     "Zweig"
    ],
    [
     "Mukund",
     "Padmanabhan"
    ]
   ],
   "title": "Exact alpha-beta computation in logarithmic space with application to MAP word graph construction",
   "original": "i00_2855",
   "page_count": 4,
   "order": 405,
   "p1": "vol. 2, 855-858",
   "pn": "",
   "abstract": [
    "The classical dynamic programming recursions for the forwards-backwards and Viterbi HMM algorithms are linear in the number of time frames being processed. Adapting the method of [1] to the context of speech recognition, this paper uses a recursive divide-and-conquer algorithm to reduce the space requirement to logarithmic in the number of frames. With this procedure, it is possible to do exact computations for observation sequences of essentially arbitrary length. The procedure works by manipulating a stack of alpha vectors, and by using sparse vectors, the space savings can be combined with those of traditional pruning techniques. We apply this technique to MAP lattice construction, and present the first results in the literature for that technique. We find that it is an e\u000bective way of creating word lattices, and that doing the exact computations enabled by the log-space technique results in lower word error rates than space saving via traditional pruning.\n",
    "",
    "",
    "J. Binder, K. Murphy and S. Russell. Space-Efficient Inference in Dynamic Probabilistic Networks. Proc. 15th Int'l. Joint Conf. on AI, 1997.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-404"
  },
  "yamamoto00c_icslp": {
   "authors": [
    [
     "Kazumasa",
     "Yamamoto"
    ],
    [
     "Seiichi",
     "Nakagawa"
    ]
   ],
   "title": "Relationship among speaking style, inter-phoneme's distance and speech recognition performance",
   "original": "i00_2859",
   "page_count": 4,
   "order": 406,
   "p1": "vol. 2, 859-862",
   "pn": "",
   "abstract": [
    "There is a limit of recognition performance for dialogue speech using acoustic models built only with read speech, because various acoustic and linguistic phenomena, which reflect the characteristics of spontaneous speech, are observed in the dialogue speech. In this paper, we inves- tigated the di\u000berences of acoustic properties which cause the limit among isolated words, read speech and spontaneous speech. Firstly, the dialogue speech was compared with the read speech through acoustic analyses. Next, the acoustic models were separately built with each of the speech databases. The recognition performance was experimentally evaluated using the acoustic models and the relations of the di\u000berences of the performance to those of the acoustic features observed in the analyses were investigated quantitatively. The e\u000bectiveness of speaker adaptation was also investigated in the same manner.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-405"
  },
  "sansegundo00_icslp": {
   "authors": [
    [
     "Ruben",
     "San-Segundo"
    ],
    [
     "José",
     "Colás"
    ],
    [
     "Javier",
     "Ferreiros"
    ],
    [
     "Javier",
     "Macías-Guarasa"
    ],
    [
     "Juan Miguel",
     "Pardo"
    ]
   ],
   "title": "Spanish recogniser of continuously spelled names over the telephone",
   "original": "i00_2863",
   "page_count": 4,
   "order": 407,
   "p1": "vol. 2, 863-866",
   "pn": "",
   "abstract": [
    "lapiz@die.upm.es http://www-gth.die.upm.es ABSTRACT In the paper, we present an analysis of the spelling task for Spanish and we describe the research and implementation of a Spanish recogniser for continuously spelled names over the telephone. We analyse and compare three different recognition architectures. The first one is a Two level architecture. This approach consists in two steps. In the first one we obtain the most likely letter sequence using the one-pass algorithm. In the second step, to obtain the name recognised, we align the sequence of letters with the different dictionary names using a Dynamic Programming (DP) algorithm. The second alternative consists on an Integrated Architecture where a constrained grammar is built with all the names from the dictionary. In this case, we have a higher Name Recognition Rate but the time processing increases a lot. Finally, we propose a combined architecture with a good compromise between recognition rate and time consuming. This approach responds to a strategy of Hypothesis and Verification. In the hypothesis stage, we obtain the most likely letter sequence (one-pass algorithm) and then we select N-candidates from the dictionary with a dynamic programming algorithm. In the verification stage, we build a dynamic grammar with the N-candidates and we recognise over it. With this system we obtain a 96.1% Name Recognition Rate in real time for the 1,000 names dictionary and, 92.3% and 89.6% for 5,000 and 10,000 names directories respectively.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-406"
  },
  "seide00_icslp": {
   "authors": [
    [
     "Frank",
     "Seide"
    ],
    [
     "Nick J.C.",
     "Wang"
    ]
   ],
   "title": "Two-stream modeling of Mandarin tones",
   "original": "i00_2867",
   "page_count": 4,
   "order": 408,
   "p1": "vol. 2, 867-870",
   "pn": "",
   "abstract": [
    "Tone modeling is a critical component for Mandarin large- vocabulary continuous-speech recognition systems. In previ- ous work on pitch-feature extraction, we reported character error rate reductions of over 30% over the non-tonal baseline [1]. In this paper, we investigate how best to integrate tone modeling with a Mandarin LVCSR system.\n",
    "The paper focusses on the two-stream method, which is based on two-stream continuous-mixture HMMs. For tonal langua- ges, sub-word units may depend on both phonetic context and tone. To alleviate for the multiplication of model pa- rameters, the two-stream method models state emission dis- tributions as products of independent spectral and tonal mix- tures. This allows sub-word units with di\u000berent dependences and independent state tying for the two streams, reducing model size and allowing tone-dependent modeling of initials.\n",
    "We systematically compared the two-stream method with two other approaches that we named two-model and single- stream. The two-model method yields 5% higher error rates and cannot use one-pass Viterbi decoding, while the single- stream approach requires 30{50% more parameters at similar accuracy.\n",
    "",
    "",
    "H. Huang and F. Seide. Pitch tracking and tone features for Mandarin speech recognition. In Proc. ICASSP'2000, Istanbul, 2000.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-407"
  },
  "seyyedsalehi00_icslp": {
   "authors": [
    [
     "Seyyed Ali",
     "Seyyed Salehi"
    ]
   ],
   "title": "A neural network speech recognizer based on the both acoustic steady portions and transitions",
   "original": "i00_2871",
   "page_count": 5,
   "order": 409,
   "p1": "vol. 2, 871-874",
   "pn": "",
   "abstract": [
    "Previous works on speech recognition utilizing neural networks have often relied on either recognition through segmentation or mapping of the representation trajectories to the phoneme space. Here, information could be missed due to the manner of border labeling techniques. Recent works have indicated that firstly, phonetic borders and transitions would have a good potential to be recognized as acoustic units, and secondly, recognition of the fast transitions by neural networks, as fixed cues in time, results in high performance detection and recognition of those events. This approach was manifested through recognition of basic units formed from the VC and CV borders in Farsi (Persian) spoken language. Analysis of the resulting errors has indicated certain discrepancies amongst the theoretical linguistic points of view and implementation outcome. Implementation results have indicated that the CV, CVC and CVCC linguistic models for Farsi syllables do not always match the reality of the acoustic space in the speech signal.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-408"
  },
  "hofmann00_icslp": {
   "authors": [
    [
     "Marc",
     "Hofmann"
    ],
    [
     "Manfred",
     "Lang"
    ]
   ],
   "title": "Belief networks for a syntactic and semantic analysis of spoken utterances for speech understanding",
   "original": "i00_2875",
   "page_count": 4,
   "order": 410,
   "p1": "vol. 2, 875-878",
   "pn": "",
   "abstract": [
    "In this paper we present a new approach towards speech understanding that merges semantic and intention decoding to one component. The algorithm is supposed to evaluate a speech recognizers utterance hypotheses regarding a) syntactical and semantical relations between words and phrases and b) potential intentions of the user. The mathematical fundament for this evaluation is probability theory. We make use of belief networks to handle the analysis of an utterance hypothesis as a process of reasoning with uncertain and incomplete information. The algorithm in general can be characterized as phrase spotting.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-409"
  },
  "sun00b_icslp": {
   "authors": [
    [
     "Jiping",
     "Sun"
    ],
    [
     "Roberto",
     "Togneri"
    ],
    [
     "Li",
     "Deng"
    ]
   ],
   "title": "A robust speech understanding system using conceptual relational grammar",
   "original": "i00_2879",
   "page_count": 4,
   "order": 411,
   "p1": "vol. 2, 879-882",
   "pn": "",
   "abstract": [
    "We describe a robust speech understanding system based on our newly developed approach to spoken language processing. We show that a robust NLU system can be rapidly developed using a relatively simple speech recognizer to provide sufficient information for database retrieval by spoken language. Our experimental system consists of three components: a speech recognizer based on HMM, a natural language parser based on conceptual relational grammar and a data retrieval system based on the ATIS database. With the use of the robust parsing strategy, database query tasks can be successfully performed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-410"
  },
  "lau00_icslp": {
   "authors": [
    [
     "Wai",
     "Lau"
    ],
    [
     "Tan",
     "Lee"
    ],
    [
     "Yiu Wing",
     "Wong"
    ],
    [
     "P. C.",
     "Ching"
    ]
   ],
   "title": "Incorporating tone information into Cantonese large-vocabulary continuous speech recognition",
   "original": "i00_2883",
   "page_count": 4,
   "order": 412,
   "p1": "vol. 2, 883-886",
   "pn": "",
   "abstract": [
    "Tone recognition is indispensable in automatic speech recognition of tonal languages like Chinese. Among the many Chinese dialects, Cantonese is well known of being rich in tones, This paper presents a comprehensive study on speaker-independent tone recognition in continuous Cantonese speech. Tone features are derived, on short-time basis, from syllable-wide F0 and energy profiles. A novel technique of moving-window normalization is proposed to effectively reduce undesirable fluctuation of the feature parameters. This technique allows on-the-fly and adaptive estimation of the speaker's intrinsic pitch range. Conventional HMM based approach is employed to recognize the normalized tone features. Using context-dependent tone models, an accuracy of 66.4% has been attained. The tone recognizer is then integrated into a Cantonese LVCSR system using the method of lattice expansion. Experimental results show that tone information, if reliably acquired and properly utilized, can contribute to noticeable improvement of the overall performance of the LVCSR system.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-411"
  },
  "kaiser00_icslp": {
   "authors": [
    [
     "Janez",
     "Kaiser"
    ],
    [
     "Bogomir",
     "Horvat"
    ],
    [
     "Zdravko",
     "Kacic"
    ]
   ],
   "title": "A novel loss function for the overall risk criterion based discriminative training of HMM models",
   "original": "i00_2887",
   "page_count": 4,
   "order": 413,
   "p1": "vol. 2, 887-890",
   "pn": "",
   "abstract": [
    "In this paper, we propose a novel loss function for the overall risk criterion estimation of hidden Markov models. For continuous speech recognition, the overall risk criterion estimation with the proposed loss function aims to directly maximise word recognition accuracy on the training database. We propose reestimation equations for the HMM parameters, which are derived using the Extended Baum-Welch algorithm. Using HMM, trained with the proposed method, a decrease of word recognition error rate of up to 17.3% has been achieved for the phoneme recognition task on the TIMIT database.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-412"
  },
  "maucec00_icslp": {
   "authors": [
    [
     "Mirjam Sepesy",
     "Maucec"
    ],
    [
     "Zdravko",
     "Kacic"
    ],
    [
     "Bogomir",
     "Horvat"
    ]
   ],
   "title": "Looking for topic similarities of highly inflected languages for language model adaptation",
   "original": "i00_2891",
   "page_count": 4,
   "order": 414,
   "p1": "vol. 2, 891-894",
   "pn": "",
   "abstract": [
    "In this paper, we propose a new framework to construct corpus-based topic-sensitive language models of highly inflected languages for large vocabulary speech recognition. We concentrate on feature extraction process devoted to languages where words are formed by many different inflectional affixatations. In our approach all words with the same meaning but different grammatical form are collected in one cluster automatically by using fuzzy comparison function. Using topic classifier sub-corpus of a large collection of training text is selected. Language models are built by interpolation of topic specific models and general model. Results of experiments on English and Slovenian corpus are reported.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-413"
  },
  "janiszek00_icslp": {
   "authors": [
    [
     "David",
     "Janiszek"
    ],
    [
     "Frédéric",
     "Béchet"
    ],
    [
     "Renato De",
     "Mori"
    ]
   ],
   "title": "Integrating MAP and linear transformation for language model adaptation",
   "original": "i00_2895",
   "page_count": 4,
   "order": 415,
   "p1": "vol. 2, 895-898",
   "pn": "",
   "abstract": [
    "This paper discusses the integration of various language model (LM) adaptations. Ways of integrating Maximum A Posteriori (MAP) adaptation and linear transformation of bigram probability vectors are introduced and evaluated. This method leads to little improvements for adaptation corpora of less than 15,000 words. Another method, based on a data augmentation technique by means of a distance between history vectors in a reduced space is also proposed. This method allows us to improve the results when using adaptation corpora larger than 30,000 words.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-414"
  },
  "tan00_icslp": {
   "authors": [
    [
     "Beng Tiong",
     "Tan"
    ],
    [
     "Yong",
     "Gu"
    ],
    [
     "Trevor",
     "Thomas"
    ]
   ],
   "title": "Utterance verification based speech recognition system",
   "original": "i00_2899",
   "page_count": 5,
   "order": 416,
   "p1": "vol. 2, 899-902",
   "pn": "",
   "abstract": [
    "Many existing search algorithms aim at searching for the best hypothesis from all possible hypotheses with the help of techniques like beam search to reduce the computational cost. These search algorithms are based on the competitive criteria because the best hypothesis is determined after we have the knowledge of all other possible hypotheses. In this paper, we investigate the possible use of a qualifier criterion. A qualifierbased search will make recognition decision without considering all possible hypotheses. Instead of finding the best hypothesis, the system will try to find a hypothesis that can meet with some criteria reflecting the goodness of a hypothesis. We investigate the use of utterance verification measurement as the qualifier measurement used in the search. An utterance verification based speech recognition (UVSR) system is proposed. The search will be terminated as soon as the qualifier criteria are meet without the need to evaluate the likelihoods of all possible hypotheses. The proposed search algorithm with UV performs very closely to the standard speech recognition with UV but with 23% less in the average searching time. If the information of language model is provided, the average searching time can be reduced by 43%.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-415"
  },
  "chengalvarayan00b_icslp": {
   "authors": [
    [
     "Rathinavelu",
     "Chengalvarayan"
    ]
   ],
   "title": "Use of linear extrapolation based linear predictive cepstral features (LE-LPCC) for Tamil speech recognition",
   "original": "i00_2903",
   "page_count": 4,
   "order": 417,
   "p1": "vol. 2, 903-906",
   "pn": "",
   "abstract": [
    "A new method, named linear prediction with linear extrapolation has been proposed in the past, which aims at modifying conventional linear prediction especially for speech coding applications. The basic idea is to reformulate the computation of linear prediction so that an optimal FIR-predictor of order 2p could be determined from p numerical values. In this work, we extend the above method to generate the cepstral features using the compressed LPC parameters and use the new feature for accurate speech recognition. Peliminary results on Tamil connected digit recognition task demonstrated that the cepstral features derived from the new approach yield more accurate modeling of speech spectra and provides better discrimination among different speech classes.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-416"
  },
  "atake00_icslp": {
   "authors": [
    [
     "Yoshinori",
     "Atake"
    ],
    [
     "Toshio",
     "Irino"
    ],
    [
     "Hideki",
     "Kawahara"
    ],
    [
     "Jinlin",
     "Lu"
    ],
    [
     "Satoshi",
     "Nakamura"
    ],
    [
     "Kiyohiro",
     "Shikano"
    ]
   ],
   "title": "Robust fundamental frequency estimation using instantaneous frequencies of harmonic components",
   "original": "i00_2907",
   "page_count": 4,
   "order": 418,
   "p1": "vol. 2, 907-910",
   "pn": "",
   "abstract": [
    "This paper proposes a noise-tolerant method for fundamental frequency (F0) extraction. This method includes several new ideas, including the estimation of the instantaneous frequencies of the higher harmonic components, and the design of an adaptive weighting function based on a bandwidth equation that combines the F0 information in the harmonic components. To evaluate the proposed method, we constructed a relatively large database of simultaneous recordings of speech waveforms and EGG (Electro Glotto Graphy). The database consists of 30 sentences pronounced by 14 male and 14 female normal subjects, i.e., 840 sentences in total. The duration of the sound is about 35 minutes including about 20 minutes of voicing. The experiments were performed with additive noise for four pitch extraction methods, i.e., the proposed method, the original TEMPO, an improved cepstrum method, and a common F0 extraction program in ESPS. The results were as follows: 1) the proposed method is always better than any of the other methods when the SNR is greater than about 2 dB; 2) for high SNR values (> 15 dB), the correct rates of the proposed method and the original TEMPO are about 95% and much better than the improved cepstrum method (92%) and the ESPS function (89%); and 3) all of the methods degrade to less than 62% when the SNR is 0 dB. As a result, the proposed method improves the performance for low SNR values and also maintains high accuracy inherent from the original TEMPO for high SNR values.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-417"
  },
  "varona00_icslp": {
   "authors": [
    [
     "Amparo",
     "Varona"
    ],
    [
     "In",
     "Torres"
    ],
    [
     "Miren Karmele",
     "López de Ipiña"
    ],
    [
     "Luis Javier",
     "Rodriguez"
    ]
   ],
   "title": "Integrating different acoustic and syntactic language models in a continuous speech recognition system",
   "original": "i00_2911",
   "page_count": 4,
   "order": 419,
   "p1": "vol. 2, 911-914",
   "pn": "",
   "abstract": [
    "[Abstract not available due to a reproduction error on the first page of the original text file.]\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-418"
  },
  "schwenk00_icslp": {
   "authors": [
    [
     "Holger",
     "Schwenk"
    ],
    [
     "Jean-Luc",
     "Gauvain"
    ]
   ],
   "title": "Combining multiple speech recognizers using voting and language model information",
   "original": "i00_2915",
   "page_count": 4,
   "order": 420,
   "p1": "vol. 2, 915-918",
   "pn": "",
   "abstract": [
    "In 1997, NIST introduced a voting scheme called ROVER for combining word scripts produced by different speech recognizers. This approach has achieved a relative word error reduction of up to 20% when used to combine the systems outputs from the 1998 and 1999 Broadcast News evaluations. Recently, there has been increasing interest in using this technique. This paper provides an analysis of several modifications of the original algorithm. Topics addressed are the order of combination, normalization/ filtering of the systems outputs prior to combining them, treatment of ties during voting and the incorporation of language model information. The modified ROVER achieves an additional 5% relative word error reduction on the 1998 and 1999 Broadcast News evaluation test sets. Links with recent theoretical work on alternative error measures are also discussed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-419"
  },
  "watanabe00_icslp": {
   "authors": [
    [
     "Keisuke",
     "Watanabe"
    ],
    [
     "Yasushi",
     "Ishikawa"
    ]
   ],
   "title": "Dialogue management based on inferred behavioral goal - improving the accuracy of understanding by dialogue context -",
   "original": "i00_2919",
   "page_count": 4,
   "order": 421,
   "p1": "vol. 2, 919-922",
   "pn": "",
   "abstract": [
    "A dialogue management method for a speech-based interactive system is described. The construction of an effective spoken dialogue system that has unrestricted input requires user-initiative dialogue management techniques. In order to realize user-initiative dialogue, however, the system must basically accept all sentences included in its language model for recognition. That creates the problem of lower recognition accuracy, because of the weaker constraints with respect to speech understanding. We have thus proposed a method for scoring the appropriateness for the dialogue context to each of the N-best speech understanding results. In this method, the users behavioral goal is inferred from a history of utterances and a dialogue context score is calculated for each of the N-best candidates based on transition probabilities of the behavioral goals. According to the results of preliminary evaluation experiments using a dialogue system focusing on a task of making hotel reservations, we succeeded in reducing the error rate (misunderstandings in acoustic scores only) to 66% for 15 sentences.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-420"
  },
  "schluter00_icslp": {
   "authors": [
    [
     "Ralf",
     "Schlüter"
    ],
    [
     "Frank",
     "Wessel"
    ],
    [
     "Hermann",
     "Ney"
    ]
   ],
   "title": "Speech recognition using context conditional word posterior probabilities",
   "original": "i00_2923",
   "page_count": 4,
   "order": 422,
   "p1": "vol. 2, 923-926",
   "pn": "",
   "abstract": [
    "In this paper two new scoring schemes for large vocabulary continuous speech recognition are compared. Instead of using the joint probability of a word sequence and a sequence of acoustic observations, we determine the best path through a word graph using posterior word probabilities with or without word context. The exact calculation of the posterior probability for a word sequence implies a sum over all possible word boundaries, which is approximated by a maximum operation in the standard scoring approach. The new scoring scheme using word posterior probabilities could be expected to lead to improved recognition performance, because it involves partial summation over word boundaries. We present experimental results on five different corpora, the Dutch Arise corpus, the German Verbmobil 98 corpus, the English North American Business 94 20k and 64k development corpora, and the English Broadcast News 96 corpus. It is shown that the Viterbi approximation within words has no effect on standard and word posterior based recognition. Using word posterior probabilities with and without word context, the relative reduction in word error rate is comparable and ranges between 1.5% and 5%. A reason why the additional consideration of word context does not further improve the recognition performance might be that the increase in word context information is traded against a decrease in the number of word sequences that contributes to a particular word posterior probability.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-421"
  },
  "meinedo00_icslp": {
   "authors": [
    [
     "Hugo",
     "Meinedo"
    ],
    [
     "Joao P.",
     "Neto"
    ]
   ],
   "title": "The use of syllable segmentation information in continuous speech recognition hybrid systems applied to the Portuguese language",
   "original": "i00_2927",
   "page_count": 4,
   "order": 423,
   "p1": "vol. 2, 927-930",
   "pn": "",
   "abstract": [
    "Recent works have shown that the use of syllables as the basic unit in a speech recognition system could be very useful. These works introduced methods exploiting syllable information as a mean to add robustness in \"traditional\" systems that use phonemes/phones as the basic unit. Being the Portuguese a highly syllabic language we expected that information from syllables would introduce potential benefits in speech recognition tasks. Following these ideas we started by developing different methods of automatic syllable segmentation. Next we applied the best segmentation method to our large vocabulary continuous speech corpus (BD-PUBLICO) achieving an accuracy of 72%. We developed a process to use the segmentation information in the acoustic models of our baseline speech recognisers for the Portuguese language. The results obtained by the modified recognition systems on 5k and 27k vocabulary tasks showed that the use of basic syllable segmentation information helps the systems to improve their overall performance by roughly 10%.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-422"
  },
  "meinedo00b_icslp": {
   "authors": [
    [
     "Hugo",
     "Meinedo"
    ],
    [
     "Joao P.",
     "Neto"
    ]
   ],
   "title": "Combination of acoustic models in continuous speech recognition hybrid systems",
   "original": "i00_2931",
   "page_count": 4,
   "order": 424,
   "p1": "vol. 2, 931-934",
   "pn": "",
   "abstract": [
    "The combination of multiple sources of information has been an attractive approach in different areas. That is the case of speech recognition area where several combination methods have been presented. Our hybrid MLP/HMM systems use acoustic models based on different set of features and different MLP classifier structures. In this work we developed a method combining phoneme probabilities generated by the different acoustic models trained on distinct feature extraction processes. Two different algorithms were implemented for combining the acoustic models probabilities. The first covers the combination in the probability domain and the second one in the log-probability domain. We made combinations of two and three alternative baseline systems where was possible to obtain relative improvements on word error rate larger than 20% for a large vocabulary speaker independent continuous speech recognition task.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-423"
  },
  "leeuwen00_icslp": {
   "authors": [
    [
     "David A. van",
     "Leeuwen"
    ],
    [
     "Sander J. van",
     "Wijngaarden"
    ]
   ],
   "title": "Automatic speech recognition of non-native speakers using consonant-vowel-consonant (CVC) words",
   "original": "i00_2935",
   "page_count": 4,
   "order": 425,
   "p1": "vol. 2, 935-938",
   "pn": "",
   "abstract": [
    "In this study we investigate whether non-native speakers using a speech recognition system would benefit from phone models of their own native language. For Dutch as the target recognition language, we found that American speakers do not in general benefit from American English models when speaking Dutch. However, using a CVC test methodology, we can conclude that for a certain level of proficiency, and for certain phones, there is a small beneficial e\u000bect of using American acoustic models for American non-native speakers of Dutch.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-424"
  },
  "zhao00c_icslp": {
   "authors": [
    [
     "Gang",
     "Zhao"
    ],
    [
     "Hong",
     "Xu"
    ]
   ],
   "title": "Understanding Chinese in spoken dialogue systems",
   "original": "i00_2939",
   "page_count": 6,
   "order": 426,
   "p1": "vol. 2, 939-942",
   "pn": "",
   "abstract": [
    "This paper presents research on semantic interpretation in a spoken dialogue system. The Template-based Semantic Interpreter is developed to perform robust understanding of Chinese in an e-mail reading dialogue system which understands the users commands and performs operations on e-mails. It is based on a robust parser of Chinese developed for this purpose. Experiments have been made with the combination of speech recognition and interpretation to show the robustness of the interpreter.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-425"
  },
  "berthommier00_icslp": {
   "authors": [
    [
     "Frédéric",
     "Berthommier"
    ],
    [
     "Hervé",
     "Glotin"
    ],
    [
     "Emmanuel",
     "Tessier"
    ]
   ],
   "title": "A front-end using the harmonicity cue for speech enhancement in loud noise",
   "original": "i00_2943",
   "page_count": 4,
   "order": 427,
   "p1": "vol. 2, 943-946",
   "pn": "",
   "abstract": [
    "We propose and test a technique for speech enhancement based on the computation of a harmonicity index, which is non linearly related to the SNR. We assume this method is close to \"segregation\" of speech and noise and it follows the aim of the CASA approach. To carry out the performance evaluation, we quantify the accuracy of reconstruction of the target speech source. We vary factors including the size of the time-frequency regions in which the enhancement process is applied and the use of demodulation. We conclude that these factors have little effect on reconstruction accuracy, but demodulation improves the reconstruction and a process applied in 4 sub-bands with 128 ms time frame-duration is satisfactory. Then, using a HMM/ANN model, we evaluate the recognition scores in comparison with those obtained with unprocessed noisy speech, J-RASTA-PLP pre-processing and training with a clean signal. A gain of 3-4dB is observed in loud noise with GWN, and 3dB with car noise, at WER=65%. We obtain the best gains after training with clean processed speech, but a significant gain is also obtained without such training.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-426"
  },
  "zhou00c_icslp": {
   "authors": [
    [
     "Qiru",
     "Zhou"
    ],
    [
     "Sergey",
     "Kosenko"
    ]
   ],
   "title": "Lucent automatic speech recognition: a speech recognition engine for internet and telephony srvice applications",
   "original": "i00_2947",
   "page_count": 4,
   "order": 428,
   "p1": "vol. 2, 947-950",
   "pn": "",
   "abstract": [
    "Based on Bell Labs speech recognition and understanding technology, we developed LASR3 (Lucent Automatic Speech Recognition, Version 3), a speaker independent, software-based continuous speech recognition engine. It is compatible with Microsoft Speech Application Programming Interface (MS SAPI). LASR3 provides support for desktop, telephony, and internet applications requiring speech recognition. It is a multilingual, scalable multi-channel speech recognition engine. LASR3 runs on 32-bit Microsoft Windows platforms. This paper discusses the LASR3 algorithms, features, and architecture design for scalable multi-channel speech recognition.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-427"
  },
  "stephenson00_icslp": {
   "authors": [
    [
     "Todd A.",
     "Stephenson"
    ],
    [
     "Hervé",
     "Bourlard"
    ],
    [
     "Samy",
     "Bengio"
    ],
    [
     "Andrew C.",
     "Morris"
    ]
   ],
   "title": "Automatic speech recognition using dynamic bayesian networks with both acoustic and articulatory variables",
   "original": "i00_2951",
   "page_count": 4,
   "order": 429,
   "p1": "vol. 2, 951-954",
   "pn": "",
   "abstract": [
    "Current technology for automatic speech recognition (ASR) uses hidden Markov models (HMMs) that recognize spoken speech using the acoustic signal. However, no use is made of the causes of the acoustic signal: the articulators. We present here a dynamic Bayesian network (DBN) model that utilizes an additional variable for representing the state of the articulators. A particular strength of the system is that, while it uses measured articulatory data during its training, it does not need to know these values during recognition. As Bayesian networks are not used often in the speech community, we give an introduction to them. After describing how they can be used in ASR, we present a system to do isolated word recognition using articulatory information. Recognition results are given, showing that a system with both acoustics and inferred articulatory positions performs better than a system with only acoustics.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-428"
  },
  "das00_icslp": {
   "authors": [
    [
     "Subrata",
     "Das"
    ],
    [
     "David",
     "Lubensky"
    ]
   ],
   "title": "Towards robust telephony speech recognition in office and automobile environments",
   "original": "i00_2955",
   "page_count": 4,
   "order": 430,
   "p1": "vol. 2, 955-958",
   "pn": "",
   "abstract": [
    "This study is concerned with improving the robustness of our telephony speech recognition system. Our previous implementation of this system handled both landline and cellular speech produced in a relatively quiet environment, such as in a regular o\u000ece. However, it was found to be unduly vulnerable to background noise. In particular, we wanted to improve the accuracy of the system in the environment of a moving automobile, without impacting its performance in a quieter background. We collected samples of automobile noise under various operating conditions. We used these noise files to artificially generate noisy training data. We applied MAP adaptation procedure to study the value of such noisy training data and found that it helped to improve the robustness of our telephony speech recognition system. In a related study, we also investigated performance scores as a function of the total number of vocabulary entries in the system, as this has implications for a practical system implementation.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-429"
  },
  "kojima00_icslp": {
   "authors": [
    [
     "Hiroaki",
     "Kojima"
    ],
    [
     "Kazuyo",
     "Tanaka"
    ]
   ],
   "title": "Extracting phonological chunks based on piecewise linear segment lattices",
   "original": "i00_2959",
   "page_count": 4,
   "order": 431,
   "p1": "vol. 2, 959-962",
   "pn": "",
   "abstract": [
    "The task of our research is to form phone-like models and a phoneme-like set from spoken word samples without using any transcriptions except for the lexical identification of each word in a vocabulary. This framework is derived from two motivations: 1) automatic design of optimal speech recognition units and structures of phone models, and 2) multi-lingual speech recognition based on language-independent intermediate phonetic codes. The procedure consists of two steps: 1) constructing a VQ codebook of sub-phonetic segments from speech samples, and 2) extracting phonological chunks from sequences of the codes. Segment model is represented with \"piecewise linear segment lattice\" model, which is a lattice structure of segments, each of which is represented as regression coefficients of feature vectors within the segment. Phonological chunks are extracted with a criterion based on Kullback- Leibler divergence between the distribution of individual VQ codes. The recognition rate yields approximately 90% on the 1542 words task with 128 VQ codes.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-430"
  },
  "galescu00b_icslp": {
   "authors": [
    [
     "Lucian",
     "Galescu"
    ],
    [
     "James",
     "Allen"
    ]
   ],
   "title": "Evaluating hierarchical hybrid statistical language models",
   "original": "i00_2963",
   "page_count": 4,
   "order": 432,
   "p1": "vol. 2, 963-966",
   "pn": "",
   "abstract": [
    "We introduce in this paper a hierarchical hybrid statistical language model, represented as a collection of local models plus a general model that binds together the local ones. The model provides a unified framework for modelling language both above and below the word level, and we exemplify with models of both kinds for a large vocabulary task domain. To our knowledge this is the first paper to report an extensive evaluation of the improvements achieved from the use of local models within a hierarchical framework in comparison with a conventional word-based trigram model.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-431"
  },
  "ogata00_icslp": {
   "authors": [
    [
     "Jun",
     "Ogata"
    ],
    [
     "Yasuo",
     "Ariki"
    ]
   ],
   "title": "An efficient lexical tree search for large vocabulary continuous speech recognition",
   "original": "i00_2967",
   "page_count": 4,
   "order": 433,
   "p1": "vol. 2, 967-970",
   "pn": "",
   "abstract": [
    "This paper describes an efficient search algorithm for a high speed and high accuracy LVCSR system. A conventionally used lexical tree search is an efficient method, but has a problem in incorporating the language probability. To solve this problem, we propose in this paper a newefficient search algorithm incorporating the language model structure. In our developed LVCSR, 2-pass search algorithm is adopted to produce a word graph as an intermediate expression. The experimental results on the 20,000-word Japanese dictation task showed that the proposed method can reduce approximately half of the processing time without increasing any errors.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-432"
  },
  "jia00_icslp": {
   "authors": [
    [
     "Bin",
     "Jia"
    ],
    [
     "Xiaoyan",
     "Zhu"
    ],
    [
     "Yupin",
     "Luo"
    ],
    [
     "Dongcheng",
     "Hu"
    ]
   ],
   "title": "Reliability evaluation of speech recognition in acoustic modeling",
   "original": "i00_2971",
   "page_count": 4,
   "order": 434,
   "p1": "vol. 2, 971-974",
   "pn": "",
   "abstract": [
    "In this paper, we present a new method for reliability evaluation of speech recognition in acoustic modeling. The new method incorporates the Integrated Model (IM), which is trained by all the speech data. For close-set verification, the IM method has theoretically the lowest equal error rate; and for open-set, the method often performs well. At the same time, it costs much less computation than other verification methods commonly used. Experiment results show the method is feasible.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-433"
  },
  "xu00c_icslp": {
   "authors": [
    [
     "Ching X.",
     "Xu"
    ]
   ],
   "title": "Using GMM for voiced/voiceless segmentation and tone decision in Mandarin continuous speech recognition",
   "original": "i00_2975",
   "page_count": 4,
   "order": 435,
   "p1": "vol. 2, 975-978",
   "pn": "",
   "abstract": [
    "In this paper, methods of Gaussian Mixture Model (GMM) are presented for both silence/voiced/voiceless segmentation and tone decision in Mandarin continuous speech recognition system. GMM has been used for silence/voiced/voiceless segmentation before, but the feature parameters can be modified to improve both accuracy and speed. As a popular method in pattern recognition, GMM is first proposed for tone decision. The two GMMs used are proved to be capable and potential.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-434"
  },
  "yim00_icslp": {
   "authors": [
    [
     "Chi H.",
     "Yim"
    ],
    [
     "Oscar C.",
     "Au"
    ],
    [
     "Wanggen",
     "Wan"
    ],
    [
     "Cyan L.",
     "Keung"
    ],
    [
     "Carrson C.",
     "Fung"
    ]
   ],
   "title": "Auditory spectrum based features (ASBF) for robust speech recognition",
   "original": "i00_2979",
   "page_count": 4,
   "order": 436,
   "p1": "vol. 2, 979-982",
   "pn": "",
   "abstract": [
    "MFCC are features commonly used in speech recognition systems today. The recognition accuracy of systems using MFCC is known to be high in clean speech environment, but it drops greatly in noisy environment. In this paper, we propose new features called the auditory spectrum based features (ASBF) that are based on the cochlear model of the human auditory system. These new features can track the formants and the selection scheme of these features is based on the second order difference cochlear model and the primary auditory nerve processing model. In our experiment, the performance of MFCC and the ASBF are compared in clean and noisy environments. The results suggest that the ASBF are much more robust to noise than MFCC.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-435"
  },
  "chang00_icslp": {
   "authors": [
    [
     "Eric",
     "Chang"
    ],
    [
     "Jianlai",
     "Zhou"
    ],
    [
     "Shuo",
     "Di"
    ],
    [
     "Chao",
     "Huang"
    ],
    [
     "Kai-Fu",
     "Lee"
    ]
   ],
   "title": "Large vocabulary Mandarin speech recognition with different approaches in modeling tones",
   "original": "i00_2983",
   "page_count": 4,
   "order": 437,
   "p1": "vol. 2, 983-986",
   "pn": "",
   "abstract": [
    "Large vocabulary continuous Mandarin speech recognition has been an important problem for speech recognition researchers for several reasons [1], [2]. First of all, it is a tonal language that requires special treatment for the modeling of tones. There are five tones in Mandarin which are necessary to disambiguate between confusable words. Secondly, the difficulty of entering Chinese by keyboard presents a great opportunity for speech recognition to improve computer usability. Previous approaches to modeling tones have included using a separate tone classifier [1] and incorporating pitch directly into the feature vector [2]. In this paper, we describe a large vocabulary Mandarin speech recognition system based on Microsofts Whisper system. Several alternatives in modeling tones and their error rates on continuous speech are compared.\n",
    "The experimental result shows a character error rate of 7.32% on a test set of 50 speakers and 1000 sentences when no special tone processing is performed in the acoustic model. When the final syllable model set is expanded to include tones, the error rate drops to 6.43% (error rate reduction of 12.2%). When pitch information and the larger final syllable set are used in combination, the error rate is 6.03% (cumulative error rate reduction of 17.6%). This result suggests that other sources of information such as energy and duration can also contribute toward disambiguating between different tones.\n",
    "s Lee L. S., et. al, \"Golden Mandarin - A Real Time Mandarin Speech Dictation Machine for Chinese Language with Very Large Vocabulary\", IEEE Trans. on Speech and Audio Processing, Vol. 1, NO. 2, pp 158-179, April 1993. Chen C. J., et. al., \"New Methods in Continuous Mandarin Recognition\", Proc. Eurospeech 97, Volume 3, pages 1543-1546.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-436"
  },
  "georgila00b_icslp": {
   "authors": [
    [
     "Kalirroi",
     "Georgila"
    ],
    [
     "Kyriakos",
     "Sgarbas"
    ],
    [
     "Nikos",
     "Fanotakis"
    ],
    [
     "George",
     "Kokkinakis"
    ]
   ],
   "title": "Fast very large vocabulary recognition based on compact DAWG-structured language models",
   "original": "i00_2987",
   "page_count": 4,
   "order": 438,
   "p1": "vol. 2, 987-990",
   "pn": "",
   "abstract": [
    "In this paper we present a method for building compact lattices for very large vocabularies, which has been applied to surname recognition in an Interactive telephone-based Directory Assistance Services svstem. The method involves the construction of a non-deterministic DAWG, which is eventually transformed into a phoneme lattice in Entropics HTK Application Programming Interface (HAPI) format. Incremental construction functions are used for the creation and update of the DAWG, whereas an algorithm for converting the DAWG into the HAPI format is presented. Furthermore, trees, graphs, and full-forms (whole words with no merging of nodes) are compared in a straightforward way under the same conditions, using the same decoder (HAPI MVX) and the same vocabularies. Experimental results showed that as we go from full-form lexicons to trees and then to graphs the size of the recognition network is reduced and therefore the recognition time too. However, recognition accuracy is retained since the same phoneme combinations are involved.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-437"
  },
  "eklund00_icslp": {
   "authors": [
    [
     "Robert",
     "Eklund"
    ]
   ],
   "title": "Crosslinguistic disfluency modeling: a comparative analysis of Swedish and tok pisin human-human ATIS dialogues",
   "original": "i00_2991",
   "page_count": 5,
   "order": 439,
   "p1": "vol. 2, 991-994",
   "pn": "",
   "abstract": [
    "This paper studies disfluencies in authentic humanhuman dialogues in Swedish and Tok Pisin. It is found that while there are no major differences as to types or frequencies on a macro level, there are dissimilarities on a micro level, notably in the characteristics of how prolonged segments are realized. The paper also discusses the results in the light of reported disfluencies in English, German, Ilokano and Tagalog.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-438"
  },
  "terashima00_icslp": {
   "authors": [
    [
     "Shiro",
     "Terashima"
    ],
    [
     "Kazuya",
     "Takeda"
    ],
    [
     "Fumitada",
     "Itakura"
    ]
   ],
   "title": "Vector space representation of language probabilities through SVD of n-gram matrix",
   "original": "i00_2995",
   "page_count": 5,
   "order": 440,
   "p1": "vol. 2, 995-998",
   "pn": "",
   "abstract": [
    "In this paper we introduce the vector space representation of the N-gram language model where vectors of K dimensions are given to both words and contexts, i.e., an N-1 word sequence, so that the scalar product of a word vector and a context vector gives the corresponding N-gram probability. The vector space representation is obtained from singular value decomposition (SVD) of the co-occurrence frequency matrix (CFM) of the context and the word. The effectiveness of the proposed representation is examined by determining how the number of N-gram parameters can be reduced through clustering and truncation of the dimensions defined on the given vector space. From the experimental results, it is confirmed that the number of model parameters can be reduced to less than 17.5% of the original number of model parameters and the proposed method is more effective than the word clustering method based on mutual information.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-439"
  },
  "kato00b_icslp": {
   "authors": [
    [
     "Yoshihide",
     "Kato"
    ],
    [
     "Shigeki",
     "Matsubara"
    ],
    [
     "Katsuhiko",
     "Toyama"
    ],
    [
     "Yasuyoshi",
     "Inagaki"
    ]
   ],
   "title": "Spoken language parsing based on incremental disambiguation",
   "original": "i00_2999",
   "page_count": 4,
   "order": 441,
   "p1": "vol. 2, 999-1002",
   "pn": "",
   "abstract": [
    "Towards a real-time spoken dialogue system, several incremental parsing methods have been proposed so far. They construct syntactic structures for an initial fragment of an input sentence. However, they have a problem that the structures do not necessarily represent the syntactic relation correctly. The problem is caused by the ambiguity of initial fragments. This paper proposes an incremental disambiguation method, which decides correct structures at an stage where the entire input is not completed. The method finds the structures which are correct independently of the remaining input. When correct structures cannot be decided, the method delays the decision. Since the disambiguation is executed for every word input, the method can find correct structures at an early stage.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-440"
  },
  "shimodaira00_icslp": {
   "authors": [
    [
     "Hiroshi",
     "Shimodaira"
    ],
    [
     "Yutaka",
     "Kato"
    ],
    [
     "Toshihiko",
     "Akae"
    ],
    [
     "Mitsuru",
     "Nakai"
    ],
    [
     "Shigeki",
     "Sagayama"
    ]
   ],
   "title": "Jacobian adaptation of HMM with initial model selection for noisy speech recognition",
   "original": "i00_2a03",
   "page_count": 4,
   "order": 442,
   "p1": "vol. 2, 1003-1006",
   "pn": "",
   "abstract": [
    "An extension of Jacobian Adaptation (JA) of HMMs for degraded speech recognition is presented in which appropriate set of initial models is selected from a number of initial-model sets designed for different noise environments. Based on the first order Taylor series approximation in the acoustic feature domain, JA adapts the acoustic model parameters trained in the initial noise environment A to the new environment B much faster than PMC that creates the acoustic models for the target environment from scratch. Despite the advantage of JA to PMC, JA has a theoretical limitation that the change of acoustic parameters from the environment A to B should be small in order that the linear approximation holds. To extend the coverage of JA, the ideas of multiple sets of initial models and their automatic selection scheme are discussed. Speaker-dependent isolated-word recognition experiments are carried out to evaluate the proposed method.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-441"
  },
  "shu00_icslp": {
   "authors": [
    [
     "Han",
     "Shu"
    ],
    [
     "Chuck",
     "Wooters"
    ],
    [
     "Owen",
     "Kimball"
    ],
    [
     "Thomas",
     "Colthurst"
    ],
    [
     "Fred",
     "Richardson"
    ],
    [
     "Spyros",
     "Matsoukas"
    ],
    [
     "Herbert",
     "Gish"
    ]
   ],
   "title": "The BBN Byblos 2000 conversational Mandarin LVCSR system",
   "original": "i00_2a07",
   "page_count": 4,
   "order": 443,
   "p1": "vol. 2, 1007-1010",
   "pn": "",
   "abstract": [
    "This paper describes the year 2000 BBN Byblos Mandarin large vocabulary conversational speech recognition (LVCSR) system, the winning (and only) Mandarin system from the Spring 2000 Hub-5 evaluation sponsored by NIST. We first outline the training and decoding procedures used in the system, and describe the performance of the system used in the evaluation. We then describe the effect of several features that were not in the evaluation system but have been added since, including Jacobian compensated Vocal Tract Length Normalization (VTLN), system combination, a higher number of system parameters, and additional training data. Together these give an additional 5.4% relative improvement on character error rate (CER) from the evaluation system.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-442"
  },
  "colthurst00_icslp": {
   "authors": [
    [
     "Thomas",
     "Colthurst"
    ],
    [
     "Owen",
     "Kimball"
    ],
    [
     "Fred",
     "Richardson"
    ],
    [
     "Han",
     "Shu"
    ],
    [
     "Chuck",
     "Wooters"
    ],
    [
     "Rukmini",
     "Iyer"
    ],
    [
     "Herbert",
     "Gish"
    ]
   ],
   "title": "The 2000 BBN Byblos LVCSR system",
   "original": "i00_2a11",
   "page_count": 5,
   "order": 444,
   "p1": "vol. 2, 1011-1014",
   "pn": "",
   "abstract": [
    "This paper describes the 2000 BBN Byblos Large Vocabulary Continuous Speech Recognition (LVCSR) system. We briefly outline the training and decoding procedures used in the system, and explain in detail the new features we have added to the system in the past year. These new features include multiple adaptation stages, parallel path rescoring, and a new word confidence system. Word error rate results for all of these additions are presented for Hub-5 English test sets containing both Switchboard II and CallHome speakers.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-443"
  },
  "chen00i_icslp": {
   "authors": [
    [
     "Langzhou",
     "Chen"
    ],
    [
     "Lori",
     "Lamel"
    ],
    [
     "Gilles",
     "Adda"
    ],
    [
     "Jean-Luc",
     "Gauvain"
    ]
   ],
   "title": "Broadcast news transcription in Mandarin",
   "original": "i00_2a15",
   "page_count": 4,
   "order": 445,
   "p1": "vol. 2, 1015-1018",
   "pn": "",
   "abstract": [
    "In this paper, our work in developing a Mandarin broadcast news transcription system is described. The main focus of this work is a port of the LIMSI American English broadcast news transcription system to the Chinese Mandarin language. The system consists of an audio partitioner and an HMM-based continuous speech recognizer. The acoustic models were trained on about 24 hours of data from the 1997 Hub4 Mandarin corpus available via LDC. In addition to the transcripts, the language models were trained on Mandarin Chinese News Corpus containing about 186 million characters. We investigate recognition performance as a function of lexical size, with and without tone in the lexicon, and with a topic dependent language model. The transcription character error rate on the DARPA 1997 test set is 18.1% using a lexicon with 3 tone levels and a topic-based language model.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-444"
  },
  "li00g_icslp": {
   "authors": [
    [
     "Yang",
     "Li"
    ],
    [
     "Tong",
     "Zhang"
    ],
    [
     "Stephen E.",
     "Levinson"
    ]
   ],
   "title": "Word concept model: a knowledge representation for dialogue agents",
   "original": "i00_2a19",
   "page_count": 4,
   "order": 446,
   "p1": "vol. 2, 1019-1022",
   "pn": "",
   "abstract": [
    "Information extraction is a key component in dialogue systems. Knowledge about the world as well as knowledge specific to each word should be used for robust semantic processing. An intelligent agent is necessary for a dialogue system when meanings are strictly defined by using a world state model. A layered concept structure is proposed to represent knowledge associated with each word in a \"speech-friendly\" way. By considering knowledge stored in the word concept model as well as knowledge base of the world model, meaning of a given sentence can be correctly identified. This paper describes the layered concept structure and how knowledge about words can be stored in this concept model.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-445"
  },
  "miyajima00_icslp": {
   "authors": [
    [
     "Chiyomi",
     "Miyajima"
    ],
    [
     "Keiichi",
     "Tokuda"
    ],
    [
     "Tadashi",
     "Kitamura"
    ]
   ],
   "title": "Audio-visual speech recognition using MCE-based hmms and model-dependent stream weights",
   "original": "i00_2a23",
   "page_count": 4,
   "order": 447,
   "p1": "vol. 2, 1023-1026",
   "pn": "",
   "abstract": [
    "This paper presents a framework for designing a hidden Markov model (HMM)-based audio-visual automatic speech recognition (ASR) system based on minimum classification error training. Audio/visual HMM parameters are optimized with the generalized probabilistic descent (GPD) method, and their likelihoods are combined using model-dependent stream weights which are also estimated with the GPD method. Experimental results of speaker independent isolated word recognition show that the audiovisual ASR performance is significantly improved by the GPD optimization of audio and visual HMMs and the introduction of model-dependent stream weights, resulting in 47% 81% error reduction over a conventional system which consists of HMMs trained based on the maximum likelihood criterion and globally-tied stream weights estimated with the GPD method.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-446"
  },
  "nanjo00_icslp": {
   "authors": [
    [
     "Hiroaki",
     "Nanjo"
    ],
    [
     "Akinobu",
     "Lee"
    ],
    [
     "Tatsuya",
     "Kawahara"
    ]
   ],
   "title": "Automatic diagnosis of recognition errors in large vocabulary continuous speech recognition systems",
   "original": "i00_2a27",
   "page_count": 4,
   "order": 448,
   "p1": "vol. 2, 1027-1030",
   "pn": "",
   "abstract": [
    "Automatic diagnosis of recognition errors in large vocabulary continuous speech recognition (LVCSR) systems is addressed. It consists of two steps. The first step is to identify the module that causes recognition errors for every erroneous segment. This statistics points out which modules to be revised. The second step is to analyze the causes of the errors in detail. Specifically, the triphone and N-gram entries related to the errors are listed. The diagnostic information provides directions for improvement. This diagnosis has been applied to three LVCSR systems: read speech dictation system, lecture speech transcription system and dialogue speech recognition system. We have observed different and interesting diagnosis results. In the dictation system, the diagnosis is useful for improving our decoder Julius. In the lecture and dialogue speech recognition systems, problems in acoustic and language modeling are made clear.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-447"
  },
  "chiang00_icslp": {
   "authors": [
    [
     "Yuang-Chin",
     "Chiang"
    ],
    [
     "Zhi-Siang",
     "Yang"
    ],
    [
     "Ren-Yuan",
     "Lyu"
    ]
   ],
   "title": "Taiwanese corpus collection via continuous speech recognition tool",
   "original": "i00_2a31",
   "page_count": 4,
   "order": 449,
   "p1": "vol. 2, 1031-1034",
   "pn": "",
   "abstract": [
    "Corpora, in their different forms for different purposes, have been the bases for modern natural language processing technology. Taiwanese (MinNan), as other language members in the Sino-Tibet family, has been marginalized due to many reasons. One of the consequences of this marginalization is that no standard written script exists, and thus collecting corpus for these languages has been extremely difficult. By (almost) arbitrarily selecting the hanlor written script (mixture of hanzi and roman characters), we are still facing the problem that only few people are capable of phonetically transcribing a given Taiwanese text. On the other hand, reading a Taiwanese text is easier due to the existence of many commonly used hanzi. By recording a persons reading of Taiwanese text, we use a continuous speech recognizer for Taiwanese to automatically transcribe the text, and end up with two kinds of corpora, one in text, one in speech. The accuracy of the automatic phonetic transcription is about 96.05% in syllable count. For marginalized languages, this automatic transcription can be very useful for corpus collection if proper error spotting scheme is implemented.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-448"
  },
  "yuan00_icslp": {
   "authors": [
    [
     "Baosheng",
     "Yuan"
    ],
    [
     "Qingwei",
     "Zhao"
    ],
    [
     "Qing",
     "Guo"
    ],
    [
     "Xiangdong",
     "Zhang"
    ],
    [
     "Zhiwei",
     "Lin"
    ]
   ],
   "title": "Optimal maximum likelihood on phonetic decision tree acoustic model for LVCSR",
   "original": "i00_2a35",
   "page_count": 3,
   "order": 450,
   "p1": "vol. 2, 1035-1038",
   "pn": "",
   "abstract": [
    "This paper introduces a method that can better maximize likelihood (ML) in state decision tree clustering under a continuous density hidden Markov model (CDHMM) framework. Under ML criterion, the conventional phonetic context rule based triphone clustering process is re-examined by checking the fitness for each triphone cluster within its tree node class clustered by its yes/no answer to the phonetic context questions. If a triphone within its class better fits the other class (in a certain degree) by the ML standard, then its class-membership is re-assigned into the better-fit class. This method, applied at every level of three node during the tree building process, can improve the overall likelihood of the tree therefore should help to improve system performance at the end. Comparison experiment shows that the proposed method cuts word error rate (WER) by 6% to 11.2% from 11.9% obtained by conventional decision tree on WSJ 20k task.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-449"
  },
  "markov00_icslp": {
   "authors": [
    [
     "Konstantin P.",
     "Markov"
    ],
    [
     "Satoshi",
     "Nakamura"
    ]
   ],
   "title": "Frame level likelihood transformations for ASR and utterance verification",
   "original": "i00_2a38",
   "page_count": 4,
   "order": 451,
   "p1": "vol. 2, 1038-1041",
   "pn": "",
   "abstract": [
    "In most of the current speech recognition systems based on HMM, existing decoding and utterance verification methods make use of state output likelihood as a measure of the acoustic match between the input data and the acoustic models. In this paper, we present a new and more generalized approach to the formation of the acoustic match score. The essence of this approach is to transform the likelihood of each acoustic vector with respect to any particular HMM state according to some non-linear function. We have investigated two types of such transformation functions. The first one, performs likelihood normalization, and the second one transforms likelihoods into exponentially ordered weights. The transformed likelihoods, as new acoustic scores, are used further for decoding, recognition and verification instead of the conventional likelihoods. In our evaluation experiments we used TIMIT database for phoneme recognition and verification and a database of 710 speakers and a total of 4252 distinct words, for iso- lated word recognition and verification. The results we achieved show that the transformed likelihood scores, in average, increase slightly the recognition accuracy and reduce the verification error rates up to 30%.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-450"
  },
  "hazen00_icslp": {
   "authors": [
    [
     "Timothy J.",
     "Hazen"
    ],
    [
     "Theresa",
     "Burianek"
    ],
    [
     "Joseph",
     "Polifroni"
    ],
    [
     "Stephanie",
     "Seneff"
    ]
   ],
   "title": "Integrating recognition confidence scoring with language understanding and dialogue modeling",
   "original": "i00_2a42",
   "page_count": 4,
   "order": 452,
   "p1": "vol. 2, 1042-1045",
   "pn": "",
   "abstract": [
    "In this paper we present a method for integrating confidence scores into the understanding and dialogue components of a speech understanding system. The understanding component of our system receives an n-best list of recognition hypotheses augmented with word-level confidence scores. The confidence scores are used by the understanding component to hypothesize when words in a recognizers n-best list have been misrecognized. The understanding component has the ability to predict the semantic class of misrecognized words based on the surrounding context and also to suggest when key words which may have been misunderstood should be re-confirmed by the user. The output of the understanding component is passed onto a dialogue control component which can act on various suggestions made by the understanding component. To evaluate the system, experiments were conducted using the JUPITER weather information system. Evaluation was performed at the understanding level using key-value pair concept error rate as the evaluation metric. When word confidence scores were integrated into the understanding component, the concept error rate was reduced by 35%.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-451"
  },
  "yu00c_icslp": {
   "authors": [
    [
     "Yibiao",
     "Yu"
    ],
    [
     "Heming",
     "Zhao"
    ]
   ],
   "title": "Speech recognition based on estimation of mutual information",
   "original": "i00_2a46",
   "page_count": 4,
   "order": 453,
   "p1": "vol. 2, 1046-1049",
   "pn": "",
   "abstract": [
    "This paper proposed a pattern matching algorithm based on estimation of mutual information for speech recognition. The preliminary experiments on connected Chinese digits recognition show 97% of test digits were recognized correctly with 4 times much less time consume than DTW or HMM, and it was 6% higher than which gotten by conventional DTW with same experiments data and condition. According to our experiments, the proposed algorithm can be considered as an applicable effective matching method for speech recognition especially for monosyllable word recognition.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-452"
  },
  "guo00_icslp": {
   "authors": [
    [
     "Qing",
     "Guo"
    ],
    [
     "Yonghong",
     "Yan"
    ],
    [
     "Zhiwei",
     "Lin"
    ],
    [
     "Baosheng",
     "Yuan"
    ],
    [
     "Qingwei",
     "Zhao"
    ],
    [
     "Jian",
     "Liu"
    ]
   ],
   "title": "Keyword spotting in auto-attendant system",
   "original": "i00_2a50",
   "page_count": 3,
   "order": 454,
   "p1": "vol. 2, 1050-1052",
   "pn": "",
   "abstract": [
    "In this paper, an auto-attendant system using finite state grammar (FSG) based on a continuous speech recognition (CSR) model is introduced. However, by using two virtual garbage models, one is to match the leading extraneous speech before the key name and the other to match the tailing extraneous speech following the key name, we managed to reach a more flexible and robust auto-attendant system. The experiment result show that, in our auto attendant system (about 240 names), to the name only test set and the sentence test set 1 composed of sentences that FSG can recognize, the recognition rate of the keyword spotting system is almost the same as that of FSG. To the sentence test set 2 composed of sentences that undefined in the FSG the keyword spotting system outperforms the FSG system remarkably. Not affecting the recognition accuracy of name only test set and the sentence test set 1, task dependent keyword models cut off additional 20% of error rate comparing with task independent keyword models in the sentence test set 2.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-453"
  },
  "ren00_icslp": {
   "authors": [
    [
     "Weimin",
     "Ren"
    ],
    [
     "Chengfa",
     "Wang"
    ],
    [
     "Wen",
     "Gao"
    ],
    [
     "Jinpei",
     "Xu"
    ]
   ],
   "title": "A new approach for modeling OOV words",
   "original": "i00_2a53",
   "page_count": 4,
   "order": 455,
   "p1": "vol. 2, 1053-1056",
   "pn": "",
   "abstract": [
    "This paper addressed the problem of Out-Of-Vocabulary (OOV) utterance detection in small vocabulary telephone keyword spotting system. We propose a new approach for modeling OOV words in the scenario of a small vocabulary of telephone keyword spotting system. The paper adopt the semi-continuous Hidden Markov Model with multiple codebooks to modeling the keywords. We propose a two pass procedure to spot the real keyword occurrence. In the first pass, the normal viterbi search procedure is applied, with the appropriate defined and trained garbage models and silence models. The output of this stage produces the N-best word hypothesis The second pass, which can be seen as a verification procedure, take the first pass output as focuses. This approach is mainly constructing a \"dynamic anti-model\" based on the detected hypothesis keyword model and the current input acoustic information.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-454"
  },
  "elmeliani00_icslp": {
   "authors": [
    [
     "Rachida",
     "El Méliani"
    ],
    [
     "Douglas",
     "O'Shaughnessy"
    ]
   ],
   "title": "Speech recognition using error spotting",
   "original": "i00_2a57",
   "page_count": 4,
   "order": 456,
   "p1": "vol. 2, 1057-1060",
   "pn": "",
   "abstract": [
    "Spontaneous conversational phone-call speech databases are di\u000ecult to recognize because of the large variation of speech rates, of pronunciations as well as noises, of acoustic degradations from the telephone channel, and of an unpredictable non-grammatical language structure including many random phenomena. Each cause of misrecognition can be addressed separately; however there is still no satisfying solution. As a misrecognition is considered by the system as being a kind of new word, we propose to apply here our keyword spotting and new-word detection design. However because of the large variety of types of misrecognitions and of the lack of information on where, why and how they occur, we had to define a di\u000berent language model than those used in previous work. Results show for most of the processed files a noticeable improvement in the recognition rate, often associated with a decrease in the number of substitutions and a slight increase in the number of the deletions.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-455"
  },
  "yang00d_icslp": {
   "authors": [
    [
     "Chung-Ho",
     "Yang"
    ],
    [
     "Ming-Shiun",
     "Hsieh"
    ]
   ],
   "title": "Robust endpoint detection for in-car speech recognition",
   "original": "i00_2a61",
   "page_count": 4,
   "order": 457,
   "p1": "vol. 2, 1061-1064",
   "pn": "",
   "abstract": [
    "The endpoint detection plays a significantly important role in the front end processing of speech recognition. It is very difficult, however, to precisely locate endpoints on the input utterance to be free on non-speech regions because of unpredictable background noise. This paper proposes a novel approach that finds robust features for better endpoint detection in a noisy incar environment. In the proposed method, we integrate both the widely used energy and entropy to form a new feature that possesses advantages of each individual while compensating the drawback of each other. By monitoring the variation of the extracted new features, more precise endpoints can be found. Experimental results present that this algorithm outperforms the energy-based algorithms in both accuracy of boundary point detection and recognition performance under a real in-car noisy environment. The result of accuracy improvement shows 10% higher comparing with energy-based algorithm.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-456"
  },
  "miwa00_icslp": {
   "authors": [
    [
     "Jouji",
     "Miwa"
    ],
    [
     "Masaru",
     "Kumagai"
    ]
   ],
   "title": "Internet speech analysis system using e-mail and web technology",
   "original": "i00_2a65",
   "page_count": 4,
   "order": 458,
   "p1": "vol. 2, 1065-1068",
   "pn": "",
   "abstract": [
    "This paper describes a system for speech analysis on the Internet using E-mail and Web technology called as INSAS/M. A user's speech waveform attached in E-mail is analyzed and converted to an image \fle with the system. Then, the user can easily see the analyzed result in a web page with a browser. The analysis functions are formant locus and section dis- played by LPC analysis, fundamental frequencies and so on. The system can be used at any time, in anywhere and for anyone so that it is very suitable for a phonetician and a busy language learner in the world.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-457"
  },
  "loog00_icslp": {
   "authors": [
    [
     "Marco",
     "Loog"
    ],
    [
     "Reinhold",
     "Haeb-Umbach"
    ]
   ],
   "title": "Multi-class linear dimension reduction by generalized Fisher criteria",
   "original": "i00_2a69",
   "page_count": 4,
   "order": 459,
   "p1": "vol. 2, 1069-1072",
   "pn": "",
   "abstract": [
    "Linear Disciminant Analysis is in general unable to find the lower-dimensional feature space which maximizes the class discrimination, even if the class distributions can be assumed to be very simple, e.g. Gaussians with identical covariance matrices. In this paper we reformulate the K-class Fisher criterion as a sum of K(K-1)/2 two-class Fisher criteria. This formulation allows to weigh class pair contributions according to their relevance for classification. Further it offers an obvious way how to cope with heteroscedastic models. We propose a particular weighting scheme which attempts to approximate the pairwise Bayes error. Moderate improvements are obtained on the TIMIT phoneme classification task.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-458"
  },
  "holmes00_icslp": {
   "authors": [
    [
     "Wendy J.",
     "Holmes"
    ]
   ],
   "title": "Improving the representation of time structure in front-ends for automatic speech recognition",
   "original": "i00_2a73",
   "page_count": 4,
   "order": 460,
   "p1": "vol. 2, 1073-1076",
   "pn": "",
   "abstract": [
    "This paper describes investigations into the use of excitation-synchronous spectral analysis to provide acoustic features for automatic speech recognition. Within each 10 ms frame the region of maximum power is located and used as the centre for the window in a subsequent Fourier transform. The method has been found to be effective in locating stop bursts and vocal-tract responses to glottal closures. This excitation-synchronous analysis has been compared with the more conventional fixed-interval analysis for window lengths ranging from 5 to 25 ms. In connected-digit recognition experiments using mel-cepstrum features, the excitation-synchronous analysis with a window length of 10 ms gave a 10% improvement in recognition performance when compared with the best of the fixed-window conditions.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-459"
  },
  "kirchhoff00_icslp": {
   "authors": [
    [
     "Katrin",
     "Kirchhoff"
    ]
   ],
   "title": "Speech analysis by rule extraction from trained artificial neural networks",
   "original": "i00_2a77",
   "page_count": 4,
   "order": 461,
   "p1": "vol. 2, 1077-1080",
   "pn": "",
   "abstract": [
    "A recent development in feature extraction is the use of neural network feature extractors, where the parameterized signal is passed through a neural network trained to discriminate between targets representing e.g. different phone classes or speakers. While the transformed feature representation often enhances class discriminability and thereby overall performance, the transformation performed by the network cannot directly be interpreted by human experts. However, explicit knowledge about this transformation could lead to the definition of a simpler function on the input features which might eventually be incorporated into the basic parameterization method. In this paper we investigate a rule extraction technique for transforming the trained network into a set of if-then rules capable of representing the transformation in a more transparent way, and apply it to the problem of distinguishing between the English fricative classes /f,v/ and /s,z/ from the TIMIT and OGI Numbers95 speech corpora.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-460"
  },
  "venugopal00_icslp": {
   "authors": [
    [
     "Jaishree",
     "Venugopal"
    ],
    [
     "Stephen A.",
     "Zahorian"
    ],
    [
     "Montri",
     "Karnjanadecha"
    ]
   ],
   "title": "Minimum mean square error spectral peak envelope estimation for automatic vowel classification",
   "original": "i00_2a81",
   "page_count": 4,
   "order": 462,
   "p1": "vol. 2, 1081-1084",
   "pn": "",
   "abstract": [
    "Spectral feature computations continue to be a very difficult problem for accurate machine recognition of vowels especially in the presence of noise or for otherwise degraded acoustic signals. In this work, a new peak envelope method for vowel classification is developed, based on a missing frequency components model of speech recognition. According to this model, vowel recognition depends only on the location of spectral peaks. Also, smoothing and interpolation of the sampled spectra, performed in the cepstral analysis method commonly used in automatic speech recognition results in a loss of valuable information. The new method for feature extraction presented in this paper is based on minimum mean square error curve fitting of cosine-like basis vectors to all peaks in the speech spectrum. A mathematical model for smoothly tracking spectral envelopes using only spectral peak information and ignoring other parts of the spectrum is presented. A software algorithm for the model was developed and tested for various speaker types using a neural network classifier. Vowel classification experiments were conducted based on the features derived from the spectral peaks. The classification rates of the peak method under various signal to noise ratios was also evaluated. The basic conclusion is that the new features perform the same as cepstral features for clean speech, but have advantages when the signal is degraded by noise.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-461"
  },
  "keung00_icslp": {
   "authors": [
    [
     "Cyan L.",
     "Keung"
    ],
    [
     "Oscar C.",
     "Au"
    ],
    [
     "Chi H.",
     "Yim"
    ],
    [
     "Carrson C.",
     "Fung"
    ]
   ],
   "title": "Probabilistic compensation of unreliable feature components for robust speech recognition",
   "original": "i00_2a85",
   "page_count": 3,
   "order": 463,
   "p1": "vol. 2, 1085-1087",
   "pn": "",
   "abstract": [
    "Missing feature theory is well studied in robust ASR context, many works have been done on additive noise of different colors. These are based mainly on classical spectral subtraction and marginal density techniques. This paper addresses the problem of temporal distortion of feature components, that is all about time domain instead of frequency one. No specific noise model and extract computation needed. We showed that the digit words recognition rate is above 95%, given test samples are clean with 10dB white noise added to middle 30% portion of speech along the time axis.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-462"
  },
  "wang00k_icslp": {
   "authors": [
    [
     "Congxiu",
     "Wang"
    ],
    [
     "Qihu",
     "Li"
    ],
    [
     "Guoying",
     "Zhao"
    ],
    [
     "Li",
     "Yin"
    ],
    [
     "Shuai",
     "Hao"
    ],
    [
     "Da",
     "Meng"
    ]
   ],
   "title": "A new tone conversion method for Mandarin by an adaptive linear prediction analysis",
   "original": "i00_2a88",
   "page_count": 4,
   "order": 464,
   "p1": "vol. 2, 1088-1091",
   "pn": "",
   "abstract": [
    "Conventional frame-based linear prediction coding(LPC) does not always de-convovle the speech signal into the vocal tract response and the voice source excitation signal exactly. In order to get better results, an adaptive linear prediction coding analysis(ALPC) method is proposed in this paper. The vocal tract responses obtained from ALPC analysis are used to synthesize a vowel syllable that has a different tone. During the synthesizing procedure, the voice source excitations are superseded by the improved LF-4 model, and the pitch-synchronized synthesizing method is used. The hearing experiments indicate that the synthesized speech by ALPC analysis, which has a different tone from the original one, has a high intelligibility and a better voice quality than that by the traditional LPC analysis method.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-463"
  },
  "oviatt00b_icslp": {
   "authors": [
    [
     "Sharon",
     "Oviatt"
    ]
   ],
   "title": "Multimodal interface research: a science without borders",
   "original": "i00_3001",
   "page_count": 6,
   "order": 465,
   "p1": "vol. 3, 1-6",
   "pn": "",
   "abstract": [
    "Multimodal research represents \"Science without Borders\" because it requires combining expertise from different component technologies, academic disciplines, and cultural/international perspectives. It also is rapidly erasing borders as it promotes the increased accessibility of computing for diverse and non-specialist users, and for field and mobile usage environments. This paper reviews two studies that highlight recent advances within the field. It also draws parallels between the multimodal areas of speech/pen and speech/lip movement research. Finally, it indicates new research challenges that will require additional bold border crossings in the near future.\n",
    "In the medical community, there is an international group called Physicians without Borders that many of you undoubtedly are familiar with (URL: http://www.dwb.org/). Physicians without Borders//Medecins sans Frontiers is an organization of volunteer medical personnel who respond to medical needs and emergencies around the world, and who frequently \"cross borders\" to get where theyre going. Personally, I have always viewed them as an inspiration, in part because I like the idea of crossing borders to get where you need to go. Maybe its just the intrigue or element of risk, but Id like to entertain the possibility that theres something more substantial about crossing borders that should appeal to us.\n",
    "Apart from crossing national borders, which we all have just done to arrive here in Beijing for this new millennium meeting of ICSLP2000, there also is the challenge of striving to cross intellectual borders when necessary to achieve rapid scientific progress. To most of us, the advantages of scientific crossfertilization and teamwork are abundantly clear, when accomplished with the right spirit. Multidisciplinary research that involves combining and shifting among very different perspectives can generate a fast-paced and creative style of research, as well as providing an antidote to intellectual insularity and parochialism. One might argue that the very existence of multimodal research, along with the advantages and opportunities that it represents, has been at least a small triumph of intellectual cross-fertilization.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-464"
  },
  "munhall00_icslp": {
   "authors": [
    [
     "K. G.",
     "Munhall"
    ],
    [
     "C.",
     "Kroos"
    ],
    [
     "T.",
     "Kuratate"
    ],
    [
     "J.",
     "Lucero"
    ],
    [
     "M.",
     "Pitermann"
    ],
    [
     "Eric",
     "Vatikiotis-Bateson"
    ],
    [
     "H.",
     "Yehia"
    ]
   ],
   "title": "Studies of audiovisual speech perception using production-based animation",
   "original": "i00_3007",
   "page_count": 4,
   "order": 466,
   "p1": "vol. 3, 7-10",
   "pn": "",
   "abstract": [
    "This paper will summarize our work at Queen's University and ATR Laboratories on cross-modal speech perception and production. Our approach has been to study these two sides of speech together and to use the multi-modal speech production data to parameterize and control audiovisual animation systems. Two approaches to production-based facial animation have been pursued - one statistical and the other physical. In both cases, realistic talking head animations are generated from continuous input of production data. The statistical animation method of AV synthesis extends our multi-linear techniques developed for the analysis of orofacial motion and speech acoustics to include the correlation between measured 3D positions on the face and deformation coefficients of the facial surface. In the physical approach, the dynamic form of the animation is determined by the biophysical characteristics of the animated object. The physical model consists of multiple structural layers: model skull and jaw surfaces, an orofacial muscle layer, and a three-layer polygon model of the soft tissue. In a series of studies using these animation approaches we have examined the conditions under which speech perception in noise is enhanced by simultaneous visual presentation. Our data show a distinction between visual prosody and segmental perception as well as demonstrating that our animated stimuli produce natural increases in speech intelligibility.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-465"
  },
  "neti00_icslp": {
   "authors": [
    [
     "Chalapathi",
     "Neti"
    ],
    [
     "Giridharan",
     "Iyengar"
    ],
    [
     "Gerasimos",
     "Potamianos"
    ],
    [
     "A.",
     "Senior"
    ],
    [
     "Benoit",
     "Maison"
    ]
   ],
   "title": "Perceptual interfaces for information interaction: joint processing of audio and visual information for human-computer interaction",
   "original": "i00_3011",
   "page_count": 4,
   "order": 467,
   "p1": "vol. 3, 11-14",
   "pn": "",
   "abstract": [
    "We are exploiting the human perceptual principle of sensory integration (the joint use of audio and visual information) to improve the recognition of human activity (speech recognition, speech event detection and speaker change), intent (intent to speak) and human identity (speaker recognition), particularly in the presence of acoustic degradation due to noise and channel. In this paper, we present experimental results in a variety of contexts that demonstrate the benefit of joint audio-visual processing.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-466"
  },
  "gao00c_icslp": {
   "authors": [
    [
     "Wen",
     "Gao"
    ],
    [
     "Jiyong",
     "Ma"
    ],
    [
     "Rui",
     "Wang"
    ],
    [
     "Hongxun",
     "Yao"
    ]
   ],
   "title": "Towards robust lipreading",
   "original": "i00_3015",
   "page_count": 6,
   "order": 468,
   "p1": "vol. 3, 15-19",
   "pn": "",
   "abstract": [
    "In this paper, a robust and fast approach to lip detecting and lip-reading is presented. The approach combines the information of lip color with the geometrical features of lips in human face. This technique makes it possible to derive lip regions in real time under regular illumination conditions. The experimental results with more than 2000 images have shown that the approach to locating lips is very efficient both in locating speed and locating accuracy. Recognition tests were conducted on Chinese phrases. The approach achieved an ccuracy of 90% for speaker dependent recognition task.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-467"
  },
  "nakamura00_icslp": {
   "authors": [
    [
     "Satoshi",
     "Nakamura"
    ],
    [
     "Hidetoshi",
     "Ito"
    ],
    [
     "Kiyohiro",
     "Shikano"
    ]
   ],
   "title": "Stream weight optimization of speech and lip image sequence for audio-visual speech recognition",
   "original": "i00_3020",
   "page_count": 5,
   "order": 469,
   "p1": "vol. 3, 20-24",
   "pn": "",
   "abstract": [
    "Bimodal speech recognition systems, with the use of visual information to supplement acoustic information, have been shown to yield better recognition performance than purely acoustic systems, especially when background noise is present. The early integration strategy for HMM-based audio-visual speech recognition is one promising approach, where the output probability is obtaned by product of output probabilites of audio and visual streams. This paper addresses a novel method which optimizes stream weights so as to maximize recognition performance. The proposed method estimates the stream weights based on a normalized log likelihood which is derived by ratio of likelihood of a correct word and highest likelihood of incorrect words. The isolated word recognition experiment results show that the audio-visual speech recognition by proposed method attains 56.2% (10 dB), 55.2% (0dB) and 15.2% (20dB) better performance compared to that only using audio information. The results also show the proposed method can reduce a number of adaptation words.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-468"
  },
  "sako00_icslp": {
   "authors": [
    [
     "Shinji",
     "Sako"
    ],
    [
     "Keiichi",
     "Tokuda"
    ],
    [
     "Takashi",
     "Masuko"
    ],
    [
     "Takao",
     "Kobayashi"
    ],
    [
     "Tadashi",
     "Kitamura"
    ]
   ],
   "title": "HMM-based text-to-audio-visual speech synthesis",
   "original": "i00_3025",
   "page_count": 4,
   "order": 470,
   "p1": "vol. 3, 25-28",
   "pn": "",
   "abstract": [
    "This paper describes a technique for text-to-audio-visual speech synthesis based on hidden Markov models (HMMs), in which lip image sequences are modeled based on imageor pixel-based approach. To reduce the dimensionality of visual speech feature space, we obtain a set of orthogonal vectors (eigenlips) by principal components analysis (PCA), and use a subset of the PCA coefficients and their dynamic features as visual speech parameters. Auditory and visual speech parameters are modeled by HMMs separately, and lip movements are synchronized with auditory speech by using phoneme boundaries of auditory speech for synthesizing lip image sequences. We confirmed that the generated auditory speech and lip image sequences are realistic and synchronized naturally.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-469"
  },
  "hewitt00_icslp": {
   "authors": [
    [
     "Jill",
     "Hewitt"
    ],
    [
     "Andi",
     "Bateman"
    ],
    [
     "Andrew",
     "Lambourne"
    ],
    [
     "A.",
     "Ariyaeeinia"
    ],
    [
     "P.",
     "Sivakumaran"
    ]
   ],
   "title": "Real-time speech-generated subtitles: problems and solutions",
   "original": "i00_3029",
   "page_count": 5,
   "order": 471,
   "p1": "vol. 3, 29-32",
   "pn": "",
   "abstract": [
    "This paper refers to work carried out in the Subspeak project in which we are investigating the use of speech recognition in live television subtitling. Research to date has shown that with current speech recognition technology it is not possible to achieve a satisfactory level of accuracy in the direct transcription of broadcast material. To circumvent this problem in our system the broadcast speech data is respoken by a native English speaker in a quiet environment. Recognition rates of up to 98% can be achieved by a trained speaker where there are no out of vocabulary words. However, using conventional keyboard input, subtitlers can currently achieve near to 100%, with typically only minor errors of spelling or punctuation. The challenge is therefore to provide a speech-based subtitling system which mirrors the conventional systems in accuracy and speed, but which requires far less time to train subtitlers to use. Subtitles must typically be output at between 150 and 180 words per minute and the delay between the broadcast speech and the appearance of the subtitle must be at most 8 seconds. In the prototype system, output from the speech recognition system is passed in to a custom-built editor from where it can be corrected and passed on to an existing subtitling system.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-470"
  },
  "huang00b_icslp": {
   "authors": [
    [
     "Xuedong",
     "Huang"
    ],
    [
     "Alex",
     "Acero"
    ],
    [
     "C.",
     "Chelba"
    ],
    [
     "Li",
     "Deng"
    ],
    [
     "D.",
     "Duchene"
    ],
    [
     "Joshua",
     "Goodman"
    ],
    [
     "H.",
     "Hon"
    ],
    [
     "D.",
     "Jacoby"
    ],
    [
     "L.",
     "Jiang"
    ],
    [
     "R.",
     "Loynd"
    ],
    [
     "M.",
     "Mahajan"
    ],
    [
     "P.",
     "Mau"
    ],
    [
     "S.",
     "Meredith"
    ],
    [
     "S.",
     "Mughal"
    ],
    [
     "S.",
     "Neto"
    ],
    [
     "Mike",
     "Plumpe"
    ],
    [
     "K.",
     "Wang"
    ],
    [
     "Y.",
     "Wang"
    ]
   ],
   "title": "Mipad: a next generation PDA prototype",
   "original": "i00_3033",
   "page_count": 4,
   "order": 472,
   "p1": "vol. 3, 33-36",
   "pn": "",
   "abstract": [
    "MiPad is one of the application prototypes in a project codenamed Dr Who. As a wireless Personal Digital Assistant (PDA), MiPad fully integrates continuous speech recognition (CSR) and spoken language understanding (SLU) to enable users to accomplish many common tasks using a multimodal interface and wireless technologies. It tries to solve the problem of pecking with tiny styluses or typing on minuscule keyboards in todays PDAs or smart phones. It also avoids the problem of being a cellular telephone that depends on speech-only interaction. MiPad incorporates a built-in microphone that activates whenever a field is selected. As a user taps the screen or uses a built-in roller to navigate, the tapping action narrows the number of possible instructions for spoken language processing. MiPad currently runs on a Windows CE Pocket PC with a Windows 2000 Server where speech recognition is performed. The Dr Who CSR engine has a 64k word vocabulary with a unified context-free grammar and ngram language model. The Dr Who SLU engine is based on a robust chart parser and a plan-based dialog manager. This paper discusses MiPads design, implementation work in progress, and preliminary user study in comparison to the existing pen-based PDA interface.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-471"
  },
  "huang00c_icslp": {
   "authors": [
    [
     "Fei",
     "Huang"
    ],
    [
     "Jie",
     "Yang"
    ],
    [
     "Alex",
     "Waibel"
    ]
   ],
   "title": "Dialogue management for multimodal user registration",
   "original": "i00_3037",
   "page_count": 6,
   "order": 473,
   "p1": "vol. 3, 37-40",
   "pn": "",
   "abstract": [
    "User registration refers to associating certain personal information with a user. It is widely used in hospitals, hotels and conferences. In this paper, we propose an approach to interactive user registration by combining face recognition, speech recognition and speech synthesis technologies together through an efficient dialogue manager. In order to minimize a users effort, we employ a new dialogue management model based on a finite state automaton (FSA), which uses a Baysian network to fuse the users information from multiple channels (e.g., face image, speech, records stored in a pre-constructed database) to reliably estimate the confidence about user identity. Instead of fixing weights, the FSA adjusts its weights dynamically by integrating partial information from multiple information sources. This is achieved by maximizing an objective function to determine an optimal action at each succeeding state according to current confidence and information cues. Thus the transition between states can be done along the shortest path from the initial state to the goal state. We have developed a multimodal user registration system to demonstrate the feasibility of the proposed approach.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-472"
  },
  "bernstein00_icslp": {
   "authors": [
    [
     "Lynne E.",
     "Bernstein"
    ]
   ],
   "title": "Segmental optical phonetics for human and machine speech processing",
   "original": "i00_3043",
   "page_count": 4,
   "order": 474,
   "p1": "vol. 3, 43-46",
   "pn": "",
   "abstract": [
    "That talkers produce optical as well as acoustic speech signals, and that perceivers process both types of signals has become well known. Although perceptual effects due to audiovisual speech integration have been a focus of research involving the visual speech stimulus, relatively little is known about visual-only speech perception and optical phonetic signals. This knowledge is needed to exploit optical signals for applications such as synthetic artificial talking heads and audiovisual ASR. One important practical concern is the wide variation in performance among individual visual perceivers and talkers. This paper focuses on variation in visual phonetic perception, phoneme distinctiveness and word recognition. The paper also introduces a project linking optical phonetics, speech kinematics, and perception.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-473"
  },
  "thathong00_icslp": {
   "authors": [
    [
     "Umavasee",
     "Thathong"
    ],
    [
     "Somchai",
     "Jitapunkul"
    ],
    [
     "Visarut",
     "Ahkuputra"
    ],
    [
     "Ekkarit",
     "Maneenoi"
    ],
    [
     "Boonchai",
     "Thampanitchawong"
    ]
   ],
   "title": "Classification of Thai consonant naming using Thai tone",
   "original": "i00_3047",
   "page_count": 4,
   "order": 475,
   "p1": "vol. 3, 47-50",
   "pn": "",
   "abstract": [
    "This paper proposes the novel technique for separation of Thai consonant naming or consonant spelling using its tones. Consonant spelling is used for many applications such as a voice-actuated typewriter that helping to correct the confusable word in [...] sound. Because fundamental frequency (F0) can be suitably used in tone classification for Thai speech recognition, which is tonal language of five patterns: mid, low, falling and rising. Consequently, classification of Thai consonant naming algorithm used F0 for distinguishing rising tone from mid tone. From the experiment result, we found that not only the level of F0 indicates tonality of sound but also considering flattening, rising, oscillation and continuity of F0 that are necessary for Thai tonal language. This paper shows performance of algorithm that classifies 996 sounds and yielding 1.72% error-rate.\n",
    "[...] not readable in PDF file\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-474"
  },
  "li00h_icslp": {
   "authors": [
    [
     "Qi",
     "Li"
    ],
    [
     "Frank K.",
     "Soong"
    ],
    [
     "Olivier",
     "Siohan"
    ]
   ],
   "title": "A high-performance auditory feature for robust speech recognition",
   "original": "i00_3051",
   "page_count": 4,
   "order": 476,
   "p1": "vol. 3, 51-54",
   "pn": "",
   "abstract": [
    "An auditory feature extraction algorithm for robust speech recognition in adverse acoustic environments is proposed. Based on the analysis of human auditory system, the feature extraction algorithm consists of several modules: FFT, outer-middle-ear transfer function, frequency conversion from linear to Bark scales, auditory filtering, nonlinearity, and discrete cosine transform. Three recognition experiments have been conducted on connected digit recognition in wireless and land-line communications using handsets and handsfree microphones. Compared to LPCC and MFCC features, the proposed feature has shown 11% to 23% error-rate reductions on average in handset and hands-free acoustic environments in the experiments.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-475"
  },
  "xia00_icslp": {
   "authors": [
    [
     "Kun",
     "Xia"
    ],
    [
     "Carol",
     "Espy-Wilson"
    ]
   ],
   "title": "A new strategy of formant tracking based on dynamic programming",
   "original": "i00_3055",
   "page_count": 4,
   "order": 477,
   "p1": "vol. 3, 55-58",
   "pn": "",
   "abstract": [
    "This paper describes a new method for estimating formant frequencies. It operates in two phases. The first phase, which is similar to a technique developed by Talkin [JASA, vol. 82, S1], finds optimal formant track estimates by imposing frequency continuity constraints using Dynamic Programming (DP). DP is used to select a mapping of candidate frequencies to formant frequencies in oral sonorant regions based on the minimum cost from all possible mappings. The second phase performs a series of postprocessing steps to make formant estimates more robust and accurate and extends the formant estimates into nasal and obstruent regions. Performance statistics comparing the formants obtained with this technique with a set of reference formants using 34 sentences randomly selected from the TIMIT database shows our algorithm gives excellent results when the formants are among the candidate frequencies.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-476"
  },
  "lu00b_icslp": {
   "authors": [
    [
     "Xugang",
     "Lu"
    ],
    [
     "Gang",
     "Li"
    ],
    [
     "Lipo",
     "Wang"
    ]
   ],
   "title": "Dominant subspace analysis for auditory spectrum",
   "original": "i00_3059",
   "page_count": 4,
   "order": 478,
   "p1": "vol. 3, 59-62",
   "pn": "",
   "abstract": [
    "In hearing perception theory, spectral structure is a most important feature for speech perception, this spectral structure is not easy to be masked in noisy condition. So if this structure is extracted and enhanced, the representation will be much more robust. In this paper, we propose a new statistical dominant subspace analysis method for auditory spectrum based on SVD(Singular Values Decomposition) and signal subspace analysis method. The auditory spectrum can be decomposed into two subspaces, one is a dominant subspace, which is expanded by useful speech auditory spectrum , another subspace is sub-dominant subspace, which there is only noise information. So we analyze the auditory spectrum in the dominant subspace, the SNR will be increased. Thus this representation is much more robust.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-477"
  },
  "potamitis00_icslp": {
   "authors": [
    [
     "Ilyas",
     "Potamitis"
    ],
    [
     "Nikos",
     "Fanotakis"
    ],
    [
     "George",
     "Kokkinakis"
    ]
   ],
   "title": "Spectral and cepstral projection bases constructed by independent component analysis",
   "original": "i00_3063",
   "page_count": 4,
   "order": 479,
   "p1": "vol. 3, 63-66",
   "pn": "",
   "abstract": [
    "The present paper addresses the question of the efficiency of Independent Coitiponent Analysis (ICA) as a statistical process for deriving optimal representational bases for the projection of spectrum and cepstrum in the context of Automatic Speech Recognition (ASR). Several decorrelation strategies have been applied on the log-spectrum and cepstrum to fulfill the practical need of a diagonal covariance HMM for uncorrelated features. In our work we question the optimality of a fixed decorrelation strategy as DCT and follow an emerging trend in ASR that designs projection bases based on the statistics of speech. We differentiate our approach from the second order statistics of Discrete Cosine Transform (DCT), Linear Discriminatio Analysis (LDA) and Principal Component Analysis (PCA) by proposing an alternative data-driven approach based on higher Order Statistics.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-478"
  },
  "krstulovic00_icslp": {
   "authors": [
    [
     "Sacha",
     "Krstulovic"
    ]
   ],
   "title": "Relating LPC modeling to a factor-based articulatory model",
   "original": "i00_3067",
   "page_count": 4,
   "order": 480,
   "p1": "vol. 3, 67-70",
   "pn": "",
   "abstract": [
    "This paper proposes a method for recovering the articulatory parameters of a factor-based vocal tract shape model from the speech waveform. This is realized by analytically relating the shape model to a Linear Prediction lattice filter. Results pertaining to human vowels are presented. They show a good agreement with phonetic characteristics in a real-time computational framework.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-479"
  },
  "shire00_icslp": {
   "authors": [
    [
     "Michael L.",
     "Shire"
    ],
    [
     "Barry Y.",
     "Chen"
    ]
   ],
   "title": "On data-derived temporal processing in speech feature extraction",
   "original": "i00_3071",
   "page_count": 4,
   "order": 481,
   "p1": "vol. 3, 71-74",
   "pn": "",
   "abstract": [
    "Temporal processing and filtering in speech feature extraction are commonly used to aid in performance and robustness in automatic speech recognition. Among the techniques successfully employed are RASTA filtering, delta calculation, and cepstral mean subtraction. The work here explores the use of temporal filter design using LDA to further enhance performance using a few preprocessing configurations. In addition to RASTA filtering, we apply the filters to modulation-spectral features and cepstra while making sure that the assumptions of LDA are observed. We additionally test the use of filters that have been trained in different reverberation conditions, noting from previous work that the presence of reverberation alters the preferred frequency range of the derived filters. Our tests indicate a consistent advantage in phone classification. Word recognition tests, in contrast, reveal that the LDA filters often do not improve upon the existing filters previously used. They can also be made less effectual by allowing contextual frames to a trained probability estimator.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-480"
  },
  "saon00_icslp": {
   "authors": [
    [
     "George",
     "Saon"
    ],
    [
     "Mukund",
     "Padmanabhan"
    ]
   ],
   "title": "Minimum Bayes error feature selection",
   "original": "i00_3075",
   "page_count": 4,
   "order": 482,
   "p1": "vol. 3, 75-78",
   "pn": "",
   "abstract": [
    "We consider the problem of designing a linear transformation Θ ∈ Rpxn, of rank p ≤ n, which projects the features of a classifier x ∈ Rn onto y = Θx ∈ p such as to achieve minimum Bayes error (or probability of misclassification). Two avenues will be explored: the first is to maximize the Θ-average divergence between the class densities and the second is to minimize the union Bhattacharyya bound in the range of Θ. While both approaches yield similar performance in practice, they outperform standard LDA features and show a 10% relative improvement in the word error rate over state-of-the-art cepstral features on a large vocabulary telephony speech recognition task.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-481"
  },
  "ellis00_icslp": {
   "authors": [
    [
     "Daniel P. W.",
     "Ellis"
    ],
    [
     "Jeff A.",
     "Bilmes"
    ]
   ],
   "title": "Using mutual information to design feature combinations",
   "original": "i00_3079",
   "page_count": 4,
   "order": 483,
   "p1": "vol. 3, 79-82",
   "pn": "",
   "abstract": [
    "Combination of different feature streams is a well-established method for improving speech recognition performance. This empirical success, however, poses theoretical problems when trying to design combination systems: is it possible to predict which feature streams will combine most advantageously, and which of the many possible combination strategies will be most successful for the particular feature streams in question? We approach these questions with the tool of conditional mutual information (CMI), estimating the amount of information that one feature stream contains about the other, given knowledge of the correct subword unit label. We argue that CMI of the raw feature streams should be useful in deciding whether to merge them together as one large stream, or to feed them separately into independent classifiers for later combination; this is only weakly supported by our results. We also argue that CMI between the outputs of independent classifiers based on each stream should help predict which streams can be combined most beneficially. Our results confirm the usefulness of this measure.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-482"
  },
  "choi00d_icslp": {
   "authors": [
    [
     "Seungjin",
     "Choi"
    ],
    [
     "Heonseok",
     "Hong"
    ],
    [
     "Hervé",
     "Glotin"
    ],
    [
     "Frédéric",
     "Berthommier"
    ]
   ],
   "title": "Multichannel signal separation for cocktail party speech recognition: a dynamic recurrent network",
   "original": "i00_3083",
   "page_count": 4,
   "order": 484,
   "p1": "vol. 3, 83-86",
   "pn": "",
   "abstract": [
    "This paper addresses the method of multichannel signal separation with its application to cocktail party speech recognition. First, we present a fundamental principle for multi-channel signal separation which describes what spatial independence criterion results in. Second, for practical implementation of the signal separation filter, we consider a dynamic recurrent network and develop a new simple learning algorithm. The performance of the proposed method is evaluated in terms of word recognition error rate (WER). Experimental results show that our proposed method dramatically improves the word recognition performance in the case of two simultaneous speeches.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-483"
  },
  "prasad00_icslp": {
   "authors": [
    [
     "V. Kamakshi",
     "Prasad"
    ],
    [
     "Hema A.",
     "Murthy"
    ]
   ],
   "title": "An automatic algorithm for segmenting and labelling a connected digit sequence",
   "original": "i00_3087",
   "page_count": 4,
   "order": 485,
   "p1": "vol. 3, 87-90",
   "pn": "",
   "abstract": [
    "Group delay functions provide an alternative representation of signal information. The main features of group delay functions are the additive and high resolution properties. The Fourier transform (FT) phase is generally featureless due to random polority and wrapping. But the group delay function which is defined as the negative derivative of phase, can be processed to derive significant information such as peaks and valleys in the spectral envelope. In this paper, we show an application of group delay function to solve the segmentation problem in speech. In the proposed method a new signal is generated by symmetrising the short term energy function. The minimum phase group delay function of this signal is computed, the valleys of which correspond to segment boundaries. The proposed technique was tested on manually segmented digit utterances of the TI-DIGITS database. The overall correct segmentation performance is 77.8%. Digitwise recognition performance on the correctly segmented database is 87.1%.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-484"
  },
  "yan00_icslp": {
   "authors": [
    [
     "Hui",
     "Yan"
    ],
    [
     "Xuegong",
     "Zhang"
    ],
    [
     "Yanda",
     "Li"
    ],
    [
     "Liqin",
     "Shen"
    ],
    [
     "Weibin",
     "Zhu"
    ]
   ],
   "title": "The signal reconstruction of speech by KPCA",
   "original": "i00_3091",
   "page_count": 3,
   "order": 486,
   "p1": "vol. 3, 91-93",
   "pn": "",
   "abstract": [
    "A new method for speech signal reconstruction is proposed by performing a nonlinear Kernel Principal Component Analysis (KPCA). By the use of kernel functions, one can efficiently compute principal components in high-dimensional feature spaces, and reconstruct vectors mapping from input space by those dominant principal components. As the reconstructed vectors is expressed in high dimensional feature space and they could not exist pre-image in input space. For finding pre-image, we use iteration method to approximate the pre-image. The experimental results using KPCA in data reconstruction and denoising in speech signal show that it had many potential advantages comparing with PCA.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-485"
  },
  "saruwatari00_icslp": {
   "authors": [
    [
     "Hiroshi",
     "Saruwatari"
    ],
    [
     "Satoshi",
     "Kurita"
    ],
    [
     "Kazuya",
     "Takeda"
    ],
    [
     "Fumitada",
     "Itakura"
    ],
    [
     "Kiyohiro",
     "Shikano"
    ]
   ],
   "title": "Blind source separation based on subband ICA and beamforming",
   "original": "i00_3094",
   "page_count": 4,
   "order": 487,
   "p1": "vol. 3, 94-97",
   "pn": "",
   "abstract": [
    "This paper describes a new blind source separation (BSS) method on microphone array using the subband independent component analysis (ICA) and beamforming. The proposed array system consists of the following three sections: (1) subband-ICA-based BSS section, (2) null beamforming section, and (3) integration of (1) and (2) based on the algorithm diversity. Using this technique, we can resolve the low-convergence problem on optimization in ICA. Signal separation and speech recognition experiments clarify that the noise reduction rate (NRR) of about 18 dB is obtained under the nonreverberant condition, and NRRs of 8 dB and 6 dB are obtained in the case that the reverberation times are 150 msec and 300 msec. These performances are superior to those of both simple ICA-based BSS and simple beamforming method. Also, the improvements of the proposed method in word recognition rates are superior to those of the conventional ICA-based BSS method under all reverberant conditions.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-486"
  },
  "estienne00_icslp": {
   "authors": [
    [
     "Claudio",
     "Estienne"
    ],
    [
     "Patricia",
     "Pelle"
    ]
   ],
   "title": "A synchrony front-end using phase-locked-loop techniques",
   "original": "i00_3098",
   "page_count": 4,
   "order": 488,
   "p1": "vol. 3, 98-101",
   "pn": "",
   "abstract": [
    "We propose a new front-end that reflects some aspects of auditory nerve response. Namely, the pattern of synchrony responses observed over auditory nerve fibers associated with F0, F1 and F2 of voiced sounds. The main goal is to get a set of features, which represents those frequency trajectories. These features should be less sensitive to adverse environmental conditions than mel-cepstrum or Fourier based front-ends. The core of the system is a computational implementation of the well-known electronic device phase locked loop (PLL). Outputs of a PLL bank interpolated and conveniently resampled define our front-end coefficients. Performance of the system was assessed in a vowels recognition task using a common HMM back-end, and compared with the performance of a mel-frequency cepstral coefficients (MFCC) front-end.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-487"
  },
  "hernando00_icslp": {
   "authors": [
    [
     "Javier",
     "Hernando"
    ]
   ],
   "title": "On the use of filter-bank energies driven from the autocorrelation sequence for noisy speech recognition",
   "original": "i00_3102",
   "page_count": 4,
   "order": 489,
   "p1": "vol. 3, 102-105",
   "pn": "",
   "abstract": [
    "The OSA-LP (One-Sided Autocorrelation LP) representation of speech signal has shown to be attractive for noisy speech recognition because of both its high recognition performance with respect to the conventional LP in severe conditions of additive broad-band noise and its computational simplicity. However, the mel-cepstrum representation, which comes from a filter bank (FB) analysis, has become increasingly popular. Furthermore, hybrid techniques that combine filter-bank and LP analysis, have also been proposed. The aim of this paper is to gain some perspective of the merit of the autocorrelation, LP and FB based techniques in a real environment.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-488"
  },
  "bod00_icslp": {
   "authors": [
    [
     "Rens",
     "Bod"
    ]
   ],
   "title": "Combining semantic and syntactic structure for language modeling",
   "original": "i00_3106",
   "page_count": 4,
   "order": 490,
   "p1": "vol. 3, 106-109",
   "pn": "",
   "abstract": [
    "Structured language models for speech recognition have been shown to remedy the weaknesses of n -gram models. All current structured language models, however, are limited in that they do not take into account dependencies between non-headwords. We show that non-headword dependencies contribute significantly to improved word error rate, and that a data-oriented parsing model trained on semantically and syntactically annotated data can exploit these dependencies. This paper contains the first published experiments with a data-oriented parsing model trained by means of a maximum likelihood reestimation procedure.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-489"
  },
  "goodman00_icslp": {
   "authors": [
    [
     "Joshua",
     "Goodman"
    ],
    [
     "Jianfeng",
     "Gao"
    ]
   ],
   "title": "Language model size reduction by pruning and clustering",
   "original": "i00_3110",
   "page_count": 4,
   "order": 491,
   "p1": "vol. 3, 110-113",
   "pn": "",
   "abstract": [
    "Several techniques are known for reducing the size of language models, including count cutoffs [1], Weighted Difference pruning [2], Stolcke pruning [3], and clustering [4]. We compare all of these techniques and show some surprising results. For instance, at low pruning thresholds, Weighted Difference and Stolcke pruning underperform count cutoffs. We then show novel clustering techniques that can be combined with Stolcke pruning to produce the smallest models at a given perplexity. The resulting models can be a factor of three or more smaller than models pruned with Stolcke pruning, at the same perplexity. The technique creates clustered models that are often larger than the unclustered models, but which can be pruned to models that are smaller than unclustered models with the same perplexity.\n",
    "s F. Jelinek, \"Self Organized Language modeling for Speech Recognition\", in Readings in Speech Recognition, A. Waibel and K. F. Lee(Eds.), Morgan Kaufmann, 1990 K. Seymore, R. Rosenfeld. \"Scalable backoff language models\", Proc. ICSLP, Vol. 1., pp.232-235, Philadelphia, 1996 A. Stolcke, \"Entropy-based Pruning of Backoff Language Models\" Proc. DARPA News Transcription and Understanding Workshop, 1998, pp. 270-274, Lansdowne, VA. P. F. Brown,V. J. DellaPietra, P. V. deSouza, J. C., Lai, R. L. Mercer. \"Class-based n-gram models of natural language\". Computational Linguistics 1990 (18), 467-479.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-490"
  },
  "wu00d_icslp": {
   "authors": [
    [
     "Jun",
     "Wu"
    ],
    [
     "Sanjeev",
     "Khudanpur"
    ]
   ],
   "title": "Efficient training methods for maximum entropy language modeling",
   "original": "i00_3114",
   "page_count": 5,
   "order": 492,
   "p1": "vol. 3, 114-118",
   "pn": "",
   "abstract": [
    "Maximum entropy language modeling techniques combine different sources of statistical dependence, such as syntactic relationships, topic cohesiveness and collocation frequency, in a unified and e\u000bective language model. These techniques however are also computationally very intensive, particularly during model estimation, compared to the more prevalent alternative of interpolating several simple models, each capturing one type of dependency. In this paper we present ways which significantly reduce this complexity by reorganizing the required computations. We show that in case of a model with N-gram constraints, each iteration of the parameter estimation algorithm requires the same amount of computation as estimating a comparable back-off N-gram model. In general, the computational cost of each iteration in model estimation is linear in the number of distinct \"histories\" seen in the training corpus, times a model-class dependent factor. The reorganization focuses mainly on reducing this multiplicative factor from the size of the vocabulary to the average number of words seen following a history. A 15-fold speed-up has been observed by using this method in estimating a language model that incorporates syntactic head-word constraints, nonterminal-label constraints and topic-unigram constraints with N-grams for the Switchboard corpus. This model achieves a perplexity reduction of 13% and a word error rate reduction of 1.5% absolute compared to a trigram model.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-491"
  },
  "deligne00_icslp": {
   "authors": [
    [
     "Sabine",
     "Deligne"
    ]
   ],
   "title": "Statistical language modeling with a class based n-multigram model",
   "original": "i00_3119",
   "page_count": 4,
   "order": 493,
   "p1": "vol. 3, 119-122",
   "pn": "",
   "abstract": [
    "In this paper, we report on speech recognition experiments with an n-multigram language model, a stochastic model which assumes dependencies of length n between variable-length phrases. The n-multigram probabilities can be estimated in a class-based framework, where both the phrase distribution and the phrase classes are learned from the data according to a Maximum Likelihood criterion, using a generalized Expectation-Maximization algorithm. In our speech recognition experiments on a database of air travel reservations, the 2-multigram model allows a reduction of 19% of the word error rate with respect to the usual trigram model, with 25% fewer param eters than in the trigram model. We also report on a scheme where some a priori information is introduced in the model via semantic tagging.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-492"
  },
  "tanigaki00_icslp": {
   "authors": [
    [
     "Koichi",
     "Tanigaki"
    ],
    [
     "Hirofumi",
     "Yamamoto"
    ],
    [
     "Yoshinori",
     "Sagisaka"
    ]
   ],
   "title": "A hierarchical language model incorporating class-dependent word models for OOV words recognition",
   "original": "i00_3123",
   "page_count": 4,
   "order": 494,
   "p1": "vol. 3, 123-126",
   "pn": "",
   "abstract": [
    "A new language model is proposed to cope with the demands for recognizing out-of-vocabulary (OOV) words not registered in the lexicon. This language model is a class N-gram incorporating a set of word models that reflect the statistical characteristics of the phonotactics, which depend on the lexical classes. Utilization of class-dependency enhances recognition accuracy and enables identification of the class of OOV words. OOV words can be recognized as transcribed portions having class labels, which provide semantic attributes of OOV words to subsequent language processing. Experimental application of the model to Japanese personal and family names showed that it performs nearly as well as the upper bound of the in-vocabulary recognition.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-493"
  },
  "zheng00c_icslp": {
   "authors": [
    [
     "Fang",
     "Zheng"
    ],
    [
     "Jian",
     "Wu"
    ],
    [
     "Wenhu",
     "Wu"
    ]
   ],
   "title": "Input Chinese sentences using digits",
   "original": "i00_3127",
   "page_count": 4,
   "order": 495,
   "p1": "vol. 3, 127-130",
   "pn": "",
   "abstract": [
    "Chinese character input is always a key issue in a variety of Chinese based applications especially when only a small number keypad is available. Though many kinds of Chinese character encoding schemes are proposed according to Chinese character characteristics, such as the shape, they are not straightforward and will take users a long time to learn. An easy way is to input via Chinese pinyins. In this paper, we establish the mapping between digit string and pinyin as well as the mapping between the pinyin string and the word, referred to as the Syllable-Digit search Tree (SDT) and the Word-Syllable search Tree (WST) respectively. By using these two search trees as well as the word N-gram language model and the syllable-synchronous network search (SSNS) algorithm, any digit string can be easily converted into Chinese word sequence or sentence. Without users selecting from candidates, the character error rate (CER) of digit-to-character (D/C) conversion is 6.6% across a test text consisting 22,083 characters.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-494"
  },
  "richardson00_icslp": {
   "authors": [
    [
     "Matt",
     "Richardson"
    ],
    [
     "Jeff",
     "Bilmes"
    ],
    [
     "Chris",
     "Diorio"
    ]
   ],
   "title": "Hidden-articulator Markov models: performance improvements and robustness to noise",
   "original": "i00_3131",
   "page_count": 4,
   "order": 496,
   "p1": "vol. 3, 131-134",
   "pn": "",
   "abstract": [
    "A Hidden-Articulator Markov Model (HAMM) is a Hidden Markov Model (HMM) in which each state represents an articulatory configuration. Articulatory knowledge, known to be useful for speech recognition [1], is represented by specifying a mapping of phonemes to articulatory configurations; vocal tract dynamics are represented via transitions between articulatory configurations.\n",
    "In previous work [2], we extended the articulatory-feature model introduced by Erler [3] by using diphone units and a new technique for model initialization. By comparing it with a purely random model, we showed that the HAMM can take advantage of articulatory knowledge.\n",
    "In this paper, we extend that work in three ways. First, we decrease the number of parameters, making it comparable in size to standard HMMs. Second, we evaluate our model in noisy contexts, verifying that articulatory knowledge can provide benefits in adverse acoustic conditions. Third, we use a corpus of sideby- side speech and articulator trajectories to show that the HAMM can reasonably predict the movement of the articulators.\n",
    "s L. Deng and D. Sun (1994). \"Phonetic Classification and Recognition Using HMM Representation of Overlapping Articulatory Features for all classes of English sounds,\" ICASSP, 1994, pp.45-48 M. Richardson, J. Bilmes, C. Diorio (2000). \"Hidden-Articulator Markov Models for Speech Recognition\", ASR2000. K. Erler and G.H. Freeman (1996). \"An HMM-based speech recognizer using overlapping articulatory features,\" J. Acoust. Soc. Am. 100, pp.2500-13\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-495"
  },
  "sandness00_icslp": {
   "authors": [
    [
     "Eric D.",
     "Sandness"
    ],
    [
     "I. Lee",
     "Hetherington"
    ]
   ],
   "title": "Keyword-based discriminative training of acoustic models",
   "original": "i00_3135",
   "page_count": 4,
   "order": 497,
   "p1": "vol. 3, 135-138",
   "pn": "",
   "abstract": [
    "In this paper, we investigate a new discriminative training technique which focuses on optimizing a keyword error rate, rather than the error rate on all words. We hypothesize that improvements in keyword error rate correlate with improvements in understanding error rates. Keyword-based discriminative training is accomplished by modifying a standard minimum classification error (MCE) training algorithm so that only segments of speech relevant to keyword errors are used in the acoustic model training. When both the standard and keyword-based techniques are used to adjust mixture weights, we find that keyword error rate reduction compared to baseline maximum likelihood (ML) trained models is nearly twice as large for the keyword-based approach. The overall word accuracy is also found to be improved for keyword-based training, and we run several experiments to investigate this phenomenon.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-496"
  },
  "goel00_icslp": {
   "authors": [
    [
     "Vaibhava",
     "Goel"
    ],
    [
     "Shankar",
     "Kumar"
    ],
    [
     "William",
     "Byrne"
    ]
   ],
   "title": "Segmental minimum Bayes-risk ASR voting strategies",
   "original": "i00_3139",
   "page_count": 5,
   "order": 498,
   "p1": "vol. 3, 139-142",
   "pn": "",
   "abstract": [
    "ROVER [1] and its successor voting procedures have been shown to be quite effective in reducing the recognition word error rate (WER). The success of these methods has been attributed to their minimum Bayes-risk (MBR) nature: they produce the hypothesis with the least expected word error. In this paper we develop a general procedure within the MBR framework, called segmental MBR recognition, that encompasses current voting techniques and allows further extensions that yield lower expected WER. It also allows incorporation of loss functions other than the WER. We present a derivation of voting procedure of N-best ROVER as an instance of segmental MBR recognition. We then present an extension, called e-ROVER, that alleviates some of the restrictions of N-best ROVER by better approximating the WER. e-ROVER is compared with N-best ROVER on multi-lingual acoustic modeling task and is shown to yield modest yet significant and easily obtained improvements.\n",
    "",
    "",
    "J. Fiscus. A post-processing system to yield reduced word error rates: Recognizer output voting error reduction (ROVER). IEEE Workshop on Automatic Speech Recognition and Understanding, pp. 347-354, Santa Barbara, CA, 1997.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-497"
  },
  "nock00_icslp": {
   "authors": [
    [
     "Harriet J.",
     "Nock"
    ],
    [
     "Steve J.",
     "Young"
    ]
   ],
   "title": "Loosely coupled HMMs for ASR",
   "original": "i00_3143",
   "page_count": 4,
   "order": 499,
   "p1": "vol. 3, 143-146",
   "pn": "",
   "abstract": [
    "Hidden Markov Models (HMMs) have been successful for modelling the dynamics of carefully dictated speech, but their performance degrades severely when used to model conversational speech. This paper presents a preliminary feasibility study of an alternative class of models: loosely coupled HMMs. Since speech is produced by a system of loosely coupled articulators, stochastic models explicitly representing this parallelism may have advantages for automatic speech recognition (ASR), particularly when trying to model the phonological effects inherent in casual spontaneous speech. The paper evaluates one coupled model on a simple ASR task, using both exact and approximate estimation schemes. We conclude such models merit further investigation.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-498"
  },
  "weber00b_icslp": {
   "authors": [
    [
     "Katrin",
     "Weber"
    ],
    [
     "Samy",
     "Bengio"
    ],
    [
     "Hervé",
     "Bourlard"
    ]
   ],
   "title": "HMM2- a novel approach to HMM emission probability estimation",
   "original": "i00_3147",
   "page_count": 4,
   "order": 500,
   "p1": "vol. 3, 147-150",
   "pn": "",
   "abstract": [
    "In this paper, we discuss and investigate a new method to estimate local emission probabilities in the framework of hidden Markov models (HMM). Each feature vector is considered to be a sequence and is supposed to be modeled by yet another HMM. Therefore, we call this approach HMM2. There is a variety of possible topologies of such HMM2 systems, e.g. incorporating trellis or ergodic HMM structures. Preliminary HMM2 speech recognition experiments on cepstral and spectral features yielded worse results than state-of-the-art systems. However, we believe that HMM2 systems have a lot of potential advantages and are therefore worth investigating further.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-499"
  },
  "singh00_icslp": {
   "authors": [
    [
     "Rita",
     "Singh"
    ],
    [
     "Bhiksha",
     "Raj"
    ],
    [
     "Richard M.",
     "Stern"
    ]
   ],
   "title": "Structured redefinition of sound units by merging and splitting for improved speech recognition",
   "original": "i00_3151",
   "page_count": 4,
   "order": 501,
   "p1": "vol. 3, 151-154",
   "pn": "",
   "abstract": [
    "The performance of speech recognition systems degrades when the basic sound units used are poorly defined or inconsistently used. Several attempts have been made to improve dictionaries automatically, either by redefining pronunciations of words in terms of existing sound units, or by redefining the sound units themselves completely. The problem with these approaches is that, while the former is limited by the sound units used, the latter discards all human information that has been incorporated into an expert-designed recognition dictionary. In this paper we propose a new merging-andsplitting algorithm that attempts to redefine the basic sound units used in the dictionary, while maintaining the expert knowledge built into a manually designed dictionary. Sound units from an existing dictionary are merged based on their inherent confusability, as measured by a Monte-Carlo based metric, and subsequently split to maximize the likelihood of the training data. Experiments with the Resource Management database indicate that this approach results in an improvement in recognition accuracy when context-independent models are used for recognition. When context-dependent models are used, the improvement observed is reduced.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-500"
  },
  "arsigny00_icslp": {
   "authors": [
    [
     "V.",
     "Arsigny"
    ],
    [
     "Gérard",
     "Chollet"
    ],
    [
     "Guillaume",
     "Gravier"
    ],
    [
     "Marc",
     "Sigelle"
    ]
   ],
   "title": "Speech modeling with state constrained Markov fields over frequency bands",
   "original": "i00_3155",
   "page_count": 4,
   "order": 502,
   "p1": "vol. 3, 155-158",
   "pn": "",
   "abstract": [
    "In this paper, we propose a model of speech segments in the time/frequency domain. This model is based on Markov random field (MRF) modeling and is an extension of our previous work on multi-band models with MRF. In this new approach, the time model in each band is defined on the constrained state space of strictly left-right Markov chains and a non-stationary synchrony model between the frequency band is added. We derive algorithms for parameter estimation and for segment scoring. Finally the model is tested and compared to HMM on an isolated word recognition task. Results show the interest of the synchrony model for test data corrupted with additive white noise but they also point out some weaknesses of the training algorithm which must be improved.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-501"
  },
  "zhu00b_icslp": {
   "authors": [
    [
     "Weibin",
     "Zhu"
    ],
    [
     "Liqin",
     "Shen"
    ],
    [
     "Xiaochuan",
     "Miu"
    ]
   ],
   "title": "Duration modeling for Chinese synthesis from C-toBI labeled corpus",
   "original": "i00_3159",
   "page_count": 4,
   "order": 503,
   "p1": "vol. 3, 159-162",
   "pn": "",
   "abstract": [
    "A set of labeling criteria, C-ToBI (Chinese Tone and Break Index) was redefined to annotate the prosodic event in continuous speech in a hierarchical structure. Therere 4 layers, i.e., intonational phrase, intermediate phrase, word, and syllable layer. The prosodic structure and break index and stress index tiers represent the core prosodic events of an utterance. The stress index represents the degree of accent of the constituents in each layer. The break tier represents the degree of the juncture of each pair of constituents in each layer. A duration model was built from a reading style corpus labeled with CToBI. The factors affecting the duration of a given segment come from two relatively independent levels. First, in segment level, the phoneme of the segment and the context do influence the duration. Second, in super-segment level, the influences come from multi-layers, which include the location and the degree of stress and break in different layers. Those factors with the property of directional invariance form the feature vector that was as the input of the linear duration model. And the model was part of a synthesis speech system, and its parameters were estimated by the statistic approach.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-502"
  },
  "wang00l_icslp": {
   "authors": [
    [
     "Bei",
     "Wang"
    ],
    [
     "Bo",
     "Zheng"
    ],
    [
     "Shinan",
     "Lu"
    ],
    [
     "Jianfen",
     "Cao"
    ],
    [
     "Yufang",
     "Yang"
    ]
   ],
   "title": "The pitch movement of word stress in Chinese",
   "original": "i00_3163",
   "page_count": 4,
   "order": 504,
   "p1": "vol. 3, 163-166",
   "pn": "",
   "abstract": [
    "The pitch movement of word stress in Chinese is studied in aspects of word stress perception and pronunciation. Three parts of work have been done, word stress perception experiment, acoustic analysis of falling tone and the matched questions and statements experiment. The results of word stress perception experiment show that high point of the pitch range is the main acoustical correlate to word stress perception among low point, pitch range and average pitch. The variation of the pitch is not only correlates to word stress but also to the location of the syllable in a sentence, that is the intonation of the sentence. The acoustic analysis of falling tone is further studied in part II. In the matched questions and statements experiment~ the stress level and the location of the target syllable are systematically regulated. The results of these two parts show that the pitch movement of stressed word is on basis of top and bottom-line intonation model. The pitch movement of high point is significant and free. The pitch movement of low point is limited by the bottom-line of intonation. The movement of high point is greater than that of the low point, which makes the pitch range vary.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-503"
  },
  "watanabe00b_icslp": {
   "authors": [
    [
     "Michiko",
     "Watanabe"
    ],
    [
     "Carlos Toshinori",
     "Ishi"
    ]
   ],
   "title": "The distribution of fillers in lectures in the Japanese language",
   "original": "i00_3167",
   "page_count": 4,
   "order": 505,
   "p1": "vol. 3, 167-170",
   "pn": "",
   "abstract": [
    "The present study investigated the distribution of five types of fillers in Japanese-language lectures. It was found that the distribution pattern differed depending on the kind of fillers; e, eto and ma tended to appear more often at major syntactic boundaries such as sentence- and clause- boundaries than ano and sono. Sono never occurred at sentence boundaries. These findings suggest that the use of different kinds of fillers may reflect different processes in speech delivery.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-504"
  },
  "harnud00_icslp": {
   "authors": [
    [
     "Huhe",
     "Harnud"
    ],
    [
     "Yuling",
     "Zheng"
    ],
    [
     "Jiayou",
     "Chen"
    ]
   ],
   "title": "Research on stress in bisyllsblic words of Mongolian",
   "original": "i00_3171",
   "page_count": 4,
   "order": 506,
   "p1": "vol. 3, 171-174",
   "pn": "",
   "abstract": [
    "This thesis proves that the main acoustic feature of Mongolian bisyllabic words is the duration of the vowels after analysis and generalization of the acoustic pattern of the prosodic features of monosyllabic and bisyllabic words of Mongolian based on the \"Database of acoustic Parameters of the prosodic features of Mongolian\" combined with perception experiments.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-505"
  },
  "imoto00_icslp": {
   "authors": [
    [
     "Kazunori",
     "Imoto"
    ],
    [
     "Masatake",
     "Dantsuji"
    ],
    [
     "Tatsuya",
     "Kawahara"
    ]
   ],
   "title": "Modelling of the perception of English sentence stress for computer-assisted language learning",
   "original": "i00_3175",
   "page_count": 4,
   "order": 507,
   "p1": "vol. 3, 175-178",
   "pn": "",
   "abstract": [
    "For learning foreign language pronunciation, prosodic features are important as much as, or more than segmental features. For Japanese speakers, one of difficulties to learn English pronunciation is rhythm because of the differences between two languages: mora-timing rhythm and stress-timing rhythm. In order to correct errors in rhythm, the method of evaluating sentence stress that constitutes rhythm is very significant. In this paper, we present a method of automatic detecting sentence stress syllables for the evaluation criterion. Using a linear discriminant function of pitch, intensity and vowel duration, about 90% of the syllables were correctly detected as to sentence stress. Also we analyzed the different and common characteristics among different English native speakers. The results revealed that the perception of the sentence stress among 11 native speakers had general agreement with respect to how to integrate three features.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-506"
  },
  "buhmann00_icslp": {
   "authors": [
    [
     "Jeska",
     "Buhmann"
    ],
    [
     "Halewijn",
     "Vereecken"
    ],
    [
     "Justin",
     "Fackrell"
    ],
    [
     "Jean-Pierre",
     "Martens"
    ],
    [
     "Bert van",
     "Coile"
    ]
   ],
   "title": "Data driven intonation modelling of 6 languages",
   "original": "i00_3179",
   "page_count": 5,
   "order": 508,
   "p1": "vol. 3, 179-182",
   "pn": "",
   "abstract": [
    "A method for creating multi-lingual intonation models is described. The method adheres closely to the pioneering work of Traber, in that a recurrent neural network (RNN) predicts a number of F0 values per syllable. An important aspect of the work presented here is the selection of linguistic and prosodic features that are suitable for predicting the observed intonation phenomena in different languages. Another aspect is the use of automatic labelling techniques for the preparation of the training data. Experiments on six languages demonstrate that even though there are differences in performance across languages, it is possible to obtain good results for all six languages. More importantly, making use of automatic labelling techniques for the construction of the training corpora, tends to give better results than making use of manual labelling techniques.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-507"
  },
  "blin00_icslp": {
   "authors": [
    [
     "Laurent",
     "Blin"
    ],
    [
     "Mike",
     "Edgington"
    ]
   ],
   "title": "Prosody prediction using a tree-structure similarity metric",
   "original": "i00_3183",
   "page_count": 4,
   "order": 509,
   "p1": "vol. 3, 183-186",
   "pn": "",
   "abstract": [
    "In this paper, we present ongoing work on prosody prediction for speech synthesis. Our approach considers sentences as treelike structures and decides on the prosody from a corpus of such structures through tree similarity measurements in a nearest neighbour context. We introduce a syntactic structure and a performance structure representation, the tree similarity metrics considered, and then we discuss the prediction method. Experiments are currently under process to qualify this approach.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-508"
  },
  "teixeira00_icslp": {
   "authors": [
    [
     "Carlos",
     "Teixeira"
    ],
    [
     "Horacio",
     "Franco"
    ],
    [
     "Elizabeth",
     "Shriberg"
    ],
    [
     "Kristin",
     "Precoda"
    ],
    [
     "Kemal",
     "Sönmez"
    ]
   ],
   "title": "Prosodic features for automatic text-independent evaluation of degree of nativeness for language learners",
   "original": "i00_3187",
   "page_count": 5,
   "order": 510,
   "p1": "vol. 3, 187-190",
   "pn": "",
   "abstract": [
    "Predicting the degree of nativeness of a student utterance is an important issue in computer-aided language learning. This task has been addressed by many studies focusing on the segmental assessment of the speech signal. To achieve improved correlations between human and automatic nativeness scores, other aspects of speech should also be considered, such as prosody. The goal of this study is to evaluate the use of prosodic information to help predict the degree of nativeness of pronunciation, independent of the text. A supervised strategy based on human grades is used in an attempt to select promising features for this task. Preliminary results show improvements in the correlation between human and automatic scores.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-509"
  },
  "minematsu00c_icslp": {
   "authors": [
    [
     "Nobuaki",
     "Minematsu"
    ],
    [
     "Seiichi",
     "Nakagawa"
    ]
   ],
   "title": "Instantaneous estimation of prosodic pronunciation habits for Japanese students to learn English pronunciation",
   "original": "i00_3191",
   "page_count": 4,
   "order": 511,
   "p1": "vol. 3, 191-194",
   "pn": "",
   "abstract": [
    "More and more efforts have been recently made to apply speech technologies to language learning and develop CALL systems. The authors have been focusing on Japanese manners of generating English word stress. This is because pronunciation habits which are inevitable to Japanese learners can be easily found in the stress generation. In our previous study, a stressed syllable detector and a pronunciation habit estimator were developed, where the estimated habits of individual learners accorded well with their English pronunciation proficiency rated by English teachers. However, the habit could be estimated only after a learner pronounced several dozens of words because the habit estimator referred to stressed syllable detection rates. In this paper, using another criterion, a method of instantaneous estimation was proposed which required only a single word utterance. Results showed that an average pattern of the instantaneously estimated habits accorded well with the habits obtained in our previous study.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-510"
  },
  "ni00_icslp": {
   "authors": [
    [
     "Jinfu",
     "Ni"
    ],
    [
     "Keikichi",
     "Hirose"
    ]
   ],
   "title": "Synthesis of fundamental FDrequency contours of standard Chinese sentences from tone sandhi and focus conditions",
   "original": "i00_3195",
   "page_count": 4,
   "order": 512,
   "p1": "vol. 3, 195-198",
   "pn": "",
   "abstract": [
    "A new method was developed to synthesize fundamental frequency (F0) contours of Chinese sentences based on a functional model, formerly developed by the authors. The model has an advantage in that decomposition process to phrase and tone components is not necessary. The developed method decides the F0 contour type for each word based on a set of tone-sandhi rules, and then shapes these word F0 contour types into the phrasal F0 contour taking focus conditions into account The tone-sandhi rules are formulated as 19 bi- and 198 tri-tone-sandhi contouremes in a parametric form, which are obtained by quantitative analysis of 84 di-, 538 tri and 938 tetra-syllable words. Each contoureme can realize F0 contours in 3 different ranges: normal, depressed and expanded. The focus conditions decide range type for each word and a peak reference-line for a phrase. When generating phrasal F0 contour, the model parameter values of contouremes, selected using tone sequence and range type information, are adjusted so that F0 peaks appear along the peak reference-line. Experimental results confirmed the validity of the proposed method.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-511"
  },
  "zu00_icslp": {
   "authors": [
    [
     "Yiqing",
     "Zu"
    ],
    [
     "Xiaoxia",
     "Chan"
    ],
    [
     "Aijun",
     "Li"
    ],
    [
     "Wu",
     "Hua"
    ],
    [
     "Guohua",
     "Sun"
    ]
   ],
   "title": "Syllable duration and its functions in standard Chinese discourse",
   "original": "i00_3199",
   "page_count": 4,
   "order": 513,
   "p1": "vol. 3, 199-202",
   "pn": "",
   "abstract": [
    "To get good understanding of prosody in continuous speech of Standard Chinese, we have collected large amount of speech in paragraph. 18 read discourse each contains 300-500 syllables are used as reading texts, which cover main discourse We are going effort on linguistic annotation. In This paper we report works reported as follows: One male speaker's 10,000 syllables duration in discourse; the relationship between silence duration in discourse waveform and the prosodic structures.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-512"
  },
  "holm00_icslp": {
   "authors": [
    [
     "Bleicke",
     "Holm"
    ],
    [
     "Gérard",
     "Bailly"
    ]
   ],
   "title": "Generating prosody by superposing multi-parametric overlapping contours",
   "original": "i00_3203",
   "page_count": 4,
   "order": 514,
   "p1": "vol. 3, 203-206",
   "pn": "",
   "abstract": [
    "We present here a model for generating prosody by superposing overlapping multi-parametric contours. These contours are associated with high-level communication tasks such as segmentation, hierarchisation or emphasis of discourse units. We propose a analysis-by-synthesis scheme for automatically learning these contours and apply this new paradigm to the enunciation of mathematical formulae and utterances carrying various attitudes.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-513"
  },
  "veldhuis00_icslp": {
   "authors": [
    [
     "Raymond",
     "Veldhuis"
    ]
   ],
   "title": "Consistent pitch marking",
   "original": "i00_3207",
   "page_count": 4,
   "order": 515,
   "p1": "vol. 3, 207-210",
   "pn": "",
   "abstract": [
    "The pitch-marking algorithm presented in this paper avoids inconsistency errors between pitch markers in subsequent fundamental periods by adding the requirements of waveform and pitch consistency. The approach is as follows. Candidate pitch markers satisfying user-defined properties for pitch marking are selected first. Dynamic programming is then used to find the sequence of candidate pitch markers which optimally satisfies the waveform- and pitchconsistency requirements. The algorithm is described in detail and results are presented.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-514"
  },
  "jun00_icslp": {
   "authors": [
    [
     "Sun-Ah",
     "Jun"
    ],
    [
     "Sook-Hyang",
     "Lee"
    ],
    [
     "Keeho",
     "Kim"
    ],
    [
     "Yong-Ju",
     "Lee"
    ]
   ],
   "title": "Labeler agreement in transcribing korean intonation with K-toBI",
   "original": "i00_3211",
   "page_count": 5,
   "order": 516,
   "p1": "vol. 3, 211-214",
   "pn": "",
   "abstract": [
    "This paper reports labeler agreement in the transcription of Korean prosody using Korean ToBI (K-ToBI). Twenty utterances representing five different types of speech were produced by 18 speakers and transcribed by 21 labelers differing in their levels of experience with K-ToBI. Following the stringent metric used for English ToBI evaluation [14,12], consistency was measured in terms of the number of transcriber pairs agreeing on the labeling of each particular word. The results show that for tonal transcriptions of the 32,130 transcriber-pair-words, agreement was 77% for the type of boundaries at the end of each word (i.e., word, AP, or IP), 78% for AP boundaries, and 91% for IP boundaries. For break indices, the agreement score for exact matching in the labeling was 59%, 69% when relaxing the presence/absence of diacritics, and 99% when relaxing within +/-1 level. In sum, the data confirm that the conventions of K-ToBI are adequate, easy to learn, and can be reliably used for research in Korean prosody and for large-scale prosodic annotation in speech databases.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-515"
  },
  "hirose00b_icslp": {
   "authors": [
    [
     "Yukiyoshi",
     "Hirose"
    ],
    [
     "Kazuhiko",
     "Ozeki"
    ],
    [
     "Kazuyuki",
     "Takagi"
    ]
   ],
   "title": "Effectiveness of prosodic features in syntactic analysis of read Japanese sentences",
   "original": "i00_3215",
   "page_count": 4,
   "order": 517,
   "p1": "vol. 3, 215-218",
   "pn": "",
   "abstract": [
    "Prosody contains information that is lost when utterances are transcribed into letters or characters. This paper is concerned with exploiting such information for syntactic analysis of read Japanese sentences. In our previous work, we employed 12 prosodic features, and made a statistical model to represent the relationship between those features and dependency distances. Then, by incorporating the model in our parser, which allows the use of numerical information as linguistic knowledge, we showed that prosodic information is in fact effective for syntactic analysis. In the present work, we took up 24 prosodic features, and conducted an extensive search for effective ones. Also the statistical model was modified to account for the actual distribution of the feature values. In open experiments using an ATR 503-sentence database, parsing accuracy was improved by 21.2% compared with the case where no prosodic information was used. The duration of pauses at phrase boundaries was consistently effective in both closed and open experiments, while the effectiveness of other features, when used together with the duration of pause, was not clear in open experiments.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-516"
  },
  "banno00_icslp": {
   "authors": [
    [
     "Mieko",
     "Banno"
    ]
   ],
   "title": "A study of F0 declination in Japanese: towards a discourse model of prosodic structure",
   "original": "i00_3219",
   "page_count": 4,
   "order": 518,
   "p1": "vol. 3, 219-222",
   "pn": "",
   "abstract": [
    "This study investigates F0 declination as a global-level prosodic phenomenon, establishing a new discourse-based model of prosodic structure in Japanese. The model includes two levels of declination in a hierarchical order: utterance units and prosodic paragraphs, a higher level of declination consisting of embedded declinations. Comparing and contrasting three types of discourse - read speech, conversation, and narrative - this study provides an accurate description of declination, based on an all-points linear regression line procedure. The results reveal a significant occurrence of declination not only in read speech but also in spontaneous speech. In addition, all three types of discourse exhibit both levels of declination, i.e. utterance units and prosodic paragraphs, providing evidence for the discourse model proposed in this study.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-517"
  },
  "sakurai00_icslp": {
   "authors": [
    [
     "Atsuhiro",
     "Sakurai"
    ],
    [
     "Nobuaki",
     "Minematsu"
    ],
    [
     "Keikichi",
     "Hirose"
    ]
   ],
   "title": "Data-driven intonation modeling using a neural network and a command response model",
   "original": "i00_3223",
   "page_count": 4,
   "order": 519,
   "p1": "vol. 3, 223-226",
   "pn": "",
   "abstract": [
    "An intonation modeling scheme for Japanese text-to-speech synthesis is proposed using a command response F0 model and a neural network to generate F0 contours of accentual phrases uttered in continuous speech. The neural network is used to predict the values of P0 model parameters for a whole sentence, focusing on accentual phrases. The features used as inputs to the neural network are: position of the accentual phrase within the sentence, number of rnorae in the accentual phrase, accent type of the accentual phrase, number of words in the accentual phrase, and parts-of-speech of the first and last words of the accentual phrase. The predicted parameters are: a flag that indicates the presence of a phrase command at the beginning of the accentual phrase, magnitude of the phrase command (if present), amplitude of the accent command, and offset values for the timing of phrase and accent commands. All features are simultaneously predicted. Three types of neural network structures are used, each one with 3 different numbers of elements in the single hidden laver: MLP (multi-layer perceptron), Elman, and Jordan. The method permits efficient prediction of F0 model parameters, as observed in evaluation experiments and informal listening tests.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-518"
  },
  "erdem00_icslp": {
   "authors": [
    [
     "Caglayan",
     "Erdem"
    ],
    [
     "Martin",
     "Holzapfel"
    ],
    [
     "Rüdiger",
     "Hoffmann"
    ]
   ],
   "title": "Natural F0 contours with a new neural-network-hybrid approach",
   "original": "i00_3227",
   "page_count": 4,
   "order": 520,
   "p1": "vol. 3, 227-230",
   "pn": "",
   "abstract": [
    "Text-to-Speech (TTS) systems still suffer from unnatural prosody generation. To increase customers acceptance a more sophisticated prosody modelling is required. In this paper a new hybrid approach combining the advantages of two existing state-of-the-art modelling strategies is presented.\n",
    "After presenting two state-of-the-art approaches with their advantages and shortcomings in section 1 we will discuss the new architecture of the hybrid approach in section 2 outlining the data driven interconnection of the two base approaches. Finally a search performed on the database will be presented using a fuzzy motivated nonlinear parametric cost and suitability function for obtaining desired fo-control parameters. The hybrid approach improved our fo-generation Module within our TTS system PAPAGENO.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-519"
  },
  "fackrell00_icslp": {
   "authors": [
    [
     "Justin",
     "Fackrell"
    ],
    [
     "Halewijn",
     "Vereecken"
    ],
    [
     "Jeska",
     "Buhmann"
    ],
    [
     "Jean-Pierre",
     "Martens"
    ],
    [
     "Bert Van",
     "Coile"
    ]
   ],
   "title": "Prosodic variation with text type",
   "original": "i00_3231",
   "page_count": 4,
   "order": 521,
   "p1": "vol. 3, 231-234",
   "pn": "",
   "abstract": [
    "This paper describes ongoing work which aims to produce a methodology for automatically deriving several prosody models, each suited for text-to-speech synthesis of a specific text type. As part of this work an experiment is described in which human readings of documents covering 10 common text types (news, weather forecasts, etc) were analysed for their acoustic/prosodic properties. This was carried out in three languages - Dutch (Belgium), English (USA) and French (France). The results confirm that there is substantial variation in prosody across different text types.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-520"
  },
  "syrdal00_icslp": {
   "authors": [
    [
     "Ann K.",
     "Syrdal"
    ],
    [
     "Julia",
     "McGory"
    ]
   ],
   "title": "Inter-transcriber reliability of toBI prosodic labeling",
   "original": "i00_3235",
   "page_count": 4,
   "order": 522,
   "p1": "vol. 3, 235-238",
   "pn": "",
   "abstract": [
    "The goal of this study was to evaluate the reliability among transcribers of a standard prosodic labeling system under relatively optimal conditions of training, supervision, facilities, procedures, and extent of speaker familiarity. The ToBI (Tones and Break Indices) model for standard American English was used in the study; break indices indicate the degree of junction between words, pitch accents designate word prominence, and edge tones mark phrase boundaries. The American English speech corpora were read by a female professional speaker and by a male professional speaker, and were composed of several types of texts to ensure prosodic variety. Each of four experienced transcribers independently labeled each corpus. For each corpus, word level agreement in break indices, pitch accents, and edge tones between all possible pairs of transcribers was analyzed, and various statistics were calculated. Agreement among labelers was generally higher than that reported in previous studies[1,2] of larger and more diverse groups of labelers. Agreement was high for some prosodic categories, but low for others. The extent of reliability for various prosodic distinctions has important implications for refining the ToBI model and for limitations in the use of prosody in speech technologies.\n",
    "s J. Pitrelli, M. Beckman, and J. Hirschberg. Evaluation of prosodic transcription labeling reliability in the ToBI framework. In Proc. 3rd Internat. Conf. Spoken Language Processing, volume 2, pages 123{126, Yokohama, 1994. ICSLP M. Grice, M. Reyelt, R. Benzmuller, J. Mayer, and A. Batliner. Consistency in transcription and labelling of German intonation with GToBI. In Proc. 4th Inter- nat. Conf. Spoken Language Processing, volume 3, pages 1716{1719, Philadelphia, 1996. ICSLP\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-521"
  },
  "kochanski00_icslp": {
   "authors": [
    [
     "Greg P.",
     "Kochanski"
    ],
    [
     "Chilin",
     "Shih"
    ]
   ],
   "title": "Stem-ML: language-independent prosody description",
   "original": "i00_3239",
   "page_count": 4,
   "order": 523,
   "p1": "vol. 3, 239-242",
   "pn": "",
   "abstract": [
    "Stem-ML is a tagging system with a completely defined algorithm for translating the tags into quantitative prosody in any language. It separates the description of prosodic intentions from their execution, by modeling the interactions between accents. We designed Stem-ML to allow automated training of accent shapes and parameters from acoustic databases.\n",
    "Stem-ML is linguistically neutral: it allows a description of any physiologically realizable prosody in terms of linguistic concepts, without imposing a restrictive theory on the data. The tag set and algorithm make no assumptions about the number of distinct types of accents or tones, or their scope. Accents and tones are treated interchangeably. Stem-ML allows, but does not require, descriptions involving phrase curves.\n",
    "The model begins with soft templates for tone or accent shapes that are specified by the user or obtained by automated training. These soft templates interact because of physically and physiologically motivated constraints that model the smooth and continuous motions of the muscles that control prosody.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-522"
  },
  "dong00_icslp": {
   "authors": [
    [
     "Minghui",
     "Dong"
    ],
    [
     "Kim Teng",
     "Lua"
    ]
   ],
   "title": "Using prosody database in Chinese speech synthesis",
   "original": "i00_3243",
   "page_count": 5,
   "order": 524,
   "p1": "vol. 3, 243-246",
   "pn": "",
   "abstract": [
    "As the difficulty of revealing the relationship between text and prosody, a corpus based approach is used in prosody generation in the research. A prosody database is built as templates for prosody generation. The general idea is that we are trying to get the prosodic information from real speech examples. We first analyze given Chinese text, and form a linguistic feature vector, which describes the phonetic and lexicon characteristics of the text. Then we search the database to find the best match of the vector, which is a similar occurrence of the text. The prosody parameters of the retrieved example will be the guideline of the prosody we are going to generate.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-523"
  },
  "erickson00b_icslp": {
   "authors": [
    [
     "Donna",
     "Erickson"
    ],
    [
     "Kikuo",
     "Maekawa"
    ],
    [
     "Michiko",
     "Hashi"
    ],
    [
     "Jianwu",
     "Dang"
    ]
   ],
   "title": "Some articulatory and acoustic changes associated with emphasis in spoken English",
   "original": "i00_3247",
   "page_count": 4,
   "order": 525,
   "p1": "vol. 3, 247-250",
   "pn": "",
   "abstract": [
    "In order to understand better the relationship between stress and tonal patterning of prosodic changes in spoken English, this paper examines articulatory and acoustic data of a large number of American English speakers producing prosodic minimal pairs on high and low-front vowels. Articulatory and acoustic measurements (from the x-ray microbeam database, University of Wisconsin) of 45 American English speakers emphasized and unemphasized vowels /i/ and /ae/ were analyzed. The results confirm previous findings that articulation and acoustics (F1,F2, and F0) change as a function of emphasis. For both emphasized /i/ and /ae/, the jaw opens more, accompanied by more extreme tongue dorsum articulation as well as more extreme formant frequencies. Results of an F0 correlation with the articulatory measures suggest there may be at least two strategies for producing emphasis: one is to rely more on F0 whereas the other is to rely more on jaw opening. These findings are relevant to developments of phonological models of articulation, and especially to the C/D Model with its view of prosody as the basis for phonetic implementation of spoken English.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-524"
  },
  "janse00_icslp": {
   "authors": [
    [
     "Esther",
     "Janse"
    ],
    [
     "Anke",
     "Sennema"
    ],
    [
     "Anneke",
     "Slis"
    ]
   ],
   "title": "Fast speech timing in Dutch: durational correlates of lexical stress and pitch accent",
   "original": "i00_3251",
   "page_count": 6,
   "order": 526,
   "p1": "vol. 3, 251-254",
   "pn": "",
   "abstract": [
    "In this study we investigated the durational correlates of lexical stress and pitch accent at normal and fast speech rate in Dutch. Previous literature on English shows that durations of lexically unstressed vowels are reduced more than stressed vowels when speakers increase their speech rate. We found that the same holds for Dutch, irrespective of whether the unstressed vowel is schwa or a \"full\" vowel. In the same line, we expected that vowels in words without a pitch accent would be shortened relatively more than vowels in words with a pitch accent. This was not the case: if anything, the accented vowels were shortened relatively more than the unaccented vowels. We conclude that duration is an important cue for lexical stress, but not for pitch accent.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-525"
  },
  "hiroshige00_icslp": {
   "authors": [
    [
     "Makoto",
     "Hiroshige"
    ],
    [
     "Kantaro",
     "Suzuki"
    ],
    [
     "Kenji",
     "Araki"
    ],
    [
     "Koji",
     "Tochinai"
    ]
   ],
   "title": "On perception of word-based local speech rate in Japanese without focusing attention",
   "original": "i00_3255",
   "page_count": 4,
   "order": 527,
   "p1": "vol. 3, 255-258",
   "pn": "",
   "abstract": [
    "Fundamental investigations about differential limen (DL) for word-based speech rate variations in Japanese are described. We carry out auditory tests with stimuli made by equally lengthening or shortening a duration of a word in a sentence. We set up the experiments to diffuse the subjects focus of attention. When the focus is diffused, the DL value of acceleration increases in several cases. The obtained DLs are 18.9 msec/mora for acceleration and 26.5 msec/mora for deceleration.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-526"
  },
  "sakurai00b_icslp": {
   "authors": [
    [
     "Atsuhiro",
     "Sakurai"
    ],
    [
     "Koji",
     "Iwano"
    ],
    [
     "Keikichi",
     "Hirose"
    ]
   ],
   "title": "Modeling and generation of accentual phrase F0 contours based on discrete HMMs synchronized at mora-unit transitions",
   "original": "i00_3259",
   "page_count": 4,
   "order": 528,
   "p1": "vol. 3, 259-262",
   "pn": "",
   "abstract": [
    "We propose a data-driven approach to intonation modeling and generation based on discrete Hidden Markov Models (HMM), where state transitions are synchronized with Japanese rhythmic units called morae. Mora-unit F0 contours are encoded using symbols that consist of two codes: the first is an index to a table of stylized mora F0 contours, and the second points to a table of quantized differences of the average F0 contour with respect to the previous mora. Both codebooks contain 32 codes. The HMM is used in generation mode, i.e., it generates a sequence of symbols for an intonational phrase without any input other than the length of the sequence, using a variation of Viterbi search with a modified distance function. In the training phase, the speech database is subdivided into classes according to the attributes of the target accentual phrase, and each class is associated to an HMM. After the output symbol sequence is generated, the F0 contour is constructed using the codebooks and a mora duration pattern. Evaluation experiments show that the HMMs are able to correctly produce F0 contours that reflect their training conditions.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-527"
  },
  "louw00_icslp": {
   "authors": [
    [
     "Philippa H.",
     "Louw"
    ],
    [
     "Justus. C.",
     "Roux"
    ],
    [
     "Elizabeth. C.",
     "Botha"
    ]
   ],
   "title": "Synthesizing prosody for commands in a Xhosa TTS system",
   "original": "i00_3263",
   "page_count": 4,
   "order": 529,
   "p1": "vol. 3, 263-266",
   "pn": "",
   "abstract": [
    "Xhosa is an African tone language spoken in South Africa. The relationship between the prosodic features duration, pitch and loudness of Xhosa commands was determined through acoustic analysis and perceptual experimentation. Combining the results of the acoustic analysis and the perceptual experiment proved to be an appropriate method of parameter extraction with which to formulate a prosodic model for the generation of perceptually acceptable imperatives in a practical Xhosa TTS system.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-528"
  },
  "christogiannis00_icslp": {
   "authors": [
    [
     "Costas",
     "Christogiannis"
    ],
    [
     "Yiannis",
     "Stavroulas"
    ],
    [
     "Yiannis",
     "Vamvakoulas"
    ],
    [
     "Theodora",
     "Varvarigou"
    ],
    [
     "Agatha",
     "Zappa"
    ],
    [
     "Chilin",
     "Shih"
    ],
    [
     "Amalia",
     "Arvaniti"
    ]
   ],
   "title": "Design and implementation of a Greek text-to-speech system based on concatenative synthesis",
   "original": "i00_3267",
   "page_count": 5,
   "order": 530,
   "p1": "vol. 3, 267-270",
   "pn": "",
   "abstract": [
    "The goal of this paper is to present the work carried out up to now for the development of the Greek Text-To-Speech (GRTTS) system by NTUA. The system under consideration is based on the method of concatenative synthesis and follows the Bell Labs approach to this technique. In order that the input text to the GRTTS is translated into continuous synthetic speech the following modules have already been studied and implemented: (i) module for the linguistic analysis of the input text; (ii) the acoustic inventory module. On the same time it is under development the duration module of the GRTTS, for the computation of the appropriate temporal structure of synthesized speech. The objectives of the above studies, in combination with the concatenative synthesis technique, which is one of the simplest methods for speech synthesis, are to bypass most of the problems encountered by other synthesis methods such as articulatory and formant synthesis systems. The major objective is to minimize abrupt discontinuities and thus maximize the naturalness of the synthesized utterances.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-529"
  },
  "baptist00_icslp": {
   "authors": [
    [
     "Lauren",
     "Baptist"
    ],
    [
     "Stephanie",
     "Seneff"
    ]
   ],
   "title": "GENESIS-II: a versatile system for language generation in conversational system applications",
   "original": "i00_3271",
   "page_count": 4,
   "order": 531,
   "p1": "vol. 3, 271-274",
   "pn": "",
   "abstract": [
    "Language generation is a fundamental component of dialogue systems. Over the past year, we have developed a new generation module for conversational systems developed at MIT using the GALAXY architecture. Our generator, which we call GENESIS-II, resolves many of the shortcomings of its predecessor, GENESIS. GENESIS-II makes it substantially easier for users to specify generation, and the generation output is often of a higher quality. In particular, GENESIS-II has improved the ease and quality of generation in foreign languages (Japanese, Chinese, Spanish) and non-traditional languages (SQL, HTML, speech waveforms). In this paper, we focus on the more advanced features of our system.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-530"
  },
  "kim00c_icslp": {
   "authors": [
    [
     "Eun-Kyoung",
     "Kim"
    ],
    [
     "Yung-Hwan",
     "Oh"
    ]
   ],
   "title": "New analysis method for harmonic plus noise model based on time-domain periodicity score",
   "original": "i00_3275",
   "page_count": 4,
   "order": 532,
   "p1": "vol. 3, 275-278",
   "pn": "",
   "abstract": [
    "Harmonic plus Noise Model (HNM) is the two-band speech model which assumes the speech signal to be composed of a harmonic part and a noise part. It is very important to determine the reasonable maximum voiced frequency which delimit harmonic and noise parts for highquality synthetic speech. In this paper, a new analysis method for HNM, especially maximum voiced frequency, is proposed, whereby the time-domain periodicity score is calculated for each harmonic band and the band of maximum score is selected for maximum voiced frequency. Moreover, recurrent equation of periodicity score is developed for fast implementation. This method has proven to be robust to the inaccuracy of pitch estimation and the frequency resolution problem in low-pitched speech.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-531"
  },
  "toda00_icslp": {
   "authors": [
    [
     "Tomoki",
     "Toda"
    ],
    [
     "Jinlin",
     "Lu"
    ],
    [
     "Hiroshi",
     "Saruwatari"
    ],
    [
     "Kiyohiro",
     "Shikano"
    ]
   ],
   "title": "Straight-based voice conversion algorithm based on Gaussian mixture model",
   "original": "i00_3279",
   "page_count": 4,
   "order": 533,
   "p1": "vol. 3, 279-282",
   "pn": "",
   "abstract": [
    "The voice conversion algorithm based on the Gaussian mixture model (GMM) has also been proposed by Stylianou et al. In this algorithm, the acoustic space of a speaker is represented continuously. In this paper, we apply this GMM-based voice conversion algorithm to STRAIGHT proposed by Kawahara et al., which is recognized as a high quality vocoder. In order to evaluate this voice conversion algorithm, we perform subjective and objective experiments on speech quality and speaker individuality, comparing with the method based on the codebook mapping. As results, the performance of the GMM-based voice conversion algorithm is better than that of the codebook mapping method. Effects by the amount of training data for the voice conversion algorithms are also investigated, as well as the number of the Gaussian mixtures. These evaluation results clarify that the GMM-based voice conversion algorithm is successfully applied to STRAIGHT.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-532"
  },
  "libossek00_icslp": {
   "authors": [
    [
     "Marion",
     "Libossek"
    ],
    [
     "Florian",
     "Schiel"
    ]
   ],
   "title": "Syllable-based text-to-phoneme conversion for German",
   "original": "i00_2283",
   "page_count": 0,
   "order": 534,
   "p1": "vol. 2, 283-286",
   "pn": "",
   "abstract": [
    "Due to the non-trivial relationship between the orthographic form and the chain of sounds in a spoken utterance in German, the text-to-phoneme conversion (TPC), as part of a text-to-speech system, is not a negligible task. Many methods that use a fixed set of rules for TPC take into account the morphological structure of words. Even though this approach results in a high accuracy, it has one major drawback: the required morphological decomposition is difficult and error prone. In this paper we propose a new approach which uses the orthographic syllable instead of morphemes. The performance compares well with the traditional method, with the added advantage that the decomposition into syllabic units can easily be achieved by using existing hyphenation algorithms implemented in currently available word processors.\n",
    ""
   ]
  },
  "hain00_icslp": {
   "authors": [
    [
     "Horst-Udo",
     "Hain"
    ]
   ],
   "title": "A hybrid approach for grapheme-to-phoneme conversion based on a combination of partial string matching and a neural network",
   "original": "i00_3291",
   "page_count": 4,
   "order": 535,
   "p1": "vol. 3, 291-294",
   "pn": "",
   "abstract": [
    "The quality of a text-to-speech (TTS) system heavily depends on the transcription quality of the words to be spoken. Obviously the best transcription can be found in a phonetic dictionary. But for out of vocabulary (OOV) words fall back routines have to be developed.\n",
    "This paper proposes a fall back routine that combines the correctness of a phonetic dictionary with the flexibility of a neural network. In the first step parts of the OOV word are looked up in the dictionary. They are then connected with the additional feature that the last phoneme of the first part is re-estimated using a neural network and a special phonetic dictionary. In the second step the word stress is determined either from the dictionary or using a second neural network.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-533"
  },
  "tillmann00_icslp": {
   "authors": [
    [
     "Hans G.",
     "Tillmann"
    ],
    [
     "Hartmut R.",
     "Pfitzinger"
    ]
   ],
   "title": "Parametric high definition (PHD) speech synthesis-by-analysis: the development of a fundamentally new system creating connected speech by modifying lexically-represented language units",
   "original": "i00_3295",
   "page_count": 3,
   "order": 536,
   "p1": "vol. 3, 295-297",
   "pn": "",
   "abstract": [
    "Our paper has 5 sections. In section (1) we will discuss critically the fact that the development of Text-to-Speech systems and Speech-to-Text systems has in the past been treated as totally separate problems (we restrict ourselves to so-called dictation systems, L2S and S2L, which either translate written language units L into speech signals S, or speech signals S into sequences of written language units L). In section (2) we argue that for this reason, in the future, theoretical and empirical work should be devoted to providing an approach that integrates the L2S and S2L components into a unified phonetic system, which is able to learn to speak a language and also to understand what other L2S-systems are saying.\n",
    "The new Munich PHD-system will be described in section (3) as an example of such a unified approach. Fundamental to this system is the selection and definition of lexically-given speech items, both acoustically and articulatorily (EMA). In section (4) we demonstrate a set of prosodic functions that take lexically-defined L-inputs and produce phonetically well-formed connected Soutputs. We discuss the possibility of combining certain elementary functions (such as those controlling F0 variation, segment duration, and sound modification) into a much more complex function which also controls the language-specific rhythmic variation of speech tempo in its locally measurable form. Finally section (5) will raise the question of analysing speech data produced by individual speakers as a means of arriving at the sound production system of a generalized representative member of the sociolect or dialect of the language in question.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-534"
  },
  "kwon00_icslp": {
   "authors": [
    [
     "Chul H.",
     "Kwon"
    ],
    [
     "Minkyu",
     "Lee"
    ],
    [
     "Joseph P.",
     "Olive"
    ]
   ],
   "title": "A new synthesis algorithm using phase information for TTS systems",
   "original": "i00_3298",
   "page_count": 4,
   "order": 537,
   "p1": "vol. 3, 298-301",
   "pn": "",
   "abstract": [
    "New speech synthesis algorithms capable of flexible prosody (especially F0) modification are desired for a high quality TTS system. TD-PSOLA is the most popular synthesis algorithm. The algorithm shows very high quality when F0 modification is limited. However, the quality degradation due to pitch epoch detection error becomes severe as the F0 modification factor becomes large. On the other hand, the vocoder framework is very flexible in F0 manipulation. The synthesized speech quality from the vocoder is far from natural human speech and suffers from buzziness. To remedy buzzy quality from the vocoder and make more natural synthetic speech, we propose a mixed phase vocoder.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-535"
  },
  "wouters00_icslp": {
   "authors": [
    [
     "Johan",
     "Wouters"
    ],
    [
     "Michael W.",
     "Macon"
    ]
   ],
   "title": "Unit fusion for concatenative speech synthesis",
   "original": "i00_3302",
   "page_count": 4,
   "order": 538,
   "p1": "vol. 3, 302-305",
   "pn": "",
   "abstract": [
    "An important problem in concatenative synthesis is the occurence of spectral discontinuities or \"concatenation mismatch\" between sonorant speech units. In this paper, we present an approach to reduce concatenation mismatch by combining spectral information from two sequences of speech units selected in parallel. Concatenation units, on one hand, define initial spectral trajectories for a target utterance. Fusion units, on the other hand, define the desired transitions between concatenated units. The two unit sequences are \"fused\" by imposing dynamic constraints defined by the fusion units on the spectral trajectories of the concatenation units. To regenerate the modified speech units, we use a synthesis algorithm based on sinusoidal + all-pole analysis of speech, which overcomes the limitations of residual-excited LPC. Results from a perceptual test show that our method is highly successful at removing concatenation artifacts in speech generated from an inventory of diphones.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-536"
  },
  "lenzo00_icslp": {
   "authors": [
    [
     "Kevin A.",
     "Lenzo"
    ],
    [
     "Alan W.",
     "Black"
    ]
   ],
   "title": "Diphone collection and synthesis",
   "original": "i00_3306",
   "page_count": 4,
   "order": 539,
   "p1": "vol. 3, 306-309",
   "pn": "",
   "abstract": [
    "In this paper, we describe the design and collection of corpora for diphone synthesis, the voice building process, and our experience in the creation of a new, publically available database of ten diphone sets of one American English speaker for the Festival Speech Synthesis System, using the FestVox document and tools. In support of our goal to make the tools and techniques available for anyone to build their own synthetic voices, we have generalized and streamlined the tasks involved from what were once arcane anecdotes, half-written one-off scripts, and partial descriptions, to detailed, complete instructions that others have followed with good results.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-537"
  },
  "portele00_icslp": {
   "authors": [
    [
     "Thomas",
     "Portele"
    ]
   ],
   "title": "Natural language generation for spoken dialogue",
   "original": "i00_3310",
   "page_count": 4,
   "order": 540,
   "p1": "vol. 3, 310-313",
   "pn": "",
   "abstract": [
    "A natural language generation module for spoken dialogue systems has been developed that performs three steps: generating multiple versions of an utterance, choosing the best version by a set of criteria, and annotating the text using structural information accumulated during the generation process. The requirements of spoken output is catered for by several design decisions.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-538"
  },
  "conkie00_icslp": {
   "authors": [
    [
     "Alistair",
     "Conkie"
    ],
    [
     "Mark C.",
     "Beutnagel"
    ],
    [
     "Ann K.",
     "Syrdal"
    ],
    [
     "Philip E.",
     "Brown"
    ]
   ],
   "title": "Preselection of candidate units in a unit selection-based text-to-speech synthesis system",
   "original": "i00_3314",
   "page_count": 4,
   "order": 541,
   "p1": "vol. 3, 314-317",
   "pn": "",
   "abstract": [
    "Unit selection-based speech synthesis has recently been the focus of much attention in the speech synthesis community. In general, the speech quality from such a system achieves a high degree of naturalness and good intelligibility. However, examining and selecting units for synthesis as a runtime operation makes the unit selection process computationally expensive. Considerable attention has been focused on reducing the complexity of unit selection while maintaining quality.\n",
    "Previous approaches to speeding up the process of runtime unit selection have focused on two aspects. (1) By limiting the number of candidate synthesis units considered in the unit selection process, the number of calculations required can be reduced. (2) By precomputing part of the needed calculations, the runtime complexity can be reduced. Much progress has been made using these methods, but usually at the expense of quality. We present two methods of reducing the complexity of the calculation that avoid any reduction in synthesis quality, while allowing a very fast unit selection process. Results are presented for the reduction in complexity of the calcu- lation process, and for a perceptual experiment that shows quality is not reduced relative to a full unit selection process.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-539"
  },
  "jensen00b_icslp": {
   "authors": [
    [
     "Kare Jean",
     "Jensen"
    ],
    [
     "Søren",
     "Riis"
    ]
   ],
   "title": "Self-organizing letter code-book for text-to-phoneme neural network model",
   "original": "i00_3318",
   "page_count": 4,
   "order": 542,
   "p1": "vol. 3, 318-321",
   "pn": "",
   "abstract": [
    "This paper describes an improved input coding method for a textto- phoneme (TTP) neural network model for speaker independent speech recognition systems. The code-book is self-organizing and is jointly optimized with the TTP model ensuring that the coding is optimal in terms of overall performance. The codebook is based on a set of single layer neural networks with shared weights. Experiments show that performance is increased compared to the NETTalk and NETSpeak models.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-540"
  },
  "yi00_icslp": {
   "authors": [
    [
     "Jon R. W.",
     "Yi"
    ],
    [
     "James R.",
     "Glass"
    ],
    [
     "I. Lee",
     "Hetherington"
    ]
   ],
   "title": "A flexible, scalable finite-state transducer architecture for corpus-based concatenative speech synthesis",
   "original": "i00_3322",
   "page_count": 4,
   "order": 543,
   "p1": "vol. 3, 322-325",
   "pn": "",
   "abstract": [
    "In this paper we describe our work involving the conversion of our phonologically-based synthesizer into a finite-state transducer (FST) representation which can be used for real-time natural-sounding synthesis. We have designed a transducer structure to efficiently perform the common task of unit selection in concatenative speech synthesis. By encapsulating domainindependent concatenative synthesis costs into a constraint kernel, we have obtained a topology that scales linearly with the size of the synthesis corpus. The FST representation provides a flexible, unified framework in which we can leverage our previous work in speech recognition in areas such as pronunciation modelling and search. The FST synthesizer has been incorporated into two servers which operate within our conversational system architecture to convert meaning representations into waveforms. We have had preliminary success with the new FST-based synthesis in several constrained spoken dialogue applications.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-541"
  },
  "wang00m_icslp": {
   "authors": [
    [
     "Changfu",
     "Wang"
    ],
    [
     "Hiroya",
     "Fujisaki"
    ],
    [
     "Ryou",
     "Tomana"
    ],
    [
     "Sumio",
     "Ohno"
    ]
   ],
   "title": "Analysis of fundamental frequency contours of standard Chinese in terms of the command-response model and its application to synthesis by rule of intonation",
   "original": "i00_3326",
   "page_count": 4,
   "order": 544,
   "p1": "vol. 3, 326-329",
   "pn": "",
   "abstract": [
    "Previous studies by the authors have shown that the commandresponse model applies quite well to F0 contours of Standard Chinese, and can be used to extract parameters as well as to derive rules for speech synthesis. The present paper describes results of analysis of F0 contours of utterances by 6 native speakers at three speech rates, conducted to examine the effects of speech rate and speaker differences. It also describes results of analysis of syllable durations at the three speech rates. These results are then applied to derive rules and quantization levels of parameters for F0 contour generation in speech synthesis of Standard Chinese. In particular, it shows that a three-level quantization of syllable duration is sufficient for the naturalness of prosody when combined with rules for F0 contour parameters.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-542"
  },
  "hirai00_icslp": {
   "authors": [
    [
     "Toshio",
     "Hirai"
    ],
    [
     "Seiichi",
     "Tenpaku"
    ],
    [
     "Kiyohiro",
     "Shikano"
    ]
   ],
   "title": "Manipulating speech pitch periods according to optimal insertion/deletion position in residual signal for intonation control in speech synthesis",
   "original": "i00_3330",
   "page_count": 4,
   "order": 545,
   "p1": "vol. 3, 330-333",
   "pn": "",
   "abstract": [
    "This paper describes the investigation of manipulating positions in a speech pitch when lengthening or shortening the pitch period, that is, lowering or raising fundamental frequency of speech. The experimental results revealed that the preferable positions were at the first half of the pitch period for pitch shortening, and at the second half of it for pitch lengthening. The findings are expected to improve the quality of speech synthesis on pitch modulation.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-543"
  },
  "mittrapiyanuruk00_icslp": {
   "authors": [
    [
     "Pradit",
     "Mittrapiyanuruk"
    ],
    [
     "Chatchawarn",
     "Hansakunbuntheung"
    ],
    [
     "Virongrong",
     "Tesprasit"
    ],
    [
     "Virach",
     "Sornlertlamvanich"
    ]
   ],
   "title": "Improving naturalness of Thai text-to-speech synthesis by prosodic rule",
   "original": "i00_3334",
   "page_count": 4,
   "order": 546,
   "p1": "vol. 3, 334-337",
   "pn": "",
   "abstract": [
    "This paper presents a method to improve the naturalness of Thai Text-to-speech synthesis, in 4 main parts. In the pausing module, its main function is to determine the break location when synthesizing a Thai text which has no explicit sentence/phrase/word boundary. In the syllable duration and tone generation, a set of rules is provided to generate proper prosodic parameters for synthesizing more natural speech. The syllable duration rule is applied using the Klatts method to handle the task in syllabic frame. The tonal rule considers the effect of tonal coarticulation and F0 downdrift in generating the F0 contour parameter. In the demisyllable concatenation, the TD-PSOLA technique is applied to modify the waveform for obtaining the required prosody. The LSP-based concatenated boundary smoothing is also included to imitate the crosssyllable coarticulation effect. The result of comparative quality test shows a significant improvement in our proposed method.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-544"
  },
  "xu00d_icslp": {
   "authors": [
    [
     "Dawei",
     "Xu"
    ],
    [
     "Hiroki",
     "Mori"
    ],
    [
     "Hideki",
     "Kasuya"
    ]
   ],
   "title": "Word-level F0 range in Mandarin Chinese and its application to inserting words into a sentence",
   "original": "i00_3338",
   "page_count": 4,
   "order": 547,
   "p1": "vol. 3, 338-341",
   "pn": "",
   "abstract": [
    "This paper considers an automatic voice response application in which a word utterance is inserted into a fixed carrier sentence. An important task here is to adjust the F0 contour of the inserted word according to the F0 context of the carrier sentence. Instead of generating the F0 contour on syllable basis, we employ an approach to adjust the F0 contour of the whole word. In this approach, two questions arise: (a) how to evaluate the F0 context and (b) how to adjust the F0 contour suitably for the context. We have found that the F0 contour of a word can be appropriately regulated in a tone-independent word-level F0 range (WF0R). After estimating the WF0Rs of the preceding and succeeding words, the WF0R of the inserted word is set at the mean of these WF0Rs. The F0 contour of the inserted word is then mapped to the WF0R taking into account the tone combination of the word. A perceptual evaluation experiment showed that the adjusted F0 was coordinated well with the context.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-545"
  },
  "isogai00_icslp": {
   "authors": [
    [
     "Mitsuaki",
     "Isogai"
    ],
    [
     "Kimihito",
     "Tanaka"
    ],
    [
     "Satoshi",
     "Takano"
    ],
    [
     "Hideyuki",
     "Mizuno"
    ],
    [
     "Masanobu",
     "Abe"
    ],
    [
     "Sinya",
     "Nakajima"
    ]
   ],
   "title": "A new Japanese TTS system based on speech-prosody database and speech modification",
   "original": "i00_3342",
   "page_count": 4,
   "order": 548,
   "p1": "vol. 3, 342-345",
   "pn": "",
   "abstract": [
    "This paper describes a new Japanese text-to-speech (TTS) system that can produce highly natural and intelligible synthetic speech. The good performance of the new TTS system derives from three new sophisticated approaches as follows; (1) A new prosody control algorithm that uses prosody data extracted from a natural speech database and a duration control algorithm based on statistical estimation. (2) A new type of synthesis unit that consists of a consonant with following vowel chain. The unit suppresses unnatural sounds and acoustic discontinuities at concatenation points by preparing synthesis units with various lengths and various F0 contours. (3) A new speech modification algorithm with harmonics reconstruction. To evaluate the new modules and the total performance of the new TTS system, listening tests are carried out. The results confirm that the new modules work together effectively, and that the new TTS system can produce high quality synthesized speech.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-546"
  },
  "sansegundo00b_icslp": {
   "authors": [
    [
     "Ruben",
     "San-Segundo"
    ],
    [
     "Juan Manuel",
     "Montero"
    ],
    [
     "Ricardo de",
     "Córdoba"
    ],
    [
     "Juana",
     "Gutiérrez-Arriola"
    ]
   ],
   "title": "Stress assignment in Spanish proper names",
   "original": "i00_3346",
   "page_count": 4,
   "order": 549,
   "p1": "vol. 3, 346-349",
   "pn": "",
   "abstract": [
    "In this paper, we propose an approach for Stress Assignment in Spanish Proper Names, based on a Multi-Layer Perceptron (MLP). When assigning stress to a word, we first analyse each vowel in the word and then calculate a Stress-Confidence Measure for it, using a MLP. The system will assign the stress to the vowel with the highest stress-confidence measure. In this paper we present and analyse different alternatives for the inputs to the Multi-Layer Perceptron. In all cases, we consider the number of vowels in the name and the vowel position in the word (taking into account only the vowels in the analysed word). For the rest of inputs, we consider a window of letters. These letters are obtained from the context of the vowel considered and from the word ending, in a similar way to [1]. We propose a Discrimination Measure to analyse the discrimination power for the different input configurations and we validate this measure and present the results obtained in each case. For the best configuration we obtain a 94.9% proper names correctly stressed (5.1% error rate). These results are compared to similar experiments using a Memory based learning approach (k- Nearest Neighbours).\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-547"
  },
  "niu00_icslp": {
   "authors": [
    [
     "Zhengyu",
     "Niu"
    ],
    [
     "Peiqi",
     "Chai"
    ]
   ],
   "title": "Segmentation of prosodic phrases for improving the naturalness of synthesized Mandarin Chinese speech",
   "original": "i00_3350",
   "page_count": 5,
   "order": 550,
   "p1": "vol. 3, 350-353",
   "pn": "",
   "abstract": [
    "It is noticed that i n natural speech sentences are breaked into breath groups. Some words seem to be more closely grouped with adjacent words: we call these groups prosodic phrases. In order to improve the naturalness of synthesized speech, prosodic processing in both text-processing component and speech generation component is needed. The text-processing component is more important because the performance of speech generation component is dependent on the ability of the previous one. This paper discussed how to break sentences into prosodic phrases.\n",
    "At first, for segmentation of prosodic phrases, the text is segmented into Chinese words. Then these words are annotated with an automatic Part-of-Speech tagger. Adjacent words which have close syntactic relation are grouped to form prosodic phrases using the POS tags and syntactic phrase structure information. When breaking prosodic phrases other factors must be taken into consideration, such as speech velocity, pragmatic knowledge, the context, and the speaker's feeling.\n",
    "The POS tagging algorithm is based on integration of the statistical method and rule method.2-Gram Markov language model is used in the algorithm. The most likely POS sequence for a given sentence is found by searching through the language model and picking the most likely path. Then the rule method is used to correct the errors caused by statistical method, which identifies a word's category using context information. Through experiments the tagger correctly tagged 94% of words in an independent test set of 1.2 thousand Chinese characters.\n",
    "Based on rules, the lexical information and phrase structure information will be used to form prosodic phrases. Through experiments we obtained a break-correct figure of 86% and a recall rate of 90%. After segmentation of prosodic phrases, these grouped words are read continuously when the text is converted to speech. And the naturalness of synthesized speech is improved.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-548"
  },
  "liu00e_icslp": {
   "authors": [
    [
     "Xiaohu",
     "Liu"
    ],
    [
     "Douglas",
     "O'Shaughnessy"
    ]
   ],
   "title": "Practical language modeling: an interpolating method",
   "original": "i00_3354",
   "page_count": 4,
   "order": 551,
   "p1": "vol. 3, 354-357",
   "pn": "",
   "abstract": [
    "Language modeling is a key component in speech and handwriting recognition. N-gram language modeling is used as the formalism of choice for a wide range of domains. Although a high order N can reduce perplexity greatly, it is unrealistic in many practical cases to get statistically reliable N-grams. We propose an interpolated model by introducing signal words and clue words into the baseline N-gram model. The initial word in a word pair with high mutual information is chosen as a signal word. In the same way, we define such words that have high mutual information with a certain morphological form as clue words. In a given context, we select a signal word with the highest score to compute the probability of the current word, and a clue word with the highest score to estimate the probability of the form of the current word. We discuss the basic requirements of designing an interpolating language model and see how our models satisfy the requirements. We got considerable reduction in perplexity, compared to the baseline model. Because both signal words and clue words are easy to collect and handle, the proposed method is practical.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-549"
  },
  "li00i_icslp": {
   "authors": [
    [
     "Gongjun",
     "Li"
    ],
    [
     "Na",
     "Dong"
    ],
    [
     "Toshiro",
     "Ishikawa"
    ]
   ],
   "title": "Combination of different n-grams based on their different assumptions",
   "original": "i00_3358",
   "page_count": 4,
   "order": 552,
   "p1": "vol. 3, 358-361",
   "pn": "",
   "abstract": [
    "This paper addresse the negative impact of assumptions artificially introduced from different ngram on its performance in natural language processing. To raise the power of modeling language information, we propose several schemes to combine conventional different order n-gram language model together by introducing probabilities of assumption. The assumption probabilities are estimated on the basis of discriminative estimation criterion. We evaluate the improved n-gram on the platform of conversion from Chinese pinyin to Chinese character. The experimental results show that the error rate could be remarkably reduced by at most 55.2%. Besides, the improved language model can solve the data sparsity problem.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-550"
  },
  "kawaguchi00_icslp": {
   "authors": [
    [
     "Nobuo",
     "Kawaguchi"
    ],
    [
     "Shigeki",
     "Matsubara"
    ],
    [
     "Hiroyuki",
     "Iwa"
    ],
    [
     "Shoji",
     "Kajita"
    ],
    [
     "Kazuya",
     "Takeda"
    ],
    [
     "Fumitada",
     "Itakura"
    ],
    [
     "Yasuyoshi",
     "Inagaki"
    ]
   ],
   "title": "Construction of speech corpus in moving car environment",
   "original": "i00_3362",
   "page_count": 4,
   "order": 553,
   "p1": "vol. 3, 362-365",
   "pn": "",
   "abstract": [
    "The Center for Integrated Acoustic Information Research (CIAIR) at Nagoya University has been collecting speech corpora in moving cars which are made available as resources to advance the research and development of robust ASRs and spoken dialogue systems under high-noise conditions. The speech corpus consists of (1) phonetically balanced sentences, (2) digit strings, (3) discrete words and (4) transcribed spoken dialogues between drivers and information systems for navigation and information retrieval. These data are collected in vehicles under both idling and driving situations. The language of the corpus is currently Japanese. The number of subjects is currently about 300, total recording time is over 200 hours and total corpus size is about 160GByte. We have also been recording video images from three different angles, vehicle-control signals, and vehicle location, all synchronized with the speech recording. We report the objective of the speech corpus, the recording methods and the recording vehicle developed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-551"
  },
  "lee00b_icslp": {
   "authors": [
    [
     "Yue-Shi",
     "Lee"
    ],
    [
     "Hsin-Hsi",
     "Chen"
    ]
   ],
   "title": "Parsing spoken dialogues",
   "original": "i00_3366",
   "page_count": 4,
   "order": 554,
   "p1": "vol. 3, 366-369",
   "pn": "",
   "abstract": [
    "This paper presents a spoken language processing system for parsing spoken dialogues. The differences between spoken data and written data are clarified. At first, we employ acoustic and prosodic cues to remove noises and identify the linguistic boundaries. Then a fast multi-level chunking-and-raising parser is used to analyze the more \"clean\" spoken data. The experimental results in parsing a Chinese spoken corpus show that the labeled precision and recall rates are 94.14% and 90.97%, respectively.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-552"
  },
  "lindberg00b_icslp": {
   "authors": [
    [
     "Børge",
     "Lindberg"
    ],
    [
     "Finn Tore",
     "Johansen"
    ],
    [
     "Narada",
     "Warakagoda"
    ],
    [
     "Gunnar",
     "Lehtinen"
    ],
    [
     "Zdravko",
     "Kacic"
    ],
    [
     "Andrej",
     "Zgank"
    ],
    [
     "Kjell",
     "Elenius"
    ],
    [
     "Giampiero",
     "Salvi"
    ]
   ],
   "title": "A noise robust multilingual reference recogniser based on SPEECHDAT(II)",
   "original": "i00_3370",
   "page_count": 4,
   "order": 555,
   "p1": "vol. 3, 370-373",
   "pn": "",
   "abstract": [
    "An important aspect of noise robustness of automatic speech recognisers (ASR) is the proper handling of non-speech acoustic events. The present paper describes further improvements of an already existing reference recogniser towards achieving such kind of robustness. The reference recogniser applied is the COST 249 SpeechDat reference recogniser, which is a fully automatic, language-independent training procedure for building a phonetic recogniser (http://www.telenor.no/fou/prosjekter/taletek/refrec). The reference recogniser relies on the HTK toolkit and a SpeechDat(II) compatible database, and is designed to serve as a reference system in multilingual speech recognition research. The paper describes version 0.96 of the reference recogniser which take into account labelled non-speech acoustic events during training and provides robustness against these during testing. Results are presented on small and medium vocabulary recognition for six languages.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-553"
  },
  "lv00_icslp": {
   "authors": [
    [
     "Muhua",
     "Lv"
    ],
    [
     "Lianhong",
     "Cai"
    ]
   ],
   "title": "The design and application of a speech database for Chinese TTS system",
   "original": "i00_3378",
   "page_count": 4,
   "order": 556,
   "p1": "vol. 3, 378-381",
   "pn": "",
   "abstract": [
    "The design and application of a speech database for Mandarin TTS system is presented in this paper. To build a scientific, versatile speech database to meet the call for improving the quality of synthesis units and enhancing previous prosodic models, is the main point of the research. The database structure and contents and the methodology for creating similar database are described, and also some statistics and some research based on the database.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-554"
  },
  "chengalvarayan00c_icslp": {
   "authors": [
    [
     "Rathinavelu",
     "Chengalvarayan"
    ]
   ],
   "title": "Use of multiple classifiers for speech recognition in wireless CDMA network environments",
   "original": "i00_3382",
   "page_count": 4,
   "order": 557,
   "p1": "vol. 3, 382-385",
   "pn": "",
   "abstract": [
    "In this paper, we address the problem and the use of multiple classifiers for robust recognition over the cellular network. The idea is to provide more variability to the system to be trained, and to support this variability with more number of model parameters. The main drawback is that the model size, and the computational complexity increases linearly related to different call environment. To alleviate this problem we first introduce a new measure called the average-arc-count into the decoding process. The main ad- vantage of this new measure is that many of the multiple classifiers can be shut down during the recognition stage if the average-arc-count of individual classifier exceeds a certain threshold limit for a given utterance. Secondly, we can also build individual classifiers with less number of parameters and without degrading the overall system performance. Experimental results on English connected digit recognition task show a string error rate reduction of as much as 40% by using the multiple classifiers when compared to individual CDMA systems.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-555"
  },
  "franz00_icslp": {
   "authors": [
    [
     "Alexander",
     "Franz"
    ],
    [
     "Keiko",
     "Horiguchi"
    ],
    [
     "Lei",
     "Duan"
    ]
   ],
   "title": "An imperative programming language for spoken language translation",
   "original": "i00_3386",
   "page_count": 4,
   "order": 558,
   "p1": "vol. 3, 386-389",
   "pn": "",
   "abstract": [
    "This paper describes the Grammar Programming Language (GPL), a new formalism for feature-structure-based linguistic computation. GPL was designed to meet the needs of spoken language translation. GPL is easy to use, concise, and efficient, and it allows the direct expression of detailed linguistic algorithms. GPL was used successfully as the basis of Sony's machine translation project.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-556"
  },
  "wakita00_icslp": {
   "authors": [
    [
     "Yumi",
     "Wakita"
    ],
    [
     "Kenji",
     "Matsui"
    ],
    [
     "Yoshinori",
     "Sagisaka"
    ]
   ],
   "title": "Fine keyword clustering using a thesaurus and example sentences for speech translation",
   "original": "i00_3390",
   "page_count": 4,
   "order": 559,
   "p1": "vol. 3, 390-393",
   "pn": "",
   "abstract": [
    "For robust speech translation, we propose a new language translation method in which speech recognition results are mapped to example sentences using keywords. In this method, the keyword clustering is used to cope with recognition errors and the wide variety of words that do not appear in the training corpus. Initial classes defined using only thesaurus are redefined by using the dependency between the keywords in limited number of example sentences. The effectiveness of our keyword clustering method is confirmed through example sentence search experiments. These experiments were done using keyword sets of (a) different sentences including keywords not in the example sentences and (b) recognition results those sentences in which recognition errors were obtained. Compared with the search method which uses keyword sets defined by using only a thesaurus, our proposed method offered improved search error rates.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-557"
  },
  "feng00_icslp": {
   "authors": [
    [
     "JunLan",
     "Feng"
    ],
    [
     "XianFang",
     "Wang"
    ],
    [
     "LiMin",
     "Du"
    ]
   ],
   "title": "Data collection and processing in a Chinese spontaneous speech corpus IIS_CSS",
   "original": "i00_3394",
   "page_count": 4,
   "order": 560,
   "p1": "vol. 3, 394-397",
   "pn": "",
   "abstract": [
    "In this paper we report on the first phase of the speech corpus ISS_CSS collection for purposes of the CEST(Chinese-English speech translation) project. The corpus is intended to provide training material for speaker independent spontaneous Chinese speech recognition and automatic dialogue management over the telephone line. This paper describes the collection measures, processing methods, annotation and contents of this corpus. It consists of two parts: human-human dialogues and human-machine dialogues. Presently, the corpus has finished 10-hour speech and the associated annotation. Finally, we will present our collecting plan in the future.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-558"
  },
  "aizawa00_icslp": {
   "authors": [
    [
     "Yasuyuki",
     "Aizawa"
    ],
    [
     "Shigeki",
     "Matsubara"
    ],
    [
     "Nobuo",
     "Kawaguchi"
    ],
    [
     "Katsuhiko",
     "Toyama"
    ],
    [
     "Yasuyoshi",
     "Inagaki"
    ]
   ],
   "title": "Spoken language corpus for machine interpretation research",
   "original": "i00_3398",
   "page_count": 4,
   "order": 561,
   "p1": "vol. 3, 398-401",
   "pn": "",
   "abstract": [
    "This paper describes a database consisting of speech and language, which we are currently constructing for the purpose of the research on machine interpretation. The database contains bilingual data of lectures and dialogues. We have collected the speech of about 72 hours in total and transcribed it into the text manually. We have investi- gated the database in order to acquire empirical knowledge of human interpreting. In this paper, we report the charac- teristic features of spoken language by Japanese-to-English interpreters.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-559"
  },
  "santen00_icslp": {
   "authors": [
    [
     "Jan van",
     "Santen"
    ],
    [
     "Michael",
     "Macon"
    ],
    [
     "Andrew",
     "Cronk"
    ],
    [
     "John-Paul",
     "Hosom"
    ],
    [
     "Alexander",
     "Kain"
    ],
    [
     "Vincent",
     "Pagel"
    ],
    [
     "Johan",
     "Wouters"
    ]
   ],
   "title": "When will synthetic speech sound human: role of rules and data",
   "original": "i00_3402",
   "page_count": 8,
   "order": 562,
   "p1": "vol. 3, 402-409",
   "pn": "",
   "abstract": [
    "Text-to-speech synthesis research has moved away from building general purpose systems based on an understanding of human language and speech production towards building systems based on statistical algorithms applied to large text and speech corpora, and, recently, towards building such systems for specific domains. Despite substantial progress, the overall quality of even the best systems is often still inadequate for broad user acceptance in applications that cannot also be handled with simple phrase splicing. This tutorial paper analyzes which problems must be addressed to achieve the goal of generating naturalsounding speech in limited domains in a cost-effective way, and the roles of data and rules as we work towards solutions.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-560"
  },
  "syrdal00b_icslp": {
   "authors": [
    [
     "Ann K.",
     "Syrdal"
    ],
    [
     "Colin W.",
     "Wightman"
    ],
    [
     "Alistair",
     "Conkie"
    ],
    [
     "Yannis",
     "Stylianou"
    ],
    [
     "Mark",
     "Beutnagel"
    ],
    [
     "Juergen",
     "Schroeter"
    ],
    [
     "Volker",
     "Strom"
    ],
    [
     "Ki-Seung",
     "Lee"
    ],
    [
     "Matthew J.",
     "Makashay"
    ]
   ],
   "title": "Corpus-based techniques in the AT&t nextgen synthesis system",
   "original": "i00_3410",
   "page_count": 6,
   "order": 563,
   "p1": "vol. 3, 410-415",
   "pn": "",
   "abstract": [
    "The AT&T text-to-speech (TTS) synthesis system has been used as a framework for experimenting with a perceptually-guided data-driven approach to speech synthesis, with primary focus on data-driven elements in the \"back end\". Statistical training techniques applied to a large corpus are used to make decisions about predicted speech events and selected speech inventory units. Our recent advances in automatic phonetic and prosodic labeling and a new faster harmonic plus noise model (HNM) and unit preselection implementations have significantly improved TTS quality and speeded up both development time and runtime.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-561"
  },
  "campbell00_icslp": {
   "authors": [
    [
     "Nick",
     "Campbell"
    ]
   ],
   "title": "Limitations to concatenative speech synthesis",
   "original": "i00_3416",
   "page_count": 4,
   "order": 564,
   "p1": "vol. 3, 416-419",
   "pn": "",
   "abstract": [
    "This paper discusses techniques for determining the linguistic needs for open-domain synthesis by concatenative methods, and reports on the design and evaluation of a tool for collecting and balancing a speech corpus automatically, in order to ensure optimal coverage of the sounds required for synthesis within a given task-domain. Synthetically-generated utterances are used to prompt speakers, and in-line acoustic analysis determines the prosodic as well as phonemic balance of the resulting speech during recording, re-prompting the speaker with textually modified versions if necessary, to elicit the desired articulation sequences. The closed-loop process, which incorporates human self-correction and evaluation, allows for more e\u000ecient collection of a balanced corpus for concatenative speech synthesis.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-562"
  },
  "kawai00_icslp": {
   "authors": [
    [
     "Hisashi",
     "Kawai"
    ],
    [
     "Seiichi",
     "Yamamoto"
    ],
    [
     "Norio",
     "Higuchi"
    ],
    [
     "Tohru",
     "Shimizu"
    ]
   ],
   "title": "A design method of speech corpus for text-to-speech synthesis taking account of prosody",
   "original": "i00_3420",
   "page_count": 6,
   "order": 565,
   "p1": "vol. 3, 420-425",
   "pn": "",
   "abstract": [
    "This paper proposes a method for designing a sentence set for utterances taking account of prosody. This method is based on a measure of coverage which incorporates two factors: (1) the distribution of voice fundamental frequency and phoneme duration predicted by the prosody generation module of a TTS; (2) perceptual damage to naturalness due to prosody modification. A set of 500 sentences with a predicted coverage of 82.6% was designed by this method, and used to collect a speech corpus. The obtained speech corpus yielded 88% of the predicted coverage. The data size was reduced to 49% in terms of number of sentences (89% in terms of number of phonemes) compared to a general-purpose corpus designed without taking prosody into account.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-563"
  },
  "sproat00_icslp": {
   "authors": [
    [
     "Richard",
     "Sproat"
    ]
   ],
   "title": "Corpus-based methods and hand-built methods",
   "original": "i00_3426",
   "page_count": 3,
   "order": 566,
   "p1": "vol. 3, 426-428",
   "pn": "",
   "abstract": [
    "Recent success of statistical corpus-based methods in a variety of areas of speech and language processing has led to the widespread view that traditional hand-built \"rule-based\" approaches are moribund. This is a misconception. As I shall argue in this talk, it is unlikely that rule-based approaches will ever be eliminated. Two examples are given to support this conclusion; one where the linguistic facts, though highly complex are basically quite regular; and another where the linguistic fact is exceedingly simple (hence hardly worth the effort of inferring from data), but where adding in this information can improve the output of a statistical model.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-564"
  },
  "picheny00_icslp": {
   "authors": [
    [
     "Michael A.",
     "Picheny"
    ]
   ],
   "title": "Heredity and environment in speech recognition: the role of a priori information vs. data",
   "original": "i00_3429",
   "page_count": 5,
   "order": 567,
   "p1": "vol. 3, 429-433",
   "pn": "",
   "abstract": [
    "Most significant advances in speech recognition over the last thirty years can be attributed to the easy availability of everincreasing corpora of speech and language data and the development of simple trainable parametric statistical models that take advantage of this data. Hidden Markov Models, n-gram language models, and linear-discriminant based feature extraction are all examples of such data-driven algorithms. However, there is a general feeling in the recognition community that there is a large untapped body of knowledge encompassing a priori sources of information in speech and language that can be mined to serve as the basis for the next generation of improvements in speech recognition systems. Such sources of information include constraints imposed by articulatory models, the grammatical structure of language, and phonology. This paper reviews previous abortive attempts to utilize a priori information in speech recognition and contrasts them with data-driven approaches that seem to more successfully capture information of a similar nature. It also highlights some recent attempts to incorporate explicit sources of speech and language knowledge and speculates on possibilities for synergy between the two approaches in the future.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-565"
  },
  "kubozono00_icslp": {
   "authors": [
    [
     "Haruo",
     "Kubozono"
    ]
   ],
   "title": "A constraint-based analysis of compound accent in Japanese",
   "original": "i00_3438",
   "page_count": 4,
   "order": 568,
   "p1": "vol. 3, 438-441",
   "pn": "",
   "abstract": [
    "This paper describes the accentuation of compound nouns in Tokyo Japanese in the framework of Optimality Theory (Prince & Smolensky 1993). This new theory assumes that output forms of language production are determined by the interaction of language-universal constraints that are ranked in a particular way for a particular grammar. In this respect it differs from previous phonological theories which held that optimal outputs are produced by (mostly language-specific) rules that apply to input structures in a derivational, i.e. step-by-step, fashion. The ultimate goal of this paper is to demonstrate that the new constraint-based approach provides a simple description of the seemingly complex system of compound noun accentuation in (Tokyo) Japanese which cannot be captured by the traditional rule-based derivational approach.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-566"
  },
  "iwahashi00_icslp": {
   "authors": [
    [
     "Naoto",
     "Iwahashi"
    ]
   ],
   "title": "Language acquisition through a human-robot interface",
   "original": "i00_3442",
   "page_count": 6,
   "order": 569,
   "p1": "vol. 3, 442-447",
   "pn": "",
   "abstract": [
    "This paper describes an algorithm for spoken language acquisition through a human-robot interface based on speech, vision, and behavior. In this algorithm the grounded language knowledge is represented by graphical statistical models consisting of hidden Markov models and stochastic context- free grammar. The learning of the lexicon is based on the independence between speech and visual features in each of lexical items. In the grammar-learning process, the syntactic structure of each spoken utterance is inferred from the conceptual structure extracted from the visual observation. The algorithm is robust against ambiguity and sparseness of learning data because it is based on information-theoretical learning.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-567"
  },
  "sagisaka00_icslp": {
   "authors": [
    [
     "Yoshinori",
     "Sagisaka"
    ],
    [
     "Hirofumi",
     "Yamamoto"
    ],
    [
     "Minoru",
     "Tsuzaki"
    ],
    [
     "Hiroaki",
     "Kato"
    ]
   ],
   "title": "Rules, but what for? - rule description as efficient and robust abstraction of corpora and optimal fitting to applications -",
   "original": "i00_3448",
   "page_count": 4,
   "order": 570,
   "p1": "vol. 3, 448-451",
   "pn": "",
   "abstract": [
    "Two recent studies are introduced in speech recognition and speech synthesis to reconsider what rules should be looked for spoken language science and technology. To abstract the neighboring characteristics expressed by Ngrams, multi-class composite N-grams have been proposed to model POS characteristics and inflectional forms separately. It is shown that statistical clustering can provide more compact and robust description of word neighboring characteristics than conventional N-grams. For speech synthesis, segmental duration modeling has been examined from the viewpoint of perceptual characteristics of duration changes. A series of perceptual experiments have shown the context dependency of sensitivity to duration change. These two examples respectively illustrate how current rules are interpreted to build scientifically acceptable engineering models and remind us of the deeper scientific meaning and limitation of generalization as a rule.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-568"
  },
  "makarova00_icslp": {
   "authors": [
    [
     "Veronika",
     "Makarova"
    ]
   ],
   "title": "Cross-linguistic aspects of intonation perception",
   "original": "i00_3452",
   "page_count": 5,
   "order": 571,
   "p1": "vol. 3, 452-453",
   "pn": "",
   "abstract": [
    "This paper compares the perception strategies employed by the speakers of British English, Japanese and Russian in sentence type processing relying on prosodic clues. The paper reports the results of an experimental phonetic study investigating the effect of the manipulations of pitch and duration in a phonetically rising-falling contour on sentence type perception by the three groups of subjects. The results of the experiment indicate the existence of similar perception patterns and of a common threshold of declarative judgement across the three groups of listeners.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-569"
  },
  "kubozono00b_icslp": {
   "authors": [
    [
     "Haruo",
     "Kubozono"
    ],
    [
     "Shosuke",
     "Haraguchi"
    ]
   ],
   "title": "Visual information and the perception of prosody",
   "original": "i00_3454",
   "page_count": 4,
   "order": 572,
   "p1": "vol. 3, 454-457",
   "pn": "",
   "abstract": [
    "This paper reports the results of a phonetic experiment carried out to explore the potential importance of visual information for the perception of Japanese prosody and its relationship with phonological structure. It specifically examines whether and to what extent native speakers of Japanese are able to accurately perceive the temporal structure of their native language on the sole basis of visual information. It has been found that the word-final contrast between short and long vowels is totally invisible to the native speakers although other types of temporal differences characteristic of mora-timed languages are readily visible as a crucial distinction in the mora-timed language. Interestingly, essentially the same confusion occurs in Japanese phonology, where word-final long vowels tend to be shortened and neutralized with their short counterparts.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-570"
  },
  "akagi00_icslp": {
   "authors": [
    [
     "Masato",
     "Akagi"
    ],
    [
     "Hironori",
     "Kitakaze"
    ]
   ],
   "title": "Perception of synthesized singing voices with fine fluctuations in their fundamental frequency contours",
   "original": "i00_3458",
   "page_count": 4,
   "order": 573,
   "p1": "vol. 3, 458-461",
   "pn": "",
   "abstract": [
    "This paper demonstrates the importance of fine fluctuations quantitatively by measuring the detection thresholds of fine fluctuations in singing-voice F0s, in which voice quality is particularly important. We analyzed the fine fluctuations left by subtracting the melody and vibrato components from estimated F0s, focusing on the modulation frequency (MF) and modulation amplitude (MA). To test a hypothesis that the fine fluctuations in the F0 of singing voices affect the perception of quality and that the magnitude of this effect depends on the MF and MA, we performed four psychoacoustic experiments using synthesized stimuli. The experimental results indicate that our hypothesis was correct, and suggest that, to produce high-quality synthesized speech, one should extract F0s containing fine fluctuations with an MF of over 7 Hz in the analysis and add not only melody and vibrato but also fine fluctuation components to the F0 contours in the synthesis.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-571"
  },
  "palomaki00_icslp": {
   "authors": [
    [
     "Kalle J.",
     "Palomäki"
    ],
    [
     "Paavo",
     "Alku"
    ],
    [
     "Ville",
     "Mäkinen"
    ],
    [
     "Patrick",
     "May"
    ],
    [
     "Hannu",
     "Tiitinen"
    ]
   ],
   "title": "Neuromagnetic study on localization of speech sounds",
   "original": "i00_3462",
   "page_count": 4,
   "order": 574,
   "p1": "vol. 3, 462-465",
   "pn": "",
   "abstract": [
    "Spatial processing of speech sounds by the human auditory cortex was studied measuring neuromagnetic responses utilizing magnetoencephalography (MEG). Realistic spatial sound environment was produced using modern stimulus generation methodology utilizing head-related transfer functions (HRTFs). In order to compare localization of speech sounds to that of nonspeech, the stimulus set involved three different stimulus types: 1) a semi-synthetic /a/-vowel, 2) a pseudo-vowel composed as a sum of sinusoids and 3) a wide band noise burst. Stimuli were filtered through HRTFs of eight horizontal equally spaced directions. The most prominent response, the cortically generated N1m, was investigated above the left and right hemisphere. We found, firstly, that cortical activity reflecting the processing of spatial sound stimuli was more pronounced in the right than in the left hemisphere. Secondly, we found that N1m amplitudes were largest for the /a/-vowel. However, behaviour of the N1m amplitude elicited by the pseudo-vowel was relatively similar to that of the /a/-vowel.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-572"
  },
  "hirose00c_icslp": {
   "authors": [
    [
     "Yukiyoshi",
     "Hirose"
    ],
    [
     "Kazuhiko",
     "Kakehi"
    ]
   ],
   "title": "Perception of identical vowel sequences in Japanese conversational speech",
   "original": "i00_3466",
   "page_count": 4,
   "order": 575,
   "p1": "vol. 3, 466-469",
   "pn": "",
   "abstract": [
    "Sequences of more than two identical vowels across word boundaries can occasionally be found in Japanese speech. Our previous study (Kakehi and Hirose, 1998) investigated how hearers perceive such vowel sequences. We used isolated sentences as test materials, provided that the duration of such vowel sequences increases in proportion to the number of morae. We found that hearers can detect the number of morae in the vowel sequence by using cues such as the pitch pattern, speech rhythm, and duration. The present study attempts to examine cases in which the vowel sequence occurs as part of natural conversation (part of planned dialogs). The materials were designed so that the target vowel sequences do not have any distinctive pitch movement. In the recorded sets of conversation that we examined, the relationship between the duration and the number of morae in the vowel sequence was roughly proportional. A series of perception tests investigated whether hearers can correctly detect the number of morae in such utterances, while the pitch movement in the target was kept stable. The results indicated that the duration of vowel sequences does not serve as a sufficient cue to detect the number of morae.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-573"
  },
  "fernandez00_icslp": {
   "authors": [
    [
     "Santiago",
     "Fernández"
    ],
    [
     "Sergio",
     "Feijóo"
    ]
   ],
   "title": "Acoustic cues to perception of vowel quality",
   "original": "i00_3470",
   "page_count": 4,
   "order": 576,
   "p1": "vol. 3, 470-473",
   "pn": "",
   "abstract": [
    "Perceptual and acoustic spaces obtained using MDS and PCA, respectively, for a set of Spanish vowels uttered in various fricative contexts are compared. Acoustic cues to vowel quality were studied through examination of the weights given by PCA to specific spectral regions. The acoustic space formed by the first two formant frequencies was also evaluated. The results show that MDS has serious difficulties to be useful for the study of Spanish vowels. PCA offers a gross characterization compared to formant frequencies. Neither of them completely explains the perception of vowel quality.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-574"
  },
  "klabbers00_icslp": {
   "authors": [
    [
     "Esther",
     "Klabbers"
    ],
    [
     "Raymond",
     "Veldhuis"
    ],
    [
     "Kim",
     "Koppen"
    ]
   ],
   "title": "A solution to the reduction of concatenation artefacts in speech synthesis",
   "original": "i00_3474",
   "page_count": 4,
   "order": 577,
   "p1": "vol. 3, 474-477",
   "pn": "",
   "abstract": [
    "One problem with speech synthesis impeding high quality is the occurrence of audible discontinuities at segment boundaries. Formant jumps across concatenation points suggest the problem to be due to spectral differences. The problem is most apparent in vowels and semi-vowels. We propose to reduce the number of audible discontinuities by adding context-sensitive diphones to the database. The number of additional diphones is limited by clustering contexts with similar spectral effects on the neighbouring vowels, using the Kullback-Leibler distance. A listening experiment has shown that the percentage of perceived discontinuities has significantly decreased.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-575"
  },
  "wang00n_icslp": {
   "authors": [
    [
     "Jhing-Fa",
     "Wang"
    ],
    [
     "Hsien-Chang",
     "Wang"
    ],
    [
     "Kin-Nan",
     "Lee"
    ],
    [
     "Chieh-Yi",
     "Huang"
    ]
   ],
   "title": "Domain-unconstrained language understanding based on CKIP-auto tag, how-net, and ART",
   "original": "i00_3478",
   "page_count": 4,
   "order": 578,
   "p1": "vol. 3, 478-481",
   "pn": "",
   "abstract": [
    "In this paper, we propose a method for domain unconstrained language understanding based on the linguistic toolsets CKIP-AutoTag and How-net, and the devised Acting Role Table (ART).\n",
    "In our approach, the analysis of article is performed sentence by sentence. For each sentence in the article, word segmentation is first performed using CKIP-AutoTag. Next, the semantics of each sentence is represented in the devised Acting-Role-Table (ART). The ART consists of acting roles of the sentences, i.e., action, agent, instrument, theme, location, and time, together with their associated modifiers. In this step, we use verb-driven syntax analysis to determine the acting roles in the sentences; and use semantic analysis to constrain time acting roles based on time features defined in How-net. Finally, the semantic network is constructed to record the relationship of each sentence.\n",
    "To test whether our approach for text understanding is feasible, an auto reading comprehension system is built trying to answer the exercises of the primary school textbook. The exercises contain many questions related to the article. Seven textbooks ranged from third-grade to ninth-grade are used as our testing articles. Most of the questions in the exercises can be answered by our system if the answers can be derived from the article.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-576"
  },
  "powell00_icslp": {
   "authors": [
    [
     "Chris",
     "Powell"
    ],
    [
     "Mary",
     "Zajicek"
    ],
    [
     "David",
     "Duce"
    ]
   ],
   "title": "The generation of representations of word meanings from dictionaries",
   "original": "i00_3482",
   "page_count": 4,
   "order": 579,
   "p1": "vol. 3, 482-485",
   "pn": "",
   "abstract": [
    "This paper describes the generation of iconic and categorical representations of word meaning, in propositional form, from the WordNet lexical database. These are derived from the list of synonyms, the descriptive gloss, and from the hypernym and meronym relations of each WordNet word sense. We demonstrate that these representations promote identification and discrimination, these being suggested qualities of representations of meaning, and finally suggest that these representations have further applications in language engineering.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-577"
  },
  "luk00_icslp": {
   "authors": [
    [
     "Po Chui",
     "Luk"
    ],
    [
     "Helen",
     "Meng"
    ],
    [
     "Filung",
     "Wang"
    ]
   ],
   "title": "Grammar partitioning and parser composition for natural language understanding",
   "original": "i00_3486",
   "page_count": 4,
   "order": 580,
   "p1": "vol. 3, 486-489",
   "pn": "",
   "abstract": [
    "This paper presents an approach for natural language understanding, which integrates multiple sub-grammars and sub-parsers; in contrast with the traditional single grammar and parser approach. The use of GLR(k) parsers for natural language understanding is hampered by the problem of exponential growth of the parsing table size as the size of grammar rules increases. Hence, we propose to partition a grammar into multiple sub-grammars. For each sub-grammar we generate its own parsing table together with its specialized GLR sub-parser. The total size of the sub-grammars parsing tables is much smaller than the size of the parsing table of the unpartitioned grammar. A parser composition algorithm then combines the sub-parsers' outputs to produce an overall parse that is identical to that produced by a single parser. Results based on natural language queries in the Air Travel Information System (ATIS) domain shows that this is a viable and efficient approach applicable to both English and Chinese.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-578"
  },
  "lai00_icslp": {
   "authors": [
    [
     "Jennifer",
     "Lai"
    ],
    [
     "Omer",
     "Tsimhoni"
    ],
    [
     "Paul",
     "Green"
    ]
   ],
   "title": "Comprehension of synthesized speech while driving and in the lab",
   "original": "i00_3490",
   "page_count": 4,
   "order": 581,
   "p1": "vol. 3, 490-493",
   "pn": "",
   "abstract": [
    "Two studies were conducted to measure the comprehensibility of synthetic speech with current text-tospeech technology. Baseline measurements for each subject were obtained using recorded natural speech. The first study was conducted in a quiet lab with no distractions. Half the subjects were allowed to take notes while listening, the other half were not. Findings showed that there was no significant difference in comprehension of synthetic speech among the five different text-to-speech engines used. Subjects that did not take notes performed significantly worse for all synthetic voice tasks when compared to natural speech tasks. In the second study, in addition to measuring comprehension levels, driving performance was examined. Findings suggest that under low workload conditions (very predictable two-lane roads with minimal traffic, no intersections and driving with cruise control), listening to messages does not interfere with driving. This holds for both natural and text-to-speech presentations, though comprehension of natural speech is better and subjects felt so.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-579"
  },
  "tyler00_icslp": {
   "authors": [
    [
     "Michael D.",
     "Tyler"
    ],
    [
     "Denis K.",
     "Burnham"
    ]
   ],
   "title": "Orthographic influences on initial phoneme addition and deletion tasks: the effect of lexical status",
   "original": "i00_3494",
   "page_count": 4,
   "order": 582,
   "p1": "vol. 3, 494-497",
   "pn": "",
   "abstract": [
    "Here a study is conducted to examine orthographic effects on initial phoneme addition and deletion tasks with an adult population. The results of a previous study [1] with children suggest that the use of an orthographic strategy depends on lexical status, so two experiments were conducted, one with real words and the other with nonwords. In both experiments, some items were amenable to both an orthographic and phonological strategy, but the orthographic representations of the other items interfered with production of the correct response. Longer reaction times were found for interfering items in the real word tasks, but not in the nonword tasks. This result lends support to the idea that lexical access involves the automatic orthographic activation.\n",
    "",
    "",
    "Stuart, M., Processing strategies in a phoneme deletion task. Quarterly Journal of Experimental Psychology. A, Human Experimental Psychology, 1990. 42(2-A): p. 305-327.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-580"
  },
  "zolfaghari00_icslp": {
   "authors": [
    [
     "Parham",
     "Zolfaghari"
    ],
    [
     "Yoshinori",
     "Atake"
    ],
    [
     "Kiyohiro",
     "Shikano"
    ],
    [
     "Hideki",
     "Kawahara"
    ]
   ],
   "title": "Investigation of analysis and synthesis parameters of straight by subjective evaluation",
   "original": "i00_3498",
   "page_count": 4,
   "order": 583,
   "p1": "vol. 3, 498-501",
   "pn": "",
   "abstract": [
    "The goal of this paper is to locate and understand the fine fundamental problems that exist in the representation of speech sounds by a very high quality speech analysis/synthesis engine namely STRAIGHT. The approach followed here is the evaluation of this system using subjective measures. We use the diagnostic rhyme test (DRT) to evaluate the intelligibility of speech analysed and synthesised by this system for various analysis frame-rates. Consequently we catagorise the fine problems and suggest possible improvements. The results from the DRT have indicated that STRAIGHT can produce speech with an average DRT score of 95 between 1-5 ms analysis frame-rate. In addition, a set of subjective quality measures using MOS and MNRU tests have been conducted. These tests have been carried out for three different versions of the STRAIGHT system: versions 17, 23 and 30. The DRT has been carried out using version 23 only. Based on the subjective evaluation results, a discussion of possible improvements to the STRAIGHT system is given.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-581"
  },
  "pargellis00_icslp": {
   "authors": [
    [
     "Andrew N.",
     "Pargellis"
    ],
    [
     "Alexandros",
     "Potamianos"
    ]
   ],
   "title": "Cross-domain classification using generalized domain acts",
   "original": "i00_3502",
   "page_count": 4,
   "order": 584,
   "p1": "vol. 3, 502-505",
   "pn": "",
   "abstract": [
    "Cross-domain classification for speech understanding is an interesting research problem because of the need for portable solutions in the design for spoken dialogue systems. In this paper, a two-tier classifier is proposed for speech understanding. The first tier consists of domain independent dialogue acts while the second tier consists of application actions that are domain specific. A maximum likelihood and a minimum classification error formulation are proposed for the first tier of the classifier, i.e., for dialogue act classification. The performance of the classifier is investigated for three application domains. Cross-domain classification error is two to four times higher than in-domain classification error. A 10-15% reduction in cross-domain classification error rate is achieved by adding generic domain independent training data for each dialogue act and by mapping words to semantic concepts.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-582"
  },
  "ramaswamy00_icslp": {
   "authors": [
    [
     "Ganesh N.",
     "Ramaswamy"
    ],
    [
     "Jan",
     "Kleindienst"
    ]
   ],
   "title": "Hierarchical feature-based translation for scalable natural language understanding",
   "original": "i00_3506",
   "page_count": 4,
   "order": 585,
   "p1": "vol. 3, 506-509",
   "pn": "",
   "abstract": [
    "For complex natural language understanding systems with a large number of statistically confusable but semantically different formal commands, there are many difficulties in performing an accurate translation of a user input into a formal command in a single step. This paper addresses scalability issues in natural language understanding, and describes a method for performing the translation in a hierarchical manner. The hierarchical method improves the system accuracy, reduces the computational complexity of the translation, provides additional numerical robustness during training and decoding, and permits a more efficient packaging of the components of the natural language understanding system.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-583"
  },
  "potamianos00b_icslp": {
   "authors": [
    [
     "Alexandros",
     "Potamianos"
    ],
    [
     "Hong-Kwang J.",
     "Kuo"
    ]
   ],
   "title": "Statistical recursive finite state machine parsing for speech understanding",
   "original": "i00_3510",
   "page_count": 4,
   "order": 586,
   "p1": "vol. 3, 510-513",
   "pn": "",
   "abstract": [
    "In this paper, a statistical framework for semantic parsing is described. The statistical model uses two information sources to disambiguate between rules: rule weights that capture vertical relationships in the parse tree, and concept n-grams that capture horizontal relationships. Rule design consists of simple local mapping rules that non-experts can write, and the rules are implemented as weighted finite state transducers. A general parser for context free grammars is implemented using a finite state machine library. Semantic decoding is implemented by recursively composing the rule transducer with the word-graph automaton produced from the speech recognizer. Detailed metrics for evaluating semantic parse accuracy are proposed. The parser is evaluated on the ATIS travel task with resulting precision and recall rates of over 95%. The proposed finite state transducer formulation allows the incorporation of rules and probabilities in a unified framework and the straightforward combination of acoustic, language, and understanding models.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-584"
  },
  "liu00f_icslp": {
   "authors": [
    [
     "Chaojun",
     "Liu"
    ],
    [
     "Yonghong",
     "Yan"
    ]
   ],
   "title": "Speaker change detection using minimum message length criterion",
   "original": "i00_3514",
   "page_count": 5,
   "order": 587,
   "p1": "vol. 3, 514-517",
   "pn": "",
   "abstract": [
    "Speaker change detection or speaker-based segmentation is useful and important in many applications, such as transcribing broadcast news or telephone conversations. It usually serves as a preliminary step prior to speech/speaker recognition. Among various methods proposed in the literature, Bayesian Information Criterion (BIC) based method has been widely used. In this paper, we propose to use a different criterion, Minimum Message Length criterion (MML), which is also well known in the statistical community, on speaker change detection problems. MML is an information theoretic criterion that aims to minimize the message length for the description of both model parameters and the data. Previous studies by Oliver etc. in the area other than speech, showed that MML might be a better criterion than BIC on segmentation problems. We extended their work and applied MML criterion to speaker change detection problems. Experiments were carried out on two different types of speech data, and so far, comparable results between BIC and MML have been obtained.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-585"
  },
  "furui00_icslp": {
   "authors": [
    [
     "Sadaoki",
     "Furui"
    ],
    [
     "Kikuo",
     "Maekawa"
    ],
    [
     "Hitoshi",
     "Isahara"
    ],
    [
     "Takahiro",
     "Shinozaki"
    ],
    [
     "Takashi",
     "Ohdaira"
    ]
   ],
   "title": "Toward the realization of spontaneous speech recognition - introduction of a Japanese priority program and preliminary results -",
   "original": "i00_3518",
   "page_count": 4,
   "order": 588,
   "p1": "vol. 3, 518-521",
   "pn": "",
   "abstract": [
    "Although high-recognition accuracy can be obtained for speech in the form of reading a written text or similar by using state-ofthe art speech recognition technology, the accuracy is quite poor for freely spoken spontaneous speech. From this perspective, a new national project for raising the technological level of speech recognition and understanding has recently commenced in Japan. This paper first briefly introduces the project and then reports some results of preliminary experiments which have been conducted at Tokyo Institute of Technology.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-586"
  },
  "takezawa00_icslp": {
   "authors": [
    [
     "Toshiyuki",
     "Takezawa"
    ],
    [
     "Fumiaki",
     "Sugaya"
    ],
    [
     "Masaki",
     "Naito"
    ],
    [
     "Seiichi",
     "Yamamoto"
    ]
   ],
   "title": "A comparative study on acoustic and linguistic characteristics using speech from human-to-human and human-to-machine conversations",
   "original": "i00_3522",
   "page_count": 4,
   "order": 589,
   "p1": "vol. 3, 522-525",
   "pn": "",
   "abstract": [
    "Speech translation and dialogue systems must accept conversational speech. In this paper, we discuss acoustic and linguistic characteristics based on results of speech recognition experiments using speech from human-to-human and human-to-machine conversations. Conversational speech inputs to machines consist of frozen expressions such as greetings and yes/no statements, and informative individual expressions like numerical data such as dates and telephone numbers. The former has a lower perplexity and acoustic characteristics close to spontaneous speech. The latter has a higher perplexity and acoustic characteristics close to read speech. Each utterance or each inter-pausal unit can be classified into the former or the latter. This new knowledge will help future research on speech translation and dialogue systems.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-587"
  },
  "yoma00_icslp": {
   "authors": [
    [
     "Néstor Becerra",
     "Yoma"
    ]
   ],
   "title": "Speaker dependent temporal constraints combined with speaker independent HMM for speech recognition in noise",
   "original": "i00_3526",
   "page_count": 5,
   "order": 590,
   "p1": "vol. 3, 526-529",
   "pn": "",
   "abstract": [
    "This paper addresses the problem of speech recognition in noise using speaker-dependent temporal constraints in the Viterbi algorithm in combination with speaker-independent HMM. It is shown that the speaker-dependent re-estimation of state duration parameters requires a low computational load and a small training database, and can lead to reductions in the error rate as high as 30% or 40% with clean signals and with signals corrupted by additive noise, without noise canceling methods. Moreover, the approach here covered could also be seen as a speaker adaptation method in which only temporal restrictions parameters are adapted.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-588"
  },
  "ito00b_icslp": {
   "authors": [
    [
     "Yoshihiro",
     "Ito"
    ],
    [
     "Hiroshi",
     "Matsumoto"
    ],
    [
     "Kazumasa",
     "Yamamoto"
    ]
   ],
   "title": "Forward masking on a generalized logarithmic scale for robust speech recognition",
   "original": "i00_3530",
   "page_count": 4,
   "order": 591,
   "p1": "vol. 3, 530-533",
   "pn": "",
   "abstract": [
    "This paper examines the forward masking on the generalized logarithmic scale for robust speech recognition to both additive and convolutional noise. The forward masking in the dynamic cepstral (DyC) representation is based upon subtraction of a masking pattern from a current spectrum on a logarithmic spectral domain, whereas the proposed method intends to make a compromise between the logarithmic and linear spectral domains by choosing an appropriate value of the power. This technique is incorporated into a modified MFCC-based frontend. The connected- digit recognition tests showed that in noisy conditions this technique outperforms the conventional techniques such as the DyC, the continuous spectral subtraction method, the cepstral mean subtraction while maintaining the robustness to the convolutional noise.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-589"
  },
  "christensen00_icslp": {
   "authors": [
    [
     "Heidi",
     "Christensen"
    ],
    [
     "Børge",
     "Lindberg"
    ],
    [
     "Ove",
     "Andersen"
    ]
   ],
   "title": "Noise robustness of heterogeneous features employing minimum classification error feature space transformations",
   "original": "i00_3534",
   "page_count": 4,
   "order": 592,
   "p1": "vol. 3, 534-537",
   "pn": "",
   "abstract": [
    "The use of heterogeneous features in automatic speech recognition has been shown to increase clean speech performance. This paper focuses on the noise robustness of systems with heterogeneous features. In particular a system where different features are extracted for different sets of phonemes. The employed features are computed by applying a linear transformation, estimated in a data-driven fashion, to standard feature processing methods. The transformed features are tested in a set of experiments employing different system configurations. Overall the experiments suggests that employing more phoneme specific features can improve speech recognition. When testing the system on noisy speech with added car or factory noise, this tendency is maintained.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-590"
  },
  "seltzer00_icslp": {
   "authors": [
    [
     "Michael L.",
     "Seltzer"
    ],
    [
     "Bhiksha",
     "Raj"
    ],
    [
     "Richard M.",
     "Stern"
    ]
   ],
   "title": "Classifier-based mask estimation for missing feature methods of robust speech recognition",
   "original": "i00_3538",
   "page_count": 4,
   "order": 593,
   "p1": "vol. 3, 538-541",
   "pn": "",
   "abstract": [
    "Missing feature methods of noise compensation for speech recognition operate by removing components of a spectrographic representation of speech that are considered to be corrupt, as indicated by a low signal-to-noise ratio. Recognition is either performed directly on the incomplete spectrograms or the missing components are reconstructed prior to recognition. These methods require a spectrographic mask which accurately labels the reliable and corrupt regions of the spectrogram. Current methods of mask estimation rely on assumptions about the corrupting noise such as stationarity. This is a significant drawback since the missing feature methods themselves have no such restrictions. We present a new mask estimation technique that uses a Bayesian classifier to determine the reliability of spectrographic elements. Features were designed that make no assumptions about the corrupting noise signal, but rather exploit characteristics of the speech signal itself. Missing feature compensation experiments were performed on speech corrupted by a variety of noises. In all cases, classifier-based mask estimation resulted in significantly better recognition accuracy than conventional mask estimation methods.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-591"
  },
  "hermus00_icslp": {
   "authors": [
    [
     "Kris",
     "Hermus"
    ],
    [
     "Werner",
     "Verhelst"
    ],
    [
     "Patrick",
     "Wambacq"
    ]
   ],
   "title": "Optimized subspace weighting for robust speech recognition in additive noise environments",
   "original": "i00_3542",
   "page_count": 4,
   "order": 594,
   "p1": "vol. 3, 542-545",
   "pn": "",
   "abstract": [
    "Signal Subspace (SS) based speech enhancement techniques obtain significant additive-noise reduction by altering the singular value spectrum of the speech observation matrix. Among the class of different possible SS weighting strategies, the Minimum Variance (MV) estimation method substantially increases the speech recognition accuracy in additive noise environments, outperforming the widely used Spectral Subtraction methods. However, these SS approaches are developed as pure speech enhancement techniques, and it is still unknown how effective they are for noise robust speech recognition. In this respect, we present the idea of 'optimal SS weighting' for speech recognition systems, and we illustrate in detail that the MV estimation closely approximates this optimum. We applied the SS weighting methods to a LV-CSR task with noisy data (10 dB SNR), and obtained relative reductions in Word Error Rate of more than 60 %.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-592"
  },
  "ming00_icslp": {
   "authors": [
    [
     "Ji",
     "Ming"
    ],
    [
     "Peter",
     "Jancovic"
    ],
    [
     "Philip",
     "Hanna"
    ],
    [
     "Darryl",
     "Stewart"
    ],
    [
     "F. Jack",
     "Smith"
    ]
   ],
   "title": "Robust feature selection using probabilistic union models",
   "original": "i00_3546",
   "page_count": 4,
   "order": 595,
   "p1": "vol. 3, 546-549",
   "pn": "",
   "abstract": [
    "This paper provides a summary of our recent work on robust speech recognition based on a new statistical approach - the probabilistic union model. In particular, we considered speech recognition involving partial corruption in frequency bands, in time duration, and further in feature components. In all these situations, we assumed no prior knowledge about the corrupting noise, e.g. its band location, occurring time and statistical distribution. The new model characterizes these partial, unknown corruptions based on the union of random events. For the evaluation, we have conducted isolated-word recognition tasks by using both a speaker-independent E-set database and the TiDigits database, each being corrupted by various types of additive noise with unknown, time-varying statistics. The results indicate that the probabilistic union model offers robustness to partial corruption in speech utterances, requiring little or no knowledge about the noise characteristics.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-593"
  },
  "hariharan00b_icslp": {
   "authors": [
    [
     "Ramalingam",
     "Hariharan"
    ],
    [
     "Imre",
     "Kiss"
    ],
    [
     "Olli",
     "Viikki"
    ],
    [
     "Jilei",
     "Tian"
    ]
   ],
   "title": "Multi-resolution front-end for noise robust speech recognition",
   "original": "i00_3550",
   "page_count": 4,
   "order": 596,
   "p1": "vol. 3, 550-553",
   "pn": "",
   "abstract": [
    "This paper proposes a new feature extraction approach for noise robust speech recognition. The recent work in multi-band and missing feature theory based Automatic Speech Recognition (ASR) has shown that sub-band processing of speech has certain advantages over the conventional full-band technique. In multiband ASR, different frequency sub-bands are usually decoded independently and a final recognition result is obtained by combining different frequency channels at some temporal level. Since it is not straightforward to determine the optimal combination level, we propose that different sub-band parameters need to be collected into a single feature vector for decoding. As the full-band parameters still carry important information for classification, we suggest that full-band features need to be included in the final feature vector. Our third observation is that the use of PCA transform for de-correlating log-spectral features provides better recognition performance than DCT. The experimental results show that the proposed front-end provides 36.2% improvement in performance over the conventional full-band technique.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-594"
  },
  "oshaughnessy00_icslp": {
   "authors": [
    [
     "Douglas",
     "O'Shaughnessy"
    ],
    [
     "Marcel",
     "Gabrea"
    ]
   ],
   "title": "Recognition of digit strings in noisy speech with limited resources",
   "original": "i00_3554",
   "page_count": 4,
   "order": 597,
   "p1": "vol. 3, 554-557",
   "pn": "",
   "abstract": [
    "Automatic recognition of continuously-spoken digits (e.g., telephone numbers or credit card numbers) is feasible with excellent accuracy, even for speaker-independent applications over telephone lines. However, even such relatively simple recognition tasks suffer decreased performance in adverse conditions, such as significant background noise or fading on portable telephone channels. If an application further imposes significant limitations on the computing resources for the recognition task, then robust limited-resource speech recognition remains a suitable challenge, even for a vocabulary as simple as the digits. Since connected-digit recognition over telephone lines is a very practical application, the amount of computer resources needed for a given level of recognition accuracy was investigated for different acoustic noise conditions. Rather than use a traditional hidden Markov model approach with cepstral analysis, which is computationally intensive and does not always work well under adverse acoustic conditions, simpler spectral analysis was used, combined with a segmental approach. The limited nature of the vocabulary (i.e., 10 digits) allows this simpler approach. High recognition accuracy can be maintained despite a large decrease (vs. traditional methods) in both memory and computation.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-595"
  },
  "tajima00_icslp": {
   "authors": [
    [
     "Keiichi",
     "Tajima"
    ],
    [
     "Donna",
     "Erickson"
    ],
    [
     "Kyoko",
     "Nagao"
    ]
   ],
   "title": "Factors affecting native Japanese speakers' production of intrusive (epenthetic) vowels in English words",
   "original": "i00_3558",
   "page_count": 4,
   "order": 598,
   "p1": "vol. 3, 558-561",
   "pn": "",
   "abstract": [
    "A salient characteristic of English spoken by native Japanese is th tendency to produce intrusive vowels between consonants or following word-final consonants (e.g., English \\stress\" as /su.to.re.su/). This study conducted speech analysis of a large set of Japanese-accented English utterances to evaluate the effects of various factors on vowel epenthesis. Results reveal effects of speaking rate and phonetic environment, but little effect of word familiarity and frequency.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-596"
  },
  "zitouni00_icslp": {
   "authors": [
    [
     "Imed",
     "Zitouni"
    ],
    [
     "Kamel",
     "Smaïli"
    ],
    [
     "Jean-Paul",
     "Haton"
    ]
   ],
   "title": "Beyond the conventional statistical language models: the variable-length sequences approach",
   "original": "i00_3562",
   "page_count": 5,
   "order": 599,
   "p1": "vol. 3, 562-565",
   "pn": "",
   "abstract": [
    "In natural language, several sequences of words are very frequent. A classical language model, like n-gram, does not adequately take into account such sequences, because it underestimates their probabilities. A better approach consists in modelling word sequences as if they were individual dictionary elements. Sequences are considered as additional entries of the word lexicon, on which language models are computed. In this paper, we present an original method for automatically determining the most important phrases in corpora. This method is based on information theoretic criteria, which insure a high statistical consistency, and on French grammatical classes which include additional type of linguistic dependencies. In addition, the perplexity is used in order to make the decision of selecting a potential sequence more accurate. We propose also several variants of language models with and without word sequences. Among them, we present a model in which the trigger pairs are more significant linguistically. The originality of this model, compared with the commonly used trigger approaches, is the use of word sequences to estimate the trigger pair without limiting itself to single words. Experimental tests, in terms of perplexity and recognition rate, are carried out on a vocabulary of 20000 words and a corpus of 43 million words. The use of word sequences proposed by our algorithm reduces perplexity by more than 16% compared to those, which are limited to single words. The introduction of these word sequences in our dictation machine improves the accuracy by approximately 15%.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-597"
  },
  "tsubota00_icslp": {
   "authors": [
    [
     "Yasushi",
     "Tsubota"
    ],
    [
     "Masatake",
     "Dantsuji"
    ],
    [
     "Tatsuya",
     "Kawahara"
    ]
   ],
   "title": "Computer-assisted English vowel learning system for Japanese speakers using cross language formant structures",
   "original": "i00_3566",
   "page_count": 4,
   "order": 600,
   "p1": "vol. 3, 566-569",
   "pn": "",
   "abstract": [
    "We present a novel Computer-Assisted Language Learning (CALL) system for Japanese students who learn English as a second language. We regard formant structure of Japanese vowels pronounced by Japanese learners of English as their own formant structure. This structure is transformed to learners ideal English formant structure based on the relationship between English and Japanese articulation charts both of which are corresponded with formant structure. When the learners English pronunciation is input, it is compared with the estimated ideal one, and articulatory instructions are given. We verified that the mapping and estimation of English vowel parameters are correct with bilingual speakers speech and observed the learning effect with five students who tried the system.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-598"
  },
  "holter00_icslp": {
   "authors": [
    [
     "Trym",
     "Holter"
    ],
    [
     "Erik",
     "Harborg"
    ],
    [
     "Magne Hallstein",
     "Johnsen"
    ],
    [
     "Torbjörn",
     "Svendsen"
    ]
   ],
   "title": "ASR-based subtitling of live TV-programs for the hearing impaired",
   "original": "i00_3570",
   "page_count": 4,
   "order": 601,
   "p1": "vol. 3, 570-573",
   "pn": "",
   "abstract": [
    "A system for on-line generation of closed captions (subtitles) for broadcast of live TV-programs is described. During broadcast, a commentator formulates a possibly condensed, but semantically correct version of the original speech. These compressed phrases are recognized by a continuous speech recognizer, and the resulting captions are fed into the teletext system. This application will provide the hearing impaired with an option to read captions for live broadcast programs, i.e., when off-line captioning is not feasible. The main advantage in using a speech recognizer rather than a stenography-based system (e.g., Velotype) is the relaxed requirements for commentator training. Also, the amount of text generated by a system based on stenography tends to be large, thus making it harder to read.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-599"
  },
  "wu00e_icslp": {
   "authors": [
    [
     "Chung-Hsien",
     "Wu"
    ],
    [
     "Yu-Hsien",
     "Chiu"
    ],
    [
     "Chi-Shiang",
     "Guo"
    ]
   ],
   "title": "Natural language processing for Taiwanese sign language to speech conversion",
   "original": "i00_3574",
   "page_count": 4,
   "order": 602,
   "p1": "vol. 3, 574-577",
   "pn": "",
   "abstract": [
    "This paper addresses some natural language processing technologies in generating sentences from ill-formed Taiwanese Sign Language (TSL) for people with speech or hearing impairments. The design and development of the PC-based TSL Augmentative and Alternative Communication (AAC) system aims to improve the input rate and accuracy of communication aids. We demonstrate 1) developing an effective TSL virtual keyboard to assist users to efficiently input sign sequences, 2) investigating TSL prediction strategies for input rate enhancement, 3) integrating bottom-up parsing with top-down filtering strategy and variable N-gram language model for sentence generation and selection. The proposed system assists people with language defects in sentence formation and grammatical correction. To evaluate the performance of our approach, a pilot study for clinical evaluation and education training was undertaken. Evaluation results show that the generation rate and subjective satisfactory level for sentence construction have been significantly improved.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-600"
  },
  "miwa00b_icslp": {
   "authors": [
    [
     "Jouji",
     "Miwa"
    ],
    [
     "Hiroshi",
     "Sasaki"
    ],
    [
     "Kazunori",
     "Tanno"
    ]
   ],
   "title": "Japanese spoken language learning system using java information technology",
   "original": "i00_3578",
   "page_count": 4,
   "order": 603,
   "p1": "vol. 3, 578-581",
   "pn": "",
   "abstract": [
    "This paper describes a system for Japanese spoken lan- guage learning using Java information technology (IT). The system is automatically evaluated Japanese speech ut- tered by trainee as computer assisted language learning (CALL) on the Internet. For the Internet, the system can be used at any time, in anywhere and for anyone so that it is very suitable for busy learners in the world. Phonetically, Japanese is a mora-timed language but\n",
    "English is a syllable-timed language. Also Japanese is a pitch-accented language but English is a stress-accented language. So many learners of Japanese as a second lan- guage can hardy hear and speak for special mora and word accent such as super-segmental feature.\n",
    "Japanese speech uttered by a learner for special mora is automatically scored as 0 to 100 degree and a type of Japanese accent for speech uttered by a learner is automat- ically decided by the system. If learner's score is less than 50 degree and/or learner's accent type is wrong, learner must train until the score goes up with the system.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-601"
  },
  "strik00_icslp": {
   "authors": [
    [
     "Helmer",
     "Strik"
    ],
    [
     "Catia",
     "Cucchiarini"
    ],
    [
     "Diana",
     "Binnenpoorte"
    ]
   ],
   "title": "L2 pronunciation quality in read and spontaneous speech",
   "original": "i00_3582",
   "page_count": 4,
   "order": 604,
   "p1": "vol. 3, 582-585",
   "pn": "",
   "abstract": [
    "This paper describes two experiments aimed at exploring the relationship between objective properties of speech and perceived pronunciation quality in read and spontaneous speech, with a view to determining whether such quantitative measures can be used to develop objective pronunciation tests. Read and spontaneous speech of two groups of 60 learners of Dutch as a second language was scored for pronunciation quality by human raters and was analyzed by means of a continuous speech recognizer to calculate six quantitative measures of speech quality related to speech timing. The results show that quantitative, temporal measures of speech are strongly related to pronunciation quality, in both read and spontaneous speech, although not all variables suitable for measuring pronunciation quality in read speech are as effective in spontaneous speech.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-602"
  },
  "kitamura00_icslp": {
   "authors": [
    [
     "Tomoko",
     "Kitamura"
    ],
    [
     "Keisuke",
     "Kinoshita"
    ],
    [
     "Takayuki",
     "Arai"
    ],
    [
     "Akiko",
     "Kusumoto"
    ],
    [
     "Yuji",
     "Murahara"
    ]
   ],
   "title": "Designing modulation filters for improving speech intelligibility in reverberant environments",
   "original": "i00_3586",
   "page_count": 4,
   "order": 605,
   "p1": "vol. 3, 586-589",
   "pn": "",
   "abstract": [
    "In this paper, we propose a new technique to design modulation filters to reduce degradation of speech intelligibility in reverberant environments. Using the inverse modulation transfer function, we design data-derived modulation filters for each speech frequency band. These filters preprocess speech signals between a microphone and a loudspeaker that radiates speech into a performance hall. Using our modulation filters, we conducted perceptual experiments with one hearingimpaired subject and two subjects with normal hearing. Test results indicate that our proposed method improves the intelligibility of reverberant speech.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-603"
  },
  "zhang00g_icslp": {
   "authors": [
    [
     "Lei",
     "Zhang"
    ],
    [
     "Jiqing",
     "Han"
    ],
    [
     "Chengguo",
     "Lv"
    ],
    [
     "Chengfa",
     "Wang"
    ]
   ],
   "title": "An environment model-based robust speech recognition",
   "original": "i00_3590",
   "page_count": 4,
   "order": 606,
   "p1": "vol. 3, 590-593",
   "pn": "",
   "abstract": [
    "In this paper, a new approach named environmental discrimination learning (EDL) is proposed to remove the effects of the environment noises including the additive noise and the channel distortions. This method optimizes the environment parameters by the minimum classification error (MCE) criterion which trains the parameters of a given class dependently on the whole classes. The EDL approach utilizes more about the information between different classes to optimize the environment parameters, therefore, it can minimize the error rate. And a generalized probabilistic descent (GPD) algorithm is adopted for discriminative training the environment parameters. A speaker independent isolated word recognition system based on whole word-HMM model is used to evaluate the proposed approach. Experimental results show that the proposed method achieves significant improvement of recognition performance.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-604"
  },
  "vermaak00_icslp": {
   "authors": [
    [
     "Jaco",
     "Vermaak"
    ],
    [
     "Christophe",
     "Andrieu"
    ],
    [
     "Arnaud",
     "Doucet"
    ]
   ],
   "title": "Particle filtering for non-stationary speech modelling and enhancement",
   "original": "i00_3594",
   "page_count": 4,
   "order": 607,
   "p1": "vol. 3, 594-597",
   "pn": "",
   "abstract": [
    "This paper applies time-varying autoregressive (TVAR) models with stochastically evolving parameters to the problem of speech modelling and enhancement. The stochastic evolution models for the TVAR parameters are Markovian diffusion processes. The main aim of the pa- per is to perform on-line estimation of the clean speech and the model parameters, and to determine the adequacy of the chosen statistical models. An eÆcient simulation- based method is developed to solve the optimal filtering problem. The algorithm combines sequential importance sampling and a selection step, and employs several variance reduction strategies to make the best use of the statistical structure of the model. The modelling and enhancement performance of the model and algorithm are evaluated in simulation studies on real speech data sets.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-605"
  },
  "graciarena00_icslp": {
   "authors": [
    [
     "Martin",
     "Graciarena"
    ]
   ],
   "title": "Maximum likelihood noise HMMm estimation in model-based robust speech recognition",
   "original": "i00_3598",
   "page_count": 4,
   "order": 608,
   "p1": "vol. 3, 598-601",
   "pn": "",
   "abstract": [
    "This paper presents a generalization of Rose's Integrated Parametric Model to the gaussian mixture hidden Markov model (HMM), formulation. Observations from clean speech HMM and noise HMM models are combined in the log spectra domain, through a corruption function, to generate noisy speech observations. In order to recognize noisy speech with the proposed model, when only the clean speech HMM and noisy speech adaptation data are available, a maximum likelihood (ML) estimation algorithm for the noise HMM parameters is provided. This algorithm uses the \"max\" approximation as the corruption function. Noisy digit recognition experiments, with NOISEX-92, show that the same performance is achieved between the proposed model using either a noise model calculated from silent sections of several utterances or the estimated noise model from a single noisy utterance.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-606"
  },
  "zeng00_icslp": {
   "authors": [
    [
     "Qingsheng",
     "Zeng"
    ],
    [
     "Douglas",
     "O'Shaughnessy"
    ]
   ],
   "title": "Microphone array within a handset or face mask for speech enhancement",
   "original": "i00_3602",
   "page_count": 4,
   "order": 609,
   "p1": "vol. 3, 602-605",
   "pn": "",
   "abstract": [
    "This paper presents some simple microphone arrays for communication in harsh noisy environments and for speakers using face masks. These arrays have better frequency responses than those developed in previous work, keeping gain match and the same uniform microphone spacing as in the past1 This paper also discusses the effects of microphone spacing and gain mismatch on array characteristics, the angular variations of source attenuation properties of these arrays and their robustnesses in respect to angular variation.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-607"
  },
  "wang00o_icslp": {
   "authors": [
    [
     "Chengfa",
     "Wang"
    ],
    [
     "Qiusheng",
     "Wang"
    ]
   ],
   "title": "Embedding visually recognizable watermarks into digital audio signals",
   "original": "i00_3606",
   "page_count": 4,
   "order": 610,
   "p1": "vol. 3, 606-609",
   "pn": "",
   "abstract": [
    "This paper presents a novel watermarking technique for digital audio signals. A visually recognizable binary image served as a watermark is embedded into an audio signal in discrete cosine transform domain. In order to enhance robustness and make good use of masking characteristics of the audio signal, the techniques of pseudo-random permutation and energy-based permutation are used during watermarking. The experimental results show that the proposed watermarking technique is robust to several signal manipulations, such as filtering, compression, and so on.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-608"
  },
  "iwaki00_icslp": {
   "authors": [
    [
     "Mamoru",
     "Iwaki"
    ]
   ],
   "title": "Auditory perception of amplitude modulated sinusoid using a pure tone and band-limited noises as modulation signals",
   "original": "i00_3610",
   "page_count": 4,
   "order": 611,
   "p1": "vol. 3, 610-613",
   "pn": "",
   "abstract": [
    "Frequency selectivity in amplitude modulated sound have been reported in terms of modulation threshold level for amplitude-modulation detection, where a pink or white noise carrier was modulated with a sinusoid and band-limited white noise, which showed similar band-pass type masking characteristics. Such previous studies treated band limited white noise carriers. In this paper, an amplitude-modulation detection level is measured in the case of sinusoidal carrier. The obtained characteristics are similar to those for pink or white noise carriers; the detection level increases around 16Hz when the 1000Hz sinusoidal carrier is modulated by 16Hz pure tone and band-limited white noise, and the increase is localized around 16Hz within 1 to 1.5 octave wide.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-609"
  },
  "geravanchizadeh00_icslp": {
   "authors": [
    [
     "Masoud",
     "Geravanchizadeh"
    ]
   ],
   "title": "Spectral voice conversion based on unsupervised clustering of acoustic space",
   "original": "i00_3614",
   "page_count": 4,
   "order": 612,
   "p1": "vol. 3, 614-617",
   "pn": "",
   "abstract": [
    "Voice conversion systems aim at modifying a source speakers speech so that it is perceived as if a target speaker had spoken it. Applying voice conversion techniques to a concatenative text-to-speech synthesizer allows for the personification of such systems, so that additional voices from a single source-speaker database can be produced quickly and automatically. This paper presents a new algorithm in which an effective and simple solution to the problem of voice conversion is suggested with the goal of maintaining high speech quality. Here, spectral conversion is performed by locally linear transformations, where the minimum mean square estimation (MMSE) method is used to compute the transformations. The acoustic features included in the conversion are vocal tract parameters, which are represented by log area ratio coefficients. Evaluation by listening tests shows that the proposed algorithm makes it possible to convert speaker individuality while maintaining high quality.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-610"
  },
  "pfitzinger00_icslp": {
   "authors": [
    [
     "Hartmut R.",
     "Pfitzinger"
    ]
   ],
   "title": "Removing hum from spoken language resources",
   "original": "i00_3618",
   "page_count": 4,
   "order": 613,
   "p1": "vol. 3, 618-621",
   "pn": "",
   "abstract": [
    "This paper presents a technique to remove the hum in speech corpora based on time domain subtraction rather than spectral subtraction. This novel method is able to avoid any perceptible or measurable artifacts and has proved to be an efficient technique for completely eliminating even complex hum waveforms from speech recordings.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-611"
  },
  "amdal00_icslp": {
   "authors": [
    [
     "Ingunn",
     "Amdal"
    ],
    [
     "Filipp",
     "Korkmazskiy"
    ],
    [
     "Arun C.",
     "Surendran"
    ]
   ],
   "title": "Joint pronunciation modelling of non-native speakers using data-driven methods",
   "original": "i00_3622",
   "page_count": 4,
   "order": 614,
   "p1": "vol. 3, 622-625",
   "pn": "",
   "abstract": [
    "Modelling non-native speakers with different mother tongues is a difficult task for automatic speech recognition due to the large variation among speakers. One possibility for jointly modelling all speakers is to use the same speaker independent acoustic models and a joint lexicon to capture the variation. We have modified the reference lexicon using pronunciation rules that are derived in a totally data-driven manner from a set of adaptation data using the reference recognizer and the reference lexicon. Deriving common rules for such diverse sources simultaneously is difficult. The challenge is to combine these rules to a common set without increasing the confusability. In this paper we compare several methods of combining the individual rules to form a common lexicon for all speakers. Using a new log likelihood rule pruning measure presented in this paper, we achieved improved performance compared with more traditional rule pruning methods based on rule probability, and with much fewer rules. With a confusability reduction scheme we reduced the number of rules even further.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-612"
  },
  "bell00b_icslp": {
   "authors": [
    [
     "Linda",
     "Bell"
    ],
    [
     "Robert",
     "Eklund"
    ],
    [
     "Joakim",
     "Gustafson"
    ]
   ],
   "title": "A comparison of disfluency distribution in a unimodal and a multimodal speech interface",
   "original": "i00_3626",
   "page_count": 4,
   "order": 615,
   "p1": "vol. 3, 626-629",
   "pn": "",
   "abstract": [
    "In this paper, we compare the distribution of disfluencies in two humancomputer dialogue corpora. One corpus consists of unimodal travel booking dialogues, which were recorded over the telephone. In this unimodal system, all components except the speech recognition were authentic. The other corpus was collected using a semi-simulated multi-modal dialogue system with an animated talking agent and a clickable map. The aim of this paper is to analyze and discuss the effects of modality, task and interface design on the distribution and frequency of disfluencies in these two corpora.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-613"
  },
  "liu00g_icslp": {
   "authors": [
    [
     "Yi",
     "Liu"
    ],
    [
     "Pascale",
     "Fung"
    ]
   ],
   "title": "Modelling pronunciation variations in spontaneous Mandarin speech",
   "original": "i00_3630",
   "page_count": 4,
   "order": 616,
   "p1": "vol. 3, 630-633",
   "pn": "",
   "abstract": [
    "Pronunciation in spontaneous Mandarin speech tends to be much more variable than in read speech. In current recognition systems, pronunciation dictionaries usually only contain one standard pronunciation for each word, so that the amount of variability that can be modelled is very limited. Most recent research work for modelling variations in spontaneous speech focuses on the lexicon level, which can only solve intra-word variations. Inter-word variations cannot be modelled effectively. Chinese is monosyllabic and has simple syllable structure, giving rise to a high amount of pronunciation variations. In this paper, we propose two methods to model pronunciation variations in spontaneous Mandarin speech. First, we generate probability lexicon to model intra-syllable variations by using DP alignment algorithm between base form and surface strings. Second, we integrate variation probability into the decoder to model intra as well as inter-syllable variations. Experimental results show that modelling intra-syllable variation with a probability lexicon reduces syllable error rate by 0.85% (phone error rate reduction of 1.4%) while adding inter-syllable variation in addition reduces syllable error rate significantly by 4.76% (phone error rate reduction of 7.6%) compared to the baseline system.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-614"
  },
  "suzuki00b_icslp": {
   "authors": [
    [
     "Tadashi",
     "Suzuki"
    ],
    [
     "Jun",
     "Ishii"
    ],
    [
     "Kunio",
     "Nakajima"
    ]
   ],
   "title": "A method of generating English pronunciation dictionary for Japanese English recognition systems",
   "original": "i00_3634",
   "page_count": 5,
   "order": 617,
   "p1": "vol. 3, 634-637",
   "pn": "",
   "abstract": [
    "In this paper, we propose a method for generating a pronunciation dictionary - extracting typical pronunciations for each word from speech data uttered by Japanese speakers - as one approach to speech recognition targeting English speech uttered by Japanese speakers whose mother tongue is not English. This method includes three processes: a process in which English phoneme HMMs (Hidden Markov Models) are adapted to the speaker using English speech uttered by a Japanese speaker; a process in which English by a Japanese speaker is translated into an English phoneme series using a phoneme typewriter; and a process by which representative phoneme series are selected with a clustering technique from multiple phoneme series derived with respect to each word. We also propose a speaker adaptation method in a recognition phase. In this method, the phoneme HMMs are adapted to the target speaker with a phoneme label series that expresses the typical pronunciation extracted using the above method. Evaluation tests by continuous speech recognition with English speech data uttered by five Japanese speakers using a pronunciation dictionary generated from other five Japanese speakers' data were carried out. The result of the tests indicated that sentence recognition errors were reduced by 72% compared to using a dictionary for native speakers.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-615"
  },
  "bonneaumaynard00_icslp": {
   "authors": [
    [
     "Hélène",
     "Bonneau-Maynard"
    ],
    [
     "L.",
     "Devillers"
    ]
   ],
   "title": "A framework for evaluating contextual understanding",
   "original": "i00_3638",
   "page_count": 4,
   "order": 618,
   "p1": "vol. 3, 638-641",
   "pn": "",
   "abstract": [
    "In this paper we propose and describe a framework for evaluating and diagnosing the understanding component of a spoken dialog system which can be applied to both literal and contextual understanding. We have observed in a previous experiment that contextual understanding performance is strongly correlated to the user satisfaction. The framework uses a glassbox approach to diagnose the interpretation modules. Results are given on a 1681 literal understanding test set and 100 contextual understanding test set.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-616"
  },
  "deng00b_icslp": {
   "authors": [
    [
     "Yonggang",
     "Deng"
    ],
    [
     "Taiyi",
     "Huang"
    ],
    [
     "Bo",
     "Xu"
    ]
   ],
   "title": "Towards high performance continuous Mandarin digit string recognition",
   "original": "i00_3642",
   "page_count": 4,
   "order": 619,
   "p1": "vol. 3, 642-645",
   "pn": "",
   "abstract": [
    "In this paper, we address the problem of high performance speaker-independent continuous Mandarin digital string recognizer and focus on exploiting context information and prosody knowledge. Data-driven decision tree method to train tri-phone acoustic model was proposed. According to Chinese language property, digital specific question set was designed and the derived tri-phone model is more accurate to describe acoustic observation. For prosody cue, a novel Gaussian Mixture Density Duration Model (GMDDM) was presented. Unlike traditional normalizing or single parameter strategy, proposed duration model is context independent. The context variation is naturally embodied into multiple Gaussian distribution mixture. The number of mixture is automatically selected according maximum likelihood criteria. This simple but effective duration models likelihood score is combined with acoustic score as heuristic information for the backward A* decoding of word graph. Experimental results show the tri-phone acoustic model could lead to average 12.9% reduce of string error rate. When GMDDM model is applied, the string error rate is further reduced by 22.7%, which demonstrates the very usefulness of GMDDM model.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-617"
  },
  "aylett00_icslp": {
   "authors": [
    [
     "Matthew",
     "Aylett"
    ]
   ],
   "title": "Stochastic suprasegmentals: relationships between redundancy, prosodic structure and care of articulation in spontaneous speech",
   "original": "i00_3646",
   "page_count": 4,
   "order": 620,
   "p1": "vol. 3, 646-649",
   "pn": "",
   "abstract": [
    "Within spontaneous speech there are wide variations in the articulation of the same word by the same speaker. This paper explores two related factors which influence variation in articulation, prosodic structure and redundancy. We argue that the constraint of producing robust communication while efficiently expending articulatory effort leads to an inverse relationship between language redundancy and care of articulation. The inverse relationship improves robustness by spreading the information more evenly across the speech signal leading to a smoother signal redundancy profile. We argue that prosodic prominence is a linguistic means of achieving smooth signal redundancy. Prosodic prominence increases care of articulation and coincides with unpredictable sections of speech. By doing so, prosodic prominence leads to a smoother signal redundancy. Results confirm the strong relationship between prosodic prominence and care of articulation as well as an inverse relationship between language redundancy and care of articulation. In addition, when variation in prosodic boundaries is controlled for, language redundancy can predict up to 65% of the variance in raw syllabic duration. This is comparable with 64% predicted by prosodic prominence (accent, lexical stress and vowel type). Moreover most (62%) of this predictive power is shared. This suggests that, in English, prosodic structure is the means with which constraints caused by a robust signal requirement are expressed in spontaneous speech.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-618"
  },
  "sakamoto00_icslp": {
   "authors": [
    [
     "Masaharu",
     "Sakamoto"
    ],
    [
     "Takashi",
     "Saitoh"
    ]
   ],
   "title": "An automatic pitch-marking method using wavelet transform",
   "original": "i00_3650",
   "page_count": 4,
   "order": 621,
   "p1": "vol. 3, 650-653",
   "pn": "",
   "abstract": [
    "This paper describes a new automatic pitch-marking method using wavelet transform. This method detects discontinuity in the speech waveform which occurs at the glottal closure instant (GCI). A time domain prosodic modification technique requires an appropriate determination of the synthesis pitch-marks. We evaluated the performance of the newly developed pitchmarking method by using our internal speech databases with an electroglottograph signal. We achieved 96 percent detection accuracy on the performance evaluation. We confirmed that the proposed pitch-marking method is suitable for waveform concatenation-based synthesis through a listening test using pitch modified speech.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-619"
  },
  "takamaru00_icslp": {
   "authors": [
    [
     "Keiichi",
     "Takamaru"
    ],
    [
     "Makoto",
     "Hiroshige"
    ],
    [
     "Kenji",
     "Araki"
    ],
    [
     "Koji",
     "Tochinai"
    ]
   ],
   "title": "A proposal of a model to extract Japanese voluntary speech rate control",
   "original": "i00_3654",
   "page_count": 4,
   "order": 622,
   "p1": "vol. 3, 654-657",
   "pn": "",
   "abstract": [
    "To extract elements of prosodic features which relate to speakers' intentional control is required for speech information processing. Speech rate variation should be a \"caution signal\" to call listeners' attention strongly. To express and detect such \"caution signals\", we have proposed a new speech rate model. This model introduces two kinds of force to control the speech rate. One is a driving force which causes a global tendency of speech rate variation, and the other is damming force with speaker's voluntary control. The proposed model have been applied to several spontaneous conversational speech which we have newly recorded. A global tendency of speech rate has been expressed appropriately. In several cases, the intentional rate variations have been expressed appropriately.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-620"
  },
  "makarova00b_icslp": {
   "authors": [
    [
     "Veronika",
     "Makarova"
    ]
   ],
   "title": "Acoustic characteristics of surprise in Russian questions",
   "original": "i00_3658",
   "page_count": 5,
   "order": 623,
   "p1": "vol. 3, 658-661",
   "pn": "",
   "abstract": [
    "This paper reports the results of an experimental phonetic study investigating the production of neutral and surprised interrogatives in Russian. The paper describes F0 and duration parameters of 22 one-word three-syllable yes/no questions with the lexical stress on the penultimate syllable pronounced by five speakers of Russian. The speakerindependent differences between the parameters of neutral and surprised interrogatives include total sentence duration, pitch peak height, duration and pitch height of the accented syllable. The study also shows that despite the existence of common ways of expressing surprise, there is a strong variability across the speakers in the choice of a particular set of parameters for the expression of the neutral/surprised interrogative contrast.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-621"
  },
  "deng00c_icslp": {
   "authors": [
    [
     "Yonggang",
     "Deng"
    ],
    [
     "Yang",
     "Cao"
    ],
    [
     "Bo",
     "Xu"
    ]
   ],
   "title": "Neural network based integration of multiple confidence measures for OOV detection",
   "original": "i00_3662",
   "page_count": 4,
   "order": 624,
   "p1": "vol. 3, 662-665",
   "pn": "",
   "abstract": [
    "In this paper we present a novel method to reject OOV words for speaker dependent dynamic command set recognition. The OOV rejection problem is regarded as the designing of recognizer with two classes: In-Vocabulary command and OOV command. Multiple soundly confidence measures derived from likelihood score of acoustic match and prosody match are defined and compete with each other at the same level automatically within neural network framework, thus elude choosing balanced sensitive threshold like traditional strategy. The network weights are trained according to Minimum Misclassification Error criterion.\n",
    "The confidence measures take whole command set into account, and objectively describe the difference between the top one and alternative hypotheses. Experimental results show that neural network based combination is rational, reliable and stable with average total error rates 9.3%, outperforming any single confidence measure threshold approach. Also the across verification results show that trained network is independent of speaker, gender and command set. Although there is performance degradation when exported to another conditions, it is acceptable in many applications.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-622"
  },
  "xu00e_icslp": {
   "authors": [
    [
     "Yi",
     "Xu"
    ],
    [
     "Xuejing",
     "Sun"
    ]
   ],
   "title": "How fast can we really change pitch? maximum speed of pitch change revisited",
   "original": "i00_3666",
   "page_count": 4,
   "order": 625,
   "p1": "vol. 3, 666-669",
   "pn": "",
   "abstract": [
    "The present paper reports preliminary data obtained in a study of maximum speed of pitch change. The study used an imitation paradigm to elicit fast alternating high and low pitch sequences from native speakers of Mandarin and English who were not professional singers. The speed of pitch change was measured both in terms of response time - time needed to complete the middle 75% of a pitch shift, as defined in previous studies, and in terms of excursion time - time needed to complete the entire pitch shift. Results show that the latter is nearly twice as long as the former, indicating that the maximum speed of pitch change is not nearly as fast as previous data may have implied. Potential implications of this finding on our understanding of F0 contour production in speech are discussed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-623"
  },
  "klabbers00b_icslp": {
   "authors": [
    [
     "Esther",
     "Klabbers"
    ],
    [
     "Jan van",
     "Santen"
    ]
   ],
   "title": "Predicting segmental durations for Dutch using the sums-of-products approach",
   "original": "i00_3670",
   "page_count": 4,
   "order": 626,
   "p1": "vol. 3, 670-673",
   "pn": "",
   "abstract": [
    "This paper presents the results of a duration study performed for Dutch using the sums-of-products approach [1]. With a relatively small corpus of 297 sentences, a duration model could be constructed with an RMSE of 27 ms, which compares well to similar models for English, French and German. In an evaluation study the predicted durations of the duration model were compared to those predicted by a rule-based duration model.\n",
    "",
    "",
    "J. van Santen. Contextual effects on vowel duration. Speech Communication, 11:513-546, 1992.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-624"
  },
  "cao00b_icslp": {
   "authors": [
    [
     "Yang",
     "Cao"
    ],
    [
     "Taiyi",
     "Huang"
    ],
    [
     "Bo",
     "Xu"
    ],
    [
     "Chengrong",
     "Li"
    ]
   ],
   "title": "A stochastic polynomial tone model for continuous Mandarin speech",
   "original": "i00_3674",
   "page_count": 4,
   "order": 627,
   "p1": "vol. 3, 674-677",
   "pn": "",
   "abstract": [
    "In this paper, a stochastic polynomial tone model is presented for tone modeling in continuous mandarin speech. In this model, the pitch contour is described by a stochastic trajectory. The mean trajectory is represented by a polynomial function of normalized time while the variance is time varying. After that, an effective training and recognition algorithm is developed respectively. Also the problem of missing observation is discussed. Decision tree is employed to cluster the tone pattern variations, which are represented by proposed model. Many possible factors other than tone of neighboring syllables were taken into consideration when the decision tree was constructed. The experiments result shows that the tone recognition speed can increase more than 10 times while the recognition error rates decreased by 16% compared with traditional HMM tone model.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-625"
  },
  "gabrea00_icslp": {
   "authors": [
    [
     "Marcel",
     "Gabrea"
    ],
    [
     "Douglas",
     "OShaughnessy"
    ]
   ],
   "title": "Detection of filled pauses in spontaneous conversational speech",
   "original": "i00_3678",
   "page_count": 4,
   "order": 628,
   "p1": "vol. 3, 678-681",
   "pn": "",
   "abstract": [
    "Most automatic speech recognition work has concentrated on read speech, whose acoustic aspects differ significantly from speech found in actual dialogues. A primary difference between read speech and spontaneous speech concerns a high rate of disfluencies (e.g., filled pauses, repetitions, repairs, false starts). Filled pauses (e.g., \"uh,\" \"um\"), unlike silences, resemble phones as part of words in continuous speech. In this paper the problem of detection of filled pauses in spontaneous speech and how this can be useful in automatic speech recognition are considered. The acoustic aspects of filled pauses in a widely-used SWITCHBOARD [1] database are examined here, from the point of view of identifying them acoustically using a combination of duration, fundamental frequency and spectra.\n",
    "",
    "",
    "Godfrey J. J., Holliman E. C., and McDaniel J. \"SWITCHBOARD Telephone Speech Corpus for Research and Development\". IEEE International Conference on Acoustics. Speech, and Signal Processing. San Francisco, 1992, Vol. I, pages 517- 520.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-626"
  },
  "lyberg00_icslp": {
   "authors": [
    [
     "Bertil",
     "Lyberg"
    ],
    [
     "Sonia",
     "Sangarig"
    ]
   ],
   "title": "Some observations on different strategies for the timing of fundamental frequency events",
   "original": "i00_3682",
   "page_count": 4,
   "order": 629,
   "p1": "vol. 3, 682-685",
   "pn": "",
   "abstract": [
    "The acoustic manifestations of the prosodic features are heavily influenced by the segmental compositions of the utterances. In order to study the acoustic correlates of the prosodic features it is necessary to diminish the influence of the segments. In investigations about the fundamental frequency contour utterances built up of only sonorants are often used and thereby the influence of constrictions in the vocal tract is avoided or at least diminished. In order to arrive at a detailed model of the fundamental frequency for e.g. speech synthesis it is necessary to exactly know what is happening in different segmental environments. This information can also shed light on the underlying mechanisms of the timing of the prosodic patterns. In the present study we are examining the effects which voiceless consonants have upon the fundamental frequency contour not only in words carrying the tonal word accents but also words carrying in sentence accent and terminal juncture. Three different hypotheses are considered the truncation, the timing adjustment and the rate adjustment hypothesis.\n",
    "The results show that the truncation hypothesis seems to be of limited validity at least for a detailed description of what is happening in the fundamental frequency contour in Swedish.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-627"
  },
  "wu00f_icslp": {
   "authors": [
    [
     "Zhiyong",
     "Wu"
    ],
    [
     "Lianhong",
     "Cai"
    ],
    [
     "Tongchun",
     "Zhou"
    ]
   ],
   "title": "Research on dynamic characters of Chinese pitch contours",
   "original": "i00_3686",
   "page_count": 4,
   "order": 630,
   "p1": "vol. 3, 686-689",
   "pn": "",
   "abstract": [
    "Chinese is a tone language. For a tone, the characters of its F0 pitch contours will be quite different in the condition of continuant speaking from the isolated speaking. The present researches about the Chinese tone are still centralized on the isolated speaking one, and about tone in fluent speech, there are some statements about the phenomenon of the two-word, threeword, four-word co-reading, but it can not represent the real characters of the tone in fluent speech very well.\n",
    "Our work bases on the spoken Chinese. And with the methods of statistics and classifications, we study and analyze the characters and behaviors of the F0 pitch contours of Chinese mandarin four tones in fluent speech.\n",
    "In the Paper, we put forward a mathematical model for describing the pitch contours: pitch contours time normal model (PiCTN model). In Chinese fluent speech, because of the coreading and different surroundings, the pitch contours of one tone will be quite different from what it will be in the isolated speaking, and will even have many different characters. And about the dynamic characters of pitch contours for Chinese Putonghua four tones in fluent speech, we also give the conclusion: the F0 pitch contour of tone 1 should be high and flat, and sometimes it will be rising with the even speed, the contour of tone 2 will be rising with the accelerated speed, and the pitch contour of tone 3 will drop with the decreasing speed, and lastly the F0 contour of tone 4 should be dropping with the speed increasing.\n",
    "The results of our work will be useful for the prosody modeling in Text-to-Speech (TTS) systems and the tone confirmation in the speech recognition.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-628"
  },
  "zhao00d_icslp": {
   "authors": [
    [
     "Bing",
     "Zhao"
    ],
    [
     "Bo",
     "Xu"
    ]
   ],
   "title": "Incorporating HMM-state sequence confusion for rapid MLLR adaptation to new speakers",
   "original": "i00_3690",
   "page_count": 4,
   "order": 631,
   "p1": "vol. 3, 690-693",
   "pn": "",
   "abstract": [
    "In this paper, we introduce the HMM-state sequence confusion characteristics as prior knowledge into the framework of MLLR to relax the transformation and reduce the risks of over-training when adaptation data size is small. There are two issues to be addressed as follows: first, how to estimate such confusion information reliably; second how to use the information in refining the estimation of MLLR adaptation. The pronunciation modeling technology was utilized to build the state sequence confusion table. Then the correlation of states is calculated according to the confusion table. Following proposed algorithm made a relaxation in the process of MLLR adaptation when the adaptation data is very small. Our experiment on a Mandarin state-tying triphone toneless LVCSR system showed that error rate reduction is 9.5% over standard MLLR with about 10 utterances (less than 30 seconds) of adaptation data.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-629"
  },
  "zhang00h_icslp": {
   "authors": [
    [
     "Zhipeng",
     "Zhang"
    ],
    [
     "Sadaoki",
     "Furui"
    ]
   ],
   "title": "An online incremental speaker adaptation method using speaker-clustered initial models",
   "original": "i00_3694",
   "page_count": 4,
   "order": 632,
   "p1": "vol. 3, 694-697",
   "pn": "",
   "abstract": [
    "We previously proposed an incremental speaker adaptation method combined with automatic speaker-change detection for broadcast news transcription where speakers change frequently and each of them utters a series of several sentences. In this method, the speaker change is detected using speakerindependent and speaker-adaptive Gaussian mixture models (GMMs). Both phone HMMs and GMMs are incrementally adapted to each speaker by the combination of MLLR, MAP and VFS methods using speaker-independent (SI) models as initial models. This paper proposes its improvement in which an initial model for speaker adaptation is selected from a set of models made by speaker clustering. Either cluster-dependent phone HMMs or GMMs are used to calculate the likelihood for selecting the best initial model. In a broadcast news transcription task, the proposed method significantly reduces word error rate compared with the method using SI-HMM as an initial model. Online incremental speaker adaptation results show that word error rate is reduced by 11.6% relative to the baseline system with no speaker adaptation. The method using GMMs for cluster selection requires a significantly less number of computations than that using HMMs.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-630"
  },
  "li00j_icslp": {
   "authors": [
    [
     "Guoqiang",
     "Li"
    ],
    [
     "Limin",
     "Du"
    ],
    [
     "Ziqiang",
     "Hou"
    ]
   ],
   "title": "Prior parameter transformation for unsupervised speaker adaptation",
   "original": "i00_3698",
   "page_count": 4,
   "order": 633,
   "p1": "vol. 3, 698-701",
   "pn": "",
   "abstract": [
    "In a strictly Bayesian approach, prior parameters are assumed known, based on common or subjective knowledge. But a practical solution for maximum a posteriori adaptation methods is to adopt an empirical Bayesian approach, where the prior parameters are estimated directly from training speech data itself. So there is a problem of mismatches between training and testing conditions in the use of prior parameters. We proposed a prior parameter transformation (PPT) adaptation approach that transforms the prior parameters to be more representative of the new speaker. In this paper we extend it to unsupervised mode. For easily confused speech units, different transformation matrices are applied to make them distinct. Initial experiments show that the PPT algorithm can get much improvement for a small amount of adaptation data even in the unsupervised mode.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-631"
  },
  "sarikaya00_icslp": {
   "authors": [
    [
     "Ruhi",
     "Sarikaya"
    ],
    [
     "John H. L.",
     "Hansen"
    ]
   ],
   "title": "Improved Jacobian adaptation for fast acoustic model adaptation in noisy speech recognition",
   "original": "i00_3702",
   "page_count": 4,
   "order": 634,
   "p1": "vol. 3, 702-705",
   "pn": "",
   "abstract": [
    "This paper describes two algorithms to improve a previously proposed Jacobian adaptation (JA) technique for fast acoustic speech recognizer model adaptation in environmental noise. The first technique introduces a new bias term, that is a function of the reference noise estimate to account for the mismatch between the reference noise estimate and noise component of the noisy speech spectrum. This functional mismatch bias is quite general, and here we choose to represent it as a linear function of the reference noise estimate. The second algorithm uses a more accurate relationship between the log and linear spectral domain versions of the HMM parameters. The combination of these new techniques achieves an increase of between 3.3-10.9% in recognition accuracy in adapting from automobile highway noise (HWY1) to such low-frequency noise sources as IBM PC cooling fan noise (PS2), large city street noise (LCI) and HWY2 (a different highway noise with different automobile at different signal-to-noise ratio (SNR)) over the original Jacobian adaptation in context independent phone recognition on TIMIT database.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-632"
  },
  "fujita00_icslp": {
   "authors": [
    [
     "Keiko",
     "Fujita"
    ],
    [
     "Yoshio",
     "Ono"
    ],
    [
     "Yoshihisa",
     "Nakatoh"
    ]
   ],
   "title": "A study of vocal tract length normalization with generation-dependent acoustic models",
   "original": "i00_3706",
   "page_count": 4,
   "order": 635,
   "p1": "vol. 3, 706-709",
   "pn": "",
   "abstract": [
    "In this paper, we propose a new speaker adaptation algorithm that employs vocal tract length normalization (VTLN) with generation-dependent acoustic models, and prove its validity with various generation subjects including small children and aged people.\n",
    "Children and aged people have particular features in their pronunciations. For example, children, whose articluatory organs are under growing, often have deficits in articulation. Aged people also have unique pronunciations caused by aging features such as loss of their original teeth. On the other hand, VTL cannot be estimated only by speakers generation since VTL is highly dependent to speakers  individuality rather than generation. Though children have rather short VTLs than adult and aged people, exact VTL for each speaker cannot be estimated without analysis of each speakers voice.\n",
    "Based on above our idea on generation features, in this paper, we propose VTLN with generation-dependent acoustic model as a speaker adaptation method suitable for various generations, and discuss the effect of our proposing method. Our results show that proposing method brings word error rate (WER) reduction by 52% for aged people, and by 63% for children.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-633"
  },
  "wang00p_icslp": {
   "authors": [
    [
     "Shaojun",
     "Wang"
    ],
    [
     "Yunxin",
     "Zhao"
    ]
   ],
   "title": "Optimal on-line Bayesian model selection for speaker adaptation",
   "original": "i00_3710",
   "page_count": 4,
   "order": 636,
   "p1": "vol. 3, 710-713",
   "pn": "",
   "abstract": [
    "In this paper, we show how to accomodate a Bayesian variant of Rissanens MDL into on-line Bayesian adaptation to control both model structural complexity and parameterization complexity to best fit an available amount of adaptation data, the goal being minimization of resulting recognition error. An efficient bottom-up dynamic programming based pruning algorithm is developed for selecting models using the MDL principle. Speaker adaptation experiments using a 26-letter English alphabet vocabulary were conducted and the proposed Bayesian variant MDL method is shown to provide an optimal tradeoff between recognition accuracy and complexity of model structure and parameterization over a full range of adaptation data size. It in general is capable of automaticlly selecting a set of model parameters that leads to best recognition performance for a given amount of adaptation data.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-634"
  },
  "zhou00d_icslp": {
   "authors": [
    [
     "Bowen",
     "Zhou"
    ],
    [
     "John H. L.",
     "Hansen"
    ]
   ],
   "title": "Unsupervised audio stream segmentation and clustering via the Bayesian information criterion",
   "original": "i00_3714",
   "page_count": 4,
   "order": 637,
   "p1": "vol. 3, 714-717",
   "pn": "",
   "abstract": [
    "In this paper, we propose an eÆcient approach for unsupervised audio stream segmentation and clustering via the Bayesian Information Criterion (BIC). The proposed method extends an earlier formulation by Chen and Gopalakrishnan [1]. In our segmentation formulation, Hotelling's T2-Statistic is used to pre-select candidate segmentation boundaries followed by BIC to make the segmentation decision. Our experiments show that we can improve the final algorithm speed by an order of 100 compared to that in [1] while achieving a 7% reduced miss rate at the expense of a 6% increase in false alarm rate using DARPA Hub4 1997 evaluation data. In the clustering stage, Gaussian Mixture Models are used for gender labeling prior to hierarchical BIC-based clustering within the gender class. Our cluster experiment show that we can achieve a cluster purity of 99.3%.\n",
    "",
    "",
    "S. Chen, P.Gopalakrishnan, \"Speaker, Environment and Channel Change Detection and Clustering via The Bayesian Information Criterion,\" Proc. Broadcast News Trans. & Under. Workshop, pp. 127-132, Feb., 1998.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-635"
  },
  "tsuge00_icslp": {
   "authors": [
    [
     "Satoru",
     "Tsuge"
    ],
    [
     "Toshiaki",
     "Fukada"
    ],
    [
     "Kenji",
     "Kita"
    ]
   ],
   "title": "Frame-period adaptation for speaking rate robust speech recognition",
   "original": "i00_3718",
   "page_count": 4,
   "order": 638,
   "p1": "vol. 3, 718-721",
   "pn": "",
   "abstract": [
    "This paper describes a frame-period adaptation method for speaking rate robust speech recognition. The proposed method determines an appropriate frame-period for each phrase by measuring its speaking rate or computing the acoustic likelihood with a set of frame-periods. Experimental results on spontaneous speech recognition show that the proposed method is effective for slower utterance. Actually, we can get about a 15% error reduction in error rate for slower utterance by using the likelihood based frame-period determination.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-636"
  },
  "nieuwoudt00_icslp": {
   "authors": [
    [
     "C.",
     "Nieuwoudt"
    ],
    [
     "Elizabeth C.",
     "Botha"
    ]
   ],
   "title": "Cross-language use of acoustic information for automatic speech recognition",
   "original": "i00_3722",
   "page_count": 4,
   "order": 639,
   "p1": "vol. 3, 722-725",
   "pn": "",
   "abstract": [
    "Techniques are investigated that use acoustic information from existing source language databases to implement automatic speech recognition (ASR) systems for new target languages for which little data are available. Strategies for cross-language use of acoustic information are proposed and are implemented via maximum a posteriori probability (MAP) and transformation-based techniques, as well as via discriminative learning techniques. The discriminative learning technique used is based on a cost-based extension of the minimum classification error (MCE) approach. Experiments are performed using relatively large amounts of English speech data from either a separate database or from the same database as smaller amounts of Afrikaans speech data to improve the performance of an Afrikaans speech recogniser. Results indicate that a significant reduction in word error rate is achievable (between 14% and 48% for experiments), depending on the method used and the amount of target language data available.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-637"
  },
  "sato00b_icslp": {
   "authors": [
    [
     "Shoei",
     "Sato"
    ],
    [
     "Toru",
     "Imai"
    ],
    [
     "Hideki",
     "Tanaka"
    ],
    [
     "Akio",
     "Ando"
    ]
   ],
   "title": "Selective training of HMMs by using two-stage clustering",
   "original": "i00_3726",
   "page_count": 4,
   "order": 640,
   "p1": "vol. 3, 726-729",
   "pn": "",
   "abstract": [
    "This paper proposes a method of constructing acoustic models from training data clustered in two stages. In the first stage, training data from a target task are clustered and generate GMMs for each cluster. The second stage uses the GMMs to select training data from a large-scale database based on the GMM likelihood. MAP estimation adapts an acoustic model for each cluster using the selected training data. In decoding, the best acoustic model is selected from all acoustic models based on the GMM likelihood using some initial frames of an input utterance. Broadcast news transcription experiments showed that the proposed models achieved a word error reduction of 20% and a processing time reduction of 22%, compared with a non-clustered model.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-638"
  },
  "torre00_icslp": {
   "authors": [
    [
     "Angel de la",
     "Torre"
    ],
    [
     "Dominique",
     "Fohr"
    ],
    [
     "Jean-Paul",
     "Haton"
    ]
   ],
   "title": "Compensation of noise effects for robust speech recognition in car environments",
   "original": "i00_3730",
   "page_count": 4,
   "order": 641,
   "p1": "vol. 3, 730-733",
   "pn": "",
   "abstract": [
    "In this paper, we propose a novel method to compensate the effect of the noise for Automatic Speech Recognition in car environments. This method can be applied to recognizers using a standard MFCC front-end. We perform a channel-by-channel compensation of the noise effect in the filter-bank output domain. In a first stage, the parameters describing the noise are estimated and secondly, we estimate the expected value of the clean speech in a probabilistic framework. The compensated filter-bank outputs are then used to obtain a compensated version of the MFCC-based parameters representing the speech signal.\n",
    "Recognition experiments using the French VODIS database (recorded in several cars running in real traffic situations) have been carried out to test the proposed compensation method. The results show the capability of the proposed method for the compensation of the noise effect in car environments.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-639"
  },
  "kim00d_icslp": {
   "authors": [
    [
     "Dong Kook",
     "Kim"
    ],
    [
     "Nam Soo",
     "Kim"
    ]
   ],
   "title": "Bayesian speaker adaptation based on probabilistic principal component analysis",
   "original": "i00_3734",
   "page_count": 4,
   "order": 642,
   "p1": "vol. 3, 734-737",
   "pn": "",
   "abstract": [
    "In this paper, we propose a Bayesian speaker adaptation technique based on the probabilistic principal component analysis (PPCA). The PPCA is employed to obtain the canonical speaker models which provide the a priori knowledge of the training speakers. The proposed approach is conveniently incorporated into the Bayesian adaptation framework where the parameters are adapted to the new speakers speech according to the maximum a posteriori (MAP) criterion. Through a number of continuous digit recognition experiments, we can find the effectiveness of the PPCA-based approach compared to the other adaptation approaches with a small amount of adaptation data.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-640"
  },
  "liu00h_icslp": {
   "authors": [
    [
     "Wai Kat",
     "Liu"
    ],
    [
     "Pascale",
     "Fung"
    ]
   ],
   "title": "MLLR-based accent model adaptation without accented data",
   "original": "i00_3738",
   "page_count": 4,
   "order": 643,
   "p1": "vol. 3, 738-741",
   "pn": "",
   "abstract": [
    "When the user has an accent different from what the automatic speech recognization system is trained with, the performance of the systems degrades. This is attributed to both acoustic and phonological differences between accents. The phonological differences between two accents are due to different phoneme inventories in two languages. Even for the same phoneme, foreigners and native speakers pronounce different sounds. Since accented data is rare but monolingual data is abundant, we propose using the accented speakers first language data directly instead of accented data in the second language for our purpose. We propose adapting the native English phoneme models to accented phoneme models using first language data in MLLR adaptation. The baseline performance is 35.25% (phone accuracy) in using native English phone models to recognize Cantoneseaccented English speech data. We compare accent adaptation by using accented data and source language data. On the average, using accented data for adaptation improves the phone accuracy by 69.98% while using source language data for adaptation improves the phone accuracy by 70.13%. This shows that both kinds of adaptation data give similar improvements. Therefore non-accented data can be used for adaptation. We can rapidly obtain an accent-adapted acoustic model without the need of collecting accented database.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-641"
  },
  "chen00j_icslp": {
   "authors": [
    [
     "Kuan-Ting",
     "Chen"
    ],
    [
     "Wen-Wei",
     "Liau"
    ],
    [
     "Hsin-Min",
     "Wang"
    ],
    [
     "Lin-Shan",
     "Lee"
    ]
   ],
   "title": "Fast speaker adaptation using eigenspace-based maximum likelihood linear regression",
   "original": "i00_3742",
   "page_count": 4,
   "order": 644,
   "p1": "vol. 3, 742-745",
   "pn": "",
   "abstract": [
    "This paper presents an eigenspace-based fast speaker adaptation approach which can improve the modeling accuracy of the conventional maximum likelihood linear regression (MLLR) techniques when only very limited adaptation data is available. The proposed eigenspace-based MLLR approach was developed by introducing a priori knowledge analysis on the training speakers via PCA, so as to construct an eigenspace for MLLR full regression matrices as well as to derive a set of bases called eigen-matrices. The full regression matrices for each outside speaker are then constrained to be located in the space spanned by the first K eigen-matrices. The proposed eigenspace-based regression matrices, serving as an initial estimate of the speaker-specific MLLR transformation, effectively reduces the number of free parameters, while precise modeling for the inter-dimensional correlation among the model parameters by full matrices was maintained. Experimental results showed that for supervised adaptation using adaptation data with a length of approximately 10 seconds, the proposed approach significantly outperformed the conventional MLLR approaches.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-642"
  },
  "potamianos00c_icslp": {
   "authors": [
    [
     "Gerasimos",
     "Potamianos"
    ],
    [
     "Chalapathy",
     "Neti"
    ]
   ],
   "title": "Stream confidence estimation for audio-visual speech recognition",
   "original": "i00_3746",
   "page_count": 4,
   "order": 645,
   "p1": "vol. 3, 746-749",
   "pn": "",
   "abstract": [
    "We investigate the use of single modality confidence measures as a means of estimating adaptive, local weights for improved audio- visual automatic speech recognition. We limit our work to the toy problem of audio-visual phonetic classification by means of a two-stream Gaussian mixture model (GMM), where each stream models the class conditional audio- or visual-only observation probability, raised to an appropriate exponent. We consider such stream exponents as two-dimensional piecewise constant functions of the audio and visual stream local confidences, and we estimate them by minimizing the misclassification error on a held-out data set. Three stream confidence measures are investigated, namely the stream entropy, the n-best likelihood ratio average, and an n-best stream likelihood dispersion measure. The later results in superior audio-visual phonetic classification, as indicated by our experiments on a 260-subject, 40-hour long, large vocabulary, continuous speech audio-visual dataset. By using local, dispersion-based stream exponents, we achieve an additional 20% phone classification accuracy improvement over the improvement that global stream exponents add to clean audio- only phonetic classification. The performance of the algorithm however still falls significantly short of an \"oracle\" (cheating) confidence estimation scheme.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-643"
  },
  "komatsu00_icslp": {
   "authors": [
    [
     "Masahiko",
     "Komatsu"
    ],
    [
     "Won",
     "Tokuma"
    ],
    [
     "Shinichi",
     "Tokuma"
    ],
    [
     "Takayuki",
     "Arai"
    ]
   ],
   "title": "The effect of reduced spectral information on Japanese consonant perception: comparison between L1 and L2 listeners",
   "original": "i00_3750",
   "page_count": 4,
   "order": 646,
   "p1": "vol. 3, 750-753",
   "pn": "",
   "abstract": [
    "We investigated how spectral information contributes to the perception of Japanese consonants, using re-synthesised samples that were created by (1) gradually reducing the order of LPC analysis in the residual excited LPC vocoder; and (2) gradually flattening the spectral peak in the frequency domain. The results of native Japanese speakers showed that the information in LPC residuals contributes significantly, if not sufficiently, to Japanese consonant perception, and that the minimum amount of spectral information is sufficient to achieve 90% identification score. It was also found that, although the perceptual error patterns were different, there were striking similarities between Japanese and non-Japanese listeners in their averaged perception scores. The phonological feature analysis of the perceptual results indicated that the residuals provide broad phonotactic information such as major class features.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-644"
  },
  "ciocca00_icslp": {
   "authors": [
    [
     "Valter",
     "Ciocca"
    ],
    [
     "Rani",
     "Aisha"
    ],
    [
     "Alex",
     "Francis"
    ],
    [
     "Lena",
     "Wong"
    ]
   ],
   "title": "Can cantonese children with cochlear implants perceive lexical tones?",
   "original": "i00_3754",
   "page_count": 4,
   "order": 647,
   "p1": "vol. 3, 754-757",
   "pn": "",
   "abstract": [
    "The purpose of this study was to determine whether prelingually deaf children fitted with a cochlear implant are able to identify Cantonese tones in monosyllabic words. Eight children participated in this study. The age of the children ranged between 4:06 and 8:11. The duration of the post-operative period varied between 11 and 41 months. All children were fitted with the Nucleus 24-channel cochlear implant system. The stimuli were the six contrastive Cantonese tones produced with the monosyllabic target word /ji/. Each stimulus represented a concrete object: for example, the target word uttered with the highlevel tone (/ji55/) means \"clothing\", while the same word produced with the low-level tone (/ji22/) means \"two\". The six target words were produced by a male native speaker of Cantonese within a carrier phrase. Listeners had to identify the target words, presented within the carrier phrase, by selecting one of two pictures. These pictures represented one of eight minimal pair contrasts: high- vs mid-level; high- vs low-level; mid- vs low-level; high-level vs high-rising; high-rising vs low-rising; low-rising vs low-level; low-falling vs low-rising; low-falling vs low-level. Within each trial, one picture represented the object corresponding to the word/tone that had been presented; the other picture represented the other word of a minimal pair. Subjects were tested individually in a soundproof room. Each contrast was presented sixteen times; each member of the minimal pairs was presented eight times for each contrast. The order of the stimuli was randomised. No feedback was given. The results showed that performance was at chance level (50%) for all contrasts: the percentage of correct responses for each contrast varied between 47 and 58%. An analysis of the data grouped by tones, rather than tone contrasts, revealed that none of the tones was identified above chance. These findings suggest that this group of cochlear implant users could not extract sufficiently accurate pitch information to identify Cantonese tones. The implications of this finding for the perception of pitch information through cochlear implants will be discussed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-645"
  },
  "yip00_icslp": {
   "authors": [
    [
     "Michael C. W.",
     "Yip"
    ]
   ],
   "title": "Recognition of spoken words in the continuous speech: effects of transitional probability",
   "original": "i00_3758",
   "page_count": 5,
   "order": 648,
   "p1": "vol. 3, 758-761",
   "pn": "",
   "abstract": [
    "Do Cantonese listeners really use phonotactics information in the segmentation process of Cantonese continuous speech? Because some phoneme transitions across Cantonese syllables occur much more often than the others, the transitional probability may cue the locations of possible syllable boundaries in Cantonese speech. Two syllable-spotting experiments were conducted. Results clearly indicated that listeners would find it easier to spot the target syllable in the nonsense sound sequence, which consisted of high transitional probability phoneme combinations than the low transitional probability phoneme combinations across syllables. These results imply that Cantonese listeners are sensitive to the probabilistic phonotactic information across syllable boundaries during spoken language segmentation.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-646"
  },
  "salomon00_icslp": {
   "authors": [
    [
     "Ariel",
     "Salomon"
    ],
    [
     "Carol",
     "Espy-Wilson"
    ]
   ],
   "title": "Detection of speech landmarks using temporal cues",
   "original": "i00_3762",
   "page_count": 4,
   "order": 649,
   "p1": "vol. 3, 762-765",
   "pn": "",
   "abstract": [
    "In order to improve the performance of speech recognizers, particularly in degraded environments, it may be beneficial to integrate use of temporal information. As literature has shown that human listeners are able to use temporal cues in speech recognition tasks, this study examines algorithms for extraction of temporal cues in a speech signal. The task under analysis is the location of landmarks to direct further analysis in a knowledge-based speech recognition system. In this study, a set of robust landmark types were located with an accuracy of 82.5%-100.0%.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-647"
  },
  "otake00_icslp": {
   "authors": [
    [
     "Takashi",
     "Otake"
    ],
    [
     "Anne",
     "Cutler"
    ]
   ],
   "title": "A set of Japanese word cohorts rated for relative familiarity",
   "original": "i00_3766",
   "page_count": 5,
   "order": 650,
   "p1": "vol. 3, 766-769",
   "pn": "",
   "abstract": [
    "A database is presented of relative familiarity ratings for 24 sets of Japanese words, each set comprising words overlapping in the initial portions. These ratings are useful for the generation of material sets for research in the recognition of spoken words.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-648"
  },
  "yamakawa00_icslp": {
   "authors": [
    [
     "Kimiko",
     "Yamakawa"
    ],
    [
     "Hiromitsu",
     "Miyazono"
    ],
    [
     "Ryoji",
     "Baba"
    ]
   ],
   "title": "The phonetic value of the devocalized vowel in Japanese - in case of velar plosive",
   "original": "i00_3770",
   "page_count": 4,
   "order": 651,
   "p1": "vol. 3, 770-773",
   "pn": "",
   "abstract": [
    "This study investigates how the native speakers of Japanese tell the differences of the words containing the devocalized vowels. The close vowels [i] and [u] devocalize in Japanese when they are situated between voiceless consonants and between pause and voiceless consonant. In this paper, we verify the hypothesis that the devocalized vowels are the voiceless fricatives. [kjitto], [kjutto], [kjatto] and [kjotto] were used for three experiments as the stimuli. The close vowels that follow the voiceless velar plosives devocalize in [kjitto] and [kjutto], on the other hand, the open vowels in [kjatto] and [kjotto] don't devocalize. The first experiment studied whether the native speakers of Japanese could distinguish the four stimuli. The second experiment studied whether the clue with which the native speakers of Japanese hear the differences of the words are the devocalized vowels or not. The stimuli of which the parts of the devocalized vowels were replaced with band limited noise were used. In the final experiment, the manipulated stimuli, in which the voiceless fricatives were inserted instead of the vowel.\n",
    "These results indicate that the parts of the devocalized vowel have the voiceless fricatives when the vowels follow the voiceless velar plosives, and that the native speakers of Japanese can hear the differences of the words containing the devocalized vowels through the differences of the values of the voiceless fricatives.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-649"
  },
  "mcqueen00_icslp": {
   "authors": [
    [
     "James M.",
     "McQueen"
    ],
    [
     "Anne",
     "Cutler"
    ],
    [
     "Dennis",
     "Norris"
    ]
   ],
   "title": "Positive and negative influences of the lexicon on phonemic decision-making",
   "original": "i00_3778",
   "page_count": 4,
   "order": 652,
   "p1": "vol. 3, 778-781",
   "pn": "",
   "abstract": [
    "Lexical knowledge influences how human listeners make decisions about speech sounds. Positive lexical effects (faster responses to target sounds in words than in nonwords) are robust across several laboratory tasks, while negative effects (slower responses to targets in more word-like nonwords than in less word-like nonwords) have been found in phonetic decision tasks but not phoneme monitoring tasks. The present experiments tested whether negative lexical effects are therefore a task-specific consequence of the forced choice required in phonetic decision. We compared phoneme monitoring and phonetic decision performance using the same Dutch materials in each task. In both experiments there were positive lexical effects, but no negative lexical effects. We observe that in all studies showing negative lexical effects, the materials were made by cross-splicing, which meant that they contained perceptual evidence supporting the lexically-consistent phonemes. Lexical knowledge seems to influence phonemic decision-making only when there is evidence for the lexically-consistent phoneme in the speech signal.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-650"
  },
  "weber00c_icslp": {
   "authors": [
    [
     "Andrea",
     "Weber"
    ]
   ],
   "title": "Phonotactic and acoustic cues for word segmentation in English",
   "original": "i00_3782",
   "page_count": 4,
   "order": 653,
   "p1": "vol. 3, 782-785",
   "pn": "",
   "abstract": [
    "This study investigates the influence of both phonotactic and acoustic cues on the segmentation of spoken English. Listeners detected embedded English words in nonsense sequences (word spotting). Words aligned with phonotactic boundaries were easier to detect than words without such alignment. Acoustic cues to boundaries could also have signaled word boundaries, especially when word onsets lacked phonotactic alignment. However, only one of several durational boundary cues showed a marginally significant correlation with response times (RTs). The results suggest that word segmentation in English is influenced primarily by phonotactic constraints and only secondarily by acoustic aspects of the speech signal.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-651"
  },
  "janse00b_icslp": {
   "authors": [
    [
     "Esther",
     "Janse"
    ]
   ],
   "title": "Intelligibility of time-compressed speech: three ways of time-compression",
   "original": "i00_3786",
   "page_count": 6,
   "order": 654,
   "p1": "vol. 3, 786-789",
   "pn": "",
   "abstract": [
    "Studies on fast speech have shown that word-level timing of fast speech differs from that of normal rate speech in that unstressed syllables are shortened more than stressed syllables as speech rate increases. An earlier experiment showed that the intelligibility of time-compressed speech could not be improved by making its temporal organisation closer to natural fast speech. To test the hypothesis that segmental intelligibility is more important than prosodic timing in listening to timecompressed speech, the intelligibility of bisyllabic words was tested in three time-compression conditions: either stressed and unstressed syllable were compressed to the same degree, or the stressed syllable was compressed more than the unstressed syllable, or the reverse. As was found before, imitating wordlevel timing of fast speech did not improve intelligibility over linear compression. However, the results did not confirm the hypothesis either: there was no difference in intelligibility between the three compression conditions. We conclude that segmental intelligibility plays an important role, but further research is necessary to decide between the contributions of prosody and segmental intelligibility to the word-level intelligibility of time-compressed speech.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-652"
  },
  "traunmuller00_icslp": {
   "authors": [
    [
     "Hartmut",
     "Traunmüller"
    ]
   ],
   "title": "Evidence for demodulation in speech perception",
   "original": "i00_3790",
   "page_count": 4,
   "order": 655,
   "p1": "vol. 3, 790-793",
   "pn": "",
   "abstract": [
    "According to the Modulation Theory, speakers modulate their voice with linguistic gestures, and listeners demodulate the signal in order to separate the linguistic from the expressive and organic information. Listeners tune in to the carrier (the voice) on the basis of an analysis of a stretch of speech and they evaluate its modulation. This is reflected in many perceptual experiments that involved manipulated introductory phrases, blocked vs. randomized speakers, and other non-linguistic variables.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-653"
  },
  "gauvain00_icslp": {
   "authors": [
    [
     "Jean-Luc",
     "Gauvain"
    ],
    [
     "Lori",
     "Lamel"
    ]
   ],
   "title": "Fast decoding for indexation of broadcast data",
   "original": "i00_3794",
   "page_count": 4,
   "order": 656,
   "p1": "vol. 3, 794-797",
   "pn": "",
   "abstract": [
    "Processing time is an important factor in making a speech transcription system viable for automatic indexation of radio and television broadcasts. When only concerned by the word error rate, it is common to design systems that run in 100 times real-time or more. This paper addresses issues in reducing the speech recognition time for automatic indexation of radio and TV broadcasts with the aim of obtaining reasonable performance for close to real-time operation. We investigated computational resources in the range 1 to 10xRT on commonly available platforms. Constraints on the computational resources led us to reconsider design issues, particularly those concerning the acoustic models and the decoding strategy. A new decoder was implemented which transcribes broadcast data in few times real-time with only a slight increase in word error rate when compared to our best system. Experiments with spoken document retrieval show that comparable IR results are obtained with a 10xRT automatic transcription or with manual transcription, and that reasonable performamce is still obtained with a 1.4xRT transcription system.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-654"
  },
  "gao00d_icslp": {
   "authors": [
    [
     "Sheng",
     "Gao"
    ],
    [
     "Bo",
     "Xu"
    ],
    [
     "Hong",
     "Zhang"
    ],
    [
     "Bing",
     "Zhao"
    ],
    [
     "Chengrong",
     "Li"
    ],
    [
     "Taiyi",
     "Huang"
    ]
   ],
   "title": "Update progress of Sinohear: advanced Mandarin LVCSR system at NLPR",
   "original": "i00_3798",
   "page_count": 4,
   "order": 657,
   "p1": "vol. 3, 798-801",
   "pn": "",
   "abstract": [
    "NLPR has been with long efforts on Mandarin speech recognition. This paper reports our recent process in this field with several significant novel characteristics: 1) Very large speech databases are used to learn more robust acoustic model; 2) Acoustic model has evolved from non-tonal class-triphone to tonal class-triphone based on tone-embedded decision tree, namely unified tone & triphone modeling. The experimental results for large test databases show 1) hybrid databases are helpful for performance improvement; 2) tone information is very useful and could contribute 20% character error reduction for high quality \"863\" database; and 3) one-pass decoder is an more efficient framework than multi-pass decoder, especially when LM and AM are accurate.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-655"
  },
  "aubert00_icslp": {
   "authors": [
    [
     "Xavier L.",
     "Aubert"
    ],
    [
     "Reinhard",
     "Blasig"
    ]
   ],
   "title": "Combined acoustic and linguistic look-ahead for one-pass time-synchronous decoding",
   "original": "i00_3802",
   "page_count": 4,
   "order": 658,
   "p1": "vol. 3, 802-805",
   "pn": "",
   "abstract": [
    "This paper describes an enhanced pruning technique aimed at a further reduction of the active search space in large vocabulary speech recognition, to speed-up decoding while maintaining the accuracy. The method is based on anticipating both the linguistic and acoustic contribution of a phonetic arc, before expanding that arc in the search. The decoder is based on a time-synchronous beam search and a lexical tree. Cross-word HMMs and M-gram language models are integrated in a single decoding pass. The new algorithm has been evaluated for one-pass trigram decoding of Broadcast news. With respect to the baseline, the search effort can be halved at almost no degradation. When pruning more aggressively to get a speed-up of 10, real-time decoding is achieved on Hub4 evaluation, however, with an increase of the base error rate by one third.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-656"
  },
  "deng00d_icslp": {
   "authors": [
    [
     "Li",
     "Deng"
    ],
    [
     "Alex",
     "Acero"
    ],
    [
     "Mike",
     "Plumpe"
    ],
    [
     "Xuedong",
     "Huang"
    ]
   ],
   "title": "Large-vocabulary speech recognition under adverse acoustic environments",
   "original": "i00_3806",
   "page_count": 5,
   "order": 659,
   "p1": "vol. 3, 806-809",
   "pn": "",
   "abstract": [
    "We report our recent work on noise-robust large-vocabulary speech recognition. Three key innovations are developed and evaluated in this work: 1) a new model learning paradigm that comprises a noise-insertion process followed by noise reduction; 2) a noise adaptive training algorithm that integrates noise reduction into probabilistic multi-style system training; and 3) a new algorithm (SPLICE) for noise reduction that makes no assumptions about noise stationarity. Evaluation on a large-vocabulary speech recognition task demonstrates significant and consistent error rate reduction using these techniques. The resulting error rate is shown to be lower than that achieved by the matched-noisy condition for both stationary and nonstationary natural, as well as simulated, noises.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-657"
  },
  "fischer00_icslp": {
   "authors": [
    [
     "Volker",
     "Fischer"
    ],
    [
     "S. J.",
     "Kunzmann"
    ]
   ],
   "title": "Acoustic language model classes for a large vocabulary continuous speech recognizer",
   "original": "i00_3810",
   "page_count": 4,
   "order": 660,
   "p1": "vol. 3, 810-813",
   "pn": "",
   "abstract": [
    "In a maximum a posteriori probability approach to speech recognition stochastic n-gram language models are used for the estimation of a word sequence's a priori probability. In any practical implementation of a large vocabulary speech recognition system the language model acts as a hypotheses filter that has to differ between candidate words with similar acoustic evidence. For that purpose, the combination of word based and class based language models is attractive, because it allows to fall back to the more reliable estimates of the class based model in case of sparse training data. However, class language models can differ between words from the same class only in terms of a priori probability. To improve the discriminative power for words with similar acoustic score, it is therefore useful to put similar sounding words into different classes.\n",
    "Based on the above considerations, the paper presents an automatic procedure for the optimal classification of a large vocabulary into classes with acoustic dissimilar words. Jn combination with a standard word based trigram model the so created acoustic class language model provides a relative reduction in word error rate of up to 16 percent and performs slightly better than a perplexity minimizing automatically created class language model.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-658"
  },
  "kummert00_icslp": {
   "authors": [
    [
     "Franz",
     "Kummert"
    ],
    [
     "Gernot A.",
     "Fink"
    ],
    [
     "Gerhard",
     "Sagerer"
    ]
   ],
   "title": "A hybrid speech recognizer combining HMMs and polynomial classification",
   "original": "i00_3814",
   "page_count": 4,
   "order": 661,
   "p1": "vol. 3, 814-817",
   "pn": "",
   "abstract": [
    "In this paper, we present a hybrid speech recognizer combining Hidden Markov Models (HMMs) and a polynomial classifier. In our approach the emission probabilities are not modeled as a mixture of Gaussians but are calculated by the polynomial classifier. However, we do not apply the classifier directly to the feature vector but we make use of the density values of L Gaussians clustering the feature space. That means we model the emission probability as a polynomial of Gaussian distributions of n-th degree. As most of these density values are approximately zero for a single feature vector the calculation of a polynomial can be done very efficiently. The usefulness of this hybrid approach was successfully tested on a large conversational speech recognition task.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-659"
  },
  "huang00d_icslp": {
   "authors": [
    [
     "Chao",
     "Huang"
    ],
    [
     "Eric",
     "Chang"
    ],
    [
     "Jianlai",
     "Zhou"
    ],
    [
     "Kai-Fu",
     "Lee"
    ]
   ],
   "title": "Accent modeling based on pronunciation dictionary adaptation for large vocabulary Mandarin speech recognition",
   "original": "i00_3818",
   "page_count": 4,
   "order": 662,
   "p1": "vol. 3, 818-821",
   "pn": "",
   "abstract": [
    "A method of accent modeling through Pronunciation Dictionary Adaptation (PDA) is presented. We derive the pronunciation variation between canonical speaker groups and accent groups and add an encoding of the differences to a canonical dictionary to create a new, adapted dictionary that reflects the accent characteristics. The pronunciation variation information is then integrated with acoustic and language models into a one-pass search framework. It is assumed that acoustic deviation and pronunciation variation are independent but complementary phenomena that cause poor performance among accented speakers. Therefore, MLLR, an efficient model adaptation technique, is also presented both alone and in combination with PDA. It is shown that when PDA, MLLR and PDA+MLLR are used, error rate reductions of 13.9%, 24.1% and 28.4% respectively are achieved.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-660"
  },
  "zhang00i_icslp": {
   "authors": [
    [
     "Jinzhong",
     "Zhang"
    ],
    [
     "Yingmin",
     "He"
    ],
    [
     "Renshu",
     "Yu"
    ]
   ],
   "title": "A mixed and code excitation LPC vocoder at 1.76 kb/s",
   "original": "i00_3822",
   "page_count": 4,
   "order": 663,
   "p1": "vol. 3, 822-825",
   "pn": "",
   "abstract": [
    "This paper describes a speech-coding algorithm that we develop recently. We call it M&CELP VOCODER. It is based on the traditional linear prediction vocoder. The excitation is constructed by not usually one of either periodic pulse or white noise, but the mixture of periodic feature-waveform (from predict error) and periodic pulse and white noise passed high pass filter in voiced speech section and white noise in the unvoiced speech. At parameter quantization, The quantizer based on Tree-search technique is applied, and it is improved. It enhanced the performance of reconstructed speech, while maintaining the lower bit rate. In addition, the scheme of pitch evaluation is improved, and this vocoder is very efficient to eliminate thumps and buzz, and it enhanced the naturalness and intelligibility. The algorithm is very efficient at speed, it can be realized at the PC computer with high configuration. Informal listening confirm that it is as good as the new U.S. federal standard: MELP at 2.4kb/s.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-661"
  },
  "kohata00_icslp": {
   "authors": [
    [
     "Minoru",
     "Kohata"
    ],
    [
     "Ikuya",
     "Mitsuya"
    ],
    [
     "Motoyuki",
     "Suzuki"
    ],
    [
     "Shozo",
     "Makino"
    ]
   ],
   "title": "Efficient segment quantization of LSP parameters for very low bit speech coding",
   "original": "i00_3826",
   "page_count": 4,
   "order": 664,
   "p1": "vol. 3, 826-829",
   "pn": "",
   "abstract": [
    "This paper presents a new segment quantization (SQ) method for LPC coefficients, which is based upon a new segmentation scheme. In order to design an efficient segment coder, a segmentation method using the self-similarity of LSP coefficients between several frames is employed. The segmentation is carried out by matching the input LSP frames with each segment in the codebook, and the codebook is trained at the same time with a method similar to the Lempel-Ziv coding method, which is one of the universal coding methods for discrete symbols. In the training process, the segment codebook is grown from null to the desired size. Two types of segment quantizer are designed based upon the proposed segmentation method, and both methods can operate at low rates of below 7 bit/frame, with low complexity and low cepstral distortion of less than 2.4 dB.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-662"
  },
  "ribeiro00_icslp": {
   "authors": [
    [
     "Carlos M.",
     "Ribeiro"
    ],
    [
     "Isabel M.",
     "Trancoso"
    ],
    [
     "Diamantino A.",
     "Caseiro"
    ]
   ],
   "title": "Phonetic vocoder assessment",
   "original": "i00_3830",
   "page_count": 4,
   "order": 665,
   "p1": "vol. 3, 830-833",
   "pn": "",
   "abstract": [
    "The efficiency of phonetic vocoders stems from the fact that the only transmitted information is the index of the recognised units and the corresponding prosodic parameters. Hence, speaker recognisability is one of the main issues in this class of coders. Our approach to minimise this drawback was to include some speaker adaptation capability. The purpose of this paper is two-folded: on one hand, to describe the recognisability and intelligibility tests that were performed with our phonetic vocoder with and without speaker adaptation; on the other hand, to present our recent developments of this coder, using the SpeechDat corpus for Portuguese, that includes telephone calls from 5000 speakers. This allowed us to generate improved HMM models, codebooks, and quantization tables, and to investigate the performance of the coder in non-clean environments and with a much wider speaker population.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-663"
  },
  "hu00b_icslp": {
   "authors": [
    [
     "Hongtao",
     "Hu"
    ],
    [
     "Limin",
     "Du"
    ]
   ],
   "title": "A new low bit rate speech coder based on intraframe waveform interpolation",
   "original": "i00_3834",
   "page_count": 4,
   "order": 666,
   "p1": "vol. 3, 834-837",
   "pn": "",
   "abstract": [
    "A new characteristic waveform (CW) interpolation coder is proposed in this paper. In the proposed coder, two characteristic waveforms are extracted from LPC residual signal at each frame. The Waveform Interpolation (WI) is operated within the frame. In the novel WI, variable dimension vector quantization (VDVQ) and power vector quantization are proposed and the low frequency band (LFB) and high frequency band (HFB) are allocated different numbers of bits according to human hearing perception. A result of a 2400 bps coder is presented and the reconstructed speech shows its quality is closed to FED-STD-1016.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-664"
  },
  "chengalvarayan00d_icslp": {
   "authors": [
    [
     "Rathinavelu",
     "Chengalvarayan"
    ],
    [
     "David L.",
     "Thomson"
    ]
   ],
   "title": "Discriminatively derived HMM-based announcement modeling approach for noise control avoiding the problem of false alarms",
   "original": "i00_3838",
   "page_count": 4,
   "order": 667,
   "p1": "vol. 3, 838-841",
   "pn": "",
   "abstract": [
    "Earlier we proposed modeling echo residuals by using multiple echo models built from a set of specific announcement. Experienced callers may interrupt the prompt by speaking the keywords over the prompt. This leads to incomplete prompt echoes that was not properly modeled by multiple echo models. In this study, we investigate further improvements by building an echo model of each word in the entire announcement, then linking each model in sequence to track the exact echo that precedes valid speech (movie title). The experimental results show that by modeling exactly, one can get better recognition accuracy and less false triggering, with a possible increase in computational complexity.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-665"
  },
  "huerta00_icslp": {
   "authors": [
    [
     "Juan M.",
     "Huerta"
    ],
    [
     "Richard M.",
     "Stern"
    ]
   ],
   "title": "Instantaneous-distortion based weighted acoustic modeling for robust recognition of coded speech",
   "original": "i00_3842",
   "page_count": 4,
   "order": 668,
   "p1": "vol. 3, 842-845",
   "pn": "",
   "abstract": [
    "In this paper we apply the Weighted Acoustic Modeling (WAM) technique to the recognition of speech coded by the full-rate GSM codec or the FS-1016 CELP codec employing various estimates of instantaneous distortion. In the WAM method, separate hidden Markov models are developed for regions of speech that exhibit low levels of codec-induced distortion and for regions with higher levels of such distortion. At recognition time, the contributions of these models are mixed together with a weighting that is determined by estimating the instantaneous distortion. In this paper instantaneous distortion was estimated from the instantaneous cepstral distortion, the long-term gain parameter of the codec, the long-term predictability of the reconstructed signal, and measurements of recoding sensitivity. We observe that the use of the long-term gain parameter produces results that are similar to those obtained by use of cepstral distortion (which can only be obtained if the original cepstra are transmitted along with the speech signal) for the GSM codec. Overall, the effect of the degradation in error rate introduced by coding can be reduced by up to 55% with these techniques for GSM coding, and by up to 38% for the CELP coding.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-666"
  },
  "rajput00_icslp": {
   "authors": [
    [
     "Nitendra",
     "Rajput"
    ],
    [
     "L. Venkata",
     "Subramaniam"
    ],
    [
     "Ashish",
     "Verma"
    ]
   ],
   "title": "Adapting phonetic decision trees between languages for continuous speech recognition",
   "original": "i00_3850",
   "page_count": 3,
   "order": 669,
   "p1": "vol. 3, 850-852",
   "pn": "",
   "abstract": [
    "In a continuous speech recognition system it is important to model the context dependent variations in the pronunciations of phones. In this work we have attempted to build decision trees for modeling phonetic context-dependency in Hindi. The approach followed is to modify a decision tree built to model context-dependency in American English. The reason the decision trees turn out to be different are that the English and Hindi phoneme sets are not identical. Then even for identical phonemes, the context-dependency is different for the two languages. Linguistic-Phonetic knowledge of Hindi is used to modify the English phone set. Since the Hindi phone set being used is derived from the English phone set, the adaptation of the English tree to Hindi follows naturally. Though here the adaptation is from English to Hindi, the method may be applicable for adapting between any two languages. The decision tree is built using either Hindi data or English data labeled with the correct Hindi contexts. This procedure is discussed and the limitations of both the methods are described.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-667"
  },
  "cox00_icslp": {
   "authors": [
    [
     "Stephen",
     "Cox"
    ]
   ],
   "title": "Speaker normalization in the MFCC domain",
   "original": "i00_3853",
   "page_count": 4,
   "order": 670,
   "p1": "vol. 3, 853-856",
   "pn": "",
   "abstract": [
    "It has been shown in several recent publications that application of vocal tract normalization (VTN) is a successful method for improving the accuracy of speaker independent recognisers. We argue that VTN can be implemented in the filterbank domain and propose a model to achieve this. We show how the model can be implemented directly in the MFCC domain, where it may be viewed as a constrained version of maximum likelihood linear regression (MLLR). The parameter estimates produced by the model are in accord with our ideas about how it should operate to perform VTN. Recognition results on a phoneme recognition task are presented which show a small improvement in accuracy.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-668"
  },
  "haebumbach00_icslp": {
   "authors": [
    [
     "Reinhold",
     "Haeb-Umbach"
    ]
   ],
   "title": "Data-driven phonetic regression class tree estimation for MLLR adaptation",
   "original": "i00_3857",
   "page_count": 4,
   "order": 671,
   "p1": "vol. 3, 857-860",
   "pn": "",
   "abstract": [
    "In this paper a method is presented to estimate a broad phonetic class regression tree to be used in MLLR adap- tation. The tree is derived from the correlation structure among phone units estimated on the training data. The al- gorithm is language-independent and showed good results on both an English and a Mandarin Chinese database. In adaptation experiments the tree outperformed a regres- sion tree obtained from clustering according to closeness in acoustic space and achieved results comparable with those of a manually designed broad phonetic class tree.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-669"
  },
  "afify00_icslp": {
   "authors": [
    [
     "Mohamed",
     "Afify"
    ],
    [
     "Olivier",
     "Siohan"
    ]
   ],
   "title": "Constrained maximum likelihood linear regression for speaker adaptation",
   "original": "i00_3861",
   "page_count": 4,
   "order": 672,
   "p1": "vol. 3, 861-864",
   "pn": "",
   "abstract": [
    "This paper proposes a new structure for use in MLLR adaptation aiming at constraining the transform for potentially better parameter estimation from sparse adaptation data. Motivations for the use of the new structure, and EM based parameter estimation are presented. Experimental results on Spoke3 of the Wall Street Journal task revealed that the proposed transformations outperform a full matrix for a small amount of adaptation data and performs equally well for large adaptation set. They also outperform diagonal transformations for all amounts of adaptation data.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-670"
  },
  "choi00e_icslp": {
   "authors": [
    [
     "Woo-Yong",
     "Choi"
    ],
    [
     "Hyung Soon",
     "Kim"
    ]
   ],
   "title": "Predictive speaker adaptation based on least squares method",
   "original": "i00_3865",
   "page_count": 5,
   "order": 673,
   "p1": "vol. 3, 865-868",
   "pn": "",
   "abstract": [
    "In this paper, we considered two representative speaker adaptation (SA) approaches - MAP and MLLR techniques for our Korean isolated word recognition task. In addition, we proposed a new speaker adaptation algorithm to improve the performance of MAP technique. It is based on least squares method between MAP adapted mean vectors and the corresponding SI mean vectors. The results of our experiment using a CDHMM system indicated that the proposed SA technique yielded high performance improvement in the recognition rate espe-cially when the number of adaptation data is very limited. Moreover, the computational load of proposed technique is much smaller than that of MLLR.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-671"
  },
  "acero00_icslp": {
   "authors": [
    [
     "Alex",
     "Acero"
    ],
    [
     "Li",
     "Deng"
    ],
    [
     "Trausti",
     "Kristjansson"
    ],
    [
     "Jerry",
     "Zhang"
    ]
   ],
   "title": "HMM adaptation using vector taylor series for noisy speech recognition",
   "original": "i00_3869",
   "page_count": 4,
   "order": 674,
   "p1": "vol. 3, 869-872",
   "pn": "",
   "abstract": [
    "In this paper we address the problem of robustness of speech recognition systems in noisy environments. The goal is to estimate the parameters of a HMM that is matched to a noisy environment, given a HMM trained with clean speech and knowledge of the acoustical environment. We propose a method based on truncated vector Taylor series that approximates the performance of a system trained with that corrupted speech. We also provide insight on the approximations used in the model of the environment and compare them with the lognormal approximation in PMC.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-672"
  },
  "vergyri00_icslp": {
   "authors": [
    [
     "Dimitra",
     "Vergyri"
    ],
    [
     "Stavros",
     "Tsakalidis"
    ],
    [
     "William",
     "Byrne"
    ]
   ],
   "title": "Minimum risk acoustic clustering for multilingual acoustic model combination",
   "original": "i00_3873",
   "page_count": 5,
   "order": 675,
   "p1": "vol. 3, 873-876",
   "pn": "",
   "abstract": [
    "In this paper we describe procedures for combining multiple acoustic models, obtained using training corpora from different languages, in order to improve ASR performance in languages for which large amounts of training data are not available. We treat these models as multiple sources of information whose scores are combined in a log-linear model to compute the hypothesis likelihood. The model combination can either be performed in a static way, with constant combination weights, or in a dynamic way, with parameters that can vary for different segments of a hypothesis. The aim is to optimize the parameters so as to achieve minimum word error rate. In order to achieve robust parameter estimation in the dynamic combination case, the parameters are defined to be piecewise constant on different phonetic classes that form a partition of the space of hypothesis segments. The partition is defined, using phonological knowledge, on segments that correspond to hypothesized phones. We examine different ways to define such a partition, including an automatic approach that gives a binary tree structured partition which tries to achieve the minimum WER with the minimum number of classes.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-673"
  },
  "oviatt00c_icslp": {
   "authors": [
    [
     "Sharon",
     "Oviatt"
    ]
   ],
   "title": "Talking to thimble jellies: children²s conversational speech with animated characters",
   "original": "i00_3877",
   "page_count": 4,
   "order": 676,
   "p1": "vol. 3, 877-880",
   "pn": "",
   "abstract": [
    "As spoken language systems become more conversational in nature, new interfaces are emerging with animated characters that are designed to elicit and manage a conversation with the user. In the present study, the spoken language of ten 6-to-10-year-old children was compared while interacting with an animated character in the I SEE! interface and again while interacting with a human adult. Analyses revealed that 25% of childrens utterances contained disfluencies or idiosyncratic lexical content that would be difficult for a recognizer to process. Children had significantly higher disfluency rates than adults, and a steeper slope in their rate of disfluencies as a function of utterance length. The long-term goal of this research is the development of appropriate and robust conversational interfaces for children.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-674"
  },
  "rodman00_icslp": {
   "authors": [
    [
     "Robert",
     "Rodman"
    ],
    [
     "David",
     "McAllister"
    ],
    [
     "Donald",
     "Bitzer"
    ],
    [
     "D.",
     "Chappell"
    ]
   ],
   "title": "A high-resolution glottal pulse tracker",
   "original": "i00_3881",
   "page_count": 4,
   "order": 677,
   "p1": "vol. 3, 881-884",
   "pn": "",
   "abstract": [
    "A new method of computing the glottal pulse period of voiced speech is given. This algorithm is based on the mathematically derived fact that the amplitudes of the odd harmonics of a periodic function with period P are zero when the function is expanded in a Fourier series whose coefficients are determined by integrating over 2P instead of the usual P. It is shown that such a glottal pulse tracker is extremely sensitive to sudden short-lived shifts in the apparent frequency of the glottal pulse that circumscribe certain consonants in the speech stream. This method may therefore be used to segment these consonants for various analytical purposes.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-675"
  },
  "alku00b_icslp": {
   "authors": [
    [
     "Paavo",
     "Alku"
    ],
    [
     "Jan G.",
     "Svec"
    ],
    [
     "Erkki",
     "Vilkman"
    ],
    [
     "Frantisek",
     "Sram"
    ]
   ],
   "title": "Analysis of voice production in breathy, normal and pressed phonation by comparing inverse filtering and videokymography",
   "original": "i00_3885",
   "page_count": 4,
   "order": 678,
   "p1": "vol. 3, 885-888",
   "pn": "",
   "abstract": [
    "The present study addresses comparison of two analysis methods of voice production, inverse filtering and videokymography (VKG). Speech data were collected from two male speakers using sustained phonation during laryngoscopy (sound corresponding approximately to the vowel /ä/). The type of phonation was varied between breathy, normal, and pressed. From the waveforms given by inverse filtering, the time length between the positive and negative peak of the glottal flow derivative and the length of the fundamental period was measured. The length of the open phase and the length of the fundamental period was measured from the VKG-images. The results suggest that the time length between the positive and negative peak of the flow derivative equals the length of the open phase of the VKG-image (i.e., the time difference between the opening and closing of the middle line of the vocal folds) for phonations with complete closure of the glottis.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-676"
  },
  "ito00c_icslp": {
   "authors": [
    [
     "Takayuki",
     "Ito"
    ],
    [
     "Hiroaki",
     "Gomi"
    ],
    [
     "Masaaki",
     "Honda"
    ]
   ],
   "title": "Model of the mechanical linkage of the upper lip-jaw for the articulatory coordination",
   "original": "i00_3889",
   "page_count": 4,
   "order": 679,
   "p1": "vol. 3, 889-892",
   "pn": "",
   "abstract": [
    "The present study considers the muscle impedance control of upper lip-jaw coordination for jaw mechanical perturbations during bilabial utterances. When we perturbed jaw movement for the /Φ/ utterance in the carrier sentences of \"kono aΦaΦa mitai\", labial distance was recovered quickly by the downward shift of the upper lip [1]. Initial downward shift (40ms after the load onset) of the upper lip by the perturbation was larger when the load was supplied at the closing phase when preparing for the first /Φ/ than that at the opening phase when preparing for the second /a/ in the sentence. This result suggests that the stiffness between upper lip and jaw varies depending on speech tasks. To characterize this difference, we estimated the stiffness changes of both utterances by using an upper lip-jaw spring model with positional shifts of the first /Φ/ and the second /a/. The estimated ratio of the upper lip-jaw stiffness for /Φ/ to that for /a/ indicates that the stiffness increased in bilabial utterances. In addition, temporal variation of stiffness highly correlated with that of muscle activity. Moreover, we have succeeded to reproduce such compensatory movements by using a dynamical simulation of the mechanical linkage model with the perturbed jaw responses and estimated stiffness-change. These results suggest that the observed compensatory movements are generated by regulating passive muscle-linkages.\n",
    "",
    "",
    "Ito, T., Gomi, H.and Honda, M.,(2000). Task dependent jaw-lip coordination examined by jaw perturbation during bilabial-consonant utterances. Proc. of sps5, 41-44.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-677"
  },
  "matsumura00_icslp": {
   "authors": [
    [
     "Masafumi",
     "Matsumura"
    ],
    [
     "Takuya",
     "Niikawa"
    ],
    [
     "Taku",
     "Torii"
    ],
    [
     "Hitoshi",
     "Yamasaki"
    ],
    [
     "Hisanaga",
     "Hara"
    ],
    [
     "Takashi",
     "Tachimura"
    ],
    [
     "Takeshi",
     "Wada"
    ]
   ],
   "title": "Measurement of palatolingual contact pressure and tongue force using a force-sensor-mounted palatal plate",
   "original": "i00_3893",
   "page_count": 4,
   "order": 680,
   "p1": "vol. 3, 893-896",
   "pn": "",
   "abstract": [
    "In this paper, we report on a 15-cantilever-type, force-sensormounted palatal plate capable of high sensitivity and accurate measurements of the palatolingual contact pressure and pattern that occur during consonant phonation. This paper also describes measurements of tongue force based on the results of the palatolingual contact pressure measurements that the 15-cantilever- type, force-sensor-mounted palatal plate allowed us to take. Tongue force can be calculated via an algorithm of surface integration over the distribution of palatolingual pressure values in the palatal region. In order to verify the sensors function, we simultaneously measured the weight of a tongue-like elastic test body by using an electronic scale and our force-sensor-mounted palatal plate. The palatolingual contact pressures and tongue forces of five adult male subjects were measured during Japanese phonation. The proposed system allows direct observation of the dynamic aspects of palatolingual contact pressure and tongue force during the phonation of consonants.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-678"
  },
  "engwall00b_icslp": {
   "authors": [
    [
     "Olov",
     "Engwall"
    ]
   ],
   "title": "A 3d tongue model based on MRI data",
   "original": "i00_3901",
   "page_count": 4,
   "order": 681,
   "p1": "vol. 3, 901-904",
   "pn": "",
   "abstract": [
    "A new three-dimensional tongue model has been developed within the KTH 3D vocal tract project using manually extracted tongue contours from MR Images of a reference subject producing 43 artificially sustained Swedish articulations. The six linear parameters jaw height, tongue body, tongue dorsum, tongue tip, tongue advance and tongue width were determined using an ordered linear factor analysis controlled by articulatory measures. 88% of the variation in the midsagittal plane and 78% of the overall sagittal variation was explained by the first five factors of the analysis. The six parameter model is able to reconstruct the modeled articulations in 3D with an overall RMS reconstruction error of 0.13 cm sagittally and 0.12 cm laterally, and it specifically handles lateral differences and the observed asymmetries in tongue shape.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-679"
  },
  "bae00_icslp": {
   "authors": [
    [
     "Jae-Hyun",
     "Bae"
    ],
    [
     "Heo-Jin",
     "Byeon"
    ],
    [
     "Yung-Hwan",
     "Oh"
    ]
   ],
   "title": "Speech quality improvement in TTS system using ABS/OLA sinusoidal model",
   "original": "i00_3905",
   "page_count": 4,
   "order": 682,
   "p1": "vol. 3, 905-908",
   "pn": "",
   "abstract": [
    "In this paper, we propose a novel unit concatenation and synthesis method using ABS/OLA sinusoidal model. Phase succession is used in the unit synthesis assuming that the pitch onset time of the first frame in a given unit is the frame center. In the unit concatenation, the phase succession and interpolation of the sinusoid amplitudes via several frames around the concatenation point is utilized. As a result of applying this method to the Text-to- Speech(TTS) system, we got speech samples which were more intelligible and natural than those produced by conventional method.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-680"
  },
  "bruyninckx00_icslp": {
   "authors": [
    [
     "Marielle",
     "Bruyninckx"
    ],
    [
     "Bernard",
     "Harmegnies"
    ]
   ],
   "title": "A study of palatal segments' production by danish speakers",
   "original": "i00_3909",
   "page_count": 5,
   "order": 683,
   "p1": "vol. 3, 909-912",
   "pn": "",
   "abstract": [
    "Most acoustical researches consider palatality through languages dominated by the palatalization process, like Russian. In this paper, we propose to apply acoustical analysis, adapted to the dynamical character of the palatal sounds to the Danish language, which is exempt from any palatalization process and has no palatal phonemes, except [j]. In all cases, we can observe the setting up of a transitionality marking the acoustical signal in one or several of its attributes. The realizations of the sequences associating the palatal segment with a nasal one offered very interesting results: some particular reorganizations of the time function, which do not exist in Russian, can be observed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-681"
  },
  "ramabhadran00_icslp": {
   "authors": [
    [
     "Bhuvana",
     "Ramabhadran"
    ],
    [
     "Yuqing",
     "Gao"
    ],
    [
     "Michael",
     "Picheny"
    ]
   ],
   "title": "Dynamic selection of feature spaces for robust speech recognition",
   "original": "i00_3913",
   "page_count": 4,
   "order": 684,
   "p1": "vol. 3, 913-916",
   "pn": "",
   "abstract": [
    "Selection of acoustic features for robust speech recognition has been the subject of research for several years. In the past, algorithms that use feature vectors from multiple frequency bands [1], or employ techniques to switch between multiple feature streams [2] have been reported in the literature to handle robustness under different acoustic conditions. Acoustic models built out of different feature sets produce different kinds of recognition errors. In this paper, we propose a likelihood-based scheme to combine the acoustic feature vectors from multiple signal processing schemes within the decoding framework, in order to extract maximum benefit from these different acoustic feature vectors and models. The proposed technique is general enough to be applied to other pattern recognition fields, such as, OCR, handwriting recognition, etc. The fundamental idea behind this approach is to pick the set of features that classifies a frame of speech accurately with no apriori information about the phonetic class or acoustic channel that this speech comes from. Two methods of merging any set of acoustic features, such as, formant-based features, cepstral feature vectors, PLP features, LDA features etc., are presented here: Use of a weighted set of likelihoods obtained from these several alternative feature sets and Selection of the feature space that ranks the best when used in a rank-based recognizer These merging algorithms provide an impressive reduction in error rate between 8% to 15% relative across a wide variety of wide-band, clean and noisy large vocabulary continuous speech recognition tasks. Much of this gain is from the reduced insertion and substitution errors. Using the approach presented in this paper, we have achieved better improved acoustic modeling without increasing the number of parameters, i.e. two 40K Gaussian systems, when merged perform better than a 180K Gaussian system trained on the better of the two feature spaces.\n",
    "s K. Paliwal, \"Spectral Subband Centroid Features for Speech recognition,\" ICASSP'98 pp. 617-620, Seattle, May, 1998 L. Jiang, \"Unified Decoding and Feature Representation for Improved Speech Recognition,\" Eurospeech'99, pp. 1331-1334, Budapest, 1999.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-682"
  },
  "fernandez00b_icslp": {
   "authors": [
    [
     "Santiago",
     "Fernández"
    ],
    [
     "Sergio",
     "Feijóo"
    ]
   ],
   "title": "A probabilistic model of integration of acoustic cues in FV syllables",
   "original": "i00_3917",
   "page_count": 4,
   "order": 685,
   "p1": "vol. 3, 917-920",
   "pn": "",
   "abstract": [
    "The interaction of consonantal and vocalic segments in FV syllables regarding identification of place of articulation of fricatives has been studied. A probabilistic model for integration of acoustic information in both segments is proposed. The model weights each segments contribution and integrates them in order to resemble listeners perception. First, the perceptual validity of the model has been assessed. Overall correlations of the probabilistic model with listeners responses to a set of natural and conflicting-cue (place of articulation) FV syllables were 0.88 and 0.74, respectively. The results showed that in order to increase correlations, the model should weight F and V segments differently for each fricative, and even for each vocalic context, since listeners gave more or less importance to the vocalic transition depending on the particular fricative and vocalic context. Acoustic analysis was also carried out computing the probabilistic model with a posteriori probabilities for F and V segments obtained with quadratic discriminant analysis. The results were disappointing, indicating that usual statistical methods fail to extract coarticulatory information. This prevents ASR systems from taking advantage of the enhancement of the characteristics of the consonant in the FV condition with respect to the F condition.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-683"
  },
  "bilmes00_icslp": {
   "authors": [
    [
     "Jeff A.",
     "Bilmes"
    ],
    [
     "Katrin",
     "Kirchhoff"
    ]
   ],
   "title": "Directed graphical models of classifier combination: application to phone recognition",
   "original": "i00_3921",
   "page_count": 4,
   "order": 686,
   "p1": "vol. 3, 921",
   "pn": "",
   "abstract": [
    "Classifier combination is a technique that often provides appreciable accuracy gains. In this paper, we argue that the underlying statistical model of classifier combination should be made explicit. Using directed graphical models (DGMs), we provide representations of two common combination schemes, the mean and product rules. We also introduce new DGMs that yield novel combination rules. We find that these new DGM-inspired rules can achieve significant accuracy gains on the TIMIT phone-classification task relative to existing combination schemes.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-684"
  },
  "jan00_icslp": {
   "authors": [
    [
     "E. E.",
     "Jan"
    ],
    [
     "Jaime",
     "Botella Ordinas"
    ],
    [
     "George",
     "Saon"
    ],
    [
     "Salim",
     "Roukos"
    ]
   ],
   "title": "Real-time multilingual HMM training robust to channel variations",
   "original": "i00_3925",
   "page_count": 4,
   "order": 687,
   "p1": "vol. 3, 925-928",
   "pn": "",
   "abstract": [
    "This paper describes our efforts towards real-time telephony multi-lingual Large Vocabulary Continuous Speech Recognition (LVCSR) system. The trilingual (English, French and Spanish) landline cellular hybrid systems is compared to each of our best monolingual systems. The results are very comparable. The degradation is approximately less than 10%. A HMM state quality measurement technique is explored to improve the performances on multilingual acoustic models. A pilot experiment on English/Spanish bilingual system demonstrates very good results. We achieved between 5% to 20% improvement on different test conditions. To further extend to speaker phone applications, we employed different front-end processing techniques, mainly CDCN prior to HDA and MLLT to reduce the error rate on the trilingual system by as many as 30%. These results suggest that trilingual acoustic models can be used for real telephony applications.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-685"
  },
  "wijngaarden00_icslp": {
   "authors": [
    [
     "Sander J. van",
     "Wijngaarden"
    ],
    [
     "Herman J.M.",
     "Steeneken"
    ]
   ],
   "title": "The intelligibility of German and English speech to Dutch listeners",
   "original": "i00_3929",
   "page_count": 4,
   "order": 688,
   "p1": "vol. 3, 929-932",
   "pn": "",
   "abstract": [
    "Speech utterances in a given language are known to be less intelligible to second-language (L2) listeners than to native listeners. This study is aimed at quantifying the reduction of speech intelligibility due to non-nativeness of the listener. Two types of experiments were carried out: Speech Reception Threshold (SRT) experiments for obtaining a quantitative measure of speech intelligibility, and Letter Guessing Procedure (LGP) experiments to assess to influence of linguistic factors of non-nativeness on speech intelligibility separately. The effects of non-nativeness of listeners were found to be in the range of 1.4 to 4.4 decibel, in terms of speech-to-noise ratio that gives 50% sentence intelligibility. The magnitude of this effect depends on linguistic experience. There is a good correlation between intelligibility and linguistic entropy. To highly proficient L2 listeners, a non-native accent in L2 (similar to their own accent) reduces intelligibility. Less proficient L2 listeners do benefit from such a non-native accent in L2 speech. This indicates a naïve phonetic discrimination.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-686"
  },
  "zhen00b_icslp": {
   "authors": [
    [
     "Bin",
     "Zhen"
    ],
    [
     "Xihong",
     "Wu"
    ],
    [
     "Zhimin",
     "Liu"
    ],
    [
     "Huisheng",
     "Chi"
    ]
   ],
   "title": "On the use of bandpass liftering in speaker recognition",
   "original": "i00_3933",
   "page_count": 4,
   "order": 689,
   "p1": "vol. 3, 933-936",
   "pn": "",
   "abstract": [
    "The measurements of speech spectral envelopes may not accurately characterize the true speech spectrum because of analysis model constraints, such as window position fluctuations, excitation interference, and measurement noise. Juang found that these undesirable spectral measurement variations could be partially reduced by bandpass liftering, and gained better results in speech recognition. In this paper, we propose a new bandpass lifiering process for speaker recognition. The new lifiering process reduces the error rate of 43% than those without the liftering and 9% than Juang's method in speaker recognition. from the experimental result, we found that the spectral contours contain both speech and speaker information and the fine spectral structures contain speaker-discriminating information.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-687"
  },
  "carre00b_icslp": {
   "authors": [
    [
     "René",
     "Carré"
    ],
    [
     "Liliane",
     "Sprenger-Charolles"
    ],
    [
     "Souhila",
     "Messaoud-Galusi"
    ],
    [
     "Willy",
     "Serniclaes"
    ]
   ],
   "title": "On auditory-phonetic short-term transformation",
   "original": "i00_3937",
   "page_count": 4,
   "order": 690,
   "p1": "vol. 3, 937-940",
   "pn": "",
   "abstract": [
    "In a previous experiment, we showed that the vowel-vowel token [ai] is perceived by adult French listeners as /ai/ for transition durations between the 2 vowels lesser than 200 ms (50% of the response) and as /aEi/ for larger durations. Recall that the /ai/ formant trajectory in the F1-F2 plane crosses the region of the vowel /E/. Further, the 200 ms /ai/-/aEi/ perceptual boundary can be related to syllable duration. The same experiment was tested with children of 6.5, and 13 years old. Six tokens [ai] with different duration transitions (50, 100, 150, 200, 250, 300 ms) were randomly presented 10 times each. The question was: \"do you hear 2 or 3 sounds?\". Six and half years old children predominantly perceived 2 vowels and their responses were only slightly affected by transition duration. This response pattern decreased progressively with age for large duration transition tokens. At 13 years, the results were closed to the adults ones. At this point of our research, we may suppose that, at the beginning of the acquisition process, the perception is holistic, i.e. global; then, progressively with age and linguistic environment (probably also through reading acquisition), an auditory-phonetic working short term memory of syllabic duration is set up according to the syllabic structure of the French language. This memory could be used to transform the speech signal into symbolic representation (phonemes, features gestures, ...).\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-688"
  },
  "hant00_icslp": {
   "authors": [
    [
     "James J.",
     "Hant"
    ],
    [
     "Abeer",
     "Alwan"
    ]
   ],
   "title": "Predicting the perceptual confusion of synthetic plosive consonants in noise",
   "original": "i00_3941",
   "page_count": 5,
   "order": 691,
   "p1": "vol. 3, 941-944",
   "pn": "",
   "abstract": [
    "In previous work, a novel, time/frequency detection model was developed based on psychoacoustic masking experiments and used to predict the noise masking of speech-like bursts and formant transitions [5]. In this paper, the same model is used to predict the discrimination of voiced synthetic plosive consonants in a variety of noisy environments. Discrimination experiments were conducted using synthetic /bV/, /dV/, and /gV/ syllables and two different additive noise maskers (speechshaped and perceptually-flat). Experiments were conducted across three vowel contexts (/a/, /i/, and /u/) using CV syllables both with and without a noise burst.\n",
    "Results show that discrimination thresholds are largely dependent on the noise masker, vowel context, and plosive consonant. For all experimental conditions, the addition of the burst has little effect on thresholds, suggesting that the perception of plosive consonants in noise is dominated by the formant transition cue.\n",
    "The previously derived, time/frequency detection model was then used to predict the perceptual data. The model is successful in predicting most of the results, but overpredicts discrimination thresholds for /bi/ and /di/.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-689"
  },
  "larson00_icslp": {
   "authors": [
    [
     "Martha",
     "Larson"
    ],
    [
     "Daniel",
     "Willett"
    ],
    [
     "Joachim",
     "Köhler"
    ],
    [
     "Gerhard",
     "Rigoll"
    ]
   ],
   "title": "Compound splitting and lexical unit recombination for improved performance of a speech recognition system for German parliamentary speeches",
   "original": "i00_3945",
   "page_count": 4,
   "order": 692,
   "p1": "vol. 3, 945-948",
   "pn": "",
   "abstract": [
    "This paper proposes a novel combined compound splitting and phrase recombination method that optimizes the composition of the speech recognition lexicon for a given domain. Data-driven compound word splitting is followed by iterative recombination of high frequency combinations. Language model perplexity and size are the criteria used to identify a balance between compound decomposition, which reduces OOV, and lexical unit recombination, which packs additional context into a fixed-size vocabulary. The method provides a basis for lexicon design for a LVCSR system on the domain of German parliamentary speeches that is to be used as the foundation of a spoken document information retrieval system. The approach achieves a 35% reduction in OOV without a prohibitively large sacrifice in recognition performance.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-690"
  },
  "zundert00b_icslp": {
   "authors": [
    [
     "Martine van",
     "Zundert"
    ],
    [
     "Jacques",
     "Terken"
    ]
   ],
   "title": "Learning and transfer of learning for synthetic speech",
   "original": "i00_3949",
   "page_count": 4,
   "order": 693,
   "p1": "vol. 3, 949-952",
   "pn": "",
   "abstract": [
    "Understanding synthetic speech involves a learning process. We address the question whether transfer of learning takes place from one kind of synthetic speech to another. An experiment is presented in which learning curves for intelligibility were determined for two diphone-based synthesis systems for Dutch, A and B, with different diphone databases. Twenty-four subjects heard eight blocks of 50 Semantically Unpredictable Sentences (SUS). Four different experimental conditions were constructed: In conditions AA and BB, the same synthesis system (A and B, respectively) was presented in blocks 1 to 4 and 5 to 8. In conditions AB and BA one system was presented in blocks 1 to 4 and the other system in blocks 5 to 8. Results show that learning effects are observed within systems (conditions AA and BB). However, we find no evidence of transfer of learning between systems (conditions AB and BA).\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-691"
  },
  "zhang00j_icslp": {
   "authors": [
    [
     "Yang",
     "Zhang"
    ],
    [
     "Patricia K.",
     "Kuhl"
    ],
    [
     "Toshiaki",
     "Imada"
    ],
    [
     "Paul",
     "Iverson"
    ],
    [
     "John",
     "Pruitt"
    ],
    [
     "Makoto",
     "Kotani"
    ],
    [
     "Erica",
     "Stevens"
    ]
   ],
   "title": "Neural plasticity revealed in perceptual training of a Japanese adult listener to learn american /l-r/ contrast: a whole-head magnetoencephalography study",
   "original": "i00_3953",
   "page_count": 4,
   "order": 694,
   "p1": "vol. 3, 953-956",
   "pn": "",
   "abstract": [
    "In this study, behavioral and brain measures were taken to assess the effects of training a Japanese adult subject to perceptually distinguish English /l/ and /¥/. Behavioral data showed significant improvement in identifying both trained and untrained speech stimuli. Correspondingly, neuromagnetic results showed enhanced mismatch field responses in the left hemisphere and reduced activities in the right hemisphere. This pattern of neural plasticity was not observed for truncated nonspeech stimuli.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-692"
  },
  "joto00_icslp": {
   "authors": [
    [
     "Akiyo",
     "Joto"
    ]
   ],
   "title": "The effect of consonantal context and acoustic characteristics on the discrimination between the English vowel /i/ and /e/ by Japanese learners",
   "original": "i00_3957",
   "page_count": 4,
   "order": 695,
   "p1": "vol. 3, 957-960",
   "pn": "",
   "abstract": [
    "This paper investigated how Japanese learners of English discriminated the American English vowel /I/ from /E/ in /CVt/ monosyllables with 23 different initial consonants, and how the differing discrimination was related to the acoustic characteristics of the English vowels compared with those of Japanese vowels /i/ and /e/ in disyllables with a final syllable of /to/. The results showed that the overall error rate of /I/ was significantly higher than that of /E/. This difference between the two vowels in discrimination could be accounted for by the acoustically closer relations of /I/ to the Japanese vowel /e/. It was found, however, that the discrimination of the two English vowels varied across the consonantal contexts. The error rate was significantly higher when the initial consonant was /tS/, /D/ or /j/ for /I/, and when it was /r/ for /E/. An acoustical analysis indicated that the greater decrease of the ratio of the F2 frequency to the F1 frequency of /I/ through the vowel, rather than the acoustical closeness of /I/ to the Japanese /e/, tended to be more closely related with the poorer discrimination of this vowel in the particular consonantal contexts.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-693"
  },
  "zhao00e_icslp": {
   "authors": [
    [
     "Li",
     "Zhao"
    ],
    [
     "Wei",
     "Lu"
    ],
    [
     "Ye",
     "Jiang"
    ],
    [
     "Zhenyang",
     "Wu"
    ]
   ],
   "title": "A study on emotional feature recognition in speech",
   "original": "i00_3961",
   "page_count": 4,
   "order": 696,
   "p1": "vol. 3, 961-964",
   "pn": "",
   "abstract": [
    "This paper analysis the feature of the time amplitude pitch and formant construction involved such four emotions as happiness anger surprise and sorrow. Through comparison with non-emotional speech signal, we sum up the distribution law of emotional feature including different emotional speech. Nine emotional features were extracted from emotional speech for recognizing emotion. We introduce three emotional recognition methods based on principal component analysis and the results show that these method can provide an effective solution to emotional recognition.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-694"
  },
  "godinollorente00_icslp": {
   "authors": [
    [
     "Juan I.",
     "Godino-Llorente"
    ],
    [
     "Santiago",
     "Aguilera-Navarro"
    ],
    [
     "Pedro",
     "Gómez-Vilda"
    ]
   ],
   "title": "LPC, LPCC and MFCC parameterisation applied to the detection of voice impairments",
   "original": "i00_3965",
   "page_count": 5,
   "order": 697,
   "p1": "vol. 3, 965-968",
   "pn": "",
   "abstract": [
    "There is an increased risk for vocal and voice diseases due to the modern way of life. It is well known that most of the vocal and voice diseases cause changes in the acoustic voice signal. These diseases have to be diagnosed and treated during an early stage. Acoustic analysis is a non-invasive technique based on digital processing of speech signal. Acoustic analysis can be a useful tool to diagnose this kind of diseases, furthermore it presents several advantages: it is a non-invasive tool, an objective diagnostic and, also, it can be used for the evaluation of surgical and pharmacological treatments and rehabilitation processes. ENT clinicians use acoustic voice analysis to characterise pathological voices. In this paper, we study threee well known parameterisation approaches applied to the automatic detection of voice disorders. Former and actual works demonstrate that impaired voice detection can be carried out by means of supervised neural nets: MLP (Multilayer perceptron). We have focused our task in detection of impaired voices by means of neural network technology (ANN) and parameters such a LPC, LPCC and MFCC extracted from the voice signal. The performance of the neural network based detector is compared with that using acoustic parameters such a Fo, NHR, NNE, Shimmer, Jitter... as input variables. The aim of this paper is to study and compare those widely used parameterisation method in speech technology applied to the detection of impaired voices.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-695"
  },
  "tsou00_icslp": {
   "authors": [
    [
     "Benjamin K.",
     "T'sou"
    ],
    [
     "Tom B. Y.",
     "Lai"
    ]
   ],
   "title": "A complementary approach to computer-aided transcription: synergy of statistical-based and kbnowledge discovery paradigms",
   "original": "i00_3969",
   "page_count": 4,
   "order": 698,
   "p1": "vol. 3, 969-972",
   "pn": "",
   "abstract": [
    "The recent implementation of legal bilingualism necessitates the development of a Chinese Computer-Aided Transcription (CAl) system to produce Chinese court proceedings conducted in Cantonese. The transcription system converts transcription shorthand codes into Chinese text, i.e., from phonetic to textual representation of the language. Cantonese and Mandarin Chinese have many homophonous characters. The main challenge lies in the resolution of the severe ambiguity of the conversion. N-gram statistical model is incorporated to estimate the most probable character string during conversion. Domain-specific corpora have been compiled to support the statistical computation. With additional enhancement features, the CAT system delivers a transcription accuracy of 96%. An intelligent error detection tool is built into the system to facilitate the manual correction of the remaining errors. Using decision tree algorithm and a range of text and linguistic attributes, the system can effectively alert the users to possible errors.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-696"
  },
  "caraty00_icslp": {
   "authors": [
    [
     "Marie-José",
     "Caraty"
    ],
    [
     "Claude",
     "Montacié"
    ]
   ],
   "title": "Teraspeech2000 : a 10,000 speakers database",
   "original": "i00_3973",
   "page_count": 5,
   "order": 699,
   "p1": "vol. 3, 973-976",
   "pn": "",
   "abstract": [
    "TeraSpeech is a bilingual database (i.e., English and French) developed in partnership with a French museum, le Musée des Sciences et de lIndustrie in Paris. A demonstration of vocal signature is the support of this data collection. Aiming at the validation of a quality plan, a scenario of the demonstration has been designed, and various protocols have been developed. The quality plan is presented as well as the solutions we found for its validation (i.e., scenario and protocols). The statistics of TeraSpeech are given. Three trends are examined for the perspectives : the validation, the exploitation and the research. Over a single year of the vocal signature exhibition, TeraSpeech2000 is a collection of more than 30,000 sentences recorded from more than 10,000 visitors. The exposition on acoustics of the museum is planned for ten years. TeraSpeech is expected to be a collection of more than 100,000 speakers recorded over the same sound acquisition channel.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-697"
  },
  "dybkjr00_icslp": {
   "authors": [
    [
     "Laila",
     "Dybkjær"
    ],
    [
     "Niels Ole",
     "Bernsen"
    ]
   ],
   "title": "The MATE workbench - a tool in support of spoken dialogue annotation and information extraction",
   "original": "i00_3977",
   "page_count": 4,
   "order": 700,
   "p1": "vol. 3, 977-980",
   "pn": "",
   "abstract": [
    "The increasing variety and sophistication of spoken language dialogue systems (SLDSs) emphasises the need for tools in support of their development and evaluation as well as for appropriate evaluation criteria. In this paper we describe how the MATE workbench can be used during SLDSs development to efficiently produce corpus-based information on SLDSs and their components. The information retrieved from the annotated corpora can be used for evaluation purposes and provide important directions for further development. Examples are drawn from dialogue management and human factors of SLDSs.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-698"
  },
  "brun00_icslp": {
   "authors": [
    [
     "Armelle",
     "Brun"
    ],
    [
     "David",
     "Langlois"
    ],
    [
     "Kamel",
     "Smaili"
    ],
    [
     "Jean-Paul",
     "Haton"
    ]
   ],
   "title": "Discarding impossible events from statistical language models",
   "original": "i00_3981",
   "page_count": 4,
   "order": 701,
   "p1": "vol. 3, 981-984",
   "pn": "",
   "abstract": [
    "This paper describes a method for detecting impossible bigrams from a space of V2 bigrams where V is the size of the vocabulary. The idea is to discard all the ungrammatical events which are impossible in a well written text and consequently to expect an improvement of the language model. We expect also, in speech recognition, to reduce the complexity of the search algorithm by making less comparisons. To achieve that, we extract the impossible bigrams by using automatic rules. These rules are based on grammatical classes. The biclass associations which are ungrammatical are detected and all the corresponding bigrams are analyzed and set as possible or impossible events. As, in natural language, grammatical rules can have exceptions, we decided to manage for each of the retrieved rules an exception list.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-699"
  },
  "lepage00_icslp": {
   "authors": [
    [
     "Yves",
     "Lepage"
    ],
    [
     "Nicolas",
     "Auclerc"
    ],
    [
     "Satoshi",
     "Shirai"
    ]
   ],
   "title": "A tool to build a treebank for conversational Chinese",
   "original": "i00_3985",
   "page_count": 4,
   "order": 702,
   "p1": "vol. 3, 985-988",
   "pn": "",
   "abstract": [
    "N-grams have been extensively used with phonemes or words as basic units in speech recognition. Recently, it has been proposed to use n-grams with phrase tree structures as units to increase speech recognition quality. In order to test this idea on Chinese, a treebank of Chinese hotel reservation con- versation utterances is needed. Because no such treebank is yet available, we have to build it. We propose to see the process of building a tree-bank as a sequence of edition and search operations: input or copy a new utterance (edit a text); search for similar existing utterances to get their corresponding structures and adapt them to the new utterance; adapt the structure (edit a tree) ; earch for similar structures to ensure representaion and coding consistency. This way of doing will have a benefic \"snow-ball\" effect: the bigger the treebank, the faster and the more consistent its extension.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-700"
  },
  "auckenthaler00_icslp": {
   "authors": [
    [
     "Roland",
     "Auckenthaler"
    ],
    [
     "Michael",
     "Carey"
    ],
    [
     "John",
     "Maso"
    ]
   ],
   "title": "Parameter reduction in a text-independent speaker verification system",
   "original": "i00_3989",
   "page_count": 4,
   "order": 703,
   "p1": "vol. 3, 989-992",
   "pn": "",
   "abstract": [
    "Different methods for reducing parameters in a Gaussian mixture model (GMM) for text-independent speaker verification are investigated in this paper. The number of parameters is directly related to the memory requirement. Reducing the parameters is important in environments with limited memory resources or limited bandwidth for data transmission.\n",
    "In contrast to standard approaches such as reducing the number of mixture components in the GMM or the dimension of the acoustic space, speaker specific parameters are selected from a global parameter set. Experiments reveal a small performance degradation when only a few parameters are chosen. Reducing the number of parameters to 25% of the original count gives a slightly better performance compared to a four times smaller global parameter set.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-701"
  },
  "gu00d_icslp": {
   "authors": [
    [
     "Yong",
     "Gu"
    ],
    [
     "Trevor",
     "Thomas"
    ]
   ],
   "title": "Advances on HMM-based text-dependent speaker verification",
   "original": "i00_3993",
   "page_count": 4,
   "order": 704,
   "p1": "vol. 3, 993-996",
   "pn": "",
   "abstract": [
    "This paper presents recent development on text-dependent speaker verification technology in EU project PICASSO, which have improved the SV performance significantly. In the project we adopt HMM approach for pattern matching. In the paper we describes four different techniques, adaptive variance flooring, multiple use of enrolment sample, generalised competitive measurement for score normalisation and hybrid method for score normalisation to improve HMM method for speaker verification. The evaluation results are given for each of these techniques.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-702"
  },
  "stapert00_icslp": {
   "authors": [
    [
     "Robert",
     "Stapert"
    ],
    [
     "John S.",
     "Mason"
    ],
    [
     "Roland",
     "Auckenthaler"
    ]
   ],
   "title": "Optimisation of GMM in speaker recognition",
   "original": "i00_3997",
   "page_count": 4,
   "order": 705,
   "p1": "vol. 3, 997-1000",
   "pn": "",
   "abstract": [
    "Given that the amount of speaker specific training data is always limited, for a given amount of data a speaker model has an optimum number of components. Here, this is investigated with regard to Gaussian mixture models (GMM) with and without world model adaption. Test results show that maximising the number of components in a speaker model can improve speaker recognition results. Comparisons with vector quantisation (VQ) indicate that sensible use of out-of-class data is essential for optimising a recognition system.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-703"
  },
  "zilca00_icslp": {
   "authors": [
    [
     "Ran D.",
     "Zilca"
    ],
    [
     "Yuval",
     "Bistritz"
    ]
   ],
   "title": "Distance-based Gaussian mixture model for speaker recognition over the telephone",
   "original": "i00_3a01",
   "page_count": 1,
   "order": 706,
   "p1": "vol. 3, 1001-1004",
   "pn": "",
   "abstract": [
    "The paper considers text independent speaker identification over the telephone using short training and testing data. Gaussian Mixture Modeling (GMM) is used in the testing phase, but the parameters of the model are taken from clusters obtained for the training data by an adequate choice of feature vectors and a distance measure without optimization in the maximum likelihood (ML) sense. This distance-based GMM (DB-GMM) approach was evaluated by experiments in speaker identification from short telephone-speech data for a few feature vectors and distance measures. The selected feature vectors were Line Spectra Pairs (LSP) and Mel Frequency Cepstra (MFC). The selected distance measures were weighted Euclidean distance with IHM and BPL, respectively. DB-GMM showed consistently better performance than GMM trained by the expectationmaximization (EM) algorithm. Another notable observation is that a full covariance GMM (that is more comfortably trained by DB-GMM) always achieved significantly better performance than diagonal covariance GMM.\n",
    "[PDF file damaged on original CD. W.H.]\n",
    ""
   ]
  },
  "liu00i_icslp": {
   "authors": [
    [
     "Jun-Hui",
     "Liu"
    ],
    [
     "Ke",
     "Chen"
    ]
   ],
   "title": "Pruning abnormal data for better making a decision in speaker verification",
   "original": "i00_3a05",
   "page_count": 5,
   "order": 707,
   "p1": "vol. 3, 1005-1008",
   "pn": "",
   "abstract": [
    "In development of a speaker verification system, a priori threshold estimation is often needed based on a training set for decision making. Such a threshold critically determines performance of a speaker verification system. From a statistical viewpoint, a speaker's voice could be modeled by a certain distribution. Thus, data for training are only some samples of this distribution in a subspace, and the statistical information acquired from the training set is usually biased to that of the whole space. In this paper, we propose a method for better estimation of underlying statistics by abnormal data elimination. Without use of more data, our method provides an alternative way to improve performance of those statisticsbased a priori threshold estimation methods in terms of generalization capability. On the basis of a benchmark database, KING, and a baseline system with a priori threshold estimation, we demonstrate the effectiveness of our method.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-704"
  },
  "bosch00_icslp": {
   "authors": [
    [
     "Louis ten",
     "Bosch"
    ]
   ],
   "title": "ASR, dialects, and acoustic/phonological distances",
   "original": "i00_3a09",
   "page_count": 5,
   "order": 708,
   "p1": "vol. 3, 1009-1012",
   "pn": "",
   "abstract": [
    "If the acoustic models in an ASR system have been built using standard pronunciations in the acoustic training database, dialect speakers usually show in a test a lower ASR performance compared to speakers of standard pronunciations. In this paper, this degree of degradation is considered to be a measure for the distance between dialect and standard pronunciation. We relate this ASR-distance with a phonologically based distance between dialect and standard pronunciation. It is concluded that phonological and acoustically based distance measures are in line with each other, but this conclusion is tentative due to the degrees of uncertainty in all measurements. Simple lexical modifications based on phonological knowledge to accommodate with the dialectal pronunciations were only moderately successful.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-705"
  },
  "nishida00_icslp": {
   "authors": [
    [
     "Masafumi",
     "Nishida"
    ],
    [
     "Yasuo",
     "Ariki"
    ]
   ],
   "title": "Speaker verification by integrating dynamic and static features using subspace method",
   "original": "i00_3a13",
   "page_count": 4,
   "order": 709,
   "p1": "vol. 3, 1013-1016",
   "pn": "",
   "abstract": [
    "In speaker recognition, it is a problem that variation of speech features is caused by sentences and time difference. Speech data includes a phonetic information and a speaker information. If they are separated each other, robust speaker verification will be realized by using only the speaker information. However, it is difficult to separate the speaker information from the phonetic information included in speech data at present. From this viewpoint, we propose a speaker verification method using a subspace method based on principal component analysis in order to extract only the speaker information included in speech data. We also propose dynamic and static features of each speaker presented in the speaker eigenspace as well as their integration for robust normalization of speech feature variations. We carried out comparative experiments between the proposed method and conventional GMM to show an effectiveness of our proposed method. As a result, integrated dynamic and static features in speaker eigenspace were shown to be effective for speaker verification.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-706"
  },
  "kim00e_icslp": {
   "authors": [
    [
     "Se-Hyun",
     "Kim"
    ],
    [
     "Gil-Jin",
     "Jang"
    ],
    [
     "Yung-Hwan",
     "Oh"
    ]
   ],
   "title": "Improvement of speaker recognition system by individual information weighting",
   "original": "i00_3a17",
   "page_count": 4,
   "order": 710,
   "p1": "vol. 3, 1017-1020",
   "pn": "",
   "abstract": [
    "In speaker recognition, it is very important to use individual information extracted from speech waves. Most of the speaker recognition methods assume that each part of speech has equal amount of information to represent a speaker, although it di\u000berently contribute to speaker recognition. The aim of this paper is to suggest a new scoring method of the HMM, which applies di\u000berent importance to all the basic portions of a sampled speech waveform. we first define the quantity of the importance of speech frames, propose how to measure it and apply to speaker recognition. The performance of the proposed method was compared to non-weighting HMM based speaker recognition system. In speaker verification experiments, the proposed method reduced equal error rates considerably as compared to a conventional method which treats all speech segments to have the same importance. In speaker identification experiments, the proposed method marked relatively 28% higher recognition rate than the baseline system, and was more robust in long-term variation. These results demonstrate that the proposed method is e\u000ecient in measuring speaker information and more appropriate for speaker recognition.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-707"
  },
  "yoma00b_icslp": {
   "authors": [
    [
     "Néstor Becerra",
     "Yoma"
    ],
    [
     "Tarciano Facco",
     "Pegoraro"
    ]
   ],
   "title": "Speaker verification in noise using temporal constraints",
   "original": "i00_3a21",
   "page_count": 5,
   "order": 711,
   "p1": "vol. 3, 1021-1024",
   "pn": "",
   "abstract": [
    "This paper addresses the problem of state duration modeling in combination with spectral subtraction and Rasta filtering to cancel both additive and convolutional noise in a text-dependent speaker verification task. The results presented in this paper suggest that temporal constraints can lead to reductions of 30 and 14% in the error rates at SNR equal to 0 and 6dB, respectively, without noise canceling techniques. However, with noise canceling methods, temporal restrictions give a lower improvement. The results here shown propose that state duration modeling can be useful in those cases when the noise reduction is low.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-708"
  },
  "sabac00_icslp": {
   "authors": [
    [
     "Bogdan",
     "Sabac"
    ],
    [
     "Inge",
     "Gavat"
    ],
    [
     "Zica",
     "Valsan"
    ]
   ],
   "title": "Speaker identification using discriminative features selection",
   "original": "i00_3a25",
   "page_count": 4,
   "order": 712,
   "p1": "vol. 3, 1025-1028",
   "pn": "",
   "abstract": [
    "A new method of text-dependent speaker identification using discriminative feature selection is proposed in this paper. The characteristics of the proposed method are as follows: feature parameters extraction, vector quantization with the growing neural gas (GNG) algorithm, model building using gaussian distributions and discriminative feature selection (DFS) according to the uniqueness of personal features. The speaker identification algorithm is evaluated on a database that includes 25 speakers each of them recorded in 12 different sessions. All speakers spoke the same phrase for 10 times in each recording session. The test results showed that both FRR (False Rejection Rate) and FAR (False Acceptance Rate) were about 1[%].\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-709"
  },
  "magrinchagnolleau00_icslp": {
   "authors": [
    [
     "Ivan",
     "Magrin-Chagnolleau"
    ],
    [
     "Guilleaume",
     "Gravier"
    ],
    [
     "Mouhamadou",
     "Seck"
    ],
    [
     "Olivier",
     "Boeffard"
    ],
    [
     "R.",
     "Blouet"
    ],
    [
     "Frédéric",
     "Bimbot"
    ]
   ],
   "title": "A further investigation on speech features for speaker characterization",
   "original": "i00_3a29",
   "page_count": 4,
   "order": 713,
   "p1": "vol. 3, 1029-1032",
   "pn": "",
   "abstract": [
    "In this article, we investigate on alternative speech features for speaker characterization. We study Line Spectrum Pairs features, Time-Frequency Principal Components and Discriminant Components of the Spectrum. These alternative features are tested and compared on a task of speaker verification. This task consists in verifying a claimed identity from a speech segment. Systems are evaluated on a subset of the evaluation data of the NIST 1999 speaker recognition campaign. The new speech features are also compared to the classical cepstral coefficients, which remain, in our experiments, the best performing features.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-710"
  },
  "balleda00_icslp": {
   "authors": [
    [
     "Jyotsana",
     "Balleda"
    ],
    [
     "Hema A",
     "Murthy"
    ],
    [
     "T.",
     "Nagarajan"
    ]
   ],
   "title": "Language identification from short segments of speech",
   "original": "i00_3a33",
   "page_count": 4,
   "order": 714,
   "p1": "vol. 3, 1033-1036",
   "pn": "",
   "abstract": [
    "Automatic language identification (LID) from the spo- ken speech utterance is a challenging problem. In this paper, we present an LID system that works for South Indian languages and Hindi. Each language is modeled using an approach based on Vector Quantisation [1]. The speech is segmented into di\u000berent sounds (CVs) and the performance of the system on each of the seg- ments is studied. Our studies indicate that the pres- ence of some CVs is crucial for each language. We al- so find that for the same Consonant and Vowel (CV) combination, the quality of the sound is di\u000berent in di\u000berent languages. We show that once the speech signal is segmented into CVs, it is possible to perfor- m LID on very short segments (100-150ms) of speech itself.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-711"
  },
  "kronenberg00_icslp": {
   "authors": [
    [
     "Susanne",
     "Kronenberg"
    ],
    [
     "Franz",
     "Kummert"
    ]
   ],
   "title": "Generation of utterances based on visual context information",
   "original": "i00_3a37",
   "page_count": 4,
   "order": 715,
   "p1": "vol. 3, 1037-1040",
   "pn": "",
   "abstract": [
    "A major aspect of spontaneous dialogs is resembled by the collaborative process of communication where participants of a dialog cooperate by the production of utterances. Accordingly, not only independent utterances are produced but an utterance started by one agent may be continued by the other one based on the structural properties provided so far by the initial utterance. For establishing collaboration in dialogs the computer has to cooperate with the user in so far that the instructions of the user are resumed and carried on by the simulation model in that an appropriate continuation is generated by the system.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-712"
  },
  "rahim00_icslp": {
   "authors": [
    [
     "Mazin",
     "Rahim"
    ],
    [
     "Roberto",
     "Pieraccini"
    ],
    [
     "Wieland",
     "Eckert"
    ],
    [
     "Esther",
     "Levin"
    ],
    [
     "Giuseppe Di",
     "Fabbrizio"
    ],
    [
     "Giuseppe",
     "Riccardi"
    ],
    [
     "Candy",
     "Kamm"
    ],
    [
     "Shrikanth",
     "Narayanan"
    ]
   ],
   "title": "A spoken dialogue system for conference/workshop services",
   "original": "i00_3a41",
   "page_count": 4,
   "order": 716,
   "p1": "vol. 3, 1041-1044",
   "pn": "",
   "abstract": [
    "This paper describes our progress towards building a telephony-based spoken dialogue system for workshop/conference services. A mixed-initiative dialogue system has been developed that is engineered to o\u000ber users natural interaction with the system, ease-of-use and robustness towards ambiguous requests and machine errors. A prototype system, known as W99, is described in this paper which was deployed in the 1999 IEEE International Workshop on Automatic Speech Recognition and Understanding (ASRU'99), Keystone, Colorado. This system integrates advanced technologies in speech and dialogue design. An evaluation of the W99 system in terms of recognition performance, understanding accuracy and dialogue success rate during the live trial of the system are presented in this paper.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-713"
  },
  "churcher00_icslp": {
   "authors": [
    [
     "Gavin",
     "Churcher"
    ],
    [
     "Peter",
     "Wyard"
    ]
   ],
   "title": "Developing robust, user-centred multimodal spoken language systems: the MUeSLI project",
   "original": "i00_3a45",
   "page_count": 4,
   "order": 717,
   "p1": "vol. 3, 1045-1048",
   "pn": "",
   "abstract": [
    "The Multimodal Spoken Language Interfaces (MUeSLI) project at BTexaCT aims to conduct practical research into developing advanced multimodal spoken dialogue systems with a distinctive user focus. In this paper we intend to convey the main aims and motivation behind the project and to describe the particular research application we have built. We recently conducted a user trial where a fully automatic version of the system was used, allowing us to investigate how users make use of the input modalities and to reveal the current limitations of the system.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-714"
  },
  "johnsen00b_icslp": {
   "authors": [
    [
     "Magne H.",
     "Johnsen"
    ],
    [
     "Torbjørn",
     "Svendsen"
    ],
    [
     "Tore",
     "Amble"
    ],
    [
     "Trym",
     "Holter"
    ],
    [
     "Erik",
     "Harborg"
    ]
   ],
   "title": "TABOR - a norwegian spoken dialogue system for bus travel information",
   "original": "i00_3a49",
   "page_count": 4,
   "order": 718,
   "p1": "vol. 3, 1049-1052",
   "pn": "",
   "abstract": [
    "This paper describes the development and testing of a pilot spoken dialogue system for bus travel information in the city of Trondheim, Norway. The system driven dialogue was designed on the basis of analyzed recordings from both human-human operator dialogues, Wizard-of-Oz (WoZ) dialogues, and a text-based inquiry system for the web. The dialogue system employs a flexible speech recognizer and an utterance concatenation procedure for speech output. Even though the system is intended for research only, it has been accessible through a public phone number since October 1999. During this period all dialogues have been recorded. From these, approximately 350 dialogues were selected for annotation and comparison to 120 dialogues from the WoZ recordings.\n",
    "The experiments showed that the turn error rate was more than twice as large for the real dialogues as for the WoZ calls, i.e., 13.3% versus 5.7%. Thus, the WoZ results did not give a reliable estimate for the true performance. Our experiments indicate that the current flexible speech recognizer should be further optimized.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-715"
  },
  "huang00e_icslp": {
   "authors": [
    [
     "Yinfei",
     "Huang"
    ],
    [
     "Fang",
     "Zheng"
    ],
    [
     "Mingxing",
     "Xu"
    ],
    [
     "Pengju",
     "Yan"
    ],
    [
     "Wenhu",
     "Wu"
    ]
   ],
   "title": "Language understanding component for Chinese dialogue system",
   "original": "i00_3a53",
   "page_count": 5,
   "order": 719,
   "p1": "vol. 3, 1053-1056",
   "pn": "",
   "abstract": [
    "In this paper we present the design and the implement of the language understanding component of a Chinese spoken language dialogue system EasyNav. In pursuing the coherence with the goal of understanding, we design the structure of system with speech decoding and language understanding integrated closely. Thus the language understanding component need to be restrictive besides portable. Actually we implement a general syntactic parser and domain-specific semantic parser for the purpose. The grammar rules written for understanding are suitable for spoken language. The feature of spoken language also exists throughout the system.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-716"
  },
  "aoyama00_icslp": {
   "authors": [
    [
     "Kazumi",
     "Aoyama"
    ],
    [
     "Izumi",
     "Hirano"
    ],
    [
     "Hideaki",
     "Kikuchi"
    ],
    [
     "Katsuhiko",
     "Shirai"
    ]
   ],
   "title": "Designing a domain independent platform of spoken dialogue system",
   "original": "i00_3a57",
   "page_count": 4,
   "order": 720,
   "p1": "vol. 3, 1057-1060",
   "pn": "",
   "abstract": [
    "Our purpose is to construct a domain independent platform for task-oriented spoken dialogue system. It is expected to generalize architecture of spoken dialogue system for making it easy to develop a system that can support and act to solve problems for user by using speech input and output. In this research, we propose a method of describing hierarchically internal knowledge for solving problems and one for controlling dialogue as rules. A system designer only should describe two types of rules those are called problem-solving rules and action-management rules. And we designed and implemented the platform and evaluated how well the rules can express knowledge.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-717"
  },
  "zhou00e_icslp": {
   "authors": [
    [
     "Qiru",
     "Zhou"
    ],
    [
     "Antoine",
     "Saad"
    ],
    [
     "Sherif",
     "Abdou"
    ]
   ],
   "title": "An enhanced BLSTIP dialogue research platform",
   "original": "i00_3a61",
   "page_count": 4,
   "order": 721,
   "p1": "vol. 3, 1061-1064",
   "pn": "",
   "abstract": [
    "This paper presents some recent enhancement to Bell Labs Speech Technology Integration Platform (BLSTIP), a common platform to integrate Bell Labs speech, telephony, Internet, and dialogue technologies for spoken and multi-modal dialogue system research and prototyping. Last year, we introduced BLSTIP to our partners as a speech technology platform for collaborative research and new application development.\n",
    "BLSTIP software is packaged as a single, network downloadable installation file. It supports a variety of speech applications such as natural language call routing/steering for call centers, natural language information system, messaging system with voice user interface, speaker verification, speech application trial and data collection, etc. As an enhancement to BLSTIP, we also designed a VoiceXML (Voice eXtensible Markup Language) integration infrastructure to study emerging web hosted speech applications such as voice portal, multimodal internet access, and wireless internet access.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-718"
  },
  "qu00_icslp": {
   "authors": [
    [
     "Weidong",
     "Qu"
    ],
    [
     "Katsuhiko",
     "Shirai"
    ]
   ],
   "title": "Using machine learning method and subword unit representations for spoken document categorization",
   "original": "i00_3a65",
   "page_count": 4,
   "order": 722,
   "p1": "vol. 3, 1065-1068",
   "pn": "",
   "abstract": [
    "In this paper, we investigate the feasibility of using machine learning method and subword units for spoken document categorization as an alternative to using words generated by word recognition or keyword spotting. An advantage of using subword acoustic unit representations to spoken document categorization is that it does not require prior knowledge about the contents of the spoken document and could attack the out of vocabulary (OOV) problem. The context-sensitive learning method is efficient on large, noisy corpora and very suitable for subword-based categorization. Given that even the best phone recognizers make a large number of mistakes, to improve phone N-gram recall, we can once again use phone lattices to obtain the bag of phone N-grams for each speech document. In this study, we examine a variety of subword unit categorization terms and measure their ability to perform effective categorization work, and also have investigated the performance when the underlying phonetic transcriptions contain different recognition errors.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-719"
  },
  "stark00_icslp": {
   "authors": [
    [
     "Litza",
     "Stark"
    ],
    [
     "Steve",
     "Whittaker"
    ],
    [
     "Julia",
     "Hirschberg"
    ]
   ],
   "title": "ASR satisficing: the effects of ASR accuracy on speech retrieval",
   "original": "i00_3a69",
   "page_count": 4,
   "order": 723,
   "p1": "vol. 3, 1069-1072",
   "pn": "",
   "abstract": [
    "We examine how differences in the accuracy of Automatic Speech Recognition transcripts affect users ability to use these in tasks requiring the retrieval of speech \"documents\". We compare performance measures, processing strategies, and preference data for subjects using transcripts and speech data to perform a series of relevance judgment and summary tasks on transcripts with different levels of accuracy. Results show effects for transcript quality on solution accuracy, time to solution, amount of speech played for the task, likelihood of subjects abandoning use of a transcript, and subject perceptions of task difficulty, transcript utility, readability, and comprehensibility.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-720"
  },
  "nishizaki00_icslp": {
   "authors": [
    [
     "Hiromitsu",
     "Nishizaki"
    ],
    [
     "Seiichi",
     "Nakagawa"
    ]
   ],
   "title": "A system for retrieving broadcast news speech documents using voice input keywords and similarity between words",
   "original": "i00_3a73",
   "page_count": 4,
   "order": 724,
   "p1": "vol. 3, 1073-1076",
   "pn": "",
   "abstract": [
    "This paper describes a robust speech documents retrieval system that uses voice input keywords. To solve the in- evitable problems which arise when the input to the system is speech, i.e. misrecognition, a novel method was devel- oped, where, before the retrieval processing, unproductive keyword candidates are discarded by a grouping process- ing using the similarity between words and the recognition score of keywords. In retrieval experiments, we used the proposed method to retrieve Japanese broadcast news doc- uments through voice keywords input to the system and showed its e\u000bectiveness.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-721"
  },
  "lai00b_icslp": {
   "authors": [
    [
     "Yu-Sheng",
     "Lai"
    ],
    [
     "Kuen-Lin",
     "Lee"
    ],
    [
     "Chung-Hsien",
     "Wu"
    ]
   ],
   "title": "Intention extraction and semantic matching for internet FAQ retrieval using spoken language query",
   "original": "i00_3a77",
   "page_count": 4,
   "order": 725,
   "p1": "vol. 3, 1077-1080",
   "pn": "",
   "abstract": [
    "An FAQ (frequently-asked question) pattern consists of a question and a text document that answers the question and contains some additional remarks. As a query is similar to the FAQs question, the FAQs answer gives a possible answer or parts of the answer of the query. On the other hand, an FAQs answer may also contain information not concerning with the corresponding FAQs question but embed the answer for other questions. For a given query, therefore, the answer can be obtained from both FAQ question and answer. In this paper, we propose a framework for Internet FAQ retrieval by using spoken language query. We aim at two points: (1) extraction of the main intention embedded in a query sentence and (2) semantic comparison between a query sentence and an FAQ pattern. To evaluate the system performance, a collection of 1022 FAQ patterns and a set of 185 query sentences are collected for experiment. In intention extraction, 91.9% of intention segments can be extracted correctly. Compared to the keyword-based approach, an improvement from 78.06% to 95.28% in recall rate for the top 10 candidates is obtained.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-722"
  },
  "vark00_icslp": {
   "authors": [
    [
     "Robert J. van",
     "Vark"
    ],
    [
     "Jelle K. de",
     "Haan"
    ],
    [
     "Leon J. M.",
     "Rothkrantz"
    ]
   ],
   "title": "A domain-independent model to improve spelling in a web environment",
   "original": "i00_3a81",
   "page_count": 4,
   "order": 726,
   "p1": "vol. 3, 1081-1084",
   "pn": "",
   "abstract": [
    "Speech controlled web agents provide a way to build cheap telephony-based information services that can be used to provide various types of information available on the Internet. High recognition error rates make it difficult to find correct information on the web. To find telephone numbers of employees on the Delft University of Technology web site, a search engine capable of finding \"fuzzy matches\" results in a list of close matches to the recognised spelled last name of the employee. Using knowledge of the errors made by the speech recogniser makes it possible to locate phone numbers even if the recognition of spelled last names is very low.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-723"
  },
  "takao00_icslp": {
   "authors": [
    [
     "Seiichi",
     "Takao"
    ],
    [
     "Jun",
     "Ogata"
    ],
    [
     "Yasuo",
     "Ariki"
    ]
   ],
   "title": "Expanded vector space model based on word space in cross media retrieval of news speech data",
   "original": "i00_3a85",
   "page_count": 4,
   "order": 727,
   "p1": "vol. 3, 1085-1088",
   "pn": "",
   "abstract": [
    "News On Demand System using speech technology usually employs automatic speech transcriptions to retrieve the news data. In the retrieval, users specify a few keywords or sentences as a query and the related news data can be retrieved using the speech transcription. However when users cant give a query clearly, a video shot of news program which users are watching will become a good query to retrieve the related news data. As one of such kinds of news data retrieval, we propose here to employ video captions as a query and to retrieve the related news data using speech transcription. We call this kind of retrieval as cross media retrieval due to its media cross over. Conventionally available method in cross media retrieval is standard cosine measure in vector space model. In this conventional method, there is a problem of impossibility of semantic level retrieval. To solve this problem, we propose here an expanded vector space model based on a word space. Experimental results found that the expanded vector space model based on the word space has superiority to the conventional vector space model.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-724"
  },
  "hansen00b_icslp": {
   "authors": [
    [
     "John H. L.",
     "Hansen"
    ],
    [
     "Bowen",
     "Zhou"
    ],
    [
     "Murat",
     "Akbacak"
    ],
    [
     "Ruhi",
     "Sarikaya"
    ],
    [
     "Bryan",
     "Pellom"
    ]
   ],
   "title": "Audio stream phrase recognition for a national gallery of the spoken word: \"one small step\"",
   "original": "i00_3a89",
   "page_count": 4,
   "order": 728,
   "p1": "vol. 3, 1089-1092",
   "pn": "",
   "abstract": [
    "In this paper, we introduce the problem of audio stream phrase recognition for information retrieval for a new National Gallery of the Spoken Word (NGSW). This will be the first large-scale repository of its kind, consisting of speeches, news broadcasts, and recordings that are of historical content from the 20th Century. We propose a system diagram and discuss critical processing tasks such as: an environment classifier, recognizer model adaptation for acoustic background noise, restricted channels, and speaker variability, natural language processor, and speech enhancement/feature processing. A probe NGSW data set is used to perform experiments using SPHINX-III LVCSR and a previously formulated RSPL-keyword spotting system. Results are reported for WSJ, BN, and NGSW corpora. Results from sub-system evaluations are reported for (i) model adaptation based on mixture weight adjustment with MLLR (reduces WER by 2.6% over a baseline BN trained model), speaker and environmental turn taking using a Bayesian Information Criterion (BIC), and statistical analysis of phrase recognition performance for confidence measure scoring. Finally, we discuss a number of research challenges needed to address the overall task of robust phrase searching in unrestricted corpora.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-725"
  },
  "nakajima00_icslp": {
   "authors": [
    [
     "Hideharu",
     "Nakajima"
    ],
    [
     "Yoshinori",
     "Sagisaka"
    ],
    [
     "Hirofumi",
     "Yamamoto"
    ]
   ],
   "title": "Pronunciation variants description using recognition error modeling with phonetic derivation hypotheses",
   "original": "i00_3a93",
   "page_count": 4,
   "order": 729,
   "p1": "vol. 3, 1093-1096",
   "pn": "",
   "abstract": [
    "This paper proposes a new method of pronunciation variant generation for reducing word error rate in conversational speech recognition. In particular, this paper focuses on the generation of alternative pronunciations from canonical forms by using the phonological knowledge derived from the analysis of a phonetic transcription corpus. The experimental results show that the pronunciation variation generated by the proposed method provides slightly better performance than a method based on manually written pronunciation. These results also demonstrate the applicability of phonological knowledge-based generation of pronunciation variation.\n",
    "Keywords: speech variants, multiple pronunciation generation, phonological knowledge, corpus based approach\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-726"
  },
  "tsukahara00_icslp": {
   "authors": [
    [
     "Wataru",
     "Tsukahara"
    ],
    [
     "Nigel",
     "Ward"
    ]
   ],
   "title": "Evaluating responsiveness in spoken dialog systems",
   "original": "i00_3a97",
   "page_count": 4,
   "order": 730,
   "p1": "vol. 3, 1097-1100",
   "pn": "",
   "abstract": [
    "Ratings of user satisfaction, although fairly easy to elicit for today's spoken language systems, can be more elusive for systems which operate at near-human levels of performance. This problem can be alleviated by adding a 're-listening' phase before eliciting judgenients: in this phase the user listens to a recording of himself interacting with the system while consulting a transcript of that interaction This technique allows more sensitive judgements of system quality by avoiding problems arising from attention limits.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-727"
  },
  "kitawaki00_icslp": {
   "authors": [
    [
     "Nobuhiko",
     "Kitawaki"
    ],
    [
     "Futoshi",
     "Asano"
    ],
    [
     "Takeshi",
     "Yamada"
    ]
   ],
   "title": "Characteristics of spoken language required for objective quality evaluation of echo cancellers",
   "original": "i00_3b01",
   "page_count": 5,
   "order": 731,
   "p1": "vol. 3, 1101-1104",
   "pn": "",
   "abstract": [
    "Performance of the echo canceller can be conventionally measured by putting white Gaussian noise into the echo canceller system. However, white Gaussian noise is not adequate as the test signal, since the performance may depend on the characteristics of input test signal, and the characteristics of the white Gaussian noise differ from those of real spoken language. This paper describes characteristics of spoken language required for objective quality evaluation of echo cancellers.\n",
    "Following test signals having various time and frequency characteristics of spoken language are examined: white Gaussian noise, frequency weighted Gaussian noise, artificial voice, composite source signal, and real voice.\n",
    "It is concluded that artificial voice having average characteristics of spoken language in time and frequency domain is satisfied for objective quality evaluation of echo cancellers other than conventional white Gaussian noise, frequency weighted white Gaussian noise, and composite source signal.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-728"
  },
  "sugaya00_icslp": {
   "authors": [
    [
     "Fumiaki",
     "Sugaya"
    ],
    [
     "Toshiyuki",
     "Takezawa"
    ],
    [
     "Akio",
     "Yokoo"
    ],
    [
     "Yoshinori",
     "Sagisaka"
    ],
    [
     "Seiichi",
     "Yamamoto"
    ]
   ],
   "title": "Evaluation of the ATR-matrix speech translation system with a pair comparison method between the system and humans",
   "original": "i00_3b05",
   "page_count": 4,
   "order": 732,
   "p1": "vol. 3, 1105-1108",
   "pn": "",
   "abstract": [
    "The main goal of the present paper is to propose a new scheme for the overall evaluation of a speech translation system that supports the design of target application systems and determines their performance. Evaluations are conducted on the Japanese-to- English ATR-MATRIX speech translation system, which was developed at ATR Interpreting Telecommunications Research Laboratories. In the proposed scheme, the system is compared with native Japanese taking the Test of English for International Communication (TOEIC) for speech translation capability. A regression analysis using evaluation results shows that the speech translation capability of ATR-MATRIX matches Japanese scoring around 500 on the TOEIC.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-729"
  },
  "maruyama00_icslp": {
   "authors": [
    [
     "Ichiro",
     "Maruyama"
    ],
    [
     "Yoshiharu",
     "Abe"
    ],
    [
     "Terumasa",
     "Ehara"
    ],
    [
     "Katsuhiko",
     "Shirai"
    ]
   ],
   "title": "An automatic timing detection method for superimposing closed captions of TV programs",
   "original": "i00_3b09",
   "page_count": 4,
   "order": 733,
   "p1": "vol. 3, 1109-1112",
   "pn": "",
   "abstract": [
    "This paper describes a way of automatically detecting the timing for the superimposition of closed captions on TV programs that use electronic manuscripts. Speech is discriminated from music to decrease the number of false alarms and the amount of speech data within which a search must be carried out. Timing is then detected serially from the first sentence to the last sentence of a program by using a phonetically HMM-based word spotter. After all timing has been determined, it is checked for errors and revised. Detection rates of 96.9% and 98.5% were obtained for allowable timing errors of one and three seconds on the aired documentary programs.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-730"
  },
  "ogner00_icslp": {
   "authors": [
    [
     "Marcel",
     "Ogner"
    ],
    [
     "Zdravko",
     "Kacic"
    ]
   ],
   "title": "Normalized time-frequency speech representation in articulation training systems",
   "original": "i00_3b13",
   "page_count": 4,
   "order": 734,
   "p1": "vol. 3, 1113-1116",
   "pn": "",
   "abstract": [
    "The aim of the work described in this paper is to develop and evaluate the speaker normalization technique based on the test to reference speaker mapping. The method is suitable for uniform time-frequency representation of speech used in speech corrector systems.\n",
    "The normalized spectrum is generated after the analysis by synthesis for the given utterance using the MBE (multiband excitation) coding. The MBE speech production model decomposes the short time spectrum into the spectral envelope and excitation spectrum. The model offers the convenient way for joint vocal tract and excitation characteristics mapping to the reference speaker and at the same time preserving the phonetically relevant information in the test speaker utterance.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-731"
  },
  "torihara00_icslp": {
   "authors": [
    [
     "Shinichi",
     "Torihara"
    ],
    [
     "Katashi",
     "Nagao"
    ]
   ],
   "title": "Semantic transcoding: making the handicapped and the aged free from their barriers in obtaining information on the web",
   "original": "i00_3b17",
   "page_count": 4,
   "order": 735,
   "p1": "vol. 3, 1117-1120",
   "pn": "",
   "abstract": [
    "Transcoding refers to the process of converting HTML files in order to adapt the physical attributes of each device. We call our transcoding \"Semantic Transcoding\" because it transcodes at the deep semantic level. The proxy server consists of translation, text summarization, video summarization, text-to-speech, speech recognition, and so on. External annotation is also possible. Semantic transcoding enables the visually challenged to browse Web pages by changing text to speech, and to understand the pictures by reading external annotated comments. The hearing impaired are provided with superimposed dialog generated by the recognition of speech and scene. Furthermore, we will propose a rapid \"diagonal\" reading method by listening to speech converted from text on the basis of linguistic information such as syntax, new and old information for the blind and visually impaired in the internet speech browser realized by semantic transcoding.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-732"
  },
  "chengalvarayan00e_icslp": {
   "authors": [
    [
     "Rathinavelu",
     "Chengalvarayan"
    ]
   ],
   "title": "The use of nonlinear energy transformation for Tamil connected-digit speech recognition",
   "original": "i00_3b21",
   "page_count": 4,
   "order": 736,
   "p1": "vol. 3, 1121-1124",
   "pn": "",
   "abstract": [
    "Generally, the input feature to the recognizer used for recognition and modeling has been extended to include dynamic information about the first and second order derivatives of the cepstral features, energy as well as the information about the cepstrum and the peak normalized energy. The problem with energy normalization approach is that it is not suitable for real-time application since it introduces long delays in determining the peak energy. In this paper, we propose a more efficient implementation approach for energy feature transformation where the energy feature is mapped into a scale of 0 to 1 using a sigmoid function and hence avoiding the need for energy normalization. The experimental results on Tamil connected digit recognition task show that a 20% string error rate reduction is obtained by using the proposed nonlinear energy transformation scheme when compared to using untransformed raw energy feature.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-733"
  },
  "chen00k_icslp": {
   "authors": [
    [
     "Aimin",
     "Chen"
    ],
    [
     "Saeed",
     "Vaseghi"
    ]
   ],
   "title": "State based sub-band Wiener filters for speech enhancement in car environments",
   "original": "i00_3b25",
   "page_count": 5,
   "order": 737,
   "p1": "vol. 3, 1125-1128",
   "pn": "",
   "abstract": [
    "The performance of Wiener filters in restoring the quality and intelligibility of noisy speech depends on: (i) the accuracy of the estimates of the power spectra or the correlation values of the noise and the speech processes, and (ii) on the Wiener filter structure. In this paper a Bayesian method is proposed where model combination and model decomposition are employed for the estimation of parameters required to implement subband Wiener filters. The use of subband Wiener filters provides advantages in terms of improved parameter estimates and also in restoring the temporal-spectral composition of speech. The method is evaluated, and compared with the parallel model combination, using the TIMIT continuous speech database with BMW and VOLVO car noise databases.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-734"
  },
  "hermus00b_icslp": {
   "authors": [
    [
     "Kris",
     "Hermus"
    ],
    [
     "Werner",
     "Verhelst"
    ],
    [
     "Patrick",
     "Wambacq"
    ],
    [
     "Philippe",
     "Lemmerling"
    ]
   ],
   "title": "Total least squares based subband modelling for scalable speech representations with damped sinusoids",
   "original": "i00_3b29",
   "page_count": 4,
   "order": 738,
   "p1": "vol. 3, 1129-1132",
   "pn": "",
   "abstract": [
    "We describe how Total Least Squares (TLS) algorithms can be applied as a powerful and eÆcient modelling tool for wideband speech. A detailed description in both time domain and fre- quency domain illustrates how the modelling functions { damped sinusoids { naturally synthesise non-stationary signals. Straightforward implementations of TLS applied to fullband speech are known to be computationally hard and they can suffer from numerical sensitivity.\n",
    "In this paper we introduce a subband approach, which leads to a significant reduction of the computational load with an enhanced numerical stability. Moreover, it enables to control the distribution of the TLS components over the spectral range of the input signal such that perceptual criteria can be incorporated in the modelling scheme.\n",
    "We also address the scalability of our design from smallband speech to high quality audio, and provide evidence for the existence of coupled components in TLS modelled segments.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-735"
  },
  "chang00b_icslp": {
   "authors": [
    [
     "Joon-Hyuk",
     "Chang"
    ],
    [
     "Nam Soo",
     "Kim"
    ]
   ],
   "title": "Speech enhancement: new approaches to soft decision",
   "original": "i00_3b33",
   "page_count": 4,
   "order": 739,
   "p1": "vol. 3, 1133-1136",
   "pn": "",
   "abstract": [
    "In this paper, we propose new approaches to speech enhancement based on soft decision. In order to enhance the statistical reliability in estimating speech activity, we introduce the concept of a global speech absence probability (GSAP). First, we compute the conventional speech absence probability (SAP) and then modify it according to the newly proposed GSAP. Moreover, for improving the performance of the SAPs at voice tails (transition periods from speech to silence), we revise the SAPs using a hang-over scheme based on hidden Markov model (HMM).\n",
    "In addition, we suggest a robust noise update algorithm in which the noise power is estimated not only in the periods of speech absence but also during speech activity by noise and speech spectrum estimation based on soft decision. Also, for improving the SAP determination and noise update routine we present a new signal to noise ratio (SNR) concept which is called the predicted SNR in this paper. The prediced SNR is defined by the ratio between estimated speech and noise spectrum makes a further improvement the discrete cosine transform (DCT). Results from the test show that the proposed algorithm which is called the speech enhancement based on soft decision (SESD) yields better performance than the conventional methods.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-736"
  },
  "glass00_icslp": {
   "authors": [
    [
     "James",
     "Glass"
    ],
    [
     "Joseph",
     "Polifroni"
    ],
    [
     "Stephanie",
     "Seneff"
    ],
    [
     "Victor",
     "Zue"
    ]
   ],
   "title": "Data collection and performance evaluation of spoken dialogue systems: the MIT experience",
   "original": "i00_4001",
   "page_count": 4,
   "order": 740,
   "p1": "vol. 4, 1-4",
   "pn": "",
   "abstract": [
    "In this paper we report our efforts in data collection and performance evaluation in support of spoken dialogue system development. We describe two understanding metrics called query density and concept efficiency which can be interpreted on a perutterance basis, but which are measured over the course of a dialogue. We also describe the evaluation infrastructure we have developed to support off-line data processing using our GALAXY client-server architecture [8]. We show how we have used these metrics and mechanisms as part of the development of a spoken dialogue system for air-travel information.\n",
    "",
    "",
    "S. Seneff, E. Hurley, R. Lau, C. Pao, P. Schmid, and V. Zue, \"GALAXY-II: A reference architecture for conversational system development,\" Proc. ICSLP, 931-934, Sydney, 1998.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-737"
  },
  "lamel00_icslp": {
   "authors": [
    [
     "Lori",
     "Lamel"
    ],
    [
     "Sophie",
     "Rosset"
    ],
    [
     "Jean-Luc",
     "Gauvain"
    ]
   ],
   "title": "Considerations in the design and evaluation of spoken language dialog systems",
   "original": "i00_4005",
   "page_count": 4,
   "order": 741,
   "p1": "vol. 4, 5-8",
   "pn": "",
   "abstract": [
    "In this paper we summarize our experience at LIMSI in the design, development and evaluation of spoken language dialog systems for information retrieval tasks. This work has been for the most part carried out in the context of several European and international projects. Evaluation plays an integral role in the development of spoken language dialog systems. While there are commonly used measures and methodologies for evaluating speech recognizers, the evaluation of spoken dialog systems is considerably more complicated due to the interactive nature and the human perception of performance. It is therefore important to assess not only the individual system components, but the overall system performance using objective and subjective measures.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-738"
  },
  "heckmann00_icslp": {
   "authors": [
    [
     "Martin",
     "Heckmann"
    ],
    [
     "Frédéric",
     "Berthommier"
    ],
    [
     "Christophe",
     "Savario"
    ],
    [
     "Kristian",
     "Kroschel"
    ]
   ],
   "title": "Labeling audio-visual speech corpora and training an ANN/HMM audio-visual speech recognition system",
   "original": "i00_4009",
   "page_count": 4,
   "order": 742,
   "p1": "vol. 4, 9-12",
   "pn": "",
   "abstract": [
    "We present a method to label an audio-visual database and to setup a system for audio-visual speech recognition based on a hybrid Artificial Neural Network/Hidden Markov Model (ANN/HMM) approach.\n",
    "The multi-stage labeling process is presented on a new audiovisual database recorded at the Institute de la Communication Parlée (ICP). The database was generated via transposition of the audio database NUMBERS95. For the labeling first a large subset of NUMBERS95 is used to achieve a bootstrap training of an ANN, which can then be employed to label the audio part of the audio-visual database. This initial labeling is further improved via readapting the ANN to the new database and reperforming the labeling. From the audio labeling then the video labeling is derived.\n",
    "Tests at different Signal to Noise Ratios (SNR) are performed to demonstrate the efficiency of the labeling process. Furthermore ways to incorporate information from a large audio database into the final audio-visual recognition system were investigated.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-739"
  },
  "li00k_icslp": {
   "authors": [
    [
     "Aijun",
     "Li"
    ],
    [
     "Maocan",
     "Lin"
    ],
    [
     "XiaoXia",
     "Chen"
    ],
    [
     "Yiqing",
     "Zu"
    ],
    [
     "Guohua",
     "Sun"
    ],
    [
     "Wu",
     "Hua"
    ],
    [
     "Zhigang",
     "Yin"
    ],
    [
     "Jingzhu",
     "Yan"
    ]
   ],
   "title": "Speech corpus of Chinese discourse and the phonetic research",
   "original": "i00_4013",
   "page_count": 7,
   "order": 743,
   "p1": "vol. 4, 13-18",
   "pn": "",
   "abstract": [
    "Speech corpus of Chinese discourse (ASCCD) was setup and annotated on segmental and prosodic and syntactic tiers. SAMPA-C and C-ToBI conventions are used for segmental and prosodic labeling. Sound variation such as assimilation, insertion and deletion are investigated on the labeled database. The prosodic research focuses on the sentence stress that involves the specification of relative prominence in prosodic structure,\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-740"
  },
  "fiscus00_icslp": {
   "authors": [
    [
     "Jonathan G.",
     "Fiscus"
    ],
    [
     "George R.",
     "Doddington"
    ]
   ],
   "title": "Results of the 1999 topic detection and tracking evaluation in Mandarin and English",
   "original": "i00_4019",
   "page_count": 6,
   "order": 744,
   "p1": "vol. 4, 19-24",
   "pn": "",
   "abstract": [
    "The National Institute of Standards and Technology (NIST) administered the second open evaluation of Topic Detection and Tracking (TDT) technologies in 1999. The TDT project supports development of technologies that automatically organize event-related news stories. The program leverages expertise in core technologies, Automatic Speech Recognition (ASR), Document Retrieval (DR), and Machine Translation (MT) to build the TDT technologies.\n",
    "The 1999 TDT project extended the 1998 TDT project in two dimensions, first by adding Mandarin Chinese audio and text sources and second by adding two new evaluation tasks. Through experimental controls and conditioned analysis of system performance, the 1999 evaluation yielded numerous insights into the effects of multilingual texts on TDT technologies. Three notable generalizations arise from the evaluation: (1) English and Mandarin story segmentation performance is similar, (2) cross-lingual topic tracking performance is 44% worse than monolingual tracking, and (3) multilingual topic detection performance is 37% worse than monolingual topic detection.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-741"
  },
  "nakamura00b_icslp": {
   "authors": [
    [
     "Satoshi",
     "Nakamura"
    ],
    [
     "Keiko",
     "Watanuki"
    ],
    [
     "Toshiyuki",
     "Takezawa"
    ],
    [
     "Satoru",
     "Hayamizu"
    ]
   ],
   "title": "Multimodal corpora for human-machine interaction research",
   "original": "i00_4025",
   "page_count": 5,
   "order": 745,
   "p1": "vol. 4, 25-28",
   "pn": "",
   "abstract": [
    "In recent years human-machine interaction has increased its importance. One approach to an ideal human-machine interaction is develop a multi-modal system behaves like human-beings. This paper introduces an overview on multimodal corpora which are currently developed in Japan for the purpose. The paper describes database of 1)Multi-modal interaction, 2)Audio-visual speech, 3)Spoken dialogue with multiple speakers, 4)Gesture of sign language and 5)Sound scene data in real acoustic environments.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-742"
  },
  "pearce00_icslp": {
   "authors": [
    [
     "David",
     "Pearce"
    ],
    [
     "Hans-Günter",
     "Hirsch"
    ]
   ],
   "title": "The aurora experimental framework for the performance evaluation of speech recognition systems under noisy conditions",
   "original": "i00_4029",
   "page_count": 4,
   "order": 746,
   "p1": "vol. 4, 29-32",
   "pn": "",
   "abstract": [
    "This paper describes a database designed to evaluate the performance of speech recognition algorithms in noisy conditions. The database may either be used to measure frontend feature extraction algorithms, using a defined HMM recognition back-end, or complete recognition systems. The source speech for this database is the TIdigits, consisting of connected digits task spoken by American English talkers (downsampled to 8kHz). A selection of 8 different real-world noises have been added to the speech over a range of signal to noise ratios with controlled filtering of the speech and noise.\n",
    "The framework was prepared as a contribution to the ETSI STQ-AURORA DSR Working Group[1]. Aurora is developing standards for Distributed Speech Recognition (DSR) where the speech analysis is done in the telecommunication terminal and the recognition at a central location in the telecom network. The framework is currently being used to evaluate alternative proposals for front-end feature extraction. The database has been made publicly available through ELRA so that other speech researchers to evaluate and compare the performance of noise robust algorithms.\n",
    "Recognition results will be presented for the first standard DSR feature extraction scheme based on a cepstral analysis.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-743"
  },
  "tillmann00b_icslp": {
   "authors": [
    [
     "Hans-Günther",
     "Tillmann"
    ],
    [
     "Florian",
     "Schiel"
    ],
    [
     "Christoph",
     "Draxler"
    ],
    [
     "Phil",
     "Hoole"
    ]
   ],
   "title": "The bavarian archive for speech signals - serving the speech community",
   "original": "i00_4033",
   "page_count": 4,
   "order": 747,
   "p1": "vol. 4, 33-36",
   "pn": "",
   "abstract": [
    "The Bavarian Archive for Speech Signals (BAS) is a joint initiative of the Bavarian State and the Ludwig Maximilians Universität München. It is located at the host organisation Institut für Phonetik und Sprachliche Kommunikation and it collects, evaluates, produces and disseminates speech based resources to the scientific community. Our focus is the German language covering a large geographical part of central Europe.\n",
    "This paper gives an concise overview about the BAS activities during the first 5 years of its existence.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-744"
  },
  "millar00_icslp": {
   "authors": [
    [
     "J. Bruce",
     "Millar"
    ]
   ],
   "title": "The development of spoken language resources in oceania",
   "original": "i00_4037",
   "page_count": 4,
   "order": 748,
   "p1": "vol. 4, 37-40",
   "pn": "",
   "abstract": [
    "This paper examines the issues that surround the task of creating spoken language resources in the region of Oceania. The geographical extent and fragmented landmass, the linguistic complexity and the economic diversity are presented as highly significant influential features. The parallel interests of linguistic analysts and language technologists are acknowledged and the scope for synergistic collaboration is promoted. Current initiatives are described, prospective initiatives are discussed, and some potentially helpful enabling areas of study are proposed. The benefits of such development to the region is seen in scientific, economic and cultural terms.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-745"
  },
  "soong00_icslp": {
   "authors": [
    [
     "Frank K.",
     "Soong"
    ],
    [
     "Eric A.",
     "Woudenberg"
    ]
   ],
   "title": "Hands-free human-machine dialogue - corpora, technology and evaluation",
   "original": "i00_4041",
   "page_count": 4,
   "order": 749,
   "p1": "vol. 4, 41-44",
   "pn": "",
   "abstract": [
    "In this paper we will review the progress of hands-free, Voice User Interface (VUI) research work at Bell Labs, including: a multichannel data base collection, technology development, and performance evaluation. Thirty-channel, simultaneous recordings have been conducted in a moving car, collecting speech from 57 subjects under various weather, road, and noise conditions. These are being used for both testing and adaptation purposes. Technology issues relevant to hands-free VUI are specifically addressed, including: (1) acoustic echo cancellation (AEC) and near-end (user's) speech detection; (2) background noise estimation and suppression; (3) reliable and timely barge-in; (4) signal pickup improvement using intelligent microphone arrangements; and (5) speaker and environment adaptation. An evaluation of the developed technologies using the car database is presented. An all software, hands-free, full duplex voice user interface demo has been implemented on a LINUX PC. The real-time demo provides services like: voice-dialing (dialing a person by name or a connected digit string), information service (accessing headline news, weather reports, sports and stock quotations), personal message service (retrieving email, voice mail and fax) and voice control of a DVD-player (selecting topics, controlling volume and video playback speeds).\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-746"
  },
  "riccardi00_icslp": {
   "authors": [
    [
     "Giuseppe",
     "Riccardi"
    ]
   ],
   "title": "On-line learning of acoustic and lexical units for domain-independent ASR",
   "original": "i00_4045",
   "page_count": 4,
   "order": 750,
   "p1": "vol. 4, 45-48",
   "pn": "",
   "abstract": [
    "We are interested in on-line acquisition of acoustic, lexical and semantic units from spontaneous speech. Traditional ASR techniques require the domain-specific knowledge of acoustic, lexicon data and more importantly the word probability distributions. In this paper we propose an algorithm for unsupervised learning of acoustic and lexical units from out-of-domain speech data. The new lexical units are used for fast adaptation of language model probabilities to a new domain. We show that starting from the Switchboard corpus (lexicon and language model) we learn the most relevant language statistics of the \"How May I Help?\" task.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-747"
  },
  "akiba00_icslp": {
   "authors": [
    [
     "Tomoyosi",
     "Akiba"
    ],
    [
     "Katsunobu",
     "Itou"
    ]
   ],
   "title": "Semi-automatic language model acquisition without large corpora",
   "original": "i00_4049",
   "page_count": 4,
   "order": 751,
   "p1": "vol. 4, 49-52",
   "pn": "",
   "abstract": [
    "In this paper, we discuss a methodology for the development of a language model for speech recognition, and introduce a semi-automatic method of acquiring a language model, which does not require large corpora.\n",
    "Statistical language models have gained a reputation as providing the overall performance for speech recognition, and so widely used in speech recognition systems today. The tasks to which statistical language models can be applied are, however, limited, because a large corpus is essential for the building of a statistical model, and the collection of a new corpus is a very costly task in terms of time and effort. Thus, if our aim is to apply speech recognition to various tasks as required, we need a way of developing a new language model for a given task at a reasonable cost.\n",
    "On the other hand, our new method is structured so that it can attempt to acquire language models from various knowledge resources. Each knowledge resource makes its own contribution to the acquired language model. For example, novice users may specify sequences of words that are and are not sentences. Experts can specify the constituents that makes a sentence, that is, what is often called grammatical knowledge. Most electronic dictionaries available today carry information about words, including part-of-speech, inflection patterns, semantic class, and so on. Of course, a corpus is considered as one of knowledge resources. In addition, we must consider about speech recognition systems; the acquired language model should be used by them.\n",
    "To integrate information from such a range of knowledge resources, a uniform representation is essential. In section 2, a specific class of attribute grammars is introduced for this purpose.\n",
    "In section 3, we introduce a semi-automatic method to acquire a Japanese language model for any new task as required. The EDR electronic dictionary [1], an existing electronic dictionary of the Japanese language, and a small set of example sentences which are intended to convey the characteristics of the task, are used instead of a large corpus. Our method is also intended to utilize the knowledge of experts as much as possible.\n",
    "",
    "",
    "Japan Electronic Dictionary Research Institute, Ltd. EDR Electronic Dictionary Technical Guide. TR-042, 1993.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-748"
  },
  "petrovskadelacretaz00_icslp": {
   "authors": [
    [
     "Dijana",
     "Petrovska-Delacrétaz"
    ],
    [
     "Allen L.",
     "Gorin"
    ],
    [
     "Jerry H.",
     "Wright"
    ],
    [
     "Giuseppe",
     "Riccardi"
    ]
   ],
   "title": "Detecting acoustic morphemes in lattices for spoken language understanding",
   "original": "i00_4053",
   "page_count": 4,
   "order": 752,
   "p1": "vol. 4, 53-56",
   "pn": "",
   "abstract": [
    "Current methods for training statistical language models for recognition and understanding require large annotated corpora. The collection, transcription and labeling of such corpora is a major bottleneck for creating new applications and for refinements of existing ones. Thus, it is of great interest to develop methods for automatically learning vocabulary, grammar and semantics from a speech corpus without transcriptions. In this paper we report on an experiment where acoustic morphemes are automatically acquired from the output of a task-independent phone recognizer. The utility of these units is experimentally evaluated for call-type classification in the How may I help you? task. Detected occurrences of the acoustic morphemes in the lattice output provide the basis for the classification of the test sentences. Using lattices, we achieve a reduction of 59% from the false rejection rate using best paths, albeit with a 5% reduction in the correct classification performance from that baseline.\n",
    "Keywords: Spoken language understanding, Salient phrase acquisition, Acoustic morphemes, Phone lattices.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-749"
  },
  "mizumachi00_icslp": {
   "authors": [
    [
     "Mitsunori",
     "Mizumachi"
    ],
    [
     "Masato",
     "Akagi"
    ],
    [
     "Satoshi",
     "Nakamura"
    ]
   ],
   "title": "Design of robust subtractive beamformer for noisy speech recognition",
   "original": "i00_4057",
   "page_count": 4,
   "order": 753,
   "p1": "vol. 4, 57-60",
   "pn": "",
   "abstract": [
    "There is a big demand for noise reduction to enhance ASR robustness. A great variety of noise reduction methods have been proposed, but almost none of them can reduce non-stationary noises. The authers have proposed an algorithm that can reduce noises that former methods, such as adaptive beamformers, find diffcult to deal with. In this paper, the authors verify the proposed method as a front-end for ASR by phoneme recognition tests. Its feasibility is also discussed under reverberant environments.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-750"
  },
  "sheikhzadeh00_icslp": {
   "authors": [
    [
     "Hamid",
     "Sheikhzadeh"
    ],
    [
     "Rassoul",
     "Amirfattahi"
    ]
   ],
   "title": "Objective long-term assessment of speech quality changes in pre-lingual cochlear implant children",
   "original": "i00_4061",
   "page_count": 5,
   "order": 754,
   "p1": "vol. 4, 61-64",
   "pn": "",
   "abstract": [
    "In this research, we have studied the static features of three main Farsi vowels, uttered by four pre-lingual cochlear implant (CI) children. Speech samples have been studied before the CI operation and every three months, up to nine months postoperation. To assess the effects of the auditory feedback (AF) on the speech quality, patients have spoken with their CI devices in both on and off positions. Quantitative results show that 1) after nine months post-operation, almost all of the static features converge towards their normal values, and 2) the dependency of the static features on the AF decreases with time, which implies that the speech production motor patterns of the patients have been trained in time.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-751"
  },
  "noth00_icslp": {
   "authors": [
    [
     "Elmar",
     "Nöth"
    ],
    [
     "Heinrich",
     "Niemann"
    ],
    [
     "Tino",
     "Haderlein"
    ],
    [
     "M.",
     "Decher"
    ],
    [
     "Uwe",
     "Eysholdt"
    ],
    [
     "F.",
     "Rosanowski"
    ],
    [
     "T.",
     "Wittenberg"
    ]
   ],
   "title": "Automatic stuttering recognition using hidden Markov models",
   "original": "i00_4065",
   "page_count": 4,
   "order": 755,
   "p1": "vol. 4, 65-68",
   "pn": "",
   "abstract": [
    "This paper describes the combination of the work of speech therapists and speech recognition systems. Our long term goal is to evaluate the degree of stuttering during therapy and to use the automatic analysis of stuttered speech as a screening method, e.g. the search for potential stutterers at an early age. The approach is to have a patient read a standard text aloud and then automatically count the unfluent parts and classify them. The text to be read by the patients is automatically transformed into a formal grammar that considers potential dysfluencies caused by stuttering.\n",
    "Recordings from stutterers were compared to recordings of nonstutterers. Word and phoneme accuracies of the stuttered text in relation to the number of detected dysfluencies showed correlation coe\u000ecients of up to 0.99. Recordings from stutterers contained much more pauses in a wider time range than from nonstutterers, especially in the interval up to 200 milliseconds (factor 10), and between 200 and 500 milliseconds (factor 2). The sum of the durations of all detected pauses and the number of repetitions were set into relation. The results seem reasonable for a distinction between stutterers with many repetitions/short pauses and stutterers with few repetitions/long pauses.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-752"
  },
  "roy00_icslp": {
   "authors": [
    [
     "Deb",
     "Roy"
    ]
   ],
   "title": "Grounded speech communication",
   "original": "i00_4069",
   "page_count": 5,
   "order": 756,
   "p1": "vol. 4, 69-72",
   "pn": "",
   "abstract": [
    "Language is grounded in sensory-motor experience. Grounding connects concepts to the physical world enabling humans to acquire and use words and sentences in context. Currently, machines which process text and spoken language are not grounded in human-like ways. Instead, semantic representations in machines are highly abstract and have meaning only when interpreted by humans. We are interested in developing computational systems which represent words, utterances, and underlying concepts in terms of sensory-motor experiences, leading to richer levels of understanding by machines. Inspired by theories of infant cognition, we present a computational model which learns from untranscribed multisensory input. Acquired words are represented in terms associations between acoustic and visual sensory experience. The system has been tested in a robotic embodiment which supports interactive language learning and understanding. Successful learning has also been demonstrated using infant-directed speech and images.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-753"
  },
  "jun00b_icslp": {
   "authors": [
    [
     "Sun-Ah",
     "Jun"
    ],
    [
     "Mira",
     "Oh"
    ]
   ],
   "title": "Acquisition of second language intonation",
   "original": "i00_4073",
   "page_count": 5,
   "order": 757,
   "p1": "vol. 4, 73-76",
   "pn": "",
   "abstract": [
    "Foreign accents in second language (L2) production are caused by interference from the phonological system and phonetic realization of the speakers first language (L1), including both segmental and prosodic features. This paper examines the intonation structure of Seoul Korean and its realization by American English speakers. Four English speakers of Korean, differing in fluency, and two Korean speakers participated in the experiment. Forty sentences were designed to test the realization of intonation patterns by varying the number of syllables within a word and a sentence, and by varying the conditions for the segment-tone interaction. Results show that, as with segmental data, more advanced L2 speakers produce more native-like intonation patterns and prosodic structure than less advanced speakers. However, although advanced L2 speakers are better at grouping words into phrases, they are not better at producing surface tonal realizations of an accentual phrase than less advanced speakers. This suggests that phonological properties of intonation are acquired earlier than phonetic properties of intonation.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-754"
  },
  "siu00_icslp": {
   "authors": [
    [
     "Man-hung",
     "Siu"
    ],
    [
     "Ka-Ming",
     "Wong"
    ],
    [
     "Man-Yan",
     "Ching"
    ],
    [
     "Mei-Sum",
     "Lau"
    ]
   ],
   "title": "Computer-aided Mandarin pronunciation learning system",
   "original": "i00_4077",
   "page_count": 4,
   "order": 758,
   "p1": "vol. 4, 77-80",
   "pn": "",
   "abstract": [
    "In this era of globalization, learning multiple languages is becoming necessary. Learning to speak a new language involves knowing how to correctly pronounce words. In many cases, corrections to pronunciation mistakes come from language teachers who can typically give students only limited time and attention. With the rapid development of automatic speech recognition (ASR) technologies, computer can now accurately transcribe spoken words.\n",
    "In this paper, we reported our work of using ASR to help students differentiate between comfusible word pairs, commonly known as minimal pairs. Most of the reported work focuses on the phonetic level. However, in Mandarin, we have to deal with the added dimension of tonal confusion where two words differ only by their lexical tones. We found that tone is consistently harder to distinguish as compared to sylable initial or final. Furthermore, the system when tested on native system can achieve an accuracy of over 90%\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-755"
  },
  "mctear00b_icslp": {
   "authors": [
    [
     "Michael",
     "McTear"
    ],
    [
     "Norma",
     "Conn"
    ],
    [
     "Nicola",
     "Phillips"
    ]
   ],
   "title": "Speech recognition software: a tool for people with dyslexia",
   "original": "i00_4081",
   "page_count": 4,
   "order": 759,
   "p1": "vol. 4, 81-84",
   "pn": "",
   "abstract": [
    "In recent years the potential benefits of speech recognition software for people with disabilities have often been cited in software reviews. As it is now possible to purchase inexpensive software that supports the use of voice for the production of documents and the control of the computer, such software could enable people with particular types of disability to have greater access to education and employment. One such disability is dyslexia. This paper reports on an exploratory study in which a group of people with dyslexia performed a set of tasks using commercially available speech recognition software. The aim of the study was to determine the benefits of using speech recognition software as a writing tool for people with dyslexia. The tasks involved normal dictation by the researcher, the use of the keyboard to input the text, the use of the software to dictate the text to the computer and correct the dictation, and the writing and dictation to the computer of a short piece of free text of the participants choice. For each task the speed of production of the text and the number of errors present in the text were recorded. The results showed that people with severe dyslexia experienced a reduction in the time taken to dictate and correct the text compared to the time taken to write the text by hand. For those with severe dyslexia the error rate was also halved using the speech recognition software. Participants with mild dyslexia gained slightly in terms of speed by using the dictation software, although an increase in the number of errors resulted from its use. The implications of these findings for the use of dictation software by people with dyslexia as a writing tool are discussed together with some directions for further research.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-756"
  },
  "bunnell00_icslp": {
   "authors": [
    [
     "H. Timothy",
     "Bunnell"
    ],
    [
     "Debra M.",
     "Yarrington"
    ],
    [
     "James B.",
     "Polikoff"
    ]
   ],
   "title": "STAR: articulation training for young children",
   "original": "i00_4085",
   "page_count": 4,
   "order": 760,
   "p1": "vol. 4, 85-88",
   "pn": "",
   "abstract": [
    "The Speech Training, Assessment, and Remediation (STAR) system is intended to assist Speech and Language Pathologists in treating children with articulation problems. The system is embedded in an interactive video game that is set in a spaceship and involves teaching aliens to understand selected words by spoken example. The sequence of events leads children through a series of successively more difficult speech production tasks, beginning with CV syllables and progressing to words/phrases. Word selection is further tailored to emphasize the contrastive nature of phonemes by the use of minimal pairs (e.g., run/won) in production sets. To assess childrens speech, a discrete hidden Markov model recognition engine is used[1]. Phone models were trained on the CMU Kids database[2]. Performance of the HMM recognizer was compared to perceptual ratings of speech recorded from children who substitute /w/ for /r/. The difference in log likelihood between /r/ and /w/ models correlates well with perceptual ratings of utterances containing substitution errors, but very poorly for correctly articulated examples. The poor correlation between perceptual and machine ratings for correctly articulated utterances may be due to very restricted variance in the perceptual data for those utterances.\n",
    "s Menéndez-Pidal, X., Polikoff, J.B., Peters, S.M., Leonzio, J.E., and Bunnell, H.T. (1996). The Nemours Database of Dysarthric Speech, Proceedings of the Fourth International Conference on Spoken Language Processing, October 3-6, Philadelphia, PA, USA. Eskenazi, M. and Mostow, J. (1997). The CMU KIDS Speech Corpus. Corpus of children's read speech digitized and transcribed on two CD-ROMs, with assistance from Multicom Research and David Graff. Published by the Linguistic Data Consortium, University of Pennsylvania.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-757"
  },
  "nakai00_icslp": {
   "authors": [
    [
     "Takayoshi",
     "Nakai"
    ],
    [
     "Keizo",
     "Ishida"
    ],
    [
     "Hisayoshi",
     "Suzuki"
    ]
   ],
   "title": "Sound pressure distributions and propagation paths in the vocal tract with the pyriform fossa and the larynx",
   "original": "i00_4089",
   "page_count": 4,
   "order": 761,
   "p1": "vol. 4, 89-92",
   "pn": "",
   "abstract": [
    "We have constructed models /i/ and /u/ of the vocal tract from the glottis to the velum with the pyriform fossa and the larynx, and have analyzed by finite element method. Sound pressure distributions in XY cross-sections at 500Hz, the first resonant frequencies, and 2500Hz are shown. The barycentric coordinates of real part of the sound intensity and of sound particle velocity in each XY cross-section are shown. It is found that sound in the vocal tract is not always propagated as a plane wave even when its frequency is low such as 500Hz. Paths of sound propagation are shown. We show the reason why the first resonant frequencies are different in the vocal tract with and without the pyriform fossa.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-758"
  },
  "czap00_icslp": {
   "authors": [
    [
     "László",
     "Czap"
    ]
   ],
   "title": "Lip representation by image ellipse",
   "original": "i00_4093",
   "page_count": 4,
   "order": 762,
   "p1": "vol. 4, 93-96",
   "pn": "",
   "abstract": [
    "Automatic speechreading systems through their use of visual information to support the acoustic signal have been shown to yield better recognition performance than purely acoustic systems, especially when background noise is present. In this paper an answer is sought to the most important questions of speechreading: Which features can represent visual information well? How can they be extracted? Well-known geometric moments are discussed as a means of visual speech representation. Proposed image ellipse axes are shown to be robust and computationally simple features for describing the shape of lips. An intelligibility study was carried out to see which part of the face gives the most support to speechreading. The whole face, mouth or lips were visible dubbed with noisy voice. Visual support to speech perception of the image ellipse model is compared to that of the parts of the natural face.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-759"
  },
  "son00_icslp": {
   "authors": [
    [
     "Rob J. J. H. van",
     "Son"
    ],
    [
     "Barbertje M.",
     "Streefkerk"
    ],
    [
     "Louis C. W.",
     "Pols"
    ]
   ],
   "title": "An acoustic profile of speech efficiency",
   "original": "i00_4097",
   "page_count": 4,
   "order": 763,
   "p1": "vol. 4, 97-100",
   "pn": "",
   "abstract": [
    "Speaking is generally considered efficient, in that less effort is spent articulating more redundant items. Two possible mechanisms for this optimization are tested. The use of prosodic structure, i.e., lexical stress and sentence accent, to (de-)emphasize (un-)important words, and the facilitation of syllable-articulation by retrieving often used motor-programs from memory. Such a \"stored versus computed\" principle in syllable articulation would implicitly result in efficient speech because of the correlations between syllable- and wordfrequencies. These mechanisms are tested for Dutch speech by means of a hand labeled single-speaker corpus of spontaneous and matched read speech, and an automatically labeled multispeaker corpus of read telephone speech. It is concluded that the use of lexical stress and sentence accent/prominence cannot explain all of the frequency-of-occurrence effects found in speech. Furthermore, at least in unstressed syllables, syllablefrequency effects proved to be more important than wordfrequency effects, leaving room for an articulatory \"stored versus computed\" mechanism in the optimization of speaking effort.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-760"
  },
  "meng00c_icslp": {
   "authors": [
    [
     "Helen M.",
     "Meng"
    ],
    [
     "W. K.",
     "Lo"
    ],
    [
     "Yuk Chi",
     "Li"
    ],
    [
     "P. C.",
     "Ching"
    ]
   ],
   "title": "Multi-scale audio indexing for Chinese spoken document retrieval",
   "original": "i00_4101",
   "page_count": 4,
   "order": 764,
   "p1": "vol. 4, 101-104",
   "pn": "",
   "abstract": [
    "The advent of the information age has brought massive digital libraries of multimedia content. This development creates a high demand for information indexing and retrieval technologies, and the capability of browsing through audio archives is much desired. This paper reports on our initial attempt in the use of syllable units for Chinese spoken document retrieval. Our experiments are based on 1861 news stories from local television broadcasts in Cantonese, a monosyllabic Chinese dialect with a rich tonal structure. Results show that indexing with overlapping bi-syllables (tonal syllables) mapped from text delivers the reference retrieval performance at average inverse rank (AIR)=0.830. Retrieval based on overlapping bisyllables (base syllables) recognized from audio achieved an AIR of 0.460.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-761"
  },
  "soltau00_icslp": {
   "authors": [
    [
     "Hagen",
     "Soltau"
    ],
    [
     "Alex",
     "Waibel"
    ]
   ],
   "title": "Phone dependent modeling of hyperarticulated effects#",
   "original": "i00_4105",
   "page_count": 4,
   "order": 765,
   "p1": "vol. 4, 105-108",
   "pn": "",
   "abstract": [
    "In spoken dialogue systems, hyperarticulation occur as an effect to recover previous recognition errors. It is commonly observed that in particular real users apply similar recovery strategies as in human-human interactions. Previous studies have shown that current speech recognizer cannot handle hyperarticulated speech. As an effect of higher word error rates at hyperarticulated speech, humans try to reinforce this speaking style which result in even more recognition errors. In this paper, we present approaches to build robust acoustic models for hyperarticulated speech. The key point is that the changes of acoustic features at hyperarticulation is a phone dependent effect. The idea is to use the likelihood criterion to decide, which phones should be treated separately. This can be done by incorporating dynamic questions about hyperarticulation into the clustering stage. Based on such phonetic decision tree, we can generate appropriate acoustic models. With this method, we achieved a word error reduction about 9% relative at hyperarticulation.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-762"
  },
  "guo00b_icslp": {
   "authors": [
    [
     "Qing",
     "Guo"
    ],
    [
     "Yonghong",
     "Yan"
    ],
    [
     "Baosheng",
     "Yuan"
    ],
    [
     "Xiangdong",
     "Zhang"
    ],
    [
     "Ying",
     "Jia"
    ],
    [
     "Xiaoxing",
     "Liu"
    ]
   ],
   "title": "Vocabulary-based acoustic model trim down and task adaptation",
   "original": "i00_4109",
   "page_count": 4,
   "order": 766,
   "p1": "vol. 4, 109-112",
   "pn": "",
   "abstract": [
    "In this paper, a vocabulary trim down algorithm is proposed in decision tree-based acoustic model to make the model more close to the given task. Using this trim down model as seed model to do task adaptation is also presented. Based on this framework, users can configure the acoustic model by themselves according to their resources (such as vocabulary knowledge, a little amount task specific data, the model size, etc.). Experimental results show that the vocabulary trim down algorithm made the model size being cut off 70% with almost the same accuracy of general model. After adapted by 143 minutes task specific data 27% word error rate reduction can be achieved comparing with the retrained model (using original general purpose data plus all available task specific data) in our Farewell99 dialog system.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-763"
  },
  "chen00l_icslp": {
   "authors": [
    [
     "Willa S.",
     "Chen"
    ],
    [
     "Abeer",
     "Alwan"
    ]
   ],
   "title": "Place of articulation cues for voiced and voiceless plosives and fricatives in syllable-initial position",
   "original": "i00_4113",
   "page_count": 4,
   "order": 767,
   "p1": "vol. 4, 113-116",
   "pn": "",
   "abstract": [
    "In this paper, the acoustic correlates of the labial and alveolar place of articulation for both plosive and fricative consonants are investigated, and the results are analyzed in terms of vowel context, voicing and manner of articulation. Several measurements, including formant and noise measurements, are reported for CVs spoken by two male and two female talkers. It was found that the spectral amplitude of frication noise relative to F1 at vowel onset results in 84% or better correct classification for the fricatives in 3 vowel contexts. For plosives, a measure which quantifies the amplitude of noise at high frequencies relative to F1 at vowel onset (Av-Ahi [1]) resulted in 81 % or better correct classification in the three vowel contexts. Formant frequency cues, on the other hand, were not reliable measures for all vowel contexts.\n",
    "",
    "",
    "K.N. Stevens, S.Y. Manuel, and M. Metthies. Revisit- ing place of articulation measures for stop consonants : Implications for models of consonant production. ICPhS Proc. 1999, 2:1117 1120, 1999.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-764"
  },
  "chen00m_icslp": {
   "authors": [
    [
     "Jingdong",
     "Chen"
    ],
    [
     "Kuldip K.",
     "Paliwal"
    ],
    [
     "Satoshi",
     "Nakamura"
    ]
   ],
   "title": "A block cosine transform and its application in speech recognition",
   "original": "i00_4117",
   "page_count": 4,
   "order": 768,
   "p1": "vol. 4, 117-120",
   "pn": "",
   "abstract": [
    "Noise robust speech recognition has become an important area of research in recent years. The fact that human listeners can recognize speech in the presence of strong noise inspires researchers to imitate some aspects of human auditory perception in automatic speech recognition. This has led to sub-band based speech recognition in which the full-band speech is split into several sub-bands and where each sub-band is processed separately. The resulting multi-band features can be combined in various ways for carrying out speech recognition task. Reported results have shown the superiority of this technique for speech recognition in strong noise conditions. In this paper, we will briefly review the multi-band feature extraction. We will then propose a block discrete cosine transform (BDCT) with its kernel transformation matrix being derived from the decomposition of the kernel of the discrete cosine transform (DCT). We show that the BDCT approximates the DCT in keeping information in decorrelating a sequence. When the BDCT is applied to the mel frequency filter bank energies (FBEs) to replace the DCT to convert them to cepstral coefficients, a new kind of MFCCs is yielded. We call these new features Block discrete cosine transform based MFCCs (BMFCCs) and show that a sub-band processing idea is implicit in the BMFCCs since the BDCT automatically divides the mel frequency FBEs into two sub-bands. We will report various speech recognition results using the BMFCCs as well as the comparison with the multi-band MFCCs and fullband MFCCs to elaborate the properties of the BMFCCs.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-765"
  },
  "hung00_icslp": {
   "authors": [
    [
     "Jeih-Weih",
     "Hung"
    ],
    [
     "Hsin-Min",
     "Wang"
    ],
    [
     "Lin-Shan",
     "Lee"
    ]
   ],
   "title": "Automatic metric-based speech segmentation for broadcast news via principal component analysis",
   "original": "i00_4121",
   "page_count": 4,
   "order": 769,
   "p1": "vol. 4, 121-124",
   "pn": "",
   "abstract": [
    "In this paper, we proposed an algorithm used to improve the performance of the metric-based segmentation techniques, by which the segmentation points are found at maxima of a distance measured between two contiguous windows shifted along the stream of speech features. In our proposed method, the PCA processes are first performed on the speech features to obtain more robust features, and then the above metric-based segmentation was applied on the PCA-derived features to decide the segmentation points. Experiment results show that our proposed method can efficiently improve the detection rates of the segmentation points up to 7% while the false alarm rates remain unchanged.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-766"
  },
  "gao00e_icslp": {
   "authors": [
    [
     "Yuqing",
     "Gao"
    ],
    [
     "Yongxin",
     "Li"
    ],
    [
     "Michael",
     "Picheny"
    ]
   ],
   "title": "Maximal rank likelihood as an optimization function for speech recognition",
   "original": "i00_4125",
   "page_count": 4,
   "order": 770,
   "p1": "vol. 4, 125-128",
   "pn": "",
   "abstract": [
    "Research has shown that rank statistics derived from context-dependent state likelihood can provide robust speech recognition. In previous work, empirical distributions were used to characterize the rank statistics. We present parametric models of the state rank and the rank likelihood, and then based on them, present a new objective function, Maximal Rank Likelihood (MRL), for estimating parameters in a HMM based speech recognition system. The objective function optimizes the average logarithm of the rank likelihood of training/adaptation data. It is a discriminative based estimation process and hence makes the training criterion close to the decoding criterion. Three applications of MRL are discussed. First one is a Linear Discriminative Projection, which optimizes the objective function using all training data and projects feature vectors into a discriminative space with a reduced dimension. The second and third applications are a feature space transformation and a model space transformation, respectively, for adaptation. The transformations are optimized to maximize the rank likelihood of the adaptation data. The experimental results show that the MRL adaptation algorithms outperform the MLLR adaptation.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-767"
  },
  "pan00c_icslp": {
   "authors": [
    [
     "Yue",
     "Pan"
    ],
    [
     "Alex",
     "Waibel"
    ]
   ],
   "title": "The effects of room acoustics on MFCC speech parameter",
   "original": "i00_4129",
   "page_count": 4,
   "order": 771,
   "p1": "vol. 4, 129-132",
   "pn": "",
   "abstract": [
    "Automatic speech recognition systems attain high performance for close-talking applications, but they deteriorate significantly in distant-talking environment. The reason is the mismatch between training and testing conditions. We have carried out a research work for a better understanding of the effects of room acoustics on speech feature by comparing simultaneous recordings of close talking and distant talking speech utterances. The characteristics of two degrading sources, background noise and room reverberation are discussed. Their impacts on the spectrum are different. The noise affects on the valley of the spectrum while the reverberation causes the distortion at the peaks at the pitch frequency and its multiples. In the situation of very few training data, we attempt to choose the efficient compensation approaches in the spectrum, spectrum subband or cepstrum domain. Vector Quantization based model is used to study the influence of the variation on feature vector distribution. The results of speaker identification experiments are presented for both close-talking and distant talking data.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-768"
  },
  "hasegawajohnson00_icslp": {
   "authors": [
    [
     "Mark",
     "Hasegawa-Johnson"
    ]
   ],
   "title": "Time-frequency distribution of partial phonetic information measured using mutual information",
   "original": "i00_4133",
   "page_count": 4,
   "order": 772,
   "p1": "vol. 4, 133-136",
   "pn": "",
   "abstract": [
    "This paper uses the method of mutual information to estimate the distribution of partial phonetic information in the time-frequency plane relative to an acoustic landmark. TIMIT transcriptions are parsed to estimate the locations of consonant closure landmarks, consonant release landmarks, manner change landmarks, and vowel or glide pivot landmarks. A mel-scale spectrogram is computed over the 250ms centered at each landmark, and the logarithmic energy of each point in time-frequency space is linearly quantized. The phoneme label associated with a landmark determines the values of 25 binary distinctive features. Finally, coincidences between feature and spectral energy values are counted, and the average log probabilities are calculated in order to produce an \\infogram\" of each distinctive feature: a measurement of the mutual information between the value of the feature and the energy of each point in the time-frequency plane.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-769"
  },
  "jiang00b_icslp": {
   "authors": [
    [
     "Li",
     "Jiang"
    ],
    [
     "Xuedong",
     "Huang"
    ]
   ],
   "title": "Subword-dependent speaker clustering for improved speech recognition",
   "original": "i00_4137",
   "page_count": 4,
   "order": 773,
   "p1": "vol. 4, 137-140",
   "pn": "",
   "abstract": [
    "Speaker variability has a significant impact to the state-of-the-art speech recognition systems. Traditionally speaker clustering is performed without considering individual or class phonetic similarities across different speakers. In fact, clustered speaker groups may have very different degrees of variations for different phonetic classes. In this paper, speaker clustering is performed at subword level or subphonetic level. With one or more instances derived from clustering for each subword or subphonetic unit, we model speaker variation explicitly across different subword or subphonetic instances. In addition, we select from massive possible combinations of speaker-clustered subword models to form our initial model for speaker adaptation. Experiments show that subword-dependent speaker clustering is more effective than the traditional speaker clustering.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-770"
  },
  "luo00b_icslp": {
   "authors": [
    [
     "Chunhua",
     "Luo"
    ],
    [
     "Fang",
     "Zheng"
    ],
    [
     "Mingxing",
     "Xu"
    ]
   ],
   "title": "An equivalent-class based MMI learning method for MGCPM",
   "original": "i00_4141",
   "page_count": 4,
   "order": 774,
   "p1": "vol. 4, 141-144",
   "pn": "",
   "abstract": [
    "In this paper, we present an Equivalent-Class Based Maximum Mutual Information (ECB-MMI) learning method for our previously proposed Mixed Gaussian Continuous Probability Model (MGCPM). Similar to HMMs, the defined object function for MGCPM training considers the mutual information among different models so as to maximally separate the Speech Recognition Units (SRUs) in model space. Experimental result shows that for MGCPM the MMI training method can improve the recognition rate by 5% compared to the traditional training method MLE (Maximum Likelihood Estimation). Because the computation amount of MMI algorithm is very large, we propose an N-Best strategy to find the corresponding equivalent class (EC) in order to reduce complexity. Our experimental result shows that this criterion works very well.\n",
    "Keywords: Equivalent Class Based-MMI, Mixed Gaussian Continuous Probability Model, Speech Recognition Unit, MLE, and Equivalent Class\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-771"
  },
  "wrench00_icslp": {
   "authors": [
    [
     "Alan A.",
     "Wrench"
    ],
    [
     "Korin",
     "Richmond"
    ]
   ],
   "title": "Continuous speech recognition using articulatory data",
   "original": "i00_4145",
   "page_count": 4,
   "order": 775,
   "p1": "vol. 4, 145-148",
   "pn": "",
   "abstract": [
    "In this paper we show that there is measurable information in the articulatory system which can help to disambiguate the acoustic signal. We measure directly the movement of the lips, tongue, jaw, velum and larynx and parameterise this articulatory feature space using principal components analysis. The parameterisation is developed and evaluated using a speaker dependent phone recognition task on a specially recorded TIMIT corpus of 460 sentences. The results show that there is useful supplementary information contained in the articulatory data which yields a small but significant improvement in phone recognition accuracy of 2%. However, preliminary attempts to estimate the articulatory data from the acoustic signal and use this to supplement the acoustic input have not yielded any significant improvement in phone accuracy.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-772"
  },
  "mak00_icslp": {
   "authors": [
    [
     "Brian",
     "Mak"
    ],
    [
     "Yik-Cheung",
     "Tam"
    ]
   ],
   "title": "Asynchrony with trained transition probabilities improves performance in multi-band speech recognition",
   "original": "i00_4149",
   "page_count": 4,
   "order": 776,
   "p1": "vol. 4, 149-152",
   "pn": "",
   "abstract": [
    "One of the central themes in multi-band automatic speech recognition (ASR) is to devise a strategy for recombining sub-band information. This in turn raises two questions: (1) at what phonetic unit should the recombination take place? (2) How asynchronously should the sub-bands be run? Theoretically asynchronous multi-band ASR should perform at least as well as synchronous multi-band ASR. However, in the past few years, there are conflicting results on the issue. In this paper, we study the asynchrony issue under the framework of HMM composition in which a model-based recombination strategy is used to recombine sub-band HMMs at the state level. We hypothesize that re-estimation of the transition probabilities is crucial for multi-band ASR (using HMM composition). Experiments on connected TI digits show that for both clean speech and noisy speech (with additive white noise of 10db), HMMs composed from sub-band HMMs in which transition probabilities are trained with Baum-Welch algorithm outperform those in which transition probabilities are set uniformly (e.g. 0.5 in common left-to-right HMMs) by about 20%. Recombining sub-bands with a maximum asynchrony limit of one state gives a further \u0018 15% improvement over synchronous recombination on both clean speech and noisy speech. Finally relaxing asynchrony to more than one state results in worse performance.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-773"
  },
  "sivadas00_icslp": {
   "authors": [
    [
     "Sunil",
     "Sivadas"
    ],
    [
     "Pratibha",
     "Jain"
    ],
    [
     "Hynek",
     "Hermansky"
    ]
   ],
   "title": "Discriminative MLPs in HMM-based recognition of speech in cellular telephony",
   "original": "i00_4153",
   "page_count": 4,
   "order": 777,
   "p1": "vol. 4, 153-156",
   "pn": "",
   "abstract": [
    "Deviating from the conventional Hidden Markov Model-Multi-Layer Perceptron (HMM-MLP) hybrid paradigm of using MLP for classification, the proposed discriminative MLP technique uses MLP as a mapping module for feature extraction for conventional HMM-based systems. The MLP is discriminatively trained on the phonetically labeled training data to generate the phoneme posterior probabilities. We achieved a relative word error rate reduction of 15-35% on AURORA Phase 2 continuous digit recognition task defined by ETSI.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-774"
  },
  "hanazawa00b_icslp": {
   "authors": [
    [
     "Toshiyuki",
     "Hanazawa"
    ],
    [
     "Jun",
     "Ishii"
    ],
    [
     "Yohei",
     "Okato"
    ],
    [
     "Kunio",
     "Nakajima"
    ]
   ],
   "title": "Acoustic modeling for spontaneous speech recognition using syllable dependent models",
   "original": "i00_4157",
   "page_count": 4,
   "order": 778,
   "p1": "vol. 4, 157-160",
   "pn": "",
   "abstract": [
    "This paper proposes a syllable context dependent model for spontaneous speech recognition. It is generally assumed that, since spontaneous speech is greatly affected by coarticulation, an acoustic model featuring a longer range phonemic context is required to achieve a high degree of recognition accuracy. This motivated the authors to investigate a tri-syllable model that takes differences in the preceding and succeeding syllables into account. Since Japanese syllables consist of either a single vowel or a consonant and vowel combination, a tri-syllable model always takes the preceding and succeeding vowels that are the primary factors in coarticulation into account. A tri-syllable model is thus capable of efficiently representing coarticulation. The tri-syllable model was trained using spontaneous speech; then, the effectiveness of continuous syllable recognition and statistical language model-based continuous word recognition were evaluated. Compared to a regular triphone model without state sharing, it was found that the correct syllable accuracy of the continuous syllable recognition improved from 64.9% to 66.3%. The word recognition accuracy for the statistical language modelbased continuous word recognition improved from 88.4% to 89.2%.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-775"
  },
  "jiang00c_icslp": {
   "authors": [
    [
     "Hui",
     "Jiang"
    ],
    [
     "Li",
     "Deng"
    ]
   ],
   "title": "A robust training strategy against extraneous acoustic variations for spontaneous speech recognition",
   "original": "i00_4161",
   "page_count": 4,
   "order": 779,
   "p1": "vol. 4, 161-164",
   "pn": "",
   "abstract": [
    "In the paper, we propose a robust training strategy to deal with extraneous acoustic variations for conversational speech recognition. This strategy generalizes speaker adaptive training, where HMM parameter transformations are used to normalize the extraneous variations in the training data according to a set of pre-defined conditions. Then a compact model and the associated prior p.d.f.s of transformation parameters are estimated using the maximum likelihood criterion. In the testing phase, the compact model and the prior p.d.f.s are used to search for the unknown word sequence based on Bayesian Prediction Classification. The proposed strategy is evaluated in a Switchboard task to deal with pronunciation variations in spontaneous speech recognition. Preliminary results show moderate word error rate reduction over a well-trained baseline system under identical experimental conditions.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-776"
  },
  "purnell00_icslp": {
   "authors": [
    [
     "Darryl W.",
     "Purnell"
    ],
    [
     "Elizabeth C.",
     "Botha"
    ]
   ],
   "title": "Improved performance and generalization of minimum classification error training for continuous speech recognition",
   "original": "i00_4165",
   "page_count": 4,
   "order": 780,
   "p1": "vol. 4, 165-168",
   "pn": "",
   "abstract": [
    "Discriminative training of hidden Markov models (HMMs) using segmental minimum classification error (MCE) training has been shown to work extremely well for certain speech recognition applications. It is, however, somewhat prone to overspecialization. This study investigates various techniques which improve performance and generalization of the MCE algorithm. Improvements of up to 7% in relative error rate on the test set are achieved.\n",
    "Keywords: speech recognition, discriminative training, minimum classification error, overspecialization, overtraining\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-777"
  },
  "jia00b_icslp": {
   "authors": [
    [
     "Ying",
     "Jia"
    ],
    [
     "Yonghong",
     "Yan"
    ],
    [
     "Baosheng",
     "Yuan"
    ]
   ],
   "title": "Dynamic threshold setting via Bayesian information criterion (BIC) in HMM training",
   "original": "i00_4169",
   "page_count": 3,
   "order": 781,
   "p1": "vol. 4, 169-171",
   "pn": "",
   "abstract": [
    "In this paper, an approach of dynamic threshold setting via Bayesian Information Criterion (BIC) in HMM training is described. The BIC threshold setting is applied to two important applications. Firstly, it is used to set the thresholds for decision tree based state tying, in place of the conventional approach of using a heuristic constant threshold. Secondly, it is applied to choosing the number of Gaussian mixture at state mixing-up stage. Experimental results on LVCSR Chinese dictation task indicate that BIC can dynamically set thresholds for cluster splitting according to the underlying complexity of the cluster parameters. Also significant performance improvement is achieved with the dynamic BIC threshold setting.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-778"
  },
  "hain00b_icslp": {
   "authors": [
    [
     "Thomas",
     "Hain"
    ],
    [
     "Philip C.",
     "Woodland"
    ]
   ],
   "title": "Modelling sub-phone insertions and deletions in continuous speech recognition",
   "original": "i00_4172",
   "page_count": 4,
   "order": 782,
   "p1": "vol. 4, 172-175",
   "pn": "",
   "abstract": [
    "Recently, an extension to standard hidden Markov mod- els for speech recognition called Hidden Model Sequence (HMS) modelling was introduced. In this approach the relationship between phones used in a pronunciation dictionary and the HMMs used to model these in context is assumed to be stochastic. One important feature of the HMS framework is the ability to handle arbitrary model to phone sequence alignments. In this paper we try to exploit that capability by using two different methods to model sub-phone insertions and deletions. Experiments on the Resource Management (RM) corpus and a subset of the Switchboard corpus show that, relative to standard HMM baseline, a reduction word error rate (WER) of 24.3% relative can be obtained on RM and 2.4% absolute on Switchboard.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-779"
  },
  "fung00_icslp": {
   "authors": [
    [
     "Carrson C.",
     "Fung"
    ],
    [
     "Oscar C.",
     "Au"
    ],
    [
     "Wanggen",
     "Wan"
    ],
    [
     "Chi H.",
     "Yim"
    ],
    [
     "Cyan L.",
     "Keung"
    ]
   ],
   "title": "Improved acoustics modeling for speech recognition using transformation techniques",
   "original": "i00_4176",
   "page_count": 3,
   "order": 783,
   "p1": "vol. 4, 176-179",
   "pn": "",
   "abstract": [
    "In statistical speech recognition, misclassification often occurs when there is a mismatch between the incoming signal and the acoustics model inside the recognizer. In order to combat this problem, techniques such as Cepstral Mean Subtraction, Vocal Tract Normalization, adaptation and pronunciation model can be used.\n",
    "In this paper, we proposed a new approach based on transformation technique where the output distribution function in the HMM model, a Gaussian probability density function, could be transformed to match the estimated distribution of the incoming signal by using a memoryless invertible nonlinearity function. Since the new density still has a Gaussian form, the function could be completely characterized by using the Expectation Maximization (EM) algorithm.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-780"
  },
  "gu00e_icslp": {
   "authors": [
    [
     "Liang",
     "Gu"
    ],
    [
     "Jayanth",
     "Nayak"
    ],
    [
     "Kenneth",
     "Rose"
    ]
   ],
   "title": "Discriminative training of tied-mixture HMM by deterministic annealing",
   "original": "i00_4183",
   "page_count": 4,
   "order": 784,
   "p1": "vol. 4, 183-186",
   "pn": "",
   "abstract": [
    "A deterministic annealing algorithm for the design of tiedmixture HMM recognizers is proposed, which reduces the training sensitivity to parameter initialization, automatically smoothes the classification error cost function to allow gradientbased optimization, and seeks better solutions than known techniques. The new approach introduces randomness into the classification rule during the training process, and minimizes the expected error rate while controlling the level of randomness via a constraint on the Shannon entropy. As the entropy constraint is gradually relaxed, the effective cost function converges to the classification error rate and the system becomes a hard (nonrandom) recognizer. Experiments show that the proposed method outperforms design by maximum likelihood reestimation and by generalized probabilistic descent.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-781"
  },
  "kuo00_icslp": {
   "authors": [
    [
     "Hong-Kwang Jeff",
     "Kuo"
    ],
    [
     "Chin-Hui",
     "Lee"
    ]
   ],
   "title": "Discriminative training in natural language call routing",
   "original": "i00_4187",
   "page_count": 4,
   "order": 785,
   "p1": "vol. 4, 187-190",
   "pn": "",
   "abstract": [
    "In this paper, we show how discriminative training can be used to improve classifiers used in natural language processing, using as an example the task of natural language call routing. In natural language call routing, callers are routed to desired departments based on natural spoken responses to an open-ended How may I direct your call? prompt. With vector-based natural language call routing, callers can be transferred using a routing matrix that is trained based on statistics of occurrence of words and word sequences in a training corpus after morphological and stop-word filtering. New user requests are represented as feature vectors and are routed based on the cosine similarity score with the model destination vectors encoded in the routing matrix. The present paper proposes the use of discriminative training on the routing matrix to improve routing accuracy and robustness. By retraining the routing matrix, a relative error rate reduction of 13-19% was achieved. Increased robustness was demonstrated in that with 10% rejection, there was a relative error rate reduction of 40%. The proposed formulation is equally applicable to algorithms addressing a broad range of speech understanding, information retrieval, and topic identification problems.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-782"
  },
  "tanaka00_icslp": {
   "authors": [
    [
     "Kazuyo",
     "Tanaka"
    ],
    [
     "Hiroaki",
     "Kojima"
    ]
   ],
   "title": "A speech recognition method with a language-independent intermediate phonetic code",
   "original": "i00_4191",
   "page_count": 4,
   "order": 786,
   "p1": "vol. 4, 191-194",
   "pn": "",
   "abstract": [
    "This paper proposes a speech recognition framework using a language-independent phonetic code system that locates in intermediate between the acoustic-phonetic layer and the language-dependent phone layer. Its original framework is directed toward an alternative to the conventional sample-based statistical methods comprised of phonetic environment-dependent techniques. In the first part, we describe the basic framework of the proposed processing, and the next, specific procedures for extracting acoustic features using non-linear spectral enhancement and for deriving the  language-independent Intermediate Phonetic Code (IPC) set. A recognition system is implemented as phrase spotting in symbolic domain on the basis of this framework. In the last, preliminary results of recognition experiments are shown, in which isolated word recognition using the language-independent IPC set is examined compared with that of a language-dependent phone set.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-783"
  },
  "lefevre00_icslp": {
   "authors": [
    [
     "Fabrice",
     "Lefèvre"
    ]
   ],
   "title": "Confidence measures based on the k-nn probability estimator",
   "original": "i00_4195",
   "page_count": 3,
   "order": 787,
   "p1": "vol. 4, 195-197",
   "pn": "",
   "abstract": [
    "In this paper, the use of the probabilities produced by a K-nearest neighbours (K-nn) estimator as confidence measure is investigated in an hypothesis verification post-processing scheme. The objective is to classify as correct or incorrect the outputs of a Gaussian mixture model (GMM) /HMM speech recognition system. Four confidence measures based on the Knn probability estimator are introduced. Preliminary experiments are reported and discussed on the TIMIT database.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-784"
  },
  "mukherjee00_icslp": {
   "authors": [
    [
     "Niloy",
     "Mukherjee"
    ],
    [
     "Nitendra",
     "Rajput"
    ],
    [
     "L. Venkata",
     "Subramaniam"
    ],
    [
     "Ashish",
     "Verma"
    ]
   ],
   "title": "On deriving a phoneme model for a new language",
   "original": "i00_4198",
   "page_count": 4,
   "order": 788,
   "p1": "vol. 4, 198-201",
   "pn": "",
   "abstract": [
    "We present a method for building an initial phoneme model for training an HMM in a new language using an already trained recognition system in a base language. HMM based phoneme recognition systems are used to model the phonemes in most large vocabulary speech recognition tasks. Mappings between the phonetic spaces of the two languages are generated and are used to populate the phonetic space of the new language. The best possible alignment of the new language data is obtained and initial phone models are built on this labeled data. A classification experiment is performed in the new language to illustrate the goodness of initial phone models. Experiments are carried out with Hindi as the new language using an English language recognition system to derive the initial phone models for Hindi language.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-785"
  },
  "saito00b_icslp": {
   "authors": [
    [
     "Tomonobu",
     "Saito"
    ],
    [
     "Kiyoshi",
     "Hashimoto"
    ]
   ],
   "title": "Estimation of semantic case of Japanese dialogue by use of distance derived from statistics of dependency",
   "original": "i00_4202",
   "page_count": 4,
   "order": 789,
   "p1": "vol. 4, 202-205",
   "pn": "",
   "abstract": [
    "In an attempt to estimate the semantic cases for nounparticle- verb triples in the ATR dialogue corpus, the authors propose a measure of distance based on statistics of dependent noun-particle-verb triples. A clustering analysis of all the triples in the corpus was conducted using the measure of distance. Competence of the proposed measure of distance is verified by examination of the distribution of the single-case clusters. By use of the score derived from the measure of distance of the training corpus, the authors conducted the estimation of the correct semantic case for a given noun-particle-verb triples in the test corpus. The result remarkably differentiates the particles with respect to the estimation accuracies. For instance, particle 'wo' has accuracies over 80 %, while 'de' has accuracies less than 40 %. The correlation analysis between the accuracy and the consistency rates indicates that the particles of higher consistency have also tendencies to higher accuracies .\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-786"
  },
  "cox00b_icslp": {
   "authors": [
    [
     "Stephen",
     "Cox"
    ],
    [
     "Srinandan",
     "Dasmahapatra"
    ]
   ],
   "title": "A semantically-based confidence measure for speech recognition",
   "original": "i00_4206",
   "page_count": 4,
   "order": 790,
   "p1": "vol. 4, 206-209",
   "pn": "",
   "abstract": [
    "In previous work, we have argued that it is beneficial to find confidence measures (CMs) that are not dependent on use of \"side information\" from a specific recogniser. Here, we extend this philosophy to include the use of semantic information in estimating the confidence that a word is correct. We are motivated by the observation that sometimes the recogniser outputs a word which can easily be spotted (by humans) as incorrect, because it bears no relation to the semantics of the rest of the decoded sentence. Latent semantic analysis (LSA) was used as a method for estimating semantic \"semantic similarity\" between words in a text corpus. From these scores, an average semantic similarity of each decoded word to the other decoded words in an utterance could be estimated, and by thresholding this similarity measure, words were tagged as CORRECT or INCORRECT. We benchmarked the performance of this semantic CM against a tried-and-tested CM, the N-best CM. The precision of the semantic CM was inferior to that of N-best when the recall (the number of words considered) was high, but it out-performed N-best for low recall, and a combined classifier showed the benefits of using both techniques. An interesting and unexpected result was that the semantic CM was better at identifying correct words than incorrect words.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-787"
  },
  "ganapathiraju00_icslp": {
   "authors": [
    [
     "Aravind",
     "Ganapathiraju"
    ],
    [
     "Joseph",
     "Picone"
    ]
   ],
   "title": "Support vector machines for automatic data cleanup",
   "original": "i00_4210",
   "page_count": 4,
   "order": 791,
   "p1": "vol. 4, 210-213",
   "pn": "",
   "abstract": [
    "Accurate training data plays a very important role in training effective acoustic models for speech recognition. In conversational speech, in several cases, the transcribed data has a significant word error rate which leads to bad acoustic models. In this paper we explore a method to automatically identify such mislabelled data in the context of a hybrid Support Vector Machine/hidden Markov model (HMM) system, thereby building accurate acoustic models. The effectiveness of this method is proven on both synthetic and real speech data. A hybrid system for OGI alphadigits using this methodology gives a significant improvement in performance over a comparable baseline HMM system.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-788"
  },
  "gu00f_icslp": {
   "authors": [
    [
     "Yong",
     "Gu"
    ],
    [
     "Trevor",
     "Thomas"
    ]
   ],
   "title": "Competition-based score analysis for utterance verification in name recognition",
   "original": "i00_4214",
   "page_count": 4,
   "order": 792,
   "p1": "vol. 4, 214-217",
   "pn": "",
   "abstract": [
    "Utterance verification based on N-Best HMM scores has been widely used in ASR system. There are a number of ways to calculate a measurement score for verification from N-Best scores. Most of proposed methods are based on the framework of the hypothesis testing. This has lead to use the second best score or an overall average of available N-Best scores for normalisation. In this study we examine N-Best UV approach from a competition-based measurement framework. With this framework different competitive measurements can be derived from a sequence of sorted likelihood ratios (SLR). The evaluation results demonstrate that OOV performance can be improved by using some selective components in SLR. In our experiments by using the first four components OOV rejection errors can be reduced about 30% in comparison with the baseline results.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-789"
  },
  "zhang00k_icslp": {
   "authors": [
    [
     "Yaxin",
     "Zhang"
    ]
   ],
   "title": "Utterance verification/rejection for speaker-dependent and speaker-independent speech recognition",
   "original": "i00_4218",
   "page_count": 5,
   "order": 793,
   "p1": "vol. 4, 218-221",
   "pn": "",
   "abstract": [
    "In this paper we discuss the differences of the utterance verification/rejection between speaker-dependent and speaker-independent speech recognition. A general guideline will be given to handle the verification task for the two type speech recognitions. We also discuss some specific methods have been commonly deployed in various speech recognition systems. In this study we describe two example approaches, an online score approach for speaker-dependent and a filler-cohort modeling approach for speaker-independent speech recognition. From our experiments we conclude that there is no common approach of confidence measure estimation for both SD and SI speech recognition.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-790"
  },
  "petrushin00_icslp": {
   "authors": [
    [
     "Valery A.",
     "Petrushin"
    ]
   ],
   "title": "Emotion recognition in speech signal: experimental study, development, and application",
   "original": "i00_4222",
   "page_count": 4,
   "order": 794,
   "p1": "vol. 2, 222-225",
   "pn": "",
   "abstract": [
    "The paper describes an experimental study on vocal emotion expression and recognition and the development of a computer agent for emotion recognition. The study deals with a corpus of 700 short utterances expressing five emotions: happiness, anger, sadness, fear, and normal (unemotional) state, which were portrayed by thirty subjects. The utterances were evaluated by twenty three subjects, twenty of whom participated in recording. The accuracy of recognition emotions in speech is the following: happiness - 61.4%, anger - 72.2%, sadness - 68.3%, fear - 49.5%, and normal - 66.3%. The human ability to portray emotions is approximately at the same level (happiness - 59.8%, anger - 71.7%, sadness - 68.1%, fear - 49.7%, and normal - 65.1%), but the standard deviation is much larger. The human ability to recognize their own emotions has been also evaluated. It turned out that people are good in recognition anger (98.1%), sadness (80%) and fear (78.8%), but are less confident for normal state (71.9%) and happiness (71.2%). A part of the corpus was used for extracting features and training computer based recognizers. Some statistics of the pitch, the first and second formants, energy and the speaking rate were selected and several types of recognizers were created and compared. The best results were obtained using the ensembles of neural network recognizers, which demonstrated the following accuracy: normal state - 55-75%, happiness - 60-70%, anger - 70-80%, sadness - 75-85%, and fear - 35-55%. The total average accuracy is about 70%. An emotion recognition agent was created that is able to analyze telephone quality speech signal and distinguish between two emotional states --\"agitation\" and \"calm\" -- with the accuracy of 77%. The agent was used as a part of a decision support system for prioritizing voice messages and assigning a proper human agent to response the message at call center environment. The architecture of the system is presented and discussed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-791"
  },
  "lyu00b_icslp": {
   "authors": [
    [
     "Ren-yuan",
     "Lyu"
    ],
    [
     "Chi-yu",
     "Chen"
    ],
    [
     "Yuang-chin",
     "Chiang"
    ],
    [
     "Min-shung",
     "Liang"
    ]
   ],
   "title": "A bi-lingual Mandarin/taiwanese (min-nan), large vocabulary, continuous speech recognition system based on the tong-yong phonetic alphabet (TYPA)",
   "original": "i00_4226",
   "page_count": 5,
   "order": 795,
   "p1": "vol. 2, 226-229",
   "pn": "",
   "abstract": [
    "In this paper, we describe the first Mandarin/Taiwanese (Min-nan) bi-lingual, continuous speech recognition system for large vocabulary or vocabulary-independent applications. A phonetic transcription system called Tong-yong Phonetic Alphabet (TYPA) is described and used to transcribe the bilingual Mandarin/Taiwanese lexicons. The Right-Context- Dependent (RCD) phonetic continuous-density Hidden Markov Models (CHMM) based on TYPA are used as the acoustic models. A lexicon tree containing 40 thousand bilingual words is used as a searching net to evaluate the performance of the recognizer. A 92.55% word accuracy is achieved on a speaker dependent case. Furthermore, we construct a continuous-speech real-time demonstration system based on the vocabulary-independent RCD models for a specific application domain of automated hospital appointment arrangement, where Mandarin/Taiwanese mixed speech is very possible to happen.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-792"
  },
  "emam00_icslp": {
   "authors": [
    [
     "Ossama",
     "Emam"
    ],
    [
     "Jorge",
     "Gonzalez"
    ],
    [
     "Carsten",
     "Günther"
    ],
    [
     "Eric",
     "Janke"
    ],
    [
     "Siegfried",
     "Kunzmann"
    ],
    [
     "Giulio",
     "Maltese"
    ],
    [
     "Claire",
     "Waast-Richard"
    ]
   ],
   "title": "A data-driven methodology for the production of multilingual conversational systems",
   "original": "i00_4230",
   "page_count": 4,
   "order": 796,
   "p1": "vol. 2, 230-233",
   "pn": "",
   "abstract": [
    "This paper describes a data-driven methodology for the design of multilingual conversational systems. The work presented here covers the various aspects of bootstrapping and deploying multilingual systems, such as phone set definition, acoustic modeling, language modeling, and language understanding. For the initial system domain, a Directory Assistance Service has been chosen. Whereas former approaches focused more on the multilinguality of single components of a dialog system we will cover the whole speech-to-speech process of a conversational system.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-793"
  },
  "vaich00_icslp": {
   "authors": [
    [
     "Tzur",
     "Vaich"
    ],
    [
     "Arnon",
     "Cohen"
    ]
   ],
   "title": "Multi-path, context dependent SC-HMM architectures for improved connected word recognition",
   "original": "i00_4234",
   "page_count": 4,
   "order": 797,
   "p1": "vol. 4, 234-237",
   "pn": "",
   "abstract": [
    "Connected Word Recognition (CWR) systems are needed for many consumer applications. In such applications cost is a major factor. Several architectures for HMM based CWR are examined to provide an optimal cost effective configuration. Three architectures were examined: a simple LTR with no skips, LTR with skips (LTR-WS) and Multi-Path LTR. The LTR-WS and the MP-LTR were shown to be superior to the simple LTR. The MPLTR exhibited the best results with long strings.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-794"
  },
  "meron00_icslp": {
   "authors": [
    [
     "Yoram",
     "Meron"
    ],
    [
     "Keikichi",
     "Hirose"
    ]
   ],
   "title": "Robust recognition using multiple utterances",
   "original": "i00_4238",
   "page_count": 4,
   "order": 798,
   "p1": "vol. 4, 238-241",
   "pn": "",
   "abstract": [
    "Increasing the reliability of the results of automatic speech recognition systems is an important research and development issue. Although recent systems have been shown to achieve quite high recognition results for limited tasks, this may not be good enough for some applications, where some bits of information are critical, and have to be recognized correctly. In this paper, we suggest a method for the improvement of the robustness of speech recognition, using a speaker independent HMM, of either a word, phrase, or a full sentence, by taking advantage of repeated utterances of the same content. The method can be applied to most configurations of HMM based recognizers, and does not require additional model training. Recognition experiments showed improvement in recognition accuracy, under quiet and noisy conditions.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-795"
  },
  "cosi00b_icslp": {
   "authors": [
    [
     "Piero",
     "Cosi"
    ],
    [
     "John-Paul",
     "Hosom"
    ],
    [
     "Fabio",
     "Tesser"
    ]
   ],
   "title": "High performance Italian continuous \"digit\" recognition",
   "original": "i00_4242",
   "page_count": 4,
   "order": 799,
   "p1": "vol. 4, 242-245",
   "pn": "",
   "abstract": [
    "The development of a speaker independent connected ¡°digits¡± recognizer for Italian is described. The CSLU Speech Toolkit was used to develop and implement the system which is based on an hybrid ANN/HMM architecture. The recognizer is trained on contextdependent categories to account for coarticulatory variation. Various front-end processing was compared and, when the best features (MFCC with CMS + Δ) were considered, there was a 98.68% word recognition accuracy (90.76% sentence recognition accuracy) on a test set of the FIELD continuous digits recognition task.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-796"
  },
  "fohr00_icslp": {
   "authors": [
    [
     "Dominique",
     "Fohr"
    ],
    [
     "Odile",
     "Mella"
    ],
    [
     "Christophe",
     "Antoine"
    ]
   ],
   "title": "The automatic speech recognition engine ESPERE: experiments on telephone speech",
   "original": "i00_4246",
   "page_count": 4,
   "order": 800,
   "p1": "vol. 4, 246-249",
   "pn": "",
   "abstract": [
    "This paper presents our automatic speech recognition engine ESPERE and several results obtained from experiments on telephone speech. ESPERE (Engine for SPEech REcognition) is a HMMbased toolbox for speech recognition allowing the user to choose the modeled unit (word, phone, triphone), define the topology of every Hidden Markov Model, train the models with the Baum-Welch algorithm and evaluate the recognition accuracy on speech databases. To validate the ESPERE toolbox, we have conducted tests on real world data: the recognition of a three-digit code to access a call center. We have investigated the influence of some parameters and some preprocessing algorithms. Finally, combining the best parameters, the recognition score reaches 96.4% at the word level and 92.1% at the sentence level.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-797"
  },
  "kiss00_icslp": {
   "authors": [
    [
     "Imre",
     "Kiss"
    ]
   ],
   "title": "A comparison of distributed and network speech recognition for mobile communication systems",
   "original": "i00_4250",
   "page_count": 4,
   "order": 801,
   "p1": "vol. 4, 250-253",
   "pn": "",
   "abstract": [
    "In this paper, we compare the conventional Network Speech Recognition (NSR) and the newly established Distributed Speech Recognition (DSR) concepts for mobile communications. These implementation approaches to Automatic Speech Recognition (ASR) are analyzed from three aspects. First, the effect on the speech recognition accuracy of ASR systems with various complexity. Second, usability in different operating environments (environmental noises). Finally, the resilience to erroneous transmission (radio channel). Our experimental results show that DSR reduces the error rate of ASR systems by 17- 25% on average compared to NSR, depending on the recognition task. The error resilience of DSR provides up to 37% error rate reduction over NSR under severe channel error conditions. In non-stationary environmental noise, DSR may outperform NSR by as much as 41% in terms of error-rate reduction. Based on these results and considering the low complexity implementation of DSR in the mobile terminals, it is concluded that DSR provides a viable, robust and economical alternative to the traditional NSR approach, especially in real-life mobile operating environments.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-798"
  },
  "frankel00_icslp": {
   "authors": [
    [
     "Joe",
     "Frankel"
    ],
    [
     "Korin",
     "Richmond"
    ],
    [
     "Simon",
     "King"
    ],
    [
     "Paul",
     "Taylor"
    ]
   ],
   "title": "An automatic speech recognition system using neural networks and linear dynamic models to recover and model articulatory traces",
   "original": "i00_4254",
   "page_count": 4,
   "order": 802,
   "p1": "vol. 4, 254-257",
   "pn": "",
   "abstract": [
    "We describe a speech recognition system which uses articulatory parameters as basic features and phone-dependent linear dynamic models. The system first estimates articulatory trajectories from the speech signal. Estimations of x and y coordinates of 7 actual articulator positions in the midsagittal plane are produced every 2 milliseconds by a recurrent neural network, trained on real articulatory data. The output of this network is then passed to a set of linear dynamic models, which perform phone recognition.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-799"
  },
  "shobaki00_icslp": {
   "authors": [
    [
     "Khaldoun",
     "Shobaki"
    ],
    [
     "John-Paul",
     "Hosom"
    ],
    [
     "Ronald A.",
     "Cole"
    ]
   ],
   "title": "The OGI kids² speech corpus and recognizers",
   "original": "i00_4258",
   "page_count": 4,
   "order": 803,
   "p1": "vol. 4, 258-261",
   "pn": "",
   "abstract": [
    "We describe a corpus of childrens speech, called the OGI Kids Speech corpus, and a speaker- and vocabularyindependent recognition system trained and evaluated with these data. The corpus is composed of both prompted and spontaneous speech from 1100 children from kindergarten through grade 10. The prompted speech was presented as text appearing below an animated character (Baldi) that produced accurate visible speech synchronized with recorded prompts. The speech and text consists of isolated words, sentences, and digit strings. A phonetic recognizer was trained using an HMM/ANN framework, with training data taken from intervals of speech associated with phonetic segments in the isolated words in the corpus. Phonetic segments were derived using automatic phonetic alignment. To find out how well the recognizer is able to generalize to new words not found in the training set, we performed two test-set evaluations: one using a new set of utterances from the set of 205 words spoken in isolation (similar to the data used to train the recognizer) and one using words from the prompted sentences. Results were dramatically different (97.5% for isolated vs. 37.9% for words in sentences), and we explore methods that may be used to improve the recognizers ability to generalize to new words.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-800"
  },
  "wu00g_icslp": {
   "authors": [
    [
     "Jian",
     "Wu"
    ],
    [
     "Fang",
     "Zheng"
    ]
   ],
   "title": "Reducing time-synchronous beam search effort using stage based look-ahead and language model rank based pruning",
   "original": "i00_4262",
   "page_count": 5,
   "order": 804,
   "p1": "vol. 4, 262-265",
   "pn": "",
   "abstract": [
    "In this paper, we present an efficient look-ahead technique based on both the Language Model (LM) Look-Ahead and the Acoustic Model (AM) Look-Ahead, for the time-synchronous beam search in the large vocabulary speech recognition. In this so-call stage based look-ahead (SLA) technique, two predicting processes with different hypothesis evaluating criteria are organized by stages according to the different requirements for pruning the unlikely surviving hypotheses. Furthermore, in order to reduce the efforts for distributing the LM over the lexical tree more effectively, the LM Rank based Pruning (LMRP) is integrated with the extension of each new phoneme node. The recognition experiments performed on the 50k-word Mandarin Dictation task (Easytalk2000) show that a reduction by 10 percents in the search effort in comparison with the standard word-conditioned search using LM look-ahead only, and a reduction of 25 percents in the word error rates in comparison with the search algorithm without any look-ahead can be achieved.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-801"
  },
  "chung00_icslp": {
   "authors": [
    [
     "Grace",
     "Chung"
    ]
   ],
   "title": "A three-stage solution for flexible vocabulary speech understanding",
   "original": "i00_4266",
   "page_count": 4,
   "order": 805,
   "p1": "vol. 4, 266-269",
   "pn": "",
   "abstract": [
    "This paper discusses our three-stage approach to a flexible vocabulary speech understanding system, which can detect out-ofvocabulary (OOV) words, and hypothesize their phonetic and orthographic transcriptions. In the first stage, we introduce the column-bigram finite-state transducer (FST)which, while embedding ANGIE sublexical models, also supports previously unseen data from unknown words. Secondly, the ANGIE models utilize grapheme information, providing tighter linguistic constraint as well as instantaneous sound-to-letter capability during recognition. Thirdly, the syllable-level lexical units of the first stage are automatically derived via an iterative procedure to optimize performance. The second-stage recognizer employs ANGIE to output a word network which is parsed by TINA, our natural language (NL) processor, in stage three. Experiments with a JUPITER implementation of this system are described in [1].\n",
    "",
    "",
    "G. Chung, \"Automatically Incorporating Unknown Words in JUPITER,\" in these Proceedings.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-802"
  },
  "barker00b_icslp": {
   "authors": [
    [
     "Jon",
     "Barker"
    ],
    [
     "Martin",
     "Cooke"
    ],
    [
     "Daniel P. W.",
     "Ellis"
    ]
   ],
   "title": "Decoding speech in the presence of other sound sources",
   "original": "i00_4270",
   "page_count": 4,
   "order": 806,
   "p1": "vol. 4, 270-273",
   "pn": "",
   "abstract": [
    "Conventional speech recognition is notoriously vulnerable to additive noise, and even the best compensation methods are defeated if the noise is nonstationary. To address this problem, we propose a new integration of bottom-up techniques to identify coherent fragments of spectro-temporal energy (based on local features), with the top-down hypothesis search of conventional speech recognition, extended to search also across possible assignments of each fragment as speech or interference. Initial tests demonstrate the feasibility of this approach, and achieve a reduction in word error rate of more than 25% relative at 5 dB SNR over stationary noise missing data recognition.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-803"
  },
  "lee00c_icslp": {
   "authors": [
    [
     "Shi-Wook",
     "Lee"
    ],
    [
     "Keikichi",
     "Hirose"
    ],
    [
     "Nobuaki",
     "Minematsu"
    ]
   ],
   "title": "Efficient search strategy in large vocabulary continuous speech recognition using prosodic boundary information",
   "original": "i00_4274",
   "page_count": 4,
   "order": 807,
   "p1": "vol. 4, 274-277",
   "pn": "",
   "abstract": [
    "Prosodic-syntactic boundary as an information source can be used to improve the performance of Large Vocabulary Continuous Speech Recognition (LVCSR) in both efficiency and accuracy. This paper presents a study of two effective methods to exploit prosodic boundary information in a multi-pass decoder. In this paper, we address the effect of a language model on setting pruning beam width and how to control the Cross-word Context Dependent (CCD) models by prosodic boundary information. In the first pass decoding, dynamic beam search strategy regarding inner-word and cross-word paths is proposed to reduce search space efficiently, and then cross-word context dependent models are optimized using prosodic boundary information in the second pass decoding. The recognition experiments, which were carried out on the Japanese Newspaper Article Sentences (JNAS) 20k word task using a multi-pass decoder, demonstrated that the proposed method led to significant reduction in the search space with accuracy improvement.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-804"
  },
  "yu00d_icslp": {
   "authors": [
    [
     "Ha-Jin",
     "Yu"
    ],
    [
     "Hoon",
     "Kim"
    ],
    [
     "Joon-Mo",
     "Hong"
    ],
    [
     "Min-Seong",
     "Kim"
    ],
    [
     "Jong-Seok",
     "Lee"
    ]
   ],
   "title": "Large vocabulary Korean continuous speech recognition using a one-pass algorithm",
   "original": "i00_4278",
   "page_count": 5,
   "order": 808,
   "p1": "vol. 4, 278-281",
   "pn": "",
   "abstract": [
    "In this paper, we describe problems in recognizing large-vocabulary Korean continuous speech, and proposed solutions to them. Korean sentences consist of eojeols, which are separated by spaces in text and consist of morphemes. When we use morpheme units, there are many word insertion and deletion errors because morpheme units are too short. We introduce a between-word phone variation lexicon that can represent many alternatives of phones of words in one structure. The decoding algorithm is composed of one pass, which is a modification of token-passing algorithm. In this algorithm, we allowed multiple tokens in a state at a time to get global best path without expanding the states when we use trigram language models. We confirmed that between-word phone variation lexicon is useful for morpheme-based recognition by observing that the improvement is higher for morpheme units than for eojeol units. Allowing multiple tokens at a state also improved the performance.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-805"
  },
  "seward00_icslp": {
   "authors": [
    [
     "Alexander",
     "Seward"
    ]
   ],
   "title": "A tree-trellis n-best decoder for stochastic context-free grammars",
   "original": "i00_4282",
   "page_count": 4,
   "order": 809,
   "p1": "vol. 4, 282-285",
   "pn": "",
   "abstract": [
    "In this paper a decoder for continuous speech recognition using stochastic context-free grammars is described. It forms the backbone of the ACE recognizer, which is a modular system for real-time speech recognition. A new rationale for automata is introduced, as well as a new model for pruning the search space.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-806"
  },
  "nguyen00_icslp": {
   "authors": [
    [
     "Patrick",
     "Nguyen"
    ],
    [
     "Luca",
     "Rigazio"
    ],
    [
     "Jean-Claude",
     "Junqua"
    ]
   ],
   "title": "EWAVES: an efficient decoding algorithm for lexical tree based speech recognition",
   "original": "i00_4286",
   "page_count": 4,
   "order": 810,
   "p1": "vol. 4, 286-289",
   "pn": "",
   "abstract": [
    "We present an optimized implementation of the Viterbi algorithm suitable for small to large vocabulary, and isolated or continuous speech recognition. The Viterbi algorithm is certainly the most popular dynamic programming algorithm used in speech recognition. In this paper we propose a new algorithm that outperforms the Viterbi algorithm in term of complexity and of memory requirements. It is based on the assumption of strictly left to right models and explores the lexical tree in an optimal way, such that book-keeping computation is minimized. The tree is encoded such that children of a node are placed contiguously and in increasing order of memory heap so that the proposed algorithm also optimizes cache usage. Even though the algorithm is asymptotically two times faster that the conventional Viterbi algorithm, in our experiments we measured an improvement of at least three.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-807"
  },
  "ogawa00_icslp": {
   "authors": [
    [
     "Atsunori",
     "Ogawa"
    ],
    [
     "Yoshiaki",
     "Noda"
    ],
    [
     "Shoichi",
     "Matsunaga"
    ]
   ],
   "title": "Novel two-pass search strategy using time-asynchronous shortest-first second-pass beam search",
   "original": "i00_4290",
   "page_count": 4,
   "order": 811,
   "p1": "vol. 4, 290-293",
   "pn": "",
   "abstract": [
    "In this paper, we describe a novel two-pass search strategy for large vocabulary continuous speech recognition. The first-pass of this strategy uses a regular time-synchronous beam search with rough models to generate a word lattice. Then, the second-pass search derives exact results from the word lattice using more accurate models. This search is time-asynchronous shortest- first beam search, which has two novel features: a time-asynchronous beam search mechanism using heuristics that are scores on the word lattice nodes and a strict pruning scheme using shortest-first hypothesis extension. 20k-word Japanese broadcast news recognition experiments show that our second-pass search is more accurate and more efficient than either N-best rescoring or A* search that are conventional second-pass search methods.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-808"
  },
  "chan00_icslp": {
   "authors": [
    [
     "Yu-Chung",
     "Chan"
    ],
    [
     "Manhung",
     "Siu"
    ],
    [
     "Brian",
     "Mak"
    ]
   ],
   "title": "Pruning of state-tying tree using bayesian information criterion with multiple mixtures",
   "original": "i00_4294",
   "page_count": 4,
   "order": 812,
   "p1": "vol. 4, 294-297",
   "pn": "",
   "abstract": [
    "The use of context-dependent phonetic units together with Gaussian mixture models allows modern-day speech recognizer to build very complex and accurate acoustic models. However, because of data sparseness issue, some sharing of data across different triphone states is necessary. The acoustic model design is typically done in two stages, namely, designing the state-tying map and growing the number of mixtures in each tied-state. In the design of the state-tying map, single Gaussians are used to represent the data, ignoring the fact that a single Gaussian is an insuÆcient model. In this paper, we propose a simple modification to the two-stage process by adding a third stage. In this added stage, the state-tying tree is pruned and the pruning is based on the mixture representation of the tied-states. We propose using Bayesian Information Criterion(BIC) as the criterion for this pruning and show that by adding this step, the resulting model is more compact and gives better recognition accuracy on the Resource Management(RM) task.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-809"
  },
  "liao00_icslp": {
   "authors": [
    [
     "Yuan-Fu",
     "Liao"
    ],
    [
     "Nick",
     "Wang"
    ],
    [
     "Max",
     "Huang"
    ],
    [
     "Hank",
     "Huang"
    ],
    [
     "Frank",
     "Seide"
    ]
   ],
   "title": "Improvements of the Philips 2000 Taiwan Mandarin benchmark system",
   "original": "i00_4298",
   "page_count": 4,
   "order": 813,
   "p1": "vol. 4, 298-301",
   "pn": "",
   "abstract": [
    "In this paper, we present the Philips large vocabulary continuous Mandarin speech recognition system developed for the 2000 Taiwan Speech Input Technology Assessment. We systematically integrated key Mandarin components with up-todate Western-language techniques to build up a state-of-the-art Mandarin speech recognition system. These technologies include robust pitch extraction/tone modeling, context-dependent preme/core-final units, Chinese phrase/syllable trigram language model, linear discriminant analysis (LDA), cross-syllable modeling/decoding, speaker clustering and maximum likelihood linear regression (MLLR) adaptation. Among them, the major breakthroughs were our robust pitch extraction/tone modeling technology and the treatment of coarticulation across syllable boundaries. For the development set, we dramatically reduced last years best error rates by relative 44.8%~67.8% on all three categories we participated. Moreover, for the evaluation set, we achieved the lowest unit error rates on all three categories.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-810"
  },
  "neukirchen00_icslp": {
   "authors": [
    [
     "Christoph",
     "Neukirchen"
    ],
    [
     "Xavier",
     "Aubert"
    ],
    [
     "Hans",
     "Dolfing"
    ]
   ],
   "title": "Extending the generation of word graphs for a cross-word m-gram decoder",
   "original": "i00_4302",
   "page_count": 4,
   "order": 814,
   "p1": "vol. 4, 302-305",
   "pn": "",
   "abstract": [
    "This paper introduces a method for constructing word graphs in the extended decoding framework of m-gram language models (m > 2) and cross-word HMMs. The generation of word hypotheses contained in the graph relies on a word m-tuple boundary optimization extending the word-pair approximatlon. Two variants of graph generation are proposed: the first one fully encodes the cross-word and LM constraints used in the search into the graph structure which leads to compact sized graphs. The second method constructs word graphs with lower order constraints compared to those used in the search, resulting in larger graphs with lower graph error rates. Results are presented from systematic experiments carried out on the 5k WSJ and the 64k NAB tasks.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-811"
  },
  "zhao00f_icslp": {
   "authors": [
    [
     "Qingwei",
     "Zhao"
    ],
    [
     "Zhiwei",
     "Lin"
    ],
    [
     "Baosheng",
     "Yuan"
    ],
    [
     "Yonghong",
     "Yan"
    ]
   ],
   "title": "Improvements in search algorithm for large vocabulary continuous speech recognition",
   "original": "i00_4306",
   "page_count": 4,
   "order": 815,
   "p1": "vol. 4, 306-309",
   "pn": "",
   "abstract": [
    "Current time-synchronous beam-search algorithm is improved from two aspects for speeding up large vocabulary continuous speech recognition. Single-triphone-tree structure is proposed to take instead of the tree copy technique for simplifying the search computation and saving the memory . By one kind of special-designed token propagation strategy, the n-gram language model can be integrated into the single-tree search algorithm. Moreover, a lexical tree based language model format is defined to store the pre-computed lookahead probabilities by deploying the back-off mechanism to limit the memory requirement within a manageable range, and in this way the online computation of lookahead language model can be effectively accelerated. Finally a language-independent general decoder is implemented, including English WSJ20k and Mandarin51k dictation system. Experiment results indicates that high accuracy recognition result can be attained only in the first pass by the single-triphone tree search algorithm, and search efforts can be reduced by 16% with the pre-computing lookahead LM technique.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-812"
  },
  "yu00e_icslp": {
   "authors": [
    [
     "Hua",
     "Yu"
    ],
    [
     "Takashi",
     "Tomokiyo"
    ],
    [
     "Zhirong",
     "Wang"
    ],
    [
     "Alex",
     "Waibel"
    ]
   ],
   "title": "New developments in automatic meeting transcription",
   "original": "i00_4310",
   "page_count": 4,
   "order": 816,
   "p1": "vol. 4, 310-313",
   "pn": "",
   "abstract": [
    "In this paper we report on new developments in the automatic meeting transcription task. Unlike other types of speech (such as those found in Broadcast News and Switchboard), meetings are unique in their richer dynamics of human-to-human interaction. An intuitive \"fingernail\" plot is proposed to visualize such turntaking behavior. We will also show how recognition of short turns can be improved by building a language model tailored specifi- cally for short turns. Out-Of-Vocabulary (OOV) words become a more salient problem in the meeting transcription task, as they are mostly topic words and proper names, lack of which not only causes Word Error Rate (WER) increase, but also limits further use of recognition hypotheses. We describe a prototype system which uses the Web as a source for vocabulary expansion, and present preliminary OOV retrieval results.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-813"
  },
  "pan00d_icslp": {
   "authors": [
    [
     "Jielin",
     "Pan"
    ],
    [
     "Baosheng",
     "Yuan"
    ],
    [
     "Yonghong",
     "Yan"
    ]
   ],
   "title": "Effective vector quantization for a highly compact acoustic model for LVCSR",
   "original": "i00_4318",
   "page_count": 4,
   "order": 817,
   "p1": "vol. 4, 318-321",
   "pn": "",
   "abstract": [
    "This paper introduces a method that can efficiently reduce acoustic model size and computation for LVCSR based on continuous-density hidden Mokov model (CDHMM). The method uses Bhattacharyya distance measure as a criterion to quantize the mean and variance vectors of Gaussian mixture. To minimize the quantization error, the feature vector was separated into multiple streams (such as MFCCs, delta-MFCCs and delta-delta MFCCs) and then the modified K-means clustering algorithm was applied to each stream. The key ideas of our modified K-means clustering algorithm are based on the strategy which dynamically splits and merges cluster according to its size and average distortion during each iteration for each cluster. The proposed approach can cut acoustic model size by 87% from 21.42MB to 2.75MB from a CDHMM baseline system (with 12 mixtures , 6k states) by using 256 and 8192 codewords for each stream of mean and variance vectors of Gaussian mixtures. The recognition experiment on Chinese LVCSR dictation system (of 51K words ) shows that using the 87% smaller compact model, the WER increased by 5% to 10.3% from 9.8% for the CDHMM baseline system. After quantization, the Gaussian likelihood can be pre-computed only once at the beginning of every frame and their values can be stored in a lookup table, so the computation during decoding is greatly reduced as well.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-814"
  },
  "yamamoto00d_icslp": {
   "authors": [
    [
     "Hiroki",
     "Yamamoto"
    ],
    [
     "Toshiaki",
     "Fukada"
    ],
    [
     "Yasuhiro",
     "Komori"
    ]
   ],
   "title": "Effective lexical tree search for large vocabulary continuous speech recognition",
   "original": "i00_4322",
   "page_count": 4,
   "order": 818,
   "p1": "vol. 4, 322-325",
   "pn": "",
   "abstract": [
    "In this paper, we present an e\u000ecient calculation of the factored LM probabilities for speeding up the large vocabulary continuous speech recognition. We introduced a novel technique based on the independent calculation of the factored LM probability. The basic idea of the proposed method is that each factored LM probability is calculated on-demand for a new combination of a previous word hypothesis and a LM look-ahead tree node, instead of calculating all the factored LM probabilities over the tree at a time. The speaker-independent continuous speech recognition experiment was performed for 20 speakers on a 60k word newspaper dictation task. As a result, the proposed method achieved 25% improvement in speed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-815"
  },
  "hori00_icslp": {
   "authors": [
    [
     "Chiori",
     "Hori"
    ],
    [
     "Sadaoki",
     "Furui"
    ]
   ],
   "title": "Improvements in automatic speech summarization and evaluation methods",
   "original": "i00_4326",
   "page_count": 4,
   "order": 819,
   "p1": "vol. 4, 326-329",
   "pn": "",
   "abstract": [
    "This paper proposes an improved method of summarizing speech in which a confidence measure of a word hypothesis is incorporated in the summarization score and also proposes a new method for evaluating the summarized sentences. The automatically summarized sentences were evaluated based on the precision of extracted keywords and each word string with a certain length in the manual summarizations by human subjects. Japanese broadcast-news speech transcribed using a large-vocabulary continuous-speech recognition (LVCSR) system was summarized using our proposed method. Experimental results show that a confidence score giving a penalty for acoustically as well as linguistically unreliable hypotheses can reduce the meaning alteration of summarizations caused by recognition errors especially when the speech recognition rate is relatively low.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-816"
  },
  "chang00c_icslp": {
   "authors": [
    [
     "Shuangyu",
     "Chang"
    ],
    [
     "Lokendra",
     "Shastri"
    ],
    [
     "Steven",
     "Greenberg"
    ]
   ],
   "title": "Automatic phonetic transcription of spontaneous speech (american English)",
   "original": "i00_4330",
   "page_count": 4,
   "order": 820,
   "p1": "vol. 4, 330-333",
   "pn": "",
   "abstract": [
    "An automatic transcription system has been developed to label and segment phonetic constituents of spontaneous American English without benefit of a word-level transcript. Instead, special-purpose neural networks classify each 10-ms frame of speech in terms of articulatory-acoustic-based phonetic features and the feature clusters are subsequently mapped to phonetic-segment labels using multilayer perceptron networks. The phonetic labels generated by this system are 80% concordant with the labels produced by human transcribers and the segmental boundaries deviate from manual segmentation by an average of 11 ms. The automatic transcription system thus generates phonetic labels and segmentation comparable in quality to those produced by human transcribers, and therefore may prove useful for phonetic annotation of novel linguistic corpora, as well as facilitating development of pronunciation models for automatic speech recognition systems.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-817"
  },
  "novak00_icslp": {
   "authors": [
    [
     "Miroslav",
     "Novak"
    ],
    [
     "Michael",
     "Picheny"
    ]
   ],
   "title": "Speed improvement of the tree-based time asynchronous search",
   "original": "i00_4334",
   "page_count": 4,
   "order": 821,
   "p1": "vol. 4, 334-337",
   "pn": "",
   "abstract": [
    "The IBM large vocabulary continuous speech recog- nition system is based on an asynchronous stack de- coding scheme. This is essentially a tree search, as described in [1]. The main advantages - e\u000ecient mem- ory utilization and a single-pass search strategy - make the system extremely suitable for real-time applica- tions. This article describes further improvements in e\u000eciency of the search method. These improvements are achieved in part by more e\u000ecient word to con- text dependent acoustic model expansion, producing equivalent search results and thus not affecting the recognition accuracy. Additional improvements are achieved by introducing an approximation in the com- putation of the likelihood of the hypothesized path. The basic idea is to allow sharing of some branches in the search tree and results in effectively a tree to network transformation.\n",
    "",
    "",
    "Rabiner, L.R., Juang, B.H., \\An introduction to hidden Markov Models\", IEEE ASSP Mag., Vol. 3, pp. 4-16, Jan. 1986\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-818"
  },
  "huang00f_icslp": {
   "authors": [
    [
     "Jing",
     "Huang"
    ],
    [
     "B.",
     "Kingsbury"
    ],
    [
     "L.",
     "Mangu"
    ],
    [
     "Mukund",
     "Padmanabhan"
    ],
    [
     "George",
     "Saon"
    ],
    [
     "Geoffrey",
     "Zweig"
    ]
   ],
   "title": "Recent improvements in speech recognition performance on large vocabulary conversational speech (voicemail and switchboard)",
   "original": "i00_4338",
   "page_count": 4,
   "order": 822,
   "p1": "vol. 4, 338-341",
   "pn": "",
   "abstract": [
    "In this paper we report recent improvements in word error performance on a voicemail transcription task. Last year, the speaker independent word error rate (WER) on the dev test set of the Voicemail Transcription task was reported at 35.45% [1]. This year, we report a relative 20% gain over this number. The improvements were obtained using several new algorithms and an increased amount of training data. In addition to benchmarking the performance of these algorithms on the Voicemail task, we have also evaluated them on the Switchboard task, and we report these results here as well. Finally, we also present the result of crossdomain experiments to evaluate the domain-independence of the constructed systems.\n",
    "",
    "",
    "M. Padmanabhan, G. Saon, S. Basu, J. Huang, G. Zweig, \"Recent improvements on a Voicemail Transcription Task\", Proceedings of Eurospeech 1999.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-819"
  },
  "he00_icslp": {
   "authors": [
    [
     "Lei",
     "He"
    ],
    [
     "Ditang",
     "Fang"
    ],
    [
     "Wenhu",
     "Wu"
    ]
   ],
   "title": "Speaker normalization training and adaptation for speech recognition",
   "original": "i00_4342",
   "page_count": 4,
   "order": 823,
   "p1": "vol. 4, 342-345",
   "pn": "",
   "abstract": [
    "This paper presents a speaker adaptation framework that combines the speaker normalization (SN) training. Because of the varieties among training speakers, more data are required in training and adaptation of speaker independent (SI) acoustic model. In this paper, a very simple but effective normalization method is presented, in which the distortions among different speakers are removed by subtracting the state-relative shift vectors between SI model and speaker dependent (SD) model. In adaptation stage, MAP estimation is used to update the models with adaptation data, and the interpolation of unseen models and smoothing of the final models are implemented by orderalterable weighted neighbor regression (WNR) method. In Mandarin syllable recognition task, with equal adaptation data, SN model as seed model makes a 5%-15% additional reduction in error rate comparing with SI model as seed model.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-820"
  },
  "tomokiyo00c_icslp": {
   "authors": [
    [
     "Laura Mayfield",
     "Tomokiyo"
    ]
   ],
   "title": "Lexical and acoustic modeling of non-native speech in LVSCR",
   "original": "i00_4346",
   "page_count": 4,
   "order": 824,
   "p1": "vol. 4, 346-349",
   "pn": "",
   "abstract": [
    "As non-native speakers become more frequent users of speech recognition applications, increasing the tolerance of the system with respect to non-native pronunciation and language use is important and is currently the focus of research in a variety of contexts. Dictionary modification, acoustic model adaptation, and acoustic model manipulation are a few of the techniques that have been reported successful in improving recognition of non-native speech. In this paper, we address the specific case of Japanese-accented English, describing the lexical and acoustic modeling techniques that give the best recognizer performance. We find that automatically generated pronunciation variants perform as well as hand-coded \"golden\" variants in reducing recognizer error, and that a significant improvement in system performance can be achieved with acoustic models retrained on a small amount of accented data.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-821"
  },
  "li00l_icslp": {
   "authors": [
    [
     "Baojie",
     "Li"
    ],
    [
     "Keikichi",
     "Hirose"
    ],
    [
     "Nobuaki",
     "Minematsu"
    ]
   ],
   "title": "Modeling phone correlation for speaker adaptive speech recognition",
   "original": "i00_4350",
   "page_count": 4,
   "order": 825,
   "p1": "vol. 4, 350-353",
   "pn": "",
   "abstract": [
    "Information of phone relationships is regarded as acting an important role in speech recognition. It has been successfully exploited in many speaker adaptation approaches. In this paper, we propose a new approach, named Phone Pair Model (PPM) re-scoring, to utilize phone relationships for speaker-adaptive speech recognition. PPM re-scoring approach does not really adapt model parameters to a new speaker. It just uses some pre-registered phones' samples from the speaker being recognized, to re-calculate the likelihood of phones that has been calculated on conventional phone HMMs, resulting in a more correct recognition result. Additionally, it can deal with not only inter-speaker acoustic variations but also intra-speaker acoustic variations adequately. Results of two recognition experiments, one using phone HMMs only and the other incorporating phone HMMs with the PPMs, showed that even by using only a few vowel samples as the pre-registered phones, PPM re-scoring approach brought an increase in recognition rate.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-822"
  },
  "botterweck00_icslp": {
   "authors": [
    [
     "Henrik",
     "Botterweck"
    ]
   ],
   "title": "Very fast adaptation for large vocabulary continuous speech recognition using eigenvoices",
   "original": "i00_4354",
   "page_count": 4,
   "order": 826,
   "p1": "vol. 4, 354-357",
   "pn": "",
   "abstract": [
    "The principle of the eigenvoice method - using a priori knowledge on the speaker variability as collected during the training for a very fast adaptation - is applied to continuous speech recognition with large vocabulary. The handling of mixture density HMM models is discussed. For the case of gender independent models, a decrease of the word error rate of up to 15% is observed for unsupervised adaptation and even the first recognized phonemes lead to considerable improvements. The first two eigenvectors of adaptation can be characterized as classifying the gender and the recording environment. Comparisons of the method with MLLR are done as far as the latter is applicable at all.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-823"
  },
  "zheng00d_icslp": {
   "authors": [
    [
     "Chengyi",
     "Zheng"
    ],
    [
     "Yonghong",
     "Yan"
    ]
   ],
   "title": "Efficiently using speaker adaptation data",
   "original": "i00_4358",
   "page_count": 4,
   "order": 827,
   "p1": "vol. 4, 358-361",
   "pn": "",
   "abstract": [
    "Transformation based speaker adaptation techniques, such as Maximum Likelihood Linear Regression (MLLR) [1] require a large amount of adaptation data to robustly estimate the transform matrices. In this paper, we present a new adaptation scheme that adjusts the adaptation data according to the feedback from recognizer. By giving different weights to different parts of the adaptation data, the proposed scheme can make use of the adaptation data more efficiently. Experiments on the WSJ 20K task show that this method achieved an additional 10% relative word error rate reduction in supervised adaptation and 2% reduction in unsupervised adaptation compared with conventional MLLR approach.\n",
    "",
    "",
    "C.L. Leggetter and P.C. Woodland. Maximum likelihood linear regression for speaker adaptation of continuous density HMMs, Computer Speech and Language, Vol.9, pp. 171-185, 1995.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-824"
  },
  "pfau00_icslp": {
   "authors": [
    [
     "Thilo",
     "Pfau"
    ],
    [
     "Robert",
     "Faltlhauser"
    ],
    [
     "Günther",
     "Ruske"
    ]
   ],
   "title": "A combination of speaker normalization and speech rate normalization for automatic speech recognition",
   "original": "i00_4362",
   "page_count": 4,
   "order": 828,
   "p1": "vol. 4, 362-365",
   "pn": "",
   "abstract": [
    "In this contribution a normalization procedure for automatic speech recognition is introduced which aims at reducing speaking rate specific variations of the features of the phonetic classes. A \"spurtwise\" calculation of normalization factors allows to capture changes of the speaking rate within one utterance. The costsaving implementation using linear interpolation of the original features and a word graph rescoring procedure leads to a moderate increase in computational load compared to the baseline system without speech rate normalization.\n",
    "In addition a two-step procedure which combines vocal tract length normalization (VTLN) and speech rate normalization (SRN) has been developed. Experiments showed, that applying SRN to a VTLN-based recognition system leads to relative reduction in word error rate of 4.2%. This is comparable to the decrease observed when using SRN on a system without VTLN. All in all the combination of VTLN and SRN results in a 15% reduction of word error rate compared to the baseline system.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-825"
  },
  "hwang00_icslp": {
   "authors": [
    [
     "Tai-Hwei",
     "Hwang"
    ],
    [
     "Kuo-Hwei",
     "Yuo"
    ],
    [
     "Hsiao-Chuan",
     "Wang"
    ]
   ],
   "title": "Speech model compensation with direct adaptation of cepstral variance to noisy environment",
   "original": "i00_4366",
   "page_count": 4,
   "order": 829,
   "p1": "vol. 4, 366-369",
   "pn": "",
   "abstract": [
    "A modified parallel model combination (PMC) for noisy speech recognition is proposed such that both speech cepstral mean and variance are adapted without the mapping of variance between cepstral and log-spectral domains. By investigating an adapted scalar random variable of log-energy in the way of PMC, we observe that the adapted variance of log-energy can be roughly predicted by the energy ratio of source signals. Based on the observation, we propose that the cepstral variance of the adapted model can be approximated according to the local signal-to-noise ratio (SNR) of a state. The combined cepstral variance is then assigned to be the variance of clean speech, the variance of noise, or the average variance of clean speech and noise. The performance of using this approximation method is compared with the original PMC. Our experiment shows that the degradation of the performance is small, but the proposed method has greatly reduced the computational cost as comparing with the PMC method.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-826"
  },
  "wu00h_icslp": {
   "authors": [
    [
     "Ji",
     "Wu"
    ],
    [
     "Zuoying",
     "Wang"
    ]
   ],
   "title": "Gaussian similarity analysis and its application in speaker adaptation",
   "original": "i00_4370",
   "page_count": 4,
   "order": 830,
   "p1": "vol. 4, 370-373",
   "pn": "",
   "abstract": [
    "A good similarity measure of random variables is crucial in many applications. The choice of distance measure directly affects quality of system design. In this paper, we present a new measure of the similarity between two random variables. The discussion here emphasizes on the case of normal distribution. Based on this Gaussian Similarity Analysis (GSA), we propose an algorithm in speaker adaptation of covariance. It is different from the traditional algorithms, which mainly focus on the adaptation of mean vector of state observation probability density. A binary decision tree is constructed offline with the similarity measure and the adaptation procedure is data-driven. It can be shown from the experiments that we can get a significant further improvement over the mean vectors adaptation.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-827"
  },
  "itoh00b_icslp": {
   "authors": [
    [
     "Nobuyasu",
     "Itoh"
    ],
    [
     "Masafumi",
     "Nishimura"
    ],
    [
     "Shinsuke",
     "Mori"
    ]
   ],
   "title": "A method for style adaptation to spontaneous speech by using a semi-linear interpolation technique",
   "original": "i00_4374",
   "page_count": 4,
   "order": 831,
   "p1": "vol. 4, 374-377",
   "pn": "",
   "abstract": [
    "This paper deals with a method for adapting a language model created from written-text corpora to spontaneous speech by using a semi-linear interpolation technique. Sizes and topic coverages of spoken language corpora are usually far smaller those of written-text corpora. We propose an approach to adapt a base language model to the styles of spontaneous speech on the basis of the following assumptions. The words that are topic-independent, that is to say, common in spontaneous speech should be predicted mainly by a model created from spontaneous speech corpora (style model), while the base model is more reliable for predicting topic-related words, because they are di\u000ecult to predict from a model based on a small corpus. We classified all words into disfluencies and normal words. The normal words are classified into two more categories; common words and topic words according to mutual information. For each category, the qualified models (base or style) with the optimal weights for linear interpolation are selected. In other words, a different linear combination of the models is used for each category of a predicted word. We conducted experiments by using a spoken-language corpus of Japanese for creating the style model. We achieved 159.1 in test-set perplexity compared with the baseline of 189.3 (simple linear interpolation) and the perplexity of the style specific model, which was 230.7.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-828"
  },
  "geutner00_icslp": {
   "authors": [
    [
     "Petra",
     "Geutner"
    ],
    [
     "Luis",
     "Arevalo"
    ],
    [
     "Joerg",
     "Breuninger"
    ]
   ],
   "title": "VODIS - voice-operated driver information systems: a usability study on advanced speech technologies for car environments",
   "original": "i00_4378",
   "page_count": 4,
   "order": 832,
   "p1": "vol. 4, 378-382",
   "pn": "",
   "abstract": [
    "This paper gives an overview over the progression and results of the VODIS (Voice-operated Driver Information Systems) project, a EU-funded project with participation of many industrial and academic partners within Europe. It describes the architecture and functionality of a driver information prototype system that is able to control telephone and audio devices, but also goes beyond these applications by enabling a potential user of the system to enter a destination within the navigation context by speech. The developed demonstrator allows the speaker-independent input of up to 70 predefined command words, phrases and even dynamically generated names (e.g. radio stations, phonebook entries) for hands- and eyes-free voice operation of the car-infotainment functions mentioned above. A prototype system has been realized in four European languages: German, French, English and Italian. User evaluations on driver level under realistic conditions have been conducted with this system and the results are presented in this paper.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-829"
  },
  "chou00_icslp": {
   "authors": [
    [
     "Wu",
     "Chou"
    ],
    [
     "Qiru",
     "Zhou"
    ],
    [
     "Hong-Kwang Jeff",
     "Kuo"
    ],
    [
     "Antoine",
     "Saad"
    ],
    [
     "David",
     "Attwater"
    ],
    [
     "Peter",
     "Durston"
    ],
    [
     "Mark",
     "Farrell"
    ],
    [
     "Frank",
     "Scahill"
    ]
   ],
   "title": "Natural language call steering for service applications",
   "original": "i00_4382",
   "page_count": 4,
   "order": 833,
   "p1": "vol. 4, 382-385",
   "pn": "",
   "abstract": [
    "In this paper, a dialogue system for natural language based call steering is described and studied. The system is based on natural language speech recognition and understanding within a mixed initiative dialogue. The system is implemented on Bell Labs. Speech Technology Integration Platform (BLSTIP) using dialogue and natural language understanding components from BT laboratories. A prototype system in the operator service domain [2] is described. In order to improve the acoustic and language modeling for natural language based dialogue applications, various approaches are described and studied. The structure of the dialogue manager is also presented in which mixed-initiative dialogue can be supported with efficiency. Call classification and steering experiments were performed. The results confirm the efficacy of the proposed approach.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-830"
  },
  "hunsinger00_icslp": {
   "authors": [
    [
     "Jörg",
     "Hunsinger"
    ],
    [
     "Manfred",
     "Lang"
    ]
   ],
   "title": "A single-stage top-down probabilistic approach towards understanding spoken and handwritten mathematical formulas",
   "original": "i00_4386",
   "page_count": 4,
   "order": 834,
   "p1": "vol. 4, 386-389",
   "pn": "",
   "abstract": [
    "We present a novel approach towards a multimodal analysis of natural speech and handwriting input for entering mathematical expressions into a computer. It utilizes an integrated, multilevel probabilistic architecture with a joint semantic and two distinct syntactic models describing speech and script properties, respectively. Compared to classical multistage solutions our single-stage strategy benefits from an implicit transfer of higher level contextual information into the lower level segmentation and pattern recognition processes involved. For visualization and postprocessing purposes, a transformation into Adobe® FrameMaker® documents is performed. Fully spoken or handwritten realistic formulas were examined, yielding a structural recognition accuracy of 61.1 % for speech (speaker independent) and 83.3 % for handwriting (writer dependent).\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-831"
  },
  "raghavan00_icslp": {
   "authors": [
    [
     "Prabhu",
     "Raghavan"
    ],
    [
     "Sunil K.",
     "Gupta"
    ]
   ],
   "title": "Low complexity connected digit recognition for mobile applications",
   "original": "i00_4390",
   "page_count": 4,
   "order": 835,
   "p1": "vol. 4, 390-393",
   "pn": "",
   "abstract": [
    "For low complexity, mobile, hands-free, speaker independent connected digit recognition, a fixed-point digital signal processor based implementation is essential. In this paper, we investigate algorithms for connected-digit recognition using whole-word digit models and a background model. We show that significant improvement can be achieved by using background model adaptation, continuously adaptive separate cepstral mean subtraction for background and speech segments and discriminative training. The system achieves almost 96% digit accuracy on a 15 speaker database of speech recorded in a car. A real-time system using the Lucent's DSP1627 has also been developed. We also present the results of our experiments in reducing complexity for the fixed-point system. These include a method to approximate state-likelihood computation using a Vector Quantization based mixture selection and use of beam width pruning during Viterbi decoding.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-832"
  },
  "nouza00_icslp": {
   "authors": [
    [
     "Jan",
     "Nouza"
    ]
   ],
   "title": "Telephone speech recognition from large lists of Czech words",
   "original": "i00_4394",
   "page_count": 5,
   "order": 836,
   "p1": "vol. 4, 394-397",
   "pn": "",
   "abstract": [
    "In the paper we investigate methods suitable for practical implementation in a recognition system that is to classify telephone input in form of isolated words/phrases belonging to large vocabularies with equiprobable entries, such as people names, city and local names, etc. Specifically for Czech language we propose a pronunciation lexicon with a prefix-stem-sufix arrangement combined with appropriate caching and pruning techniques and a 2-level (monophone and triphone) based classification. In experiments done with telephone speech containing items from a 5347-word city-name vocabulary we obtained 90.1 % recognition score in average time 645 ms per word. Acoustic models for these experiments have been trained on an only available multi-speaker database that was originally recorded by a microphone and later transferred over telephone lines and automatically realigned.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-833"
  },
  "wu00i_icslp": {
   "authors": [
    [
     "Duanpei",
     "Wu"
    ],
    [
     "X.",
     "Menendez-Pidal"
    ],
    [
     "L.",
     "Olorenshaw"
    ],
    [
     "R.",
     "Chen"
    ],
    [
     "M.",
     "Tanaka"
    ],
    [
     "M.",
     "Amador"
    ]
   ],
   "title": "Speech and word detection algorithms for hands-free applications",
   "original": "i00_4398",
   "page_count": 4,
   "order": 837,
   "p1": "vol. 4, 398-401",
   "pn": "",
   "abstract": [
    "This paper describes a robust speech detection algorithm for speech-activated hands-free applications. The system consists of three techniques: (1) noise suppression with efficient implementation, (2) robust endpoint detection and (3) speech verification using garbage modeling and confidence measure. With efficient implementation, noise suppression improves the SNR by roughly 10-20 dB. The endpoint detection uses the technique described in [1] with improvement for non-stationary noise. Garbage modeling and confidence measure are used to handle out-of-vocabulary (OOV) words and background pulse noise.\n",
    "",
    "",
    "Wu, D., M. Tanaka, R. Chen and L. Olorenshaw, \"A Robust Endpoint Detection Algorithm for Speech Recognition in Cars\" Proceedings-97 of Sony Research Forum, Tokyo, 1997.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-834"
  },
  "rao00_icslp": {
   "authors": [
    [
     "Ashwin",
     "Rao"
    ],
    [
     "Bob",
     "Roth"
    ],
    [
     "Venkatesh",
     "Nagesha"
    ],
    [
     "Don",
     "McAllaster"
    ],
    [
     "Natalie",
     "Liberman"
    ],
    [
     "Larry",
     "Gillick"
    ]
   ],
   "title": "Large vocabulary continuous speech recognition of read speech over cellular and landline networks",
   "original": "i00_4402",
   "page_count": 4,
   "order": 838,
   "p1": "vol. 4, 402-405",
   "pn": "",
   "abstract": [
    "We report results of large vocabulary continuous speech recognition (LVCSR) experiments, conducted using speech data read over cellular and landline phones. Specifically, we compare (using stereo recordings) the speaker-independent and speakeradapted recognition word error rates (WERs) measured over cellular and landline networks, with those measured using a closetalking noise-canceling headset microphone, which serves as a baseline. A test set consisting of speech data recorded by 25 speakers is used; each speaker providing test and adaptation data. We use acoustic models trained from relatively high-quality training data and an interpolated trigram language model. Some insights into the relative degradation in WERs over telephone networks are also provided by examining the recognition error rates for bandlimited and coded microphone speech.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-835"
  },
  "yamamoto00e_icslp": {
   "authors": [
    [
     "Seiichi",
     "Yamamoto"
    ]
   ],
   "title": "Toward speech communications beyond language barrier - research of spoken language translation technologies at ATR -",
   "original": "i00_4406",
   "page_count": 6,
   "order": 839,
   "p1": "vol. 4, 406-411",
   "pn": "",
   "abstract": [
    "In the coming 21st century, the demand for global communications among speakers of different languages is expected to grow, and consequently, speech translation technologies will be indispensable for such global communications. Based on this reasoning, research on speech translation technologies has been progressing for about 20 years at Advanced Telecommunications Research Institute (ATR) in Japan and a part of these technologies are entering into the stage of practical use. The paper describes the research activities of speech translation technologies at ATR Interpreting Telecommunications Research Labs, and future research plans of ATR Spoken Language Translation Research Labs.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-836"
  },
  "blanchon00_icslp": {
   "authors": [
    [
     "Hervé",
     "Blanchon"
    ],
    [
     "Christian",
     "Boitet"
    ]
   ],
   "title": "Speech translation for French within the c-STAR II consortium and future perspectives",
   "original": "i00_4412",
   "page_count": 6,
   "order": 840,
   "p1": "vol. 4, 412-417",
   "pn": "",
   "abstract": [
    "Despite joining the C-STAR II consortium in late 1996, the CLIPS++ group succeeded in building the French parts of a multilingual task-oriented spoken dialogue translation system and took part in multilingual, intercontinental demonstrations held on July 22nd 1999 by CLIPS (France), CMU (United States), ETRI (South Korea), ATR (Japan), IRST (Italy), and UKA (Germany). The challenge was to reach the minimum quality level adequate for handling specific tasks, which is quite higher than what is sufficient for casual chatting and can be achieved by putting together commercially available components.\n",
    "After presenting the modules and the architecture of our C-STAR II demonstrator, we evaluate the results, both externally and internally. While the reactions to the final demonstrations were very positive, and many said that these prototypes should quickly lead to products, we feel that there is still much room for improving the overall quality in significant ways. In the last part, we focus on future avenues of research to further improve the quality of task-oriented speech translation, in particular by defining a more powerful and orthogonal taskoriented semantic pivot, using the linguistic and dialogic context, and generating information usable by speech synthesis to generate better prosody.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-837"
  },
  "zong00_icslp": {
   "authors": [
    [
     "Chengqing",
     "Zong"
    ],
    [
     "Yumi",
     "Wakita"
    ],
    [
     "Bo",
     "Xu"
    ],
    [
     "Zhenbiao",
     "Chen"
    ],
    [
     "Kenji",
     "Matsui"
    ]
   ],
   "title": "Japanese-to-Chinese spoken language translation based on the simple expression",
   "original": "i00_4418",
   "page_count": 4,
   "order": 841,
   "p1": "vol. 4, 418-421",
   "pn": "",
   "abstract": [
    "This paper describes a Japanese-to-Chinese spoken language translation (SLT) method based on simple expression and presents the experimental results. The method is aimed at developing a compact speech translation system, which is robust for spontaneous spoken language phenomena, including the recognition errors and different expression from various speakers. The idea of translation method based on simple expression is that the mechanism interprets speech-act rather than the direct translation of the speakers words. The method is realized by mapping the simple expression instead of deep parsing. In this method, only keywords in speech recognition results are extracted, and the corresponding target sentences are extracted from the database by selecting similar example sentences with the keywords. All training sentences in the bilingual corpus are rewritten into the simple expression and grouped by rules. The rules are specially designed to develop the compact size example database for translation. Comparing with other example-based approaches to SLT, the method based on simple expression is easy to realize, and especially it is practical to develop the SLT systems limited in specific domains.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-838"
  },
  "bangalore00b_icslp": {
   "authors": [
    [
     "Srinivas",
     "Bangalore"
    ],
    [
     "Giuseppe",
     "Riccardi"
    ]
   ],
   "title": "Finite-state models for lexical reordering in spoken language translation",
   "original": "i00_4422",
   "page_count": 4,
   "order": 842,
   "p1": "vol. 4, 422-425",
   "pn": "",
   "abstract": [
    "The problem of machine translation can be viewed as consisting of two phases: (a) lexical choice phase where appropriate target language lexical items (words or phrases) are chosen for each source language lexical item and (b) reordering phase where the chosen target language lexical items are reordered to produce a meaningful target language string. In earlier work we have shown that finite-state models for lexical choice can be learned from bilingual corpora [1]. In this paper, we focus on stochastic finite-state models for lexical reordering and describe an algorithm to learn them from bilingual corpora. We have developed a stochastic finite-state English-Japanese translation system by composing finite-state lexical choice and lexical reordering model. We have evaluated it using the string edit distance of the translated string from a given reference string. Using this metric, the English-Japanese translation system scored 70.9% on English speech transcriptions.\n",
    "",
    "",
    "Srinivas Bangalore and Giuseppe Riccardi. Stochastic finite-state models for spoken language machine translation. In Proceedings of the Workshop on Embedded Machine Translation Systems, pages 52-59, 2000.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-839"
  },
  "engel00_icslp": {
   "authors": [
    [
     "Ralf",
     "Engel"
    ]
   ],
   "title": "CHUNKY: an example based machine translation system for spoken dialogs",
   "original": "i00_4426",
   "page_count": 4,
   "order": 843,
   "p1": "vol. 4, 426-429",
   "pn": "",
   "abstract": [
    "This paper presents an experimental example based machine translation system for spoken German-English dialogs. The system consists of a syntactic analysis, a module for automatic alignment of sentence pairs, a translation module and a generation module. The translation module splits the input sentence into parts and finds the best translations for these parts using the sample corpus and a distance function considering context.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-840"
  },
  "lazzari00_icslp": {
   "authors": [
    [
     "Gianni",
     "Lazzari"
    ]
   ],
   "title": "Spoken translation: challenges and opportunities",
   "original": "i00_4430",
   "page_count": 6,
   "order": 844,
   "p1": "vol. 4, 430-435",
   "pn": "",
   "abstract": [
    "In this paper the research issues and approach to the spoken translation problem will be presented and discussed in the framework of new opportunities offered to multilingual person to person communication on the Web. New challenges arise in the research paradigm due to the availability of huge multimedia content that can be used in the communication process. The NESPOLE! project, a common EU NSF funded project exploring future applications in the e-commerce and e-service sectors, will also be presented and discussed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-841"
  },
  "boitet00_icslp": {
   "authors": [
    [
     "Christian",
     "Boitet"
    ],
    [
     "Jean-Philippe",
     "Guilbaud"
    ]
   ],
   "title": "Analysis into a formal task-oriented pivot without clear abstract - semantics is best handled as \"usual\" translation",
   "original": "i00_4436",
   "page_count": 4,
   "order": 845,
   "p1": "vol. 4, 436-439",
   "pn": "",
   "abstract": [
    "During the development of a multilingual international demo implemented by the CSTAR consortium, five of the six partners have adopted a task-oriented (task and domain specific) pivot architecture. To add French to the system, we have developed, among other components, an analyzer which converts written utterances, coming from a speech recognition system, into the pivot language known as IF (Interchange Format). Perhaps paradoxically, the natural character of the relevant utterances and the lack of formal semantics in the resulting IF structures has led us to construct this analyzer as if we were translating between two poorly defined natural languages. We will describe how we have used the Ariane-G5 environment while adopting a technique inspired both by the example-based machine translation paradigm and by older, \"semantic\" machine translation approaches.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-842"
  },
  "zong00b_icslp": {
   "authors": [
    [
     "Chengqing",
     "Zong"
    ],
    [
     "Taiyi",
     "Huang"
    ],
    [
     "Bo",
     "Xu"
    ]
   ],
   "title": "An improved template-based approach to spoken language translation",
   "original": "i00_4440",
   "page_count": 5,
   "order": 846,
   "p1": "vol. 4, 440-443",
   "pn": "",
   "abstract": [
    "In this paper, we describe an improved template-based approach to Chinese-to-English Spoken Language Translation (SLT) and present experimental results. The improved template-based translation approach uses flexible expression format to describe the template condition. The condition of a template may consist of keywords, parts-of-speech and also semantic features, so the input may be matched with a template from shallow level to deep level. In the condition of a template, the distance between two fixed keywords is stretchable, thus some needless words in the input utterances may be skipped in matching operation. And also the translation results of the same template are alterable. The proper results are finally generated according to the specific context. That is, the relation between a template and translated utterance is one-to-n (where, n is an integer and n³1). The experiments were performed with input of both text transcription and results of speech recognition. The preliminary experimental results have proven the approach is practical.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-843"
  },
  "watanabe00c_icslp": {
   "authors": [
    [
     "Takao",
     "Watanabe"
    ],
    [
     "Akitoshi",
     "Okumura"
    ],
    [
     "Shinsuke",
     "Sakai"
    ],
    [
     "Kiyoshi",
     "Yamabana"
    ],
    [
     "Shinichi",
     "Doi"
    ],
    [
     "Ken",
     "Hanazawa"
    ]
   ],
   "title": "An automatic interpretation system for travel conversation",
   "original": "i00_4444",
   "page_count": 4,
   "order": 847,
   "p1": "vol. 4, 444-447",
   "pn": "",
   "abstract": [
    "We have developed an automatic interpretation system running on a mobile PC that helps oral communication between Japanese and English speakers in various situations during their travel abroad. In order to allow a wide range of expressions and topics in the applied domain, we adopted an approach which utilizes the general linguistic knowledge as well as the domainspecific linguistic knowledge. Speech recognition module performs speaker-independent large-vocabulary (50,000 Japanese words and 10,000 English words) continuous speech recognition. Translation module performs syntax directed translation based on a new lexicalized grammar rule formalism.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-844"
  },
  "gruhn00_icslp": {
   "authors": [
    [
     "Rainer",
     "Gruhn"
    ],
    [
     "Harald",
     "Singer"
    ],
    [
     "Hajime",
     "Tsukada"
    ],
    [
     "Masaki",
     "Naito"
    ],
    [
     "Atsushi",
     "Nishino"
    ],
    [
     "Atsushi",
     "Nakamura"
    ],
    [
     "Yoshinori",
     "Sagisaka"
    ],
    [
     "Satoshi",
     "Nakamura"
    ]
   ],
   "title": "Cellular-phone based speech-to-speech translation system ATR-MATRIX",
   "original": "i00_4448",
   "page_count": 4,
   "order": 848,
   "p1": "vol. 4, 448-451",
   "pn": "",
   "abstract": [
    "We describe the implementation of a cellular-phone based speech translation system without telephone quality speech database or special CT hardware. The purpose is to quickly build a prototype service system that can be used for data collection with real users. To train the acoustic model for the speech recognition system, available high-quality databases were made usable by 1.) appropriate downsampling and filtering of high-quality databases, and 2.) by piping, similar to the NTIMIT and CTIMIT paradigms. An evaluation of acoustic models with filtered, piped and real cellular-phone data is given. Recognition rates are at same levels as for wideband speech.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-845"
  },
  "beringer00_icslp": {
   "authors": [
    [
     "Nicole",
     "Beringer"
    ],
    [
     "Tsuyoshi",
     "Ito"
    ],
    [
     "Marcia",
     "Neff"
    ]
   ],
   "title": "Generation of pronunciation rule sets for automatic segmentation of American English and Japanese",
   "original": "i00_4452",
   "page_count": 4,
   "order": 849,
   "p1": "vol. 4, 452-455",
   "pn": "",
   "abstract": [
    "The goal of this paper is to create an extended rule corpus with approximately 2366 phonetic rules which model segmental variation on a three language task. The phonetic rules express at a broad phonetic level phenomena of phonetic reduction in German, English and Japanese that occur within words and across word boundaries. In order to get an improvement in automatic segmentation of regional speech variants, these rules are clustered and implemented depending on language specification in the Munich Automatic Segmentation System.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-846"
  },
  "samudravijaya00_icslp": {
   "authors": [
    [
     "K.",
     "Samudravijaya"
    ],
    [
     "P. V. S.",
     "Rao"
    ],
    [
     "S. S.",
     "Agrawal"
    ]
   ],
   "title": "Hindi speech database",
   "original": "i00_4456",
   "page_count": 4,
   "order": 850,
   "p1": "vol. 4, 456-459",
   "pn": "",
   "abstract": [
    "The design and development of an annotated and time-aligned speech database for Hindi language is described here. Although this continuous speech database is principally intended for training of a speech recognition system for Hindi, the design specifications of the database are general so that it can also be useful in tasks such as speaker recognition, study of acousticphonetic correlates of the language. The database consists of a total of 500 sentences spoken by 50 speakers. There are two sets of sentences. The first set of 2 sentences (containing most Hindi phonemes) was read by each and every speaker. The second sets of sentences (8 distinct sentences per speaker) were designed such that they collectively cover most phonemic contexts. The database is comprehensive enough to effectively capture phonetic, acoustic, intra-speaker and inter-speaker variabilities in Hindi speech. The speech data was simultaneously recorded using a close talking microphone and another desktop \"far field\" microphone. The former speech data was manually segmented and labeled in terms of sub-phonetic units by trained personnel.\n",
    "The database was used to conduct a study of the prosodic characteristics of the Hindi vowels. There are five pairs of vowels in Indian languages; one member is longer in duration than the other. It was observed that native speakers of Hindi seem to give more importance to the duration attribute to contrast vowels in a vowel pair than non native speakers.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-847"
  },
  "wang00q_icslp": {
   "authors": [
    [
     "Hsiao-Chuan",
     "Wang"
    ],
    [
     "Frank",
     "Seide"
    ],
    [
     "Chiu-Yu",
     "Tseng"
    ],
    [
     "Lin-Shan",
     "Lee"
    ]
   ],
   "title": "MAT-2000 - design, collection, and validation of a Mandarin 2000-speaker telephone speech database",
   "original": "i00_4460",
   "page_count": 4,
   "order": 851,
   "p1": "vol. 4, 460-463",
   "pn": "",
   "abstract": [
    "Mandarin speech data Across Taiwan (MAT) is a project initiated by members of the Association for Computational Linguistics and Chinese Language Processing (ACLCLP) to collect speech data through public telephone networks in Taiwan. Totally over 7000 Taiwanese individuals have provided speech data. The results were released as a series of MAT speech databases to the research community in Taiwan. Two databases, MAT-160 and MAT-400, have been used for the first and second Assessment of Speech Recognition Technique in Taiwan. Now, release preparation of a larger database of over 2000 speakers, called MAT-2000, has been completed. In this joint project conducted by ACLCLP and Philips Research East-Asia, considerable effort has been spent on validating the database to ensure its quality. MAT-2000 consists of over 80 hours of recordings and contains about 640,000 Mandarin syllables in over 140,000 speech files. These speech files are grouped into five sub-databases for different application purposes.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-848"
  },
  "sjolander00_icslp": {
   "authors": [
    [
     "Kåre",
     "Sjölander"
    ],
    [
     "Jonas",
     "Beskow"
    ]
   ],
   "title": "Wavesurfer - an open source speech tool",
   "original": "i00_4464",
   "page_count": 4,
   "order": 852,
   "p1": "vol. 4, 464-467",
   "pn": "",
   "abstract": [
    "In the speech technology research community there is an increasing trend to use open source solutions. We present a new tool in that spirit, WaveSurfer, which has been developed at the Centre for Speech Technology at KTH. It has been designed for tasks such as viewing, editing, and labeling of audio data. WaveSurfer is built around a small core to which most functionality is added in the form of plug-ins. The tool has been designed to work on most common platforms and with the aims that it should be easy to configure and extend. WaveSurfer is provided as open source, under the GPL license with the explicit goal that the speech community jointly will improve and expand its scope and capabilities.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-849"
  },
  "campbell00b_icslp": {
   "authors": [
    [
     "Nick",
     "Campbell"
    ],
    [
     "Toru",
     "Marumoto"
    ]
   ],
   "title": "Automatic labelling of voice-quality in speech databases for synthesis",
   "original": "i00_4468",
   "page_count": 4,
   "order": 853,
   "p1": "vol. 4, 468-471",
   "pn": "",
   "abstract": [
    "A series of experiments was performed to determine the extent to which voice-quality differences could be labelled automatically in a speech database. Using speech corpora of three different speaking styles from the same speaker as test material, hidden-Markov models were trained to distinguish the prosodic and acoustic characteristics of each style, and were used to re-label the voiced-segments in order to provide a single, merged, labelled corpus. Perceptual tests of speech synthesised by concatenation using CHATR showed that both prosodic and voice-quality cues to stylistic variation (in this case emotion) can be detected and labelled by the trained models. However, speech synthesised from the original separate databases was perceived as being more expressive.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-850"
  },
  "timoney00_icslp": {
   "authors": [
    [
     "Joe",
     "Timoney"
    ],
    [
     "J. Brian",
     "Foley"
    ]
   ],
   "title": "Speech quality evaluation based on AM-FM time-frequency representations",
   "original": "i00_4472",
   "page_count": 4,
   "order": 854,
   "p1": "vol. 4, 472-475",
   "pn": "",
   "abstract": [
    "This paper deals with the application of information extracted from AM and FM time-frequency representations of speech to the task of determining speech quality. The representations are introduced and then the procedure for data extraction is outlined. The experimental setup for the assessment of objective quality covers distortions typically found in speech communication systems. To determine how well these quality measures perform, regression analysis is used to evaluate how well they estimate the results of subjective testing. Considering each class of distortions individually the objective measures demonstrate good performance, however, this level does not seem to hold as well in the aggregate case. This leads to suggestions as to where possible improvements can be made to the procedure.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-851"
  },
  "kawahara00_icslp": {
   "authors": [
    [
     "Tatsuya",
     "Kawahara"
    ],
    [
     "Akinobu",
     "Lee"
    ],
    [
     "Tetsunori",
     "Kobayashi"
    ],
    [
     "Kazuya",
     "Takeda"
    ],
    [
     "Nobuaki",
     "Minematsu"
    ],
    [
     "Shigeki",
     "Sagayama"
    ],
    [
     "Katsunobu",
     "Itou"
    ],
    [
     "Akinori",
     "Ito"
    ],
    [
     "Mikio",
     "Yamamoto"
    ],
    [
     "Atsushi",
     "Yamada"
    ],
    [
     "Takehito",
     "Utsuro"
    ],
    [
     "Kiyohiro",
     "Shikano"
    ]
   ],
   "title": "Free software toolkit for Japanese large vocabulary continuous speech recognition",
   "original": "i00_4476",
   "page_count": 4,
   "order": 855,
   "p1": "vol. 4, 476-479",
   "pn": "",
   "abstract": [
    "A sharable software repository for Japanese LVCSR (Large Vocabulary Continuous Speech Recognition) is introduced. It is designed as a baseline platform for research and developed by researchers of different academic institutes under a governmental support. The repository consists of a recognition engine (Julius), Japanese acoustic models and statistical language models as well as Japanese morphological analysis tools. These modules can be easily integrated and replaced under a plug-and-play framework, which makes it possible to fairly evaluate components and to develop specific application systems. Assessment of these modules and systems in a 20000-word dictation task is reported. The software repository is freely available to the public.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-852"
  },
  "huo00_icslp": {
   "authors": [
    [
     "Qiang",
     "Huo"
    ],
    [
     "Bin",
     "Ma"
    ]
   ],
   "title": "Robust speech recognition based on off-line elicitation of multiple priors and on-line adaptive prior fusion",
   "original": "i00_4480",
   "page_count": 4,
   "order": 856,
   "p1": "vol. 4, 480-483",
   "pn": "",
   "abstract": [
    "We propose a new Bayesian approach to cope with a class of difficult robust speech recognition problems for those applications which only involve a couple of utterances, but every utterance involves a distinct yet complicated \"distortion channel\" from the intended message a speaker wants to convey to the received signal of a speech recognizer. It works by composing on-the-fly a testing-condition dependent prior distribution of HMM parameters for each testing utterance from a set of prior distributions elicited off-line from a rich set of training data. Encouraging results are obtained by applying this new method to a task of speaker independent continuous Mandarin speech recognition for speakers with different degree of accents.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-853"
  },
  "roberts00_icslp": {
   "authors": [
    [
     "William J.J.",
     "Roberts"
    ],
    [
     "Sadaoki",
     "Furui"
    ]
   ],
   "title": "Robust speech recognition via modeling spectral coefficients with HMM's with complex Gaussian components",
   "original": "i00_4484",
   "page_count": 4,
   "order": 857,
   "p1": "vol. 4, 484-487",
   "pn": "",
   "abstract": [
    "Robust speech recognition via hidden Markov model- ing of spectral vectors is studied in this paper. The hid- den Markov model (HMM) mixture components are as- sumed complex Gaussian with zero mean, diagonal co- variance, and with incorporating an unknown scalar gain term. The gain term is associated with each spectral vec- tor and it models the varying energy of speech signals. It is estimated by applying the maximum likelihood (ML) criterion. On an isolated digit database, in clean condi- tions, the spectral modeling with ML gain estimation ap- proach achieved similar performance to cepstral modeling of speech.\n",
    "Two additive noise compensation approaches for the spectral modeling scheme are also considered. The first approach requires a full noise HMM. This HMM is com- bined with the clean speech HMM to yield a noisy speech HMM. The second approach requires only the spectral shape of the noise. A term dependent on the spectral shape, together with an unknown magnitude term, is in- corporated into the clean speech HMM to yield a noisy speech HMM. The unknown magnitude of the noise is es- timated via the ML criterion. The performance of these two approaches for isolated digit recognition in noise is demonstrated and compared to a robust cepstral model- ing approach from the literature.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-854"
  },
  "wester00b_icslp": {
   "authors": [
    [
     "Mirjam",
     "Wester"
    ],
    [
     "Judith M.",
     "Kessens"
    ],
    [
     "Helmer",
     "Strik"
    ]
   ],
   "title": "Pronunciation variation in ASR: which variation to model?",
   "original": "i00_4488",
   "page_count": 5,
   "order": 858,
   "p1": "vol. 4, 488-491",
   "pn": "",
   "abstract": [
    "This paper describes how the performance of a continuous speech recognizer for Dutch has been improved by modeling within-word and cross-word pronunciation variation. A relative improvement of 8.8% in WER was found compared to baseline system performance. However, as WERs do not reveal the full effect of modeling pronunciation variation, we performed a detailed analysis of the differences in recognition results that occur due to modeling pronunciation variation and found that indeed a lot of the differences in recognition results are not reflected in the error rates. Furthermore, error analysis revealed that testing sets of variants in isolation does not predict their behavior in combination. However, these results appeared to be corpus dependent.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-855"
  },
  "mou00_icslp": {
   "authors": [
    [
     "Xiaolong",
     "Mou"
    ],
    [
     "Victor",
     "Zue"
    ]
   ],
   "title": "The use of dynamic reliability scoring in speech recognition",
   "original": "i00_4492",
   "page_count": 4,
   "order": 859,
   "p1": "vol. 4, 492-495",
   "pn": "",
   "abstract": [
    "Typically, along a recognizers search path, some acoustic units are modeled more reliably than others, due to differences in their acoustic-phonetic features and many other factors. This paper presents a dynamic reliability scoring scheme which can help adjust the partial path scores while the recognizer searches through the composed lexical and acoustic-phonetic network. The reliability models are trained on the acoustic scores of the correct arc and its immediate competing arcs extending the current partial path. During recognition, if, according to the trained reliability models, an arc can be more easily distinguished from the competing alternatives, that arc is more likely to be in the right path, and the partial path score can be adjusted accordingly on the fly to have a more accurate path hypothesis. We have applied this reliability scoring mechanism in two weather related domains, JUPITER [1] (for English) and PANDA (a predecessor of MUXING [2] for Mandarin Chinese). We get 9.8% word error rate (WER) reduction in the JUPITER domain and 12.4% WER reduction in the PANDA domain, thus demonstrating the effectiveness of this approach.\n",
    "s V. Zue, S. Seneff, J. Glass, J. Polifroni, C. Pao, T. Hazen, and L. Hetherington, \"JUPITER: A telephone-based conversational interface for weather information,\" IEEE Trans. on Speech and Audio Processing, vol. 8, no. 1, pp. 85-96, Jan. 2000. C. Wang, S. Cyphers, X. Mou, J. Polifroni, S. Seneff, J. Yi, and V. Zue, \"A telephone-access mandarin conversational system in the weather domain,\" in these proceedings.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-856"
  },
  "maciasguarasa00b_icslp": {
   "authors": [
    [
     "Javier",
     "Macías-Guarasa"
    ],
    [
     "Javier",
     "Ferreiros"
    ],
    [
     "Ruben",
     "San-Segundo"
    ],
    [
     "Juan Manuel",
     "Montero"
    ],
    [
     "Juan Manuel",
     "Pardo"
    ]
   ],
   "title": "Acoustical and lexical based confidence measures for a very large vocabulary telephone speech hypothesis-verification system",
   "original": "i00_4496",
   "page_count": 4,
   "order": 860,
   "p1": "vol. 4, 496-499",
   "pn": "",
   "abstract": [
    "In the context of large vocabulary speech recognition system, its of major interest to classify every utterance as being correctly or incorrectly recognised. In this paper we are presenting a preliminary study on a wordlevel confidence estimation system based on the output of a neural network. We use a combination of multiple features extracted from the acoustical and lexical decoders of our reference system, those available in the hypothesis stage of a hypothesis-verification very large vocabulary telephone speech recognition system. We will show the system architecture, describe the experiments leading to the selection of the set of parameters to be used by the NN and the final performance, showing promising results as compared with the use of standard log-likelihood ratio techniques for confidence scoring.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-857"
  },
  "goronzy00_icslp": {
   "authors": [
    [
     "Silke",
     "Goronzy"
    ],
    [
     "Krzysztof",
     "Marasek"
    ],
    [
     "Ralf",
     "Kompe"
    ],
    [
     "Andreas",
     "Haag"
    ]
   ],
   "title": "Phone-duration-based confidence measures for embedded applications",
   "original": "i00_4500",
   "page_count": 4,
   "order": 861,
   "p1": "vol. 4, 500-503",
   "pn": "",
   "abstract": [
    "In order to detect misrecognitions that may result from a mismatch between training and testing data, we use a con- fidence measure (CM) that collects a set of features during recognition and from the N-best list that is output by the recognizer. A neural network (NN) then calculates the probability that the utterance was recognized correctly based on these features. Since for misrecognized utterances the resulting phoneme alignments are often erroneous, we introduced some new features that are based on phoneme durations. The durations found by the recognizer are compared to the durations present in the training data base and the results of these comparisons serve as input for the NN. A great advantage of the duration-related features is that they are independent of the recognizer in contrast to e.g. acoustic scorebased features. We also use some score-related features that have proven to be useful in the past. Simultaneously with determining the confidence for a recognition result, we try to detect if in case of a misrecognition the utterance was an out of vocabulary (OOV) utterance. Using the complete set of 46 features we can achieve a correct classification rate of 90%. The word error rate can be reduced by 92% at a false rejection rate of 5.1% on a test task that consists of 35 speakers and includes more than 50% OOV utterances. OOV words were detected correctly in 91% of the cases. The presented CM is also used in a semi-supervised speaker adaptation scheme.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-858"
  },
  "ganapathiraju00b_icslp": {
   "authors": [
    [
     "Aravind",
     "Ganapathiraju"
    ],
    [
     "Jonathan",
     "Hamaker"
    ],
    [
     "Joseph",
     "Picone"
    ]
   ],
   "title": "Hybrid SVM/HMM architectures for speech recognition",
   "original": "i00_4504",
   "page_count": 4,
   "order": 862,
   "p1": "vol. 4, 504-507",
   "pn": "",
   "abstract": [
    "In this paper, we describe the use of a powerful machine learning scheme, Support Vector Machines (SVM), within the framework of hidden Markov model (HMM) based speech recognition. The hybrid SVM/HMM system has been developed based on our public domain toolkit. The hybrid system has been evaluated on the OGI Alphadigits corpus and performs at 11.6% WER, as compared to 12.7% with a triphone mixture-Gaussian HMM system, while using only a fifth of the training data used by triphone system. Several important issues that arise out of the nature of SVM classifiers have been addressed. We are in the process of migrating this technology to large vocabulary recognition tasks like SWITCHBOARD.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-859"
  },
  "sasaki00_icslp": {
   "authors": [
    [
     "Koki",
     "Sasaki"
    ],
    [
     "Hui",
     "Jiang"
    ],
    [
     "Keikichi",
     "Hirose"
    ]
   ],
   "title": "Rapid adaptation of n-gram language models using inter-word correlation for speech recognition",
   "original": "i00_4508",
   "page_count": 4,
   "order": 863,
   "p1": "vol. 4, 508-511",
   "pn": "",
   "abstract": [
    "In this paper, we study the fast adaptation problem of n-gram language model under the MAP estimation framework. We have proposed a heuristic method to explore inter-word correlation to accelerate MAP adaptation of n-gram model. According to their correlations, the occurrence of one word can be used to predict all other words in adaptation text. In this way, a large n-gram model can be efficiently adapted with a small amount of adaptation data. The proposed fast adaptation approach is evaluated in a Japanese newspaper corpus. We have observed a significant perplexity reduction even when we have only several hundred adaptation sentences.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-860"
  },
  "moore00_icslp": {
   "authors": [
    [
     "Gareth",
     "Moore"
    ],
    [
     "Steve",
     "Young"
    ]
   ],
   "title": "Class-based language model adaptation using mixtures of word-class weights",
   "original": "i00_4512",
   "page_count": 4,
   "order": 864,
   "p1": "vol. 4, 512-515",
   "pn": "",
   "abstract": [
    "This paper describes the use of a weighted mixture of classbased n-gram language models to perform topic adaptation. By using a fixed class n-gram history and variable word-given-class probabilities we obtain large improvements in the performance of the class-based language model, giving it similar accuracy to a word n-gram model, and an associated small but statistically significant improvement when we interpolate with a word-based n-gram language model.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-861"
  },
  "sun00c_icslp": {
   "authors": [
    [
     "Jiasong",
     "Sun"
    ],
    [
     "Xiaodong",
     "Cui"
    ],
    [
     "Zuoying",
     "Wang"
    ],
    [
     "Yang",
     "Liu"
    ]
   ],
   "title": "A language model adaptation approach based on text classification",
   "original": "i00_4516",
   "page_count": 4,
   "order": 865,
   "p1": "vol. 4, 516-519",
   "pn": "",
   "abstract": [
    "In our paper, we divide the corpus into 8 domains through text classification using K-means algorithm, and calculate the trigram LMs for each one. But the experiment shows the performance in some ones becomes worse. In order to solve this problem, we try to do the LM adaptation based on the domain LMs. The adaptation is done by mixing the domain LMs with the background LM by a linear interpolation. Relative word error rate reductions of between 5 and 10 % over the pruned background LM are achieved.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-862"
  },
  "chung00b_icslp": {
   "authors": [
    [
     "Grace",
     "Chung"
    ]
   ],
   "title": "Automatically incorporating unknown words in JUPITER",
   "original": "i00_4520",
   "page_count": 4,
   "order": 866,
   "p1": "vol. 4, 520-523",
   "pn": "",
   "abstract": [
    "This paper concerns the handling of out-of-vocabulary (OOV) words in the JUPITER weather information system. Specifically our objective is to deal with weather queries regarding unknown cities. We have implemented a system which can detect the presence of an unknown city name, and immediately propose a plausible spelling for that city. Potentially, the city can be dynamically incorporated into the recognizer lexicon. The three-stage system described in [1] was implemented in the JUPITER domain, and this paper will detail the development of a system that uses an ANGIE-based framework to model both spelling and pronunciation simultaneously, and uses automatically derived novel lexical units in the first stage. We report results on an independent test set containing unknown cities. Compared with a single-stage baseline, word error was reduced by 29.3% (from 24.6% to 17.4%) and understanding error was reduced by 67.5% (from 67.0% to 21.8%) on the three-stage configuration.\n",
    "",
    "",
    "G. Chung, \"A Three-Stage Solution for Flexible Vocabulary Speech Understanding,\" in these Proceedings.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-863"
  },
  "chengalvarayan00f_icslp": {
   "authors": [
    [
     "Rathinavelu",
     "Chengalvarayan"
    ]
   ],
   "title": "Look-ahead sequential feature vector normalization for noisy speech recognition",
   "original": "i00_4524",
   "page_count": 4,
   "order": 867,
   "p1": "vol. 4, 524-527",
   "pn": "",
   "abstract": [
    "Cepstral mean subtraction (CMS), which is a simple long-term bias removal, is used to compensate for transmission and linear fixed channel effects. In order to process the non-linear channel, a two-level CMS was proposed where separate channel compensation is performed for segments that are classified as speech and for segments classified as background. In this paper, methods for extending the two-level CMS to real-time implementation is proposed using a finite number of look-a-head frame delay, which further reduces computation and memory requirements of the compensation process. The on-line bias compensation shows similar characteristic curve as that of batch-mode and has the effect of greatly reducing the sensitivity of the recognizer to transmission noise variability.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-864"
  },
  "iwahashi00b_icslp": {
   "authors": [
    [
     "Naoto",
     "Iwahashi"
    ],
    [
     "Akihiko",
     "Kawasaki"
    ]
   ],
   "title": "Speaker adaptation in noisy environments based on parameter estimation using uncertain data",
   "original": "i00_4528",
   "page_count": 4,
   "order": 868,
   "p1": "vol. 4, 528-531",
   "pn": "",
   "abstract": [
    "This paper describes new method for the speaker adaptation of HMM parameters in environments with background noise. This method is based on Bayesian estimation, and calculates the a posteriori distribution of clean-speech HMM parameters from their a priori distribution by using noisy speech observations. The advantage of the method is that the distribution of the noise can be taken into account in adapting clean-speech HMMs to a target speakers speech without noise. The results of the experiments using noninformative prior show that the recognition performance in a noise-free environment was improved by this method even when the SNR of the noisy speech data used for the adaptation was -6 dB.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-865"
  },
  "acero00b_icslp": {
   "authors": [
    [
     "Alex",
     "Acero"
    ],
    [
     "Steven",
     "Altschuler"
    ],
    [
     "Lani",
     "Wu"
    ]
   ],
   "title": "Speech/noise separation using two microphones and a VQ model of speech signals",
   "original": "i00_4532",
   "page_count": 4,
   "order": 869,
   "p1": "vol. 4, 532-535",
   "pn": "",
   "abstract": [
    "In this paper we address the problem of using two or more microphones to enhance speech corrupted by nonstationary noise, such as that of a competing speaker (cocktail party effect) at very low SNR, by means of linear filtering of two microphone signals. This work is a variant to the probabilistic Independent Component Analysis (ICA) method but using a more accurate probability distribution of the speech signal based on a mixture Autoregressive model. Comparison with other algorithms on published real recordings shows more separation than with previously existing ICA algorithms.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-866"
  },
  "bacchiani00_icslp": {
   "authors": [
    [
     "Michiel",
     "Bacchiani"
    ]
   ],
   "title": "Using maximum likelihood linear regression for segment clustering and speaker identification",
   "original": "i00_4536",
   "page_count": 4,
   "order": 870,
   "p1": "vol. 4, 536-539",
   "pn": "",
   "abstract": [
    "Many adaptation scenarios rely on clustering of either the test or training data. Although consistency between the clustering and adaptation objective functions is desired, most previous approaches have not implemented such consistency. This paper shows that the statistics used in Maximum Likelihood Linear Regression (MLLR) adaptation are su\u000ecient to cluster data with a consistent Maximum Likelihood (ML) criterion. In addition, as the algorithm uses the same statistics for both adaptation and clustering, it is computationally e\u000ecient. Clustering experiments contrasting the performance of this algorithm with the widely used text independent Gaussian mixture model approach show increased adaptation likelihoods and consistency of within-cluster speaker identity. In a speaker identification experiment the adaptation-based scoring showed improved classi fication performance compared to the mixture model-based scoring.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-867"
  },
  "myrvoll00_icslp": {
   "authors": [
    [
     "Tor André",
     "Myrvoll"
    ],
    [
     "Olivier",
     "Siohan"
    ],
    [
     "Chin-Hui",
     "Lee"
    ],
    [
     "Wu",
     "Chou"
    ]
   ],
   "title": "Structural maximum a-posteriori linear regression for unsupervised speaker adaptation",
   "original": "i00_4540",
   "page_count": 4,
   "order": 871,
   "p1": "vol. 4, 540-543",
   "pn": "",
   "abstract": [
    "In this paper we introduce an approach to transformation based model adaptation techniques. Previously published schemes like MLLR define a set of affine transformations to be applied on clusters of model parameters. Although it has been shown that this approach can yield good results when adaptation data is scarce, an inherent problem needs to be considered: the number of transformations used has a significant influence on the adaptation performance. Using too many transformations will result in poorly estimated transformation parameters, eventually leading to a model that overfits the adaptation data. On the other hand, when too few transformations are used, a restricted mapping is obtained, leading to a suboptimal adapted model. We address this problem by estimating the transform parameters in a maximum a posteriori sense, using a set of hierarchical priors arranged in a tree structure. We show that this approach yields a significant improvement compared to MLLR when doing unsupervised model adaptation on the WSJ spoke 3 test.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-868"
  },
  "chien00_icslp": {
   "authors": [
    [
     "Jen-Tzung",
     "Chien"
    ],
    [
     "Guo-Hong",
     "Liao"
    ]
   ],
   "title": "Transformation-based Bayesian predictive classification for online environmental learning and robust speech recognition",
   "original": "i00_4544",
   "page_count": 5,
   "order": 872,
   "p1": "vol. 4, 544-547",
   "pn": "",
   "abstract": [
    "The mismatch between training and testing environments makes the necessity of speech recognizers to be adaptive both in acoustic modeling and decision rule. Accordingly, the speech hidden Markov models (HMMs) should be able to incrementally capture the evolving statistics of environments. Also, the speech recognizer should incorporate the inevitable parameter uncertainty for robust decision. This paper presents a transformation-based Bayesian predictive classification where the uncertainties of transformation parameters of HMM mean vector and precision matrix are adequately represented by a conjugate prior density. Due to the benefit of conjugate density, we generate the reproducible prior/posterior pair such that the hyperparameters of prior density could be evolved successively to new environments using online test data. The evolved hyperparameters could suitably describe the parameter uncertainty for TBPC decision. Therefore, a novel framework of TBPC geared with online prior evolution is developed for robust speech recognition. This framework is examined to be effective and efficient on the recognition task of connected Chinese digits in hands-free car environments.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-869"
  },
  "pitz00_icslp": {
   "authors": [
    [
     "Michael",
     "Pitz"
    ],
    [
     "Frank",
     "Wessel"
    ],
    [
     "Hermann",
     "Ney"
    ]
   ],
   "title": "Improved MLLR speaker adaptation using confidence measures for conversational speech recognition",
   "original": "i00_4548",
   "page_count": 4,
   "order": 873,
   "p1": "vol. 4, 548-551",
   "pn": "",
   "abstract": [
    "Automatic recognition of conversational speech tends to have higher word error rates (WER) than read speech. Improvements gained from unsupervised speaker adaptation methods like Maximum Likelihood Linear Regression (MLLR) [1] are reduced because of their sensitivity to recognition errors in the first pass. We show that a more detailed modeling of adaptation classes and the use of con- fidence measures improve the adaptation performance. We present experimental results on the VERBMOBIL task, a German conversational speech corpus.\n",
    "",
    "",
    "C.J. Leggetter, P.C.Woodland: \"Maximum Likelihood linear regression for speaker adaptation of continuous density hidden Markov models\", Computer, Speech and Language, vol. 9, pp. 171-185, 1995.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-870"
  },
  "chengalvarayan00g_icslp": {
   "authors": [
    [
     "Rathinavelu",
     "Chengalvarayan"
    ]
   ],
   "title": "Unified acoustic modeling for continuous speech recognition",
   "original": "i00_4552",
   "page_count": 4,
   "order": 874,
   "p1": "vol. 4, 552-555",
   "pn": "",
   "abstract": [
    "Usually the speech and the silence models are trained together depending upon the type of recognition task. For example, if the recognition task is only on con- nected-digits then the corresponding digit models are built using only the connected-digit training corpus. Similarly for large-vocabulary recognition tasks, the subword or the phoneme models are generated using only the subword training set. Further the alphabet models are separately trained using the alphabet training data for letter recognition. In certain applications the developer needs to perform mixed-mode operations like alphabet followed by digits, digits succeeded by keywords, letters preceded by keywords etc. So there is a need to robustly design a speech recognizer for such kind of specific applications. In that context, we propose several acoustic modeling techniques to improve the unified model performance for applications that require mixed-mode operations.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-871"
  },
  "dharanipragada00b_icslp": {
   "authors": [
    [
     "Satya",
     "Dharanipragada"
    ],
    [
     "Mukund",
     "Padmanabhan"
    ]
   ],
   "title": "A nonlinear unsupervised adaptation technique for speech recognition",
   "original": "i00_4556",
   "page_count": 4,
   "order": 875,
   "p1": "vol. 4, 556-559",
   "pn": "",
   "abstract": [
    "This paper describes a computationally inexpensive, nonlin- ear feature transformation technique for rapid adaptation of a speech recognition system to new acoustic conditions. One of the advantages of the method is that it does not require any initial decoding of the adaptation data for computing the nonlinear transform. This technique performs as well as the more expensive unsupervised MLLR technique. Furthermore, it significantly adds to the improvement when combined with unsupervised MLLR.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-872"
  },
  "doh00_icslp": {
   "authors": [
    [
     "Sam-Joo",
     "Doh"
    ],
    [
     "Richard M.",
     "Stern"
    ]
   ],
   "title": "Using class weighting in inter-class MLLR",
   "original": "i00_4560",
   "page_count": 4,
   "order": 876,
   "p1": "vol. 4, 560-563",
   "pn": "",
   "abstract": [
    "A new adaptation method called inter-class MLLR has recently been introduced. Inter-class MLLR utilizes relationships among different transformation functions to achieve more reliable estimates of MLLR parameters across multiple classes, and it produces lower word error rates (WER) than conventional MLLR in circumstances where very little speaker-specific adaptation data are available. This paper describes the application of weights to the neighboring classes to improve the effectiveness with which they are combined with the target class in inter-class MLLR. These weights are obtained from the variance of the estimation error considering the weighted least squares estimation in classical linear regression. In our experiments, the weights provided small improvements in WER for supervised adaptation but almost no improvement in unsupervised adaptation using only a small amount of adaptation data. We also discuss the effect of decreasing the number of neighboring classes as more adaptation data become available, the development of inter-class transformations from the test speaker, and the combination of inter-class MLLR with principal-component MLLR. None of the feasible variations of weighted inter-class MLLR provided significant improvements to recognition accuracy.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-873"
  },
  "hosom00_icslp": {
   "authors": [
    [
     "John-Paul",
     "Hosom"
    ],
    [
     "Ronald A.",
     "Cole"
    ]
   ],
   "title": "Burst detection based on measurements of intensity discrimination",
   "original": "i00_4564",
   "page_count": 4,
   "order": 877,
   "p1": "vol. 4, 564-567",
   "pn": "",
   "abstract": [
    "Detection of burst-related impulses, such as those accompanying plosive stop consonants, is an important problem for accurate measurement of acoustic features for recogntion (e.g., voice-onset-time) and for accurate automatic phonetic alignment. The proposed method of burst detection utilizes techniques for identifying and combining information about specific acoustic characteristics of bursts. One key element of the proposed method is the use of a measurement of intensity discrimination based on models from perceptual studies. Our experiments compared the proposed method of burst detection to the support vector machine (SVM) method, described below. The total error rate for the proposed method is 13.2% on the test-set partition of the TIMIT corpus, compared to a total error rate of 24% for the SVM method.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-874"
  },
  "ferreiroslopez00_icslp": {
   "authors": [
    [
     "Javier",
     "Ferreiros López"
    ],
    [
     "Daniel P. W.",
     "Ellis"
    ]
   ],
   "title": "Using acoustic condition clustering to improve acoustic change detection on broadcast news",
   "original": "i00_4568",
   "page_count": 4,
   "order": 878,
   "p1": "vol. 4, 568-571",
   "pn": "",
   "abstract": [
    "We have developed a system that breaks input speech into segments using an acoustic similarity measure. The aim is to detect the time points where the acoustic characteristics change, usually due to speaker changes but also resulting from changes in the acoustic environment. We have also developed a system to cluster the segments generated by the first system into clusters composed of homogeneous acoustic conditions. In this paper, we present a technique to improve the robustness of the acoustic change detection by feeding back the results of the segment clustering, exploiting the extra information available in the distance between the two clusters to which the segments belong. The interaction between the acoustic change detection and clustering systems gives us a substantial improvement over results previously reported on the 1997 Hub-4 Broadcast News test set that we employed [1][2]: Feedback of clustering information improved the Equal Error Rate (EER) of our acoustic change detection (ACD) system from 26.5% to 18%.\n",
    "s Scott Shaobing Chen, P.S. Gopalakrishnan, \"Speaker, environment and channel change detection and clustering via the bayesian information criterion\", 1998 DARPA Broadcast News Transcription & Understanding Workshop. Daben Liu, Francis Kubala, \"Fast speaker change detection for broadcast news transcription and indexing\", Eurospeech 99.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-875"
  },
  "nedel00_icslp": {
   "authors": [
    [
     "Jon P.",
     "Nedel"
    ],
    [
     "Rita",
     "Singh"
    ],
    [
     "Richard M.",
     "Stern"
    ]
   ],
   "title": "Phone transition acoustic modeling: application to speaker independent and spontaneous speech systems",
   "original": "i00_4572",
   "page_count": 4,
   "order": 879,
   "p1": "vol. 4, 572-575",
   "pn": "",
   "abstract": [
    "HMM-based large vocabulary speech recognition systems usually have a very large number of statistical parameters. For better estimation, the number of parameters is reduced by sharing them across models. The parameter sharing is decided by regression trees which are built using phonetic classes designed either by a human expert or by data-driven methods. In situations where neither of these are reliable, it may be useful to have techniques for non-decision-tree based state tying which perform comparably to those based on traditional methods. In this paper we propose two methods for non-decision tree based parameter learning in HMM-based systems. In the first method (context-dependent state tying), we restructure acoustic models to explicitly capture the transitions between phones in continuous speech. In the second method (transition-based subword units), we redefine the basic sound units used to model speech to model transitions between sounds explicitly. Experiments show that context-dependent state tying is a viable option for large vocabulary systems. They also show that using transition-based subword units can improve performance on spontaneous speech.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-876"
  },
  "shen00_icslp": {
   "authors": [
    [
     "Liqin",
     "Shen"
    ],
    [
     "Guokang",
     "Fu"
    ],
    [
     "Haixin",
     "Chai"
    ],
    [
     "Yong",
     "Qin"
    ]
   ],
   "title": "The measurement of acoustic similarity and its applications",
   "original": "i00_4576",
   "page_count": 5,
   "order": 880,
   "p1": "vol. 4, 576-579",
   "pn": "",
   "abstract": [
    "Its always helpful if we can predict how well a recognition system can perform for different tasks and evaluate the quality of different acoustic models. This paper presents a way to compute the acoustic similarity between words of a recognition task from the statistic models directly based on 2 kinds of definitions of the acoustic distance between recogniton units. Its applications for command and control task complexity prediction and acoustic model evaluation are discussed. The experimental results shows its a useful measurement.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-877"
  },
  "yi00b_icslp": {
   "authors": [
    [
     "Sopae",
     "Yi"
    ],
    [
     "Hyung Soon",
     "Kim"
    ],
    [
     "One Good",
     "Lee"
    ]
   ],
   "title": "Glottal parameters contributing to the perceotion of loud voices",
   "original": "i00_4580",
   "page_count": 4,
   "order": 881,
   "p1": "vol. 4, 580-583",
   "pn": "",
   "abstract": [
    "This paper focused on glottal parameters contributing to the perception of loud voices because energy of a voice is not the only effective factor. We used a formant synthesizer to synthesize loud voices. We divided F0 tilt (the tilt of F0 contour), SQ (Speed Quotient), OQ (Open Quotient) and TL (spectral Tilt Level) into three levels to get different conibinations with default values for the other synthesizer parameters. Analysis of listening tests indicates that F0 tilt, SQ, OQ and TL in descending order had significant influence on the perception of loud voices. F0 tilt had far more significant effect than the others. The influence of SQ increased a lot with exclusion of F0 tilt as a factor. The interaction between parameters was not significant.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-878"
  },
  "schillo00_icslp": {
   "authors": [
    [
     "Christoph",
     "Schillo"
    ],
    [
     "Gernot A.",
     "Fink"
    ],
    [
     "Franz",
     "Kummert"
    ]
   ],
   "title": "Grapheme based speech recognition for large vocabularies",
   "original": "i00_4584",
   "page_count": 4,
   "order": 882,
   "p1": "vol. 4, 584-587",
   "pn": "",
   "abstract": [
    "Common speech recognition systems use phonetically motivated subword units. To utilize words in these systems, one has to translate the available graphemic word representation into a phonetic one. To reduce this manual effort we propose to build grapheme based recognition systems. They can be used as speech interfaces for devices that can provide a graphemic representation of words like city names of navigation systems. Results of experiments on a 10,000 word lexicon of German cities are presented.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-879"
  },
  "nedel00b_icslp": {
   "authors": [
    [
     "Jon P.",
     "Nedel"
    ],
    [
     "Rita",
     "Singh"
    ],
    [
     "Richard M.",
     "Stern"
    ]
   ],
   "title": "Automatic subword unit refinement for spontaneous speech recognition via phone splitting",
   "original": "i00_4588",
   "page_count": 4,
   "order": 883,
   "p1": "vol. 4, 588-591",
   "pn": "",
   "abstract": [
    "Spontaneous speech is highly variable and rarely conforms to conventional assumptions and linguistically defined pronunciation rules. Specifically, there may be many different continuous speech realizations for each expertly defined phonetic unit in the dictionary. The phones may be realized in a clean and complete fashion as in read speech, or they may be realized in a sloppy and incomplete fashion as in highly spontaneous speech. For spontaneous speech, therefore, it may be beneficial to model incompletely realized variants of any phonetic unit as separate units. In this paper we test this hypothesis by introducing two possible modeling classes for the phones AA and IY in the standard English CMU recognition dictionary. We propose three different automatic methods of segregating the training data properly in order to identify and label the appropriate variants. Each of these methods results in improved recognition performance over the baseline, leading to the conclusion that finer modeling frameworks can be helpful to parameterize properly and recognize spontaneous speech.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-880"
  },
  "tarui00_icslp": {
   "authors": [
    [
     "Takeshi",
     "Tarui"
    ]
   ],
   "title": "Rhythm timing in Japanese English",
   "original": "i00_4592",
   "page_count": 4,
   "order": 884,
   "p1": "vol. 4, 592-595",
   "pn": "",
   "abstract": [
    "Due to the different sound systems of English and Japanese, obtaining a proper English speech rhythm is one of the greatest obstacles that Japanese face in their learning of English as a foreign language. Aims of this study are: 1) an explicit description of rhythm timing in English produced by four groups of Japanese subjects with different levels of English learning experiences in comparison with native speakers of English and 2) examinations and analyses of some underlying elements or factors which present obstacles for Japanese in obtaining proper English rhythm.\n",
    "English speech rhythm at a sentence level is investigated, focusing on feet or ISLs (interstress intervals) and their sound structure with the following criteria; a) proper use of function and content words and b) proper application of isochronous English rhythm in sentences. In this study all the factors are to be examined and analysed with respect to duration.\n",
    "Crucial transferred factors from underlying Japanese sound systems in Japanese English as well as the degree of difficulty in production have been obtained through comparison with the target English expressions.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-881"
  },
  "iwaki00b_icslp": {
   "authors": [
    [
     "Mamoru",
     "Iwaki"
    ]
   ],
   "title": "A vocal tract area ratio estimation from spectral parameter extracted by straight",
   "original": "i00_4596",
   "page_count": 4,
   "order": 885,
   "p1": "vol. 4, 596-599",
   "pn": "",
   "abstract": [
    "Approximating vocal tract by a series of acoustical tubes, reflection coefficients between the tubes are essential parameters for vocal tract area estimation. These parameters have been obtained through PARCOR analysis, with some devices to eliminate undesirable effects of fundamental period and radiation between lips. In this paper, vocal tract ratio estimation method is proposed using temporally and spectrally smoothed spectral envelope obtained by STRAIGHT speech analysis and synthesis method. The spectrum is transformed PARCOR coefficients, which are used to estimate the vocal area ratios. As a result, the proposed method can make a stable estimate of vocal tract area ratios in both temporal and spectral.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-882"
  },
  "ramabhadran00b_icslp": {
   "authors": [
    [
     "Bhuvana",
     "Ramabhadran"
    ],
    [
     "Yuqing",
     "Gao"
    ]
   ],
   "title": "Decision tree based rate of speech modeling for speech recognition",
   "original": "i00_4600",
   "page_count": 4,
   "order": 886,
   "p1": "vol. 4, 600-603",
   "pn": "",
   "abstract": [
    "A real-world speech recognition system encounters several speaking styles and speaking rates and its accuracy depends highly on the speaking rate, i.e., degrades sharply with very fast or very slow speech (including hyperarticulated speech) In this paper, we propose a generic modeling scheme to capture a range of speaking rates from very slow to very fast with the use of decision trees. This approach improves recognition performance on fast and slow speech, without degrading the performance on normal speech. The main idea behind this scheme is to model the context-dependent HMM state likelihoods differently for different speaking rates as the joint probability of observing the sequence of durations given the sequence of the acoustic states, without having to rely on any explicit duration computation during run-time.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-883"
  },
  "padmanabhan00_icslp": {
   "authors": [
    [
     "Mukund",
     "Padmanabhan"
    ]
   ],
   "title": "Spectral peak tracking and its use in speech recognition",
   "original": "i00_4604",
   "page_count": 4,
   "order": 887,
   "p1": "vol. 4, 604-607",
   "pn": "",
   "abstract": [
    "In this paper, we address the issue of making use of spectral peak location information in a speech recognition system. The cepstral features that are used in most speech recognition systems, though perceptually motivated, do not explicitly model spectral peak trajectory information, which is a valuable clue to identifying the underlying phone. We present a study that examines the utility of using this information in speech recognition, to augment the information present in the cepstra.\n",
    "We propose a method based on bandpass filtering the speech signal using several filters with different passbands, and using an adaptive IIR filter to track the locations of the spectral peaks in each bandpass output. This method has the advantage that along with the estimate of the spectral peak frequency, it also provides the energy at the spectral peaks (a feature that turns out to be quite informative). In initial experiments, the bandpass filters were chosen to correspond to the formant ranges, consequently, the locations of the spectral peaks are expected to correspond to the locations of the formants, for voiced sounds.\n",
    "We next investigated the utility of using this spectral peak information to help discriminate between the phones used in speech recognition. In order to quantify the information provided by the new features (over and above the information provided by the cepstra), we measure the mutual information between the augmented feature vector (cepstra augmented with the new features) and the phonetic class labels, and compare it to the mutual information between the classes and the cepstra. Finally, we experimented with feature fusion techniques, where the new features were appended to the cepstra, and a new speech recognition system was trained on the augmented features.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-884"
  },
  "li00m_icslp": {
   "authors": [
    [
     "Yongxin",
     "Li"
    ],
    [
     "Yuqing",
     "Gao"
    ],
    [
     "Hakan",
     "Erdogan"
    ]
   ],
   "title": "Weighted pairwise scatter to improve linear discriminant analysis",
   "original": "i00_4608",
   "page_count": 4,
   "order": 888,
   "p1": "vol. 4, 608-611",
   "pn": "",
   "abstract": [
    "Linear Discriminant Analysis (LDA) aims to transform an original feature space to a lower dimensional space with as little loss in discrimination as possible. We introduce a novel LDA matrix computation that incorporates confusability information between classes into the transform. Our goal is to improve discrimination in LDA. In conventional LDA, a between class covariance matrix that is based on the scatter of class means around the global mean is used. By rewriting the between class covariance expression in a more revealing way, we unveil that each class pair is considered equally confusable in the conventional LDA. We introduce a weighting factor for each pairwise scatter that enables to integrate the confusability information into the between class covariance matrix. There are many possibilities to choose the weighting factors. We consider few of them that depend on Euclidean and Kullback-Leibler distances between classes when a single Gaussian approximation is used for each class. The method combined with speaker cluster based transformation decreases the error rate by about relative 10% on a large vocabulary speech recognition task using IBM's speech recognition engine.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-885"
  },
  "matousek00_icslp": {
   "authors": [
    [
     "Jindrich",
     "Matousek"
    ],
    [
     "Josef",
     "Psutka"
    ]
   ],
   "title": "ARTIC: a new Czech text-to-speech system using statistical approach to speech segment database construction",
   "original": "i00_4612",
   "page_count": 4,
   "order": 889,
   "p1": "vol. 4, 612-615",
   "pn": "",
   "abstract": [
    "This paper presents ARTIC1, a brand-new Czech text-to-speech (TTS) system. ARTIC (ARtificial Talker In Czech) is a concatenation-based system that consists of three main, relatively independent, components: speech segment database, text analyzer and speech synthesizer. A statistical approach to speech segment database construction is used: Hidden Markov models are employed to model triphones on the basis of the large speech corpus and to segment the corpus into triphonebased speech units - basic speech units used by the synthesizer. A speech segment selection algorithm is described to choose the representative instance of each speech unit from the segmented speech corpus. A text processing module converts the written text at the input of TTS system to the sequence of phones - basic phonetic units needed to describe the pronunciation of the input text - and prosodic marks. Finally, speech processing is performed using two versions of a PSOLA algorithm.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-886"
  },
  "chou00b_icslp": {
   "authors": [
    [
     "Wu",
     "Chou"
    ],
    [
     "Olivier",
     "Siohan"
    ],
    [
     "Tor André",
     "Myrvoll"
    ],
    [
     "Chin-Hui",
     "Lee"
    ]
   ],
   "title": "Extended maximum a posterior linear regression (EMAPLR) model adaptation for speech recognition",
   "original": "i00_4616",
   "page_count": 4,
   "order": 890,
   "p1": "vol. 4, 616-619",
   "pn": "",
   "abstract": [
    "In this paper, a new approach for model adaptation, extended maximum a posterior linear regression (EMAPLR), is described and studied. EMAPLR is an extension of maximum a posterior linear regression (MAPLR) for transform based model adaptation. The proposed approach has a close form solution under the elliptic symmetric matrix variate priors, and it is effective in our speech recognition experiments. EMAPLR is based on a direct MAPLR solution of the transform imageW\u0018s without explicitly solving the transformation matrix W. This is fundamentally different from conventionalMAPLR and MLLR. Moreover, the proposed EMAPLR approach is incorporated with the structured prior evolution which significantly improves the algorithm efficiency and robustness. The structure of prior evolution in MAPLR is studied and it is shown that under the structured prior evolution, the priors in MAPLR follows a recursive formulation. Experimental results on WSJ (Spoke 3) non-native speaker adaptation task indicates that significant gain over MLLR and MAPLR can be obtained with same amount of adaptation data.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-887"
  },
  "maneenoi00_icslp": {
   "authors": [
    [
     "Ekkarit",
     "Maneenoi"
    ],
    [
     "Somchai",
     "Jitapunkul"
    ],
    [
     "Visarut",
     "Ahkuputra"
    ],
    [
     "Umavasee",
     "Thathong"
    ],
    [
     "Boonchai",
     "Thampanitchawong"
    ],
    [
     "Sudaporn",
     "Luksaneeyanawin"
    ]
   ],
   "title": "Thai monophthong recognition using continuous density hidden Markov model and LPC cepstral coefficients",
   "original": "i00_4620",
   "page_count": 4,
   "order": 891,
   "p1": "vol. 4, 620-623",
   "pn": "",
   "abstract": [
    "This paper presents Thai monophthongs recognition. The monophthongs were qualitatively recognized by the 3-state left-to-right continuous density hidden Markov model. The LPC cepstral coefficients were used as feature which represented specch signal. The temporal cepstral derivative was additionally utilized in order to compare efficiency of the additional feature with that of the single LPC cepstral coefficients. The number of coefficient orders was varied in order to determine an appropriate order. Thai single, double, and triple polysyllabic words were used in this research. The 18 monophthongs from the polysyllabic words were qualitatively recognized as 9 different vowels. The highest recognition rate of the single feature obtained from 18-order LPC cepstral coefficient is 86.983 percent, while the recognition rate of the 16-order LPC cepstral coefficient accompanied by temporal derivative is 94.580 percent. The misclassification is examined and concluded that this resulted from excessively overlapped distributions of vowels in low and in back vowel group respectively.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-888"
  },
  "wu00j_icslp": {
   "authors": [
    [
     "Chung-Hsien",
     "Wu"
    ],
    [
     "Yeou-Jiunn",
     "Chen"
    ],
    [
     "Cher-Yao",
     "Yang"
    ]
   ],
   "title": "Error recovery and sentence verification using statistical partial pattern tree for conversational speech",
   "original": "i00_4624",
   "page_count": 4,
   "order": 892,
   "p1": "vol. 4, 624-627",
   "pn": "",
   "abstract": [
    "In this paper, in order to deal with the problems of disfluencies in conversational speech, partial pattern tree (PPT) and a PPT-based statistical language model are proposed. A partial pattern is defined to represent a sub-sentence with a key-phrase and some optional/functional phrases. The PPT is an integrated tree structure of the partial patterns generated from the training sentences and used to model the n-gram and grammatical constraints. In addition, a PPT merging algorithm is also proposed to reduce the number of partial patterns with similar syntactic structure by\tminimizing an objective cost function. Using the PPT, the undetected/misdetected errors due to disfluencies can be recovered. Finally, a sentence verification approach is proposed to re-rank the recovered sentences generated from the PPT. In order to assess the performance, a faculty name inquiry system with 2583 names has been implemented. The recognition acculacy of the system using the proposed PPT achieved 77.2%. We also contrasted this method with previous conventional approaches to show its superior performance.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-889"
  },
  "howitt00_icslp": {
   "authors": [
    [
     "Andrew Wilson",
     "Howitt"
    ]
   ],
   "title": "Vowel landmark detection",
   "original": "i00_4628",
   "page_count": 4,
   "order": 893,
   "p1": "vol. 4, 628-631",
   "pn": "",
   "abstract": [
    "Landmark based speech processing is a component of Lex- ical Access From Features (LAFF), a novel paradigm for feature based speech recognition. Detection and classification of landmarks is a crucial first step in a LAFF system. This work tests the theoretical characteristics of vowels, and shows results for work in progress on a Vowel Landmark Detector.\n",
    "Acoustic theory predicts first formant peaks in vowels, both in frequency and amplitude (at least for vowels between orally closed consonants). Formant tracking measurements found peaks in about 94% of vowels in the TIMIT database. Vowels which do not show a peak generally do not obey the theoretical assumptions, or are liable to formant tracker error due to nasalization, glottalization, or aspiration. Amplitude peaks are more reliable than frequency peaks. Peaks tend to occur early in the vowel, and frequency peaks tend to occur slightly before amplitude peaks. A fixed spectral band gave performance comparable to the formant tracker for this task, allowing a simpler detection algorithm.\n",
    "Previous work on a Vowel Landmark Detector is extended by use of a multilayer perceptron (MLP) to combine knowledge-based acoustic cues. The MLP decreases error rate to about 12%, of which about 8% are deletions. Since about 6% of vowels had no detectable peak, this performance is close to the expected limit of a peak picking algorithm. Work is continuing on algorithm improvements, including the output of confidence scores.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-890"
  },
  "meyer00_icslp": {
   "authors": [
    [
     "Carsten",
     "Meyer"
    ],
    [
     "Georg",
     "Rose"
    ]
   ],
   "title": "Rival training: efficient use of data in discriminative training",
   "original": "i00_4632",
   "page_count": 4,
   "order": 894,
   "p1": "vol. 4, 632-635",
   "pn": "",
   "abstract": [
    "We evaluate a simple extension of the corrective training algorithm for reestimation of the acoustic parameters, using | in addition to misrecognized sentences - also a selection of correctly recognized sentences for discrimination. Our approach (called \"rival training\") is implementationally much less expensive than lattice{based discriminative training methods, since we apply a \\hard\" threshold criterion to select a subset of sentences for which a single competitor is used for discrimination. Still, significant performance gains are obtained compared to maximum likelihood and corrective training even for triphone models with 61 densities per mixture (on a digit string and a large vocabulary isolated word recognition task). Further, the hard selection scheme may be used to accelerate the training process due to faster convergence and by restricting the training process to a fixed subset of training utterances.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-891"
  },
  "chen00n_icslp": {
   "authors": [
    [
     "Marilyn Y.",
     "Chen"
    ]
   ],
   "title": "Nasal detection module for a knowledge-based speech recognition system",
   "original": "i00_4636",
   "page_count": 4,
   "order": 895,
   "p1": "vol. 4, 636-639",
   "pn": "",
   "abstract": [
    "The Lexical Access From Features (LAFF) project tries to model the representation and perception of speech by human listeners. The derivation of such a representation involves first finding certain acoustic landmarks. Based on the landmarks and the acoustic cues surrounding the landmarks, distinctive features of the speech segments may be deciphered. The present study concentrates on the nasality module that attempts to detect the presence of an underlying nasal consonant, which is almost always adjacent to a vowel. For an underlying nasal in English, the features [+voiced, +sonorant, +consonant, +nasal, -continuant] are specified. The features are then mapped into measurable acoustic properties. Normally, cues from three regions in the sound indicate the presence of a nasal consonant: (1) abrupt spectral change from the vowel to the nasal murmur, (2) vowel nasalization, and (3) nasal murmur. These cues are quantified by acoustic parameters whose values are combined to indicate the presence of a nasal. The nasality module that has been developed is a sonorant landmark detector that greatly reduces false landmark detection and distinguishes nasals from laterals by incorporating additional nasal manner cues. The module also addresses cases where one or more of the three nasal cues is absent.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-892"
  },
  "liu00j_icslp": {
   "authors": [
    [
     "Jun",
     "Liu"
    ],
    [
     "Xiaoyan",
     "Zhu"
    ],
    [
     "Bin",
     "Jia"
    ]
   ],
   "title": "Semi-continuous segmental probability model for speech signals",
   "original": "i00_4640",
   "page_count": 4,
   "order": 896,
   "p1": "vol. 4, 640-643",
   "pn": "",
   "abstract": [
    "A semi-continuous segmental probability model, which can be considered as a special form of continuous mixture segmental probability model with continuous output probability density functions sharing in a mixture Gaussian density codebook, is proposed in this paper. The amount of training data required, as well as the computational complexity of the semi-continuous segmental probability model(SCSPM)[2], can be significantly reduced in comparison with the continuous segmental probability model(CSPM). Parameters of the vector quantization codebook and segmental probability model can be mutually optimized to achieve an optimal model/codebook combination, which leads to a unified modeling approach to vector quantization and segmental probability modeling of speech signals. The experimental results show that the recognition accuracy of the semi-continuous segmental probability model is higher than the semi-continuous hidden Markov model and continuous segmental probability model.\n",
    "Keywords: hidden Markov model, segmental probability model, semi-continuous segmental probability model, speech recognition\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-893"
  },
  "jan00b_icslp": {
   "authors": [
    [
     "Ea-Ee",
     "Jan"
    ],
    [
     "Jaime",
     "Botella Ordinas"
    ]
   ],
   "title": "Cross-domain robust acoustic training",
   "original": "i00_4644",
   "page_count": 4,
   "order": 897,
   "p1": "vol. 4, 644-647",
   "pn": "",
   "abstract": [
    "This paper describes our efforts towards cross-domain acoustic training for LargeVocabulary Continuous Speech Recognition (LVCSR) systems. We used weighted multi-style training by pooling insufficient telephony landline and cellular data with down sampled wide band clean data to develop better hybrid acoustic models. We explored the effects on decision tree size to accuracy by approximately 10%. The results show that by fixing number of parameters, system with smaller number of context dependentHMMstates yields better accuracy. It leads to a smaller phone set design. We then investigated the performance degradation on two reduced phone sets for Spanish. Based on these studies, we are able to develop a hybrid system for 8KHz closing talking microphone, telephony landline and cellular phone environments. The acoustic model is evaluated on both flat grammars, digit and name at department, and language model tasks, ATIS and general dictation, using the IBM ViaVoice product engine.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-894"
  },
  "wang00r_icslp": {
   "authors": [
    [
     "Fan",
     "Wang"
    ],
    [
     "Fang",
     "Zheng"
    ],
    [
     "Wenhu",
     "Wu"
    ]
   ],
   "title": "A c/v segmentation method for Mandarin speech based on multiscale fractal dimension",
   "original": "i00_4648",
   "page_count": 4,
   "order": 898,
   "p1": "vol. 4, 648-651",
   "pn": "",
   "abstract": [
    "This paper proposes a new algorithm for Mandarin speech Consonant and Vowel (C/V) segmentation based on the fractal theory. The new method focuses on searching the transient region between the Consonant and Vowel parts in a Mandarin syliable that in general is a concatenation of a consonant followed by a vowel. The Multiscale Fractal Dimension Set (MFD) stands for the fractal dimensions at multiple maximum resolutions of computation. Just using the r-variance of MFD (the degree of the difference from all elements of a MFD) to distinguish clearly between the stable phonemes and their transient region, the algorithm can directly search the speech frame with minimum r-variance of MFD as the C/V segmentation boundary. A result of 95.2% segmentation accuracy is obtained for clean test corpus, and 82.3% accuracy in noisy environment with the SNR of 10 dB. This shows that the new C/V segmentation algorithm is qualified for the task of continuous Mandarin speech recognition.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-895"
  },
  "chen00o_icslp": {
   "authors": [
    [
     "Xiaoxia",
     "Chen"
    ],
    [
     "Aijun",
     "Li"
    ],
    [
     "Guohua",
     "Sun"
    ],
    [
     "Wu",
     "Hua"
    ],
    [
     "Zhigang",
     "Yu"
    ]
   ],
   "title": "An application of SAMPA-c for standard Chinese",
   "original": "i00_4652",
   "page_count": 4,
   "order": 899,
   "p1": "vol. 4, 652-655",
   "pn": "",
   "abstract": [
    "Labeling segment is an important work in database building. This paper presents a labeling system for Standard Chinese named SAMPA-C. We give some charts: consonant chart, vowel chart, tone chart, retroflex final chart, sound variation chart and non-speech symbol chart. Then this labeling system is used in two corpora labeling. The result shows that the labeling system is suitable for Standard Chinese.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-896"
  },
  "lu00c_icslp": {
   "authors": [
    [
     "Wenkai",
     "Lu"
    ],
    [
     "Xuegong",
     "Zhang"
    ],
    [
     "Yanda",
     "Li"
    ],
    [
     "Shen",
     "Liqin"
    ],
    [
     "Zhu",
     "Weibin"
    ]
   ],
   "title": "Joint speech signal enhancement based on spectral subtraction and SVD filter",
   "original": "i00_4656",
   "page_count": 4,
   "order": 900,
   "p1": "vol. 4, 656-659",
   "pn": "",
   "abstract": [
    "A joint speech signal enhancement based on singular value decomposition filter after spectral subtraction (SSVD) is proposed in this paper. The residual noise after spectral subtraction, which results for audible musical noise, is reduced further by SVD filter. The matrix size in spectral domain can be reduced half, and larger step-length adopted by SVD filter in spectral domain leads to lower cost, which make sure that the system can work in real-time. A novel speech/pause detector based on entropy(ESPD) is proposed too. The new detector improves the performance of the whole noise suppression system significantly.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-897"
  },
  "krstulovic00b_icslp": {
   "authors": [
    [
     "Sacha",
     "Krstulovic"
    ],
    [
     "Frédéric",
     "Bimbot"
    ]
   ],
   "title": "Inverse lattice filtering of speech with adapted non-uniform delays",
   "original": "i00_4660",
   "page_count": 4,
   "order": 901,
   "p1": "vol. 4, 660-663",
   "pn": "",
   "abstract": [
    "A particular form of constraint is incorporated to Linear Prediction lattice filter models in the form of unequal-length delays. This constraint amounts to reducing the number of intrinsic degrees of freedom defined by the reflection coeÆcients without modifying the LPC order of the corresponding transfer function. It can be optimized by a simple exhaustive search scheme. Preliminary results show that the prediction error is slightly decreased with respect to a conventional predictor using the same number of reflection coefficients.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-898"
  },
  "kawahara00b_icslp": {
   "authors": [
    [
     "Hideki",
     "Kawahara"
    ],
    [
     "Yoshinori",
     "Atake"
    ],
    [
     "Parham",
     "Zolfaghari"
    ]
   ],
   "title": "Accurate vocal event detection method based on a fixed-point analysis of mapping from time to weighted average group delay",
   "original": "i00_4664",
   "page_count": 4,
   "order": 902,
   "p1": "vol. 4, 664-667",
   "pn": "",
   "abstract": [
    "A new procedure for event detection and characterization is proposed based on group delay and fixed point analysis. This method enables the detection of precise timing and spread of speech events such as a vocal fold closure. A mapping from the center of a Gaussian time window to the mean time provides event locations as its fixed points. Refining these initial estimates using minimum phase group delay functions derived from the amplitude spectra provides accurate estimates of event locations and durations of excitations of each event. The proposed algorithm was tested using synthetic speech samples and natural speech database of simultaneously recorded sound waveforms and EGG signals. These tests revealed that the proposed method provides estimates of vocal fold closure instants with timing accuracy within 60 µs to 210 µs standard deviations. This algorithm is implemented to be suitable for real-time operation by making extensive use of FFTs without introducing any iterative procedures. It is potentially a very powerful tool for speech diagnosis and construction of very high quality speech manipulation systems.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-899"
  },
  "huang00g_icslp": {
   "authors": [
    [
     "Jun",
     "Huang"
    ],
    [
     "Mukund",
     "Padmanabhan"
    ]
   ],
   "title": "Filterbank-based feature extraction for speech recognition and its application to voice mail transcription",
   "original": "i00_4668",
   "page_count": 4,
   "order": 903,
   "p1": "vol. 4, 668-671",
   "pn": "",
   "abstract": [
    "In this paper, we propose a filterbank-based technique to extract more robust and discriminative features for the application of telephony speech recognition. First, we propose an extended Lerner grouping method to approximate the shape of the Mel filters in MFCC while reducing the cross-correlation between filterbank outputs. Then we used welch processing to reduce the variance of the spectral features while retaining the spectral resolution. Finally, we describe experiments where we augment the cepstral features with formant related features, computed using an adaptive filterbank. The new features represent the trajectory of the frequency components within different formant bands. Experimental results showed that the welch processing consistently improved the word error rate on a task of large vocabulary voice mail transcription and the formant related features provide higher discriminability than the MFCC features.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-900"
  },
  "murphy00_icslp": {
   "authors": [
    [
     "Peter J.",
     "Murphy"
    ]
   ],
   "title": "A cepstrum-based harmonics-to-noise ratio in voice signals",
   "original": "i00_4672",
   "page_count": 4,
   "order": 904,
   "p1": "vol. 4, 672-675",
   "pn": "",
   "abstract": [
    "A new cepstrum-based technique is developed in order to provide an alternative means of estimating the harmonics-to-noise ratio in voice signals. The geometric mean harmonics-to-noise ratio (GHNR) is defined as the mean of the individual spectral (i.e. at specific frequency locations) harmonics-to-noise ratios in dB. A heuristic development of the method treats the harmonic spectrum (in dB) of voiced speech taken over several cycles of the waveform as a more usual time domain signal, which is Fourier transformed. The sum of the resulting cepstral peaks (rahmonics) gives a direct estimation of the geometric mean harmonics-to-noise ratio (GHNR). The need for, inverse Fourier transform of the masked cepstrum back into the frequency domain, baseline correction and the usual harmonics-to-noise ratio (HNR) calculation is avoided by this approach. The technique is examined using synthetically generated voice signals.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-901"
  },
  "sun00d_icslp": {
   "authors": [
    [
     "Xuejing",
     "Sun"
    ]
   ],
   "title": "A pitch determination algorithm based on subharmonic-to-harmonic ratio",
   "original": "i00_4676",
   "page_count": 4,
   "order": 905,
   "p1": "vol. 4, 676-679",
   "pn": "",
   "abstract": [
    "In the present paper, a pitch determination algorithm (PDA) based on Subharmonic-to-Harmonic Ratio (SHR) is proposed. The algorithm is motivated by the results of a recent study on the perceived pitch of alternate pulse cycles in speech [1]. The algorithm employs a logarithmic frequency scale and a spectrum shifting technique to obtain the amplitude summation of harmonics and subharmonics, respectively. Through comparing the amplitude ratio of subharmonics and harmonics with the pitch perception results, the pitch of normal speech as well as speech with alternate pulse cycles (APC) can be determined. . Evaluation of the algorithm is performed on CSTRs database and on synthesized speech with APC. The results show that this algorithm is one of the most reliable PDAs. Furthermore, superior to most other algorithms, it handles subharmonics reasonably well.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-902"
  },
  "soleicasals00_icslp": {
   "authors": [
    [
     "Jordi",
     "Solé i Casals"
    ],
    [
     "Enric",
     "Monte i Moreno"
    ],
    [
     "Christian",
     "Jutten"
    ],
    [
     "Anisse",
     "Taleb"
    ]
   ],
   "title": "Source separation techniques applied to speech linear prediction",
   "original": "i00_4680",
   "page_count": 4,
   "order": 906,
   "p1": "vol. 4, 680-683",
   "pn": "",
   "abstract": [
    "The prediction filters are well known models for speech signal, in communications, control and many others areas. The classical method for deriving linear prediction coding (LPC) filters is often based on the minimization of a mean square error (MSE). Consequently, second order statistics are only required, but the estimation is only optimal if the residue is independent and identically distributed (iid) Gaussian. However, if the residue is not Gaussian, the estimation is no longer optimal. If one knows the theoretical statistics, it is possible to improve the estimation by using optimal (odd value higher order) statistics. Otherwise, i.e. if the statistics is not known, one can wonder how to implementing a quasi-optimal estimation. In this paper, we derive the ML estimate of the prediction filter. Relationships with robust estimation of auto-regressive (AR) processes, with blind deconvolution and with source separation based on mutual information minimization are shown. The algorithm, based on the minimization of a high-order statistics criterion, uses on-line estimation of the residue statistics. Improvements in the experimental results with speech signals emphasize on the interest of this approach.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-903"
  },
  "sugiyama00_icslp": {
   "authors": [
    [
     "Masahide",
     "Sugiyama"
    ]
   ],
   "title": "Model based voice decomposition method",
   "original": "i00_4684",
   "page_count": 4,
   "order": 907,
   "p1": "vol. 4, 684-687",
   "pn": "",
   "abstract": [
    "This paper proposes a voice decomposition method based on voice model. Using correlation distance as a criterion frequency domain decomposition is formulated. For a pair of correlation coe \u000ecient vectors minimizing powers of spectra is characterized using quadratic programming problem with constraint, where values of powers are non-negative. The solution of the problem without constraint is calculated using a linear equation where the coe\u000ecients are determined by correlation coe\u000ecient vectors. A correspondingmatrix for the linear equation has a nonnegative determinant, and the su\u000ecient and necessary condition for existence of reverse matrix is that given correlation coe\u000ecient vectors are linear independent. The solution with constraint is also given by solving a linear equation.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-904"
  },
  "funaki00_icslp": {
   "authors": [
    [
     "Keiichi",
     "Funaki"
    ]
   ],
   "title": "A time-varying complex speech analysis based on IV method",
   "original": "i00_4688",
   "page_count": 4,
   "order": 908,
   "p1": "vol. 4, 688-691",
   "pn": "",
   "abstract": [
    "Feature extraction of speech has been an important problem on speech processing. We are deeply interested in time-varying complex speech analysis method and we have already developed two kinds of time-varying complex AR (TV-CAR) parameter estimation algorithms for analytic speech signal, which are based on minimizing mean square error (MMSE) and Huber's robust M-estimation. This paper presents new robust parameter estimation algorithm for the TV~CAR model on the basis of an instrumental variable (IV) approach that may be robust against additive noise. The experiments with natural speech corrupted by white Gaussian demonstrate that the time-varying complex AR method achieves robust spectral estimation against additive white Gaussian owing to the improved resolution in the low frequencies.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-905"
  },
  "zolfaghari00b_icslp": {
   "authors": [
    [
     "Parham",
     "Zolfaghari"
    ],
    [
     "Hideki",
     "Kawahara"
    ]
   ],
   "title": "A sinusoidal model based on frequency-to-instantaneous frequency mapping",
   "original": "i00_4692",
   "page_count": 4,
   "order": 909,
   "p1": "vol. 4, 692-695",
   "pn": "",
   "abstract": [
    "In this paper we describe a sinusoidal analysis and synthesis framework which uses a novel method of extracting the sinusoidal components and fundamental frequency. This method is based on a mapping from linearly spaced filter centre frequencies to the instantaneous frequencies of the filter outputs. Frequency domain fixed points are obtained from this mapping which result in the extraction of the constituent sinusoidal components of the input signal. A robust fundamental frequency extraction technique based on a wavelet representation of this model is also used. These form the essential parts of the sinusoidal analysis framework which also includes a sinusoidal component trajectory continuation scheme. In order to reconstruct the spectrum, the inverse FFT method is used in synthesis [1]. This model has been shown to produce speech of high quality and is also applicable to other sound sources.\n",
    "",
    "",
    "Sdepalle, P., and Rodet, X. Synthèse additive par FFT inverse. Rapport Interne IRCAM (1990).\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-906"
  },
  "farooq00b_icslp": {
   "authors": [
    [
     "Omar",
     "Farooq"
    ],
    [
     "Sekharjit",
     "Datta"
    ]
   ],
   "title": "Dynamic feature extraction by wavelet analysis",
   "original": "i00_4696",
   "page_count": 4,
   "order": 910,
   "p1": "vol. 4, 696-699",
   "pn": "",
   "abstract": [
    "Phoneme recognition is a difficult task in speech recognition as it is variable in length and its acoustic properties change due to co-articulation and variation in dialects. The performance of the speech recognition system is heavily based on features extracted for the phonemes. The conventional technique of Short Time Fourier Transform (STFT) has a serious limitation in resolving the stop (plosive) sounds. This shortcoming can be overcome by using the multi-resolution capability of Wavelet Analysis. In this paper we perform a comparative study of Discrete Wavelet Transform (DWT) and Wavelet Packet (WP) for new dynamic features extraction of phonemes.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-907"
  },
  "karnjanadecha00_icslp": {
   "authors": [
    [
     "Montri",
     "Karnjanadecha"
    ],
    [
     "Stephen A.",
     "Zahorian"
    ]
   ],
   "title": "An investigation of variable block length methods for calculation of spectral/temporal features for automatic speech recognition",
   "original": "i00_4700",
   "page_count": 4,
   "order": 911,
   "p1": "vol. 4, 700-703",
   "pn": "",
   "abstract": [
    "This paper presents an investigation of non-uniform time sampling methods for spectral/temporal feature extraction for use in automatic speech recognition. In most current methods for signal modeling of speech information, dynamic features are determined from frame-based parameters using a fixed time sampling, i.e., fixed block length and fixed block spacing. This work explores new methods in which block length and/or block spacing are variable. Three methods are suggested and each was tested with the TIMIT database using a standard HMM recognizer. Phone recognition experiments were conducted using the standard 39 phone set. The methods were also evaluated with various HMM model complexities. Experimental results indicated that none of the proposed nonuniform feature time sampling methods perform significantly better than fixed time sampling methods. However, the best results obtained with the front end are comparable to those obtained with current state-of-the-art systems. Also the performance of our monophone system surpasses that of most reported context-dependent monophone systems.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-908"
  },
  "sasou00_icslp": {
   "authors": [
    [
     "Akira",
     "Sasou"
    ],
    [
     "Kazuyo",
     "Tanaka"
    ]
   ],
   "title": "Glottal excitation modeling using HMM with application to robust analysis of speech signal",
   "original": "i00_4704",
   "page_count": 4,
   "order": 912,
   "p1": "vol. 4, 704-707",
   "pn": "",
   "abstract": [
    "This paper describes a robust analysis method for high fundamental frequency speech signal. In the proposed method, a Hidden Markov Model (HMM) is applied in order to represent the non-stationary property of the glottal source. Experiments are carried out using both synthetic and natural speeches to confirm the effectiveness of the method. Experimental results indicate (1) in the case of using synthetic speech in the pitch range of up to 750Hz, the proposed method can precisely estimate the original spectrum, and (2) the spectrum estimated from natural speech of pitch frequency 666Hz is less affected by the harmonics of glottal excitation, compared with result of the conventional method.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-909"
  },
  "dociofernandez00_icslp": {
   "authors": [
    [
     "Laura",
     "Docío-Fernández"
    ],
    [
     "Carmen",
     "García-Mateo"
    ]
   ],
   "title": "Automatic segmentation of speech based on hidden Markov models and acoustic features",
   "original": "i00_4708",
   "page_count": 5,
   "order": 913,
   "p1": "vol. 4, 708-711",
   "pn": "",
   "abstract": [
    "An accurate database segmented and labeled at phonetic, subword or word level is very important for speech research. However, manual segmentation and labeling is a time consuming and error prone task. This paper describes an automatic procedure for the segmentation of speech in a set of acoustic sub-words units: given either the linguistic or the phonetic content of a speech utterance, the system provides unit boundaries. The technique is based on the use of an acoustic sub-word unit Hidden Markov Model (HMM) recognizer in order to provide a coarse segmentation based on Viterbi alignment, which is refined later by means of an acoustic segmentation and a small set of rules based on acoustic features. These rules represent phonetic knowledge and address the correction of unexpected segmentation errors which are a major problem of such HMM recognizers. In addition, these rules are useful to analyze sequences of sounds including sonorants or several successive vowels. Segmentation experiments have been conducted in a Galician speech database to check the reliability of the resulting system.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-910"
  },
  "kurematsu00c_icslp": {
   "authors": [
    [
     "Akira",
     "Kurematsu"
    ],
    [
     "Youichi",
     "Akegami"
    ],
    [
     "Susanne",
     "Burge"
    ],
    [
     "Susanne",
     "Jekat"
    ],
    [
     "Brigitte",
     "Lause"
    ],
    [
     "Victoria L.",
     "Maclaren"
    ],
    [
     "Daniela",
     "Oppermann"
    ],
    [
     "Tanja",
     "Schultz"
    ]
   ],
   "title": "VERBMOBIL dialogues: multifaced analysis",
   "original": "i00_4712",
   "page_count": 4,
   "order": 914,
   "p1": "vol. 4, 712-715",
   "pn": "",
   "abstract": [
    "This paper describes the outline of collecting and transcribing spontaneous spoken dialogues for VERBMOBIL, the German research project on mult ilingual processing of spontaneous speech. The method and conditions of data collection performed using the same scenario and the transliteration convention of spontaneous speech were described. The characteristics of VERBMOBIL corpus were presented in terms of the size of dialogues, turns, sentences, words, perplexity based on the linguistic analysis.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-911"
  },
  "zhang00l_icslp": {
   "authors": [
    [
     "Jin-Jie",
     "Zhang"
    ],
    [
     "Zhi-Gang",
     "Cao"
    ],
    [
     "Zheng-Xin",
     "Ma"
    ]
   ],
   "title": "A computation-efficient parameter adaptation algorithm for the generalized spectral subtraction method",
   "original": "i00_4716",
   "page_count": 4,
   "order": 915,
   "p1": "vol. 4, 716-719",
   "pn": "",
   "abstract": [
    "The background noise in speech is not only objectionable to listeners with proper hearing but also very harmful to those hearing impaired. Hence, there is a strong need to employ speech enhancement to suppress the background noise in speech. In this paper, a computation-efficient parameter-adaptation algorithm is proposed for the generalized spectral subtraction method, which can reduce noise level more significantly.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-912"
  },
  "araki00_icslp": {
   "authors": [
    [
     "Masahiro",
     "Araki"
    ],
    [
     "Kiyoshi",
     "Ueda"
    ],
    [
     "Takuya",
     "Nishimoto"
    ],
    [
     "Yasuhisa",
     "Niimi"
    ]
   ],
   "title": "A semantic tagging tool for spoken dialogue corpus",
   "original": "i00_4720",
   "page_count": 4,
   "order": 916,
   "p1": "vol. 4, 720-723",
   "pn": "",
   "abstract": [
    "In this paper, we report our semantic tagging tool for spoken dialogue corpus. This tagging tool can acquire analysis rules using Transformation-based Learning (TBL) from small scale training corpus. It can learn dialogue act tagging rules and semantic frame tagging rules. The precisions are 72% in dialogue act tagging and 58% of semantic frame tagging in open test.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-913"
  },
  "li00n_icslp": {
   "authors": [
    [
     "Aijun",
     "Li"
    ],
    [
     "Xiaoxia",
     "Chen"
    ],
    [
     "Guohua",
     "Sun"
    ],
    [
     "Wu",
     "Hua"
    ],
    [
     "Zhigang",
     "Yin"
    ],
    [
     "Yiqing",
     "Zu"
    ],
    [
     "Fang",
     "Zheng"
    ],
    [
     "Zhanjiang",
     "Song"
    ]
   ],
   "title": "The phonetic labeling on read and spontaneous discourse corpora",
   "original": "i00_4724",
   "page_count": 4,
   "order": 917,
   "p1": "vol. 4, 724-727",
   "pn": "",
   "abstract": [
    "Read and spontaneous discourses are two different but very significant speech styles to be investigated. So phonetic labeling on read and spontaneous discourse corpora are made one is ASCCD, a 10 hours read discourse corpus and the other is CASS, a 4 hours spontaneous discourse corpus. First the principles and conventions of transcription are presented. Then, these two speech styles are compared from phonetic and syntactic point of view, including the statistic results of different phonetic units got from the annotated corpora.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-914"
  },
  "beringer00b_icslp": {
   "authors": [
    [
     "Nicole",
     "Beringer"
    ],
    [
     "Florian",
     "Schiel"
    ]
   ],
   "title": "The quality of multilingual automatic segmentation using German MAUS",
   "original": "i00_4728",
   "page_count": 4,
   "order": 918,
   "p1": "vol. 4, 728-731",
   "pn": "",
   "abstract": [
    "The goal of this work is to demonstrate the quality of multilingual automatic segmentations using the German MAUS system in order to substitute costly manually segmented data by automatically segmented corpora. In this study we investigated the influence of language specific HMMs in a cross-language task namely the automatic segmentations of English, Irench and Japanese with HMMs trained on German acoustic data. Given the orthographic transcription of an utterance we were able to produce quite good segmentations with the \"wrong\" acoustic models which will be described in detail in the following sections. The reason for this can either be based on the bigger influence of intra-/inter-speaker variability compared to the \"interlingual variability\" or on universal coarticulation processes as discussed below.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-915"
  },
  "radova00_icslp": {
   "authors": [
    [
     "Vlasta",
     "Radová"
    ],
    [
     "Josef",
     "Psutka"
    ]
   ],
   "title": "UWB_S01 corpus - a czech read-speech corpus",
   "original": "i00_4732",
   "page_count": 4,
   "order": 919,
   "p1": "vol. 4, 732-735",
   "pn": "",
   "abstract": [
    "The UWB_S01 corpus is a read-speech corpus that is intended to be used mainly for training of Czech continuous speech recognition systems. It has been developed at the Department of Cybernetics at the University of West Bohemia in Pilsen since 1998. This paper describes the structure of the corpus and deals with all necessary steps of the corpus construction: the preprocessing of the texts, the selection of proper sentences that will form the corpus, and the recording and the annotation of the utterances.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-916"
  },
  "fabbrizio00_icslp": {
   "authors": [
    [
     "Giuseppe Di",
     "Fabbrizio"
    ],
    [
     "Shrikanth",
     "Narayanan"
    ]
   ],
   "title": "Web-based monitoring, logging and reporting tools for multi-service multi-modal systems",
   "original": "i00_4736",
   "page_count": 5,
   "order": 920,
   "p1": "vol. 4, 736-739",
   "pn": "",
   "abstract": [
    "This paper describes MILER (Multi-modal data Logger for Evaluation and Report), a web-based multi-service monitoring, logging and reporting tool for advanced multimodal dialog systems. MILER has been designed to directly arrange and synchronize logging data collected from live services and to provide real-time reports about service usage and system performance. Special attention has been given to the architecture design in order to achieve service and access-device independence and reliable synchronization of data from distributed logs. MILER allows researchers to analyze multi-modal interactions, analyze the call flow, reconstruct the system/user dialogue turns, play the recorded user utterances, and provide a preliminary dialogue performance evaluation. It also supports labeling and annotation of the dialogue turns for further offline analysis. Once the user inputs (i.e. speech and other input modalities) are manually transcribed and labeled, along with detailed log events from each dialog, MILER derives a set of objective measures, which includes word and concept accuracy, number of attempts per concept, dialog turn counts and duration, and task completion rates. Subjective measures extracted from user's surveys, including perceived task success and ease of use measures, can be combined with the objective measures and the results used later for accuracy computation.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-917"
  },
  "strik00b_icslp": {
   "authors": [
    [
     "Helmer",
     "Strik"
    ],
    [
     "Catia",
     "Cucchiarini"
    ],
    [
     "Judith M.",
     "Kessens"
    ]
   ],
   "title": "Comparing the recognition performance of CSRs: in search of an adequate metric and statistical significance test",
   "original": "i00_4740",
   "page_count": 4,
   "order": 921,
   "p1": "vol. 4, 740-743",
   "pn": "",
   "abstract": [
    "In this paper a new measure of recognition accuracy is introduced which can be used when comparing the performance of two speech recognizers, to establish which is the better one. This metric combines the advantages of previous measures, but excludes their disadvantages. Essentially, the metric is an attempt to quantify the degree of recognition accuracy for each sentence, thus obtaining a more informative measure than either correct or incorrect, in such a way that the statistical significance of the observed differences can be tested. The advantages of our assessment method are illustrated on the basis of both artificial and real performance data of different recognizers.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-918"
  },
  "raake00_icslp": {
   "authors": [
    [
     "Alexander",
     "Raake"
    ]
   ],
   "title": "Perceptual dimensions of speech sound quality in modern transmission systems",
   "original": "i00_4744",
   "page_count": 4,
   "order": 922,
   "p1": "vol. 4, 744-747",
   "pn": "",
   "abstract": [
    "A study is reported which was carried out to determine factors governing the perception of sound quality of human, natural speech transmitted over telephone systems of different transmission-channel bandwidths, and their impact on perceived overall quality. The data collected in the described auditory tests is supposed to form the basis for instrumental modelling approaches of overall and especially speech-sound quality in wide-band systems. Measurement parameters of the listening-only test (LOT) were the lower and upper frequency limits of the transmission-channel as well as the terminal equipment used for speech presentation, with the aim of investigating the role of expectation and psychoacoustic reference for speech-sound quality perception. In this way, the terminal equipment best suitable for the assessment of speech-sound quality with respect to the channel frequency band was identified. With this approach, the test subjects expectation towards different terminal equipment was taken into consideration, while the acoustical properties of the terminal were adjusted to an ideal reference. In the tests, speech samples were presented over an electrostatic headphone, monotic and diotic, as well as telephone handsets modified to allow wide-band presentation. Results show that an interdependence exists between the presentation method (handset vs. headphone) and the transmission bandwidth with respect to overall quality. The paper discusses reasons for this effect, which is assumed to be caused by the expectation listeners show towards specific types of terminal equipment.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.2000-919"
  }
 },
 "sessions": [
  {
   "title": "Speech Production Control (Special Session)",
   "papers": [
    "liljencrants00_icslp",
    "honda00_icslp",
    "fujisaki00_icslp",
    "carre00_icslp",
    "engwall00_icslp",
    "lu00_icslp",
    "gao00_icslp",
    "fujimura00_icslp",
    "ohman00_icslp",
    "jiang00_icslp"
   ]
  },
  {
   "title": "Linguistics, Phonology, Phonetics, and Psycholinguistics 1, 2",
   "papers": [
    "whiteside00_icslp",
    "hanna00_icslp",
    "lindstrom00_icslp",
    "den00_icslp",
    "jongman00_icslp",
    "muller00_icslp",
    "fujisaki00b_icslp",
    "yang00_icslp",
    "swerts00_icslp",
    "wrede00_icslp",
    "zheng00_icslp",
    "mella00_icslp",
    "kurdi00_icslp"
   ]
  },
  {
   "title": "Discourse and Dialogue 1, 2",
   "papers": [
    "kurematsu00_icslp",
    "abdou00_icslp",
    "meng00_icslp",
    "mctear00_icslp",
    "niimi00_icslp",
    "xu00_icslp",
    "georgila00_icslp",
    "yang00b_icslp",
    "maes00_icslp",
    "muller00b_icslp",
    "paek00_icslp",
    "shriver00_icslp",
    "boda00_icslp",
    "zajicek00_icslp"
   ]
  },
  {
   "title": "Recognition and Understanding of Spoken Language 1, 2",
   "papers": [
    "takagi00_icslp",
    "luo00_icslp",
    "kato00_icslp",
    "guillen00_icslp",
    "whittaker00_icslp",
    "choi00_icslp",
    "visweswariah00_icslp",
    "zhang00_icslp",
    "galescu00_icslp",
    "yamamoto00_icslp",
    "carpenter00_icslp",
    "wu00_icslp",
    "xu00b_icslp",
    "savova00_icslp",
    "weilhammer00_icslp",
    "esteve00_icslp",
    "johnsen00_icslp",
    "takara00_icslp",
    "horvitz00_icslp",
    "deng00_icslp",
    "martin00_icslp",
    "kogure00_icslp",
    "lin00_icslp",
    "ito00_icslp",
    "zhang00b_icslp",
    "hirschberg00_icslp",
    "bellegarda00_icslp",
    "hanazawa00_icslp",
    "addadecker00_icslp",
    "wester00_icslp",
    "kessens00_icslp",
    "zhang00c_icslp",
    "antoniou00_icslp",
    "hon00_icslp",
    "li00_icslp",
    "kitazoe00_icslp",
    "smith00_icslp",
    "umesh00_icslp",
    "fujimoto00_icslp",
    "gu00_icslp",
    "tam00_icslp",
    "faltlhauser00_icslp",
    "toyama00_icslp",
    "sonmez00_icslp",
    "daoudi00_icslp",
    "glotin00_icslp",
    "okada00_icslp",
    "zhu00_icslp",
    "hagen00_icslp",
    "hagen00b_icslp",
    "yu00_icslp",
    "raj00_icslp",
    "sturm00_icslp",
    "kadambe00_icslp",
    "barker00_icslp",
    "liu00_icslp",
    "zhang00d_icslp",
    "nitta00_icslp",
    "zheng00b_icslp",
    "farooq00_icslp",
    "couvreur00_icslp",
    "bazzi00_icslp",
    "gajic00_icslp",
    "morris00_icslp",
    "matsuda00_icslp",
    "yang00c_icslp",
    "tran00_icslp",
    "quillen00_icslp",
    "sturm00b_icslp",
    "bazzi00b_icslp",
    "sun00_icslp",
    "vasilache00_icslp",
    "qi00_icslp",
    "blasig00_icslp"
   ]
  },
  {
   "title": "Production of Spoken Language",
   "papers": [
    "tseng00_icslp",
    "dang00_icslp",
    "motoki00_icslp",
    "menard00_icslp",
    "smith00b_icslp",
    "orr00_icslp",
    "iseli00_icslp"
   ]
  },
  {
   "title": "Linguistics, Phonology, Phonetics, and Psycholinguistics 3",
   "papers": [
    "blache00_icslp",
    "li00b_icslp",
    "yamamoto00b_icslp",
    "chen00_icslp",
    "gao00b_icslp",
    "palou00_icslp",
    "belvin00_icslp"
   ]
  },
  {
   "title": "Dialogue Systems and Speech Input",
   "papers": [
    "higashida00_icslp",
    "wang00_icslp",
    "dharanipragada00_icslp",
    "chen00b_icslp",
    "hansen00_icslp",
    "kim00_icslp"
   ]
  },
  {
   "title": "Miscellaneous 1 [A,B,C,G,H,L,O,Q,X]",
   "papers": [
    "sadigh00_icslp",
    "jongman00b_icslp",
    "shattuckhufnagel00_icslp",
    "lopezdeipina00_icslp",
    "plauche00_icslp",
    "widera00_icslp",
    "fox00_icslp",
    "kurematsu00b_icslp",
    "caspers00_icslp",
    "yamashita00_icslp",
    "veilleux00_icslp",
    "mori00_icslp",
    "wolters00_icslp",
    "falavigna00_icslp",
    "bell00_icslp",
    "cutler00_icslp",
    "eldin00_icslp",
    "gadde00_icslp",
    "venditti00_icslp",
    "minowa00_icslp",
    "chen00c_icslp",
    "minematsu00_icslp",
    "montero00_icslp",
    "fernandezsalgado00_icslp",
    "applebaum00_icslp",
    "gu00b_icslp",
    "chu00_icslp",
    "vainio00_icslp",
    "jokisch00_icslp",
    "narusawa00_icslp",
    "kitazoe00b_icslp",
    "watanuki00_icslp",
    "abutalebi00_icslp",
    "huber00_icslp",
    "barry00_icslp",
    "zundert00_icslp",
    "tomokiyo00_icslp",
    "deroo00_icslp",
    "escalona00_icslp",
    "wu00b_icslp",
    "yu00b_icslp",
    "boulademareuil00_icslp",
    "eichner00_icslp",
    "breen00_icslp",
    "blouin00_icslp",
    "janicki00_icslp",
    "hirschfeld00_icslp",
    "pan00_icslp",
    "nishizawa00_icslp",
    "jokisch00b_icslp",
    "olinsky00_icslp",
    "sen00_icslp",
    "jung00_icslp",
    "kim00b_icslp",
    "moller00_icslp",
    "chengalvarayan00_icslp",
    "chen00d_icslp",
    "dolfing00_icslp",
    "macho00_icslp",
    "yao00_icslp",
    "ye00_icslp",
    "altosaar00_icslp",
    "tian00_icslp",
    "ishi00_icslp"
   ]
  },
  {
   "title": "Speech Perception, Comprehension, and Production (Special Session)",
   "papers": [
    "patterson00_icslp",
    "tatsumi00_icslp",
    "alku00_icslp",
    "patterson00b_icslp",
    "martin00b_icslp",
    "fushimi00_icslp",
    "ijuin00_icslp",
    "wydell00_icslp",
    "uno00_icslp",
    "zhou00_icslp"
   ]
  },
  {
   "title": "Prosody 1, 2",
   "papers": [
    "zhou00b_icslp",
    "hu00_icslp",
    "chen00e_icslp",
    "verhelst00_icslp",
    "shih00_icslp",
    "wightman00_icslp",
    "muller00c_icslp",
    "li00c_icslp",
    "wang00b_icslp",
    "zhang00e_icslp",
    "gussenhoven00_icslp",
    "tseng00b_icslp",
    "mixdorff00_icslp"
   ]
  },
  {
   "title": "Speech Interface and Dialogue Systems",
   "papers": [
    "rosenfeld00_icslp",
    "russell00_icslp",
    "fafiotte00_icslp",
    "denecke00_icslp",
    "abe00_icslp",
    "levin00_icslp"
   ]
  },
  {
   "title": "Multimodal, Translingual, and Dialogue Systems",
   "papers": [
    "bangalore00_icslp",
    "rudnicky00_icslp",
    "gustafson00_icslp",
    "wang00c_icslp",
    "seneff00_icslp",
    "duan00_icslp",
    "meng00b_icslp",
    "hirasawa00_icslp"
   ]
  },
  {
   "title": "Production of Spoken Language (Poster)",
   "papers": [
    "mokhtari00_icslp",
    "perrier00_icslp",
    "vaxelaire00_icslp",
    "honda00b_icslp",
    "niikawa00_icslp",
    "ouni00_icslp",
    "hamza00_icslp",
    "chen00f_icslp",
    "moller00b_icslp",
    "li00d_icslp",
    "maes00b_icslp",
    "ohmura00_icslp",
    "li00e_icslp",
    "choi00b_icslp",
    "jensen00_icslp",
    "park00_icslp",
    "kiang00_icslp"
   ]
  },
  {
   "title": "Speaker, Dialect, and Language Recognition (Poster)",
   "papers": [
    "rodriguezlinares00_icslp",
    "dutat00_icslp",
    "tanprasert00_icslp",
    "schwardt00_icslp",
    "thyes00_icslp",
    "surendran00_icslp",
    "jin00_icslp",
    "schwardt00b_icslp",
    "lindberg00_icslp",
    "ortegagarcia00_icslp",
    "liu00b_icslp",
    "peters00_icslp",
    "tsoi00_icslp",
    "ma00_icslp",
    "si00_icslp",
    "mariethoz00_icslp",
    "pan00b_icslp",
    "wang00d_icslp",
    "chaudhari00_icslp",
    "masuko00_icslp",
    "parveen00_icslp",
    "itoh00_icslp",
    "tsai00_icslp",
    "ezzaidi00_icslp",
    "faundezzanu00_icslp",
    "uchibe00_icslp",
    "liu00c_icslp"
   ]
  },
  {
   "title": "Prosody and Paralinguistics (Special Session)",
   "papers": [
    "lee00_icslp",
    "fant00_icslp",
    "kasuya00_icslp",
    "maekawa00_icslp",
    "ohno00_icslp",
    "cao00_icslp",
    "eda00_icslp",
    "erickson00_icslp",
    "hirose00_icslp",
    "mozziconacci00_icslp",
    "scherer00_icslp",
    "kang00_icslp"
   ]
  },
  {
   "title": "Generation and Synthesis of Spoken Language 1, 2",
   "papers": [
    "morais00_icslp",
    "wang00e_icslp",
    "coorman00_icslp",
    "lyu00_icslp",
    "yamada00_icslp",
    "law00_icslp",
    "black00_icslp",
    "nakatani00_icslp",
    "takara00b_icslp",
    "pearson00_icslp",
    "goubanova00_icslp",
    "hirschfeld00b_icslp",
    "jain00_icslp"
   ]
  },
  {
   "title": "Speaker, Dialect, and Language Recognition 1, 2",
   "papers": [
    "choi00c_icslp",
    "tran00b_icslp",
    "gu00c_icslp",
    "heck00_icslp",
    "sivakumaran00_icslp",
    "liu00d_icslp",
    "jin00b_icslp",
    "reynolds00_icslp",
    "rosenberg00_icslp",
    "montacie00_icslp",
    "andrews00_icslp",
    "zhen00_icslp",
    "quatieri00_icslp",
    "teunen00_icslp"
   ]
  },
  {
   "title": "Linguistics, Phonology, Phonetics, and Psycholinguistics (Poster)",
   "papers": [
    "millerockhuizen00_icslp",
    "matsui00_icslp",
    "bijankhan00_icslp",
    "wang00f_icslp",
    "whiteside00b_icslp",
    "shimizu00_icslp",
    "zhang00f_icslp",
    "cosi00_icslp",
    "lopezdeipina00b_icslp",
    "gowjr00_icslp",
    "kajarakar00_icslp",
    "vasilescu00_icslp",
    "behne00_icslp",
    "chotimongkol00_icslp",
    "fon00_icslp",
    "arvaniti00_icslp",
    "ouden00_icslp",
    "tsukada00_icslp",
    "ward00_icslp",
    "elimam00_icslp",
    "schramm00_icslp",
    "colin00_icslp",
    "chen00g_icslp",
    "belvin00b_icslp",
    "chen00h_icslp",
    "wang00g_icslp",
    "potamianos00_icslp",
    "han00_icslp",
    "caspers00b_icslp",
    "swerts00b_icslp",
    "fry00_icslp",
    "tomokiyo00b_icslp",
    "noguchi00_icslp",
    "sato00_icslp"
   ]
  },
  {
   "title": "Spoken and Multi-Modal Dialogue Systems",
   "papers": [
    "narayanan00_icslp",
    "thompson00_icslp",
    "wang00h_icslp",
    "komatani00_icslp",
    "strom00_icslp",
    "breen00b_icslp",
    "brugnara00_icslp",
    "chuang00_icslp",
    "suzuki00_icslp",
    "ng00_icslp",
    "logan00_icslp",
    "jin00c_icslp",
    "weber00_icslp",
    "koumpis00_icslp",
    "tsai00b_icslp",
    "oviatt00_icslp",
    "chai00_icslp",
    "han00b_icslp",
    "miyazaki00_icslp",
    "kikuchi00_icslp",
    "wang00i_icslp",
    "turunen00_icslp",
    "pellom00_icslp",
    "bilici00_icslp",
    "bechet00_icslp",
    "mao00_icslp",
    "dohsaka00_icslp",
    "cheng00_icslp"
   ]
  },
  {
   "title": "Speech, Facial Expression, and Gesture",
   "papers": [
    "chu00b_icslp",
    "ma00b_icslp",
    "reveret00_icslp",
    "minnis00_icslp"
   ]
  },
  {
   "title": "Generation and Synthesis of Spoken Language 3",
   "papers": [
    "wu00c_icslp",
    "seneff00b_icslp",
    "saito00_icslp",
    "huang00_icslp",
    "minematsu00b_icslp",
    "lindgren00_icslp",
    "ohtsuka00_icslp"
   ]
  },
  {
   "title": "Speaker, Dialect, and Language Recognition 3",
   "papers": [
    "yuo00_icslp",
    "zhao00_icslp",
    "bellot00_icslp",
    "beaugendre00_icslp",
    "scherer00b_icslp",
    "wang00j_icslp"
   ]
  },
  {
   "title": "Miscellaneous Topics 2 [M,J]",
   "papers": [
    "zhao00b_icslp",
    "li00f_icslp",
    "maciasguarasa00_icslp",
    "gallardoantolin00_icslp",
    "suontausta00_icslp",
    "meunier00_icslp",
    "vair00_icslp",
    "gemello00_icslp",
    "hariharan00_icslp",
    "eide00_icslp",
    "zweig00_icslp",
    "yamamoto00c_icslp",
    "sansegundo00_icslp",
    "seide00_icslp",
    "seyyedsalehi00_icslp",
    "hofmann00_icslp",
    "sun00b_icslp",
    "lau00_icslp",
    "kaiser00_icslp",
    "maucec00_icslp",
    "janiszek00_icslp",
    "tan00_icslp",
    "chengalvarayan00b_icslp",
    "atake00_icslp",
    "varona00_icslp",
    "schwenk00_icslp",
    "watanabe00_icslp",
    "schluter00_icslp",
    "meinedo00_icslp",
    "meinedo00b_icslp",
    "leeuwen00_icslp",
    "zhao00c_icslp",
    "berthommier00_icslp",
    "zhou00c_icslp",
    "stephenson00_icslp",
    "das00_icslp",
    "kojima00_icslp",
    "galescu00b_icslp",
    "ogata00_icslp",
    "jia00_icslp",
    "xu00c_icslp",
    "yim00_icslp",
    "chang00_icslp",
    "georgila00b_icslp",
    "eklund00_icslp",
    "terashima00_icslp",
    "kato00b_icslp",
    "shimodaira00_icslp",
    "shu00_icslp",
    "colthurst00_icslp",
    "chen00i_icslp",
    "li00g_icslp",
    "miyajima00_icslp",
    "nanjo00_icslp",
    "chiang00_icslp",
    "yuan00_icslp",
    "markov00_icslp",
    "hazen00_icslp",
    "yu00c_icslp",
    "guo00_icslp",
    "ren00_icslp",
    "elmeliani00_icslp",
    "yang00d_icslp",
    "miwa00_icslp",
    "loog00_icslp",
    "holmes00_icslp",
    "kirchhoff00_icslp",
    "venugopal00_icslp",
    "keung00_icslp",
    "wang00k_icslp"
   ]
  },
  {
   "title": "Trans-Modal and Multi-Modal Human-Computer Interaction (Special Session)",
   "papers": [
    "oviatt00b_icslp",
    "munhall00_icslp",
    "neti00_icslp",
    "gao00c_icslp",
    "nakamura00_icslp",
    "sako00_icslp",
    "hewitt00_icslp",
    "huang00b_icslp",
    "huang00c_icslp",
    "bernstein00_icslp",
    "thathong00_icslp"
   ]
  },
  {
   "title": "Signal Analysis, Processing, and Feature Extraction 1, 2",
   "papers": [
    "li00h_icslp",
    "xia00_icslp",
    "lu00b_icslp",
    "potamitis00_icslp",
    "krstulovic00_icslp",
    "shire00_icslp",
    "saon00_icslp",
    "ellis00_icslp",
    "choi00d_icslp",
    "prasad00_icslp",
    "yan00_icslp",
    "saruwatari00_icslp",
    "estienne00_icslp",
    "hernando00_icslp"
   ]
  },
  {
   "title": "Language Modeling",
   "papers": [
    "bod00_icslp",
    "goodman00_icslp",
    "wu00d_icslp",
    "deligne00_icslp",
    "tanigaki00_icslp",
    "zheng00c_icslp"
   ]
  },
  {
   "title": "Acoustic Modeling",
   "papers": [
    "richardson00_icslp",
    "sandness00_icslp",
    "goel00_icslp",
    "nock00_icslp",
    "weber00b_icslp",
    "singh00_icslp",
    "arsigny00_icslp"
   ]
  },
  {
   "title": "Prosody (Poster)",
   "papers": [
    "zhu00b_icslp",
    "wang00l_icslp",
    "watanabe00b_icslp",
    "harnud00_icslp",
    "imoto00_icslp",
    "buhmann00_icslp",
    "blin00_icslp",
    "teixeira00_icslp",
    "minematsu00c_icslp",
    "ni00_icslp",
    "zu00_icslp",
    "holm00_icslp",
    "veldhuis00_icslp",
    "jun00_icslp",
    "hirose00b_icslp",
    "banno00_icslp",
    "sakurai00_icslp",
    "erdem00_icslp",
    "fackrell00_icslp",
    "syrdal00_icslp",
    "kochanski00_icslp",
    "dong00_icslp",
    "erickson00b_icslp",
    "janse00_icslp",
    "hiroshige00_icslp",
    "sakurai00b_icslp",
    "louw00_icslp"
   ]
  },
  {
   "title": "Generation and Synthesis of Spoken Language (Poster)",
   "papers": [
    "christogiannis00_icslp",
    "baptist00_icslp",
    "kim00c_icslp",
    "toda00_icslp",
    "libossek00_icslp",
    "hain00_icslp",
    "tillmann00_icslp",
    "kwon00_icslp",
    "wouters00_icslp",
    "lenzo00_icslp",
    "portele00_icslp",
    "conkie00_icslp",
    "jensen00b_icslp",
    "yi00_icslp",
    "wang00m_icslp",
    "hirai00_icslp",
    "mittrapiyanuruk00_icslp",
    "xu00d_icslp",
    "isogai00_icslp",
    "sansegundo00b_icslp",
    "niu00_icslp",
    "liu00e_icslp",
    "li00i_icslp",
    "kawaguchi00_icslp",
    "lee00b_icslp",
    "lindberg00b_icslp",
    "lv00_icslp",
    "chengalvarayan00c_icslp",
    "franz00_icslp",
    "wakita00_icslp",
    "feng00_icslp",
    "aizawa00_icslp"
   ]
  },
  {
   "title": "Rules and Corpora (Special Session)",
   "papers": [
    "santen00_icslp",
    "syrdal00b_icslp",
    "campbell00_icslp",
    "kawai00_icslp",
    "sproat00_icslp",
    "picheny00_icslp",
    "kubozono00_icslp",
    "iwahashi00_icslp",
    "sagisaka00_icslp"
   ]
  },
  {
   "title": "Perception and Comprehension of Spoken Language 1, 2",
   "papers": [
    "makarova00_icslp",
    "kubozono00b_icslp",
    "akagi00_icslp",
    "palomaki00_icslp",
    "hirose00c_icslp",
    "fernandez00_icslp",
    "klabbers00_icslp",
    "wang00n_icslp",
    "powell00_icslp",
    "luk00_icslp",
    "lai00_icslp",
    "tyler00_icslp",
    "zolfaghari00_icslp"
   ]
  },
  {
   "title": "Spoken Language Processing",
   "papers": [
    "pargellis00_icslp",
    "ramaswamy00_icslp",
    "potamianos00b_icslp",
    "liu00f_icslp",
    "furui00_icslp",
    "takezawa00_icslp",
    "yoma00_icslp"
   ]
  },
  {
   "title": "Acoustic Features for Robust Speech Recognition",
   "papers": [
    "ito00b_icslp",
    "christensen00_icslp",
    "seltzer00_icslp",
    "hermus00_icslp",
    "ming00_icslp",
    "hariharan00b_icslp",
    "oshaughnessy00_icslp"
   ]
  },
  {
   "title": "Prosody, Acquisition, and Learning",
   "papers": [
    "tajima00_icslp",
    "zitouni00_icslp",
    "tsubota00_icslp",
    "holter00_icslp",
    "wu00e_icslp",
    "miwa00b_icslp",
    "strik00_icslp",
    "kitamura00_icslp",
    "zhang00g_icslp",
    "vermaak00_icslp",
    "graciarena00_icslp",
    "zeng00_icslp",
    "wang00o_icslp",
    "iwaki00_icslp",
    "geravanchizadeh00_icslp",
    "pfitzinger00_icslp",
    "amdal00_icslp",
    "bell00b_icslp",
    "liu00g_icslp",
    "suzuki00b_icslp",
    "bonneaumaynard00_icslp",
    "deng00b_icslp",
    "aylett00_icslp",
    "sakamoto00_icslp",
    "takamaru00_icslp",
    "makarova00b_icslp",
    "deng00c_icslp",
    "xu00e_icslp",
    "klabbers00b_icslp",
    "cao00b_icslp",
    "gabrea00_icslp",
    "lyberg00_icslp",
    "wu00f_icslp"
   ]
  },
  {
   "title": "Adaptation and Acquisition in Spoken Language Processing (Poster)",
   "papers": [
    "zhao00d_icslp",
    "zhang00h_icslp",
    "li00j_icslp",
    "sarikaya00_icslp",
    "fujita00_icslp",
    "wang00p_icslp",
    "zhou00d_icslp",
    "tsuge00_icslp",
    "nieuwoudt00_icslp",
    "sato00b_icslp",
    "torre00_icslp",
    "kim00d_icslp",
    "liu00h_icslp",
    "chen00j_icslp",
    "potamianos00c_icslp",
    "komatsu00_icslp",
    "ciocca00_icslp",
    "yip00_icslp",
    "salomon00_icslp",
    "otake00_icslp",
    "yamakawa00_icslp",
    "mcqueen00_icslp",
    "weber00c_icslp",
    "janse00b_icslp",
    "traunmuller00_icslp"
   ]
  },
  {
   "title": "Large Vocabulary Continuous Speech Recognition",
   "papers": [
    "gauvain00_icslp",
    "gao00d_icslp",
    "aubert00_icslp",
    "deng00d_icslp",
    "fischer00_icslp",
    "kummert00_icslp",
    "huang00d_icslp"
   ]
  },
  {
   "title": "Speech Coding and Transmission",
   "papers": [
    "zhang00i_icslp",
    "kohata00_icslp",
    "ribeiro00_icslp",
    "hu00b_icslp",
    "chengalvarayan00d_icslp",
    "huerta00_icslp"
   ]
  },
  {
   "title": "Acoustic Model Adaptation",
   "papers": [
    "rajput00_icslp",
    "cox00_icslp",
    "haebumbach00_icslp",
    "afify00_icslp",
    "choi00e_icslp",
    "acero00_icslp",
    "vergyri00_icslp"
   ]
  },
  {
   "title": "Miscellaneous 3 [D,E,F,I,P,N,R,S,U,W,Y,Z]",
   "papers": [
    "oviatt00c_icslp",
    "rodman00_icslp",
    "alku00b_icslp",
    "ito00c_icslp",
    "matsumura00_icslp",
    "engwall00b_icslp",
    "bae00_icslp",
    "bruyninckx00_icslp",
    "ramabhadran00_icslp",
    "fernandez00b_icslp",
    "bilmes00_icslp",
    "jan00_icslp",
    "wijngaarden00_icslp",
    "zhen00b_icslp",
    "carre00b_icslp",
    "hant00_icslp",
    "larson00_icslp",
    "zundert00b_icslp",
    "zhang00j_icslp",
    "joto00_icslp",
    "zhao00e_icslp",
    "godinollorente00_icslp",
    "tsou00_icslp",
    "caraty00_icslp",
    "dybkjr00_icslp",
    "brun00_icslp",
    "lepage00_icslp",
    "auckenthaler00_icslp",
    "gu00d_icslp",
    "stapert00_icslp",
    "zilca00_icslp",
    "liu00i_icslp",
    "bosch00_icslp",
    "nishida00_icslp",
    "kim00e_icslp",
    "yoma00b_icslp",
    "sabac00_icslp",
    "magrinchagnolleau00_icslp",
    "balleda00_icslp",
    "kronenberg00_icslp",
    "rahim00_icslp",
    "churcher00_icslp",
    "johnsen00b_icslp",
    "huang00e_icslp",
    "aoyama00_icslp",
    "zhou00e_icslp",
    "qu00_icslp",
    "stark00_icslp",
    "nishizaki00_icslp",
    "lai00b_icslp",
    "vark00_icslp",
    "takao00_icslp",
    "hansen00b_icslp",
    "nakajima00_icslp",
    "tsukahara00_icslp",
    "kitawaki00_icslp",
    "sugaya00_icslp",
    "maruyama00_icslp",
    "ogner00_icslp",
    "torihara00_icslp",
    "chengalvarayan00e_icslp",
    "chen00k_icslp",
    "hermus00b_icslp",
    "chang00b_icslp"
   ]
  },
  {
   "title": "Language Resources and Technology Evaluation (Special Session)",
   "papers": [
    "glass00_icslp",
    "lamel00_icslp",
    "heckmann00_icslp",
    "li00k_icslp",
    "fiscus00_icslp",
    "nakamura00b_icslp",
    "pearce00_icslp",
    "tillmann00b_icslp",
    "millar00_icslp",
    "soong00_icslp"
   ]
  },
  {
   "title": "Acquisition and Learning of Spoken Language 1, 2",
   "papers": [
    "riccardi00_icslp",
    "akiba00_icslp",
    "petrovskadelacretaz00_icslp",
    "mizumachi00_icslp",
    "sheikhzadeh00_icslp",
    "noth00_icslp",
    "roy00_icslp",
    "jun00b_icslp",
    "siu00_icslp",
    "mctear00b_icslp",
    "bunnell00_icslp"
   ]
  },
  {
   "title": "Acoustics of Spoken Language 1, 2",
   "papers": [
    "nakai00_icslp",
    "czap00_icslp",
    "son00_icslp",
    "meng00c_icslp",
    "soltau00_icslp",
    "guo00b_icslp",
    "chen00l_icslp",
    "chen00m_icslp",
    "hung00_icslp",
    "gao00e_icslp",
    "pan00c_icslp",
    "hasegawajohnson00_icslp"
   ]
  },
  {
   "title": "Recognition and Understanding of Spoken Language 3, 4",
   "papers": [
    "jiang00b_icslp",
    "luo00b_icslp",
    "wrench00_icslp",
    "mak00_icslp",
    "sivadas00_icslp",
    "hanazawa00b_icslp",
    "jiang00c_icslp",
    "purnell00_icslp",
    "jia00b_icslp",
    "hain00b_icslp",
    "fung00_icslp",
    "gu00e_icslp",
    "kuo00_icslp",
    "tanaka00_icslp",
    "lefevre00_icslp",
    "mukherjee00_icslp",
    "saito00b_icslp",
    "cox00b_icslp",
    "ganapathiraju00_icslp",
    "gu00f_icslp",
    "zhang00k_icslp",
    "petrushin00_icslp",
    "lyu00b_icslp",
    "emam00_icslp",
    "vaich00_icslp",
    "meron00_icslp",
    "cosi00b_icslp",
    "fohr00_icslp",
    "kiss00_icslp",
    "frankel00_icslp",
    "shobaki00_icslp",
    "wu00g_icslp",
    "chung00_icslp",
    "barker00b_icslp",
    "lee00c_icslp",
    "yu00d_icslp",
    "seward00_icslp",
    "nguyen00_icslp",
    "ogawa00_icslp",
    "chan00_icslp",
    "liao00_icslp",
    "neukirchen00_icslp",
    "zhao00f_icslp",
    "yu00e_icslp",
    "pan00d_icslp",
    "yamamoto00d_icslp",
    "hori00_icslp",
    "chang00c_icslp",
    "novak00_icslp",
    "huang00f_icslp",
    "he00_icslp",
    "tomokiyo00c_icslp",
    "li00l_icslp",
    "botterweck00_icslp",
    "zheng00d_icslp",
    "pfau00_icslp",
    "hwang00_icslp",
    "wu00h_icslp",
    "itoh00b_icslp",
    "geutner00_icslp",
    "chou00_icslp",
    "hunsinger00_icslp",
    "raghavan00_icslp",
    "nouza00_icslp",
    "wu00i_icslp",
    "rao00_icslp"
   ]
  },
  {
   "title": "Problems and Prospects of Trans-Lingual Communication (Special Session)",
   "papers": [
    "yamamoto00e_icslp",
    "blanchon00_icslp",
    "zong00_icslp",
    "bangalore00b_icslp",
    "engel00_icslp",
    "lazzari00_icslp",
    "boitet00_icslp",
    "zong00b_icslp",
    "watanabe00c_icslp",
    "gruhn00_icslp"
   ]
  },
  {
   "title": "Spoken Language Resources, Labeling, and Assessment",
   "papers": [
    "beringer00_icslp",
    "samudravijaya00_icslp",
    "wang00q_icslp",
    "sjolander00_icslp",
    "campbell00b_icslp",
    "timoney00_icslp",
    "kawahara00_icslp"
   ]
  },
  {
   "title": "Robust Modeling",
   "papers": [
    "huo00_icslp",
    "roberts00_icslp",
    "wester00b_icslp",
    "mou00_icslp",
    "maciasguarasa00b_icslp",
    "goronzy00_icslp",
    "ganapathiraju00b_icslp"
   ]
  },
  {
   "title": "Adaptation and Acquisition in Spoken Language Processing 1, 2",
   "papers": [
    "sasaki00_icslp",
    "moore00_icslp",
    "sun00c_icslp",
    "chung00b_icslp",
    "chengalvarayan00f_icslp",
    "iwahashi00b_icslp",
    "acero00b_icslp",
    "bacchiani00_icslp",
    "myrvoll00_icslp",
    "chien00_icslp",
    "pitz00_icslp",
    "chengalvarayan00g_icslp",
    "dharanipragada00b_icslp",
    "doh00_icslp"
   ]
  },
  {
   "title": "Acoustics of Spoken Language (Poster)",
   "papers": [
    "hosom00_icslp",
    "ferreiroslopez00_icslp",
    "nedel00_icslp",
    "shen00_icslp",
    "yi00b_icslp",
    "schillo00_icslp",
    "nedel00b_icslp",
    "tarui00_icslp",
    "iwaki00b_icslp",
    "ramabhadran00b_icslp",
    "padmanabhan00_icslp",
    "li00m_icslp",
    "matousek00_icslp",
    "chou00b_icslp",
    "maneenoi00_icslp",
    "wu00j_icslp",
    "howitt00_icslp",
    "meyer00_icslp",
    "chen00n_icslp",
    "liu00j_icslp",
    "jan00b_icslp",
    "wang00r_icslp",
    "chen00o_icslp"
   ]
  },
  {
   "title": "Signal Analysis, Processing, and Feature Extraction",
   "papers": [
    "lu00c_icslp",
    "krstulovic00b_icslp",
    "kawahara00b_icslp",
    "huang00g_icslp",
    "murphy00_icslp",
    "sun00d_icslp",
    "soleicasals00_icslp",
    "sugiyama00_icslp",
    "funaki00_icslp",
    "zolfaghari00b_icslp",
    "farooq00b_icslp",
    "karnjanadecha00_icslp",
    "sasou00_icslp",
    "dociofernandez00_icslp",
    "kurematsu00c_icslp",
    "zhang00l_icslp",
    "araki00_icslp",
    "li00n_icslp",
    "beringer00b_icslp",
    "radova00_icslp",
    "fabbrizio00_icslp",
    "strik00b_icslp",
    "raake00_icslp"
   ]
  }
 ],
 "doi": "10.21437/ICSLP.2000"
}