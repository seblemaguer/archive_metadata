{
 "location": "Santa Cruz, CA, USA",
 "startDate": "7/8/1999",
 "endDate": "10/8/1999",
 "conf": "AVSP",
 "year": "1999",
 "name": "avsp_1999",
 "series": "AVSP",
 "SIG": "AVISA",
 "title": "Auditory-Visual Speech Processing",
 "title1": "Auditory-Visual Speech Processing",
 "date": "7-10 August 1999",
 "papers": {
  "nass99_avsp": {
   "authors": [
    [
     "Clifford",
     "Nass"
    ],
    [
     "Li",
     "Gong"
    ]
   ],
   "title": "Maximized Modality or constrained consistency?",
   "original": "av99_001",
   "page_count": 5,
   "order": 1,
   "p1": "paper 1",
   "pn": "",
   "abstract": [
    "A key debate in the interface literature is whether to make each modality as human-like as possible or instead to match the level of humanness of each modality. A three-condition experiment (recorded speech with synthesized face; synthesized speech with a synthesized face; or recorded speech with no face) was conducted (N = 36). When the modalities were matched, individuals exhibited significantly greater impression management and willingness to disclose personal information and felt more comfortable using the interface.\n",
    ""
   ]
  },
  "lewkowicz99_avsp": {
   "authors": [
    [
     "David J.",
     "Lewkowicz"
    ]
   ],
   "title": "Infants' perception of the audible, visible and bimodal attributes of talking and singing faces",
   "original": "av99_002",
   "page_count": 17,
   "order": 2,
   "p1": "paper 2",
   "pn": "",
   "abstract": [
    "Human faces and accompanying voices are a ubiquitous part of the infant's perceptual experience. They serve as a major vehicle for the acquisition of linguistic, social, and emotional skills, just to name a few. Therefore, the study of how infants perceive the audible, visible and combined attributes of face/voice compounds can show how various features of face/voice compounds contribute to the acquisition of higher-level skills. I review the findings from a series of studies in which we investigated infants' response to a variety of features of face/voice compounds. These findings show that there are important developmental differences in the way infants respond to the audible, visible and bimodal components of face/voice compounds and that these differences are dependent on the specific nature of the information given. Overall, the results suggest that infants, like adults, may respond to multiple sources of information when attempting to evaluate the \"message\" carried by bimodal speech but that their functional use of this information is limited by the infant's developmental status.\n",
    ""
   ]
  },
  "stein99_avsp": {
   "authors": [
    [
     "Barry E.",
     "Stein"
    ],
    [
     "Mark T.",
     "Wallace"
    ],
    [
     "Wan",
     "Jiang"
    ],
    [
     "Huai",
     "Jian"
    ],
    [
     "J. William",
     "Vaughn"
    ]
   ],
   "title": "Cross-Modal Integration: Bringing Coherence to the Sensory World",
   "original": "av99_003",
   "page_count": 6,
   "order": 3,
   "p1": "paper 3",
   "pn": "",
   "abstract": [
    "The multisensory superior colliculus (SC) neuron has been used as an effective model for exploring the neural bases of multisensory integration, in part because of the high incidence of such neurons in the deep layers of the structure and, in part, because of the important role of deep layer neurons in overt attentive and orientation behavior. These factors make it possible not only to obtain a reasonable sample with which to examine how single neurons synthesize cross-modal information during electrophysiological investigations, but also makes it possible to closely correlate findings from physiological and behavioral studies [1].\n",
    ""
   ]
  },
  "norrix99_avsp": {
   "authors": [
    [
     "Linda W.",
     "Norrix"
    ],
    [
     "Kerry P.",
     "Green"
    ]
   ],
   "title": "Visual context effects on the perception of /r/ and /l/: Varying F1 and F2 acoustic characteristics",
   "original": "av99_004",
   "page_count": 6,
   "order": 4,
   "p1": "paper 4",
   "pn": "",
   "abstract": [
    "Context effects occur when the phonetic perception of an acoustic signal is modified by the surrounding phonetic context. Two experiments were conducted investigating visual context effects on the perception of acoustic speech tokens. In the first experiment a visual bilabial articulation [ibi] was paired with synthetic acoustic tokens from /iri/ to /ili/ continua. In the second experiment a visual [aba] was paired with tokens from synthetic /ara/ to /ala/ continua. Listeners were asked if they perceived the disyllables as containing an [r] or [l].  We found that auditory-visual identification functions were associated with a shift toward more [l] responses compared to auditory only identification functions. These data are important for developing and testing models of auditory-visual context effects in speech perception.\n",
    ""
   ]
  },
  "cox99_avsp": {
   "authors": [
    [
     "Ethan A.",
     "Cox"
    ],
    [
     "Linda W.",
     "Norrix"
    ],
    [
     "Kerry P.",
     "Green"
    ]
   ],
   "title": "The contribution of visual information to on-line sentence processing: Evidence from phoneme monitoring",
   "original": "av99_005",
   "page_count": 6,
   "order": 5,
   "p1": "paper 5",
   "pn": "",
   "abstract": [
    "Previous studies in audiovisual speech perception have concentrated mainly on situations in which the auditory information is degraded or presented in noise, or situations in which the visual and auditory information are in conflict, as in the McGurk effect. Relatively little work has addressed the issue of how the availability of visual information might contribute to levels of processing beyond that of the phonetic. We report here on some initial results from a series of studies targeting audiovisual processing in lexical and sentential structures. We also present pilot data that indicate visual information may speed reaction times in phoneme monitoring for targets presented in neutral sentence contexts.\n",
    ""
   ]
  },
  "haan99_avsp": {
   "authors": [
    [
     "M. de",
     "Haan"
    ],
    [
     "Ruth",
     "Campbell"
    ]
   ],
   "title": "Lateralized event-related cortical potentials in discriminating images of facial speech",
   "original": "av99_006",
   "page_count": 4,
   "order": 6,
   "p1": "paper 6",
   "pn": "",
   "abstract": [
    "Scalp electrical potentials (event related potentials - ERPs) were measured in right-handed hearing participants making a speeded choice to two types of facial image: images of speech (\"oo\" or \"ee?\") and images of expression (\"happy\" or \"sad?\"). A different spatial distribution of brain electrical activity was found for the two tasks. While the early-latency P1 potential was distributed similarly for the two tasks, the face-sensitive N170 waveform showed task sensitivity. The N170 showed greater left hemisphere (LH) localization for the speech classification task, but more bilateral, or right hemisphere (RH) localization for the expression classification task. The precise pattern depended on whether a full-face (Experiment 1) or a half-face (Experiment 2 ) was seen but was similar for both. The N170 has not previously been shown to be influenced by the nature of an active decision in a face processing task..\n",
    ""
   ]
  },
  "campbell99_avsp": {
   "authors": [
    [
     "Ruth",
     "Campbell"
    ],
    [
     "G.",
     "Calvert"
    ],
    [
     "M.",
     "Brammer"
    ],
    [
     "M.",
     "MacSweeney"
    ],
    [
     "S.",
     "Surguladze"
    ],
    [
     "P.",
     "McGuire"
    ],
    [
     "B.",
     "Woll"
    ],
    [
     "S.",
     "Williams"
    ],
    [
     "E.",
     "Amaro"
    ],
    [
     "A.S.",
     "David"
    ]
   ],
   "title": "Activation in auditory cortex by speechreading in hearing people: FMRI studies",
   "original": "av99_007",
   "page_count": 6,
   "order": 7,
   "p1": "paper 7",
   "pn": "",
   "abstract": [
    "Which brain networks support simple silent speechreading (identifying spoken numbers by eye alone)? We summarize recent fMRI findings that show auditory cortex is reliably activated by seen speech. Activation of these regions is robust under numerous testing conditions, and parts of the network are specific to speechreading rather than viewing faces performing rhythmic lower face movements that cannot be construed as speech (facial gurning). In one adult with a longstanding malfunction of auditory cortex, speechreading was both functionally impaired and cortically anomalous. Areas traditionally described as auditory cortex are probably specialized for the perception of natural segmented language, rather than for the perception of heard signals.\n",
    ""
   ]
  },
  "kanzaki99_avsp": {
   "authors": [
    [
     "Rika",
     "Kanzaki"
    ],
    [
     "Ruth",
     "Campbell"
    ]
   ],
   "title": "Effect of facial brightness reversal on visual and audiovisual speech perception",
   "original": "av99_008",
   "page_count": 5,
   "order": 8,
   "p1": "paper 8",
   "pn": "",
   "abstract": [
    "To investigate the nature of facial information involved in visual and audiovisual speech perception, we examined the influence of brightness reversal on speechreading and the McGurk effect. Brightness reversed (photonegative) images are harder to 'read as faces' than images with normal brightness relations, but, when animated, maintain all the movement information of a speaking face. If speechreading were dependent solely on analysis of the dynamic features of a speaking face, this manipulation should have little effect on susceptibility to McGurk fusion illusions. Conversely, if speechreading made use primarily of visual end-state forms ('key frames'), then this manipulation should profoundly reduce susceptibility to McGurk illusions. Although the brightness reversed static images of non-labial articulations were hard to identify correctly, natural movement of brightness-reversed faces generated a McGurk effect, although it was weaker than for normal brightness faces. These results suggest that both dynamic (time-varying) and image quality (not time-varying) are involved in the visual perception of spoken language and that the strength of their relative influence depends on the demands of the task.\n",
    ""
   ]
  },
  "gagne99_avsp": {
   "authors": [
    [
     "J.P.",
     "Gagné"
    ],
    [
     "M.J.",
     "Charest"
    ],
    [
     "A.J.",
     "Rochette"
    ]
   ],
   "title": "An analysis of the effects of clear speech on the visual-speech intelligibility of consonants",
   "original": "av99_009",
   "page_count": 6,
   "order": 9,
   "p1": "paper 9",
   "pn": "",
   "abstract": [
    "Gagne and Rochette [1] reported that the use of clear speech improved the overall visual-speech intelligibility of consonants. The data from that investigation were re-analyzed to examine the effects of clear speech on six individual consonants (/b,v,d,z,Z,g/) presented in three different symmetrical v-C-v contexts (/a,i,y/). The stimuli were spoken by six female talkers who produced four iterations of the stimulus set in two different speaking styles: conversational and clear speech. The results were based on the responses obtained from 13 adult subjects. Only the data from the visual-speech recognition task were considered. The results showed significant main effects of consonant and vowel context, as well as a significant interaction between consonant and vowel context. Overall, the findings suggest that the visual-speech intelligibility of consonants is strongly influenced by the vowel context in which they are spoken. In general, greater clear speech benefits were observed for consonants with a place of articulation located at the back of the oral cavity (i.e., post-alveolars and velars) than those with a place of articulation that is more centrally located in the oral cavity (i.e., labio-dentals and alveolars). The effect of clear speech on the visual-speech intelligibility of bilabials was negligible.\n",
    ""
   ]
  },
  "seitz99_avsp": {
   "authors": [
    [
     "Philip Franz",
     "Seitz"
    ],
    [
     "Ken W.",
     "Grant"
    ]
   ],
   "title": "Modality, perceptual encoding speed, and time-course of phonetic information",
   "original": "av99_010",
   "page_count": 5,
   "order": 10,
   "p1": "paper 10",
   "pn": "",
   "abstract": [
    "This study examined the perceptual processing of time-gated auditory-visual (AV), auditory (A), and visual (V) spoken words. The primary goal was to assess the extent to which stimulus information versus perceptual processing limitations underlie modality-related perceptual encoding speed differences in AV, A, and V spoken word recognition. Another goal was to add to the scant literature on the comparative time-course of phonetic information in AV, A, and V spoken words [1]. In terms of duration of speech signal required for accurate word identification, it was found that AV<A<V. For individual word stimuli, there were strong predictive relations between unimodal encoding speed and gating measures. Perceptual encoding of V words is slower than predicted based on stimulus information alone.\n",
    ""
   ]
  },
  "brancazio99_avsp": {
   "authors": [
    [
     "Lawrence",
     "Brancazio"
    ]
   ],
   "title": "Lexical influences on the McGurk effect",
   "original": "av99_011",
   "page_count": 7,
   "order": 11,
   "p1": "paper 11",
   "pn": "",
   "abstract": [
    "The purpose of this research was to explore the interrelationship of audiovisual speech perception and spoken word recognition. I tested whether an index of audiovisual integration, the \"McGurk effect,\" would be influenced by the lexical status of the stimuli. There was a significant increase in the McGurk effect when the visually-influenced percept formed a word than when it formed a nonword, and an increase in the effect when the auditory stimulus was a nonword compared to when it was a word. A second experiment ruled out a response-bias account of these findings by demonstrating a similar effect in nonword stimuli that differed in their number of word \"neighbors.\" I discuss the implications of these findings for theories of audiovisual speech perception.\n",
    ""
   ]
  },
  "davis99_avsp": {
   "authors": [
    [
     "Chris",
     "Davis"
    ],
    [
     "Jeesun",
     "Kim"
    ]
   ],
   "title": "Perception of clearly presented foreign language sounds: The effects of visible speech",
   "original": "av99_012",
   "page_count": 6,
   "order": 12,
   "p1": "paper 12",
   "pn": "",
   "abstract": [
    "Learning the sounds of a foreign language is difficult. The experiments reported here investigated whether visible speech can help. The focus of the experiments was on whether visible speech affects perception as well as the production of foreign speech sounds. The first experiment examined whether visible speech assists in the detection of a syllable within an unfamiliar foreign phrase. It was found that a syllable was more likely to be detected within a phrase when the participants could see the speaker's face. The second experiment investigated whether judgments about the duration of a foreign language phrase would be more accurate with visible speech compared to a sound only condition. It was found that in the visible speech condition participant's estimates of phrase duration correlated positively with actual duration, whereas in the sound only condition there was a negative correlation. Furthermore, with visible speech, estimates were close to the actual durations whereas those in the sound only condition tended to underestimate duration. The results are discussed with respect to previous findings and future applications.\n",
    ""
   ]
  },
  "burnham99_avsp": {
   "authors": [
    [
     "Denis",
     "Burnham"
    ],
    [
     "Susanna",
     "Lau"
    ]
   ],
   "title": "The integration of auditory and visual speech information with foreign speakers: The role of expectancy",
   "original": "av99_013",
   "page_count": 7,
   "order": 13,
   "p1": "paper 13",
   "pn": "",
   "abstract": [
    "It has been found that auditory-visual integration in the McGurk effect is affected by the relationship between the language of the speaker and that of the perceiver: for a foreign speaker, perceivers tend to incorporate visual information to a greater extent. We investigated whether this is due to the perceivers' detection of the speech sounds as foreign or to an expectancy based upon the appearance of the speaker. English, Japanese, Cantonese, and Thai subjects were presented with English, Japanese, Cantonese, and Thai speakers in a condition in which an expectancy was set (trials blocked by speaker language) or in a random trials condition. There were indeed foreign language effects, albeit in the opposite direction to that expected, and these occurred mainly as the results of expectancies based on the appearance of the speaker rather than the perceived deviation of foreign speech sounds from native language prototypes.\n",
    ""
   ]
  },
  "baldwin99_avsp": {
   "authors": [
    [
     "James F.",
     "Baldwin"
    ],
    [
     "Trevor P.",
     "Martin"
    ],
    [
     "Mehreen",
     "Saeed"
    ]
   ],
   "title": "Automatic computer lip-reading using fuzzy set theory",
   "original": "av99_014",
   "page_count": 6,
   "order": 14,
   "p1": "paper 14",
   "pn": "",
   "abstract": [
    "This paper presents the application of fuzzy set theory to automatic computer lip-reading from video images. Simple rules based on fuzzy sets were generated using the mass assignment theory and were used for automatic feature extraction from video sequences. Probabilistic grid models were used to derive a knowledge base representing the visual data for phonemes or sounds. Phonemes from a medium sized vocabulary of words were used for training and testing and a reasonable accuracy for classification was achieved. The methods were also applied to the Tulips1 database and the results illustrate that the learning techniques are efficient and general enough to be applied to different speakers.\n",
    ""
   ]
  },
  "movellan99_avsp": {
   "authors": [
    [
     "Javier R.",
     "Movellan"
    ],
    [
     "Paul",
     "Mineiro"
    ]
   ],
   "title": "A diffusion network approach to visual speech recognition",
   "original": "av99_015",
   "page_count": 5,
   "order": 15,
   "p1": "paper 15",
   "pn": "",
   "abstract": [
    "In this paper we present an alternative to hidden Markov models for the recognition of image sequences. The approach is based on a stochastic version of recurrent neural networks, which we call diffusion networks. Contrary to hidden Markov models, diffusion networks operate with continuous state dynamics, and generate continuous paths. This aspect that may be beneficial in computer vision tasks in which continuity is a useful constraint. In this paper we review results required for the implementation of diffusion networks, and then apply them to a visual speech recognition task. Diffusion networks outperformed the results obtained with the best hidden Markov models.\n",
    ""
   ]
  },
  "niyogi99_avsp": {
   "authors": [
    [
     "Partha",
     "Niyogi"
    ],
    [
     "Eric",
     "Petajan"
    ],
    [
     "Jialin",
     "Zhong"
    ]
   ],
   "title": "Feature based representation for audio-visual speech recognition",
   "original": "av99_016",
   "page_count": 6,
   "order": 16,
   "p1": "paper 16",
   "pn": "",
   "abstract": [
    "In this paper, we consider the integration of acoustic and visual stimuli at the subphonemic level of the distinctive feature. We argue that this provides a natural intermediate level for audio-visual integration and discuss the visual and acoustic feature detection problems that are associated with this task.\n",
    ""
   ]
  },
  "talle99_avsp": {
   "authors": [
    [
     "B.",
     "Talle"
    ],
    [
     "A.",
     "Wichert"
    ]
   ],
   "title": "Audio-visual sensor fusion with neural architectures",
   "original": "av99_017",
   "page_count": 5,
   "order": 17,
   "p1": "paper 17",
   "pn": "",
   "abstract": [
    "In this paper we present a new word recognition system for monosyllabic words consisting of two types of neural networks which allows in an easy way the investigation of three different fusion architectures for audio-visual signals. Furthermore, two different kinds of preprocessing are compared: Besides low level data, a linear discriminant analysis is used for the audio and visual signals to reduce the dimensionality. Our cross-validation experiments show a slight advantage for an intermediate fusion model compared with an early fusion model which uses jointly preprocessed audio and visual data.\n",
    ""
   ]
  },
  "senior99_avsp": {
   "authors": [
    [
     "Andrew",
     "Senior"
    ],
    [
     "Chalapathy V.",
     "Neti"
    ],
    [
     "Benoit",
     "Maison"
    ]
   ],
   "title": "On the use of visual information for improving audio-based speaker recognition",
   "original": "av99_018",
   "page_count": 4,
   "order": 18,
   "p1": "paper 18",
   "pn": "",
   "abstract": [
    "Audio-based speaker identification degrades severely when there is a mismatch between training and test conditions either due to channel or noise. In this paper, we explore various techniques to fuse video based speaker identification with audio-based speaker identification to improve the performance under mismatch conditions.\n",
    ""
   ]
  },
  "barker99_avsp": {
   "authors": [
    [
     "J. P.",
     "Barker"
    ],
    [
     "F.",
     "Berthommier"
    ]
   ],
   "title": "Estimation of speech acoustics from visual speech features: A comparison of linear and non-linear models",
   "original": "av99_019",
   "page_count": 6,
   "order": 19,
   "p1": "paper 19",
   "pn": "",
   "abstract": [
    "This paper examines the degree of correlation between lip and jaw configuration and speech acoustics. The lip and jaw positions are characterised by a system of measurements taken from video images of the speaker's face and profile, and the acoustics are represented using line spectral pair parameters and a measure of RMS energy. A correlation is found between the measured acoustic parameters and a linear estimate of the acoustics recovered from the visual data. This correlation exists despite the simplicity of the mapping and is in rough agreement with correlations measured in earlier work by Yehia et al. The linear estimates are also compared to estimates made using nonlinear models. In particular it is shown that although performance of the two models is remarkably similar for static visual features, non-linear models are better able to handle dynamic features.\n",
    ""
   ]
  },
  "vatikiotisbateson99_avsp": {
   "authors": [
    [
     "E.",
     "Vatikiotis-Bateson"
    ],
    [
     "Takaaki",
     "Kuratate"
    ],
    [
     "Myuki",
     "Kamachi"
    ],
    [
     "Hani",
     "Yehia"
    ]
   ],
   "title": "Facial deformation parameters for audiovisual synthesis",
   "original": "av99_020",
   "page_count": 5,
   "order": 20,
   "p1": "paper 20",
   "pn": "",
   "abstract": [
    "Extracting reliable 3D facial deformation parameters from static facial postures is a major component of our system for audiovisual synthesis. This paper describes several important improvements to that process, including reduction of position alignment errors, simplification of the generic face mesh and, most important, increasing the range and variety of static postures used.\n",
    ""
   ]
  },
  "agelfors99_avsp": {
   "authors": [
    [
     "Eva",
     "Agelfors"
    ],
    [
     "Jonas",
     "Beskow"
    ],
    [
     "Björn",
     "Granström"
    ],
    [
     "Magnus",
     "Lundeberg"
    ],
    [
     "Giampiero",
     "Salvi"
    ],
    [
     "Karl-Eric",
     "Spens"
    ],
    [
     "Tobias",
     "Öhman"
    ]
   ],
   "title": "Synthetic visual speech driven from auditory speech",
   "original": "av99_021",
   "page_count": 5,
   "order": 21,
   "p1": "paper 21",
   "pn": "",
   "abstract": [
    "We have developed two different methods for using auditory, telephone speech to drive the movements of a synthetic face. In the first method, Hidden Markov Models (HMMs) were trained on a phonetically transcribed telephone speech database. The output of the HMMs was then fed into a rule-based visual speech synthesizer as a string of phonemes together with time labels. In the second method, Artificial Neural Networks (ANNs) were trained on the same database to map acoustic parameters directly to facial control parameters. These target parameter trajectories were generated by using phoneme strings from a database as input to the visual speech synthesis The two methods were evaluated through audio-visual intelligibility tests with ten hearing impaired persons, and compared to \"ideal\" articulations (where no recognition was involved), a natural face, and to the intelligibility of the audio alone. It was found that the HMM method performs considerably better than the audio alone condition (54% and 34% keywords correct respectively), but not as well as the \"ideal\" articulating artificial face (64%). The intelligibility for the ANN method was 34% keywords correct.\n",
    ""
   ]
  },
  "vignoli99_avsp": {
   "authors": [
    [
     "Fabio",
     "Vignoli"
    ],
    [
     "Carlo",
     "Braccini"
    ]
   ],
   "title": "A text-speech synchronization technique with applications to talking heads",
   "original": "av99_022",
   "page_count": 5,
   "order": 22,
   "p1": "paper 22",
   "pn": "",
   "abstract": [
    "In human communication, speech understanding is greatly improved by the bimodal acoustic-visual effect with respect to simple speech communication, in particular when the communication takes place in noisy environments. In this paper we propose a novel synchronization procedure between text and speech, to reduce the time consumption in the development of friendly audio--visual interfaces or authoring tools for multimedia production. The technique consists of a neural network based processing of speech and a time alignment algorithm. The proposed algorithm is fast and speaker independent since it uses neural networks trained to discriminate among broad phoneme classes and not to recognize speech. This technique has been used to animate the MPEG-4 compliant face model developed at DIST [3].\n",
    ""
   ]
  },
  "massaro99_avsp": {
   "authors": [
    [
     "Dominic W.",
     "Massaro"
    ],
    [
     "Jonas",
     "Beskow"
    ],
    [
     "Michael M.",
     "Cohen"
    ],
    [
     "Christopher L.",
     "Fry"
    ],
    [
     "Tony",
     "Rodriguez"
    ]
   ],
   "title": "Picture my voice: Audio to visual speech synthesis using artificial neural networks",
   "original": "av99_023",
   "page_count": 6,
   "order": 23,
   "p1": "paper 23",
   "pn": "",
   "abstract": [
    "This paper presents an initial implementation and evaluation of a system that synthesizes visual speech directly from the acoustic waveform. An artificial neural network (ANN) was trained to map the cepstral coefficients of an individual's natural speech to the control parameters of an animated synthetic talking head. We trained on two data sets; one was a set of 400 words spoken in isolation by a single speaker and the other a subset of extemporaneous speech from 10 different speakers. The system showed learning in both cases. A perceptual evaluation test indicated that the system's generalization to new words by the same speaker provides significant visible information, but significantly below that given by a text-to-speech algorithm.\n",
    ""
   ]
  },
  "imaizumi99_avsp": {
   "authors": [
    [
     "Kazuya",
     "Imaizumi"
    ],
    [
     "Shizuo",
     "Hiki"
    ],
    [
     "Yumiko",
     "Fukuda"
    ]
   ],
   "title": "A symbolic system for multi-purpose description of the mouth shapes",
   "original": "av99_024",
   "page_count": 6,
   "order": 24,
   "p1": "paper 24",
   "pn": "",
   "abstract": [
    "A multi-purpose symbolic system which can describe all of the mouth shapes conveying various kinds of linguistic information in signing, speech dialogue, and speechreading is developed, taking account of the constraints of anatomical structure and neural control and the possibility of visual discrimination. In order to make the changes in the three-dimensional shape of lips, the changes are described by four scales, namely, upward and downward heights and lateralward width of the opening of the lips, and forward and backward depths of the corners and centers. Then, the space which is composed by those scales is divided into visually discriminable parts.\n",
    ""
   ]
  },
  "fries99_avsp": {
   "authors": [
    [
     "Georg",
     "Fries"
    ],
    [
     "Aldo",
     "Paradiso"
    ],
    [
     "Frank",
     "Nack"
    ],
    [
     "Karlheinz",
     "Schuhmacher"
    ]
   ],
   "title": "A tool for designing MPEG-4 compliant expressions and animations on VRML cartoon-faces",
   "original": "av99_025",
   "page_count": 6,
   "order": 25,
   "p1": "paper 25",
   "pn": "",
   "abstract": [
    "We present a design environment which allows the generation, modification, and visual speech animation of 3D cartoon-like faces - Tinky. Our underlying face model is not based on a set of independent parameters that control specific abstract muscle emulations but is directed by a set of objects representing the elements of the face. In order to provide an easy to use authoring system producing standardized visemes, expressions and animations of web oriented talking faces, we adopt an object oriented approach for face design and combine it with the authoring qualities of VRML and the standardized set of facial animation parameters defined by MPEG-4 SNHC.\n",
    ""
   ]
  },
  "lundeberg99_avsp": {
   "authors": [
    [
     "Magnus",
     "Lundeberg"
    ],
    [
     "Jonas",
     "Beskow"
    ]
   ],
   "title": "Developing a 3D-agent for the august dialogue system",
   "original": "av99_026",
   "page_count": 6,
   "order": 26,
   "p1": "paper 26",
   "pn": "",
   "abstract": [
    "In our continuing work with multimodal text-to-speech synthesis with high quality for speechreading, a new talking head has been developed with the purpose of acting as an interactive agent in a dialogue system, set up in a public exhibition area in downtown Stockholm. The new agent conforms to the same set of basic control parameters as our earlier faces, allowing us to control it using existing rules for visual speech synthesis. To add to the realism and believability of the dialogue system, the agent has been given a rich repertoire of extra-linguistic gestures and expressions, including emotional cues, turn-taking signals and prosodic cues such as punctuators and emphasizers. Studies of user reactions indicated that people have a positive attitude towards our new agent.\n",
    ""
   ]
  },
  "olives99_avsp": {
   "authors": [
    [
     "Jean-Luc",
     "Olives"
    ],
    [
     "Riikka",
     "Mottonen"
    ],
    [
     "Janne",
     "Kulju"
    ],
    [
     "Mikko",
     "Sams"
    ]
   ],
   "title": "Audio-visual speech synthesis for finnish",
   "original": "av99_027",
   "page_count": 6,
   "order": 27,
   "p1": "paper 27",
   "pn": "",
   "abstract": [
    "We describe our Finnish audio-visual speech synthesizer, its evaluation and discuss possible improvements. We have combined a three dimensional facial model with a commercial audio text-to-speech synthesizer. The visual speech is based on a letter-to-viseme mapping and the animation is created by linear interpolation between the visemes. An intelligibility test was run to quantify the benefit of seeing the synthetic and natural face on hearing the synthetic and natural voice presented at different signal to noise ratios. Both natural and synthetic faces improved the intelligibility of both natural and synthetic auditory speech. We examined the confusion patterns of consonants and the identification of the Finnish visemes. We also propose how the viseme repertoire of the talking head can be improved.\n",
    ""
   ]
  },
  "ritter99_avsp": {
   "authors": [
    [
     "Max",
     "Ritter"
    ],
    [
     "Uwe",
     "Meier"
    ],
    [
     "Jie",
     "Yang"
    ],
    [
     "Alex",
     "Waibel"
    ]
   ],
   "title": "Face translation: A multimodal translation agent",
   "original": "av99_028",
   "page_count": 5,
   "order": 28,
   "p1": "paper 28",
   "pn": "",
   "abstract": [
    "In this paper, we present Face Translation, a translation agent for people who speak different languages. The system can not only translate a spoken utterance into another language, but also produce an audio-visual output with the speaker's face and synchronized lip movement. The visual output is synthesized from real images based on image morphing technology. Both mouth and eye movements are generated according to linguistic and social cues. An automatic feature extracting system can automatically initialize the system. After initialization, the system can generate synchronized visual output based on a few pre-stored images. The system is useful for a video conference application with a limited bandwidth. We have demonstrated the system in a travel planning application where a foreign tourist plans a trip with a travel agent over the Internet in a multimedia collaborative working space using a multimodal interface.\n",
    ""
   ]
  }
 },
 "sessions": [
  {
   "title": "Plenary Papers",
   "papers": [
    "nass99_avsp",
    "lewkowicz99_avsp",
    "stein99_avsp"
   ]
  },
  {
   "title": "Auditory/Visual Speech Perception I (Dedicated to Kerry P. Green)",
   "papers": [
    "norrix99_avsp",
    "cox99_avsp",
    "haan99_avsp",
    "campbell99_avsp",
    "kanzaki99_avsp"
   ]
  },
  {
   "title": "Auditory/Visual Speech Perception II",
   "papers": [
    "gagne99_avsp",
    "seitz99_avsp",
    "brancazio99_avsp",
    "davis99_avsp",
    "burnham99_avsp"
   ]
  },
  {
   "title": "Speech Analysis and Recognition by Machine",
   "papers": [
    "baldwin99_avsp",
    "movellan99_avsp",
    "niyogi99_avsp",
    "talle99_avsp",
    "senior99_avsp"
   ]
  },
  {
   "title": "Correspondences between Auditory and Visual Speech and Auditory Speech to Visual Speech (AStVS) Synthesis",
   "papers": [
    "barker99_avsp",
    "vatikiotisbateson99_avsp",
    "agelfors99_avsp",
    "vignoli99_avsp",
    "massaro99_avsp"
   ]
  },
  {
   "title": "Communicating Characters",
   "papers": [
    "imaizumi99_avsp",
    "fries99_avsp",
    "lundeberg99_avsp",
    "olives99_avsp",
    "ritter99_avsp"
   ]
  }
 ]
}