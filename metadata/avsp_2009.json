{
 "location": "University of East Anglia, Norwich, UK",
 "startDate": "10/9/2009",
 "endDate": "13/9/2009",
 "conf": "AVSP",
 "year": "2009",
 "name": "avsp_2009",
 "series": "AVSP",
 "SIG": "AVISA",
 "title": "Auditory-Visual Speech Processing",
 "title1": "Auditory-Visual Speech Processing",
 "date": "10-13 September 2009",
 "booklet": "avsp_2009.pdf",
 "papers": {
  "mol09_avsp": {
   "authors": [
    [
     "Lisette",
     "Mol"
    ],
    [
     "Emiel",
     "Krahmer"
    ],
    [
     "Marc",
     "Swerts"
    ]
   ],
   "title": "Alignment in iconic gestures: does it make sense?",
   "original": "av09_003",
   "page_count": 6,
   "order": 1,
   "p1": "3",
   "pn": "8",
   "abstract": [
    "Many studies have shown that people imitate and repeat each others behaviors. This holds for both verbal and nonverbal behavior. The production of co-speech hand gestures is a special case of nonverbal behavior, because it is believed to be tightly linked to verbal language production and because gestures can carry meaning in a way that is similar to verbal language. It has been shown that people reuse each others hand shapes for cospeech gestures. This study looks at the relevance of gestures meaning and their relation to speech for such mimicry to occur. In two studies we found that speakers mimicked iconic gestures that they had observed, but only if these gestures were consistent with the co-occurring speech. This is evidence that the mimicry of iconic gestures is an instance of convergence of linguistic behavior, also known as alignment, rather than a supercial imitation of physical behavior.\n",
    "",
    "",
    "Index Terms: alignment, gesture, narration\n",
    ""
   ]
  },
  "sakamoto09_avsp": {
   "authors": [
    [
     "Shuichi",
     "Sakamoto"
    ],
    [
     "Akihiro",
     "Tanaka"
    ],
    [
     "Shun",
     "Numahata"
    ],
    [
     "Atsushi",
     "Imai"
    ],
    [
     "Tohru",
     "Takagi"
    ],
    [
     "Yôiti",
     "Suzuki"
    ]
   ],
   "title": "Aging effect on audio-visual speech asynchrony perception: comparison of time-expanded speech and a moving image of a talker's face",
   "original": "av09_009",
   "page_count": 4,
   "order": 2,
   "p1": "9",
   "pn": "12",
   "abstract": [
    "In this study, we measured detection and tolerance thresholds of auditory-visual asynchrony between time-expanded speech and a moving image of the talkers face for older adults. During experiments, words were presented under two conditions: asynchrony by time-expanded speech (expansion condition, EXP) and simple timing shift (asynchronous condition, ASYN). We used 16 Japanese shorter words (four morae) and 20 Japanese longer words (seven or eight morae). For EXP, auditory speech signals were expanded and combined with the visual signals so that the onset of the utterance was synchronous. For ASYN, the auditory speech signal was simply lagged behind the visual speech signal. Detection and tolerance thresholds for auditory-visual asynchrony obtained for older adults was higher than these obtained for younger adults, which suggests that older adults are tolerant of audio-visual asynchrony.\n",
    "",
    "",
    "Index Terms: lip-reading, auditory-visual asynchrony, timeexpanded speech, detection and tolerance thresholds, aging\n",
    ""
   ]
  },
  "cosi09_avsp": {
   "authors": [
    [
     "Piero",
     "Cosi"
    ],
    [
     "Graziano",
     "Tisato"
    ]
   ],
   "title": "LW2a: an easy tool to transform voice WAV files into talking animations",
   "original": "av09_013",
   "page_count": 5,
   "order": 3,
   "p1": "13",
   "pn": "17",
   "abstract": [
    "LW2A (LuciaWav2Animation) is an easy tool to transform voice wav files into talking animations. LUCIA, an Italian animated talking face is provided with an automatic segmentation tool based on an Italian clean speech ASR system for creating realistic and effective lip-sync facial animations from audio files.\n",
    "",
    "",
    "Index Terms: visual speech synthesis, speech-driven facial animation, audio-to-visual mapping\n",
    ""
   ]
  },
  "fagel09_avsp": {
   "authors": [
    [
     "Sascha",
     "Fagel"
    ]
   ],
   "title": "Effects of smiled speech on lips, larynx and acoustics",
   "original": "av09_018",
   "page_count": 4,
   "order": 4,
   "p1": "18",
   "pn": "21",
   "abstract": [
    "The present paper reports on results of a study investigating changes of lip features, larynx position and acoustics caused by smiling while speaking. 20 triplets of words containing one of the vowels /a:/, /i:/, /u:/ were spoken and audiovisually recorded. Lip features were extracted manually as well as using a 3D motion capture technique, formants were measured in the acoustic signal, and the vertical larynx position was determined where visible. Results show that during production of /u:/ F1 and F2 are not significantly affected despite of changes of lip features while F3 is increased. For /a:/ F1 and F3 are unchanged where for /i:/ only F3 is not affected. Furthermore, while the effect of smiling on the outer lip features is comparable between vowels, inner lip features are differently affected for different vowels. These differences in the impact on /a:/, /i:/ and /u:/ suggest that the effect of smiling on vowel production is vowel dependent.\n",
    "",
    "",
    "Index Terms: smiled speech, motion capture, vowel production\n",
    ""
   ]
  },
  "jesse09_avsp": {
   "authors": [
    [
     "Alexandra",
     "Jesse"
    ],
    [
     "Esther",
     "Janse"
    ]
   ],
   "title": "Visual speech information aids elderly adults in stream segregation",
   "original": "av09_022",
   "page_count": 6,
   "order": 5,
   "p1": "22",
   "pn": "27",
   "abstract": [
    "Listening to a speaker while hearing another speaker talks is a challenging task for elderly listeners. We show that elderly listeners over the age of 65 with various degrees of age-related hearing loss benefit in this situation from also seeing the speaker they intend to listen to. In a phoneme monitoring task, listeners monitored the speech of a target speaker for either the phoneme /p/ or /k/ while simultaneously hearing a competing speaker. Critically, on some trials, the target speaker was also visible. Elderly listeners benefited in their response times and accuracy levels from seeing the target speaker when monitoring for the less visible /k/, but more so when monitoring for the highly visible /p/. Visual speech therefore aids elderly listeners not only by providing segmental information about the target phoneme, but also by providing more global information that allows for better performance in this adverse listening situation.\n",
    "",
    "",
    "Index Terms: speech perception, audiovisual alignment, stream segregation, aging\n",
    ""
   ]
  },
  "kyle09_avsp": {
   "authors": [
    [
     "Fiona",
     "Kyle"
    ],
    [
     "Mairead",
     "MacSweeney"
    ],
    [
     "Tara",
     "Mohammed"
    ],
    [
     "Ruth",
     "Campbell"
    ]
   ],
   "title": "The development of speechreading in deaf and hearing children: introducing a new test of child speechreading (toCS)",
   "original": "av09_028",
   "page_count": 4,
   "order": 6,
   "p1": "28",
   "pn": "31",
   "abstract": [
    "This paper describes the development of a new Test of Child Speechreading (ToCS) that was specifically designed to be suitable for use with deaf children. Speechreading is a skill which is required for deaf children to access the language of the hearing community. ToCS is a child-friendly, computer-based, speechreading test that measures speechreading (silent lipreading) at three psycholinguistic levels: words, sentences and short stories. A detailed description of the design, procedure and validity of ToCS is provided. 86 severely and profoundly deaf and 91 hearing children aged between 5 and 14 years participated in the standardisation study. Deaf and hearing children showed remarkably similar performance across all subtests on ToCS. Speechreading improved with age but was not associated with non-verbal IQ. For both deaf and hearing children, performance on ToCS was significantly related to reading accuracy and reading comprehension.\n",
    "",
    "",
    "Index Terms: speechreading, deafness, language, reading, assessment\n",
    ""
   ]
  },
  "chetty09_avsp": {
   "authors": [
    [
     "Girija",
     "Chetty"
    ],
    [
     "Roland",
     "Göcke"
    ],
    [
     "Michael",
     "Wagner"
    ]
   ],
   "title": "Audio-visual mutual dependency models for biometric liveness checks",
   "original": "av09_032",
   "page_count": 6,
   "order": 7,
   "p1": "32",
   "pn": "37",
   "abstract": [
    "In this paper we propose liveness checking technique for multimodal biometric authentication systems based on audiovisual mutual dependency models. Liveness checking ensures that biometric cues are acquired from a live person who is actually present at the time of capture for authenticating the identity. The liveness check based on mutual dependency models is performed by fusion of acoustic and visual speech features which measure the degree of synchrony between the lips and the voice extracted from speaking face video sequences. Performance evaluation in terms of DET (Detector Error Tradeoff) curves and EERs(Equal Error Rates) on publicly available audiovisual speech databases show a significant improvement in performance of proposed fusion of face-voice features based on mutual dependency models.\n",
    "",
    "",
    "Index Terms: multimodal, face-voice, speaker verification, ancillary speaker characteristics,\n",
    ""
   ]
  },
  "hisanaga09_avsp": {
   "authors": [
    [
     "Satoko",
     "Hisanaga"
    ],
    [
     "Kaoru",
     "Sekiyama"
    ],
    [
     "Tomohiko",
     "Igasaki"
    ],
    [
     "Nobuki",
     "Murayama"
    ]
   ],
   "title": "Audiovisual speech perception in Japanese and English: inter-language differences examined by event-related potentials",
   "original": "av09_038",
   "page_count": 5,
   "order": 8,
   "p1": "38",
   "pn": "42",
   "abstract": [
    "By using event-related potentials (ERPs, Experiment 2) and reaction times (RTs, Experiment 1), the present study examined interlanguage differences between Japanese and English in audiovisual speech perception [1, 2, 3, 4]. There were an auditory-only (AO) and a congruent auditory-visual (AV) conditions. In Experiment 1, RTs showed opposite tendencies between English-language (EL) and Japanese-language (JL) groups for the AO-AV relationship: the additional congruent visual information speeded up the speech perception processes for the EL group, but it slowed down the processes for the JL group. Thus, the visual influence was promoting for the EL but disturbing for the JL group. In Experiment 2, different ERP patterns were found between the EL and JL groups: Whereas the visual influence was sustained (maintained from N1 to P2) in the EL group, the influence was transient (limited only to N1) in the JL group. The ERPs and RTs data were both consistent with the reported interlanguage differences that the JL perceivers use visual information to the less extent than the EL perceivers do.\n",
    "",
    "",
    "Index Terms: native language, audiovisual speech perception, event-related potentials (ERPs)\n",
    ""
   ]
  },
  "almoubayed09_avsp": {
   "authors": [
    [
     "Samer",
     "Al Moubayed"
    ],
    [
     "Jonas",
     "Beskow"
    ]
   ],
   "title": "Effects of visual prominence cues on speech intelligibility",
   "original": "av09_043",
   "page_count": 4,
   "order": 9,
   "p1": "43",
   "pn": "46",
   "abstract": [
    "This study reports experimental results on the effect of visual prominence, presented as gestures, on speech intelligibility. 30 acoustically vocoded sentences, permutated into different gestural conditions were presented audio-visually to 12 subjects. The analysis of correct word recognition shows a significant increase in intelligibility when focally-accented (prominent) words are supplemented with head-nods or with eye-brow raise gestures. The paper also examines coupling other acoustic phenomena to brow-raise gestures. As a result, the paper introduces new evidence on the ability of the non-verbal movements in the visual modality to support audio-visual speech perception.\n",
    "",
    "",
    "Index Terms: prominence, head-nod, eye-brow, speech intelligibility, talking heads, lip-reading, gesture, visual prosody.\n",
    ""
   ]
  },
  "mattheyses09_avsp": {
   "authors": [
    [
     "Wesley",
     "Mattheyses"
    ],
    [
     "Lukas",
     "Latacz"
    ],
    [
     "Werner",
     "Verhelst"
    ]
   ],
   "title": "Multimodal coherency issues in designing and optimizing audiovisual speech synthesis techniques",
   "original": "av09_047",
   "page_count": 6,
   "order": 10,
   "p1": "47",
   "pn": "53",
   "abstract": [
    "This paper proposes a 2D audiovisual text-to-speech synthesis system that constructs the output signal by selecting and concatenating multimodal segments containing natural combinations of audio and video. We describe the experiments that were conducted in order to assess the impact of this joint audio/video synthesis technique on the perceived quality of the synthetic speech. The experiments indicate that a maximal level of audiovisual coherence present in the output speech improves the perceived quality when compared to the traditional approach of synthesizing the visual signal separately from the audio. In addition, we measured that there is a same maximum allowable desynchronization between the audio and the image sequence, irrespective whether the degree of desynchronization is constant or time varying. This tolerance is used in the synthesizer for further optimizing the segment cuttings points in the audio and in the video mode.\n",
    "",
    "",
    "Index Terms: audiovisual speech synthesis, multimodal unit selection, audiovisual synchrony\n",
    ""
   ]
  },
  "haq09_avsp": {
   "authors": [
    [
     "Sanaul",
     "Haq"
    ],
    [
     "Philip J. B.",
     "Jackson"
    ]
   ],
   "title": "Speaker-dependent audio-visual emotion recognition",
   "original": "av09_053",
   "page_count": 6,
   "order": 11,
   "p1": "53",
   "pn": "58",
   "abstract": [
    "This paper explores the recognition of expressed emotion from speech and facial gestures for the speaker-dependent case. Experiments were performed on an English audio-visual emotional database consisting of 480 utterances from 4 English male actors in 7 emotions. A total of 106 audio and 240 visual features were extracted and features were selected with Plus l-Take Away r algorithm based on Bhattacharyya distance criterion. Linear transformation methods, principal component analysis (PCA) and linear discriminant analysis (LDA), were applied to the selected features and Gaussian classifiers were used for classification. The performance was higher for LDA features compared to PCA features. The visual features performed better than the audio features and overall performance improved for the audio-visual features. In case of 7 emotion classes, an average recognition rate of 56% was achieved with the audio features, 95% with the visual features and 98% with the audio-visual features selected by Bhattacharyya distance and transformed by LDA. Grouping emotions into 4 classes, an average recognition rate of 69% was achieved with the audio features, 98% with the visual features and 98% with the audio-visual features fused at decision level. The results were comparable to the measured human recognition rate with this multimodal data set. 1\n",
    "",
    "",
    "Index Terms: audio-visual emotion, data evaluation, linear transformation, speaker-dependent\n",
    ""
   ]
  },
  "phillips09_avsp": {
   "authors": [
    [
     "Natalie A.",
     "Phillips"
    ],
    [
     "Shari",
     "Baum"
    ],
    [
     "Vanessa",
     "Taler"
    ]
   ],
   "title": "Audio-visual speech perception in mild cognitive impairment and healthy elderly controls",
   "original": "av09_059",
   "page_count": 6,
   "order": 12,
   "p1": "59",
   "pn": "64",
   "abstract": [
    "An audio-visual (AV) speech presentation mode can significantly improve spoken word identification. Language comprehension and communication in patients with Alzheimer disease (AD) can be compromised; however, little is known about the extent to which patients might benefit from an AV mode. Patients with mild cognitive impairment (MCI) are at risk for developing AD and can demonstrate parallel but milder difficulties in aspects of language function. Here we report on preliminary findings of a study that investigates the impact of AV speech and sentence context on word identification in patients with MCI and healthy elderly controls. Although both groups performed better in the AV condition compared to an auditory-alone condition and when a constraining sentence context was present, the patients performed worse than controls overall and in the condition that should afford the greatest benefit. This suggests that the cognitive deficits present in MCI may limit their ability to benefit fully from supportive perceptual and linguistic cues.\n",
    "Index terms: audio-visual speech perception, sentence context, aging, mild cognitive impairment, dementia\n",
    ""
   ]
  },
  "kuratate09_avsp": {
   "authors": [
    [
     "Takaaki",
     "Kuratate"
    ],
    [
     "Kathryn",
     "Ayers"
    ],
    [
     "Jeesun",
     "Kim"
    ],
    [
     "Marcia",
     "Riley"
    ],
    [
     "Denis",
     "Burnham"
    ]
   ],
   "title": "Are virtual humans uncanny?: varying speech, appearance and motion to better understand the acceptability of synthetic humans",
   "original": "av09_065",
   "page_count": 5,
   "order": 13,
   "p1": "65",
   "pn": "69",
   "abstract": [
    "In this experiment we examine the uncanny valley effect as a guide, albeit incomplete, to designing engaging humanlike interfaces. In this effect falling just short of perfection in creating synthetic humans elicits a pronounced negative reaction and here this is explored with talking head animations focusing on naturalness in speech, appearance and face motion. We implemented a set of techniques to manipulate naturalness for each of these aspects, and present the results of our study evaluating these stimuli. Outcomes of this method provide insights for the choice of the degree of realism or naturalness required for various talking heads.\n",
    "",
    "",
    "Index Terms: uncanny valley, multi-modal perception, face animation\n",
    ""
   ]
  },
  "kroos09_avsp": {
   "authors": [
    [
     "Christian",
     "Kroos"
    ],
    [
     "Katherine",
     "Hogan"
    ]
   ],
   "title": "Visual influence on auditory perception: is speech special?",
   "original": "av09_070",
   "page_count": 6,
   "order": 14,
   "p1": "70",
   "pn": "75",
   "abstract": [
    "Few studies have investigated whether visual perception can influence auditory perception outside the speech domain in a way comparable to the McGurk effect. Here we used common environmental events, a wooden and a metal spoon hitting a granite surface, to test for a change in auditory perception of the spoons material induced by an incongruent video. To obtain a sensitive measure and avoid directing the participants attention towards the stimulus manipulation, an AXB matching task was employed where in the target trials all three auditory stimuli were the same, while on the visual side the X stimulus was presented with no video and A and B were presented with a control video and a video either showing the wooden or the metal spoon. No visual influence was found. A second even more sensitive test investigated whether two neighbouring stimuli of an acoustic wood-to-metal continuum that were found to be auditorily non-discriminable became discriminable when one of them was enforced with the appropriate visual stimulus. Again, no effect was found. We interpreted the results as showing that the McGurk effect is limited to vocal-tract-like events.\n",
    "",
    "",
    "Index Terms: cross-modal perception, McGurk effect, nonspeech events\n",
    ""
   ]
  },
  "coulon09_avsp": {
   "authors": [
    [
     "Marion",
     "Coulon"
    ],
    [
     "Bahia",
     "Guellaï"
    ],
    [
     "Arlette",
     "Streri"
    ]
   ],
   "title": "Auditory-visual perception of talking faces at birth: a new paradigm",
   "original": "av09_076",
   "page_count": 4,
   "order": 15,
   "p1": "76",
   "pn": "79",
   "abstract": [
    "Newborn infants prefer faces to all other visual displays. All previous studies of face recognition in newborns used schematic faces, photographs or static real faces. In our study, we used video films to explore, for the first time, recognition of talking faces, in newborns. Our results suggest that video films of talking faces are very salient stimuli for newborns and can enhance face recognition a few hours after birth.\n",
    "Index terms : newborns, face recognition, videos, talking faces, cross-modal\n",
    ""
   ]
  },
  "do09_avsp": {
   "authors": [
    [
     "Cong-Thanh",
     "Do"
    ],
    [
     "Abdeldjalil",
     "Aissa-El-Bey"
    ],
    [
     "Dominique",
     "Pastor"
    ],
    [
     "André",
     "Goalic"
    ]
   ],
   "title": "Area of mouth opening estimation from speech acoustics using blind deconvolution technique",
   "original": "av09_080",
   "page_count": 6,
   "order": 16,
   "p1": "80",
   "pn": "85",
   "abstract": [
    "We propose a new method for estimation of area of mouth opening from a video sequence of the speaking person. In a paper published in 2000, Grant and Seitz have reported the different degrees of correlation between acoustic envelopes and visible movements. In our method, we exploit these correlations to establish a mathematical model of a Single-Input Multiple-Output (SIMO) system in which the area of mouth opening is the unknown Single Input that we need to estimate. The subband Root Mean Squared (RMS) energies of the speech signal are the observable Multiple Outputs of the model. The unknown input signal can be directly estimated by using the existing blind deconvolution techniques. Our method necessitates only an audio sequence to estimate directly the area of mouth opening in the corresponding video sequence. Consequently, using this method permits us to avoid using complex images processing techniques of the conventional visual features extraction methods, or the training of the estimators in the audioto- visual mapping methods. The audio-visual sequences used for the estimation tests have been recorded by an ordinary webcam. Estimation result is promising; the estimated area of mouth opening is sufficiently correlated with the manually measured one; the average of correlation coefficients obtained by the most effective configuration of the proposed method, on a set of 16 French sentences, is 0.73.\n",
    "",
    "",
    "Index Terms: Lip geometric feature, area of mouth opening, speech temporal envelope processing, SIMO, blind deconvolution\n",
    ""
   ]
  },
  "hilder09_avsp": {
   "authors": [
    [
     "Sarah",
     "Hilder"
    ],
    [
     "Richard",
     "Harvey"
    ],
    [
     "Barry-John",
     "Theobald"
    ]
   ],
   "title": "Comparison of human and machine-based lip-reading",
   "original": "av09_086",
   "page_count": 4,
   "order": 17,
   "p1": "86",
   "pn": "89",
   "abstract": [
    "We investigate the performance of a machine-based lip-reading system using both shape-only parameters and full shape and appearance parameters. Furthermore, we contrast the performance of a machine-based lip-reading system with human lip-reading ability. We find that the automated system outperforms human lip-readers. Curiously however, for relatively simple tasks there is little improvement in recognition accuracy when adding full appearance features to the machine-based system, whereas for human lip-readers we observe significant improvements in performance. Finally, we measure the effect of speaker training on human lip-reading ability and we find even very limited training is sufficient to improve performance.\n",
    "",
    "",
    "Index Terms: automated lip-reading, speechreading\n",
    ""
   ]
  },
  "hoetjes09_avsp": {
   "authors": [
    [
     "Marieke",
     "Hoetjes"
    ],
    [
     "Emiel",
     "Krahmer"
    ],
    [
     "Marc",
     "Swerts"
    ]
   ],
   "title": "Untying the knot between gestures and speech",
   "original": "av09_090",
   "page_count": 6,
   "order": 18,
   "p1": "90",
   "pn": "95",
   "abstract": [
    "Do people speak differently when they cannot use their hands? This study looks at the influence of gestures on speech by having participants take part in an instructional task, half of which had to be performed while sitting on their hands. Other factors that influence the ease of communication, such as visibility and cognitive load, were also taken into account. Results show that lack of visibility or the inability to gesture as well as cognitive load lead to changes in speech and that these factors may influence the successfulness of the instructions.\n",
    "",
    "",
    "Index Terms: gesture, visibility, cognitive load, instructional task\n",
    ""
   ]
  },
  "engwall09_avsp": {
   "authors": [
    [
     "Olov",
     "Engwall"
    ],
    [
     "Preben",
     "Wik"
    ]
   ],
   "title": "Can you tell if tongue movements are real or synthesized?",
   "original": "av09_096",
   "page_count": 6,
   "order": 19,
   "p1": "96",
   "pn": "101",
   "abstract": [
    "We have investigated if subjects are aware of what natural tongue movements look like, by showing them animations based on either measurements or rule-based synthesis. The issue is of interest since a previous audiovisual speech perception study recently showed that the word recognition rate in sentences with degraded audio was significantly better with real tongue movements than with synthesized. The subjects in the current study could as a group not tell which movements were real, with a classification score at chance level. About half of the subjects were significantly better at discriminating between the two types of animations, but their classification score was as often well below chance as above. The correlation between classification score and word recognition rate for subjects who also participated in the perception study was very weak, suggesting that the higher recognition score for real tongue movements may be due to subconscious, rather than conscious, processes. This finding could potentially be interpreted as an indication that audiovisual speech perception is based on articulatory gestures.\n",
    "",
    "",
    "Index Terms: augmented reality, tongue reading, visual speech synthesis, data-driven animation\n",
    ""
   ]
  },
  "lan09_avsp": {
   "authors": [
    [
     "Yuxuan",
     "Lan"
    ],
    [
     "Richard",
     "Harvey"
    ],
    [
     "Barry-John",
     "Theobald"
    ],
    [
     "Eng-Jon",
     "Ong"
    ],
    [
     "Richard",
     "Bowden"
    ]
   ],
   "title": "Comparing visual features for lipreading",
   "original": "av09_102",
   "page_count": 5,
   "order": 20,
   "p1": "102",
   "pn": "106",
   "abstract": [
    "For automatic lipreading, there are many competing methods for feature extraction. Often, because of the complexity of the task these methods are tested on only quite restricted datasets, such as the letters of the alphabet or digits, and from only a few speakers. In this paper we compare some of the leading methods for lip feature extraction and compare them on the GRID dataset which uses a constrained vocabulary over, in this case, 15 speakers. Previously the GRID data has had restricted attention because of the requirements to track the face and lips accurately. We overcome this via the use of a novel linear predictor (LP) tracker which we use to control an Active Appearance Model (AAM).   By ignoring shape and/or appearance parameters from the AAM we can quantify the effect of appearance and/or shape when lip-reading. We find that shape alone is a useful cue for lipreading (which is consistent with human experiments). However, the incremental effect of shape on appearance appears to be not significant which implies that the inner appearance of the mouth contains more information than the shape.\n",
    "",
    "",
    "Index Terms: lip-reading, feature extraction, feature comparison, tracking\n",
    ""
   ]
  },
  "shochi09_avsp": {
   "authors": [
    [
     "Takaaki",
     "Shochi"
    ],
    [
     "Kaoru",
     "Sekiyama"
    ],
    [
     "Nicole",
     "Lees"
    ],
    [
     "Mark",
     "Boyce"
    ],
    [
     "Roland",
     "Göcke"
    ],
    [
     "Denis",
     "Burnham"
    ]
   ],
   "title": "Auditory-visual infant directed speech in Japanese and English",
   "original": "av09_107",
   "page_count": 6,
   "order": 21,
   "p1": "107",
   "pn": "112",
   "abstract": [
    "The aim of this project is to compare (i) the acoustic vs. visual characteristics of infant-directed speech (IDS), (ii) IDS vs adult-directed speech (ADS), and (iii) the acoustic/visual characteristics of IDS and ADS cross-linguistically, in Australian English (AusE) vs Japanese. Acoustic data are presented along with preliminary visual data. Native AusE and Japanese speaking mothers spoke to their 4- to 9-month-old infants and another adult using target words containing one of four vowels [a,i,u,o]. Results show higher F0 mean and greater F0 variation for IDS than ADS in both language groups, and longer vowel duration in IDS than ADS but this was only significantly so for the AusE mothers. Finally, there was a tendency for vowel hyper-articulation in AusE mothers IDS, but for vowel hypo-articulation in Japanese mothers IDS, and overall vowel hyperarticulation was greater in AusE than Japanese IDS. Preliminary visual data suggest that there appears to be, contrary to what would be expected, a substantial decrement in visual vowel lip area in Japanese IDS compared to ADS, a finding that is in concert with tendency for vowel hypo-articulation in Japanese than English IDS.\n",
    "",
    "",
    "Index Terms: Infant Directed Speech, Adult Directed Speech, hyper articulation, cross cultural comparison\n",
    ""
   ]
  },
  "tanaka09_avsp": {
   "authors": [
    [
     "Akihiro",
     "Tanaka"
    ],
    [
     "Kaori",
     "Asakawa"
    ],
    [
     "Hisato",
     "Imai"
    ]
   ],
   "title": "Recalibration of audiovisual simultaneity in speech",
   "original": "av09_113",
   "page_count": 4,
   "order": 22,
   "p1": "113",
   "pn": "116",
   "abstract": [
    "Recent studies have shown that the audio-visual synchrony is recalibrated after adaptation to a constant timing difference between auditory and visual signals (i.e. temporal recalibration). Here we investigated whether the temporal recalibration occurs for audio-visual speech using an off-line adaptation method. After 3 minutes of lag observation, the audio-visual synchrony is recalibrated toward the adapted lag. The point of subjective simultaneity shifted after 10 seconds of lag observation, whereas the just noticeable difference did not change during this short observation period. The width of the temporal window extended only to the direction of audio delay. These findings extend the findings in previous studies and suggest different properties of temporal recalibration in speech.\n",
    "Index Terms: audio-visual integration; speech perception; temporal recalibration; temporal order judgment\n",
    ""
   ]
  },
  "kolossa09_avsp": {
   "authors": [
    [
     "Dorothea",
     "Kolossa"
    ],
    [
     "Steffen",
     "Zeiler"
    ],
    [
     "Alexander",
     "Vorwerk"
    ],
    [
     "Reinhold",
     "Orglmeister"
    ]
   ],
   "title": "Audiovisual speech recognition with missing or unreliable data",
   "original": "av09_117",
   "page_count": 6,
   "order": 23,
   "p1": "117",
   "pn": "122",
   "abstract": [
    "In order to robustly recognize distorted speech, use of visual information has been proven valuable in many recent investigations. However, visual features may not always be available, and they can be unreliable in unfavorable recording conditions. The same is true for distorted audio information, where noise and interference can corrupt some of the acoustic speech features used for recognition. In this paper, missing feature techniques for coupled HMMs are shown to be successful in coping with both uncertain audio and video information. Since binary uncertainty information may be easily obtained at little computational effort, this results in an effective approach that can be implemented to obtain significant performance improvements for a wide range of statistical model based audiovisual recognition systems.\n",
    "",
    "",
    "Index Terms: missing data techniques, audiovisual speech recognition, coupled HMM\n",
    ""
   ]
  },
  "winneke09_avsp": {
   "authors": [
    [
     "Axel H.",
     "Winneke"
    ],
    [
     "Natalie A.",
     "Phillips"
    ]
   ],
   "title": "Older and younger adults use fewer neural resources during audiovisual than during auditory speech perception",
   "original": "av09_123",
   "page_count": 4,
   "order": 24,
   "p1": "123",
   "pn": "126",
   "abstract": [
    "This study looks at age-related differences in the brain processes involved in audiovisual (AV) speech perception in multi-talker background babble. The behavioural findings clearly show that both younger adults (YA) and older adults (OA) benefited equally well from AV speech relative to auditory-only (A) speech. Results pertaining to a condition that presented only a photograph alongside spoken words (AVphoto) supports the notion that an AV speech benefit cannot be achieved without the availability of dynamic visual speech cues provided by the lips. Interestingly, OA performed more poorly than YA in speechreading but, in line with the inverse effectiveness hypothesis, OA showed larger auditory enhancement effects suggesting that OA benefit more from AV speech. Analyses of the auditory N1 event-related potential (ERP) showed that AV speech trials lead to an amplitude reduction relative to A-only trials. This reduction was similar in YA and OA. In addition to the amplitude reduction, in both age groups the N1 related to AV speech trials peaked earlier, but this latency shift was larger for OA indicating that OA benefit more from AV speech than YA. These findings suggest that AV speech processing is more efficient because fewer neural resources are required to achieve superior performance. This idea of efficiency is further discussed with implications to higher-level cognition and successful aging.\n",
    "",
    "",
    "Index Terms: audiovisual speech, event-related potentials, aging, background noise\n",
    ""
   ]
  },
  "eger09_avsp": {
   "authors": [
    [
     "Jana",
     "Eger"
    ],
    [
     "Hans-Heinrich",
     "Bothe"
    ]
   ],
   "title": "Startegies and results for the evaluation of the naturalness of the LIPPS facial animation system",
   "original": "av09_127",
   "page_count": 3,
   "order": 25,
   "p1": "127",
   "pn": "129",
   "abstract": [
    "The paper describes strategy and results for an evaluation of the naturalness of a facial animation system with the help of hearing-impaired persons. It shows perspectives for improvement of the facial animation model, independent on the animation model itself. The fundamental thesis of the evaluation is that the comparison of presented and perceived visual information has to be performed on base of the viseme structure of the language.\n",
    ""
   ]
  },
  "davis09_avsp": {
   "authors": [
    [
     "Chris",
     "Davis"
    ],
    [
     "Jeesun",
     "Kim"
    ]
   ],
   "title": "Recognizing spoken vowels in multi-talker babble: spectral and visual speech cues",
   "original": "av09_130",
   "page_count": 4,
   "order": 26,
   "p1": "130",
   "pn": "133",
   "abstract": [
    "It has been proposed that both spectral and visual speech cues assist in segregating a talker from noise. To test how these cues interact, the experiment examined vowel identification (in hVd context) when presented in multi-talker babble. The availability of spectral cues was manipulated by filtering the signal into (1) 8 frequency amplitude-envelope bands or (2) the same bands with additional spectral cues. The availability of visual speech cues was manipulated by using auditoryonly (AO) and auditory-visual (AV) presentations. It was found that the intelligibility benefit when spectral and visual speech cues were combined appeared to be less than that produced by adding the benefits for each cue type when tested separately. This pattern suggests that both cues provide similar information.\n",
    ""
   ]
  },
  "almajai09_avsp": {
   "authors": [
    [
     "Ibrahim",
     "Almajai"
    ],
    [
     "Ben",
     "Milner"
    ]
   ],
   "title": "Effective visually-derived Wiener filtering for audio-visual speech processing",
   "original": "av09_134",
   "page_count": 6,
   "order": 27,
   "p1": "134",
   "pn": "139",
   "abstract": [
    "This work presents a novel approach to speech enhancement by exploiting the bimodality of speech and the correlation that exists between audio and visual speech features. For speech enhancement, a visually-derived Wiener filter is developed. This obtains clean speech statistics from visual features by modelling their joint density and making a maximum a posteriori estimate of clean audio from visual speech features. Noise statistics for the Wiener filter utilise an audio-visual voice activity detector which classifies input audio as speech or nonspeech, enabling a noise model to be updated. Analysis shows estimation of speech and noise statistics to be effective with speech quality assessed objectively and subjectively measuring the effectiveness of the resulting Wiener filter. The use of this enhancement method is also considered for ASR purposes.\n",
    "",
    "",
    "Index Terms: Audio-visual, speech enhancement, Wiener filter, AVSR, MAP\n",
    ""
   ]
  },
  "devergie09_avsp": {
   "authors": [
    [
     "Aymeric",
     "Devergie"
    ],
    [
     "Frédéric",
     "Berthommier"
    ],
    [
     "Nicolas",
     "Grimault"
    ]
   ],
   "title": "Pairing audio speech and various visual displays: binding or not binding?",
   "original": "av09_140",
   "page_count": 5,
   "order": 28,
   "p1": "140",
   "pn": "146",
   "abstract": [
    "Recent findings demonstrate that audiovisual fusion during speech perception may involve pre-phonetic processing. The aim of the current experiment is to investigate this hypothesis using a pairing task between auditory sequences of vowels and non speech visual cues. The audio sequences are composed of 6 auditory French vowels alternating in pitch (or not) in order to build 2 interleaved streams of 3 vowels each. Various elementary visual displays are mounted in synchrony with one vowel stream out of the two. Our hypothesis is that, in a forced choice pairing task, the AV synchronized vowels will be found more frequently if such a perceptual binding operates. We show that the most efficient visual feature increasing pairing performance is the movement. Surprisingly, some features we manipulated do not provide the increase in pairing performances. The visual cue of contrast variation is not correctly paired with the synchronized auditory vowels. Moreover, the auditory segregation, based on the pitch difference between the vowels streams, has no additional effect on pairing. In addition, the modulation of the auditory envelop, synchronized with the variation of the visual cue, has also no effect. Finally, when we introduce a phonetic cue in the visual display, pairing increases in comparison with non specific visual cues. The relative contribution of perceptual binding and late phonetic fusion is discussed.\n",
    "",
    "",
    "Index Terms: Audiovisual fusion, perceptual binding, multimodal phonetic processing\n",
    ""
   ]
  },
  "wollermann09_avsp": {
   "authors": [
    [
     "Charlotte",
     "Wollermann"
    ],
    [
     "Bernhard",
     "Schröder"
    ]
   ],
   "title": "Effects of exhaustivity and uncertainty on audiovisual focus production",
   "original": "av09_145",
   "page_count": 6,
   "order": 29,
   "p1": "145",
   "pn": "150",
   "abstract": [
    "This paper presents an experimental study on the role of uncertainty and exhaustivity for audiovisual pragmatic focus production. We investigate whether audiovisual cues of uncertainty appear more often for the production of non-exhaustive answers than for exhaustive answers. Results for the audio modality suggest that rising intonation occurs more often in connection with non-exhaustivity as with exhaustive answers, but no effects on fillers and pauses are observed. For the visual channel eyebrow and head movements accompanying the speech signal are found, but effects are relatively weak.\n",
    "",
    "",
    "Index Terms: audiovisual prosody, pragmatic focus production, uncertainty, exhaustivity, question-answering\n",
    ""
   ]
  },
  "takeuchi09_avsp": {
   "authors": [
    [
     "Shinichi",
     "Takeuchi"
    ],
    [
     "Takashi",
     "Hashiba"
    ],
    [
     "Satoshi",
     "Tamura"
    ],
    [
     "Satoru",
     "Hayamizu"
    ]
   ],
   "title": "Voice activity detection based on fusion of audio and visual information",
   "original": "av09_151",
   "page_count": 4,
   "order": 30,
   "p1": "151",
   "pn": "154",
   "abstract": [
    "In this paper, we propose a multi-modal voice activity detection system (VAD) that uses audio and visual information. Audioonly VAD systems typically are not robust to (acoustic) noise. Incorporating visual information, for example information extracted from mouth images, can improve the robustness since the visual information is not affected by the acoustic noise. In multi-modal (speech) signal processing, there are two methods for fusing the audio and the visual information: concatenating the audio and visual features, and employing audio-only and visual-only classifiers, then fusing the unimodal decisions. We investigate the effectiveness of these methods and also compare model-based and model-free methods for VAD. Experimental results show feature fusion methods to generally be more effective, and decision fusion methods generally perform better using model-free methods.\n",
    "",
    "",
    "Index Terms: voice activity detection, multi-modal, AVVAD\n",
    ""
   ]
  },
  "pachoud09_avsp": {
   "authors": [
    [
     "Samuel",
     "Pachoud"
    ],
    [
     "Shaogang",
     "Gong"
    ],
    [
     "Andrea",
     "Cavallaro"
    ]
   ],
   "title": "Space-time audio-visual speech recognition with multiple multi-class probabilistic support vector machines",
   "original": "av09_155",
   "page_count": 6,
   "order": 31,
   "p1": "155",
   "pn": "160",
   "abstract": [
    "We extract relevant and informative audio-visual features using multiple multi-class Support Vector Machines with probabilistic outputs, and demonstrate the approach in a noisy audio-visual speech reading scenario. We first extract visual spatio-temporal features and audio cepstral coefficients from pronounced digit sequences. Two classifiers are then trained on a single modality to obtain confidence factors that are used to select the most appropriate fusion strategy. A final classifier is trained on the joint audiovisual feature space and used to recognize digits. We demonstrate the proposed approach on a standard database and compare it with alternative methods. The evaluation shows that the proposed approach outperforms the alternatives both in terms of recognition accuracy and in terms of robustness.\n",
    ""
   ]
  },
  "krnoul09_avsp": {
   "authors": [
    [
     "Zdeněk",
     "Krňoul"
    ]
   ],
   "title": "Refinement of lip shape in sign speech synthesis",
   "original": "av09_161",
   "page_count": 5,
   "order": 32,
   "p1": "161",
   "pn": "165",
   "abstract": [
    "This paper deals with an analysis of lip shapes during speech that accompanies sign language, referred to as sign speech. A new sign speech database is collected and a new framework for the analysis of mouth patterns is introduced. Using a shape model restricted to the outer lip contour, we show that the articulatory parameters for visual speech alone are not sufficient for representing sign speech. The errors occur mainly for the mouth opening. A correction to the standard articulatory parameters and additional articulatory parameters are investigated to cover the observed mouth patterns and thus refine the synthesised sign speech.\n",
    "",
    "",
    "Index Terms: visual speech synthesis, talking head, sign speech synthesis, articulatory parameters\n",
    ""
   ]
  },
  "liu09_avsp": {
   "authors": [
    [
     "Kang",
     "Liu"
    ],
    [
     "Joern",
     "Ostermann"
    ]
   ],
   "title": "An image-based talking head system",
   "original": "av09_166",
   "page_count": 1,
   "order": 33,
   "p1": "166",
   "pn": "",
   "abstract": [
    "This paper presents an image-based talking head system, which includes two parts: analysis and synthesis. The analysis is to create a database containing a large number of mouth images and their associated facial and speech features. The synthesis is to generate realistic facial animations from phonetic transcripts of text. The facial animation is produced by selecting and concatenating appropriate mouth images that match the spoken words of the talking head. Subjective tests show that 60% of the animations are indistinguishable from real recordings.\n",
    "",
    "",
    "Index Terms: talking head, unit selection, evaluation\n",
    ""
   ]
  },
  "krnoul09b_avsp": {
   "authors": [
    [
     "Zdeněk",
     "Krňoul"
    ],
    [
     "Miloš",
     "Železný"
    ]
   ],
   "title": "The UWB 3d talking head text-driven system controlled by the SAT method used for the LIPS 2009 challenge",
   "original": "av09_167",
   "page_count": 2,
   "order": 34,
   "p1": "167",
   "pn": "168",
   "abstract": [
    "This paper describes the 3D talking head text-driven system controlled by the SAT (Selection of Articulatory Targets) method developed at the University of West Bohemia (UWB) that will be used for participation in the LIPS 2009 challenge. It gives an overview of methods used for visual speech animation, parameterization of a human face and a tongue, and a synthesis method. A 3D animation model is used for a pseudo-muscular animation schema to create visual speech animation usable for lipreading.\n",
    "",
    "",
    "Index Terms: facial animation, audio-visual speech synthesis, audio-to-visual mapping\n",
    ""
   ]
  },
  "beskow09_avsp": {
   "authors": [
    [
     "Jonas",
     "Beskow"
    ],
    [
     "Giampiero",
     "Salvi"
    ],
    [
     "Samer",
     "Al Moubayed"
    ]
   ],
   "title": "Synface - verbal and non-verbal face animation from audio",
   "original": "av09_169",
   "page_count": 1,
   "order": 35,
   "p1": "169",
   "pn": "",
   "abstract": [
    "We give an overview of SynFace, a speech-driven face animation system originally developed for the needs of hard-of-hearing users of the telephone. For the 2009 LIPS challenge, SynFace includes not only articulatory motion but also non-verbal motion of gaze, eyebrows and head, triggered by detection of acoustic correlates of prominence and cues for interaction control. In perceptual evaluations, both verbal and non-verbal movmements have been found to have positive impact on word recognition scores.\n",
    ""
   ]
  },
  "wang09_avsp": {
   "authors": [
    [
     "Lijuan",
     "Wang"
    ],
    [
     "Wei",
     "Han"
    ],
    [
     "Xiaojun",
     "Qian"
    ],
    [
     "Frank",
     "Soong"
    ]
   ],
   "title": "HMM-based motion trajectory generation for speech animation synthesis",
   "original": "av09_170",
   "page_count": 1,
   "order": 36,
   "p1": "170",
   "pn": "",
   "abstract": [
    "Synthesis of realistic facial animation for arbitrary speech is an important but difficult problem. The difficulties lie in the synchronization between lip motion and speech, articulation variation under different phonetic context, and expression variation in different speaking style. To solve these problems, we propose a visual speech synthesis system based on a fivestate, multi-stream HMM, which generates synchronized motion trajectories for the given text and speech input. Since the motion and the speech are modeled as different but coherent streams, the synchronization at each state is guaranteed. By considering phonetic context and suprasegmental information, the contextual dependent phone models are constructed and clustered using classification and regression, which capture the variable phonetic context and speaking style. The experiment results show that the HMMbased method can generate realistic lip animation while keeping the detailed articulation and transitions. Moreover, it is capable of presenting articulation variation under different phonetic context and expressing various speaking styles, such as emphasized speech.\n",
    ""
   ]
  }
 },
 "sessions": [
  {
   "title": "Papers",
   "papers": [
    "mol09_avsp",
    "sakamoto09_avsp",
    "cosi09_avsp",
    "fagel09_avsp",
    "jesse09_avsp",
    "kyle09_avsp",
    "chetty09_avsp",
    "hisanaga09_avsp",
    "almoubayed09_avsp",
    "mattheyses09_avsp",
    "haq09_avsp",
    "phillips09_avsp",
    "kuratate09_avsp",
    "kroos09_avsp",
    "coulon09_avsp",
    "do09_avsp",
    "hilder09_avsp",
    "hoetjes09_avsp",
    "engwall09_avsp",
    "lan09_avsp",
    "shochi09_avsp",
    "tanaka09_avsp",
    "kolossa09_avsp",
    "winneke09_avsp",
    "eger09_avsp",
    "davis09_avsp",
    "almajai09_avsp",
    "devergie09_avsp",
    "wollermann09_avsp",
    "takeuchi09_avsp",
    "pachoud09_avsp",
    "krnoul09_avsp",
    "liu09_avsp",
    "krnoul09b_avsp",
    "beskow09_avsp",
    "wang09_avsp"
   ]
  }
 ]
}