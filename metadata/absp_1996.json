{
 "title": "Workshop on the Auditory Basis of Speech Perception",
 "location": "Keele University, UK",
 "startDate": "15/7/1996",
 "endDate": "19/7/1996",
 "conf": "ABSP",
 "year": "1996",
 "name": "absp_1996",
 "series": "",
 "SIG": "",
 "title1": "Workshop on the Auditory Basis of Speech Perception",
 "date": "15-19 July 1996",
 "papers": {
  "greenberg96_absp": {
   "authors": [
    [
     "Steven",
     "Greenberg"
    ]
   ],
   "title": "Understanding speech understanding: towards a unified theory of speech perception",
   "original": "asp6_001",
   "page_count": 8,
   "order": 1,
   "p1": "1",
   "pn": "8",
   "abstract": [
    "Ever since Helmholtz, the perceptual basis of speech has been associated with the energy distribution across frequency. However, there is now accumulating evidence that speech understanding does not require a detailed spectral portraiture of the signal. As a consequence, a new theoretical perspective, focused on time, is beginning to emerge. This framework emphasizes the temporal evolution of coarse spectral patterns as the primary carrier of information within the speech signal, and provides an efficient and effective means of shielding linguistic information against the potentially hostile forces of the natural soundscape, such as reverberation and background acoustic interference. The auditory system may extract this relational information through computation of the low-frequency modulation spectrum in the auditory cortex, and this representation provides a principled basis for segmentation of the speech signal into syllabic units. Because of the systematic relationship between the syllable and higher-level lexico-grammatical organization it is possible, in principle, to gain direct access to the lexicon and grammar through such an auditory analysis of speech.\n",
    ""
   ]
  },
  "hackney96_absp": {
   "authors": [
    [
     "Carole M.",
     "Hackney"
    ]
   ],
   "title": "From cochlea to cortex: a simple anatomical description",
   "original": "asp6_009",
   "page_count": 8,
   "order": 2,
   "p1": "9",
   "pn": "16",
   "abstract": [
    "The hair cells of the organ of Corti transduce vibrations within the cochlea into neural signals which are transmitted by the afferent fibres of the cochlear nerve to the auditory brainstem. Here the fibres bifurcate, projecting to the dorsal and ventral nuclei of the cochlear nuclear complex, a divergence continued in the two main routes taken by the ascending pathways to the midbrain. One runs from the ventral cochlear nucleus to the superior olivary complex and thence to the inferior colliculus whilst the other runs from the dorsal cochlear nucleus direct to the inferior colliculus. Fibres running from the brainstem to the inferior colliculus form a tract, the lateral lemniscus, and may make contact with one of the nuclei within it. The ascending auditory pathway continues via the medial geniculate bodies in the thalamus to the auditory areas in the temporal lobe of the cortex, preserving cochleotopicity within the central auditory regions at all levels. The presence of commissural and decussating connections from the brainstem onwards provides the anatomical substrate for the analysis of binaural information whilst both the descending system which parallels the ascending system throughout, and the intrinsic circuitry at each level, provide the basis for feature extraction from the neural output of the cochlea.\n",
    ""
   ]
  },
  "pols96_absp": {
   "authors": [
    [
     "Louis C. W.",
     "Pols"
    ]
   ],
   "title": "Analysis and perception of dynamic events and of reduction phenomena in speech",
   "original": "asp6_017",
   "page_count": 6,
   "order": 3,
   "p1": "17",
   "pn": "22",
   "abstract": [
    "By now it is generally accepted that natural speech is more than a simple concatenation of stationary events, and that spectro-temporal dynamics plays an important role, whereas also variability in realization, including substantial reduction, adds to the naturalness of speech. It is much less clear how dynamic events can best be analyzed and represented and how they are perceived by listeners. By using methods from both psycho-acoustics and speech perception we try to specify the perceptual relevance of various sources of variation. By comparing dynamic spectral and duration data between conversational and read speech, normal- and fast-rate speech, stressed and unstressed syllables, high-frequent and low-frequent words, etc. we try to get better insight in how much variation occurs and how systematic that variation is.\n",
    ""
   ]
  },
  "clark96_absp": {
   "authors": [
    [
     "Graeme M.",
     "Clark"
    ]
   ],
   "title": "Cochlear implant speech processing for severely-to-profoundly deaf people",
   "original": "asp6_023",
   "page_count": 8,
   "order": 4,
   "p1": "23",
   "pn": "30",
   "abstract": [
    "A cochlear implant is a device which restores some hearing in severely-to-profoundly deaf people when the organ of Corti has not developed or is destroyed by disease or injury to such an extent no comparable hearing can be obtained with a hearing aid. When the organ of Corti is severely malfunctioning or absent, sound vibrations cannot be transduced into temporo-spatial patterns of action potentials along the auditory nerve for the coding of frequency and intensity. As a result, a hearing aid which amplifies sound, is of little or no use.\n",
    ""
   ]
  },
  "suga96_absp": {
   "authors": [
    [
     "Nobuo",
     "Suga"
    ]
   ],
   "title": "Basic acoustic patterns and neural mechanisms shared by humans and animals for auditory perception: a neuroethologist's view",
   "original": "asp6_031",
   "page_count": 8,
   "order": 5,
   "p1": "31",
   "pn": "38",
   "abstract": [
    "Human speech and animal sounds share at least three basic acoustic patterns called information-bearing elements (IBE's). These are constant frequency (CF), frequency modulated (FM) and noise burst (NB) components. Combinations of these IBE's are important to the auditory behaviors of both humans and animals. The combinations are more or less stereotyped in animals, but vary enormously in human speech. The central auditory systems of different species of animals contain neurons which respond best to either CF tones, FM sounds, noise bursts, or specific combinations of IBE's. The information-bearing parameters (IBP's) characterizing individual IBEs and combinations of IBE's show significant variations between individual animals (speakers) and are used to recognize them. In the mustached bat, the relationship in frequency between two CF components and that in time between two FM components are systematically represented in different cortical areas. In the macaque monkey, the relationship between the center frequency and the bandwidth characterizing noise bursts is systematically mapped. Therefore, for processing speech sounds, the human auditory system may create many combination-sensitive neurons and functional subdivisions where the relationship between formants (CF components), transitions (FM components) or parameters characterizing fills (NB components) are mapped.\n",
    ""
   ]
  },
  "adams96_absp": {
   "authors": [
    [
     "Joe C.",
     "Adams"
    ]
   ],
   "title": "Neural circuits in the human auditory brainstem",
   "original": "asp6_039",
   "page_count": 6,
   "order": 6,
   "p1": "39",
   "pn": "44",
   "abstract": [
    "Analysis of cell classes as the functional units of the auditory system has established major auditory circuits originating in the cochlear nucleus of animals. Extending these investigations throughout the system will result in a comprehensive picture of its organization. Establishing characteristic physiological, morphological, and cyto-chemical traits of cell classes in animals permits one to analyze human tissue and infer functional properties of identified cell classes. When cell classes which have morphological and cytochemical traits that have been associated with given functional properties in animals can be identified in humans, the functional properties of these cells can be inferred in humans. Such cell classes can serve as compelling models of functional elements involved in speech processing. It seems clear that the human auditory brainstem contains fewer cell classes and nuclei than most small mammals due to humans' lack of systems that are specialized for processing high frequency sounds. An overview of our present knowledge of auditory neuronal circuits is presented with emphasis on circuits that are and are not common to humans and commonly used laboratory animals.\n",
    ""
   ]
  },
  "dinse96_absp": {
   "authors": [
    [
     "Hubert R.",
     "Dinse"
    ],
    [
     "Christoph E.",
     "Schreiner"
    ]
   ],
   "title": "Dynamic frequency tuning of cat auditory cortical neurons: specific adaptations to the processing of complex sounds?",
   "original": "asp6_045",
   "page_count": 4,
   "order": 7,
   "p1": "45",
   "pn": "48",
   "abstract": [
    "We address the question of processing of fast acoustic signal components in the millisecond range by analyzing dynamic properties of cat auditory cortical neurons using a time-slice technique. We found evidence for profound time-variant receptive field properties. As a consequence, sharpness of tuning increased over time and asymmetries of the tuning properties became apparent that, with no exception, evolved late. Complex diagonal bands of activity across the frequency-intensity planes became visible which were found in equal proportions along the low or high frequency side of the tuning curves. All of these dynamic features were concealed in conventional time-averaged tuning curves. The results are discussed concerning a possible specific adaptation for processing of complex sounds as frequency transitions of formants and static slopes of formants in human speech resemble the properties observed during the temporal development of time-variant receptive field organization.\n",
    ""
   ]
  },
  "palmer96_absp": {
   "authors": [
    [
     "A. R.",
     "Palmer"
    ]
   ],
   "title": "Low-level processing of speech sounds in the auditory nervous system",
   "original": "asp6_049",
   "page_count": 8,
   "order": 8,
   "p1": "49",
   "pn": "56",
   "abstract": [
    "The way in which elements of speech sounds are encoded and represented in the discharge patterns of populations of neurones in the peripheral auditory system is described. The distribution of activity along the cochlea, and therefore across the best frequencies of the bank of bandpass auditory-nerve fibre filters, is a powerful way in which spectra are represented. The way that components falling within each frequency channel is signalled is less certain, since the utility of synchronized activity is open to question and mean rate coding, at least for vowels, is problematic under some listening conditions. For vowels, it seems unlikely that the distribution of synchronized activity across the neural population is preserved beyond the cochlear nucleus, but a simple place representation may be passed to higher levels. The timing of impulses is likely to be important for signalling cues such as the voice pitch.\n",
    ""
   ]
  },
  "riquimaroux96_absp": {
   "authors": [
    [
     "Hiroshi",
     "Riquimaroux"
    ]
   ],
   "title": "Processing of sound sequence in the auditory cortex",
   "original": "asp6_057",
   "page_count": 4,
   "order": 9,
   "p1": "57",
   "pn": "60",
   "abstract": [
    "The present study has examined how the temporal pattern of sound stimuli was represented in neurons of the primary auditory cortex (AI). We used adult Japanese monkeys (Macaca fuscata) prepared for chronic recording. Stimuli were tone bursts and temporally paired tone bursts. Unit recordings were made from the contralateral AI. During recording sessions, the animal was anesthetized with a mixture of nitrous oxide and oxygen. Post-stimulus-time histograms were analyzed. Results demonstrated that a single neuron in AI often showed totally different temporal response patterns when different frequencies were presented. Moreover, neurons of this type facilitatively responded to a temporally paired tone bursts of different frequencies with a particular interval (At). The interval which elicited the facilitation ranged from -50 msec up to -200 msec under the various conditions used. These neurons were sensitive to the At, frequencies and the order of presentation, implying that they are capable to represent temporally changing environmental sounds and communication sounds.\n",
    ""
   ]
  },
  "wong96_absp": {
   "authors": [
    [
     "Chen Pang",
     "Wong"
    ],
    [
     "Michael J.",
     "Pont"
    ]
   ],
   "title": "Automatic selection of parameters for a computer simulation of extracellular auditory nerve fibre activity",
   "original": "asp6_061",
   "page_count": 4,
   "order": 10,
   "p1": "61",
   "pn": "64",
   "abstract": [
    "The Meddis computational model of auditory nerve fibre activity is widely used. In this paper, we describe a novel method for automatically determining the parameters required to simulate a range of rate-intensity responses from auditory nerve fibres using this model. A genetic algorithm is employed to explore possible parameter combinations and to determine a \"best fit\" solution. Two sets of experiments used to demonstrate the flexibility of the technique are described. Some possible wider applications of the technique are discussed.\n",
    ""
   ]
  },
  "wong96b_absp": {
   "authors": [
    [
     "Kien Seng",
     "Wong"
    ],
    [
     "Michael J.",
     "Pont"
    ]
   ],
   "title": "A computer model of a ventral cochlear nucleus onset-c unit",
   "original": "asp6_065",
   "page_count": 4,
   "order": 11,
   "p1": "65",
   "pn": "68",
   "abstract": [
    "A computational modelling approach is adopted to study a subset of cochlear nucleus stellate cells known to generate 'onset-C responses. Such units are characterised by the following responses to short tones at best frequency: (i) large dynamic range; and (ii) evidence of well-timed onset responses [10]. We describe a detailed computer model capable of reproducing these responses. The correct operation of the model is shown to rely on the anatomical arrangement of the associated AN fibre input, and on an appropriate choice of cell properties.\n",
    ""
   ]
  },
  "damper96_absp": {
   "authors": [
    [
     "Robert I.",
     "Damper"
    ],
    [
     "S.",
     "Harnad"
    ],
    [
     "M. O.",
     "Gore"
    ]
   ],
   "title": "The auditory basis of the perception of voicing",
   "original": "asp6_069",
   "page_count": 6,
   "order": 12,
   "p1": "69",
   "pn": "74",
   "abstract": [
    "Voicing is perhaps the most basic articulatory and acoustic distinction in speech, yet we know little about the underlying auditory mechanisms of its perception. Responses of both human and animal listeners to synthetic stop-consonant/vowel stimuli in which voice onset time (VOT) is uniformly varied are known to be 'categorical' but an explanation of this phenomenon remains elusive. A 'composite' computational model - consisting of a biologically-realistic auditory model feeding its patterns of neural firing to an artificial neural network (ANN) - is known to be capable of reproducing listeners' behaviour in classical categorical perception (CP) studies. In this paper, we show that the behaviour of the composite model is robust to changes in the ANN architecture and learning algorithm. A contribution analysis of the learned weights reveals that subtle (sub-)features of auditory nerve activity underlie the voiced/unvoiced distinction.\n",
    ""
   ]
  },
  "deng96_absp": {
   "authors": [
    [
     "Li",
     "Deng"
    ],
    [
     "H.",
     "Sheikhzadeh"
    ]
   ],
   "title": "Temporal and rate aspects of speech encoding in the auditory system: simulation results on TIMIT data using a layered neural network interfaced with a cochlear model",
   "original": "asp6_075",
   "page_count": 4,
   "order": 13,
   "p1": "75",
   "pn": "78",
   "abstract": [
    "A study on temporal and rate aspects of the auditory representation for major manner classes of speech sounds in American English, using fluent speech examples excised from TIMIT database, is reported in this paper. A modeling approach is taken in which a cochlear model is used to generate parallel sets of auditory-nerve (AN) instantaneous firing rates in response to the TIMIT utterances. These temporal responses at ANs are further fed to a layered neural network (NN) which includes such neural mechanisms as lateral inhibition, coincidence detection, and short-term temporal integration of post synaptic potentials for action potential generation. Correspondence between a temporal-nonplace code at ANs and a rate-place code at the NN model output is shown and discussed.\n",
    ""
   ]
  },
  "hansen96_absp": {
   "authors": [
    [
     "Martin",
     "Hansen"
    ],
    [
     "Birger",
     "Kollmeier"
    ]
   ],
   "title": "Implementation of a psychoacoustical preprocessing model for sound quality measurement",
   "original": "asp6_079",
   "page_count": 4,
   "order": 14,
   "p1": "79",
   "pn": "82",
   "abstract": [
    "This study investigates the applicability of a psychoacoustical preprocessing model for measuring the speech quality of low-bit-rate codecs. The method for deriving an objective measure of the sound quality is to transform the input and output signal of a speech coding device into a so-called \"internal representation\" of the sound. Differences between the internal representations of input and output signal, calculated as the correlation coefficient between them, correspond to a decreased quality of the output signal. The preprocessing model consists of a critical-band filterbank which simulates the filtering by the basilar membrane, followed by a nonlinear adaptation circuit modeling amplitude compression and temporal masking effects, and a further filtering stage which analyzes fluctuations of the temporal envelope of the sound signal. This signal processing seems to be a successful \"universal\" model to simulate a variety of psychoacoustical data as well as the perception of speech sound at the same time.\n",
    ""
   ]
  },
  "hirahara96_absp": {
   "authors": [
    [
     "Tatsuya",
     "Hirahara"
    ],
    [
     "Peter",
     "Cariani"
    ],
    [
     "Bertrand",
     "Delgutte"
    ]
   ],
   "title": "Representation of low-frequency vowel formants in the auditory nerve",
   "original": "asp6_083",
   "page_count": 4,
   "order": 15,
   "p1": "83",
   "pn": "86",
   "abstract": [
    "We have investigated the auditory representation of vowels with low-frequency formants by recording the activity of auditory-nerve fibers in anesthetized cats in response to Japanese /i/-/e/ synthetic-vowel continua. Vowels having either low (150 Hz) or high (350 Hz) fundamental frequency F0 were varied in either first-formant frequency Fl or the level of a \"crucial harmonic\" near Fl to span the HI-Id continuum. Two different neural representations of the stimulus spectrum in the Fl region were examined: a population rate-place profile and a population interspike interval distribution. Characteristics of both representations depend on F0. When individual harmonics are resolved by the ear, as for high F0s, first formant frequency does not have explicit correlates in either ANF rate-place patterns or interspike interval distributions. Rather, both representations show clear patterns corresponding to individual harmonics, as well as the amplitude ratios of \"crucial harmonics\" near Fl that determine vowel identity in psyehophysical tests/When harmonics are not resolved, as for low F0s, both rate-place and population-interval profiles of individual harmonics fuse to form broader, single peaks near Fl, providing an explicit neural representation of formant frequency.\n",
    ""
   ]
  },
  "jones96_absp": {
   "authors": [
    [
     "E.",
     "Jones"
    ],
    [
     "E.",
     "Ambikairajah"
    ]
   ],
   "title": "Auditory signal processing using the wavelet packet transform",
   "original": "asp6_087",
   "page_count": 4,
   "order": 16,
   "p1": "87",
   "pn": "90",
   "abstract": [
    "This paper describes the application of the wavelet packet transform for speech analysis. In particular, the paper compares the performance of the wavelet packet transform with that of both the discrete wavelet transform, and a cascade auditory model. A novel hybrid wavelet transform analysis method is proposed, which combines the best features of the discrete wavelet transform and the fast wavelet transform. Results are presented in the form of spectrogram plots.\n",
    ""
   ]
  },
  "langner96_absp": {
   "authors": [
    [
     "Gerhard",
     "Langner"
    ],
    [
     "H.",
     "Schulze"
    ],
    [
     "M.",
     "Sams"
    ],
    [
     "P.",
     "Heil"
    ]
   ],
   "title": "The topographic representation of periodicity pitch in the auditory cortex",
   "original": "asp6_091",
   "page_count": 7,
   "order": 17,
   "p1": "91",
   "pn": "97",
   "abstract": [
    "An adaptation of auditory systems important for the processing of speech is that for periodic envelopes. Periodicity is a property not only of signals from vocal chords, but from many physical sound sources. Evidence is accumulating in support of a theory of temporal periodicity analysis as a supplement and refinement of the cochlear frequency analysis. Temporal representations of periodic signals are processed by neurons in the auditory midbrain acting as coincidence detectors and transferring temporal information into rate-place code. The resulting map for periodicity is arranged orthogonal to the tonotopic map. However, so far little evidence exists for a topographic representation of periodicity information in the mammalian cortex. The present paper compares the results of a neurophysiological study in the Mongolian gerbil and a magnetoencephalographic study of the human auditory cortex which together demonstrate that there might be indeed also periodotopic organizations in the cortex.\n",
    ""
   ]
  },
  "lauter96_absp": {
   "authors": [
    [
     "Judith L.",
     "Lauter"
    ]
   ],
   "title": "The auditory system as repertory company: a new approach to the neurobiology of speech perception",
   "original": "asp6_098",
   "page_count": 4,
   "order": 18,
   "p1": "98",
   "pn": "101",
   "abstract": [
    "Over the last century, the use of mechanical metaphors to model brain function has failed to illuminate the neurobiological bases of complex human behaviors such as speech perception. Here we use auditory processing of speech to illustrate a new, more organic systems-level metaphor for brain function ~ the theatrical repertory company. The metaphor is based on five detailed analogies between the structural and functional organization of a repertory company, compared with the human auditory nervous system. Illustrations of applications of this metaphor to auditory processing of speech will draw on research using noninvasive methods for brain monitoring and imaging. These illustrations demonstrate the promise of the new methods not only for freeing us from restrictions of the old mechanical metaphors, but also for providing novel insights into the ways in which the brain's \"repertory company\" performs the everyday miracle of listening to speech.\n",
    ""
   ]
  },
  "lublinskaja96_absp": {
   "authors": [
    [
     "Valentina V.",
     "Lublinskaja"
    ]
   ],
   "title": "The \"center of gravity\" effect in dynamics",
   "original": "asp6_102",
   "page_count": 4,
   "order": 19,
   "p1": "102",
   "pn": "105",
   "abstract": [
    "The \"center of gravity\" effect was observed in the identification of synthetic vowels when the amplitude relation of the second (F2) and the third (F3) formants was changed gradually in time the formants frequency being constant. The perceptive reactions were analogous with the perception of diphthong-like vowels with the frequency transitions of the second formant placed within the area between F2 and F3 of the stimuli above mentioned but considerably weaker. The effect was developed in the F3-F2 area up to 4.3 Bark, where the boundary of between diphthong-like vowels /Y, W and stationary /U/ was observed.\n",
    ""
   ]
  },
  "samarta96_absp": {
   "authors": [
    [
     "Eduardo",
     "Sá Marta"
    ],
    [
     "Fernando",
     "Perdigao"
    ],
    [
     "Luis",
     "Vieira de Sá"
    ]
   ],
   "title": "Psychophysical evidence for a sampling process, related to properties of onset cells, in stop consonant perception",
   "original": "asp6_106",
   "page_count": 4,
   "order": 20,
   "p1": "106",
   "pn": "109",
   "abstract": [
    "The poor frequency discrimination but superb temporal acuity of onset cells in the cochlear nucleus has led several authors to suggest that these cells might have a role somewhat akin to that of a \"sampling gate\", but specification of relevant neurally-represented \"sampled signals\" has been lacking. In this paper, it is proposed that initial tone-burst-like segments of F2 aspiration may generate a contribution to dental perception in stop consonants, one which could be represented by cochlear nucleus cells possessing CF-exeitation, strong lower-frequency inhibition and some higher-frequency inhibition, and a graded decay of response, Psychophysical experiments with partially edited natural sounds show that the contribution of these segments to dental perception is enhanced by temporal coincidence with wideband clicks that are known to cause firing of onset cells; on the other hand, if the short tone burst occurs 4 ms or more after the click, perception migrates away from dental.\n",
    ""
   ]
  },
  "mashari96_absp": {
   "authors": [
    [
     "Seyed J.",
     "Mashari"
    ],
    [
     "Michael J.",
     "Pont"
    ]
   ],
   "title": "The role of the auditory nerve in the perception of voicing: a computational modelling study",
   "original": "asp6_110",
   "page_count": 4,
   "order": 21,
   "p1": "110",
   "pn": "113",
   "abstract": [
    "In this paper, we present a novel, hybrid, neural network model which is intended to be used in studies of language acquisition. The hybrid model is built of two distinct parts. The first component is a physiologically motivated model of the auditory system and the second component an artificial neural network (Kohonen net). The construction of the hybrid model was motivated by a desire to investigate the neural mechanisms underlying the perception of voicing in initial stop-consonants. The results of this study suggest that the categorical perception of speech sounds differing in VOT is the result of representation of these sounds at the auditory nerve level.\n",
    ""
   ]
  },
  "omard96_absp": {
   "authors": [
    [
     "L. P.",
     "O'Mard"
    ],
    [
     "R.",
     "Meddis"
    ]
   ],
   "title": "A computational model of non-linear auditory frequency selectivity",
   "original": "asp6_114",
   "page_count": 6,
   "order": 22,
   "p1": "114",
   "pn": "119",
   "abstract": [
    "We present a phenomenological filter model that simulates the non-linear characteristics of basilar membrane filtering in mammalian cochlea. Cochlear non-linearity has already been shown to have important functions in the processing of complex stimuli by mammals. The Dual Resonance Non- linearity (DRNL) model closely simulates empirical results obtained from mechanical cochlear experiments. This purely passive model demonstrates a centre frequency shift, and filter width increase with a rise in stimulus level. It also reproduces two-tone suppression, combination tone distortion products and other data dependent upon cochlear non-linearity. The DRNL model is a composite filter consisting of two parallel filter paths: one linear and the other containing a compressive non-linearity. The model parameters were optimised at best frequencies of 0.3, 8 and 18 kHz, using simulated annealing. The model was incorporated into an auditory simulation computing library (LUTEar) and used to demonstrate non-linear auditory phenomena.\n",
    ""
   ]
  },
  "sachs96_absp": {
   "authors": [
    [
     "M. B.",
     "Sachs"
    ],
    [
     "B. J.",
     "May"
    ],
    [
     "G. S. Le",
     "Prell"
    ],
    [
     "R. D.",
     "Hienz"
    ]
   ],
   "title": "Adequacy of auditory-nerve rate representations of vowels: comparison with behavioral measures in cat",
   "original": "asp6_120",
   "page_count": 7,
   "order": 23,
   "p1": "120",
   "pn": "126",
   "abstract": [
    "Earlier results suggested that vowel spectra can be represented by profiles of discharge rate versus best frequency (BF) if the CNS performed a \"selective listening\" process. By this we mean that CNS responses are controlled by high spontaneous rate (SR) inputs at low sound levels and by low SR inputs at high levels. In this paper we review recent results which suggest that rate responses of high SR fibers alone may be adequate to represent vowels even at high levels. Predictions of formant frequency discrimination based on simple auditory-nerve models suggest that selective listening is only necessary for this task if the CNS makes decisions on the basis of rate responses of one or a very small number ox auditory-nerve fibers.\n",
    ""
   ]
  },
  "schreiner96_absp": {
   "authors": [
    [
     "Christoph E.",
     "Schreiner"
    ],
    [
     "S. W.",
     "Wong"
    ]
   ],
   "title": "Spatial-temporal representation of syllables in cat primary auditory cortex",
   "original": "asp6_127",
   "page_count": 6,
   "order": 24,
   "p1": "127",
   "pn": "132",
   "abstract": [
    "The spatial-temporal distribution of neuronal activity in primary auditory cortex (AI) was reconstructed for responses to consonant-vowel (CV) syllables with particular phonetic contrasts. Distinct differences in the distribution were evident for different CVs. Details of the spatial activity pattern changed when background noise was added.\n",
    ""
   ]
  },
  "bonneau96_absp": {
   "authors": [
    [
     "Anne",
     "Bonneau"
    ]
   ],
   "title": "Identification of vowels from French stop bursts",
   "original": "asp6_133",
   "page_count": 4,
   "order": 25,
   "p1": "133",
   "pn": "136",
   "abstract": [
    "This paper deals with the perception of vowels from French stop bursts. The corpus was made up of 90 stimuli of 20-25 ms duration extracted from natural CVC and CV words. The syllables com- bined the initial stops /p,t,k/ with the vowels /i,a,u/. In order to cut off all traces of vocalic segment, bursts whose duration was too short were lengthened. Eight native speakers of French served as listeners in the experiment. Results showed that a burst onset which did not contain any traces of vocalic segment provided substantial vocalic information (the overall identification rate was 80%). The vowel HI was clearly identified from /t/ and /k/, and the vowel /u/ very clearly identified from /k/. The vowel /a/, with high identification rate, was often chosen in the absence of a clear vocalic timbre.\n",
    ""
   ]
  },
  "formby96_absp": {
   "authors": [
    [
     "C.",
     "Formby"
    ],
    [
     "L. P.",
     "Sherlock"
    ],
    [
     "S.",
     "Li"
    ]
   ],
   "title": "Auditory temporal acuity and the perceptual organization of complex sounds",
   "original": "asp6_137",
   "page_count": 4,
   "order": 26,
   "p1": "137",
   "pn": "140",
   "abstract": [
    "A variation on a seemingly routine experiment, with an obvious outcome, is described that yields a set of findings which challenges our current understanding of auditory temporal perception and theory. The unexpected results reveal that detection threshold for a silent temporal gap may increase by an order of magnitude depending upon the number, temporal position, and frequency relations of the components that mark the gap. These results, which are not intuitive and cannot be predicted with existing models, provide a novel perspective on the intricacies of auditory temporal processing and the perceptual organization of complex sounds.\n",
    ""
   ]
  },
  "ghitza96_absp": {
   "authors": [
    [
     "Oded",
     "Ghitza"
    ],
    [
     "M. Mohan",
     "Sondhi"
    ]
   ],
   "title": "On the perceptual distance between speech segments",
   "original": "asp6_141",
   "page_count": 3,
   "order": 27,
   "p1": "141",
   "pn": "143",
   "abstract": [
    "For many tasks in speech signal processing it is of interest to develop an objective measure that correlates well with the perceptual distance between speech segments. (By speech segments we mean pieces of a speech signal of duration 50-150 milliseconds. For concreteness we will consider a segment to mean a diphone.) Such a distance metric would be useful for speech coding at low bit rates. Saving bits in those systems relies on a perceptual tolerance to acoustic deviations from the original speech, deviations that typically last for several tens of milliseconds. Such a distance metric would also be useful for automatic speech recognition on the assumption that perceptual invariance to adverse signal conditions (noise, microphone and channel distortions, room reverberations) and to phonemic variability (due to non-uniqueness of articulatory gestures) may provide a basis for robust performance. In this talk we will describe our attempts at defining such a metric.\n",
    ""
   ]
  },
  "kawahara96_absp": {
   "authors": [
    [
     "Hideki",
     "Kawahara"
    ]
   ],
   "title": "Auditory effects on speech production: an alternative approach to pitch perception mechanisms",
   "original": "asp6_144",
   "page_count": 4,
   "order": 28,
   "p1": "144",
   "pn": "147",
   "abstract": [
    "Auditory effects on speech production were investigated using a new experimental paradigm TAF (transformed auditory feedback). The method replaces a natural side tone path with an artificial path which modifies parameters of the feedback speech sounds in real-time, and measures effects in the produced speech sounds. Estimated impulse responses from pitch perception to voice Fo control illustrated that the response can be compensatory and is decomposed into two 2nd order responses. The results suggest that our auditory system has two different pitch related processes working in parallel and integrated in an additive manner.\n",
    ""
   ]
  },
  "kewleyport96_absp": {
   "authors": [
    [
     "Diane",
     "Kewley-Port"
    ]
   ],
   "title": "Psychophysical studies of vowel formants",
   "original": "asp6_148",
   "page_count": 6,
   "order": 29,
   "p1": "148",
   "pn": "153",
   "abstract": [
    "A series of studies concerning the human ability to discriminate vowels are reported. The first study established the thresholds for discrimination under optimal listening conditions for isolated vowels synthesized from a female talker. Extensions of that research have revealed several new findings. First, the stimulus factors fundamental frequency and consonantal context can significantly degrade formant frequency resolution. Second, changes in training protocols for listeners or the levels of stimulus uncertainty degrades formant resolution substantially. Another set of studies found that listeners with moderate, sloping hearing impairment had good resolution for Fl formants but poor resolution for F2 even when clearly audible. Several auditory models have been developed to account for significant variability due to formant and fundamental frequency observed for formant thresholds. Two major classes of models have been examined, both of which are based on spectral-difference metrics between the standard vowel and the vowel that is just discriminable at threshold. The first class used excitation patterns as the spectral representation and the second derives a specific loudness metric called A Sone. Both metrics were shown to account for the variability in formant thresholds, with the A Sone metric the most successful thus far.\n",
    ""
   ]
  },
  "klasmeyer96_absp": {
   "authors": [
    [
     "Gudrun",
     "Klasmeyer"
    ]
   ],
   "title": "Perceptual cues for emotional speech",
   "original": "asp6_154",
   "page_count": 4,
   "order": 30,
   "p1": "154",
   "pn": "157",
   "abstract": [
    "Emotional speech is an important field of study, because natural communication situations are seldom neutral or unemotional. In this study semantically neutral acoustic speech signals with emotional content in prosody and tone of voice are investigated. Acoustical parameters describing the temporal structure, pitch-contour and voice quality are measured in relation to an according neutral reference template. Results for happiness, anger, sadness, fear and boredom are presented.\n",
    ""
   ]
  },
  "lopezbascuas96_absp": {
   "authors": [
    [
     "Luis E.",
     "López-Bascuas"
    ]
   ],
   "title": "Speech signals might ignore auditory processors",
   "original": "asp6_158",
   "page_count": 4,
   "order": 31,
   "p1": "158",
   "pn": "161",
   "abstract": [
    "Identification ratings were taken from English monolingual subjects on a noise-lead-time (NLT) continuum. An improved signal detection model was employed for fitting the results. Previous experiments showed that the model could fit a TOT (tone-onset-time) but not a VOT (voice-onset-time) continuum. It could be argued that this difference arises from the different acoustic complexity of the two continua. In order to show that acoustic complexity cannot account for these results, a more complex nonspeech continuum was used in this experiment. It is shown that the model cannot fit the NLT continuum either. The results are discussed in terms of the possible modularity of speech processes.\n",
    ""
   ]
  },
  "pind96_absp": {
   "authors": [
    [
     "Jörgen",
     "Pind"
    ]
   ],
   "title": "A case of \"auditory handicap' in speech perception?",
   "original": "asp6_162",
   "page_count": 4,
   "order": 32,
   "p1": "162",
   "pn": "165",
   "abstract": [
    "Numerous attempts have been made in recent years to explain phonetic systems in auditory terms, as reflecting constraints imposed by the auditory system. Thus it has been suggested by Bladon [1] that the reason preaspiration is a rare phonetic contrast can be explained by it being auditorily disadvantaged. It has previously been shown by Pind [2] that preaspiration, cued by Voice Offset Time, shows much greater sensitivity to vowel lengthening than does postaspiration (Voice Onset Time). While this could be explained in auditory terms, e.g. in terms of asymmetric masking, a different explanation holds that this follows from the linguistic structure of the syllable containing preaspiration. Experiments, using sine-wave analogs of aspiration and preaspiration, and synthetic speech, do not lend support to the auditory theory of preaspiration perception but do point to the importance of language-specific factors in explicating the time course of Voice Offset Time perception.\n",
    ""
   ]
  },
  "wieringen96_absp": {
   "authors": [
    [
     "Astrid van",
     "Wieringen"
    ],
    [
     "Louis C. W.",
     "Pols"
    ]
   ],
   "title": "The auditory basis of vocalic speech transitions",
   "original": "asp6_166",
   "page_count": 4,
   "order": 33,
   "p1": "166",
   "pn": "169",
   "abstract": [
    "Discrimination experiments were performed to determine auditory sensitivity to changes in endpoint frequency, duration and rate-of-frequency change of short and rapid vocalic transitions varying in stimulus complexity from tone glides to multi-formant complex stimuli. The experiments show that the global pattern of discrimination functions is similar in many respects for the different kinds of stimuli, but that the perceptual importance of cues vary, depending on the changing stimulus property. The auditory basis of these different types of short transitions is examined further in speech perception experiments (ABX-discrimination, /b/ and /d/ classification, and absolute identification). The results of these tests suggest that the perception of vocalic transitions in speech is based on general auditory properties, and that it is limited more by attentional and masking constraints than by a speech-specific mechanism based on linguistic experience.\n",
    ""
   ]
  },
  "berthommier96_absp": {
   "authors": [
    [
     "Frédéric",
     "Berthommier"
    ]
   ],
   "title": "Direct separation of sounds based on knowledge of F0 and ITD",
   "original": "asp6_170",
   "page_count": 4,
   "order": 34,
   "p1": "170",
   "pn": "173",
   "abstract": [
    "I address the question of source separation using low-level knowledge about signals. Assuming that we are able to well estimate characteristics of the signals, like fundamental frequency (f0) and interaural time differency (ITD), I show how to use these measures in order to directly recover the signal produced by a given source. This ideal f0-guided segregation process well works with quasi-stationary signals, with no or well-described f0 variations. It enables to recover periodic signals embedded in noise and to separate double-vowels. I discuss widely the place of this new algorithm in the context of auditory scene analysis (ASA).\n",
    ""
   ]
  },
  "brown96_absp": {
   "authors": [
    [
     "Guy J.",
     "Brown"
    ],
    [
     "Martin",
     "Cooke"
    ],
    [
     "Eric",
     "Mousset"
    ]
   ],
   "title": "Are neural oscillations the substrate of auditory grouping?",
   "original": "asp6_174",
   "page_count": 6,
   "order": 35,
   "p1": "174",
   "pn": "179",
   "abstract": [
    "How are acoustic features that are extracted in remote regions of the auditory system bound together to form a perceptual whole? We consider the evidence for a solution to this so-called binding problem, which proposes that the responses of feature detecting cells are bound together by the synchronisation of oscillatory firing activity. Four models of auditory grouping based on neural oscillators are reviewed, and issues arising from these models are discussed.\n",
    ""
   ]
  },
  "cheveigne96_absp": {
   "authors": [
    [
     "Alain de",
     "Cheveigné"
    ]
   ],
   "title": "A neural cancellation model of F0-guided sound separation",
   "original": "asp6_180",
   "page_count": 6,
   "order": 36,
   "p1": "180",
   "pn": "185",
   "abstract": [
    "Listeners were presented with pairs of concurrent vowels and requested to report one or two vowels. The dF0 was either 0 or 6%, and RMS levels before mixing were either the same or different by 10 or 20 dB. Responses for each vowel within a stimulus were classified according to relative level (-20, -10, 0, 10, 20 dB) and dF0(0 and 6%). Identification was better at dF0=6%, and this effect was greatest when the target was weak (-20 and -10 dB). This outcome is difficult to account for with current models, but can be explained by invoking a within-channel neural cancellation filter. A model of concurrent vowel identification based on this filter is consistent with our experimental data, and agrees with results that show that the auditory system segregates harmonic sounds by cancelling the harmonic background.\n",
    ""
   ]
  },
  "cooke96_absp": {
   "authors": [
    [
     "Martin",
     "Cooke"
    ]
   ],
   "title": "Auditory organisation and speech perception: arguments for an integrated computational theory",
   "original": "asp6_186",
   "page_count": 8,
   "order": 37,
   "p1": "186",
   "pn": "193",
   "abstract": [
    "The first decade of work into computational auditory scene analysis (CASA) of speech has been dominated by approaches which regard auditory organisation as a preprocessor for speech recognition. In this paper, I will argue that for CASA to progress beyond speech enhancement is no simple task and will require the development of a computational theory of speech perception which may look quite different from the accounts provided by the best performing recognition scheme we possess at present, based on hidden Markov models. Some of the criteria which an adequate, perceptually-inspired model needs to account for are outlined, and these are contrasted with those which currently motivate robust automatic speech recognition systems. Modelling studies addressing some of the competencies required for a computational approach to speech perception are described. One study considers the requirement to cope with missing data. Another concerns the role of counter-evidence suggested by principles of perceived auditory continuity. The paper concludes by considering explicit aspects of the integration of auditory organisation and speech perception.\n",
    ""
   ]
  },
  "cosi96_absp": {
   "authors": [
    [
     "Piero",
     "Cosi"
    ],
    [
     "Enrico",
     "Zovato"
    ]
   ],
   "title": "Lyon's auditory model inversion: a tool for sound separation and speech enhancement",
   "original": "asp6_194",
   "page_count": 4,
   "order": 38,
   "p1": "194",
   "pn": "197",
   "abstract": [
    "A new implementation of Lyon's Auditory Model and an optimised inversion procedure will be presented. Both the passive and active Lyon's cochlea models were studied as new signal processing analysis schemes, while only the first one was considered regarding the inversion procedure. Following the work of M. Slaney, sound resynthesis was obtained inverting the correlogram representation by a new optimised algorithm. The utility of auditory model inversion will be emphasised focusing on the problem of speech enhancement and sound separation.\n",
    ""
   ]
  },
  "ellis96_absp": {
   "authors": [
    [
     "Daniel P.W.",
     "Ellis"
    ]
   ],
   "title": "Prediction-driven computational auditory scene analysis for dense sound mixtures",
   "original": "asp6_198",
   "page_count": 6,
   "order": 39,
   "p1": "198",
   "pn": "203",
   "abstract": [
    "We interpret the sound reaching our ears as the combined effect of independent, sound-producing entities in the external world; hearing would have limited usefulness if were defeated by over-lapping sounds. Computer systems that are to interpret real-world sounds - for speech recognition or for multimedia indexing - must similarly interpret complex mixtures. However, existing functional models of audition employ only data-driven processing incapable of making context-dependent inferences in the face of interference. We propose a.prediction-driven approach to this problem, raising numerous issues including the need to represent any kind of sound, and to handle multiple competing hypotheses. Results from an implementation of this approach illustrate its ability to analyze complex, ambient sound scenes that would confound previous systems.\n",
    ""
   ]
  },
  "janker96_absp": {
   "authors": [
    [
     "Peter M.",
     "Janker"
    ]
   ],
   "title": "The range of subjective simultaneousness in tapping experiments with speech stimuli",
   "original": "asp6_204",
   "page_count": 4,
   "order": 40,
   "p1": "204",
   "pn": "207",
   "abstract": [
    "Recently the prosodic aspects of speech have received more and more attention. Tapping and adjustment methods to determine the rhythmical differences introduced by structural and/or physical modified stimuli are quite common in investigations concerning the parameters which influence the rhythmical structure of speech. For the experiments described here the subjects had to perform a synchronisation task by tapping to sequences of German monosyllabic words or \"click\" sounds. Despite comparatively small but significant mean differences and compared to the literature normal standard deviations the range of simultaneousness as determined by the subjects is quite large (up to 420 ms). Discussing small but significant mean values to determine whether or not the modifications of the stimulus material had had an influence in the expected manner one therefore should keep in mind that the millisecond scale might not be the best scale to judge perceptual timing differences and that the actual simultaneousness judgments might still differ considerably.\n",
    ""
   ]
  },
  "mccabe96_absp": {
   "authors": [
    [
     "Susan L.",
     "McCabe"
    ],
    [
     "Michael J.",
     "Denham"
    ]
   ],
   "title": "A neurocomputational model of auditory streaming",
   "original": "asp6_208",
   "page_count": 4,
   "order": 41,
   "p1": "208",
   "pn": "211",
   "abstract": [
    "The need to interact effectively with a complex world imposes severe processing requirements on the central nervous system, making selective and predictive sensory perception essential. In this paper we consider the importance of active perception and its implications for auditory processing. A model of the early stages of auditory streaming, shown to be in agreement with a number of psychophysical results, is described. The model demonstrates how such streaming might result from interactions between the tonotopic patterns of activity of incoming signals and traces of previous activity which feed back and influence the way in which subsequent signals are processed. In extensions to the topographic streaming model, we explore how directional and memory based influences may be incorporated into a more extensive model of auditory scene analysis, and how the proposed processes might relate to the structure of the auditory system and the requirements of intelligent sensory processing.\n",
    ""
   ]
  },
  "meyer96_absp": {
   "authors": [
    [
     "Georg F.",
     "Meyer"
    ],
    [
     "Frédéric",
     "Berthommier"
    ]
   ],
   "title": "Vowel segregation with amplitude modulation maps: a re-evaluation of place and place-time models",
   "original": "asp6_212",
   "page_count": 4,
   "order": 42,
   "p1": "212",
   "pn": "215",
   "abstract": [
    "Two fundamentally different models explaining perceptual segregation of simultaneous vowels are investigated: a low-level segregation model based on amplitude modulation maps and a high-level pattern matching model based on the place representation seen in an auditory model. A third model combines the two models and allows a switch between them. Model predictions are compared with human performance and used as a basis to re-examine the relationship between signal representations and pattern matching strategies. Neither the primitive grouping model nor the  pattern matching stage alone are able to fully explain  human performance while a combination of the two  models matches human data well.\n",
    ""
   ]
  },
  "todd96_absp": {
   "authors": [
    [
     "Neil",
     "Todd"
    ]
   ],
   "title": "Towards a theory of the principal monaural pathway: pitch, time and auditory grouping",
   "original": "asp6_216",
   "page_count": 6,
   "order": 43,
   "p1": "216",
   "pn": "221",
   "abstract": [
    "A theory of the central auditory system is discussed in which temporal information is spatially coded on three dimensions roughly corresponding to the level of the cochlea, the inferior colliculus and the cortex. This theory is embodied in the form of a computational model which simulates peripheral and central processing and which has the following main components: (1) a one dimensional linear filter-bank to simulate the cochlea, (2) a two dimensional modulation filter-bank to simulate the inferior colliculus, (3) a three dimensional modulation filter-bank to simulate layer III/IV receptive cells in the cortex (4) a central pattern-recognition mechanism, and (5) a cortical cross-channel correlation mechanism.\n",
    ""
   ]
  },
  "varin96_absp": {
   "authors": [
    [
     "Laurent",
     "Varin"
    ],
    [
     "Frédéric",
     "Berthommier"
    ]
   ],
   "title": "Identification of concurrent sources with a perceptron-based model",
   "original": "asp6_222",
   "page_count": 4,
   "order": 44,
   "p1": "222",
   "pn": "225",
   "abstract": [
    "We propose a model able to identify superimposed patterns. Rather than a new connectionist scheme, this consists in controlling the recognition process performed by a 'classical' perceptron tuned by a regular learning stage. Hence, separation is based on a priori knowledge of the patterns, A key point of this model is the bifurcation process which is a creation of a second copy from a single-channel input. These two versions are processed in parallel. The dominant pattern is removed from the copy in order to get a second label. With the multilayer perceptron, we obtain remarkable results for the double-vowels (d-vowels) recognition task : when inputs are short-term (50 ms) warped place-coded spectra, double-recognition (d-recognition) score reaches 88% on a set of 6 synthetic vowels having various fundamental frequencies (f0). In addition, we briefly present a dual method based on linear discriminant analysis (LDA), working well with natural vowels, and allowing reconstruction of input signals.\n",
    ""
   ]
  },
  "warren96_absp": {
   "authors": [
    [
     "Richard M.",
     "Warren"
    ]
   ],
   "title": "Processing of speech and other auditory patterns: some similarities and differences",
   "original": "asp6_226",
   "page_count": 6,
   "order": 45,
   "p1": "226",
   "pn": "231",
   "abstract": [
    "Understanding of speech perception can be enhanced by understanding how we recognize other sequences of brief sounds. In addition, studies of speech perception can lead to a deeperunderstanding of the general rules of auditory pattern perception. This reciprocity will be illustrated with examples drawn chiefly from three topics we have studied in our laboratory. The first topic concerns a sophisticated mechanism used to restore portions of verbal and nonverbal signals masked by extraneous sounds. The second topic concerns the organization of sequences of brief sounds, whether phones or nonspeech, into \"temporal compounds\" that are recognized without resolution into component elements. The temporal compounds heard with sequences consisting of brief steady-state vowels are English syllables and English words. Surprisingly, vowel sequences split into two simultaneous voices corresponding to separate spectral ranges, and this observation has led to the third topic dealing with spectral redundancy and spectral restoration.\n",
    ""
   ]
  },
  "woods96_absp": {
   "authors": [
    [
     "William S.",
     "Woods"
    ],
    [
     "Martin",
     "Hansen"
    ],
    [
     "Thomas",
     "Wittkop"
    ],
    [
     "Birger",
     "Kollmeier"
    ]
   ],
   "title": "A scene analyzer for speech processing",
   "original": "asp6_232",
   "page_count": 4,
   "order": 46,
   "p1": "232",
   "pn": "235",
   "abstract": [
    "An architecture designed to combine separately operating estimators is described and evaluated. This architecture takes advantage of the constraints on the estimators to determine the accuracy of the estimates they produce, and combines the estimates based on their accuracies to produce a final estimate. Parameter values concerning the target being estimated and required by the estimators are determined from the final estimate and fed back to the preliminary estimators for use in the next processing frame. An implementation of the architecture is evaluated using a male target talker and female jammer talker under several spatial and target-to-jammer ratio (TJR) conditions. The implementation is able to yield improved TJR under unfavorable TJR, but does not do so consistently across TJR or spatial conditions. The architecture is discussed in terms of its relation to human auditory scene analysis and phenomena.\n",
    ""
   ]
  },
  "carlyon96_absp": {
   "authors": [
    [
     "Robert P.",
     "Carlyon"
    ]
   ],
   "title": "Within-channel cues for concurrent sound segregation",
   "original": "asp6_236",
   "page_count": 5,
   "order": 47,
   "p1": "236",
   "pn": "240",
   "abstract": [
    "Our ability to segregate two groups of harmonics that occupy the same frequency region depends on whether or not the harmonics in each group are resolved by the peripheral auditory system. Tasks requiring listeners to segregate the two sounds yield good performance when the harmonics in each group are resolved from each other, even when they are unresolved from some of the components in the other group. In contrast, when the harmonics in each group are unresolved, listeners are very poor at segregating the two sounds. Experiments are described which reveal the types of processing employed in these two situations.\n",
    ""
   ]
  },
  "darlington96_absp": {
   "authors": [
    [
     "David J.",
     "Darlington"
    ],
    [
     "Douglas R.",
     "Campbell"
    ]
   ],
   "title": "Sub-band adaptive filtering applied to speech enhancement",
   "original": "asp6_241",
   "page_count": 4,
   "order": 48,
   "p1": "241",
   "pn": "244",
   "abstract": [
    "An adaptive noise cancellation scheme for speech processing is proposed. In this, the adaptive filters are implemented in frequency-limited sub-bands. In previous work, the filters had been distributed in a linear fashion in the frequency domain. This work investigates the effects of spacing the filters more in sympathy with the signal power and spectral characteristics. It emerges that improvements in signal-to-noise ratio of processed noisy speech signals may be obtained when the sub-bands are spaced according to a published cochlear function.\n",
    ""
   ]
  },
  "patterson96_absp": {
   "authors": [
    [
     "Roy D.",
     "Patterson"
    ],
    [
     "Timothy R.",
     "Anderson"
    ],
    [
     "Keith",
     "Francis"
    ]
   ],
   "title": "Binaural auditory images and a noise-resistant, binaural auditory spectrogram for speech recognition",
   "original": "asp6_245",
   "page_count": 8,
   "order": 49,
   "p1": "245",
   "pn": "252",
   "abstract": [
    "This paper describes the development of a binaural version of the Auditory Image Model (AIM) [1] and speech recognition performance with monaural and binaural versions of AIM. It is argued that phase-locking information in the auditory nerve can enhance speech recognition in noise significantly, and that coincidence detection is preferable to cross-correlation for preserving phase-locking information during the construction of binaural auditory images.\n",
    ""
   ]
  },
  "campbell96_absp": {
   "authors": [
    [
     "Douglas R.",
     "Campbell"
    ]
   ],
   "title": "Binaural processing for hearing aids",
   "original": "asp6_253",
   "page_count": 4,
   "order": 50,
   "p1": "253",
   "pn": "256",
   "abstract": [
    "The practical ineffectiveness of hearing aids in noisy reverberant surroundings remains a frequent source of complaint and discomfort. Given the current capabilities and pace of development in microelectronics, the major problem is to find successful speech enhancement schemes. Binaural unmasking experiments demonstrate that binaural correlation properties can lower the hearing threshold in noise and there is evidence that this may operate in frequency subbands. The main enhancement advantage of binaural hearing may be in the ability to perform binaural unmasking. The performance is presented of an adaptive noise cancellation scheme which supports the possibility of performing \"binaural unmasking\" outwith the body, and is shown to be capable of out-performing a standard noise-cancellation scheme in the presence of reverberation.\n",
    ""
   ]
  },
  "drullman96_absp": {
   "authors": [
    [
     "Rob",
     "Drullman"
    ],
    [
     "Guido F.",
     "Smoorenburg"
    ]
   ],
   "title": "Effect of multichannel compression on auditory-visual speech reception by the profoundly hearing impaired",
   "original": "asp6_257",
   "page_count": 4,
   "order": 51,
   "p1": "257",
   "pn": "260",
   "abstract": [
    "This study aims at optimizing the presentation of speech signals in the severely reduced dynamic range of the profoundly hearing impaired by means of multichannel compression and amplification. Compression was performed in six 1-oct channels, using compression ratios (CR) of 1, 2, 3, or 5, and a compression threshold of 35 dB below peak level. The CR per channel was varied in 8 conditions. Sentences were presented auditory-visually to 16 profoundly hearing impaired subjects and syllable intelligibility was measured. Results show that all auditory signals are valuable supplements to lipreading. No overall preference is found for any of the conditions, but high CRs (> 3-5) have a significantly detrimental effect. Inspection of the individual results reveals that compression may be beneficial for one subject.\n",
    ""
   ]
  },
  "faulkner96_absp": {
   "authors": [
    [
     "Andrew",
     "Faulkner"
    ],
    [
     "Stuart",
     "Rosen"
    ]
   ],
   "title": "The contribution of temporally-coded acoustic speech patterns to audio-visual speech perception in normally hearing and profoundly hearing-impaired listeners",
   "original": "asp6_261",
   "page_count": 4,
   "order": 52,
   "p1": "261",
   "pn": "264",
   "abstract": [
    "Studies simulating hearing impairment in normally hearing subjects have investigated auditory and audio-visual consonant identification with acoustic signals representing isolated and combined temporal speech pattern elements. These elements comprise the temporal patterning of both periodic laryngeal excitation and aperiodic voiceless excitation, voice fundamental frequency, and the speech amplitude envelope. In consonant identification, the principal auditory contributions to audio-visual speech perception came from the on-and-off patterning of silence, periodic and aperiodic excitation. Variations in amplitude envelope and fundamental frequency provided little further information. In audio-visual sentence recognition, however, speech amplitude information did provide significant information beyond that from the temporal patterning and fundamental frequency of laryngeal excitation. A speech analysing hearing aid, the SiVo-II aid, has been employed to implement these codings. It employs an artificial neural-net classifier trained to extract laryngeal excitation information from speech in noise, and also extracts speech amplitude envelope. In a group of profoundly hearing- impaired listeners who derive little lipreading support from amplified speech, encoded speech pattern elements show a significant advantage, especially in noise.\n",
    ""
   ]
  },
  "geurts96_absp": {
   "authors": [
    [
     "L.",
     "Geurts"
    ],
    [
     "G.",
     "Govaerts"
    ],
    [
     "S.",
     "Peeters"
    ],
    [
     "L. Van",
     "Immerseel"
    ],
    [
     "Astrid van",
     "Wieringen"
    ],
    [
     "Johan",
     "Wouters"
    ]
   ],
   "title": "Speech processing for cochlear implants: the implementation and evaluation of phase-locking in high rate coding strategies",
   "original": "asp6_265",
   "page_count": 4,
   "order": 53,
   "p1": "265",
   "pn": "268",
   "abstract": [
    "Three speech processing schemes for cochlear implants are tested in the present study, one based on the standard continuous interleaved sampling (CIS) strategy, and two designed at incorporating information delivered by the temporal discharge pattern of populations of auditory nerve fibers. Such phase-locking strategies may improve transmission of periodicity and voicing cues in speech recognition. The perceptual importance of temporal information was further examined by changing the time constant in the CIS strategy. Performance of pre- and postlingual LAURA-users was evaluated by means of identification experiments. The vowel and consonant confusion data are analyzed in terms of information transmission scores and multidimensional analyses. First experiments demonstrate the importance of incorporating temporal properties of the speech signal in order to improve speech recognition.\n",
    ""
   ]
  },
  "gosy96_absp": {
   "authors": [
    [
     "Mária",
     "Gósy"
    ]
   ],
   "title": "Consequences of temporary auditory deficiencies for speech perception",
   "original": "asp6_269",
   "page_count": 4,
   "order": 54,
   "p1": "269",
   "pn": "272",
   "abstract": [
    "Children often suffer from temporary hearing losses - when their threshold curves are at 20, 30 or 40 dB - caused by various reasons (adenoid vegetation, allergy or repeated catarrh). Our hypothesis is that these temporary hearing losses (i) cause deficits in the speech perception process, (ii) have long lasting consequences, and (iii) are detectable even after years. 137 children (ages 5 to 9), who had been reported to suffer from catarrh more than a year, were tested in order to evaluate their speech perception processes. Perceptual deficits were found with all these children affecting almost all tested areas from acoustic perception up to comprehension.\n",
    ""
   ]
  },
  "moore96_absp": {
   "authors": [
    [
     "Brian C. J.",
     "Moore"
    ],
    [
     "Thomas",
     "Baer"
    ],
    [
     "Brian R.",
     "Glasberg"
    ],
    [
     "Deborah A.",
     "Vickers"
    ]
   ],
   "title": "Simulations of the effect of hearing impairment on speech perception",
   "original": "asp6_273",
   "page_count": 6,
   "order": 55,
   "p1": "273",
   "pn": "278",
   "abstract": [
    "Cochlear hearing impairment is associated with reduced frequency selectivity and with loudness recruitment. This paper describes a series of studies that assess the importance of these factors for speech intelligibility by simulation of their effects, either singly or in combination. The effects of reduced frequency selectivity are simulated by spectral smearing, using the overlap-add method. The smearing has adverse effects on speech intelligibility especially at adverse speech-to-background ratios. The effects of loudness recruitment combined with threshold elevation are simulated using fast-acting expansion applied independently in thirteen frequency bands. The intelligibility of speech in quiet at low levels and in noise at higher levels is adversely affected. The adverse effects in quiet and in speech-shaped noise are overcome by frequency-selective amplification applied prior to the simulation. However, the effect in a background of a single talker is not compensated by linear amplification. Recently, the combined effects of reduced frequency selectivity and loudness recruitment have been simulated in the normal ears of subjects with unilateral cochlear losses. Generally, the performance with stimuli simulating impairment in the normal ear was somewhat better than that obtained in the impaired ear. We are currently investigating the possibility that the worse performance in the impaired ear can be attributed to \"dead\" regions in the cochlea, where there are no functioning inner hair cells or auditory neurones.\n",
    ""
   ]
  },
  "skljarov96_absp": {
   "authors": [
    [
     "O.",
     "Skljarov"
    ]
   ],
   "title": "The perception of the own delay speech as a tool for start-upping of the rhythm at the stuttering",
   "original": "asp6_279",
   "page_count": 4,
   "order": 56,
   "p1": "279",
   "pn": "282",
   "abstract": [
    "We are regarding the speech perception and speech production as the unified problem of the selforganization (or the speech development), arising, in all likelihood, on the control level. It's known that the selforganization is the process of the change of the preceded stable state lost its stability by new stable state [1]. This process followed by the arising of the rhythmic organization (both temporal and spatial). It may be shown that this rhythm is described by recurrent bifurcation mapping in the \"slow\" time (which is analogous in the \"slow\" neuronal dynamics) and in the specific neuronal space [2]. Impossibility to realize this rhythm we treat as the some homeostatic state that is responsible for stuttering. In this paper we shall have demonstrated how to overcome this homeostatic state by increasing of the sensory feedback, in particular, by the own delay speech feedback.\n",
    ""
   ]
  },
  "sundaramoorthy96_absp": {
   "authors": [
    [
     "Vasanthi",
     "Sundaramoorthy"
    ],
    [
     "Michael J.",
     "Pont"
    ]
   ],
   "title": "Towards a computer simulation of the electrocochleogram",
   "original": "asp6_283",
   "page_count": 4,
   "order": 57,
   "p1": "283",
   "pn": "286",
   "abstract": [
    "The electrocochleogram (ECochG) is an evoked electrical response produced when the auditory system is stimulated with appropriate stimuli, such as a click. In this paper, we describe a novel computer simulation of the ECochG. We demonstrate that the simulation can reproduce the range of ECochGs from a normal subject in response to click stimuli at various intensities. We suggest ways in which future versions of the simulation might be used in hearing research and in a clinical setting.\n",
    ""
   ]
  },
  "beet96_absp": {
   "authors": [
    [
     "S. W.",
     "Beet"
    ],
    [
     "L.",
     "Baghai-Ravary"
    ]
   ],
   "title": "Towards a better auditory representation for speech recognition",
   "original": "asp6_287",
   "page_count": 4,
   "order": 58,
   "p1": "287",
   "pn": "290",
   "abstract": [
    "This paper compares a number of different auditory power spectral density representations of speech signals in a phoneme recognition task. The numerical properties of the various representations are quite different even though they are calculated from the same intermediate representation. The results presented here clearly indicate that the degree of variability in results is large, even when it is ostensibly the same parameter which is being estimated. Thus, it is not merely 'what' is calculated, but 'how' its value is estimated, which ultimately may determine recognition performance. Two similar but different sets of comparisons have been made to confirm that a significant difference does indeed exist. In both cases, the maximum entropy method of power spectrum estimation is significantly better than the others, even though both this and the maximum likelihood method are based on the same initial linear prediction analysis of the signal. The maximum likelihood method's performance is very nearly the same as that of the Blackman-Tukey method.\n",
    ""
   ]
  },
  "bodden96_absp": {
   "authors": [
    [
     "Markus",
     "Bodden"
    ],
    [
     "Klaus",
     "Rateitschek"
    ]
   ],
   "title": "Noise-robust speech recognition based on a binaural auditory model",
   "original": "asp6_291",
   "page_count": 6,
   "order": 59,
   "p1": "291",
   "pn": "296",
   "abstract": [
    "A binaural auditory model, known as the Cocktail-Party-Processor, was used as a front-end in isolated word recognition based on a HMM recognizer. Experiments which compared the performance of the binaural models' representation to that of a standard monaural one showed that the binaural model per- formed significantly better in terms of recognition score for different kinds of additive noise (pink noise, concurrent speech) and a wide range of signal-to-noise ratios. Since the binaural representation is robust towards variations of the direction of incidence of the extracted signal, a new training of the speech recognizer for different acoustic situations is not necessary.\n",
    ""
   ]
  },
  "cooke96b_absp": {
   "authors": [
    [
     "Martin",
     "Cooke"
    ],
    [
     "Andrew",
     "Morris"
    ],
    [
     "Phil",
     "Green"
    ]
   ],
   "title": "Recognising occluded speech",
   "original": "asp6_297",
   "page_count": 4,
   "order": 60,
   "p1": "297",
   "pn": "300",
   "abstract": [
    "Auditory representations of clean speech contain much redundancy. Arguably, it is this redundancy which enables listeners to recognise speech in adverse conditions. Under the assumption that some time-frequency regions are too heavily masked to derive any useful estimate of speech level, the auditory system faces the missing data problem: how to achieve robust performance with incomplete evidence. This paper develops several techniques for overcoming the missing data problem and demonstrates that high-performance automatic speech recognition can be achieved for quite high degrees of data masking, but that further information, such as provided by context or through auditory induction constraints, will be required to handle realistic masking conditions.\n",
    ""
   ]
  },
  "dobrin96_absp": {
   "authors": [
    [
     "Cristina",
     "Dobrin"
    ]
   ],
   "title": "A new type of synchrony detector",
   "original": "asp6_301",
   "page_count": 4,
   "order": 61,
   "p1": "301",
   "pn": "304",
   "abstract": [
    "In this paper, a real environment application of an auditory-based front end for speech recognition will be described. The auditory model used as a basis for improvement is the Seneff model, for which several small-scale applications in speech segmentation, speech analysis have been described (see refs. [1], [3]). A qualitative and quantitative comparison with the traditional FFT-based front-end is presented. Practical constraints will lead to a more suitable implementation of the feature extraction block of the auditory front-end.\n",
    ""
   ]
  },
  "hoffmann96_absp": {
   "authors": [
    [
     "Rüdiger",
     "Hoffmann"
    ],
    [
     "Christian-M.",
     "Westendorf"
    ]
   ],
   "title": "Performance evaluation of a low cost filter bank basing on investigations of vowel perception",
   "original": "asp6_305",
   "page_count": 4,
   "order": 62,
   "p1": "305",
   "pn": "308",
   "abstract": [
    "This paper compares the performance of a filter bank with merely 8 channels to other preprocessing methods in a speech recognition task. The filter bank is the product of former investigations in perception of German long vowels which are shortly summarized. Although the application of the 8 channel filter bank reduces the computing expense drastically, the recognition results are comparable to those of other algorithms. Consequently, the filter bank may be recommended for low cost preprocessing.\n",
    ""
   ]
  },
  "lippmann96_absp": {
   "authors": [
    [
     "Richard P.",
     "Lippmann"
    ]
   ],
   "title": "Speech perception by humans and machines",
   "original": "asp6_309",
   "page_count": 8,
   "order": 63,
   "p1": "309",
   "pn": "316",
   "abstract": [
    "This paper reviews past research on human speech perception and recent studies which compare the performance of humans and speech recognizers using six modern speech corpora with vocabularies ranging from 10 to 65,000 words. Error rates of machines are often more than an order of magnitude greater than those of humans for quiet, clearly spoken speech. Machine performance degrades further below that of humans in noise and under other stressing conditions. Human performance remains high with natural variability caused by new talkers, spontaneous speaking styles, noise, and reverberation. Human performance also remains high with unnatural degradations caused by waveform clipping, band-reject filtering, and analog waveform scrambling. Humans can also recognize quiet, clearly spoken nonsense syllables and words without high-level grammatical information. Much further algorithm development is required before even the low-level acoustic-phonetic accuracy of machines equals that of humans on real-world tasks.\n",
    ""
   ]
  },
  "boda96_absp": {
   "authors": [
    [
     "Peter-Pal",
     "Boda"
    ],
    [
     "Johan de",
     "Veth"
    ],
    [
     "Louis",
     "Boves"
    ]
   ],
   "title": "Channel normalisation by using RASTA filtering and the dynamic cepstrum for automatic speech recognition over the phone",
   "original": "asp6_317",
   "page_count": 4,
   "order": 64,
   "p1": "317",
   "pn": "320",
   "abstract": [
    "Human auditory perception is perfectly capable to deal with time-invariant linear filter effects, such as those introduced by telephone handsets and telephone channels. We compared two different schemes for modeling human auditory time-frequency masking: RASTA filtering and the dynamic cepstrum representation (DCR). We used a small set of context-independent phone hidden Markov models for a recognition task of connected digit strings over the telephone. We found that RASTA filtering out-performed the Gaussian DCR approach, despite the fact that RASTA represents a more crude approximation of human forward masking. Our results may be influenced by the choice of the mel-frequency cepstral representation that we used. The superiour performance of the RASTA technique may also be explained by the fact that the frequency response of the RASTA filter is better matched to the region of modulation frequencies where human auditory perception is most sensitive.\n",
    ""
   ]
  },
  "pont96_absp": {
   "authors": [
    [
     "Michael J.",
     "Pont"
    ],
    [
     "Paul I. J.",
     "Keeton"
    ],
    [
     "Pramod",
     "Palooran"
    ]
   ],
   "title": "Speech recognition using a combination of auditory models and conventional neural networks",
   "original": "asp6_321",
   "page_count": 4,
   "order": 65,
   "p1": "321",
   "pn": "324",
   "abstract": [
    "This paper compares the utility of simulated auditory nerve and dorsal cochlear nucleus representations of sound for the purposes of speech recognition. Under test was the hypothesis that the \"higher\" up the auditory pathway we go, the \"better\" the representation we would find. In relation to the initial hypothesis, the results from this study are interesting but inconclusive.\n",
    ""
   ]
  }
 },
 "sessions": [
  {
   "title": "Overview of the Workshop's Key Themes",
   "papers": [
    "greenberg96_absp",
    "hackney96_absp",
    "pols96_absp",
    "clark96_absp",
    "suga96_absp"
   ]
  },
  {
   "title": "Anatomy and Physiology of the Auditory System as it Pertains to Speech",
   "papers": [
    "adams96_absp",
    "dinse96_absp",
    "palmer96_absp",
    "riquimaroux96_absp",
    "wong96_absp",
    "wong96b_absp"
   ]
  },
  {
   "title": "Auditory Models and Representations of Speech",
   "papers": [
    "damper96_absp",
    "deng96_absp",
    "hansen96_absp",
    "hirahara96_absp",
    "jones96_absp",
    "langner96_absp",
    "lauter96_absp",
    "lublinskaja96_absp",
    "samarta96_absp",
    "mashari96_absp",
    "omard96_absp",
    "sachs96_absp",
    "schreiner96_absp"
   ]
  },
  {
   "title": "Perceptual Cues and Features",
   "papers": [
    "bonneau96_absp",
    "formby96_absp",
    "ghitza96_absp",
    "kawahara96_absp",
    "kewleyport96_absp",
    "klasmeyer96_absp",
    "lopezbascuas96_absp",
    "pind96_absp",
    "wieringen96_absp"
   ]
  },
  {
   "title": "Auditory Scene Analysis and Perceptual Organisation",
   "papers": [
    "berthommier96_absp",
    "brown96_absp",
    "cheveigne96_absp",
    "cooke96_absp",
    "cosi96_absp",
    "ellis96_absp",
    "janker96_absp",
    "mccabe96_absp",
    "meyer96_absp",
    "todd96_absp",
    "varin96_absp",
    "warren96_absp",
    "woods96_absp"
   ]
  },
  {
   "title": "Speech Processing under Adverse Conditions",
   "papers": [
    "carlyon96_absp",
    "darlington96_absp",
    "patterson96_absp"
   ]
  },
  {
   "title": "Speech Perception by the Hearing Impaired",
   "papers": [
    "campbell96_absp",
    "drullman96_absp",
    "faulkner96_absp",
    "geurts96_absp",
    "gosy96_absp",
    "moore96_absp",
    "skljarov96_absp",
    "sundaramoorthy96_absp"
   ]
  },
  {
   "title": "Auditory Processing for Speech Recognition",
   "papers": [
    "beet96_absp",
    "bodden96_absp",
    "cooke96b_absp",
    "dobrin96_absp",
    "hoffmann96_absp",
    "lippmann96_absp",
    "boda96_absp",
    "pont96_absp"
   ]
  }
 ]
}