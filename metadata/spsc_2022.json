{
 "series": "SPSC",
 "title": "2nd Symposium on Security and Privacy in Speech Communication",
 "location": "Incheon, Korea",
 "startDate": "23/09/2022",
 "endDate": "24/09/2022",
 "URL": "https://spsc-symposium2022.mobileds.de/",
 "chair": "Chairs: Ingo Siegert and Natalia Tomashenko and Jennifer Williams",
 "intro": "intro.pdf",
 "ISSN": "",
 "conf": "SPSC",
 "year": "2022",
 "name": "spsc_2022",
 "SIG": "",
 "title1": "2nd Symposium on Security and Privacy in Speech Communication",
 "date": "23-24 September 2022",
 "booklet": "spsc_2022.pdf",
 "papers": {
  "sloot22_spsc": {
   "authors": [
    [
     "Bart van der",
     "Sloot"
    ]
   ],
   "title": "Deepfakes: regulatory challenges for the synthetic society",
   "original": "OpeningKeynote",
   "page_count": 0,
   "order": 1,
   "p1": "",
   "pn": "",
   "abstract": [
    "With the rise of deepfakes and synthetic media, the question as to what is real and what is not will become increasingly important and politized. Deepfakes can be used to spread fake news, influence elections, introduce highly realistic fake evidence in courts and make fake porno movies. Each of these applications potentially has a big impact on society, social relationships, democracy and the rule of law. The question this talk shall assess is whether the current regulatory regime suffices to address these potential harms and if not, which additional rules and principles should be adopted. It will discuss several potential amendments to the privacy and data protection regime, limitations to the freedom of expression and ex ante rules on the distribution of use of deepfake-technologies.  \n"
   ]
  },
  "stine22_spsc": {
   "authors": [
    [
     "Jon",
     "Stine"
    ]
   ],
   "title": "The five issues between voice and its value",
   "original": "ClosingKeynote",
   "page_count": 0,
   "order": 2,
   "p1": "",
   "pn": "",
   "abstract": [
    "How big should voice be? How big can voice become? For some, conversational AI is a toy-like technology of convenience – an alarm, a music player, a teller of tales and news headlines. For businesses, conversational AI is most often a technology of automation and efficiency – one that alleviates call center burdens. But let’s pause and consider, for a moment, the possibilities. Here is an inclusive interface that will soon be resident on every digital device – in a world where every device is digital. Here’s a capability that will exist within every website, every smart system, every AI. We’re at the edge – and let’s dare to say it – of a worldwide voice web. Join Jon Stine, Executive Director of the Open Voice Network, a community of the Linux Foundation, for an exploration of where voice is, where it can go (for optimal societal and economic benefit), and what stands in its way. (Spoiler alert: he’s going to talk about interoperability and data ownership.)  \n"
   ]
  },
  "wu22_spsc": {
   "authors": [
    [
     "Zhizheng",
     "Wu"
    ]
   ],
   "title": " Detecting manipulated and synthetic audio",
   "original": "InvitedTalk1",
   "page_count": 0,
   "order": 3,
   "p1": "",
   "pn": "",
   "abstract": [
    "In recent years, we have witnessed the astonishing advancement of speech generation technology, thanks to the rapid development of deep learning. The state-of-the-art speech synthesis technology can clone a speaker’s voice with a few training samples and generate natural-sounding audio samples that the speaker never said. The technology can be misused to create misinformation, which spreads farther, faster, and more broadly than the truth and erodes our trust in online information. It can also be misused to attack voice biometric systems. This talk will first present a high-level overview of approaches to manipulate and synthesize audio. Then, it will highlight recent technical developments to detect manipulated and synthetic audio. This talk will also discuss some current challenges and the needs from a user point of view. \n"
   ]
  },
  "kamocki22_spsc": {
   "authors": [
    [
     "Pawel",
     "Kamocki"
    ]
   ],
   "title": "Choose a pseudonym. Legal perspective on pseudonymisation of speech data",
   "original": "InvitedTalk2",
   "page_count": 0,
   "order": 4,
   "p1": "",
   "pn": "",
   "abstract": [
    "Often overlooked, pseudonymisation can be an interesting alternative to anonymisation, especially in the context of speech data. Recognised as a safeguard for the rights and freedoms of data subjects, pseudonymisation can make GDPR compliance considerably easier to achieve. This talk will discuss the advantages of pseudonymisation, its special role for speech data (e.g. according to the European Data Protection Board's guidelines on voice assistants), and its seemingly bright future under the EU Data Governance Act.\n"
   ]
  },
  "williams22_spsc": {
   "authors": [
    [
     "Jennifer",
     "Williams"
    ],
    [
     "Karla",
     "Pizzi"
    ],
    [
     "Shuvayanti",
     "Das"
    ],
    [
     "Paul-Gauthier",
     "Noé"
    ]
   ],
   "title": "New challenges for content privacy in speech and audio",
   "original": "SPSC_Paper1",
   "page_count": 6,
   "order": 5,
   "p1": 1,
   "pn": 6,
   "abstract": [
    "Privacy in speech and audio has many facets. A particularly under-developed area of privacy in this domain involves consideration for information related to content and context. Speech content can include words and their meaning or even stylistic markers, pathological speech, intonation patterns, or emotion. More generally, audio captured in-the-wild may contain background speech or reveal contextual information such as markers of location, room characteristics, paralinguistic sounds, or other audible events. Audio recording devices and speech technologies are becoming increasingly commonplace in everyday life. At the same time, commercialised speech and audio technologies do not provide consumers with a range of privacy choices. Even where privacy is regulated or protected by law, technical solutions to privacy assurance and enforcement fall short. This position paper introduces three important and timely research challenges for content privacy in speech and audio. We highlight current gaps and opportunities, and identify focus areas, that could have significant implications for developing ethical and safer speech technologies.\n"
   ],
   "doi": "10.21437/SPSC.2022-1"
  },
  "gogate22_spsc": {
   "authors": [
    [
     "Mandar",
     "Gogate"
    ],
    [
     "Kia",
     "Dashtipour"
    ],
    [
     "Amir",
     "Hussain"
    ]
   ],
   "title": "Towards real-time privacy-preserving audio-visual speech enhancement",
   "original": "SPSC_Paper2",
   "page_count": 4,
   "order": 6,
   "p1": 7,
   "pn": 10,
   "abstract": [
    "Human auditory cortex in everyday noisy situations is known to exploit aural and visual cues that are contextually combined by the brain’s multi-level integration strategies to selectively suppress the background noise and focus on the target speaker. The multimodal nature of speech is well established, with listeners known to unconsciously lip read to improve the intelligibility of speech in noise. However, despite significant research in the area of audio-visual (AV) speech enhancement real-time processing models, with low latency remains a formidable technical challenge. In this paper, we propose a novel audio-visual speech enhancement model based on Temporal Convolutional Networks (TCN) that exploit the privacy preserving lip-landmark flow features for speech enhancement in multitalker cocktail party environments. In addition, we propose an efficient implementation of TCN, called Fast-TCN, to enable real time deployment of the proposed framework. The comparative simulation results in terms of speech quality and intelligibility demonstrate the effectiveness of our proposed AV model as compared to benchmark audio-only and audio-visual approaches for speaker and noise independent scenarios.\n"
   ],
   "doi": "10.21437/SPSC.2022-2"
  },
  "pizzi22_spsc": {
   "authors": [
    [
     "Karla",
     "Pizzi"
    ],
    [
     "Franziska",
     "Boenisch"
    ],
    [
     "Ugur",
     "Sahin"
    ],
    [
     "Konstantin",
     "Böttinger"
    ]
   ],
   "title": "Introducing model inversion attacks on automatic speaker recognition",
   "original": "SPSC_Paper3",
   "page_count": 6,
   "order": 7,
   "p1": 11,
   "pn": 16,
   "abstract": [
    "Model inversion (MI) attacks allow to reconstruct average perclass representations of a machine learning (ML) model’s training data. It has been shown that in scenarios where each class corresponds to a different individual, such as face classifiers, this represents a severe privacy risk. In this work, we explore a new application for MI: the extraction of speakers’ voices from a speaker recognition system. We present an approach to (1) reconstruct audio samples from a trained ML model and (2) extract intermediate voice feature representations which provide valuable insights into the speakers’ biometrics. Therefore, we propose an extension of MI attacks which we call sliding model inversion. Our sliding MI extends standard MI by iteratively inverting overlapping chunks of the audio samples and thereby leveraging the sequential properties of audio data for enhanced inversion performance. We show that one can use the inverted audio data to generate spoofed audio samples to impersonate a speaker, and execute voice-protected commands for highly secured systems on their behalf. To the best of our knowledge, our work is the first one extending MI attacks to audio data, and our results highlight the security risks resulting from the extraction of the biometric data in that setup.\n"
   ],
   "doi": "10.21437/SPSC.2022-3"
  },
  "sinha22_spsc": {
   "authors": [
    [
     "Yamini",
     "Sinha"
    ],
    [
     "Jan",
     "Hintz"
    ],
    [
     "Matthias",
     "Busch"
    ],
    [
     "Tim",
     "Polzehl"
    ],
    [
     "Matthias",
     "Haase"
    ],
    [
     "Andreas",
     "Wendemuth"
    ],
    [
     "Ingo",
     "Siegert"
    ]
   ],
   "title": "Why Eli Roth should not use TTS-Systems for anonymization",
   "original": "SPSC_Paper4",
   "page_count": 6,
   "order": 8,
   "p1": 17,
   "pn": 22,
   "abstract": [
    "This paper evaluates the impact of using TTS-based speaker anonymization with objective and subjective methods. A pretrained automatic speaker verification (ASV) VGGVox model (95.66% recognition rate on Voxceleb 1), enrolled with human voices, is tested on the anonymized voices obtained from eSpeak TTS, to objectively verify the anonymization. We used one of the benchmark datasets for a speaker verification task with 1,251 speakers and over 100,000 utterances, consisting of spontaneous speech called VoxCeleb1. Upon anonymizing 40 speakers from the VoxCeleb1 test dataset, the objective evaluation shows that ASV systems, if presented with synthetic speech samples, are vulnerable to false acceptance. Experimental results show that after anonymization, approximately 6% of the TTS speaker samples were falsely accepted as the counterfeited human speaker. This confusion about a TTS speaker as a human speaker may lie in the accuracy of the ASV model and the similarity metric used. Furthermore, we examined these confused speaker pairs against non-confused speaker pairs using a subjective measure (listener’s ratings) with 200 test subjects. In the subjective evaluations using a crowd-sourced platform, no significant results could be concluded, as human raters were unsure whether or not the voices were similar.\n"
   ],
   "doi": "10.21437/SPSC.2022-4"
  },
  "das22_spsc": {
   "authors": [
    [
     "Sneha",
     "Das"
    ],
    [
     "Nicole Nadine",
     "Lønfeldt"
    ],
    [
     "Anne Katrine",
     "Pagsberg"
    ],
    [
     "Line. H.",
     "Clemmensen"
    ],
    [
     "Nicklas Leander",
     "Lund"
    ]
   ],
   "title": "Zero-shot cross-lingual speech emotion recognition: a study of loss functions and feature importance",
   "original": "SPSC_Paper5",
   "page_count": 7,
   "order": 9,
   "p1": 23,
   "pn": 29,
   "abstract": [
    "Deep learning has led to the rapid advancement of speech emotion recognition (SER) hence enabling its application and deployment in wide ranging applications and sectors. However, conventional challenges like generalizing over unseen corpora and languages, and newer challenges like the lack of interpretability and transparency of deep learning models impact the security of these methods, thereby negatively influencing their usability and acceptability in real-world applications. Here, we address this gap by investigating the influence of the formulation and design of the learning function on the ability to transfer emotion representation learned in one language to other languages. Furthermore, we examine the importance of the different feature groups for the emotion classes, and the associations between the feature groups and the learning functions. From the evaluation, we conclude that the dimensional model of emotion, specifically activation is more transferable than emotion classes over unseen languages than valence. However, this transferability does not necessarily translate to higher classification accuracy.\n"
   ],
   "doi": "10.21437/SPSC.2022-5"
  },
  "liao22_spsc": {
   "authors": [
    [
     "Yen-Lun",
     "Liao"
    ],
    [
     "Xuanjun",
     "Chen"
    ],
    [
     "Chung-Che",
     "Wang"
    ],
    [
     "Jyh-Shing Roger",
     "Jang"
    ]
   ],
   "title": "Adversarial speaker distillation for countermeasure model on automatic speaker verification",
   "original": "SPSC_Paper6",
   "page_count": 5,
   "order": 10,
   "p1": 30,
   "pn": 34,
   "abstract": [
    "The countermeasure (CM) model is developed to protect ASV systems from spoof attacks and prevent resulting personal information leakage in Automatic Speaker Verification (ASV) system. Based on practicality and security considerations, the CM model is usually deployed on edge devices, which have more limited computing resources and storage space than cloud-based systems, confining the model size under a limitation. To better trade off the CM model sizes and performance, we proposed an adversarial speaker distillation method, which is an improved version of knowledge distillation method combined with generalized end-to-end (GE2E) pre-training and adversarial fine-tuning. In the evaluation phase of the ASVspoof 2021 Logical Access task, our proposed adversarial speaker distillation ResNetSE (ASD-ResNetSE) model reaches 0.2695 min t-DCF and 3.54% EER. ASD-ResNetSE only used 22.5% of parameters and 19.4% of multiply and accumulate operands of ResNetSE model.\n"
   ],
   "doi": "10.21437/SPSC.2022-6"
  },
  "mawalim22_spsc": {
   "authors": [
    [
     "Candy Olivia",
     "Mawalim"
    ],
    [
     "Shogo",
     "Okada"
    ],
    [
     "Masashi",
     "Unoki"
    ]
   ],
   "title": "Speaker anonymization by pitch shifting based on time-scale modification",
   "original": "VPC_Paper1",
   "page_count": 8,
   "order": 11,
   "p1": 35,
   "pn": 42,
   "abstract": [
    " The increasing usage of speech in digital technology raises a privacy issue because speech contains biometric information. Several methods of dealing with this issue have been proposed, including speaker anonymization or de-identification. Speaker anonymization aims to suppress personally identifiable information (PII) while keeping the other speech properties, including linguistic information. In this study, we utilize time-scale modification (TSM) speech signal processing for speaker anonymization. Speech signal processing approaches are significantly less complex than the state-of-the-art x-vector-based speaker anonymization method because it does not require a training process. We propose anonymization methods using two major categories of TSM, synchronous overlap-add (SOLA)-based algorithm and phase vocoder-based TSM (PV-TSM). For evaluating our proposed methods, we utilize the standard objective evaluation introduced in the VoicePrivacy challenge. The results show that our method based on the PV-TSM balances privacy and utility metrics better than baseline systems, especially when evaluating with an automatic speaker verification (ASV) system in anonymized enrollment and anonymized trials (a-a). Further, our method outperformed the x-vector-based speaker method, which has limitations in its complex training process, low privacy in an a-a scenario, and low voice distinctiveness.\n"
   ],
   "doi": "10.21437/SPSC.2022-7"
  },
  "khamsehashari22_spsc": {
   "authors": [
    [
     "Razieh",
     "Khamsehashari"
    ],
    [
     "Yamini",
     "Sinha"
    ],
    [
     "Jan",
     "Hintz"
    ],
    [
     "Suhita",
     "Ghosh"
    ],
    [
     "Tim",
     "Polzehl"
    ],
    [
     "Carlos",
     "Franzreb"
    ],
    [
     "Sebastian",
     "Stober"
    ],
    [
     "Ingo",
     "Siegert"
    ]
   ],
   "title": "Voice Privacy - leveraging multi-scale blocks with ECAPA-TDNN SE-Res2NeXt extension for speaker anonymization",
   "original": "VPC_Paper2",
   "page_count": 6,
   "order": 12,
   "p1": 43,
   "pn": 48,
   "abstract": [
    "This paper presents the ongoing efforts on voice anonymization with the purpose to securely anonymize a speaker’s identity in a hotline call scenario. Our hotline seeks out to provide help by remote assessment, treatment and prevention against child sexual abuse in Germany. The presented work originates from the joint contribution to the VoicePrivacy Challenge 2022 and the Symposium on Security and Privacy in Speech Communication in 2022. Having analyzed in depth the results of the first instantiation of the Voice Privacy Challenge in 2020, the current experiments aim to improve the robustness of two distinct components of the challenge baseline. First, we analyze ASR embeddings, in order to present a more precise and resistant representation of the source speech that is used in the challenge baseline GAN. First experiments using wav2vec show promising results. Second, to alleviate modeling and matching of source and target speaker characteristics, we propose to exchange the baseline x-vectors speaker identity features with the more robust ECAPA-TDNN embedding, in order to leverage its higher resolution multi-scale architecture. Also, improving on ECAPA-TDNN, we propose to extend the model architecture by integrating SE-Res2NeXt units, as the expectation that by representing features at various scales using a cutting-edge building block for CNNs, the latter will perform better than the SE-Res2Net block that creates hierarchical residual-like connections within a single residual block, allowing them to represent features at multiple scales. This expands the range of receptive fields for each network layer and depicts multi-scale features at a finer level. Ultimately, when including a more precise speaker identity embedding we expect to reach improvements for future anonymization for various application cases.\n"
   ],
   "doi": "10.21437/SPSC.2022-8"
  },
  "meyer22_spsc": {
   "authors": [
    [
     "Sarina",
     "Meyer"
    ],
    [
     "Pascal",
     "Tilli"
    ],
    [
     "Florian",
     "Lux"
    ],
    [
     "Pavel",
     "Denisov"
    ],
    [
     "Julia",
     "Koch"
    ],
    [
     "Ngoc Thang",
     "Vu"
    ]
   ],
   "title": "Cascade of phonetic speech recognition, speaker embeddings gan and multispeaker speech synthesis for the VoicePrivacy 2022 Challenge  ",
   "original": "VPC_Paper4",
   "page_count": 0,
   "order": 13,
   "p1": "",
   "pn": "",
   "abstract": [
    "Speaker anonymization is the task of modifying speech recordings to hide the identity of the original speaker by changing the voice in the audio. Simultaneously, the anonymized audio should remain usable for downstream tasks and thus keep other information of the original audio like the linguistic content. This typically creates a privacy-utility trade-off of anonymization techniques. In our submission to the VoicePrivacy 2022 Challenge, we aim to reduce this trade-off by creating a speech-to-speech pipeline that (a) eliminates all clues about speaker identity by reducing the audio to phonetic transcriptions, (b) generates a new, non-existent voice using a Generative Adversarial Network, leading to artificial yet natural-like and distinctive speakers, and (c) synthesizes an anonymous version of the original utterance based on the transcriptions, anonymous speaker embedding, and estimated pitch. According to the objective evaluation, this anonymization method leads to almost perfect privacy and voice distinctiveness, and clearly outperforms all baseline systems for these two metrics. For the speech recognition utility metric, we achieve similar good results on LibriSpeech and much better ones on VCTK as compared to the baselines and the original non-anonymized data. Solely for pitch correlation, we only just meet the required threshold because our system does not use the original pitch trajectory for synthesis. Overall, our approach successfully hides the speaker identity while keeping the linguistic content, proving to be generally more effective than any of the baselines of the VoicePrivacy 2022 Challenge.\n"
   ]
  },
  "yao22_spsc": {
   "authors": [
    [
     "Jixun",
     "Yao"
    ],
    [
     "Qing",
     "Wang"
    ],
    [
     "Li",
     "Zhang"
    ],
    [
     "Pengcheng",
     "Guo"
    ],
    [
     "Yuhao",
     "Liang"
    ],
    [
     "Lei",
     "Xie"
    ]
   ],
   "title": "NWPU-ASLP System for the VoicePrivacy 2022 Challenge",
   "original": "VPC_Paper5",
   "page_count": 0,
   "order": 14,
   "p1": "",
   "pn": "",
   "abstract": [
    "This paper presents the NWPU-ASLP speaker anonymization system for VoicePrivacy 2022 Challenge. Our submission does not involve additional Automatic Speaker Verification (ASV) model or x-vector pool. Our system consists of four modules, including feature extractor, acoustic model, anonymization module, and neural vocoder. First, the feature extractor extracts the Phonetic Posteriorgram (PPG) and pitch from the input speech signal. Then, we reserve a pseudo speaker ID from a speaker look-up table (LUT), which is subsequently fed into a speaker encoder to generate the pseudo speaker embedding that is not corresponding to any real speaker. To ensure the pseudo speaker is distinguishable, we further average the randomly selected speaker embedding and weighted concatenate it with the pseudo speaker embedding to generate the anonymized speaker embedding. Finally, the acoustic model outputs the anonymized mel-spectrogram from the anonymized speaker embedding and a modified version of HifiGAN transforms the mel-spectrogram into the anonymized speech waveform. Experimental results demonstrate the effectiveness of our proposed anonymization system.\n",
    ""
   ]
  },
  "chen22_spsc": {
   "authors": [
    [
     "Xiaojiao",
     "Chen"
    ],
    [
     "Guangxing",
     "Li"
    ],
    [
     "Hao",
     "Huang"
    ],
    [
     "Wangjin",
     "Zhou"
    ],
    [
     "Sheng",
     "Li"
    ],
    [
     "Yang",
     "Cao"
    ],
    [
     "Yi",
     "Zhao"
    ]
   ],
   "title": "System description for Voice Privacy Challenge 2022 ",
   "original": "VPC_Paper6",
   "page_count": 0,
   "order": 15,
   "p1": "",
   "pn": "",
   "abstract": [
    "This paper introduces our system submitted to Voice Privacy Challenge 2022. We adopted the following methods to improve the traditional methods. Firstly, the adversarial anonymization method was used, further hiding speaker information. Then, we extracted the embedding from the encoder of the transformer based ASR systems because ASR has rich speaker information, so we do not have to train an individual speaker recognition/verification system for speaker embedding extraction. Experimental results prove that the proposed methods can be used for speaker anonymization tasks.\n",
    ""
   ]
  },
  "gaznepoglu22_spsc": {
   "authors": [
    [
     "Unal Ege",
     "Gaznepoglu"
    ],
    [
     "Anna",
     "Leschanowsky"
    ],
    [
     "Nils",
     "Peters"
    ]
   ],
   "title": "VoicePrivacy 2022 system description: speaker anonymization with feature-matched f0 trajectories",
   "original": "VPC_Paper7",
   "page_count": 0,
   "order": 16,
   "p1": "",
   "pn": "",
   "abstract": [
    "We introduce a novel method to improve the performance of the VoicePrivacy Challenge 2022 baseline B1 variants. Among the known deficiencies of x-vector-based anonymization systems is the insufficient disentangling of the input features. In particular, the fundamental frequency (F0) trajectories, which are used for voice synthesis without any modifications. Especially in cross-gender conversion, this situation causes unnatural sounding voices, increases word error rates (WERs), and personal information leakage. Our submission overcomes this problem by synthesizing an F0 trajectory, which better harmonizes with the anonymized x-vector. We utilized a low-complexity deep neural network to estimate an appropriate F0 value per frame, using the linguistic content from the bottleneck features (BN) and the anonymized x-vector. Our approach results in a significantly improved anonymization system and increased naturalness of the synthesized voice. Consequently, our results suggest that F0 extraction is not required for voice anonymization.\n",
    ""
   ]
  }
 },
 "sessions": [
  {
   "title": "Keynotes",
   "papers": [
    "sloot22_spsc",
    "stine22_spsc"
   ]
  },
  {
   "title": "Invited Talks",
   "papers": [
    "wu22_spsc",
    "kamocki22_spsc"
   ]
  },
  {
   "title": "Security and Privacy in Speech Communication Part 1",
   "papers": [
    "williams22_spsc",
    "gogate22_spsc",
    "pizzi22_spsc"
   ]
  },
  {
   "title": "Security and Privacy in Speech Communication Part 2",
   "papers": [
    "sinha22_spsc",
    "das22_spsc",
    "liao22_spsc"
   ]
  },
  {
   "title": "VoicePrivacy Challenge",
   "papers": [
    "mawalim22_spsc",
    "khamsehashari22_spsc",
    "meyer22_spsc",
    "yao22_spsc",
    "chen22_spsc",
    "gaznepoglu22_spsc"
   ]
  }
 ],
 "doi": "10.21437/SPSC.2022"
}