{
 "title": "ITRW on Speech and Emotion",
 "location": "Newcastle, Northern Ireland, UK",
 "startDate": "5/9/2000",
 "endDate": "7/9/2000",
 "conf": "SpeechEmotion",
 "year": "2000",
 "name": "speechemotion_2000",
 "series": "",
 "SIG": "",
 "title1": "ITRW on Speech and Emotion",
 "date": "5-7 September 2000",
 "booklet": "speechemotion_2000.pdf",
 "papers": {
  "cornelius00_speechemotion": {
   "authors": [
    [
     "Randolph R.",
     "Cornelius"
    ]
   ],
   "title": "Theoretical approaches to emotion",
   "original": "spem_003",
   "page_count": 8,
   "order": 1,
   "p1": "3-10 (Invited review paper)",
   "pn": "",
   "abstract": [
    "Four major theoretical perspectives on emotion in psychology are described. Examples of the ways in which research on emotion and speech utilize aspects of the various perspectives are presented and a plea is made for students of emotion and speech to consider more self-consciously the place of their research within each of the perspectives. integration of theories from the four perspectives.\n",
    ""
   ]
  },
  "cowie00_speechemotion": {
   "authors": [
    [
     "Roddy",
     "Cowie"
    ]
   ],
   "title": "Describing the emotional states expressed in speech",
   "original": "spem_011",
   "page_count": 8,
   "order": 2,
   "p1": "11-18 (Invited review paper)",
   "pn": "",
   "abstract": [
    "Describing relationships between speech and emotion depends on identifying appropriate ways of describing the emotional states that speech conveys. Several forms of description are potentially relevant. The most familiar have obvious shortcomings, and alternatives urgently need to be explored.\n",
    ""
   ]
  },
  "cowie00b_speechemotion": {
   "authors": [
    [
     "Roddy",
     "Cowie"
    ],
    [
     "Ellen",
     "Douglas-Cowie"
    ],
    [
     "Susie",
     "Savvidou"
    ],
    [
     "Edelle",
     "McMahon"
    ],
    [
     "Martin",
     "Sawey"
    ],
    [
     "Marc",
     "Schröder"
    ]
   ],
   "title": "FEELTRACE: An instrument for recording perceived emotion in real time",
   "original": "spem_019",
   "page_count": 6,
   "order": 3,
   "p1": "19",
   "pn": "24",
   "abstract": [
    "FEELTRACE is an instrument developed to let observers track the emotional content of a stimulus as they perceive it over time, allowing the emotional dynamics of speech episodes to be examined. It is based on activation-evaluation space, a representation derived from psychology. The activation dimension measures how dynamic the emotional state is; the evaluation dimension is a global measure of the positive or negative feeling associated with the state. Research suggests that the space is naturally circular, i.e. states which are at the limit of emotional intensity define a circle, with alert neutrality at the centre.\n",
    "To turn those ideas into a recording tool, the space was represented by a circle on a computer screen, and observers described perceived emotional state by moving a pointer (in the form of a disc) to the appropriate point in the circle, using a mouse. Prototypes were tested, and in the light of results, refinements were made to ensure that outputs were as consistent and meaningful as possible. They include colour coding the pointer in a way that users readily associate with the relevant emotional state; presenting key emotion words as landmarks at the strategic points in the space; and developing an induction procedure to introduce observers to the system.\n",
    "An experiment assessed the reliability of the developed system. Stimuli were 16 clips from TV programs, two showing relatively strong emotions in each quadrant of activationevaluation space, each paired with one of the same person in a relatively neural state. 24 raters took part.\n",
    "Differences between clips chosen to contrast were statistically robust. Results were plotted in activation-evaluation space as ellipses, each with its centre at the mean co-ordinates for the clip, and its width proportional to standard deviation across raters. The size of the ellipses meant that about 25 could be fitted into the space, i.e. FEELTRACE has resolving power comparable to an emotion vocabulary of 20 non-overlapping words, with the advantage of allowing intermediate ratings, and above all, the ability to track impressions continuously. constant. Instead it shifts, often gradually, but sometimes sharply - either because of a change in the speakers state, or because something about the speakers state suddenly becomes apparent.\n",
    ""
   ]
  },
  "pereira00_speechemotion": {
   "authors": [
    [
     "Cécile",
     "Pereira"
    ]
   ],
   "title": "Dimensions of emotional meaning in speech",
   "original": "spem_025",
   "page_count": 4,
   "order": 4,
   "p1": "25",
   "pn": "28",
   "abstract": [
    "A number of psychologists have conceptualized the communication of affect as three-dimensional (e.g. Davitz, 1964). The three dimensions proposed are arousal, pleasure and power. This paper reports the findings of a perception study where 31 normally-hearing subjects rated utterances said in the emotion of happiness, sadness, two forms of anger (hot and cold) and a neutral state on the dimensional scales of arousal, pleasure and power. Findings show that the concept of the dimensions of emotion is useful to describe and distinguish emotions; and that emotions with a similar level of arousal, and sometimes a similar level of power, share acoustic characteristics in terms of F0 range and mean, and particularly intensity mean. It is suggested that this contributes to perceived similarity between emotions, and consequently confusions, especially in the hearing-impaired.\n",
    ""
   ]
  },
  "amir00_speechemotion": {
   "authors": [
    [
     "Noam",
     "Amir"
    ],
    [
     "Samuel",
     "Ron"
    ],
    [
     "Nathaniel",
     "Laor"
    ]
   ],
   "title": "Analysis of an emotional speech corpus in Hebrew based on objective criteria",
   "original": "spem_029",
   "page_count": 5,
   "order": 5,
   "p1": "29",
   "pn": "32",
   "abstract": [
    "To build a corpus of emotion in spontaneous speech, subjects were asked to recall an emotional event. Physiologic evaluations were then applied to validate the emotional content. This evaluation consisted of measuring several physiological parameters. Forty subjects were examined, each expressing five basic emotions (anger, fear, joy, sadness and disgust). Subjects who responded within the set criterion of the physiological evaluation were considered to represent the said emotion. This was followed by computer analysis in order to determine a set of criteria that could represent each emotion. The method employed was based on the analysis of the signal over sliding windows and extracting a representative feature set. At each instantce, the distance of the measured feature set from a set of reference points is calculated, and used to compute a fuzzy membership index for each emotion, which distance we called \"emotional index\". We present initial results of this analysis: the characteristics of various emotions are presented, and we discuss the extent to which they are common across different individuals. Our initial results show that when tested on a Hebrew emotional speech corpus the proposed method yields reliable results.\n",
    ""
   ]
  },
  "campbell00_speechemotion": {
   "authors": [
    [
     "Nick",
     "Campbell"
    ]
   ],
   "title": "Databases of emotional speech",
   "original": "spem_034",
   "page_count": 5,
   "order": 6,
   "p1": "34-38 (Invited review paper)",
   "pn": "",
   "abstract": [
    "This paper presents a personal view of some the problems facing speech technologists in the study of emotional speech. It describes some databases that are currently being used, and points out that the majority of them use actors to reproduce the emotions, thereby possibly falsely representing the true characteristics of emotion in speech. Databases of real emotional speech, on the other hand, present serious ethical and moral problems, since the nature of their contents must, by definition, reveal personal and intimate details about the speakers.\n",
    ""
   ]
  },
  "douglascowie00_speechemotion": {
   "authors": [
    [
     "Ellen",
     "Douglas-Cowie"
    ],
    [
     "Roddy",
     "Cowie"
    ],
    [
     "Marc",
     "Schröder"
    ]
   ],
   "title": "A new emotion database: Considerations, sources and scope",
   "original": "spem_039",
   "page_count": 6,
   "order": 7,
   "p1": "39",
   "pn": "44",
   "abstract": [
    "Research on the expression of emotion is underpinned by databases. Reviewing available resources persuaded us of the need to develop one that prioritised ecological validity. The basic unit of the database is a clip, which is an audiovisual recording of an episode that appears to be reasonably selfcontained. Clips range from 10 to 60 secs, and are captured as MPEG files. They were drawn from two main sources. People were recorded discussing emotive subjects either with each other, or with one of the research team. We also recorded extracts from television programs where members of the public interact in a way that at least appears essentially spontaneous. Associated with each clip are two additional types of file. An audio file (.wav format) contains speech alone, edited to remove sounds other than the main speaker. An interpretation file describes the emotional state that observers attribute to the main speaker, using the FEELTRACE system to provide a continuous record of the perceived ebb and flow of emotion. Clips have been extracted for 100 speakers, with at least two for each speaker (one relatively neutral and others showing marked emotions of different kinds).\n",
    ""
   ]
  },
  "mozziconacci00_speechemotion": {
   "authors": [
    [
     "Sylvie J. L.",
     "Mozziconacci"
    ]
   ],
   "title": "The expression of emotion considered in the framework of an intonation model",
   "original": "spem_045",
   "page_count": 8,
   "order": 8,
   "p1": "45-52 (Invited review paper)",
   "pn": "",
   "abstract": [
    "Studying intonational variation conveying the expression of emotion in speech is presented here as being particularly attractive when the pitch variation is represented in the theoretical framework of a model of intonation. Using such a framework while considering the meaning associated with these variations in terms of identification of emotion in speech facilitates a generalization of study results. Moreover, such an approach provides the opportunity to test the adequacy of the model for processing such extreme variations as the ones occurring in emotional speech. The model can be evaluated and could eventually be extended or fine-tuned. Ultimately, interpreting data in the framework of such a model provides a methodological background for investigating intonation phenomena conveying emotion in speech against the background of intonation phenomena relevant to speech communication. The approach proposed is discussed and illustrated by reporting how it has been applied, in particular to own work, using a specific model of intonation. Using this approach while investigating how prosody conveys meaning would stimulate us to consider the power of existing intonation models for modeling expressiveness. Hence, such an approach helps shedding light on functionality of intonation in terms of conveying meaningful information in the oral communication stream. From this point of view, studies on emotional speech contribute to developing and refining theories concerned with prosody.\n",
    ""
   ]
  },
  "roach00_speechemotion": {
   "authors": [
    [
     "Peter",
     "Roach"
    ]
   ],
   "title": "Techniques for the phonetic description of emotional speech",
   "original": "spem_053",
   "page_count": 7,
   "order": 9,
   "p1": "53-59 (Invited review paper)",
   "pn": "",
   "abstract": [
    "It is inconceivable that there could be information present in the speech signal that could be detected by the human auditory system but which is not accessible to acoustic analysis and phonetic categorisation. We know that humans can reliably recognise a range of emotions produced by speakers of their own language on the basis of the acoustic signal alone, yet it appears that our ability to identify the relevant acoustic correlates is at present rather limited. This paper proposes that we have to build a bridge between the human perceptual experience and the measurable properties of the acoustic signal by developing an analytic framework based partly on auditory analysis. A possible framework is outlined which is based on the work of the Reading/Leeds Emotional Speech Database. The project was funded by ESRC Grant no. R000235285.\n",
    ""
   ]
  },
  "stibbard00_speechemotion": {
   "authors": [
    [
     "Richard",
     "Stibbard"
    ]
   ],
   "title": "Automated extraction of ToBI annotation data from the Reading/Leeds emotional speechcorpus",
   "original": "spem_060",
   "page_count": 6,
   "order": 10,
   "p1": "60",
   "pn": "65",
   "abstract": [
    "In the interest of achieving experimental control, most studies of the effects of emotional arousal on the voice have avoided field data, preferring instead to work with data simulated by actors, or, more rarely due to the ethical and practical difficulties involved, induced in the laboratory. These methods allow for control over unwanted variables such as recording conditions, verbal content, accent, age, etc., but entail a corresponding loss of emotional realism (Murray & Arnott, 1993: 1100). Interest is now growing in improving psychological validity by using more natural speech samples in emotion research, despite the corresponding loss in experimental control.\n",
    ""
   ]
  },
  "klasmeyer00_speechemotion": {
   "authors": [
    [
     "Gudrun",
     "Klasmeyer"
    ]
   ],
   "title": "An automatic description tool for time-contours and long-term average voice features inlarge emotional speech databases",
   "original": "spem_066",
   "page_count": 6,
   "order": 11,
   "p1": "66",
   "pn": "71",
   "abstract": [
    "Speech databases used in studies on emotional expression are often too small to represent a realistic survey on how humans express their emotions in spoken language. Large databases give a better survey, but with them acoustic analyses can hardly be performed manually. In consequence all those effects a trained phonetician might discover using acoustic or visual representations of the signals have to be defined in programs before any \"automatic\" measurement of these features can be carried out.\n",
    ""
   ]
  },
  "paeschke00_speechemotion": {
   "authors": [
    [
     "Astrid",
     "Paeschke"
    ],
    [
     "Walter F.",
     "Sendlmeier"
    ]
   ],
   "title": "Prosodic characteristics of emotional speech: Measurements of fundamental frequencymovements",
   "original": "spem_075",
   "page_count": 6,
   "order": 12,
   "p1": "75",
   "pn": "80",
   "abstract": [
    "Recent data on prosodic features of emotional speech in German are reported. Ten sentences, spoken by actors in a happy, fearful, sad, bored, angry and a neutral way served as the basis of the analyses. The features under investigation are 1. different range parameters (differences between sentence accent peak, word accent peaks, minima between accent peaks and sentence final lowest F0), 2. the declination within phrases and 3. the duration and steepness of rising and falling F0 movements within accents. Significant differences between emotions were found for most of the measurements.\n",
    ""
   ]
  },
  "piot00_speechemotion": {
   "authors": [
    [
     "Olivier",
     "Piot"
    ]
   ],
   "title": "Attitudes and yes-no questions in standard French: Testing two hypotheses",
   "original": "spem_081",
   "page_count": 5,
   "order": 13,
   "p1": "81",
   "pn": "85",
   "abstract": [
    "Two hypotheses are tested here, one about the use of F0 to convey an attitude of ignorance, the other about the expression of motivation by means of F0, vocal intensity and speech rate. Sixteen Klatt-synthesised prosodic variations of a yes-no question « Natacha? » (In French) are used for this test. Results show that greater ignorance is expressed by higher final F0 (and not by speech rate, final /a/ initial F0 or intensity rise), greater desire to know by both higher speech rate and higher final vowel pitch register. Both hypotheses are satisfyingly verified, and this experiment allows to state them in a more precise way, so it may contribute to the theoretical study of the expression of emotions and attitudes by prosody.\n",
    ""
   ]
  },
  "trouvain00_speechemotion": {
   "authors": [
    [
     "Jürgen",
     "Trouvain"
    ],
    [
     "William J.",
     "Barry"
    ]
   ],
   "title": "The prosody of excitement in horse race commentaries",
   "original": "spem_086",
   "page_count": 6,
   "order": 14,
   "p1": "86",
   "pn": "91",
   "abstract": [
    "This study investigates examples of horse race commentaries and compares the acoustic properties with an auditorily based description of the typical suspense pattern from calm to very excited at the finish and relaxation after the finish. With the exception of tempo, the auditory impressions were basically confirmed. The examination shows further that the results of the investigated prosodic parameters pause duration, pausing and breathing rate, F0 level and range, intensity, and spectral tilt fit well with other forms of excitement such as anger or elation. Additionally, it is discussed how the specific speaking style of horse race commentators can be classified. Finally, the role of prosodic descriptions for modelling those speaking styles and emotions, especially for speech technology, is considered.\n",
    ""
   ]
  },
  "kienast00_speechemotion": {
   "authors": [
    [
     "Miriam",
     "Kienast"
    ],
    [
     "Walter F.",
     "Sendlmeier"
    ]
   ],
   "title": "Acoustical analysis of spectral and temporal changes in emotional speech",
   "original": "spem_092",
   "page_count": 6,
   "order": 15,
   "p1": "92",
   "pn": "97",
   "abstract": [
    "In the present study, the vocal expressions of the emotions anger, happiness, fear, boredom and sadness are acoustically analyzed in relation to neutral speech. The emotional speech material produced by actors is investigated especially with regard to spectral and segmental changes which are caused by different articulatory behavior accompanying emotional arousal. The findings are interpreted in relation to temporal variations.\n",
    ""
   ]
  },
  "mitchell00_speechemotion": {
   "authors": [
    [
     "Corey J.",
     "Mitchell"
    ],
    [
     "Caroline",
     "Menezes"
    ],
    [
     "J. C.",
     "Williams"
    ],
    [
     "Bryan",
     "Pardo"
    ],
    [
     "Donna",
     "Erickson"
    ],
    [
     "Osamu",
     "Fujimura"
    ]
   ],
   "title": "Changes in syllable and boundary strengths due to irritation",
   "original": "spem_098",
   "page_count": 6,
   "order": 16,
   "p1": "98",
   "pn": "103",
   "abstract": [
    "This study examines prosodic characteristics of speech in dialogue exchanges. Subjects were asked to repeat the same correction of one digit in a three-digit sequence consisting of five or nine, followed by Pine Street. Articulatory and acoustic signals were obtained on four speakers of General American English at the University of Wisconsin x-ray microbeam facility. These data are analyzed using computational algorithms based on the theoretical framework of the Converter/Distributor (C/D) model [7]. The data analysis primarily pertains to jaw movements to evaluate syllable and boundary magnitudes in varied prosodic conditions, represented in the form of a linear syllableboundary pulse train [7] that is interpreted as the rhythmic structure of an utterance. Preliminary results on syllable and boundary conditions indicate that not only does the magnitude of the corrected syllable change with repeated correction and perceived irritation, but also the magnitude and occurrence pattern of boundaries change, thus suggesting phonetic phrasal reorganization.\n",
    ""
   ]
  },
  "tickle00_speechemotion": {
   "authors": [
    [
     "Alison",
     "Tickle"
    ]
   ],
   "title": "English and Japanese speakers' emotion vocalisation and recognition: A comparisonhighlighting vowel quality",
   "original": "spem_104",
   "page_count": 6,
   "order": 17,
   "p1": "104",
   "pn": "109",
   "abstract": [
    "A major question in work on phonetic correlates of emotion is to what extent vocalisation of emotion is due to psychobiological response mechanisms and is therefore quasiuniversal and to what extent it is due to social convention. Cross-cultural research gives an angle in on this question. This paper describes the design and discusses results so far of a study in progress which attempts to shed light on this question and to address some of the very difficult methodological issues in cross-language studies of vocal correlates of emotion. The study compares the cross-cultural decoding accuracy and phonetic correlates of emotion vocalisations encoded by native English and native Japanese speakers. Nonsense utterances and quasi-universally recognised facial expressions of emotions are used. These help deal with translation, ethical problems in data collection, the trade-off between artificiality of data and consistency and the masking of verbal utterances whilst allowing any influence exerted by specific vowel qualities to be highlighted.\n",
    ""
   ]
  },
  "abelin00_speechemotion": {
   "authors": [
    [
     "Åsa",
     "Abelin"
    ],
    [
     "Jens",
     "Allwood"
    ]
   ],
   "title": "Cross linguistic interpretation of emotional prosody",
   "original": "spem_110",
   "page_count": 4,
   "order": 18,
   "p1": "110",
   "pn": "113",
   "abstract": [
    "This study has three purposes: the first is to study if there is any stability in the way we interpret different emotions and attitudes from prosodic patterns, the second is to see if this interpretation is dependent on the listeners cultural and linguistic background, and the third is to find out if there is any reoccurring relation between acoustic and semantic properties of the stimuli.\n",
    "Recordings of a Swedish speaker uttering a phrase while expressing different emotions was interpreted by listeners with different L1:s, Swedish, English, Finnish and Spanish, who were to judge the emotional contents of the expressions.\n",
    "The results show that some emotions are interpreted in accordance with intended emotion in a greater degree than the other emotions were, e.g. \"anger\", \"fear\", \"sadness\" and \"surprise\", while other emotions are interpreted as expected to a lesser degree. Furthermore emotions are interpreted with different degrees of success depending on the L1 of listeners; native listeners were the most successful. There is evidence that emotions with similar semantic features, e.g. \"anger\" and \"dominance\" or \"fear\" and \"shyness\" have similar acoustic features e.g. short duration and strong intensity (\"anger\" and \"dominance\") or longer duration and weak intensity (\"fear\" and \"shyness\").\n",
    ""
   ]
  },
  "massaro00_speechemotion": {
   "authors": [
    [
     "Dominic W.",
     "Massaro"
    ]
   ],
   "title": "Multimodal emotion perception: Analogous to speech processes",
   "original": "spem_114",
   "page_count": 8,
   "order": 19,
   "p1": "114-121 (Invited review paper)",
   "pn": "",
   "abstract": [
    "The fuzzy logical model of perception (FLMP) has been successful in accounting for a variety of unimodal and multimodal aspects of speech perception. This same framework has been extended to account for the perception of emotion from the face and the voice. The FLMP accurately describes how perceivers evaluate and integrate these sources of information to determine the affect signaled by the talker. This same research falsifies emotion processing as following a specialized analysis such as holistic or categorical perception.\n",
    ""
   ]
  },
  "auberge00_speechemotion": {
   "authors": [
    [
     "Véronique",
     "Aubergé"
    ],
    [
     "Ludovic",
     "Lemaître"
    ]
   ],
   "title": "The prosody of smile",
   "original": "spem_122",
   "page_count": 5,
   "order": 20,
   "p1": "122",
   "pn": "126",
   "abstract": [
    "The amusement expression is obviously visual with smile and laugh: in a perceptive task, subjects could perceive visual and audio-visual stimuli of amused speech with the same performances [8]. Tartter [7] demonstrated that the acoustic consequences of the (mechanical) smile gesture are perceived as amusement expression. The hypothesis developed in this work is that the expression of amusement in speech is prosodically controlled: that is, the acoustic expression of the amusement emotion is not only a quality of voice changing but a controlled processing which owns to the prosody dimension. Some speech stimuli have been produced by French speakers following different tasks (spontaneously amused speech, acted amused speech, mechanical smiling). In a previous experiment, [8] we have shown that the acoustic expression of amusement must not be reduced to an acoustic correlate of the smile: listeners can clearly discriminate the speech of a \"mechanical smile\" and the speech of a spontaneous smile. Moreover [13] a Mc Gurk paradigm applied to amused/mechanical stimuli clearly shows that prosody perturbs the visual decoding of amusement vs. mechanical (that is not emotional) speech. The stimuli have been then analyzed with a large set of parameters chosen following Tartter [7], Banse & Sherer [14] and Mozziconacci [1]. It was shown that some prosodic parameters (mainly intensity and F0 declination line) are used by speakers following different strategies.\n",
    ""
   ]
  },
  "cauldwell00_speechemotion": {
   "authors": [
    [
     "Richard T.",
     "Cauldwell"
    ]
   ],
   "title": "Where did the anger go? The role of context in interpreting emotion in speech",
   "original": "spem_127",
   "page_count": 5,
   "order": 21,
   "p1": "127",
   "pn": "131",
   "abstract": [
    "Context has greater explanatory force regarding the perception of emotions in speech than any causative association we may attempt to make between phonetic features and psychological labels. Informants, when asked to judge whether a speaker sounds angry or not, make different decisions according to whether they hear utterances in-isolation or in-context. There was a strong tendency for anger to be heard in the isolated extracts, but not in the in-context forms.\n",
    ""
   ]
  },
  "schroder00_speechemotion": {
   "authors": [
    [
     "Marc",
     "Schröder"
    ]
   ],
   "title": "Experimental study of affect bursts",
   "original": "spem_132",
   "page_count": 6,
   "order": 22,
   "p1": "132",
   "pn": "137",
   "abstract": [
    "The study described here investigates the perceived emotional content of \"affect bursts\" for German. Affect bursts are defined as short emotional non-speech expressions interrupting speech. This study shows that affect bursts, presented without context, can convey a clearly identifiable emotional meaning. Affect bursts expressing ten emotions were produced by actors. After a pre-selection procedure, \"good examples\" for each emotion were presented in a perception test. The mean recognition score of 81% indicates that affect bursts seem to be an effective means of expressing emotions. Affect bursts are grouped into classes on the basis of phonetic similarity. Recognition and confusion patterns are examined for these classes.\n",
    ""
   ]
  },
  "alter00_speechemotion": {
   "authors": [
    [
     "Kai",
     "Alter"
    ],
    [
     "Erhard",
     "Rank"
    ],
    [
     "Sonja A.",
     "Kotz"
    ],
    [
     "Ulrike",
     "Toepel"
    ],
    [
     "Mireille",
     "Besson"
    ],
    [
     "Annett",
     "Schirmer"
    ],
    [
     "Angela D.",
     "Friederici"
    ]
   ],
   "title": "Accentuation and emotions - two different systems?",
   "original": "spem_138",
   "page_count": 5,
   "order": 23,
   "p1": "138",
   "pn": "142",
   "abstract": [
    "Current investigations point to a relationship between syntax and prosody. However, prosody can also be linked to emotional markers of an utterance. We tested the later option with event-related brain potentials (ERPs) showing more electrophysiological negativity for utterances with neutral emotional states than for happiness and cold anger. In addition, the material was analyzed using an estimation of the harmonics-to-noise ratio (HNR), a measure for spectral flatness as well as the maximum prediction gain for a speech production model computed by the mutual information (MI) function. The results indicate that the HNR estimation correlates with accentuation, depending on the position of the vowel, whereas a low maximum prediction gain indicates positive or negative emotional state of the speaker in comparison to the neutral state. Comparing ERPs and the acoustic data, a relationship between the maximum prediction gain and the perception of emotions can be established.\n",
    ""
   ]
  },
  "wichmann00_speechemotion": {
   "authors": [
    [
     "Anne",
     "Wichmann"
    ]
   ],
   "title": "The attitudinal effects of prosody, and how they relate to emotion",
   "original": "spem_143",
   "page_count": 5,
   "order": 24,
   "p1": "143",
   "pn": "148",
   "abstract": [
    "The aim of this paper is to contribute to a theoretical framework for the study of affective intonation. I draw a distinction between 'attitude' and 'emotion', suggesting that only the latter is likely to be reflected directly in the speech signal, while 'attitude' is reflected indirectly, and can only be explained by a process of linguistic analysis. The term 'attitude', as applied to intonation and prosody, is a problematic one. It has been used differently in different fields, such as social psychology and linguistics, and is not made any clearer by the proliferation of 'attitudinal' labels in the intonation literature. I suggest that while there are clearly prosodic signals in speech which contribute to the impression of 'attitude', this perceived meaning should be treated as a pragmatic implicature or a pragmatic inference. This means that it can only be explained by taking into account contextual features, such as speaker-hearer relationship, and the text itself. The same intonational feature can be attitudinally neutral, or signal positive and negative attitudes depending on a complex interaction between prosody, text and context.\n",
    ""
   ]
  },
  "burkhardt00_speechemotion": {
   "authors": [
    [
     "Felix",
     "Burkhardt"
    ],
    [
     "Walter F.",
     "Sendlmeier"
    ]
   ],
   "title": "Verification of acoustical correlates of emotional speech using formant-synthesis",
   "original": "spem_151",
   "page_count": 6,
   "order": 25,
   "p1": "151",
   "pn": "156",
   "abstract": [
    "This paper explores the perceptual relevance of acoustical correlates of emotional speech by means of speech synthesis. Besides, the research aims at the development of »emotionrules « which enable an optimized speech synthesis system to generate emotional speech. Two investigations using this synthesizer are described: 1) the systematic variation of selected acoustical features to gain a preliminary impression regarding the importance of certain acoustical features for emotional expression, and 2) the specific manipulation of a stimulus spoken under emotionally neutral condition to investigate further the effect of certain features and the overall ability of the synthesizer to generate recognizable emotional expression. It is shown that this approach is indeed capable of generating emotional speech that is recognized almost as well as utterances realized by actors.\n",
    ""
   ]
  },
  "vine00_speechemotion": {
   "authors": [
    [
     "D. S. G.",
     "Vine"
    ],
    [
     "R.",
     "Sahandi"
    ]
   ],
   "title": "Synthesising emotional speech by concatenating multiple pitch recorded speech units",
   "original": "spem_157",
   "page_count": 4,
   "order": 26,
   "p1": "157",
   "pn": "160",
   "abstract": [
    "Whilst TD-PSOLA remains an adequate solution for neutral speaking styles, it is less suitable for synthesising emotions, which require more extreme pitch manipulation. With TDPSOLA, extreme pitch manipulation can introduce distortions into the synthetic speech. These distortions could be reduced by recording concatenative units at a pitch which is similar to the target intonation. A recording method called Reference Pitch Prompting has thus been devised in which a speaker records concatenative units at a set pitch, guided by a Reference Pitch Prompt (RPP), which is a monotonic, hummed note. Speech is synthesised by concatenating RPP-recorded syllables, potentially of different f0 values, and manipulating prosody using TD-PSOLA. This synthesis method, called Multiple Pitch RP-PSOLA, involves selecting concatenative units to approximate to the target f0 contour. In Multiple Pitch RPPSOLA the waveform inventory contains several versions of each syllable, each at a different pitch.\n",
    "Multiple Pitch RP-PSOLA is an extended version of Single Pitch RP-PSOLA, which uses only monotonic speech units. The Single Pitch RP-PSOLA and Multiple Pitch RP-PSOLA synthesis methods were compared in terms of perceived distortion, via a listening experiment. The stimuli were synthetic sentences based on three different emotions. Intonation contours were based on a corpus of emotionally spoken sentences. The Multiple Pitch RP-PSOLA stimuli were perceived to be slightly less distorted than Single Pitch RP-PSOLA stimuli.\n",
    ""
   ]
  },
  "iriondo00_speechemotion": {
   "authors": [
    [
     "Ignasi",
     "Iriondo"
    ],
    [
     "Roger",
     "Guaus"
    ],
    [
     "Angel",
     "Rodríguez"
    ],
    [
     "Patricia",
     "Lázaro"
    ],
    [
     "Norminanda",
     "Montoya"
    ],
    [
     "Josep M.",
     "Blanco"
    ],
    [
     "Dolors",
     "Bernadas"
    ],
    [
     "Josep",
     "Manel Oliver"
    ],
    [
     "Daniel",
     "Tena"
    ],
    [
     "Ludovico",
     "Longhi"
    ]
   ],
   "title": "Validation of an acoustical modelling of emotional expression in Spanish using speechsynthesis techniques",
   "original": "spem_161",
   "page_count": 6,
   "order": 27,
   "p1": "161",
   "pn": "166",
   "abstract": [
    "This paper describes the methodology used for validating the results obtained in a study about acoustical modelling of emotional expression in Castilian Spanish.\n",
    "We have obtained a set of rules that describes the behaviour of the most significant parameters of speech related with the emotional expression.\n",
    "The validation of the results of the study has been achieved by the use of synthetic speech that has been generated following the different rules we have obtained for each emotion.\n",
    ""
   ]
  },
  "iida00_speechemotion": {
   "authors": [
    [
     "Akemi",
     "Iida"
    ],
    [
     "Nick",
     "Campbell"
    ],
    [
     "Soichiro",
     "Iga"
    ],
    [
     "Fumito",
     "Higuchi"
    ],
    [
     "Michiaki",
     "Yasumura"
    ]
   ],
   "title": "A speech synthesis system with emotion for assisting communication",
   "original": "spem_167",
   "page_count": 6,
   "order": 28,
   "p1": "167",
   "pn": "172",
   "abstract": [
    "The authors have proposed a method of generating synthetic speech with emotion by creating three copora of emotional speech for use with CHATR [1][2], the concatenative speech synthesis system developed at ATR [3][4]. The corpora express joy, anger and sadness. For the previous trial, speech corpora were made with female voice. Having added speech corpora of male voice, the authors developed a prototype of a speech synthesis system with emotion, \"CHATAKO,\" which works on a notebook PC in Japanese language environment. The ultimate goal of this system is to assist people with communication problems. In this paper, first, design principles are stated, followed by system configuration and the evaluation on the synthetic speech with emotion generated by our method.\n",
    ""
   ]
  },
  "murray00_speechemotion": {
   "authors": [
    [
     "Iain R.",
     "Murray"
    ],
    [
     "Mike D.",
     "Edgington"
    ],
    [
     "Diane",
     "Campion"
    ],
    [
     "Justin",
     "Lynn"
    ]
   ],
   "title": "Rule-based emotion synthesis using concatenated speech",
   "original": "spem_173",
   "page_count": 5,
   "order": 29,
   "p1": "173",
   "pn": "177",
   "abstract": [
    "Concatenative speech synthesis is increasing in popularity, as it offers higher quality output than previous formant synthesisers. However, it is based on recorded speech units, concatenative synthesis offers a lesser degree of parametric control during resynthesis. Consequently, adding pragmatic effects such as different speaking styles and emotions at the synthesis stage is fundamentally more difficult than with formant synthesis.\n",
    "This paper describes the results of a preliminary attempt to add emotion to concatenative synthetic speech (using BT's Laureate synthesiser), initially using techniques already applied successfully to formant synthesis. A new intonation contour (including both pitch and duration changes) was applied to the concatenated segments during production of the final audible utterance, and some of the available synthesis parameters were systematically modified to increase the affective content. The output digital speech samples were then subject to further manipulation with a waveform editing package, to produce the final output utterance. The results of this process were a small number of manually-produced utterances, but which illustrated that affective manipulations were possible on this type of synthesiser.\n",
    "Further work has produced rule-based implementations which allow automatic production of emotional utterances. Development of these systems will be described, and some initial results from listener studies will be presented.\n",
    ""
   ]
  },
  "gobl00_speechemotion": {
   "authors": [
    [
     "Christer",
     "Gobl"
    ],
    [
     "Ailbhe",
     "Ní Chasaide"
    ]
   ],
   "title": "Testing affective correlates of voice quality through analysis and resynthesis",
   "original": "spem_178",
   "page_count": 6,
   "order": 30,
   "p1": "178",
   "pn": "183",
   "abstract": [
    "This paper outlines a strategy for exploring the paralinguistic signalling of emotion, mood and attitude. The focus here is on the role of voice quality and the approach adopted involves three distinct phases. The first involves detailed acoustic analyses of a range of voice qualities. The second involves the resynthesis and perceptual testing of individual voice qualities. The third stage, and the main focus of the present paper, involves the elicitation of the perceived affective colouring of voice qualities. A pilot experiment is described where listeners reactions to seven voice qualities were elicited in terms of eight pairs of opposing affective attributes: ratings were obtained for each pair on a seven-point scale. Results confirm that voice quality adjustments can alone evoke rather different affective colourings. Broadly, present results suggest an initial dichotomy between voice qualities that are associated with more aroused and aggressive states, and those associated with a more relaxed and unaggressive state. There is some support for traditional impressionistic observations (such as the association of creaky voice with boredom). However, results suggest some refinements on these, and suggest that there is no simple one-toone mapping between affective state and voice quality. Those affective attributes that correspond to speaker state, mood and attitude were more readily perceived for these seven voice qualities than were emotions per se.\n",
    ""
   ]
  },
  "kopecek00_speechemotion": {
   "authors": [
    [
     "Ivan",
     "Kopecek"
    ]
   ],
   "title": "Emotions and prosody in dialogues: An algebraic approach based on user modelling",
   "original": "spem_184",
   "page_count": 5,
   "order": 31,
   "p1": "184",
   "pn": "188",
   "abstract": [
    "The model of the dialogue based on dialogue automata presented in the paper relates internal states of the participant of a dialogue with the attributes of dialogue utterances by means of transition and output functions. Consequently, this model relates emotions (that are described by attributes of the internal states) with prosody (which is described by attributes of the dialogue utterances) and other quantities that are entering the model. This approach can be used for predicting the behaviour of the user as well as determining unknown attribute value in the context of dialogue.\n",
    ""
   ]
  },
  "bosch00_speechemotion": {
   "authors": [
    [
     "Louis ten",
     "Bosch"
    ]
   ],
   "title": "Emotions: What is possible in the ASR framework",
   "original": "spem_189",
   "page_count": 6,
   "order": 32,
   "p1": "189-194 (Invited review paper)",
   "pn": "",
   "abstract": [
    "This paper discusses the possibilities to extract features from the speech signal that can be used for the detection of emotional state of the speaker, using the ASR framework.\n",
    "After the introduction, a short overview of the ASR framework is presented. Next, we discuss the relation between recognition of emotion and ASR, and the different approaches found in the literature to tackle the correspondence between emotions and acoustic features. The conclusion is that emotion itself will be very difficult to predict with high accuracy, but in ASR general prosodic information is potentially powerful to improve the (word) accuracy for tasks on a limited domain.\n",
    ""
   ]
  },
  "batliner00_speechemotion": {
   "authors": [
    [
     "Anton",
     "Batliner"
    ],
    [
     "K.",
     "Fischer"
    ],
    [
     "R.",
     "Huber"
    ],
    [
     "Jörg",
     "Spilker"
    ],
    [
     "Elmar",
     "Nöth"
    ]
   ],
   "title": "Desperately seeking emotions or: Actors, wizards, and human beings",
   "original": "spem_195",
   "page_count": 6,
   "order": 33,
   "p1": "195",
   "pn": "200",
   "abstract": [
    "Automatic dialogue systems used in call-centers, for instance, should be able to determine in a critical phase of the dialogue - indicated by the costumers vocal expression of anger/irritation - when it is better to pass over to a human operator. At a first glance, this seems not to be a com- plicated task: It is reported in the literature that emotions can be told apart quite reliably on the basis of prosodic features. However, these results are most of the time achieved in a laboratory setting, with experienced speakers (actors), and with elicited, controlled speech. We report classification results obtained within different experimental settings for the two-class-problem \"&neutral vs. anger\"& using a vector of prosodic features and discuss the impact of single features on the classification rate. Recognition rates for these settings are best for a speaker-specific classifier (one experienced speaker, acting), worse for a speaker-independent classifier (several less experienced speakers, reading), and even worse for a speaker-independent classifier with naive subjects performing the task of appointment scheduling in a Wizard-of-Oz-scenario where a malfunctioning system is simulated in order to evoke anger. The first situation mirrors most of the settings reported in the literature, the third is closest to the \"real-life\"-task. It thus turns out that prosody alone is not reliable as an indicator of the speakers emotional state the closer we get to a realistic scenario. As a consequence, the prosodic classifier was combined with other knowledge sources in the module Monitoring Of User State [especially of] Emotion (MoUSE).\n",
    ""
   ]
  },
  "polzin00_speechemotion": {
   "authors": [
    [
     "Thomas S.",
     "Polzin"
    ],
    [
     "Alexander",
     "Waibel"
    ]
   ],
   "title": "Emotion-sensitive human-computer interfaces",
   "original": "spem_201",
   "page_count": 6,
   "order": 34,
   "p1": "201",
   "pn": "206",
   "abstract": [
    "People are polite to their computers. They are flattered by them, form teams with them and even interact emotionally with them. In their experiments, Reeves and Nass (The Media Equation, 1996) showed that humans impose their interpersonal behavioral patterns onto their computers. Thus, the design of humancomputer interfaces should reflect this observation in order to facilitate an effective communication.\n",
    "In order to build a human-computer interface that is sensitive to the user's expressed emotion, we investigated spectral, prosodic, and verbal cues in the user's utterance. Based on these cues, we showed that the classification system achieved accuracies comparable to human performance.\n",
    "Finally, we demonstrate how to integrate information about the expressed emotion into a dialog system. The dialog system employs different discourse strategies depending on the expressed emotion allowing for a natural and effective communication between the user and the system.\n",
    ""
   ]
  },
  "mcgilloway00_speechemotion": {
   "authors": [
    [
     "Sinéad",
     "McGilloway"
    ],
    [
     "Roddy",
     "Cowie"
    ],
    [
     "Ellen",
     "Douglas-Cowie"
    ],
    [
     "Stan",
     "Gielen"
    ],
    [
     "Machiel",
     "Westerdijk"
    ],
    [
     "Sybert",
     "Stroeve"
    ]
   ],
   "title": "Approaching automatic recognition of emotion from voice: A rough benchmark",
   "original": "spem_207",
   "page_count": 6,
   "order": 35,
   "p1": "207",
   "pn": "212",
   "abstract": [
    "Automatic recognition of a speakers emotions is a natural objective for research, but is difficult to gauge the level of performance that is currently attainable. We describe a study that offers a rough benchmark. Speech data came from five passages of about 100 syllables each. They had been selected following pilot studies because they were effective at evoking specific emotion - fear, anger, happiness, sadness, and neutrality. 40 subjects were recorded reading them. A battery of 32 potentially relevant features was extracted using our ASSESS system. They were broadly speaking prosodic, derived from contours tracing the movement of intensity and pitch. They were input to statistical decision mechanisms, of two types. Discriminant analysis uses linear combinations of variables to separate samples that belong to different categories. There are reasons to suspect that linear combination will not be appropriate, so neural net classifiers were also considered. An automatic relevance determination procedure was used to identify the most relevant parameters.\n",
    "Discriminant analysis outperformed the neural networks. Using 90% of the data for training, and testing on the remaining 10%, a classification rate of 55 % (+/- 0.08%) was achieved. The most useful predictors covered a variety of properties - intensity (relative to the start of the passage) and its spread; pitch spread; durations of silences, rises in intensity, and syllables; and a property related to the shape of tunes, the number of inflections in the F0 contour per tune. Many more variables were less important, but nevertheless contributed.\n",
    ""
   ]
  },
  "klasmeyer00b_speechemotion": {
   "authors": [
    [
     "Gudrun",
     "Klasmeyer"
    ],
    [
     "Tom",
     "Johnstone"
    ],
    [
     "Tanja",
     "Bänziger"
    ],
    [
     "Christopher",
     "Sappok"
    ],
    [
     "Klaus R.",
     "Scherer"
    ]
   ],
   "title": "Emotional voice variability in speaker verification",
   "original": "spem_213",
   "page_count": 6,
   "order": 36,
   "p1": "213",
   "pn": "218",
   "abstract": [
    "During the last years a growing interest in automatic speaker verification (ASV) systems developed. Recent ASV systems still produce two kinds of errors to a considerable extent: false acceptances and false rejections of speakers. The systems generally react extremely sensitive to intra-speaker variability of voice and speaking style, caused for example by different psychological states of the speaker. To improve the performance of ASV systems, different attempts can be made to modify the underlying mathematical or statistical models. Furthermore, most of the systems still use a set of spectral parameters as originally developed for speech recognition systems. The performance of ASV systems could be improved by selecting a set of acoustic parameters which show both, minimal intraspeaker variation and maximal inter-speaker variation.\n",
    ""
   ]
  },
  "fernandez00_speechemotion": {
   "authors": [
    [
     "Raul",
     "Fernandez"
    ],
    [
     "Rosalind W.",
     "Picard"
    ]
   ],
   "title": "Modeling drivers speech under stress",
   "original": "spem_219",
   "page_count": 6,
   "order": 37,
   "p1": "219",
   "pn": "224",
   "abstract": [
    "In this paper we explore the use of features derived from multiresolution analysis of speech and the Teager Energy Operator for classification of drivers' speech under stressed conditions. We apply this set of features to a database of short speech utterances to create user-dependent discriminants of four stress categories. In addition we address the problem of choosing a suitable tem- poral scale for representing categorical differences of the data. This leads to two sets of modeling techniques. In the first approach, we model the dynamics of the feature set within the utterance with a family of dynamic classifiers. In the second approach, we model the mean value of the features across the utterance with a family of static classifiers. We report and compare classification performances on the sparser and full dynamic representations for a set of four subjects.\n",
    ""
   ]
  }
 },
 "sessions": [
  {
   "title": "1. Theory, Sources, Tools",
   "papers": [
    "cornelius00_speechemotion",
    "cowie00_speechemotion",
    "cowie00b_speechemotion",
    "pereira00_speechemotion",
    "amir00_speechemotion",
    "campbell00_speechemotion",
    "douglascowie00_speechemotion",
    "mozziconacci00_speechemotion",
    "roach00_speechemotion",
    "stibbard00_speechemotion",
    "klasmeyer00_speechemotion"
   ]
  },
  {
   "title": "2. Description of Emotion in Speech",
   "papers": [
    "paeschke00_speechemotion",
    "piot00_speechemotion",
    "trouvain00_speechemotion",
    "kienast00_speechemotion",
    "mitchell00_speechemotion",
    "tickle00_speechemotion",
    "abelin00_speechemotion",
    "massaro00_speechemotion",
    "auberge00_speechemotion",
    "cauldwell00_speechemotion",
    "schroder00_speechemotion",
    "alter00_speechemotion",
    "wichmann00_speechemotion"
   ]
  },
  {
   "title": "3. Machine Application: Speech Synthesis and Recognition",
   "papers": [
    "burkhardt00_speechemotion",
    "vine00_speechemotion",
    "iriondo00_speechemotion",
    "iida00_speechemotion",
    "murray00_speechemotion",
    "gobl00_speechemotion",
    "kopecek00_speechemotion",
    "bosch00_speechemotion",
    "batliner00_speechemotion",
    "polzin00_speechemotion",
    "mcgilloway00_speechemotion",
    "klasmeyer00b_speechemotion",
    "fernandez00_speechemotion"
   ]
  }
 ]
}