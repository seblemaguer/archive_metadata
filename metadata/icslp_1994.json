{
 "title": "3rd International Conference on Spoken Language Processing (ICSLP 1994)",
 "location": "Yokohama, Japan",
 "startDate": "18/9/1994",
 "endDate": "22/9/1994",
 "conf": "ICSLP",
 "year": "1994",
 "name": "icslp_1994",
 "series": "ICSLP",
 "SIG": "",
 "title1": "3rd International Conference on Spoken Language Processing",
 "title2": "(ICSLP 1994)",
 "date": "18-22 September 1994",
 "papers": {
  "lehiste94_icslp": {
   "authors": [
    [
     "Ilse",
     "Lehiste"
    ]
   ],
   "title": "Poetic metre, prominence, and the perception of prosody: a case of intersection of art and science of spoken language",
   "original": "i94_2237",
   "page_count": 7,
   "order": 1,
   "p1": "2237",
   "pn": "2244",
   "abstract": [
    "The paper explores ways in which the prosody of a language can be investigated by studying the prosody of poetry created in that language. Certain poetic metres (such as the trochaeic or dactylic metre) are perceived as being 'the same' in essential ways, regardless of the language in the poetry of which these metres are employed. Acoustic analysis of realizations of these metres, produced orally by native speakers of the different languages, reveals distinctive differences that in turn shed light on the prosodic structure of the languages themselves. Languages reported on in this paper include Japanese, Finnish, Faroese, Estonian, Latvian, Lithuanian, and Swedish.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-1"
  },
  "hiki94_icslp": {
   "authors": [
    [
     "Shizuo",
     "Hiki"
    ]
   ],
   "title": "Possibilities of compensating for defects in speech perception and production",
   "original": "i94_2245",
   "page_count": 8,
   "order": 2,
   "p1": "2245",
   "pn": "2252",
   "abstract": [
    "In order to investigate possibilities of compensating for defects in both speech perception and production processes, functions involved in possible modes of linguistic information processing for speech and letters are modeled, taking into account the structural parallel between steps involved in the processing of speech and letters. Interconnections between perception and production processes, and between phonological and orthographic components are also provided. First, using this model, various aspects of compensation found in normal processes are discussed, using examples of speech production such as lip spreading or rounding for vowels, and lingual contact with the palate for vowels and consonants. Then, the possibility of the use of substitutive sensation is overviewed, considering enhancement of ordinary functions, perception of speech through other sensations, speech supplements through other sensations, speech-to-letter conversion, and perception of letters for speech. The use of substitutive organs is overviewed, considering substitutive aids for ordinary organs, production through other organs, and production of letters for speech. Finally, to promote the combined use of auditory, visual and tactile channels of sensation in speech perception, complementary effects between the sensation channels are discussed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-2"
  },
  "levelt94_icslp": {
   "authors": [
    [
     "Willem J. M.",
     "Levelt"
    ]
   ],
   "title": "On the skill of speaking: how do we access words?",
   "original": "i94_2253",
   "page_count": 6,
   "order": 3,
   "p1": "2253",
   "pn": "2258",
   "abstract": [
    "Central to the skill of speaking is our ability to select words that appropriately express our intentions, to retrieve their syntactic and phonological properties and to compute the ultimate articulatory shape of these words in the context of the utterance as a whole. The generation of words in speech involves a number of processing stages. This paper discusses a stage of conceptual preparation, which leads to the activation of a lexical concept; a stage of lexical selection, which leads to the retrieval of an appropriate word or lemma from the mental lexicon; a stage of phonological encoding in which phonological words, consisting of phonological syllables are created; a stage of phonetic encoding, which produces a string of syllabic gestural scores, which can eventually be executed during the final stage of articulation.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-3"
  },
  "takezawa94_icslp": {
   "authors": [
    [
     "Toshiyuki",
     "Takezawa"
    ],
    [
     "Tsuyoshi",
     "Morimoto"
    ]
   ],
   "title": "An efficient predictive LR parser using pause information for continuously spoken sentence recognition",
   "original": "i94_0001",
   "page_count": 4,
   "order": 4,
   "p1": "1",
   "pn": "4",
   "abstract": [
    "This study aims to examine how to combine reliable bottom-up information such as pauses in a top-down approach. As the fir&t step, we select pause information as reliable bottom-up information and study an efficient recognition method that accepts syntactically well-formed spoken sentences that contain natural pauses at free positions. In order to reduce the computational costs that increase exponentially in the recognition process, we examine two ways to use pause information. One way is to merge/pack candidates of the same syntactic categories when detecting pauses in the input speech. The other way is to reduce the phone verification range by using the pause information. We have conducted preliminary experiments using a conference registration task (Japanese speech). The results suggest that this method becomes effective more as the input speech becomes longer.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-4"
  },
  "kim94_icslp": {
   "authors": [
    [
     "Kyunghee",
     "Kim"
    ],
    [
     "Geunbae",
     "Lee"
    ],
    [
     "Jong-Hyeok",
     "Lee"
    ],
    [
     "Hong",
     "Jeong"
    ]
   ],
   "title": "Integrating TDNN-based diphone recognition with table-driven morphology parsing for understanding of spoken Korean",
   "original": "i94_0005",
   "page_count": 4,
   "order": 5,
   "p1": "5",
   "pn": "8",
   "abstract": [
    "In this paper, we propose a spoken Korean morphological analysis model extensible to large vocabulary continuous speech recognition. This model consists of a diphone recognizer, a diphone2phoneme filter and a CYK-morphological analyzer. Two-level hierarchical TDNNs (time-delayed neural networks) recognize Korean diphones which are transformed into a phoneme lattice (a set of phoneme candidates hypothesized by a speech recognition module) by a diphone2phoneme filter. The morphological analyzer parses the phoneme lattice with the phonological changes handling and produces the morphology-segmented Korean words (called Eojeols). Using the TDNN diphone speech recognizer, we obtained 95.2% of 17 Korean vowel recognition and 93.7% of 72 diphone recognition. The speaker-dependent and continuous Eojeol recognition experiments using the current model show that the morphological analysis for spoken Korean can be achieved for medium sized vocabularies with 90.6% of success rate.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-5"
  },
  "wallerstein94_icslp": {
   "authors": [
    [
     "Frank O.",
     "Wallerstein"
    ],
    [
     "Akio",
     "Amano"
    ],
    [
     "Nobuo",
     "Hataoka"
    ]
   ],
   "title": "Implementation issues and parsing speed evaluation of HMM-LR parser",
   "original": "i94_0009",
   "page_count": 4,
   "order": 6,
   "p1": "9",
   "pn": "12",
   "abstract": [
    "This paper describes implementation issues of the HMM-LR parser and preliminary evaluation results for parsing speed on a continuous HMM recognition system. The HMM-LR parser is based on an LR parsing technique which can use context-free grammars, and thought as one of possible parsing techniques which would realise real time processing. However, the current problem is that the total recognition system still needs much memory space and much processing time. We have implemented the HMM-LR parser to expand our existing speech recognition system to a sentence understanding system. In this paper, we present software implementation results and propose a couple of new techniques for reducing parsing time and memory space. The new ideas proposed are based on the encoding scheme of the LR table entries, table compression methods, and a novel beam search control. We have found that fast parsing speed has been realised for a textual input consisting of 1000 sentences. However, preliminary evaluations on the total system suggested the necessity of more intensive improvement on the parsing algorithm and implementation.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-6"
  },
  "kita94_icslp": {
   "authors": [
    [
     "Kenji",
     "Kita"
    ],
    [
     "Yoneo",
     "Yano"
    ],
    [
     "Tsuyoshi",
     "Morimoto"
    ]
   ],
   "title": "One-pass continuous speech recognition directed by generalized LR parsing",
   "original": "i94_0013",
   "page_count": 4,
   "order": 7,
   "p1": "13",
   "pn": "16",
   "abstract": [
    "In this paper, we will formulate a novel continuous speech recognition algorithm that integrates (1) a generalized LR parser for handling context-free grammar (CFG) constraints, and (2) the one-pass search algorithm for achieving an efficient search. Our algorithm enables us to efficiently find an optimal hypothesis for a given speech signal according to a specified CFG in a frame-synchronous process. Although some CFG-based speech recognition algorithms have been proposed, our algorithm is very efficient because of the introduction of three novel techniques: (1) LR path merging, (2) shared tree-structured stack, and (3) LR-parser-based dynamic network generation. We conducted an experimental comparison between our algorithm and the following two methods: (1) the one-pass search algorithm using the finite-state approximation for a CFG, and (2) the HMM-LR algorithm. Our experiments showed that our algorithm outperformed the other two algorithms.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-7"
  },
  "plannerer94_icslp": {
   "authors": [
    [
     "Bernd",
     "Plannerer"
    ],
    [
     "Tobias",
     "Einsele"
    ],
    [
     "Martin",
     "Beham"
    ],
    [
     "Günther",
     "Ruske"
    ]
   ],
   "title": "A continuous speech recognition system integrating additional acoustic knowledge sources in a data-driven beam search algorithm",
   "original": "i94_0017",
   "page_count": 4,
   "order": 8,
   "p1": "17",
   "pn": "20",
   "abstract": [
    "The paper presents a continuous speech recognition system which integrates an additional acoustic knowledge source into the data-driven beam search algorithm. Details of the object oriented implementation of the beam search algorithm will be given. Integration of additional knowledge sources is treated within the flexible framework of Dempster-Shafer theory. As a first example, a rule-based plosive detector is added to the baseline system.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-8"
  },
  "brown94_icslp": {
   "authors": [
    [
     "Michael K.",
     "Brown"
    ],
    [
     "Bruce M.",
     "Buntschuh"
    ]
   ],
   "title": "A context-free grammar compiler for speech understanding systems",
   "original": "i94_0021",
   "page_count": 4,
   "order": 9,
   "p1": "21",
   "pn": "24",
   "abstract": [
    "This paper describes a new syntactic/semantic grammar specification language and optimizing compiler that allows the user to write a single compact context-free grammar specification that unambiguously describes both the syntax and semantics for a given task. The compiler produces all of the necessary language processing components needed to efficiently couple speech to actions. The optimizations performed are specifically designed to significantly increase the efficiency of hypothesis scoring in recognizers. They include FSA determinization, minimization, transition consolidation and elimination of redundant epsilon-transitions as well as a transformation that eliminates left-recursive rules. While the primary target application is speech recognition, these tools are also being used for building stochastic language models for other applications like handwriting recognition. The grammars are written in a high-level language called GSL (Grammar Specification Language). The compiler generates both an optimized recursive transition network for syntactic constraints and a semantic processor that performs lexical semantic analysis. When driven by the results of the recognizer, the semantic processor dynamically constructs an evaluation tree that describes how semantic actions are to be performed. With the addition of a user defined library of semantic action functions or a simple interface to an existing application (e.g., DBMS), a complete limited-domain speech understanding system can quickly be constructed. Since the syntax and semantics are specified by the same language, any changes to the specification are consistently applied to the entire system.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-9"
  },
  "nagao94_icslp": {
   "authors": [
    [
     "Katashi",
     "Nagao"
    ],
    [
     "Koiti",
     "Hasida"
    ],
    [
     "Takashi",
     "Miyata"
    ]
   ],
   "title": "Probabilistic constraint for integrated speech and language processing",
   "original": "i94_0025",
   "page_count": 4,
   "order": 10,
   "p1": "25",
   "pn": "28",
   "abstract": [
    "A totally constraint-based computational architecture is applied to integration of speech and natural language processing. A major research issue in designing information processing systems based on constraint (level of description abstracting away from information flow) is how to guarantee global adequacy of computation. A probabilistic semantics of Horn clause programs is introduced, which is a generalization of hidden Markov models, stochastic context-free grammars, etc., and a computational method for maximum likelihood estimation is proposed. This computation deals efficiently with probabilistically dependent events, and is regarded as a sort of A* search in a general sense. Furthermore, this computational architecture supports omnidirectional information flow among heterogeneous knowledge sources, from acoustics to pragmatics, and naturally resolves problems in spoken language understanding with-out any domain/task dependent prescription of information flow.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-10"
  },
  "edmondson94_icslp": {
   "authors": [
    [
     "William",
     "Edmondson"
    ],
    [
     "Jon",
     "Iles"
    ]
   ],
   "title": "A non-linear architecture for speech and natural language processing",
   "original": "i94_0029",
   "page_count": 4,
   "order": 11,
   "p1": "29",
   "pn": "32",
   "abstract": [
    "In this paper we present a suitable architecture for the unification of many aspects of natural language processing research. The architecture - called Pantome - has been developed from non-linear phonology, but applies more generally to other linguistic activity. The approach taken is cognitively motivated, inherently parallel, and requires revision of the notion of segment. We outline how we can demonstrate the validity of our claims through use of the architecture in a text-to-speech conversion application. Related papers [9, 16] present other aspects of the implementation of the architecture in this speech and language processing application.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-11"
  },
  "erickson94_icslp": {
   "authors": [
    [
     "Donna",
     "Erickson"
    ],
    [
     "Kevin",
     "Lenzo"
    ],
    [
     "Masashi",
     "Sawada"
    ]
   ],
   "title": "Manifestations of contrastive emphasis in jaw movement in dialogue",
   "original": "i94_0033",
   "page_count": 3,
   "order": 12,
   "p1": "33",
   "pn": "36",
   "abstract": [
    "Using an automatic tool for measuring and segmenting movement segments of x-ray microbeam pellet tracings, we measured various dimensions of jaw movement in the vertical direction during productions of contrastive emphasis in elicited dialogue situations. We find that the measures maximum vertical displacement and total area seem to correlate highly with contrastive emphasis, and are sufficient for differentiating between syllables spoken with contrastive emphasis and those spoken without contrastive emphasis. Moreover, these measures increase as the speaker is asked to repeat the same correction two to five times.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-12"
  },
  "lee94_icslp": {
   "authors": [
    [
     "Sook-hyang",
     "Lee"
    ],
    [
     "Mary E.",
     "Beckman"
    ],
    [
     "Michel",
     "Jackson"
    ]
   ],
   "title": "Jaw targets for strident fricatives",
   "original": "i94_0037",
   "page_count": 4,
   "order": 13,
   "p1": "37",
   "pn": "40",
   "abstract": [
    "As in more traditional place-manner models of consonant production, the jaw is not a primary articulator in Browman and Goldstein's Articulatory Phonology, although it does differentiate Arabic \"guttural\" consonants from the \"oral\" consonants more commonly seen in the languages of the world. The jaw's role in consonant place of articulation was examined by measuring jaw height in many tokens of VCV sequences with consonants at all places of articulation that contrast in Arabic and French. Results supported the generally secondary role of the jaw in tongue and lip raising for oral consonants. However, the jaw's behavior in strident fricatives did not fit the general picture, and suggests that Articulatory Phonology should be enriched to allow for jaw height to be specified directly in an aerodynamically crucial task for the feature \"strident\".\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-13"
  },
  "ostry94_icslp": {
   "authors": [
    [
     "David J.",
     "Ostry"
    ],
    [
     "Eric",
     "Vatikiotis-Bateson"
    ]
   ],
   "title": "Jaw motions in speech are controlled in (at least) three degrees of freedom",
   "original": "i94_0041",
   "page_count": 4,
   "order": 14,
   "p1": "41",
   "pn": "44",
   "abstract": [
    "In vocal tract models, jaw motions are almost always characterized in terms of sagittal plane rotations about a fixed axis. However, in fact, jaw motion in speech consists of rotation in the midsagittal plane and horizontal and vertical translation. Small amplitude lateral rotations (yaw) are also observed. In this paper, we assess these jaw motion components in speech by examining their patterns of kinematic variation and we contrast these behaviours with the patterns observed in chewing. We also assess the relationship between sagittal plane jaw motions and lateral motions of the jaw. When jaw movements in speech were plotted to show sagittal plane rotation as a function of horizontal translation we observed straight line paths in which the component rotations and translations could vary independently. The vertical elevation of the jaw could also vary, independent of the actual motion path of the jaw. The occurrence of lateral jaw motions in mastication suggests this degree of freedom may be controlled as well.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-14"
  },
  "tiede94_icslp": {
   "authors": [
    [
     "Mark K.",
     "Tiede"
    ],
    [
     "Eric",
     "Vatikiotis-Bateson"
    ]
   ],
   "title": "Extracting articulator movement parameters from a videodisc-based cineradiographic database",
   "original": "i94_0045",
   "page_count": 4,
   "order": 15,
   "p1": "45",
   "pn": "48",
   "abstract": [
    "The recent transfer of a selection of cineradiographic movies of the vocal tract to the laserdisc format has now made the application of standard video processing techniques to such data feasible on inexpensive microcomputers. This research explores some of the possibilities inherent in this new look at old data: we have developed software capable of automatically reducing noise, detecting air-tissue boundaries, and tracking articulator displacements (e.g. jaw, lips, tongue) through successive frames. Our goal is to characterize tongue dynamics in terms of changes over time to a normalized parameterization of the tract surface profile. This information will complement other modes for investigating speech production for which either dynamics or surface profiles are unavailable (MRI, magnetometer).\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-15"
  },
  "stone94_icslp": {
   "authors": [
    [
     "Maureen",
     "Stone"
    ],
    [
     "Andrew",
     "Lundberg"
    ]
   ],
   "title": "Tongue-palate interactions in consonants vs. vowels",
   "original": "i94_0049",
   "page_count": 4,
   "order": 16,
   "p1": "49",
   "pn": "52",
   "abstract": [
    "Consonants and vowels have often been described as two parallel systems of production. To make vowels, the articulators produce slow or tonic movements. To make consonants, the articulators use fast or phasic movements. This paper examined differences in 3D tongue surface shapes for consonants vs. vowels. 3D tongue surface shapes were reconstructed from 20 cross-sectional slices of the tongue surface, each 3 ° apart in a radial array. Steady state consonants and vowels were measured. The data supported the theory that for vowels, the tongue behaves like a muscular hydrostat. Local expansion and compression patterns follow tongue muscle morphology, and tongue shape is predictable from tongue location. However, for consonants, simple volume preserving deformations do not adequately explain the shape changes. Location, shape and pressure of tongue-palate contact must be known in order to adequately represent tongue shape for consonants.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-16"
  },
  "hoole94_icslp": {
   "authors": [
    [
     "Philip",
     "Hoole"
    ],
    [
     "Christine",
     "Mooshammer"
    ],
    [
     "Hans G.",
     "Tillmann"
    ]
   ],
   "title": "Kinematic analysis of vowel production in German",
   "original": "i94_0053",
   "page_count": 4,
   "order": 17,
   "p1": "53",
   "pn": "56",
   "abstract": [
    "The tense and lax vowels of German were compared, based on an analysis of the duration, amplitude and velocity characteristics of lip and tongue movement. This study examined firstly whether they show different patterns of compression over changes in speech rate, and secondly whether velocity profiles would reveal evidence of different underlying control mechanisms. CVC movements were segmented into CV, nucleus and VC portions. Speech rate affected duration of CV and VC movements similarly for tense and lax vowels. However, the effect on nucleus duration was vastly greater for the tense vowels. Analysis of the velocity profiles of CV and VC movements in terms of the ratio of peak to average velocity showed no differences between tense and lax vowels, once differences in duration were taken into account. The conclusion is that tense and lax vowels share similar control mechanisms for the elementary CV and VC movements, but differ radically in the way these elements are concatenated.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-17"
  },
  "hawkins94_icslp": {
   "authors": [
    [
     "Sarah",
     "Hawkins"
    ],
    [
     "Andrew",
     "Slater"
    ]
   ],
   "title": "Spread of CV and v-to-v coarticulation in british English: implications for the intelligibility of synthetic speech",
   "original": "i94_0057",
   "page_count": 4,
   "order": 18,
   "p1": "57",
   "pn": "60",
   "abstract": [
    "Formant-based synthetic speech is less robust in noise than natural speech and is often criticised as \"robot-like\". One reason may be the failure to model systematic spectral variation that is not crucial to phoneme identification. This study investigates how some aspects of consonantal context and stress contribute to this variation. CV sequences were embedded in carrier phrases to give quasi-meaningful English sentences. Vowels were /@ u i @/ or /V u i O/; /z/ or /r/ followed /u/; other consonants were either all Pol or all /d/. Stress was on the second syllable (Set 1), or the first and third (Set 2). F2 and F3 frequencies were lower when consonants were /r/ and /b/ rather than /z/ and /d/, but \"r-lowering\" was not significant in Set 2, presumably because vowel quality, and hence tongue position, were more constrained by stress. R-lowering may spread to syllables which are not adjacent to hi, typically across unstressed vowels and labial consonants. The measured differences were usually audible. Modelled in synthetic speech, they can increase the phonemic intelligibility of the speech in noise by about 15%.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-18"
  },
  "kondo94_icslp": {
   "authors": [
    [
     "Mariko",
     "Kondo"
    ]
   ],
   "title": "Mechanisms of vowel devoicing in Japanese",
   "original": "i94_0061",
   "page_count": 4,
   "order": 19,
   "p1": "61",
   "pn": "64",
   "abstract": [
    "The intensity measurements of vowels in devoicing environments suggest that high vowel devoicing in Japanese is a natural vowel reduction process resulting from glottal gestural overlap. Earlier work showed that high vowel devoicing appears almost compulsory at the normal speaking rate so long as there is no devoiceable vowel in an adjacent mora. High vowels can sometimes remain voiced in devoicing environments, but their intensity is smaller than that of ordinary vowels. However, in the consecutive devoicing environments, voiced vowels in devoicing environments are not weakened. Two different mechanisms seem to control vowel devoicing depending on the environments.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-19"
  },
  "jusczyk94_icslp": {
   "authors": [
    [
     "Peter W.",
     "Jusczyk"
    ]
   ],
   "title": "The development of word recognition",
   "original": "i94_0065",
   "page_count": 6,
   "order": 20,
   "p1": "65",
   "pn": "70",
   "abstract": [
    "The Word Recognition And Phonetic Structure Acquistion Model (WRAPSA) was proposed several years ago as an account of how infant speech perception capacities evolve into ones that support word recognition in fluent speech. The model was based largely on evidence concerning infants' perception of speech contrasts between isolated syllables. The present paper reviews more recent evidence from investigations concerning the development of word recognition during the first year of life. The more recent findings are compatible with the main assumptions of the model, however, the data suggest that word recognition processes may begin at an earlier age than had been expected.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-20"
  },
  "norris94_icslp": {
   "authors": [
    [
     "Dennis",
     "Norris"
    ],
    [
     "James M.",
     "McQueen"
    ],
    [
     "Anne",
     "Cutler"
    ]
   ],
   "title": "Competition and segmentation in spoken word recognition",
   "original": "i94_0071",
   "page_count": 4,
   "order": 21,
   "p1": "71",
   "pn": "74",
   "abstract": [
    "This paper describes recent experimental evidence which shows that models of spoken word recognition must incorporate both inhibition between competing lexical candidates and a sensitivity to metrical cues to lexical segmentation. A new version of the Shortlist [1][2] model incorporating the Metrical Segmentation Strategy [3] provides a detailed simulation of the data.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-21"
  },
  "kuhn94_icslp": {
   "authors": [
    [
     "Roland",
     "Kuhn"
    ],
    [
     "Renato De",
     "Mori"
    ]
   ],
   "title": "Recent results in automatic learning rules for semantic interpretation",
   "original": "i94_0075",
   "page_count": 4,
   "order": 22,
   "p1": "75",
   "pn": "78",
   "abstract": [
    "This article describes a new method for building a natural language understanding (NLU) system, in which the system's rules are learnt automatically from training data. The method has been applied to design of a speech understanding (SU) system. Designers of such systems rely increasingly on robust matchers to perform the task of extracting meaning from one or several word sequence hypotheses generated by a speech recognizer; a robust matcher processes semantically important islands of words and constituents rather than attempting to parse the entire word sequence.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-22"
  },
  "gorin94_icslp": {
   "authors": [
    [
     "Allen L.",
     "Gorin"
    ]
   ],
   "title": "Semantic associations, acoustic metrics and adaptive language acquisition",
   "original": "i94_0079",
   "page_count": 4,
   "order": 23,
   "p1": "79",
   "pn": "82",
   "abstract": [
    "We investigate devices which learn to understand and act upon spoken input. The ultimate goal of these speech understanding systems is to extract meaning from the speech signal. A crucial issue in engineering such devices is to quantify the information content of spoken natural language, and to be able to measure a machine's success in extracting that information. This paper discusses some recent progress in this direction.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-23"
  },
  "ward94_icslp": {
   "authors": [
    [
     "Wayne",
     "Ward"
    ]
   ],
   "title": "Extracting information in spontaneous speech",
   "original": "i94_0083",
   "page_count": 4,
   "order": 24,
   "p1": "83",
   "pn": "86",
   "abstract": [
    "This paper describes the processing strategy used by the Phoenix Spoken Language System, developed at Carnegie Mellon University. This system is designed to understand and respond to spontaneous speech for a specific domain. Considering the noisy and ill-formed nature of spontaneous speech and the uncertainty of the speech decoding process, robustness is a primary concern. Our goal is to respond appropriately to input, even though coverage is not complete. The natural language component of our system is oriented toward the extraction of information relevant to a task, and seeks to directly optimize the correctness of the extracted information, and therefore the system response. A flexible parser is used to map input onto semantic frames. Slots in frames represent the basic semantic entities known to the system. Frames associate sets of slots, representing specific items of information, with actions to be taken by the system. We have implemented a version of this system for the Air Travel Information Service (ATIS) task, and present evaluation results. This paper will also present results from experiments on integrating the understanding process i with the speech decoding process. We have used the semantic networks of the parser in conjunction with a bigram as a modified language model for the decoder.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-24"
  },
  "kameyama94_icslp": {
   "authors": [
    [
     "Megumi",
     "Kameyama"
    ],
    [
     "Isao",
     "Arima"
    ]
   ],
   "title": "Coping with aboutness complexity in information extraction from spoken dialogues",
   "original": "i94_0087",
   "page_count": 4,
   "order": 25,
   "p1": "87",
   "pn": "90",
   "abstract": [
    "We report on the strategies in automatic summarization of spontaneous spoken dialogues. Our dialogue summarization system, MIMI, recognizes key linguisitc patterns and merges information to construct a summary of conference room scheduling dialogues. Dialogues about single reservations can be accurately summarized with a simple merging scheme, but we need several extensions and changes to cope with the aboutncss complexity in unrestricted dialogues - knowing exactly how many reservations are being discussed and which reservation values must be updated by subsequent utterances. A side effect of these extensions is a new problem of overrecognition caused by spoken language disfluencies. Keywords: Spontaneous Dialogues, Summarization, Information Extraction\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-25"
  },
  "shirotsuka94_icslp": {
   "authors": [
    [
     "Otoya",
     "Shirotsuka"
    ],
    [
     "Ken'Ya",
     "Murakami"
    ]
   ],
   "title": "An example-based approach to semantic information extraction from Japanese spontaneous speech",
   "original": "i94_0091",
   "page_count": 4,
   "order": 26,
   "p1": "91",
   "pn": "94",
   "abstract": [
    "Dealing with the linguistic phenomena of spontaneous speech by the existing rule-based approach requires the preparation of complex analysis rules, which takes a great deal of effort. This paper describes a new method of extracting semantic information extraction from Japanese spontaneous speech by an example-based approach (EBA). Compared to the rule-based approach, EBA is robust and requires little effort for knowledge acquisition and its formation. In experimental evaluations of a semantic information extractor based on EBA, transcriptions of one hundred spontaneous dialogues are used as an example corpus and a testing corpus. The best performances of the extractor are 81.6% for precision rate and 62.2% for coverage rate in semantic feature extraction. The results suggest that our method is robust against unknown words and ill-formed sentences, and the extractor proved that EBA can be used as an effective tool for extracting semantic information from spontaneous speech.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-26"
  },
  "nagai94_icslp": {
   "authors": [
    [
     "Akito",
     "Nagai"
    ],
    [
     "Yasushi",
     "Ishikawa"
    ],
    [
     "Kunio",
     "Nakajima"
    ]
   ],
   "title": "A semantic interpretation based on detecting concepts for spontaneous speech understanding",
   "original": "i94_0095",
   "page_count": 4,
   "order": 27,
   "p1": "95",
   "pn": "98",
   "abstract": [
    "This paper describes a two-stage semantic interpretation method where an intention is associated with an input sentence and it integrates concept hypotheses detected from the sentence. In this approach, a concept represented by several phrases is an unit of semantic interpretation, and a spontaneously spoken sentence is regarded as a sequence of concepts. Syntactic constraint is limited within a phrase to robustly understand various sentential expressions. For disambiguation of concept hypotheses, they are evaluated with scoring criteria based on linguistic knowledge. The method has been evaluated on test sets collected as text data including various type of sentences which subjects made with no limitation. The experimental results of semantic understanding using 22 concept frames with approximately 1000-word vocabulary shows that the scoring method contributes to disambiguation of meaning candidates, and that 75 % of test sentences are correctly understood.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-27"
  },
  "shimazu94_icslp": {
   "authors": [
    [
     "Akira",
     "Shimazu"
    ],
    [
     "Kiyoshi",
     "Kogure"
    ],
    [
     "Mikio",
     "Nakano"
    ]
   ],
   "title": "Cooperative distributed processing for understanding dialogue utterances",
   "original": "i94_0099",
   "page_count": 4,
   "order": 28,
   "p1": "99",
   "pn": "102",
   "abstract": [
    "This paper proposes a cooperative distributed natural language understanding model that does syntactic, semantic, and pragmatic analyses independently and in parallel, unifying analysis results (logical forms) with those of other processes and complementing each other. The model is capable of analyzing irregular expressions in spontaneous speech and of processing them in real-time. An experimental system that implements this model demonstrates robust analysis.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-28"
  },
  "okada94_icslp": {
   "authors": [
    [
     "Michio",
     "Okada"
    ],
    [
     "Satoshi",
     "Kurihara"
    ],
    [
     "Ryohei",
     "Nakatsu"
    ]
   ],
   "title": "Incremental elaboration in generating and interpreting spontaneous speech",
   "original": "i94_0103",
   "page_count": 4,
   "order": 29,
   "p1": "103",
   "pn": "106",
   "abstract": [
    "In this paper, we outline a computational model for incremental elaboration and some other disfluencies in spontaneous speech. In the model, spontaneity and disfluencies are assumed to be behaviors that emerge as the result of many local interactions between routine activities and their environment. Incremental elaboration can be thought to arise from interaction among heterogeneous modules. In the model presented here, we also introduce a framework for accomplishing desired actions with an under-specification control strategy. To specify the behavior, this strategy tries to indirectly arrange conditions (a situation) for the emergence of the desired actions. We then show that incremental elaboration is also a strategy for generating and interpreting an utterance with minimal specification. The model presented here provides a way of thinking about the design of an interpreting system that is robust and efficient architecture for spontaneous speech.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-29"
  },
  "eckert94_icslp": {
   "authors": [
    [
     "W.",
     "Eckert"
    ],
    [
     "Heinrich",
     "Niemann"
    ]
   ],
   "title": "Semantic analysis in a robust spoken dialog system",
   "original": "i94_0107",
   "page_count": 4,
   "order": 30,
   "p1": "107",
   "pn": "110",
   "abstract": [
    "In this paper we describe the semantic interpretation process of utterances in a spoken dialog system for train table inquiries. Spoken dialogs show a large set of problems in human-machine-communicationlike stops, corrections, filled pauses, non grammatical sentences, ellipses, unconnected phrases etc. In our robust approach we are able to handle a substantial amount of them. While the principles of robustness are shared in several modules, in this paper we concentrate on the aspect of robust semantic analysis and domain specific interpretation of spoken utterances. Keywords: robustness, semantic analysis, spoken dialog\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-30"
  },
  "kanazawa94_icslp": {
   "authors": [
    [
     "Hiroshi",
     "Kanazawa"
    ],
    [
     "Shigenobu",
     "Seto"
    ],
    [
     "Hideki",
     "Hashimoto"
    ],
    [
     "Hideaki",
     "Shinchi"
    ],
    [
     "Yoichi",
     "Takebayashi"
    ]
   ],
   "title": "A user-initiated dialogue model and its implementation for spontaneous human-computer interaction",
   "original": "i94_0111",
   "page_count": 4,
   "order": 31,
   "p1": "111",
   "pn": "114",
   "abstract": [
    "This paper describes a user-initiated dialogue model and its implementation on TOSBURG II, a real-time task-oriented spontaneous speech dialogue system, for achieving robust human-computer interaction. In order to make TOS-BURG II user-initiative and flexible, we designed it on a semantic interpretation and user-initiated dialogue management basis. To implement the user-initiated dialogue model on a robust user-unspecified system, we have introduced the ATN (Augmented Transition Network) and represented the model, including eight user states and 19 system states, within the ATN framework. Through dialogue experiments on unspecified users, TOSBURG II has shown to be highly robust and friendly. It is confirmed that our dialogue model facilitates user-initiated spontaneous human-computer interaction.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-31"
  },
  "kieling94_icslp": {
   "authors": [
    [
     "Andreas",
     "Kießling"
    ],
    [
     "Ralf",
     "Kompe"
    ],
    [
     "Anton",
     "Batliner"
    ],
    [
     "Heinrich",
     "Niemann"
    ],
    [
     "Elmar",
     "Nöth"
    ]
   ],
   "title": "Automatic labeling of phrase accents in German",
   "original": "i94_0115",
   "page_count": 4,
   "order": 32,
   "p1": "115",
   "pn": "118",
   "abstract": [
    "In this paper a method for the automatic labeling of phrase accents is described, based on a large text corpus that has been generated automatically and read by 100 speakers. Perception experiments on a subset of 500 utterances show a high agreement between the automatically generated accent labels and the judgment scores obtained. We computed different prosodic feature vectors from the speech signal for each syllable and trained different Gaussian distribution classifiers and artificial neural networks using the automatically generated accent labels. Recognition rates of up to 83% could be achieved for the distinction of accentuated vs. unaccentuated syllables. Similar results could be obtained for the comparison of the listeners judgments with the automatic classification.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-32"
  },
  "maekawa94_icslp": {
   "authors": [
    [
     "Kikuo",
     "Maekawa"
    ]
   ],
   "title": "Intonational structure of kumamoto Japanese: a perceptual validation",
   "original": "i94_0119",
   "page_count": 4,
   "order": 33,
   "p1": "119",
   "pn": "122",
   "abstract": [
    "A phonological model of intonation was proposed for Kumamoto accentless Japanese and the validity of the model was examined by means of perception experiment. Twenty intonation contours including both well-formed and ill-formed ones were synthesized from the original by means of LPC resynthesis. Naturalness judgments given by 19 native Kumamoto speakers indicated that well-formed contours were more natural than ill-formed ones.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-33"
  },
  "pitrelli94_icslp": {
   "authors": [
    [
     "John F.",
     "Pitrelli"
    ],
    [
     "Mary E.",
     "Beckman"
    ],
    [
     "Julia",
     "Hirschberg"
    ]
   ],
   "title": "Evaluation of prosodic transcription labeling reliability in the tobi framework",
   "original": "i94_0123",
   "page_count": 4,
   "order": 34,
   "p1": "123",
   "pn": "126",
   "abstract": [
    "A diverse group of speech scientists and engineers has developed the ToBI (TOnes and Break Indices) prosodic transcription system and materials to teach it to transcribers. ToBI consists of parallel tiers reflecting the multiple components of prosody, the most important being a tone tier, for intonational analysis, and a break index tier, for indicating strength of coherence or disjunctive between adjacent words. To assess the system, we measured inter-transcriber agreement on utterances representative of the varied types of speech important to researchers, employing a diverse set of transcribers ranging from experts to newly-trained users. Consistency was measured in terms of number of transcriber pairs agreeing on the labeling of each particular word, a stringent metric. Using this metric, we observe 88% agreement on the presence or absence of a particular category of tonal element, and 81% agreement on the exact label for a tonal category. For break indices, agreement to within one level occurs 92% of the time. We conclude that the ToBI standard and its training materials have been refined to the point that they can be used fruitfully for largescale annotation of prosodic phenomena in speech databases.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-34"
  },
  "todd94_icslp": {
   "authors": [
    [
     "Neil P. McAngus",
     "Todd"
    ],
    [
     "Guy J.",
     "Brown"
    ]
   ],
   "title": "A computational model of prosody perception",
   "original": "i94_0127",
   "page_count": 4,
   "order": 35,
   "p1": "127",
   "pn": "130",
   "abstract": [
    "This paper describes a computational model of auditory rhythm perception, and demonstrates its application to the extraction of prosodic information from spoken language. The model consists of three stages. In the first stage, the speech waveform is processed by a simulation of the auditory periphery. Secondly, the output of the auditory periphery is processed by a multiscale filtering mechanism, analogous to a short-term auditory memory. Finally, peaks in the response of the multiscale mechanism are accumulated in a long-term auditory store, and plotted to give a representation referred to as a rhythmogram. It is demonstrated that there is a close relationship between the rhythmogram of an utterance and its corresponding stress hierarchy derived by phonological analysis.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-35"
  },
  "kakita94_icslp": {
   "authors": [
    [
     "Kuniko",
     "Kakita"
    ]
   ],
   "title": "Inter-speaker interaction in speech rhythm: some durational properties of sentences and intersentence intervals",
   "original": "i94_0131",
   "page_count": 4,
   "order": 36,
   "p1": "131",
   "pn": "134",
   "abstract": [
    "The present study aims to investigate inter-speaker interaction in speech production. Utterance texts consisting of five short sentences were read by five subjects in two different ways. In one instance, a series of five sentences were read through by a single speaker; in the other instance, the second speaker took over from the first speaker after the third sentence. The duration of the sentences and intersentence intervals following the takeover were compared with those at corresponding locations in single speaker readings. The results showed that both the sentence duration and the interval duration following the takeover deviated from one's 'preferred' duration obtained from single speaker readings. The deviation was mostly assimilative, i.e., the sentence and interval durations became more like those of the preceding speaker's. The results also indicated that sentence duration and interval duration differed characteristically in the way they were affected by another speaker's speech.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-36"
  },
  "lyberg94_icslp": {
   "authors": [
    [
     "Bertil",
     "Lyberg"
    ],
    [
     "Barbro",
     "Ekholm"
    ]
   ],
   "title": "The final lengthening phenomenon in Swedish - a consequence of default sentence accent?",
   "original": "i94_0135",
   "page_count": 4,
   "order": 37,
   "p1": "135",
   "pn": "138",
   "abstract": [
    "At the word and phrase levels the segment duration is found to vary depending on the position in the word and in the phrase. The greatest positional effect is the phenomenon of final lengthening that appears to be of considerable generality as a phonetic phenomenon. In this investigation the segment duration is studied at different speech rates and with focus assignment systematically varied along the sentences. The subjects mimicked the prosodic pattern of the utterances by using reiterant speech. From the different utterances an \"out-of-focus-utterance\" was constructed by extracting the nonsense words not being in focus position. In most of these \"out-of-focus-utterances\" there was only a slight tendency to a final lengthening effect and in some utterances there was no final lengthening at all. This finding is contradictory to earlier obtained results for Swedish.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-37"
  },
  "behne94_icslp": {
   "authors": [
    [
     "Dawn M.",
     "Behne"
    ],
    [
     "Bente",
     "Moxness"
    ]
   ],
   "title": "Concurrent effects of focal stress, postvocalic voicing and distinctive vowel length on syllable-internal timing in norwegian",
   "original": "i94_0139",
   "page_count": 4,
   "order": 38,
   "p1": "139",
   "pn": "142",
   "abstract": [
    "The relative duration of consonants and vowels of a syllable can be affected by factors such as focal stress, postvocalic voicing, and distinctive vowel length. Previous research has shown that focal stress generally affects both vowel and consonant durations within a syllable, whereas for both postvocalic voicing and distinctive vowel length, vowel and postvocalic consonant durations are inversely related. In fluent speech, the timing within a syllable will simultaneously reflect the influence of focal stress, postvocalic voicing and distinctive vowel length. The goal of the present study is to examine the combined effects of focal stress, postvocalic voicing and distinctive vowel length on vowel duration and neighboring consonants, and to characterize the concurrent effect of these factors on syllable- and rhyme-internal timing. Based on measurements within Norwegian C1VC2S, results indicate that (1) vowel duration is affected by the three factors investigated, (2) focal stress affects both vowel and consonant durations, and (3) closure duration of C2 is inversely related to the effects of postvocalic voicing and distinctive vowel length on vowel duration. These findings are discussed in terms of syllable- and rhyme-internal timing.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-38"
  },
  "takagi94_icslp": {
   "authors": [
    [
     "Kazuyuki",
     "Takagi"
    ],
    [
     "Shuichi",
     "Itahashi"
    ]
   ],
   "title": "Prosodic pattern of utterance units in Japanese spoken dialogs",
   "original": "i94_0143",
   "page_count": 4,
   "order": 39,
   "p1": "143",
   "pn": "146",
   "abstract": [
    "This paper reports a study to classify spontaneously spoken utterances by their prosodic features only. First, input speech was segmented into utterance units, each of which is a speech interval segmented by pauses or distinctive utterances such as interjections and fillers. Secondly, the cluster analysis was conducted using seven prosodic parameters of an utterance unit, i.e., speech rate, F0 onset, final, values and the positions of the maximum and the minimum Fq. On the 10 spoken dialogs uttered by 9 speakers selected from ASJ Continuous Speech Corpus, the utterance units were classified into 5 distinctive clusters: \"Initial & Middle\", \"Final\", \"Slow\", \"Devoiced\", and \"Others\". Each cluster was characterized by syntactic and prosodic features of utterance units contained in the cluster. Results of this paper can be used for a preprocessing of spontaneous speech understanding systems.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-39"
  },
  "ichikawa94_icslp": {
   "authors": [
    [
     "Akira",
     "Ichikawa"
    ],
    [
     "Shinji",
     "Sato"
    ]
   ],
   "title": "Some prosodical characteristics in spontaneous spoken dialogue",
   "original": "i94_0147",
   "page_count": 4,
   "order": 40,
   "p1": "147",
   "pn": "150",
   "abstract": [
    "In dialogue, prosody plays essential roles. The first one is as the semantic structure of speech. The second is as real-time control information for dialogue. The third is as information of the speaker's psychological state. In this paper, we report some characteristics of prosody from the viewpoints of the second and third functions in a spontaneous spoken Japanese dialogue. These results will be useful in developing a man-machine interface system that understands spoken dialogue which allows the user to speak in natural manner.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-40"
  },
  "karlsson94_icslp": {
   "authors": [
    [
     "Inger",
     "Karlsson"
    ],
    [
     "Johan",
     "Liljencrants"
    ]
   ],
   "title": "Wrestling the two-mass model to conform with real glottal wave forms",
   "original": "i94_0151",
   "page_count": 4,
   "order": 41,
   "p1": "151",
   "pn": "154",
   "abstract": [
    "This paper reports on some experiments to simulate female glottal wave shapes using a mechanical glottal model. The model is a modified mechanical two-mass model [9]. The simulated wave forms were compared to natural inverse filtered speech from one female speaker. Physiological data for the informant and from the literature were inserted in the model. Some properties of the model proved to be essential, for example bulging vocal cords and incomplete closure. The correspondence between simulated and natural wave forms is considerable.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-41"
  },
  "strik94_icslp": {
   "authors": [
    [
     "Helmer",
     "Strik"
    ],
    [
     "Louis",
     "Boves"
    ]
   ],
   "title": "Automatic estimation of voice source parameters",
   "original": "i94_0155",
   "page_count": 4,
   "order": 42,
   "p1": "155",
   "pn": "158",
   "abstract": [
    "Voice source parameters can be estimated by fitting a voice source model to the glottal flow signal which is obtained by means of inverse filtering. In this paper we investigate the behaviour of the LF-model in a number of non-linear parameter estimation procedures. It is concluded that (1) the parameter estimates are robust against additive (white and narrow band) noise in the flow waveforms, (2) simplex search algorithms perform better than steepest descent algorithms, provided that (3) the LF-pulse is generated with an algorithm that treats all parameters as real numbers.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-42"
  },
  "ding94_icslp": {
   "authors": [
    [
     "Wen",
     "Ding"
    ],
    [
     "Hideki",
     "Kasuya"
    ],
    [
     "Shuichi",
     "Adachi"
    ]
   ],
   "title": "Simultaneous estimation of vocal tract and voice source parameters with application to speech synthesis",
   "original": "i94_0159",
   "page_count": 4,
   "order": 43,
   "p1": "159",
   "pn": "162",
   "abstract": [
    "In order to synthesize natural sounding speech with voice quality variations, we propose a concatenative synthesis method based on stored formant/antiformant templates of vowel-consonant-vowel (VCV) segments and on sophisticated control of voice source parameters. By using the parametric Rosenberg-Klatt (RK) model to generate a voiced source waveform and an autoregressive exogenous (ARX) model to represent voiced speech production process, a new adaptive pitch-synchronous analysis method has been devised to estimate the model parameters from which the templates are semiautomatically created. The Kalman filter algorithm deals with the ARX model identification and a simulated annealing method is used for the nonlinear optimization to estimate the voice source parameters. The method has been tested with synthetic speech sounds by comparing with some other approaches in terms of the accuracy of estimated parameter values. Preliminary synthesis experiments have shown that natural sounding speech with various voice qualities can be generated with the proposed method by manipulating the voice source parameters.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-43"
  },
  "badin94_icslp": {
   "authors": [
    [
     "Pierre",
     "Badin"
    ],
    [
     "Christine H.",
     "Shadle"
    ],
    [
     "Y. Pham Thi",
     "Ngoc"
    ],
    [
     "J. N.",
     "Carter"
    ],
    [
     "W. S. C.",
     "Chiu"
    ],
    [
     "Ceila",
     "Scully"
    ],
    [
     "K.",
     "Stromberg"
    ]
   ],
   "title": "Frication and aspiration noise sources: contribution of experimental data to articulatory synthesis",
   "original": "i94_0163",
   "page_count": 4,
   "order": 44,
   "p1": "163",
   "pn": "166",
   "abstract": [
    "In spite of several pioneering studies in the field, the acoustic, geometric and aerodynamic characteristics of fricative consonants are far from being completely understood. The work presented here aims at clarifying the relationships between the noise source spectrum and the vocal tract geometry and aerodynamics. It has been in part motivated by the need for knowledge of sources for articulatory synthesis. Transfer function and radiated sound spectra have been measured quasi-simultaneously on a subject producing sustained whispered vowels and voiceless fricative consonants at different speech effort levels. For the same human subject, an enhanced Electropalatography (eEPG) system has been used to determine the size and shape of the constriction independently of aerodynamically-based estimates. Source spectra have been obtained by a pseudo-inverse filtering technique. The dependency of overall SPL and source tilt upon the glottal area for vowels and upon the constriction area for fricatives and the flow in the vocal tract is quantified. A tendency of a slight increase of constriction area with speech effort has also been shown by both eEPG and aerodynamically determined areas, and could possibly be ascribed to the subject's strategies for producing different effort levels.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-44"
  },
  "miki94_icslp": {
   "authors": [
    [
     "Nobuhiro",
     "Miki"
    ],
    [
     "Pierre",
     "Badin"
    ],
    [
     "Y. Pham Thi",
     "Ngoc"
    ],
    [
     "Yoshihiko",
     "Ogawa"
    ]
   ],
   "title": "Vocal tract model and 3-dimensional effect of articulation",
   "original": "i94_0167",
   "page_count": 4,
   "order": 45,
   "p1": "167",
   "pn": "170",
   "abstract": [
    "The vocal-tract shapes of Japanese vowels are measured by MRI. Since the 3D figures from this measurement data are very complex, it is important to discover how much the complexity of the 3D shape brings about changes in the acoustic characteristics of vocal-tracts. We estimate the 3D effect from the complex shape by using our 3D FEM models, in which we introduce an elongation factor of the closed section. From this FEM analysis, though we see the 3D effect in the pressure distribution, we can get equivalent parameters of the one dimensional transmission line model. Moreover, we point out the effect of mechanical vibration of the larynx, and show that the movement of the laryngeal part influences the transfer function of vocal tracts even in isolated vowels or stationary vowels, because the glottis is moving as an open and close phase, or the pitch is slightly changing. Since the 3D shape also varies near the glottis, we evaluate the effect of this variation in the spectral estimation.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-45"
  },
  "suzuki94_icslp": {
   "authors": [
    [
     "Hisayoshi",
     "Suzuki"
    ],
    [
     "Jianwu",
     "Dang"
    ],
    [
     "Takayoshi",
     "Nakai"
    ],
    [
     "Akira",
     "Ishida"
    ],
    [
     "Hiroshi",
     "Sakakibara"
    ]
   ],
   "title": "3-d FEM analysis of sound propagation in the nasal and paranasal cavities",
   "original": "i94_0171",
   "page_count": 4,
   "order": 46,
   "p1": "171",
   "pn": "174",
   "abstract": [
    "Six 3-D acoustic tube models of the nasal and paranasal cavities are constructed based on morphological data obtained from one male subject by Magnetic Resonance Imaging(MRI). For the models, sound propagation in the nasal tract is calculated by solving the Helmholtz equation with the 3-D finite element method (FEM). Sound pressure distributions are investigated in three dimensions. The transfer function of sound pressure from the velopharyngeal port to the nostrils is analyzed and compared with results obtained from a conventional electric circuit model. The peak frequencies in the range above 1.5 kHz are clearly lower in the transfer function obtained from FEM than that from the electric circuit model. FEM provides more powerful capabilities for a tube with a complex shape, such as the nasal cavity, than does the conventional method.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-46"
  },
  "honda94_icslp": {
   "authors": [
    [
     "Kiyoshi",
     "Honda"
    ],
    [
     "Hiroyuki",
     "Hirai"
    ],
    [
     "Jianwu",
     "Dang"
    ]
   ],
   "title": "A physiological model of speech production and the implication of tongue-larynx interaction",
   "original": "i94_0175",
   "page_count": 4,
   "order": 47,
   "p1": "175",
   "pn": "178",
   "abstract": [
    "A articulatory model which combines laryngeal and supra-laryngeal articulators has been designed based on our morphological and physiological studies of speech production. The model consists of a finite element model of the tongue and a mass-spring model of rigid structures. Static postures of the model are determined by computing equilibrium of muscle forces on all of the components, and are used to calculate cross-sectional areas of the vocal tract and transfer function of the model with reference to volumetric MRI data. The cricothyroid angle determines parameters for vibration of the two-mass model, and the source sounds are coupled to the model to output synthesized vowel sounds. In this model, biomechanical connections among the jaw, the hyoid bone, the tongue and the laryngeal cartilage are represented by mass-spring actions. The so-called tongue-larynx interaction which is observed in natural speech are demonstrated in the simulation. The overall performance of the model during vowels with various F0 levels has been tested by comparison of the acoustic data from the model's output with recorded speech.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-47"
  },
  "honda94b_icslp": {
   "authors": [
    [
     "Masaaki",
     "Honda"
    ],
    [
     "Tokihiko",
     "Kaburagi"
    ]
   ],
   "title": "A dynamical articulatory model using potential task representation",
   "original": "i94_0179",
   "page_count": 4,
   "order": 48,
   "p1": "179",
   "pn": "182",
   "abstract": [
    "A computational model for converting a phoneme specific task sequence into articulatory movements is presented. The task is defined as the vocal tract gestures in a coordinative structure of an articulatory system and is represented phoneme-specifically in a metric form of weighted squared distance from the task target value, where the magnitude of the weights controls the significance level of each gesture. Articulatory trajectories are determined so as to minimize the objective function represented by the weighted sum of the smoothness and the task metric criteria on the trajectories. We also describe how to produce a variable pattern of coarticulatory behavior from the phoneme-specific task sequence using this model. The computed articulatory trajectories are compared with the empirical data obtained with an electro-magnetic instrument.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-48"
  },
  "stevens94_icslp": {
   "authors": [
    [
     "Kenneth N.",
     "Stevens"
    ],
    [
     "Corine A.",
     "Bickley"
    ],
    [
     "David R.",
     "Williams"
    ]
   ],
   "title": "Control of a klatt synthesizer by articulatory parameters",
   "original": "i94_0183",
   "page_count": 4,
   "order": 49,
   "p1": "183",
   "pn": "186",
   "abstract": [
    "The movements of the articulators during speech production perform two functions: (1) to generate sources in the vicinity of constrictions within the airway, and (2) to shape the vocal tract to filter these sources. In an articulatory synthesizer, these two functions are automatic consequences of the movements and states of the articulators, whereas in a formant synthesizer such as a Klatt synthesizer control of the sources and of the filtering are performed independently. This paper describes an attempt to combine the simplicity of control based on articulatory parameters with the demonstrated capability of a formant synthesizer for generating natural-sounding and intelligible speech. A set of higher-level parameters related to articulation is selected, and these parameters are mapped into a larger set of acoustically-based parameters that control the Klatt synthesizer. Strategies for generating these parameters from a linguistic description of an utterance are described.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-49"
  },
  "minematsu94_icslp": {
   "authors": [
    [
     "Nobuaki",
     "Minematsu"
    ],
    [
     "Keikichi",
     "Hirose"
    ]
   ],
   "title": "Speech recognition using HMM with decreased intra-group variation in the temporal structure",
   "original": "i94_0187",
   "page_count": 4,
   "order": 50,
   "p1": "187",
   "pn": "190",
   "abstract": [
    "A new clustering scheme was proposed for the improvement of HMM-based phoneme recognition with temporal modeling. A precise observation of the temporal correspondences between the training data and their corresponding phoneme HMMs indicated that there were two extreme cases, one with several types of correspondences in a phoneme group that were completely different one from another, and the other with only one type. Although temporal modeling technique was commonly used to incorporate the temporal information in the HMMs, good modeling was not obtained for the former case. To cope with this problem, a new scheme was proposed where the training data for the phoneme of the former case were clustered into several smaller groups. The clustering was conducted so as to reduce the variation in the temporal correspondence in a group. After the clustering, a new HMM was constructed for each divided group. Using the proposed method, speaker dependent recognition experiments were conducted for the phonemes segmented from isolated words. A few-percent increase was observed in the recognition rate, indicating the validity of the proposed method.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-50"
  },
  "osaka94_icslp": {
   "authors": [
    [
     "Yukihiro",
     "Osaka"
    ],
    [
     "Shozo",
     "Makino"
    ],
    [
     "Toshio",
     "Sone"
    ]
   ],
   "title": "Spoken word recognition using phoneme duration information estimated from speaking rate of input speech",
   "original": "i94_0191",
   "page_count": 4,
   "order": 51,
   "p1": "191",
   "pn": "194",
   "abstract": [
    "This paper describes a spoken word recognition system is based on phoneme duration estimated from the speaking rate of an input speech. We found that the normalization of phoneme duration with the average vowel duration of input speech and with the average duration of each phoneme class was very effective to reduce the variation of phoneme duration. For the normalization, we propose the first-order linear regressive equation as a function of the average vowel duration for estimating the duration of each phoneme in input speech. We applied this method to isolated spoken word recognition. We prepared several kinds of equations by taking into account various phoneme contexts and then examined them by word recognition scores. The word recognition score was 97.3% for the 212 word vocabulary, using the equation based on the weighted sum of two estimates from the preceding and the following phoneme dependent estimation. The score increased by 1.6% comparing to that without the information of speaking rate.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-51"
  },
  "wakita94_icslp": {
   "authors": [
    [
     "Yumi",
     "Wakita"
    ],
    [
     "Eiichi",
     "Tsuboka"
    ]
   ],
   "title": "State duration constraint using syllable duration for speech recognition",
   "original": "i94_0195",
   "page_count": 4,
   "order": 52,
   "p1": "195",
   "pn": "198",
   "abstract": [
    "For speech recognition using HMMs, we propose an adaptive syllable duration constraint method. The method constrains syllable durations using a relation each syllable included in the same utterance [1]. The duration of t-th syllable d(t) is predicted by using d0(l)... d0(t-1) the durations of syllables which have been recognized . After a syllable is recognized, if the durations of the t-th syllable is very different from the predicted value, the result is rejected. Advantages of this method are Its applicability is independent of the speed of speech. The durations of syllables within a sentence don't vary unnaturally. These qualities are not found in non-adaptive duration constraint method. We confirmed that this method improves recognition rate. If this syllable duration prediction (SDP) method can be used for constraining the duration of HMM states, the duration constraint can be integrated with matching and will bring SDP's improvements in recognition rate and computing time.\n",
    "This paper proposes a new method of state duration constraint using SDP. At first the duration of s-th state of t-th syllable is predicted using the duration of t-th syllable which is predicted by SDP. Next the matching period of the state is constrained using the predicted state duration.\n",
    "We evaluate this method using word and sentence recognition. For word recognition (100 words and 9 speakers, open test ), the error reduction is 14% and the matching speed is 25% shorter. For sentence recognition ( 50 sentences and 6 speakers, open test ), the error reduction is 46% and the matching speed is 50% shorter.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-52"
  },
  "hayamizu94_icslp": {
   "authors": [
    [
     "Satoru",
     "Hayamizu"
    ],
    [
     "Kazuyo",
     "Tanaka"
    ]
   ],
   "title": "Statistical modeling and recognition of rhythm in speech",
   "original": "i94_0199",
   "page_count": 4,
   "order": 53,
   "p1": "199",
   "pn": "202",
   "abstract": [
    "This paper proposes a new framework for processing rhythm in speech where temporal types are recognized using statistical models of mora durations. Temporal patterns, such as rhythm and tempo in speech, contain some basic information about communication through the spoken language. This information has not yet been fully used in speech recognition. This paper proposes that temporal types themselves be modeled and recognized by statistical models. Using the ASJ Continuous Speech Database, experiments for recognizing temporal types of bunsetsu (short phrases) were conducted. Approximately 72% of temporal types were identified correctly using these models, without using information about the length of pauses and fundamental frequencies. The recognized types were very consistent (approximately 94% were of the same types) for closed and open models. These results show the promising potential of the proposed framework.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-53"
  },
  "hu94_icslp": {
   "authors": [
    [
     "Xinhui",
     "Hu"
    ],
    [
     "Keikichi",
     "Hirose"
    ]
   ],
   "title": "Recognition of Chinese tones in monosyllabic and disyllabic speech using HMM",
   "original": "i94_0203",
   "page_count": 4,
   "order": 54,
   "p1": "203",
   "pn": "206",
   "abstract": [
    "HMM-based tone recognition methods were developed for monosyllabic and disyllabic speech of standard Chinese. Two dimensional feature vectors were used for these methods to represent well both macroscopic and microscopic features of fundamental frequency contours well. In order to realize a function of speaker normalization in the methods, an offset was introduced to the fundamental frequency. It was shown experimentally that the best function was obtained when mean fundamental frequency averaged over several word utterances of the speaker being used. The words should include those of every tone types equally. As for the disyllabic tone recognition, the developed method does not require segmentation process into syllables. Besides four lexical tone models, two models were added to represent the half 3rd tone and the first-syllabic 4th tone in 4th tone plus 4th tone sequence. The unvoiced region of a disyllable usually corresponds to the initial consonant of the second syllable. A model was also assigned to this region to reduce the coarticulation effect between two syllables. As for the neutralized tone, it was included in the 4th tone group tentatively, and, after the HMM-based recognition process, it was separated depending on the durational difference. With the developed methods above, correct recognition rate of 98.5% was achieved for mono-syllables of multiple speakers, and, for disyllables of a speaker, 94.5% was obtained.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-54"
  },
  "wu94_icslp": {
   "authors": [
    [
     "Jun",
     "Wu"
    ],
    [
     "Zuoying",
     "Wang"
    ],
    [
     "Jiasong",
     "Sun"
    ],
    [
     "Jin",
     "Guo"
    ]
   ],
   "title": "Chinese speech understanding and spelling-word translation based on the statistics of corpus",
   "original": "i94_0207",
   "page_count": 4,
   "order": 55,
   "p1": "207",
   "pn": "210",
   "abstract": [
    "In this paper, a new natural language processing approach based on the statistics of corpus is proposed and has been successfully applied to THED-919 Chinese speech recognition system to eliminate acoustic recognition errors and to translate spellings into Chinese words. The accuracy rate of spelling-word translation of unrestricted text is 98.4% and 2/3 of acoustic recognition errors are eliminated. Keywords: Speech Recognition, Speech Understanding, Markov Chain, N-Gram Model.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-55"
  },
  "wang94_icslp": {
   "authors": [
    [
     "Ren-Hua",
     "Wang"
    ],
    [
     "Hui",
     "Jiang"
    ]
   ],
   "title": "State-codebook based quasi continuous density hidden Markov model with applications to recognition of Chinese syllables",
   "original": "i94_0211",
   "page_count": 4,
   "order": 56,
   "p1": "211",
   "pn": "214",
   "abstract": [
    "In this paper a new improved type of HMM, called State-CodeBook based quasi continuous density HMM (SCBHMM) is proposed and is tested in the recognition of Chinese syllables . The SCBHMM is composed of a set of model parameters, which can explicitly incorporate more acoustic characteristics of speech under limited training data. Here, the observation probability is associated with the static feature of speech within certain state and state transition probability is related to the temporal variations in speech spectra. SCBHMM suggests an effective method to integrate static and dynamic features in speech recognition. Preliminary experiments on the standard Chinese Speech Database CRDB x, showed that the proposed SCBHMM not only achieved a great improvement over the original HMM, but also greatly reduced the computation consumption in the training process. An accuracy of more than 92% for the top one candidate and 99% for the top five candidates was achieved in the Chinese Syllables recognition.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-56"
  },
  "parris94_icslp": {
   "authors": [
    [
     "Eluned S.",
     "Parris"
    ],
    [
     "Michael J.",
     "Carey"
    ]
   ],
   "title": "Estimating linear discriminant parameters for continuous density hidden Markov models",
   "original": "i94_0215",
   "page_count": 4,
   "order": 57,
   "p1": "215",
   "pn": "218",
   "abstract": [
    "This paper describes a new technique for performing Linear Discriminant Analysis (LDA) on Hidden Markov Models(HMMs) incorporating state specific mixture densities. Previous work with LDA in speech recognition has used models comprising a unimodal multivariate Gaussian density per state or semi-continuous models using tied densities across states. As the number of models in the mixture densities in a HMM are increased and speech frames are mapped into the new feature space the LDA does not produce the expected unit variance distributions. By treating each state as a single class the assumption that a state can be described as a single multivariate Gaussian is violated. A new technique has been developed which maintains the mapping into the LDA feature space and constructs new HMMs directly from the transformed speech frames, considerably reducing the computation required. An 11% reduction in error rate has been achieved over the standard LDA technique for monophone HMMs and 25% of the system parameters can be discarded without loss in performance.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-57"
  },
  "wolfertstetter94_icslp": {
   "authors": [
    [
     "F.",
     "Wolfertstetter"
    ],
    [
     "Günther",
     "Ruske"
    ]
   ],
   "title": "Discriminative state-weighting in hidden Markov models",
   "original": "i94_0219",
   "page_count": 4,
   "order": 58,
   "p1": "219",
   "pn": "222",
   "abstract": [
    "This paper proposes a method for explicit modeling of the time varying discriminative power in phoneme-sized Hidden Markov Models (HMMs), which is expected to improve generalization ability by focussing on typical temporal segments of each class. Since HMM states generally cover stationary or homogeneous segments within a class, we use one weight parameter per state. In this way, each model obtains a distribution of the discriminative power over the states, which directly corresponds to a distribution of the influence of each state on the total emission score of a feature vector sequence. An algorithm based on a gradient descent method is used to estimate the additional HMM parameters. It has been found to be very important that the search for the optimal path within a model is not influenced by the weights of the states. To allow for this, an extended Viterbi based time-synchronous continuous speech recognizer is proposed. The idea behind this strategy is to control the influence of each state on the total emission score by parameters containing a 'measure of classification relevance* wherever the models compete with each other. The modeling of and the processing within the individual units is based on the common maximum likelihood approach.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-58"
  },
  "watanabe94_icslp": {
   "authors": [
    [
     "Takao",
     "Watanabe"
    ],
    [
     "Koichi",
     "Shinoda"
    ],
    [
     "Keizaburo",
     "Takagi"
    ],
    [
     "Eiko",
     "Yamada"
    ]
   ],
   "title": "Speech recognition using tree-structured probability density function",
   "original": "i94_0223",
   "page_count": 4,
   "order": 59,
   "p1": "223",
   "pn": "226",
   "abstract": [
    "This paper proposes a new speech recognition method using tree-structured probability density functioned (pdf) to realize high speed HMM based speech recognition. In order to reduce likelihood calculation for a pdf set composed of the Gaussian pdjs for all mixture components, all states and all recognition units, the likelihood calculation is coarsely done for the element pdf (element of the pdf set) whose likelihood Nk[xt] at time t is not likely to be large. The pdf set is expressed as the tree-structured form. A leaf node of the tree corresponds to an element pdf. A non-leaf node corresponds to a cluster composed of element pdfs. To each cluster is attached a cluster obtained by approximating the mixture of all element pdfs in the cluster by a single Gaussian pdf In the recognition, the likelihood set is calculated by searching the tree; by calculating the likelihood from the cluster pdf at the node and traversing the nodes with the largest likelihood from the root node. Recognition experiments showed that the amount of computation was drastically reduced by the proposed method with little degradation in the recognition accuracy for both speaker-independent and speaker-adaptive modes.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-59"
  },
  "roe94_icslp": {
   "authors": [
    [
     "David B.",
     "Roe"
    ],
    [
     "Michael D.",
     "Riley"
    ]
   ],
   "title": "Prediction of word confusabilities for speech recognition",
   "original": "i94_0227",
   "page_count": 4,
   "order": 60,
   "p1": "227",
   "pn": "230",
   "abstract": [
    "Words which are similar in pronunciation cause errors by speech recognizers. In an application of speech recognition, the vocabulary should be chosen so as to avoid similar sounding words or phrases. Phonetically similar words (such as \"wait\" and \"eight\") or short words (\"on\" and \"off) may be confused by the speech recognizer, with undesirable consequences. We have developed a software tool, word_confuse, that detects confusable words. The confusability between pairs of words is calculated from two sources of information: the phonetic pronunciation of words as determined by the AT&T text-to-speech synthesizer, and the phonetic confusions exhibited by an AT&T phone-based speech recognizer. The calculation of confusability is based on searches through a finite state network that represents probabilistically the phonetic pronunciation of words. The metric of similarity is based on the Bhattacharyya distance. Word_confuse can be used to detect and eliminate confusable words from the vocabulary used in speech recognition applications.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-60"
  },
  "zhao94_icslp": {
   "authors": [
    [
     "Li",
     "Zhao"
    ],
    [
     "Hideyuki",
     "Suzuki"
    ],
    [
     "Seiichi",
     "Nakagawa"
    ]
   ],
   "title": "A comparison study of output probability functions in HMMs through spoken digit recognition",
   "original": "i94_0231",
   "page_count": 4,
   "order": 61,
   "p1": "231",
   "pn": "234",
   "abstract": [
    "In speech recognition, HMM, as one of the most effective methods, has been used to model speech statistically. Traditionally, the discrete distribution HMM and the continuous distribution HMM have been widely used in various applications. On the other hand, to further improve recognition performance, in recent years, HMMs with various output probability functions have been proposed. The representative models of which are the mixture continuous distributed HMM and the semi-continuous distributed HMM. However, recently we have also proposed the RBF(radius basis function) based HMM and the VQ-distortion based HMM by combining the RBF function and the VQ-distortion measure with HMM, respectively. These use a RBF function and VQ-distortion measure at each state respectively instead of an output probability density function used by traditional HMMs.\n",
    "In this paper, we first describe the RBF based HMM and the VQ-distortion based HMM. In addition, to confirm the performance of RBF based HMM and the VQ-distortion based HMM, we compared them with the discrete distributed HMM, the mixture continuous distributed HMM(full covariances matrix and diagonal matrix) and the semi-continuous distributed HMM based on their speech recognition performance rates through experiments on speaker-independent spoken digit recognition. From these comparison, we confirmed that the RBF based HMM and the VQ-distortion based HMM are robust and superior to conventional HMMs.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-61"
  },
  "takara94_icslp": {
   "authors": [
    [
     "Tomio",
     "Takara"
    ],
    [
     "Naoto",
     "Matayoshi"
    ],
    [
     "Kazuya",
     "Higa"
    ]
   ],
   "title": "Connected spoken word recognition using a many-state Markov model",
   "original": "i94_0235",
   "page_count": 4,
   "order": 62,
   "p1": "235",
   "pn": "238",
   "abstract": [
    "This paper is a report on an application of the Markov model to an automatic speech recognition system, in which a large number of states are adopted to model the transitional characteristics of speech more accurately. Unlike the traditional HMM, the feature vectors of this model are regarded to be the states of the Markov model. The transition-probability of the state is, in its initial condition, assumed to be represented by multidimensional normal density function of the feature vector. The many-state model is obtained by quantizing the feature vector (state) space and sampling the probability density function at each code vector. The resulting recognizer was tested and compared on a vocabulary of four-digit numerals using 3 dimensional feature vector sequences. The many-state model attained a recognition score of 98.2%, which was 1.6% higher than that of a five-state traditional HMM.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-62"
  },
  "johansen94_icslp": {
   "authors": [
    [
     "Finn Tore",
     "Johansen"
    ]
   ],
   "title": "Global optimisation of HMM input transformations",
   "original": "i94_0239",
   "page_count": 4,
   "order": 63,
   "p1": "239",
   "pn": "242",
   "abstract": [
    "This paper deals with discriminative optimisation of HMMs and hybrid models for continuous speech recognition. Using the well-known, sentence discriminative MMI criterion, we have observed a mismatch between error criterion and Viterbi recognition rate. We show how to overcome this, either by total likelihood scoring in the decoder, or by state-sequence discriminative optimisation criteria. We apply discriminative optimisation to a hybrid model, consisting of a continuous density HMM, and a linear or non-linear input transformation. Results are presented for TIMIT phone recognition, both on a small 5-class task, and the \"standard\" 39-class task.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-63"
  },
  "sun94_icslp": {
   "authors": [
    [
     "D.",
     "Sun"
    ],
    [
     "L.",
     "Deng"
    ]
   ],
   "title": "Nonstationary-state hidden Markov model with state-dependent time warping: application to speech recognition",
   "original": "i94_0243",
   "page_count": 4,
   "order": 64,
   "p1": "243",
   "pn": "246",
   "abstract": [
    "We present a new algorithm for estimating state-dependent polynomial coefficients in the nonstationary-state (or trended) hidden Markov model (HMM), which allows for the flexibility of linear time warping or scaling in individual model states. The need for the state-dependent time warping arises from the observation that multiple state-segmented speech data sequences used for training a single set of polynomial coefficients often vary appreciably in their sequence lengths due to speaking rate variation and other temporal factors. The algorithm is developed based on a general framework with use of auxiliary parameters, which, of no interests in themselves, provide an means for achieving maximal accuracy for estimating the polynomial coefficients in the model. The speech recognition experiment results based on TIMIT database demonstrate the advantages of the time-warping trended HMMs over the regular trended HMMs.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-64"
  },
  "mari94_icslp": {
   "authors": [
    [
     "Jean-Francois",
     "Mari"
    ],
    [
     "Jean-Paul",
     "Haton"
    ]
   ],
   "title": "Automatic word recognition based on second-order hidden Markov models",
   "original": "i94_0247",
   "page_count": 4,
   "order": 65,
   "p1": "247",
   "pn": "250",
   "abstract": [
    "A new recognizer for isolated and connected words based on second-order hidden Markov models is presented. This system uses a modified formulation of the Baum-Welch algorithm and an extension of the Viterbi algorithm. A comparative study between first-order (HMMl) and second-order (HMM2) Markov models is carried out. In order to evaluate the performances of both models, the TI-NIST database of isolated and connected digits was used.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-65"
  },
  "chen94_icslp": {
   "authors": [
    [
     "Xixian",
     "Chen"
    ],
    [
     "Yinong",
     "Li"
    ],
    [
     "Xiaoming",
     "Ma"
    ],
    [
     "Lie",
     "Zhang"
    ]
   ],
   "title": "On the application of multiple transition branch hidden Markov models to Chinese digit recognition",
   "original": "i94_0251",
   "page_count": 4,
   "order": 66,
   "p1": "251",
   "pn": "254",
   "abstract": [
    "In this paper we propose a multiple branch hidden Markov model(MBHMM) which is different from the conventional ones. In the basic HMMs, there is only one transition branch from one state to another one. Our new model has multiple transition branches between two states. As a result, it can hold much more spectral information in the speech signal than the basic HMMs. The evaluation, decoding, and training algorithms associated with MBHMM are also derived. The resulting recognizer is tested on a vocabulary of ten Chinese digits over 20 speakers. The recognition results show that MBHMM significantly outperforms the conventional discrete HMM(DHMM).\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-66"
  },
  "gales94_icslp": {
   "authors": [
    [
     "M. J. F.",
     "Gales"
    ],
    [
     "Steve J.",
     "Young"
    ]
   ],
   "title": "Parallel model combination on a noise corrupted resource management task",
   "original": "i94_0255",
   "page_count": 4,
   "order": 67,
   "p1": "255",
   "pn": "258",
   "abstract": [
    "This paper addresses the problem of automatic speech recognition in the presence of interfering noise. It focuses on the Parallel Model Combination (PMC) scheme, which has been shown to be a powerful technique for achieving noise robustness. However, most experiments reported on PMC to date have been on small, 10-50 word vocabulary systems. Resource Management (RM) is a 1000 word continuous speech recognition task, which reveals compensation requirements not highlighted by the smaller vocabulary tasks. In particular, it is necessary to compensate the differential as well as the static parameters to achieve good recognition performance. The database used for these experiments was the RM speaker independent task with Lynx helicopter noise from the NOISEX-92 database added. The experiments reported here used the HTK RM recogniser developed at CUED modified to include PMC based compensation for the static, delta and delta-delta parameters. After training on clean speech data, adding noise at 18-20dB signal to noise ratio was found to seriously degrade the performance of the recogniser. However, using PMC the performance was restored to a level comparable with that obtained when training directly in the noise corrupted environment.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-67"
  },
  "puel94_icslp": {
   "authors": [
    [
     "Jean-Baptiste",
     "Puel"
    ],
    [
     "Regine",
     "André-Obrecht"
    ]
   ],
   "title": "Robust signal preprocessing for HMM speech recognition in adverse conditions",
   "original": "i94_0259",
   "page_count": 4,
   "order": 68,
   "p1": "259",
   "pn": "262",
   "abstract": [
    "The detection of speech endpoints is a strategic process for speech recognition systems in adverse conditions, but it remains a rather delicate problem. We introduce two signal processing methods that offer a good robustness without requiring high level informations about the signal. The first approach uses temporal parameters, the other frequential ones. We discuss and compare their performances using the ARS ESPRIT database (isolated words pronounced in a car). We show that these methods coupled with a statistical segmentation offer very good discrimination between noisy segments and speech segments, and a better precision for locating the speech boundaries. The preprocessing is introduced in a HMM speech recognition system.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-68"
  },
  "katoh94_icslp": {
   "authors": [
    [
     "Masaharu",
     "Katoh"
    ],
    [
     "Masaki",
     "Kohda"
    ]
   ],
   "title": "A study on viterbi best-first search for isolated word recognition using duration-controlled HMM",
   "original": "i94_0263",
   "page_count": 4,
   "order": 69,
   "p1": "263",
   "pn": "266",
   "abstract": [
    "In the conventional hidden Markov model (HMM), the prob- ability of duration of a state decreases exponentially with time. It is not appropriate for representing the temporal structure of speech. To overcome this problem, the use of HMMs with duration models or time-dependent transition probabilities has been proposed [1][2]. These models accomplish the task with a large increase in the computation complexity. In this paper we present the Viterbi best-first searching algorithm using duration-controlled HMMs. To set a heuristic score appropriately, how the constraint is imposed on HMMs used in the backward Viterbi is investigated. The new searching algorithm is evaluated on isolated word recognition experiments. Experimental results show that the conventional Viterbi search with duration control takes 280-290 times of computation cost necessary for that without duration control, while the new searching algorithm takes only 10~15% of the computation cost keeping the same recognition rate.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-69"
  },
  "takahashi94_icslp": {
   "authors": [
    [
     "Satoshi",
     "Takahashi"
    ],
    [
     "Yasuhiro",
     "Minami"
    ],
    [
     "Kiyohiro",
     "Shikano"
    ]
   ],
   "title": "An HMM duration control algorithm with a low computational cost",
   "original": "i94_0267",
   "page_count": 4,
   "order": 70,
   "p1": "267",
   "pn": "270",
   "abstract": [
    "This paper proposes a new duration control algorithm for limiting both the maximum and the minimum state durations in an HMM trellis likelihood calculation. The amount of computation required by this algorithm is only order zero (O(0)) for the maximum state duration; that is, the computation amount is independent of the maximum state duration. Thus, the algorithm can drastically reduce the computation needed for duration control. The algorithm was evaluated using a large-vocabulary speaker-independent speech recognition system for telephone directory assistance. The average error reduction rates were about 6-7% in the sentence understanding error for the continuous word utterance mode and the spontaneous utterance mode.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-70"
  },
  "beyerlein94_icslp": {
   "authors": [
    [
     "Peter",
     "Beyerlein"
    ]
   ],
   "title": "Fast log-likelihood computation for mixture densities in a high-dimensional feature space",
   "original": "i94_0271",
   "page_count": 4,
   "order": 71,
   "p1": "271",
   "pn": "274",
   "abstract": [
    "A computationally very expensive task arising within speech recognition systems using continuous mixture density HMMs is the log-likelihood computation. In the Philips large vocabulary continuous-speech recognition system it consumes 50%-75% of the computing resources. In our system the log-likelihood computation amounts to a nearest-neighbor search, i.e. to a search for the component density of a mixture density whose mean vector has a minimal distance to the observed feature vector. Fast nearest-neighbor search techniques based on the triangle inequality are very powerful if the dimension of the feature space is lower than about 10. However, a direct application of these techniques is prohibitive in our framework which is characterized by a high-dimensional feature space and a small number of component densities per mixture density. In a typical setup we have 120 component densities per mixture density and a dimension of 63. This paper introduces an efficient nearest-neighbor search algorithm adapted to the conditions of a high dimensional feature space and sparse data, gives a theoretical explanation and an experimental validation for the constraints of fast nearest-neighbor search techniques in a high-dimensional space.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-71"
  },
  "cremelie94_icslp": {
   "authors": [
    [
     "Nick",
     "Cremelie"
    ],
    [
     "Jean-Pierre",
     "Martens"
    ]
   ],
   "title": "Time synchronous heuristic search in a stochastic segment based recognizer",
   "original": "i94_0275",
   "page_count": 4,
   "order": 72,
   "p1": "275",
   "pn": "278",
   "abstract": [
    "A single pass heuristic search method to be included in a stochastic segment based recognizer is presented. Thanks to a novel and efficient implementation of Nilsson's A/A* graph search algorithm, and thanks to the introduction of an appropriate heuristic function, the presented algorithm significantly outperforms the standard Viterbi beam search. Moreover, it was possible to conceive a time-synchronous search (no prior knowledge of the endpoint needed), and to restrict the amount of storage required. As such, the algorithm is extremely suitable for real-time implementation.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-72"
  },
  "wesenick94_icslp": {
   "authors": [
    [
     "Maria-Barbara",
     "Wesenick"
    ],
    [
     "Florian",
     "Schiel"
    ]
   ],
   "title": "Applying speech verification to a large data base of German to obtain a statistical survey about rules of pronunciation",
   "original": "i94_0279",
   "page_count": 4,
   "order": 73,
   "p1": "279",
   "pn": "282",
   "abstract": [
    "In this paper we present a new research project to obtain a statistical survey of the pronunciation of German using an automatic system for segmentation and labeling of speech data and a very large data base of spoken German (GermAn Spoken in Public, GASP). It mainly involves the development of two components: a) An automatic system of speech verification (PHONSEG) which produces a segmentation with semi-continuous HMMs corresponding to a given input-string of phonetic segment labels, b) A rule corpus of German pronunciation (PHONHYP) with which various possible forms of pronunciation can be derived from a citation form in order to model the variability of speech on a relatively broad segmental level. The rules express processes in German that are well-known and that can be observed in manual transcriptions as for example contained in the German PhonDat database. The input to the system as a whole is the speech wave and the orthographic representation or the citation forms of the words contained in the utterance as a string of phonetic symbols. PHONSEG matches all possible forms that have been derived by PHONHYP on the signal and evaluates the likelihood. The output is the segmentation with the highest maximum overall likelihood, the corresponding labels and for each segment the time-normalized log likelihood.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-73"
  },
  "jouvet94_icslp": {
   "authors": [
    [
     "D.",
     "Jouvet"
    ],
    [
     "Katarina",
     "Bartkova"
    ],
    [
     "A.",
     "Stouff"
    ]
   ],
   "title": "Structure of allophonic models and reliable estimation of the contextual parameters",
   "original": "i94_0283",
   "page_count": 4,
   "order": 74,
   "p1": "283",
   "pn": "286",
   "abstract": [
    "This paper presents a contextual modeling of phonemes, and describes a new technique that renders a reliable estimation of contextual parameters. Using this approach the modeling of all of the acoustic realizations of a given sound is integrated into a single complex unit, for which each entry and exit state is assigned to a specific context. Context clustering trees are defined and used in order to provide a reliable estimation of the contextual parameters. Using hand-made trees, a 12 % reduction in the error rate is achieved on a 250-word vocabulary set, which is distinct from the training vocabulary. Finally, an automatic context clustering procedure is presented and applied in order to automatically generate these clustering trees. Using this automated approach the reduction in the error rate is comparable to that of the hand-made trees.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-74"
  },
  "windheuser94_icslp": {
   "authors": [
    [
     "Christoph",
     "Windheuser"
    ],
    [
     "Frédéric",
     "Bimbot"
    ],
    [
     "Patrick",
     "Haffner"
    ]
   ],
   "title": "A probabilistic framework for word recognition using phonetic features",
   "original": "i94_0287",
   "page_count": 4,
   "order": 75,
   "p1": "287",
   "pn": "290",
   "abstract": [
    "In this paper we propose a way to include a phonetic representation using binary distinctive phonetic features into the probabilistic framework of a hybrid system for word recognition. This could be done under two assumptions: 1.) The features are changing synchronously at the borders of the phonemes and 2.) the features are conditionally independent with one another. We report experiments we have done with a hybrid system working with a feature representation with which we get a recognition rate of 96.9 % word recognition on a spelled letter task. Then we describe some ways to weaken the assumption of independence between the features. We demonstrate that these ways do not improve significantly the recognition rate but lead to a simpler system with the same performance.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-75"
  },
  "afify94_icslp": {
   "authors": [
    [
     "Mohamed",
     "Afify"
    ],
    [
     "Yifan",
     "Gong"
    ],
    [
     "Jean-Paul",
     "Haton"
    ]
   ],
   "title": "Nonlinear time alignment in stochastic trajectory models for speech recognition",
   "original": "i94_0291",
   "page_count": 3,
   "order": 76,
   "p1": "291",
   "pn": "294",
   "abstract": [
    "A nonlinear time alignment technique is presented in the framework of stochastic trajectory models (STM). We show how to obtain maximum likelihood (ML) estimates of model parameters, and how to use the technique during recognition with a slight additional computational overhead. Experimental results for a French 850 word continuous speech task are given. For a 10 speaker population, we test with various degrees of nonlinearity, and the introduced technique provides a slight improvement (about 1%) in the average word recognition rate.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-76"
  },
  "lubensky94_icslp": {
   "authors": [
    [
     "David M.",
     "Lubensky"
    ],
    [
     "Ayman O.",
     "Asadi"
    ],
    [
     "Jayant M.",
     "Naik"
    ]
   ],
   "title": "Connected digit recognition using connectionist probability estimators and mixture-Gaussian densities",
   "original": "i94_0295",
   "page_count": 4,
   "order": 77,
   "p1": "295",
   "pn": "298",
   "abstract": [
    "We report on some recent improvements to a continuous density hidden Markov model (CDHMM) based speech recognition system which is being developed for a variety of telecommunication applications. In particular, we are concerned with computing the emission probabilities of an HMM state using a combination of multi-layer perceptrons (MLPs) as probability estimators and mixture-Gaussian densities. Using MLPs as state observation estimators has been shown to improve accuracy on a speaker verification task [11]. In this paper, we describe a connected digit recognition system which incorporates both MLPs and mixture, Gaussian densities. The results are reported on the standard Texas Instruments (TI) connected digit database [9] which was digitally filtered to the telephone bandwidth (300 Hz-3.2 kHz) and downsampled to 8 kHz. A hybrid MLP/HMM system led to 15% improvement in performance. The final string error rate is 1.7% for unknown length strings.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-77"
  },
  "takeda94_icslp": {
   "authors": [
    [
     "Kazuya",
     "Takeda"
    ],
    [
     "Tetsunori",
     "Murakami"
    ],
    [
     "Shingo",
     "Kuroiwa"
    ],
    [
     "Seiichi",
     "Yamamoto"
    ]
   ],
   "title": "A trellis-based implementation of minimum error rate training",
   "original": "i94_0299",
   "page_count": 4,
   "order": 78,
   "p1": "299",
   "pn": "302",
   "abstract": [
    "A new implementation of ME (Minimum Error rate) training is proposed. The most important difference from conventional ME training is the use of a trellis-based calculation for the discriminant function, instead of the Viterbi based calculation of the conventional training. The key idea of the training is to use a matrix representation of state transit probabilities of an HMM for calculating the discriminant function so as to simplify the differential operation on the misclassification and loss functions. From the non-segmental characteristics of the discriminant function, loss functions for substitution, insertion and/or deletion errors are easily calculated by substituting, inserting and/or deleting the matrices for the corresponding HMM units of the loss function. Based on the proposed training, therefore, both string level and unit level error minimizations are easily integrated.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-78"
  },
  "yi94_icslp": {
   "authors": [
    [
     "Me",
     "Yi"
    ]
   ],
   "title": "Concatenated training of subword HMMs using detected labels",
   "original": "i94_0303",
   "page_count": 4,
   "order": 79,
   "p1": "303",
   "pn": "306",
   "abstract": [
    "In this paper we describe a method that trains HMMs from non-label training data by using concatenated training. We must solve two problems in using training speech data and corresponding text to perform concatenated training. One problem is how to make a correspondence between text and phonetic descriptions, and the other is how to get a one-and-only phonetic description which corresponding to the real utterance. We use the speech recognition as a preprocess before concatenated training. We create a finite state automaton to find the correct phonetic labels. This automaton connects sequentially all the words, from the first to the last of the training sentence, allows a pause between any two words, and aligns all possible labels when fail in transcribing a word to a one-and-only phonetic label. In a continuous speech recognition experiment, word accuracy has been improved from 91.3% to 95.2%.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-79"
  },
  "lin94_icslp": {
   "authors": [
    [
     "Chih-Heng",
     "Lin"
    ],
    [
     "Pao-Chung",
     "Chang"
    ],
    [
     "Chien-Hsing",
     "Wu"
    ]
   ],
   "title": "An initial study on speaker adaptation for Mandarin syllable recognition with minimum error discriminative training",
   "original": "i94_0307",
   "page_count": 4,
   "order": 80,
   "p1": "307",
   "pn": "310",
   "abstract": [
    "This paper presents a method of speaker adaptation for Mandarin syllable recognition. Based on a minimum error classification (MEC) criterion, we use the generalized probabilistic decent (GPD) algorithm to adjust iteratively the parameters of the hidden Markov models (HMM). The experiments on the multi-speaker Mandarin syllable database of Telecommunication Laboratories (T.L.) yield the following results: 1) Efficient speaker adaptation can be achieved through discriminative training using the MEC criterion and the GPD algorithm. 2) The computations required can be reduced through the use of the confusion sets in Mandarin base syllables. 3) For the discriminative training, the adjustment on the mean values of the Gaussian mixtures has the most prominent effect on speaker adaptation.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-80"
  },
  "kondo94b_icslp": {
   "authors": [
    [
     "Yuko",
     "Kondo"
    ]
   ],
   "title": "Phonetic underspecification in schwa",
   "original": "i94_0311",
   "page_count": 4,
   "order": 81,
   "p1": "311",
   "pn": "314",
   "abstract": [
    "This paper reports acoustic studies of vowel-to-vowel as well as consonant-to-vowel coarticulation effects on British English schwa in real-word sentences. The effects of both vocalic and consonantal contexts on F1 and F2 values of schwa are systematic. Different consonant contexts, in particular, introduce large variability in F2, covering a range of about 1000 Hz. The large variability was observed not only at the transitions but in the vowel midpoint. The extent of variability observed suggests that schwa is transparent in F2; its F2 values seem to be determined by the immediate consonant contexts plus the transconsonantal vocalic contexts. Thus schwa seems to be phonetically unspecified in F2 with no target value of its own apart from the value imposed by its contexts. On the other hand, variation in F1 was small and it was concluded that schwa is targeted in F1. That is, schwa is specified for [Height] but not for [Backness] in the traditional phonological terms.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-81"
  },
  "tanaka94_icslp": {
   "authors": [
    [
     "Shin'ichi",
     "Tanaka"
    ],
    [
     "Haruo",
     "Kubozono"
    ]
   ],
   "title": "Some remarks on the compound accent rule in Japanese",
   "original": "i94_0315",
   "page_count": 4,
   "order": 82,
   "p1": "315",
   "pn": "318",
   "abstract": [
    "The present paper discusses some idiosyncratic patterns behind the compound accent rules in Tokyo Japanese with a view to revealing the various linguistic factors which seem responsible for the regular-irregular contrast. Our discussion centers around the analysis of compound nouns whose righthand element is more than two morae long, end with a long (i.e. bimoraic) syllable and is accented on the penultimate mora when pronounced in isolation. Compounds of this type fall into two classes depending on the accent pattern they yield, those whose accentual pattern can be explained by the traditional compound accent rule and those which show an irregular accent pattern. Detailed analysis of the second type of compounds reveals that their irregular accentual behavior can be predicted largely from their semantic and/or morphological structures. It is suggested that these compound nouns show an accent pattern intermediate between compound accent and phrasal accent.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-82"
  },
  "potapova94_icslp": {
   "authors": [
    [
     "Rodmonga K.",
     "Potapova"
    ]
   ],
   "title": "Modifications of acoustic features in Russian connected speech",
   "original": "i94_0319",
   "page_count": 3,
   "order": 83,
   "p1": "319",
   "pn": "322",
   "abstract": [
    "This paper describes method and results of experimental investigation on the domain of acoustical analysis of a spoken text especially for differences between acoustic parameters of a set of words with pauses and without ones. The aim of investigation was a comparison between separate isolated pronounced words and the same words in a speech flow in text for Russian language. The present study was aimed also at revealing the differences between the prosodic features ( such as the fundamental frequency, intensity, duration) of isolated words vs. the same words in speed, (in text). Acoustical analysis has revealed that for a number of parameters there are regular differences between the separately pronounced words and the same words in the pause-free connected speech. The results were used as a part of linguistic information in recognition system.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-83"
  },
  "jun94_icslp": {
   "authors": [
    [
     "Sun-Ah",
     "Jun"
    ],
    [
     "Mira",
     "Oh"
    ]
   ],
   "title": "A prosodic analysis of three sentence types with \"WH\" words in Korean",
   "original": "i94_0323",
   "page_count": 4,
   "order": 84,
   "p1": "323",
   "pn": "326",
   "abstract": [
    "\"Wh\" words in Korean have two interpretations: either as wh-pronouns as in a wh-question or as indefinite pronouns as in a yes/no-question. The lexical ambiguity of a wh-word has been impressionistically noted to be differentiated by prosodic features. In this paper, we investigate which prosodic features, such as pitch range and accentual phrasing, differentiate these two usages, both in production and perception. We additionally investigate a third usage involving incredulity. Production data show that the three question types are distinguished by boundary tones, pitch ranges and accentual phrasing. Yes/no questions and other types are distinguished by different accentual phrasing. Incredulity and wh-questions are in general distinguished by different peak amplitudes and pitch ranges, in addition to a boundary tone. Interestingly, not all speakers use the same strategy to distinguish incredulity and wh-questions. The results of the perception test show that subjects often confused incredulity questions with wh-questions, suggesting that accentual phrasing is a stronger perceptual cue than pitch range or amplitude difference.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-84"
  },
  "hata94_icslp": {
   "authors": [
    [
     "Kazue",
     "Hata"
    ],
    [
     "Heather",
     "Moran"
    ],
    [
     "Steve",
     "Pearson"
    ]
   ],
   "title": "Distinguishing the voiceless fricatives f and TH in English: a study of relevant acoustic properties",
   "original": "i94_0327",
   "page_count": 4,
   "order": 85,
   "p1": "327",
   "pn": "330",
   "abstract": [
    "Distinguishing between the voiceless fricatives F and TH is a difficult problem for both natural speech and synthetic speech. We report the results of experiments and spectral analyses designed to find distinguishing acoustic characteristics of the voiceless fricatives F and TH. These experiments were also designed in consideration of our hybrid text-to-speech system, which combines formant synthesis with concatenated units from natural speech. In our system, the intelligibility of initial stops has improved dramatically in comparison with the formant-synthesizer-only version of our system, but F and TH are still highly confusable [1]. In this study, we used only natural speech, and conducted perceptual experiments by using frication-only stimuli and stimuli combining frication with segments of the following vowel. The results showed that when a frication portion alone was presented, F was correctly identified more often than TH. When the frication portion with the entire following vowel was presented, the identification of F showed significant improvement, reaching more than 90% for the /a u/ vowel contexts, and for the /i/ context, increasing from 40 to 78%. By contrast, the identification of TH failed to show any significant improvement.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-85"
  },
  "itoh94_icslp": {
   "authors": [
    [
     "Kenzo",
     "Itoh"
    ]
   ],
   "title": "Correlation analysis between speech power and pitch frequency for twenty spoken languages",
   "original": "i94_0331",
   "page_count": 4,
   "order": 86,
   "p1": "331",
   "pn": "334",
   "abstract": [
    "This paper describes the relationship between speech signal power and pitch frequency for twenty main languages. The goal is to confirm of the applicability of our earlier proposed power control rule using the relationship. First, an overall analysis is conducted for each language. Second, the short term correlation is analyzed to study the relationship between high correlation values and other characteristics of the speech signal. Last, in order to get information for developing an English Text-To-Speech (TTS) system, averaged phoneme power and pitch frequency is analyzed using American English speech signals with phoneme labeled data. Main results are shown below. (l)The average correlation coefficient ranged from +0.72 to +0.44 for the twenty languages. (2)The short term analysis found that high level speech was accompanied by high correlation values. The reason for this assumed to be the relationship between accentuation or stress. (3)The prospect for phoneme power control in an American English TTS system is excellent given the strong relationship between power and pitch, Those results strongly suggest that the relationship between speech power and pitch frequency can be used in speech processing systems for all spoken languages.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-86"
  },
  "jun94b_icslp": {
   "authors": [
    [
     "Jongho",
     "Jun"
    ]
   ],
   "title": "On gestural reduction and gestural overlap in Korean and English /PK/ clusters",
   "original": "i94_0335",
   "page_count": 4,
   "order": 87,
   "p1": "335",
   "pn": "338",
   "abstract": [
    "The purpose of this paper is to investigate the distinct roles of gestural reduction and gestural overlap in consonant place assimilation. It is found that gestural overlap alone does not give rise to perceptual assimilation in Korean pk clusters; instead, gestural reduction of the labial does. It is also found that there is no significant difference in the degrees of overlap between Korean and English pk clusters. A marked difference is observed in the gestural reduction of the labial; it often reduces in Korean but does not reduce in English. The difference between Korean and English place assimilations is therefore not due to different degrees of gestural overlap, but to an asymmetry of gestural reduction. Consequently, all these findings support the hypothesis that gestural reduction, not gestural overlap, plays the main role in casual speech place assimilation.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-87"
  },
  "gussenhoven94_icslp": {
   "authors": [
    [
     "Carlos",
     "Gussenhoven"
    ],
    [
     "Toni",
     "Rietveld"
    ]
   ],
   "title": "Intonation contours and the prominence of F0 peaks",
   "original": "i94_0339",
   "page_count": 4,
   "order": 88,
   "p1": "339",
   "pn": "342",
   "abstract": [
    "This paper reports the results of an experiment that sought to establish if the parameters in an existing F0 implementation model uniquely correspond to perceptual dimensions like 'emphasis' and 'liveliness'. These results are negative. A discussion of the rationale behind different kinds of perceptual experiments into perceived prominence leads to the conclusion that such experiments may reveal the effects of contour-internal relations like downstep and declination, or may more generally reveal the existence of various aspects of canonical contour shapes, but will not yield a calculus for determining the perceived prominence of pitch accents on the basis of anchor points in the surrounding unaccented parts of the contour.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-88"
  },
  "belotelgrenie94_icslp": {
   "authors": [
    [
     "Agnès",
     "Belotel-Grenié"
    ],
    [
     "Michel",
     "Grenié"
    ]
   ],
   "title": "Phonation types analysis in standard Chinese",
   "original": "i94_0343",
   "page_count": 4,
   "order": 89,
   "p1": "343",
   "pn": "346",
   "abstract": [
    "Standard Chinese is a tone language. Differencies in tone patterns of a syllable may lead to lexically semantic distinctions. The present study tries to examine and to caracterise phonation types in Standard Chinese. It was found that changes in phonation types frequently occurred for tones 3 and t4 and only few examples of creaky voice were observed for female speakers. Two measures using H2-H1 and Fl-H1 differences in dB were used on words containing vowel lal to quantify as precisely as possible changes in phonation types. Fl-H1 showed more salient and statistically significant differences. Moreover we noted a negative correlation between F0 values and Fl-H1 values for all speakers. These observations confirm that creaky voice phonation is not distinctive and suggest that creaky voice is used as a redundant cue of tone 3 and tone 4. Keywords: Standard Chinese, phonation types, creaky voice, tone\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-89"
  },
  "nakai94_icslp": {
   "authors": [
    [
     "Mitsuru",
     "Nakai"
    ],
    [
     "Hiroshi",
     "Shimodaira"
    ]
   ],
   "title": "Accent phrase segmentation by finding n-best sequences of pitch pattern templates",
   "original": "i94_0347",
   "page_count": 4,
   "order": 90,
   "p1": "347",
   "pn": "350",
   "abstract": [
    "This paper describes a prosodic method for segmenting continuous speech into accent phrases. Optimum sequences are obtained on the basis of least squared error criterion by using dynamic time warping between F0 contours of input speech and reference accent patterns called 'pitch pattern templates'. But the optimum sequence does not always give good agreement with phrase boundaries labeled by hand, while the second or the third optimum candidate sequence does well. Therefore, we expand our system to be able to find out multiple candidates by using N-best algorithm. Evaluation tests were carried out using the ATR continuous speech database of 10 speakers. The results showed about 97% of phrase boundaries were correctly detected when we took 30-best candidates, and this accuracy is 7.5% higher than the conventional method without using N-best search algorithm.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-90"
  },
  "derwing94_icslp": {
   "authors": [
    [
     "Bruce L.",
     "Derwing"
    ],
    [
     "Terrance M.",
     "Nearey"
    ]
   ],
   "title": "Sound similarity judgments and segment prominence: a cross-linguistic study",
   "original": "i94_0351",
   "page_count": 4,
   "order": 91,
   "p1": "351",
   "pn": "354",
   "abstract": [
    "This paper presents the results of applying the global sound similarity judgment (SS J) task to the investigation of phonological units in a variety of typologically diverse languages. Prior SSJ studies helped to demonstrate the prominence of the phonemic segment as a basic unit for English, and evidence was also found for the independent status of the rime unit. The segment also predominated when the SSJ task was extended to.Arabic words with a general CVCVC canonical form, but consonants contributed more to similarity scores than vowels and the medial consonant contributed much more than either the initial or final ones. When the SSJ technique was extended to Taiwanese, the segment again emerged as the basic phonological unit, but here the vowel was the most prominent unit overall and the initial C was far more prominent than the final C. In Japanese the mora outranked the segment in prominence, but each of the independent segments also made significant contributions to the SSJ ratings. Finally, in Korean, while the individual segments were again most prominent, a CV or \"body\" unit also emerged as a significant factor. Thus the segment was important throughout, though the weighting of specific segments and other units varied from language to language.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-91"
  },
  "fujisaki94_icslp": {
   "authors": [
    [
     "Hiroya",
     "Fujisaki"
    ],
    [
     "Sumio",
     "Ohno"
    ],
    [
     "Kei-ichi",
     "Nakamura"
    ],
    [
     "Miguelina",
     "Guirao"
    ],
    [
     "Jorge",
     "Gurlekian"
    ]
   ],
   "title": "Analysis of accent and intonation in Spanish based on a quantitative model",
   "original": "i94_0355",
   "page_count": 4,
   "order": 92,
   "p1": "355",
   "pn": "358",
   "abstract": [
    "The aim of the present study is to obtain an analytic and quantitative description of the prosodic characteristics of spoken Spanish and to find out the relationship between the description and the underlying linguistic and paralinguistic information. This paper presents results of our preliminary effort to apply a method of analysis of F$ contours that has been proposed and proved to be valid for Japanese and several other languages. Analysis of utterances of declarative sentences of Spanish indicates the basic validity of the method and provides useful findings on the prosodic characteristics of accent and intonation in Spanish.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-92"
  },
  "farnetani94_icslp": {
   "authors": [
    [
     "Edda",
     "Farnetani"
    ],
    [
     "M. Grazia",
     "Busa"
    ]
   ],
   "title": "Italian clusters in continuous speech",
   "original": "i94_0359",
   "page_count": 4,
   "order": 93,
   "p1": "359",
   "pn": "362",
   "abstract": [
    "This study explores the spatio-temporal interactions between Cl and C2 in Italian clusters in the light of current phonological theories. The clusters are /nt/, /nt?/, /n?/, /nk/, /kn/, /nr/ and /rn/, they were produced in continuous speech, and analyzed with EPG. The results indicate that neither featural nor gestural phonology alone can fully account for the data in this study. In particular in the clusters of alveolar + postalveolar or prepalatal, Cl seems to undergo graded changes as a function of C2 in two of the three subjects, while in the alveolar + velar clusters the data clearly indicate that a categorical process of assimilation is at work. The clusters of alveolar + trill show deletion of the first gesture in 95% of the cases.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-93"
  },
  "grover94_icslp": {
   "authors": [
    [
     "Cynthia",
     "Grover"
    ],
    [
     "Jacques",
     "Terken"
    ]
   ],
   "title": "Rhythmic constraints in durational control",
   "original": "i94_0363",
   "page_count": 4,
   "order": 94,
   "p1": "363",
   "pn": "366",
   "abstract": [
    "Two potential factors in durational control are addressed. First, we investigate whether lengthening a syllable implies lengthening all of its constituent phonemes in a regular way. Analysis of a small corpus of syllables shows that this is not the case. Second, we investigate the influence of rhythm by inspecting the pattern of compression of unstressed or unaccented syllables as a function of the number and position of syllables in the stress or accent group. We find no evidence of compression. We conclude that speakers do not control the timing of phonemes within a syllable very precisely, and that speakers do not compress or stretch unstressed syllables to produce more rhythmic speech.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-94"
  },
  "kurisu94_icslp": {
   "authors": [
    [
     "Kazutaka",
     "Kurisu"
    ]
   ],
   "title": "Further evidence for bi-moraic foot in Japanese",
   "original": "i94_0367",
   "page_count": 4,
   "order": 95,
   "p1": "367",
   "pn": "370",
   "abstract": [
    "Intensive study regarding phonological units in Japanese has been focused upon for the past few years. Numerous researchers have proposed some phonological units, among which bi-moraic foot is a valid candidate, considering various phonological phenomena of the Japanese language, and thus far, it has been thought to play important roles as a phonological basic unit in Japanese ([2] [3][6][7][8]). This paper reports further evidence for bi-moraic foot in Japanese given by explicit segmentation experiments, following Otake (1992,1993) ([4] [5]). In the earlier research, the segmentations at the second mora alone could be construed as evidence for bi-moraic foot in Japanese ([6]). In this paper, it is discussed that not only the segmentations at the second mora but also the ones at the fourth mora can be evidence for bi-moraic foot. This is based upon the idea that if the second moraic segmentations are to be evidence for bi-moraic foot, then the fourth moraic segmentations are multiples of the second moraic segmentations. Hence, the fourth moraic segmentations can be interpreted as binary bi-moraic foot, which support the bi-moraic foot theory in Japanese.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-95"
  },
  "sagawa94_icslp": {
   "authors": [
    [
     "Yuji",
     "Sagawa"
    ],
    [
     "Masahiro",
     "Ito"
    ],
    [
     "Noboru",
     "Ohnishi"
    ],
    [
     "Noboru",
     "Sugie"
    ]
   ],
   "title": "A model for generating self-repairs",
   "original": "i94_0371",
   "page_count": 4,
   "order": 96,
   "p1": "371",
   "pn": "374",
   "abstract": [
    "Self-repairs give us a key to understand how a human generates utterances. In this paper, we report a result of our analysis of timing factor of selfrepair. We categorize an error repaired by a stage it is generated, and investigate relation between a category and timing factor of the repair. The result shows that it takes more time to repair an error generated at earlier stage of generation. From the result, we develop a model of utterance generation. Finally, we try to account for some characteristics of self-repair with the model.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-96"
  },
  "cleirigh94_icslp": {
   "authors": [
    [
     "Chris",
     "Cleirigh"
    ],
    [
     "Julie",
     "Vonwiller"
    ]
   ],
   "title": "Accent identification with a view to assisting recognition (work in progress)",
   "original": "i94_0375",
   "page_count": 4,
   "order": 97,
   "p1": "375",
   "pn": "378",
   "abstract": [
    "This paper reports on work in progress to identify accented Australian English varieties to facilitate identification. Accented speech sounds different because the choosing and chaining of segments in production of speech is influenced by a speaker's linguistic background. The paper describes the development of a descriptive model of the phonology of Australian English from the theoretical standpoint of Systemic Phonology. The schema proposes morphemes composed of an obligatory core syllable element and optional peripheral elements. Core syllables are unmarked for grammatical distribution, but peripheral elements are restricted in their distribution to morpheme boundaries. We propose that this gives us a more powerful tool for description and hence identification of the accents observed.We hypothesise that using this approach has several benefits for recognition systems.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-97"
  },
  "reddy94_icslp": {
   "authors": [
    [
     "K. Nagamma",
     "Reddy"
    ]
   ],
   "title": "Phonetic, phonological, morpho-syntactic and semantic functons of segmental duration in spoken telugu: acoustic evidence",
   "original": "i94_0379",
   "page_count": 4,
   "order": 98,
   "p1": "379",
   "pn": "382",
   "abstract": [
    "This paper examines different aspects of the duration of Telugu speech sounds at both word level and sentence level, which is not only effective in contributing towards a better understanding of the spoken language, but is also essential for theoretical, practical and clinical applications. Telugu has multidimensional organization of segmental duration. It has various contrastive functions of duration at different levels of linguistic signal. There is a contrastive duration both in vowels and consonants at the phonological segmental level and the duration also combines with other higher level structures to bring out the difference in meaning. The intrinsic duration of segments also varies depending upon the (sound) type, position in the utterance, surrounding phonetic context, number of syllables in a word and so forth. Variations in duration of the segments have been correlated with phonetic, phonological, syllable, word, morphological, syntactic, semantic, pragmatic and some other levels of speech.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-98"
  },
  "mcrobbieutasi94_icslp": {
   "authors": [
    [
     "Zita",
     "McRobbie-Utasi"
    ]
   ],
   "title": "Timing strategies within the paragraph",
   "original": "i94_0383",
   "page_count": 4,
   "order": 99,
   "p1": "383",
   "pn": "386",
   "abstract": [
    "The objective of the experiment reported on in this paper was to determine the temporal patterns associated with the paragraph in Skolt Sami (a Finno-Ugric language). The acoustic analysis of duration indicates that there is a correlation between sentence duration and sentence order within the paragraph. It will be argued that although preboundary lengthening does not appear to be relevant as a signal of the paragraph boundary, there is a clearly observable temporal pattern that keeps the duration of the paragraph constant. It can be stated that speakers make a conscious effort to organize paragraph constituent and pause durations in a manner showing evident correlation and planning of the temporal organization of the paragraph. These efforts are recognized as definite timing strategies for achieving the durational target associated with the paragraphs analyzed. Four types of such strategies were identified in this experiment. The existence of such recognizable timing strategies appears to support the claim that speakers aim at a certain durational target.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-99"
  },
  "sekimoto94_icslp": {
   "authors": [
    [
     "Sotaro",
     "Sekimoto"
    ]
   ],
   "title": "The effect of the following vowel on the frequency normalization in the perception of voiceless stop consonants",
   "original": "i94_0387",
   "page_count": 4,
   "order": 100,
   "p1": "387",
   "pn": "390",
   "abstract": [
    "The purpose of the present study is to investigate the effect of the succeeding vowel characteristics on the frequency normalization for expansion or compression of the frequency axis in the perception of voiceless stop consonants. For this purpose, two hearing experiments were carried out. Synthetic voiceless stop monosyllables that were composed of an initial noise portion and the succeeding vowel portion were used as stimuli. The noise portion was simulated by single-pole noise and the resonant frequency was systematically varied. Identification test was made to determine the categorical boundary between /t/ and /k/, or /k/ and /p/ on the continuum of the resonant frequency of the noise pole. These categorical boundaries were compared for various frequency expansion or compression ratios to assess the effect of the following vowel characteristics on the perceptual frequency normalization. The influences of the vowel identification (Exp. 1) as well as the acoustic characteristics of the following vowel (Exp. 2) were examined. The results showed that the perceptual normalization for the expansion or compression of the frequency axis depended on the formant frequencies of the following vowel. The identification of the following vowel, on the other hand, showed lesser effect on the perceptual normalization.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-100"
  },
  "muranaka94_icslp": {
   "authors": [
    [
     "Toshiko",
     "Muranaka"
    ],
    [
     "Noriyo",
     "Hara"
    ]
   ],
   "title": "Features of prominent particles in Japanese discourse, frequency, functions and acoustic features",
   "original": "i94_0395",
   "page_count": 4,
   "order": 101,
   "p1": "395",
   "pn": "398",
   "abstract": [
    "Japanese particles have lexically no accent. However, some particles are sometimes uttered with higher tone in Japanese natural discourse. This phenomenon, prominence on particles, plays an important role for naturalness and understandability of discourse. In this study, discourses on six different topics uttered by a male announcer with no instruction at five speech rates were analyzed. The incidence of prominence on particles was higher in explanatory discourses than that in dramatic discourses, and was higher for a slower speech rate. As for grammatical function, prominent particles were used (1)when they expressed the theme or make clear syntactic structure or semantic boundary, or (2) when they were followed by an assistant negative phrase. Fundamental frequency (F0) contour was rising gradually in case 1 and rising abruptly in case 2. These results suggest that grammatical function of prominent particles is reflected in the F0 contour.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-101"
  },
  "ran94_icslp": {
   "authors": [
    [
     "Shuping",
     "Ran"
    ],
    [
     "Bruce",
     "Millar"
    ],
    [
     "Iain",
     "Macleod"
    ]
   ],
   "title": "Vowel quality assessment based on analysis of distinctive features",
   "original": "i94_0399",
   "page_count": 4,
   "order": 102,
   "p1": "399",
   "pn": "402",
   "abstract": [
    "This paper presents a novel method for vowel quality assessment which is based on the analysis of distinctive features. A sub-set of the features proposed by Jakobson, Fant and Halle are used. The acoustic features associated with each selected feature are encoded in the weights of a multilayer perceptron, The processing of an individual vowel sound by such feature-detecting perceptrons yields an estimate of that vowel's position in a distinctive feature space. The relative positions of one speaker's monophthongal vowel set within both a two-dimensional and a three-dimensional distinctive feature space are compared with vowel spaces based on articulatory considerations. The extent to which articulatory descriptions of acoustic vowels can be estimated using this method is discussed. Keyword: Distinctive features; Artificial neural networks, Articulatory features, Vowel quality assessment.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-102"
  },
  "delogu94_icslp": {
   "authors": [
    [
     "Cristina",
     "Delogu"
    ],
    [
     "Stella",
     "Conte"
    ],
    [
     "Giro",
     "Sementina"
    ]
   ],
   "title": "Differences in the fluctuation of attention during the listening of natural and synthetic passages",
   "original": "i94_0403",
   "page_count": 4,
   "order": 103,
   "p1": "403",
   "pn": "406",
   "abstract": [
    "Fluctuation of sustained attention is an important aspect of synthetic speech perception because it is related to problems of vigilance which are crucial for real applications. To study whether and how sustained attention fluctuates in synthetic speech, we run an experiment where, during a long passage produced by a synthesizer or by a human speaker, subjects had to recognize computer generated clicks distributed randomly in the passage. The results showed differences in the fluctuation of sustained attention during the listening of passages produced by a TTS system with respect to the same passage read by a human speaker, suggesting that listeners of synthetic passages may be required to pay more attention than listeners of natural passages.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-103"
  },
  "heuft94_icslp": {
   "authors": [
    [
     "Barbara",
     "Heuft"
    ],
    [
     "Thomas",
     "Portele"
    ]
   ],
   "title": "Production and perception of words with identical segmental structure but different number of syllables",
   "original": "i94_0407",
   "page_count": 4,
   "order": 104,
   "p1": "407",
   "pn": "410",
   "abstract": [
    "A common reduction phenomenon in German colloquial speech is the elimination or assimilation of the schwa sound in word final syllables which are reduced to a single segment. This study investigates whether those syllabic segments are perceptually different from their non-syllabic counterparts and which acoustic parameters are responsible for these differences. Two types of test words were chosen: words with schwa elimination between two nasals and words with schwa assimilation before a vocalized /r/. Evidence is given that differences are cued not only within the segments in question but in the preceding syllable as well.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-104"
  },
  "huang94_icslp": {
   "authors": [
    [
     "Caroline B.",
     "Huang"
    ],
    [
     "Mark A.",
     "Son-Bell"
    ],
    [
     "David M.",
     "Baggett"
    ]
   ],
   "title": "Generation of pronunciations from orthographies using transformation-based error-driven learning",
   "original": "i94_0411",
   "page_count": 4,
   "order": 105,
   "p1": "411",
   "pn": "414",
   "abstract": [
    "Two systems for automatically generating pronunciations from orthography are described. Efforts have been made to make the implementations language-independent. The first system is a rewrite engine which takes as input rewrite rules written by a human expert. The second system derives rewrite rules from an existing lexicon of orthographies and pronunciations using the transformation-based error-driven method. Results are given for German and American English.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-105"
  },
  "usuki94_icslp": {
   "authors": [
    [
     "Hidenori",
     "Usuki"
    ],
    [
     "Jouji",
     "Suzuki"
    ],
    [
     "Tetsuya",
     "Shimamura"
    ]
   ],
   "title": "Characteristics of mispronunciation and hesitation in Japanese tongue twister",
   "original": "i94_0415",
   "page_count": 4,
   "order": 106,
   "p1": "415",
   "pn": "418",
   "abstract": [
    "Frequently we produce mispronunciation and hesitation in phonation due to the difficulty in pronunciation. Characteristics of such troubles in phonation, however, have not been clearly understood. This paper analyzes characteristics in Japanese tongue twister statistically and the characteristics of mispronunciation and hesitation in phonation by employing questionnaire. Employed speech samples are Japanese tongue twister, because we consider that tongue twister has many factors of mispronunciation and hesitation. The results of questionnaire show that hesitation frequently occurs at vowels, particularly at vowel /a/. Palatalized sounds are found many times in the part of mispronunciation and hesitation. These results will contribute to improve the quality of speech synthesized by rule.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-106"
  },
  "junqua94_icslp": {
   "authors": [
    [
     "Jean-Claude",
     "Junqua"
    ]
   ],
   "title": "A duration study of speech vowels produced in noise",
   "original": "i94_0419",
   "page_count": 4,
   "order": 107,
   "p1": "419",
   "pn": "422",
   "abstract": [
    "In this paper we present a duration study for vowels produced in several noisy and non-noisy conditions in a \"s-s\" context. While confirming results from earlier work, such as a vowel duration increase when vowels are produced in noise, we also found that the type of noise influences vowel duration. While the type of masking noise affects the vowel duration it does not affect the duration of the preceding \"s\" consonant. This result does not hold for articulated speech, which lengthens the duration of the preceding \"s\" consonant compared to the quiet speech case while providing the larger vowel duration increase among the different speaking styles studied.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-107"
  },
  "coile94_icslp": {
   "authors": [
    [
     "Bert Van",
     "Coile"
    ],
    [
     "L. Van",
     "Tichelen"
    ],
    [
     "Annemie",
     "Vorstermans"
    ],
    [
     "J. W.",
     "Jang"
    ],
    [
     "M.",
     "Staessen"
    ]
   ],
   "title": "PROTRAN: a prosody transplantation tool for text-to-speech applications",
   "original": "i94_0423",
   "page_count": 4,
   "order": 108,
   "p1": "423",
   "pn": "426",
   "abstract": [
    "This paper describes the technique of Prosody Transplantation, its advantages and disadvantages. Special attention is paid to a development tool, called ProTran. This integrated tool running under MS- Windows was designed to automate and speed-up all steps that are involved in the prosody transplantation process.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-108"
  },
  "kohler94_icslp": {
   "authors": [
    [
     "Klaus J.",
     "Kohler"
    ]
   ],
   "title": "Complementary phonology a theoretical frame for labelling an acoustic data base of dialogues",
   "original": "i94_0427",
   "page_count": 4,
   "order": 109,
   "p1": "427",
   "pn": "430",
   "abstract": [
    "The theoretical concept of complementarity, as known from physics, is introduced into phonology with reference to the integration of segmental and componential approaches. The principle is illustrated by examples from an acoustic data base of German dialogues.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-109"
  },
  "jun94c_icslp": {
   "authors": [
    [
     "Sun-Ah",
     "Jun"
    ],
    [
     "Mary E.",
     "Beckman"
    ]
   ],
   "title": "Distribution of devoiced high vowels in Korean",
   "original": "i94_0479",
   "page_count": 4,
   "order": 110,
   "p1": "479",
   "pn": "482",
   "abstract": [
    "Korean has a process much like the well-studied \"vowel devoicing\" of Japanese. Evidence from Japanese suggests that the process is a phonetic undershoot of the vowel's gestures due to overlap with a preceding voiceless consonant. The similar Korean phenomenon would offer further support to this interpretation if vowel devoicing occurs differentially in the context of the three contrasting types of voiceless consonants, which have very different glottal gestures. The phonetic-undershoot account predicts that the vowel devoicing should be most common after plain fricatives or aspirated stops, where the glottal-opening gesture is largest and most likely to overweigh the vowel's voicing gesture. In syllables with lenis-stop initials, the vowel devoicing should occur less often in an accentual phrase medial position. The distribution of devoiced vowels was examined in a controlled corpus of dialogues containing CVCV target words, where the two consonants were voiceless and the first vowel was high. Each target word was produced either in phrase initial or in phrase medial position. The predictions were borne out. In addition, distributional differences involving the following consonant type suggest further testable differences about gestural overlap across syllable boundaries.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-110"
  },
  "yoon94_icslp": {
   "authors": [
    [
     "Yeo Bom",
     "Yoon"
    ]
   ],
   "title": "CV as a phonological unit in Korean",
   "original": "i94_0483",
   "page_count": 4,
   "order": 111,
   "p1": "483",
   "pn": "486",
   "abstract": [
    "The rime as a viable phonological unit in English has been supported by a wide range of sources of evidence: language acquisition, speech errors, a rime-based poetic tradition, experimental word games, etc. In Korean, however, there is evidence for CV (or the body), a unit composed of a vowel and the preceding consonant. This paper discusses (I) some evidence for the body unit in Korean, (II) a word-blending experiment that shows the correlation between the sonority value of a prevocalic consonant and its stickiness to the vowel, and (III) a recent extension involving sound similarity judgments that confirms the body as a major sub-syllabic unit in Korean.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-111"
  },
  "ohala94_icslp": {
   "authors": [
    [
     "Manjari",
     "Ohala"
    ]
   ],
   "title": "Experiments on the syllable in hindi",
   "original": "i94_0487",
   "page_count": 4,
   "order": 112,
   "p1": "487",
   "pn": "490",
   "abstract": [
    "This paper reports the results of two experiments testing aspects of the syllable in Hindi. The first experiment provides support for the universality of the 'onset-first' principle of syllabification proposed by phonologists for intervocalic single consonants. However, the data do not support this principle in the case of intervocalic consonant clusters. A second experiment explored claims regarding the internal structure of the syllable. Subjects showed no preference between an 'onset-rhyme' (C-VC) division vs. a 'head-coda' (CV-C) division, suggesting that the organization of the syllable in Hindi might be 'flat'.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-112"
  },
  "ohala94b_icslp": {
   "authors": [
    [
     "John J.",
     "Ohala"
    ]
   ],
   "title": "Towards a universal, phonetically-based, theory of vowel harmony",
   "original": "i94_0491",
   "page_count": 4,
   "order": 113,
   "p1": "491",
   "pn": "494",
   "abstract": [
    "Vowel Harmony (VH), phonological co-occurrence constraints between the features of vowels in polysyllabic words, is a fossilized remnant of an earlier phonetic process involving vowel-to-vowel assimilation. An understanding of the workings and constraints of VH can give us clues on how such inter-syllabic assimilations work and would be important for spoken language processing, e.g., motivating speech recognition using triphones. A universal, phonetically-based, theory of vowel harmony should explain why only certain distinctive features of vowels are subject to harmony, how \"neutral\" vowels arise, what intervening phonetic contexts favor or disfavor harmony.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-113"
  },
  "ingram94_icslp": {
   "authors": [
    [
     "John",
     "Ingram"
    ],
    [
     "Tom",
     "Mylne"
    ]
   ],
   "title": "Perceptual parsing of nasal vowels",
   "original": "i94_0495",
   "page_count": 4,
   "order": 114,
   "p1": "495",
   "pn": "498",
   "abstract": [
    "An experiment is reported on the perception of nasal resonance by French listeners responding to gated monosyllabic words containing phonemically nasal or oral vowels, or vowels before nasal consonants. At issue, following Lahiri and Marslen-Wilson´s [2] experiment on the perception of distinctive and allophonic nasality in Bengali and English, is the question of phonetic under-specificity in the recognition lexicon. However, we argue that the results are more informative of perceptual parsing strategies in the assignment of phonological structure to phonetic input, which argument is further supported by analysis of acoustic cues to oral-nasal resonance in the stimuli.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-114"
  },
  "ghitza94_icslp": {
   "authors": [
    [
     "Oded",
     "Ghitza"
    ],
    [
     "M. Mohan",
     "Sondhi"
    ]
   ],
   "title": "On the perceptual distance between speech segments",
   "original": "i94_0499",
   "page_count": 4,
   "order": 115,
   "p1": "499",
   "pn": "502",
   "abstract": [
    "For many tasks in speech signal processing it is of interest to develop an objective measure that correlates well with the perceptual distance between speech segments. (By speech segments we mean pieces of a speech signal, of duration 50-200 milliseconds. For concreteness we will consider a segment to mean a diphone.) Such a distance metric would be useful for low bit rate speech coders because perturbations introduced by such coders typically last for several tens of milliseconds. It would also be useful for automatic speech recognition on the assumption that mimicking human behavior will improve recognition performance. Yet a third use for such a metric would be to define a just noticeable difference for diphones (a \"phonemic\" JND). (If a diphone is perturbed, how far from the original must the perturbed diphone be, in order to be perceived as a different diphone?) In this talk we will describe our attempts at defining such a metric.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-115"
  },
  "akagi94_icslp": {
   "authors": [
    [
     "Masato",
     "Akagi"
    ],
    [
     "Astrid van",
     "Wieringen"
    ],
    [
     "Louis C. W.",
     "Pols"
    ]
   ],
   "title": "Perception of central vowel with pre- and post-anchors",
   "original": "i94_0503",
   "page_count": 4,
   "order": 116,
   "p1": "503",
   "pn": "506",
   "abstract": [
    "A vowel identification and a vowel matching experiment were performed to examine how preceding and following anchor signals affect central vowel perception. Previous experiments had shown that dynamical aspects of stimuli and the relation between the central vowel and the adjacent anchors may induce overshoot or extrapolation during stimulus processing. The present experiments examine the relative importance of the surrounding steady-state and the formant transitions with regard to center vowel extrapolation. Assuming that continuous phoneme sequences such as consonant-vowel-consonant (CVC) or VVV can be constructed with steady-states, transitions and vowels, four stimulus conditions are used: vowel presented in isolation (called Ref.), vowel with transitions (called A), isolated vowel surrounded by steady- states (called n), and vowel with transitions and steady-states (called Q). The central vowels were always 5-formant synthesized vowels, whereas the surrounding steady state and the transitions were either single-formant type sounds or 5-formant type sounds. The experimental results suggest that: (1) central vowel extrapolation occurs with Q-type stimuli, in both single- and 5-formant conditions, whereas averaging effects are observed with A- and II-type stimuli for some of the subjects. The overall order of the amount of overshoot is Q. > U > A > Ref. in the 5-formant condition. The most natural-sounding Q-type stimuli showed the largest amount of overshoot, and (2) the amount of overshoot with a 5-formant steady-state is larger than with a single-formant steady-state especially for the Il-type stimuli. This might be an indication that the Vowelness' of the pre- and post-anchors also contributes to the amount of overshoot. The matching results were less consistent.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-116"
  },
  "rossi94_icslp": {
   "authors": [
    [
     "Mario",
     "Rossi"
    ],
    [
     "Evelyne",
     "Peter-Defare"
    ],
    [
     "Regine",
     "Vial"
    ]
   ],
   "title": "Phonological mechanisms of French speech errors",
   "original": "i94_0507",
   "page_count": 4,
   "order": 117,
   "p1": "507",
   "pn": "510",
   "abstract": [
    "The present study, which reports the first stage of analysis of five corpora containing a total of 1488 errors, was aimed firstly at classifying French speech errors and determining whether our results are consistent with those obtained by most authors; secondly, by restricting our analysis to the phonological level, we have tried to discover the underlying mechanisms of phonologically-based lapsus linguae. Most of the results obtained are in line with those of previous studies. The results on phonological errors thus corroborate Shattuck-Hufnagels (1980) availability and similarity theory. However the results on vowel errors showed in particular that speech errors are not confusions and cannot be compared to phonological neutralizations, given that the similarity principle must be based on robust features.\n",
    "Accurate analysis of the feature substitution hierarchy showed that a permutation graph whose axes are the dimensions of the vocal tract is the best model for predicting speech errors.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-117"
  },
  "abubakar94_icslp": {
   "authors": [
    [
     "Mukhlis",
     "Abu-Bakar"
    ],
    [
     "Nick",
     "Chater"
    ]
   ],
   "title": "Phonetic prototypes: modelling the effects of speaking rate on the internal structure of a voiceless category using recurrent neural networks",
   "original": "i94_0511",
   "page_count": 4,
   "order": 118,
   "p1": "511",
   "pn": "514",
   "abstract": [
    "This is the last in a series of studies that attempts to account for rate effects in phonetic perception using a recurrent neural network. We present new findings that extend this research, emphasising particularly the effects of syllable duration on the internal structure of stimuli representing the voiceless /p/ category. In a series of simulations, network performance reveals a systematic rate effect that closely parallels that found in studies of human categorization. First, the internal structure of the voiceless category undergoes extensive alteration with changes in syllable duration. Second, in a selective adaptation type of experiment, the maximally effective adaptor shifts toward a longer VOT as syllable duration increases. The performance can be traced to the network's sensitivity to the training frequency of the individual stimulus, consistent with exemplar-based accounts of human category formation.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-118"
  },
  "hardcastle94_icslp": {
   "authors": [
    [
     "William J.",
     "Hardcastle"
    ]
   ],
   "title": "EPG and acoustic study of some connected speech processes",
   "original": "i94_0515",
   "page_count": 4,
   "order": 119,
   "p1": "515",
   "pn": "518",
   "abstract": [
    "Simultaneous EPG, acoustic, aerodynamic and electroglottographic data were obtained for 5 repetitions of a test sentence 'Fred can go, Susan can't go and Linda is uncertain' spoken by 7 native English speakers at normal conversational rate. Seven different types of EPG patterns were found to characterise the alveolarvelar consonant sequences in the sentences ranging from fully assimilated (no evidence of alveolar contact) to fully unassimilated (with audible alveolar release). There was considerable inter- and intra-subject variability in the production of the consonant sequences. The alveolar nasals seem more susceptible to assimilation than the plosive. A number of connected speech processes were also identified including vowel elision, glottalisation and nasalisation on vowels.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-119"
  },
  "fujimura94_icslp": {
   "authors": [
    [
     "Osamu",
     "Fujimura"
    ]
   ],
   "title": "Syllable timing computation in the c/d model",
   "original": "i94_0519",
   "page_count": 4,
   "order": 120,
   "p1": "519",
   "pn": "522",
   "abstract": [
    "The C/D model, a syllable-based phonetic implementation theory, is reviewed, and a revised algorithm for computing the temporal structure of speech in specified utterance situations is described. The theory assumes that all prosodic information is given in the form of a pulse train representing syllables and boundaries, where the height of each pulse is a scalar measure of the strength of the syllable or boundary. Temporal characteristics of the syllables are also computed through this magnitude pattern. Syllable edges are assigned time values according to the syllable concatenation principle, and articulatory gestures are evoked for consonantal elements by subordinate demisyllabic pulses that inherit the pulse height. Implications regarding correlations in magnitudes among tautosyllabic articulatory manifestations are discussed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-120"
  },
  "slamacazacu94_icslp": {
   "authors": [
    [
     "Tatiana",
     "Slama-Cazacu"
    ]
   ],
   "title": "Contribution of psycholinguistic perspective for speech technologies",
   "original": "i94_0523",
   "page_count": 4,
   "order": 121,
   "p1": "523",
   "pn": "526",
   "abstract": [
    "The need of a psyeholinguistic (PL) perspective for Speech Technologies (STs) is argued in the paper, toghether with the importance of selecting an adequate PL model, as a first requirement in this concern. A realistic PL model, which might really be efficient for STs, is presented (the \"contextual-dynamic model\" - Slama-Cazacu). Research on the role of veibal stimuli (VS) in confront with the nonverbal ones (NVS) in automatic factories and in laboratory simulated conditions demonstrated the greater efficiency (more rapid reaction time and coirect reactions) for VS, wherefrom application derives for computerized speech recognition and synthesis, and generally for a real communication situation m human activities involving computers (Cs). Other applications of the PL approach; to machine translation; for the identification of speakers; research on child language and education to be used in STs; PL analyses of \"reading/writing\" with C; PL research on the use of C as a \"supemedium\" in or for communication; the PL bases for an extension of the use of C with or for oral communication; decoding ihe new or fiiture \"hypertexts\" on the C's screen; the PL based arguments for \"multimedia\". Scientific research on the effects on the human being of the new STs are much advocated for.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-121"
  },
  "tsurumi94_icslp": {
   "authors": [
    [
     "Yutaka",
     "Tsurumi"
    ],
    [
     "Seiichi",
     "Nakagawa"
    ]
   ],
   "title": "An unsupervised speaker adaptation method for continuous parameter HMM by maximum a posteriori probability estimation",
   "original": "i94_0431",
   "page_count": 4,
   "order": 122,
   "p1": "431",
   "pn": "434",
   "abstract": [
    "We studied an unsupervised speaker adaptation method on the sequential training that used the theory of Maximum A Posteriori probability (MAP) estimation for continuous parameter hidden Markov model (HMM). In this method, we should only specify the syllable label sequence for the utterance. The label sequences were provided automatically by the recognizer using a speaker independent (or adapted) model in advance and the language model. The syllable recognition rate using a language model for a given task is expected to have an accuracy of more than 90%, even if we use a speaker independent (SI) model. The experimental results on continuous speech recognition, for sentence or syllable, showed that the better initial model gave a performance comparable to that of supervised adaptation.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-122"
  },
  "shinoda94_icslp": {
   "authors": [
    [
     "Koichi",
     "Shinoda"
    ],
    [
     "Takao",
     "Watanabe"
    ]
   ],
   "title": "Unsupervised speaker adaptation for speech recognition using demi-syllable HMM",
   "original": "i94_0435",
   "page_count": 4,
   "order": 123,
   "p1": "435",
   "pn": "438",
   "abstract": [
    "An unsupervised speaker adaptation method is proposed for application to a speaker independent recognition system which uses a demi-syllable based, continuous mixture-density HMM. The spectral interpolation technique, which has been used in supervised adaptation, and a scheme of utilizing recognition outputs of recognition system are employed. In an experimental application employing results obtained from a 5000-word Japanese vocabulary set recognition task, the error rate was reduced with the method from 15.5 % to 8.7 %, which is comparable to the error rate obtained by supervised adaptation. The method has also shown promise in the utilization of results obtained from a connected syllable recognition task in which the Japanese vocabulary set was unlimited.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-123"
  },
  "chou94_icslp": {
   "authors": [
    [
     "W.",
     "Chou"
    ],
    [
     "C.-E.",
     "Lee"
    ],
    [
     "Biing-Hwang",
     "Juang"
    ]
   ],
   "title": "Minimum error rate training of inter-word context dependent acoustic model units in speech recognition",
   "original": "i94_0439",
   "page_count": 4,
   "order": 124,
   "p1": "439",
   "pn": "442",
   "abstract": [
    "In this paper, we study the issues related to string level acoustic modeling in continuous speech recognition. A new approach based on the minimum string error rate criterion is proposed to the training of inter-word context dependent acoustic model units. Under the proposed approach, the inter-word context dependent acoustic model units are modeled at the global string level by directly applying the minimum string error rate based discriminative analysis to string level acoustic model matching. Experimental results indicate that a significant error rate reduction can be achieved through the proposed approach. Based on the proposed approach, the best performance obtained by a gender-independent model on the TI connected digit corpus is 0.24% word error rate and 0.72% string error rate.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-124"
  },
  "shen94_icslp": {
   "authors": [
    [
     "Jia-lin",
     "Shen"
    ],
    [
     "Hsin-min",
     "Wang"
    ],
    [
     "Ren-yuan",
     "Lyu"
    ],
    [
     "Lin-shan",
     "Lee"
    ]
   ],
   "title": "Incremental speaker adaptation using phonetically balanced training sentences for Mandarin syllable recognition based on segmental probability models",
   "original": "i94_0443",
   "page_count": 4,
   "order": 125,
   "p1": "443",
   "pn": "446",
   "abstract": [
    "This paper presents a new incremental speaker adaptation technique for isolated Mandarin syllables using four sets of phonetically balanced sentences. This algorithm was based on a newly developed Segmental Probability Model(SPM) which was found specially suitable for isolated Mandarin syllable recognition. Each Mandarin syllable is conventionally divided into INITIAL and FINAL parts and based on the INITIAL/FINAL structure of Mandarin syllables, a segment sharing concept is first proposed. A computer algorithm was then developed to select automatically four sets of phonetically balanced sentences with different selection criterion from a large Chinese text corpus. After the four-stage adaptation procedure, the recognition rate for a new speaker can be improved from 63.05% to 92.96%.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-125"
  },
  "fissore94_icslp": {
   "authors": [
    [
     "Lorenzo",
     "Fissore"
    ],
    [
     "Giorgio",
     "Micca"
    ],
    [
     "F.",
     "Ravera"
    ]
   ],
   "title": "Incremental training of a speech recognizer for voice dialling-by-name",
   "original": "i94_0447",
   "page_count": 4,
   "order": 126,
   "p1": "447",
   "pn": "450",
   "abstract": [
    "The present paper describes an incremental approach to the optimization of an HMM recognizer for a dialling-by-name task in a telephone environment. A Bayesian adaptive learning scheme was used to obtain domain-adapted models of sub-word speech units deriving from general, application independent models. The Maximum A Posteriori (MAP) theory was exploited in order to combine two sources of information, the Domain-Independent (DI) and the Domain-Dependent (DD) speech corpora. This approach was tested on a Voice Dialling-by-Name recognition task over the telephone in a speaker-independent mode. A nearly 92% recognition score was obtained for the best hypothesis using the optimal model adaptation procedure, whereas a 97% score has resulted by inclusion of the second best hypothesis.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-126"
  },
  "leggetter94_icslp": {
   "authors": [
    [
     "C. J.",
     "Leggetter"
    ],
    [
     "Phil C.",
     "Woodland"
    ]
   ],
   "title": "Speaker adaptation of continuous density HMMs using multivariate linear regression",
   "original": "i94_0451",
   "page_count": 4,
   "order": 127,
   "p1": "451",
   "pn": "454",
   "abstract": [
    "A method of speaker adaptation for continuous density mixture Gaussian HMMs is presented. A transformation for the component mixture means is derived by linear regression using a maximum likelihood optimisation criteria. The best use is made of the available adaptation data by defining equivalence classes of regression transforms and tying one regression matrix to a number of component mixtures. This allows successful adaptation on any amount of adaptation data. Tests on the RM1 database show that successful adaptation can be achieved with only 11 seconds of speech, and performance converges towards that of speaker dependent training as more adaptation data is used.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-127"
  },
  "ohkura94_icslp": {
   "authors": [
    [
     "Kazumi",
     "Ohkura"
    ],
    [
     "Hiroki",
     "Ohnishi"
    ],
    [
     "Masayuki",
     "Iida"
    ]
   ],
   "title": "Speaker adaptation based on transfer vectors of multiple reference speakers",
   "original": "i94_0455",
   "page_count": 4,
   "order": 128,
   "p1": "455",
   "pn": "458",
   "abstract": [
    "This paper addresses a speaker adaptation method for speaker-independent phoneme HMMs (SI-HMMs). The type of these HMMs is a Gaussian continuous mixture density HMM. It is important for speaker adaptation to make up for a deficiency of training data. In our proposed method, the TRAnsfer VEctors of multiple Reference Speakers estimated from sufficient training data are used to make up for a deficiency of the training data. We call this method TRAVERSE. TRAVERSE was evaluated by 100 isolated words recognition experiment. The testing speakers were five males. The average recognition rate for the five speakers was 80% with SI-HMMs. Applying TRAVERSE to SI-HMMs, the recognition rates increased 86.1% and 90.5% with 1 and 5 isolated words for speaker adaptation.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-128"
  },
  "strom94_icslp": {
   "authors": [
    [
     "Nikko",
     "Strom"
    ]
   ],
   "title": "Experiments with a new algorithm for fast speaker adaptation",
   "original": "i94_0459",
   "page_count": 4,
   "order": 129,
   "p1": "459",
   "pn": "462",
   "abstract": [
    "A new method for improving continuous speech recognition (CSR) using speaker adaptation is introduced. The proposed method is unsupervised - no prior information about the speaker is used and there is no training phase involved. The effect of speaker adaptation is achieved by assuming that a set of speaker parameters is constant over the utterance. Both the acoustic information and the speaker parameters condition the phoneme classification. This approach makes it possible to search for the optimal speaker parameters and the optimal phoneme sequence in a unified optimisation procedure. The general method is tested using speech recognition based on a segmental artificial neural network (ANN). The results from this first test are reported. The performance on the utterance level, measured as the position of the correct phoneme string in the sorted list of highest scoring hypotheses (N-best list) is improved even for very short utterances. The contribution to phoneme classification performance is positive only when recognising long utterances.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-129"
  },
  "chiang94_icslp": {
   "authors": [
    [
     "Tung-Hui",
     "Chiang"
    ],
    [
     "Yi-Chung",
     "Lin"
    ],
    [
     "Keh-Yih",
     "Su"
    ]
   ],
   "title": "A study of applying adaptive learning to a multi-module system",
   "original": "i94_0463",
   "page_count": 4,
   "order": 130,
   "p1": "463",
   "pn": "466",
   "abstract": [
    "Adaptive learning procedures have been widely used in speech recognition tasks for minimizing system error rate. However, they were usually applied to a single module. For many applications, systems are composed of multiple modules. A variety of learning strategies, including isolated, incremental and joint learning, therefore, are proposed in this paper to train the parameters of a multi-module system. We have employed these learning procedures to Chinese phonetic typewriter systems for recognizing 1,000 sentences in the speaker-independent, isolated character input mode, with a very large vocabulary of 90,495 words. According to the simulation, the isolated learning strategy is basically unable to achieve the best system performance. It even deteriorates the performance in some cases. In general, the joint learning procedure is recommended for its potential superiority. However, the joint learning strategy cannot show its benefit if the later modules are modeled in terms of a large number of parameters. In this circumstance, the incremental learning procedure is suggested.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-130"
  },
  "nakahashi94_icslp": {
   "authors": [
    [
     "Jun'ichi",
     "Nakahashi"
    ],
    [
     "Eiichi",
     "Tsuboka"
    ]
   ],
   "title": "Speaker adaptation based on fuzzy vector quantization",
   "original": "i94_0467",
   "page_count": 4,
   "order": 131,
   "p1": "467",
   "pn": "470",
   "abstract": [
    "In this paper, we propose the following four speaker adaptation methods based on Fuzzy Vector Quantization (FVQ), Supervised speaker adaptation based on maximization of likelihood from HMM. Supervised speaker adaptation based on minimization of fuzzy objective function weighted by path probabilities from HMM. Simplified unsupervised speaker adaptation based on minimization of fuzzy objective function. Successive unsupervised speaker adaptation based on minimization of fuzzy objective function.\n",
    "All these methods adapt the reference codebook using adaptation vector, estimated for a certain criterion. The experiments show that all these methods have better performance in comparison with speaker-independent isolated word recognition, when applied to FVQ/HMM. In particular unsupervised speaker adaptation shows that the word recognition rate for \"outlier\" speakers has been improved from 84.2 % to 88.9 % using 5 adaptation words.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-131"
  },
  "kong94_icslp": {
   "authors": [
    [
     "Myung-Kwang",
     "Kong"
    ],
    [
     "Seong-Kwon",
     "Lee"
    ],
    [
     "Soon-Hyob",
     "Kim"
    ]
   ],
   "title": "A study on the simulated annealing of self organized map algorithm for Korean phoneme recognition",
   "original": "i94_0471",
   "page_count": 4,
   "order": 132,
   "p1": "471",
   "pn": "474",
   "abstract": [
    "In this paper, we describe the new unsupervised learning algorithm, SASOM(Simulated Annealing Self Organized Map). It can solve the defects of the conventional SOM(Self-Organized Map) that the state of network can't converge to the minimum point. The proposed algorithm uses the object function which can evaluate the state of network in learning and adjusts the learning rate flexibly according to the evaluation of the object function. We implement the simulated annealing which is applied to the conventional network using the object function and the learning rate. Finally, the proposed algorithm can make the state of network converged to the global minimum. Using the two-dimensional input vectors with uniform distribution, we graphically compared the ordering ability of SOM with that of SASOM. We carried out the recognition on the new algorithm for all Korean phonemes and some continuous speech.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-132"
  },
  "torre94_icslp": {
   "authors": [
    [
     "Celinda de la",
     "Torre"
    ],
    [
     "Alejandro",
     "Acero"
    ]
   ],
   "title": "Discriminative training of garbage model for non-vocabulary utterance rejection",
   "original": "i94_0475",
   "page_count": 4,
   "order": 133,
   "p1": "475",
   "pn": "478",
   "abstract": [
    "The aim of this paper is to describe a novel application of the Discriminative Training (DT) procedure for non-keyword rejection based on the principle of minimizing a weighted error criterion. This technique is applied to our Keyword Recognition System, a speaker-independent semicontinuous Density Hidden Markov Model (SCDHMM) recognizer. The proposed algorithm is evaluated for two different isolated word recognition tasks on telephone-line recordings containing both keywords and non-keywords utterances. We will compare the results with those obtained with Maximum Likelihood Estimation (MLE). In the Rejection Application the proposed procedure offers an automatic way of tunning the parameters for the desired application and a decrease on the Cost Function. Good results have been also obtained with the Discriminative Trained Garbage Model in the Word- Spotting Application.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-133"
  },
  "vatikiotisbateson94_icslp": {
   "authors": [
    [
     "Eric",
     "Vatikiotis-Bateson"
    ],
    [
     "Inge-Marie",
     "Eigsti"
    ],
    [
     "Sumio",
     "Yano"
    ]
   ],
   "title": "Listener eye movement behavior during audiovisual speech perception",
   "original": "i94_0527",
   "page_count": 4,
   "order": 134,
   "p1": "527",
   "pn": "530",
   "abstract": [
    "Results are presented for two experiments in which listener eye movements were recorded during video presentations of extended monologues. Monologues were presented with various levels of acoustic masking noise. The first experiment verified that the presence of visual stimuli enhanced intelligibility of the audiovisual stimuli. The second experiment showed a clear tendency for listeners of both Japanese and English to move their eyes less as perceptual difficulty increased, even when the image size of the stimuli was many times larger than life-size. Furthermore, preliminary results are discussed concerning the role of the eyes and mouth as targets for visual cues to speech communication, and an hypothesis for audiovisual enhancement of perception is discussed that integrates auditory localization and tracking of visual cues to speech.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-134"
  },
  "massaro94_icslp": {
   "authors": [
    [
     "Dominic W.",
     "Massaro"
    ],
    [
     "Michael M.",
     "Cohen"
    ]
   ],
   "title": "Auditory/visual speech in multimodal human interfaces",
   "original": "i94_0531",
   "page_count": 4,
   "order": 135,
   "p1": "531",
   "pn": "534",
   "abstract": [
    "It has long been a hope, expectation, and prediction that speech would be the primary medium of communication between humans and machines. To date, this dream has not been realized. We predict that exploiting the multimodal nature of spoken language will facilitate the use of this medium. We begin our paper with a general frame-work for the analysis of speech recognition by humans and a theoretical model. We then present a system for auditory/visual speech synthesis that performs complete text-to-speech synthesis. This system should improve the quality as well as the attractiveness of speech as one of a machine's primary output communication medium. Mirroring the value of multimodal speech synthesis, multimodal channels Should also enhance speech recognition by machine.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-135"
  },
  "kondo94c_icslp": {
   "authors": [
    [
     "Tadahisa",
     "Kondo"
    ],
    [
     "Kazuhiko",
     "Kakehi"
    ]
   ],
   "title": "Effects of phonological and semantic information of kanji and kana characters on speech perception",
   "original": "i94_0535",
   "page_count": 4,
   "order": 136,
   "p1": "535",
   "pn": "538",
   "abstract": [
    "We investigate the effects of visually presented words and non-words on the auditory perception of phonemes. The subjects in our experiments heard four-mora Japanese words and non-words, in which the third mora was partially or totally obscured by noise. The subjects' task was to perceive the missing or degraded mora. Concurrent with the auditory presentation, a word or non-word written in Kana (the Japanese syllabary) or Kanji (Chinese characters) was presented visually. The results confirm that visually presented words and non-words influence phoneme perception in auditory words and non-words. In cases where there is more than one candidate for the perceived phoneme due to ambiguity of auditory processing, visual information is used to decide the most appropriate phoneme, if the visual and auditory stimuli are related to each other either phonologically or semantically. These results can be explained by a model of auditory and visual word perception.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-136"
  },
  "kuhl94_icslp": {
   "authors": [
    [
     "Patricia K.",
     "Kuhl"
    ],
    [
     "Minoru",
     "Tsuzaki"
    ],
    [
     "Yoh'ichi",
     "Tohkura"
    ],
    [
     "Andrew N.",
     "Meltzoff"
    ]
   ],
   "title": "Human processing of auditory-visual information in speech perception: potential for multimodal human-machine interfaces",
   "original": "i94_0539",
   "page_count": 4,
   "order": 137,
   "p1": "539",
   "pn": "542",
   "abstract": [
    "Speech perception is not a unimodal process. Observers who see and hear a talker take both auditory and visual information into account in determining what the talker said. This is best illustrated by experiments in which discrepant speech information is delivered to the two modalities. In this situation, observers perceive neither the syllable sent to the auditory modality nor the syllable sent to the visual modality, but a combination of the two. Recent research in our laboratories has established two additional facts. First, the auditory-visual effect occurs in both American and Japanese subjects. Moreover, our studies show that the effect interacts with the language spoken by the talker one watches. Japanese subjects show significantly stronger auditory-visual effects when watching a foreign-language speaker than when watching a native- language speaker. In American subjects, this difference is less pronounced. Second, in investigating auditory-visual effects, we have found that minimal visual information is necessary to produce auditory-visual effects. Data from human observers have implications for human-machine interfaces that utilize multimedia technology.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-137"
  },
  "pentland94_icslp": {
   "authors": [
    [
     "Alex P.",
     "Pentland"
    ],
    [
     "Trevor",
     "Darrell"
    ]
   ],
   "title": "Visual perception of human bodies and faces for multi-modal interfaces",
   "original": "i94_0543",
   "page_count": 4,
   "order": 138,
   "p1": "543",
   "pn": "546",
   "abstract": [
    "In this paper we describe recent work in our laboratory on the use of computer vision techniques for real-time multi-modal interfaces. The methods described here allow the non-invasive perception of human users; no special markers or identifying features are assumed. Both user-independent and user-dependent algorithms for gesture recognition are used, depending on the context. We apply the same techniques used for recognition to the problem of generation of animated forms to accompany spoken language. Both realtime recognition and animation of facial gestures (e.g., a lip-synched \"talking head\") have been implemented within our framework.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-138"
  },
  "duchnowski94_icslp": {
   "authors": [
    [
     "Paul",
     "Duchnowski"
    ],
    [
     "Uwe",
     "Meier"
    ],
    [
     "Alex",
     "Waibel"
    ]
   ],
   "title": "See me, hear me: integrating automatic speech recognition and lip-reading",
   "original": "i94_0547",
   "page_count": 4,
   "order": 139,
   "p1": "547",
   "pn": "550",
   "abstract": [
    "We present recent work on integration of visual information (automatic lip-reading) with acoustic speech for better overall speech recognition. A Multi-State Time Delay Neural Network performs the recognition of spelled letter sequences taking advantage of lip images from a standard camera. The problems addressed include efficient but effective representation of the visual information and optimum manner of combining the two modalities when rendering a decision. We show results for several alternatives to direct gray level image as the visual evidence. These are: Principal Components, Linear Discriminants, and DFT coefficients. Dimensionality of the input is decreased by a factor of 12 while maintaining recognition rates. Combination of the visual and acoustic information is performed at three different levels of abstraction. Results suggest that integration of higher order input features works best. On a continuous spelling task, visual-alone recognition of 45-55%, when combined with acoustic data, lowers audio-alone error rates by 30-40%.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-139"
  },
  "oviatt94_icslp": {
   "authors": [
    [
     "Sharon",
     "Oviatt"
    ],
    [
     "Erik",
     "Olsen"
    ]
   ],
   "title": "Integration themes in multimodal human-computer interaction",
   "original": "i94_0551",
   "page_count": 4,
   "order": 140,
   "p1": "551",
   "pn": "554",
   "abstract": [
    "This research examines how people integrate spoken and written input during multimodal human-computer interaction. Three studies used a semi-automatic simulation technique to collect data on people's free use of spoken and written input. Within-subject repeated-measures studies were designed, with data analyzed from 44 subjects and 240 tasks. The primary factors were evaluated that govern people's selection to write versus speak at given points during a human-computer exchange. Analyses revealed that people write digits more often than textual content, and proper names more often than other text. A fonn-kased presentation, in comparison with an unconstrained format, also increased the likelihood of writing. However, the most influential factor in patterning people's integrated use of speech and writing is contractive functionalityr, or the use of spoken and written input in a contrastive way to designate a shift in content or functionality, such as original versus corrected input, data versus command, and digits versus text. Different patterns of contrastive mode use accounted for approximately 57% of the integrated pen/voice use observed in these studies. Information also is summarized on preferential mode use, and simultaneity of pen/voice input. One long-term goal of this research is the development of quantitative predictive models of natural modality integration, which could provide guidance on the strategic design of robust multimodal systems.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-140"
  },
  "berkley94_icslp": {
   "authors": [
    [
     "D. A.",
     "Berkley"
    ],
    [
     "James L.",
     "Flanagan"
    ],
    [
     "K. L.",
     "Shipley"
    ],
    [
     "Lawrence R.",
     "Rabiner"
    ]
   ],
   "title": "A multimodal teleconferencing system using hands-free voice control",
   "original": "i94_0555",
   "page_count": 4,
   "order": 141,
   "p1": "555",
   "pn": "558",
   "abstract": [
    "This talk describes the design and implementation of a digital teleconferencing system that seamlessly integrates a number of speech and image processing technologies together with data transmission capability. The goal is to provide a variety of sophisticated communication features that are easy to learn and fairly natural to use. The system is called HuMaNet, for Human/Machine Network. The system is con- trolled totally and interactively in a hands-free manner using spoken commands. The HuMaNet system combines the technologies of speech coding, speech recognition, text-to-speech synthesis, and talker verification with autodirective microphone arrays, image compression, data and hypertext management to provide high-quality audio, image and video conferencing over basic-rate ISDN (Integrated Services Digital Network). The present public-switched transport capacity provides \"2B-f-D\", or two 64 kbits/sec circuit-switched channels (2B), and one 16 kbits/sec packet-switched channel (D). For teleconferencing, the audio can be coded using a wideband (7 kHz) speech coder at 32 kbps, and the video is coded at 128 kbps using a commercial video conferencing codec. Image compression is used to transmit still images at rates of 64-128 kbps (using 2D-subband coding). Adaptive camera control, for group teleconferencing, can be achieved using the speech detection capability of the autodirective microphone array, thereby automatically and rapidly repositioning the camera for new talkers in a group teleconference.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-141"
  },
  "bertelson94_icslp": {
   "authors": [
    [
     "Paul",
     "Bertelson"
    ],
    [
     "Jean",
     "Vroomen"
    ],
    [
     "Geert",
     "Wiegeraad"
    ],
    [
     "Beatrice de",
     "Gelder"
    ]
   ],
   "title": "Exploring the relation between mcgurk interference and ventriloquism",
   "original": "i94_0559",
   "page_count": 4,
   "order": 142,
   "p1": "559",
   "pn": "562",
   "abstract": [
    "An experiment is reported in which, on each trial, an ambiguous fragment of auditory speech was delivered on one of an array of hidden loudspeakers, and a face, either upright or upside-down, was shown on a centrally located screen, and articulated one of two different utterances or remained still. Subjects pointed to the apparent origin of the speech sounds and reported what had been said. Identification responses were strongly influenced by the nature of the seen movements (McGurk effect [1]) and localisation of laterally heard items was shifted toward the centrally located video monitor on trials with a moving face compared to those with a still face (ventriloquism [2,3]). Degree of McGurk interference was practically independent of spatial separation between auditory and visual source. Face inversion had no effect on ventriloquism, but it reduced the McGurk effect. This experimental dissociation suggests that the two phenomena originate in different components of the cognitive architecture.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-142"
  },
  "junqua94b_icslp": {
   "authors": [
    [
     "Jean-Claude",
     "Junqua"
    ],
    [
     "Philippe",
     "Morin"
    ]
   ],
   "title": "Naturalness of the interaction in multimodal applications",
   "original": "i94_0563",
   "page_count": 4,
   "order": 143,
   "p1": "563",
   "pn": "566",
   "abstract": [
    "In this paper we focus on the naturalness of the interaction in multimodal dialogue applications. After presenting some characteristics useful to assess naturalness in dialogue applications, we concentrate on the description of several techniques aiming at improving naturalness. We first describe a general algorithm to represent a contextual sub-language and briefly mention how this algorithm can be applied to word prediction and language acquisition for non-frequent users. Then, we focus on several mechanisms aimed at a more human-like interaction. We also report on the integration of these techniques in the multimodal dialogue system PARTNER currently under development.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-143"
  },
  "ando94_icslp": {
   "authors": [
    [
     "Haru",
     "Ando"
    ],
    [
     "Yoshinori",
     "Kitahara"
    ],
    [
     "Nobuo",
     "Hataoka"
    ]
   ],
   "title": "Evaluation of multimodal interface using spoken language and pointing gesture on interior design system",
   "original": "i94_0567",
   "page_count": 4,
   "order": 144,
   "p1": "567",
   "pn": "570",
   "abstract": [
    "This paper describes evaluation results of a multimodal interface using speech and pointing gestures. We have developed an \"Interior Design System\" as a prototype for evaluating multimodal interfaces. Experiments assess the effectiveness of the proposed multimodal interface and investigate desirable specifications of the interface. These experiments have been performed by two different conditions, first on the prototype which has a speech input processing unit, and second by a \"Wizard of OZ\" method. Through these experiments, we have compared multimodal interfaces with unimodal interfaces, and compared command utterances with sentence utterances to check the best means for speech input by users. From the results, we have confirmed that the proposed multimodal interface is effective, and that command utterances are better than sentence utterances on the condition of using the current speech recognition technology.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-144"
  },
  "lokenkim94_icslp": {
   "authors": [
    [
     "Kyung-ho",
     "Loken-Kim"
    ],
    [
     "Fumihiro",
     "Yato"
    ],
    [
     "Laurel",
     "Fais"
    ],
    [
     "Tsuyoshi",
     "Morimoto"
    ],
    [
     "Akira",
     "Kurematsu"
    ]
   ],
   "title": "Linguistic and paralinguistic differences between multimodal and telephone-only dialogues",
   "original": "i94_0571",
   "page_count": 4,
   "order": 145,
   "p1": "571",
   "pn": "574",
   "abstract": [
    "The results of a pilot study comparing linguistic behavior of speakers in telephone-only and multimodal communicative environments, are reported. The subjects, native speakers of Japanese, conducted goal-oriented conversations in each environment. Linguistic differences (disfluency rates, and use of intention types, syntactic structures, and deictic expressions) and paralinguistic differences (ease of use, and utilization of media) are discussed. Suggestions are made for incorporating these findings into the design of multimodal spoken language interpretation systems.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-145"
  },
  "rose94_icslp": {
   "authors": [
    [
     "R. C.",
     "Rose"
    ],
    [
     "Juergen",
     "Schroeter"
    ],
    [
     "M. Mohan",
     "Sondhi"
    ]
   ],
   "title": "An investigation of the potential role of speech production models in automatic speech recognition",
   "original": "i94_0575",
   "page_count": 4,
   "order": 146,
   "p1": "575",
   "pn": "578",
   "abstract": [
    "This paper investigates the issues that are associated with applying speech production models to automatic speech recognition (ASR). The applicability of articulatory representations to ASRis considered here independent of the role of articulatory representations in speech perception. While the issue of whether it is necessary or even possible for human listeners to recover the state of the articulators during the process of perceiving speech is an important one, it is not considered here. Hence, we refrain from posing completely new paradigms for ASR which more closely parallel the relationship between speech production and human speech understanding. Instead, we describe work aimed at integrating speech production models into existing ASR formalisms.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-146"
  },
  "kaburagi94_icslp": {
   "authors": [
    [
     "Tokihiko",
     "Kaburagi"
    ],
    [
     "Masaaki",
     "Honda"
    ]
   ],
   "title": "A trajectory formation model of articulatory movements based on the motor tasks of phoneme-specific vocal tract shapes",
   "original": "i94_0579",
   "page_count": 4,
   "order": 147,
   "p1": "579",
   "pn": "582",
   "abstract": [
    "This paper presents a study of modeling articulatory movements for continuous speech utterances. Our model determines the trajectory of the articulatory movements so that it can form the vocal tract into phoneme-specific shapes. It accounts for the degrees-of-freedom problem in forming specific vocal tract shapes, and redundancies in the articulation system are solved by minimizing an appropriate objective function so that articulatory movement is uniquely determined. Articulatory movements were simulated for a phoneme sequence, and the results were compared with measured data.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-147"
  },
  "george94_icslp": {
   "authors": [
    [
     "Martine",
     "George"
    ],
    [
     "Paul",
     "Jospa"
    ],
    [
     "Alain",
     "Soquet"
    ]
   ],
   "title": "Articulatory trajectories generated by the control of the vocal tract by a neural network",
   "original": "i94_0583",
   "page_count": 4,
   "order": 148,
   "p1": "583",
   "pn": "586",
   "abstract": [
    "In this paper, we present a method of generating articulatory trajectories from formant transitions. A multilayer feedforward neural network controls a vocal tract model on the basis of its first three resonances (the formants) in order to compute 8 articulatory parameters (the cross sectional areas of 7 regions of the Distinctive Regions Model and the vocal tract length). 4 learning modes have been studied and compared: autonomous, supervised, hybrid and supervised-hybrid. After training in one of the 4 modes, the neural network is able to generate articulatory configurations. Each formant triplet time sample is successively introduced into the network. For each triplet, a local optimisation is performed by the neural network in an autonomous learning mode. Several articulatory trajectories are presented.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-148"
  },
  "hirayama94_icslp": {
   "authors": [
    [
     "Makoto",
     "Hirayama"
    ],
    [
     "Eric",
     "Vatikiotis-Bateson"
    ],
    [
     "Vincent",
     "Gracco"
    ],
    [
     "Mitsuo",
     "Kawato"
    ]
   ],
   "title": "Neural network prediction of lip shape from muscle EMG in Japanese speech",
   "original": "i94_0587",
   "page_count": 4,
   "order": 149,
   "p1": "587",
   "pn": "590",
   "abstract": [
    "Lip movements during utterances of Japanese short sentences were predicted from orofacial muscle activity (EMG), using artificial neural network models. Inputs to the network model were EMG signals for six orofacial muscles. The lip movement parameters for output of the model were the horizontal distance between the corners of the mouth and the distance between the midsagittal lower lip and jaw markers. In addition to the relation between muscle EMG and articulator motion, the network learned the shape of a time-delay filter. Comparison of position, velocity, and acceleration prediction networks showed that the position prediction network performed best at recovering lip movement.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-149"
  },
  "hiraike94_icslp": {
   "authors": [
    [
     "Masahiro",
     "Hiraike"
    ],
    [
     "Shigehisa",
     "Shimizu"
    ],
    [
     "Takao",
     "Mizutani"
    ],
    [
     "Kiyoshi",
     "Hashimoto"
    ]
   ],
   "title": "Estimation of the lateral shape of a tongue from speech",
   "original": "i94_0591",
   "page_count": 4,
   "order": 150,
   "p1": "591",
   "pn": "594",
   "abstract": [
    "A neural network was used to estimate the lateral shape of a tongue from speech. An experimental result indicates the PARCOR coefficients as being the best parameter. The average error attained in the estimation is 1.2 mm in rms. For speaker adaptation, ultrasonic image of the dorsal part of the tongue was used and the result verifies its effectiveness.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-150"
  },
  "jospa94_icslp": {
   "authors": [
    [
     "Paul",
     "Jospa"
    ],
    [
     "Alain",
     "Soquet"
    ]
   ],
   "title": "The acoustic-articulatory mapping and the variational method",
   "original": "i94_0595",
   "page_count": 4,
   "order": 151,
   "p1": "595",
   "pn": "598",
   "abstract": [
    "If work carried out in the framework of acoustic-articulatory inversion is taken into consideration, the usefulness of having a fast and accurate method to compute both the formant frequencies and the Jacobian matrix of the articulatory-acoustic transformation immediately becomes evident. For this purpose, a variational formulation of the resonance modes in the vocal tract is presented. This formulation is able to handle a labial condition that fits the opened as well as the closed tract at the lips, and at the same time gives us analytical expressions for the sensitivity functions, from which the Jacobian matrix can be deduce.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-151"
  },
  "pelorson94_icslp": {
   "authors": [
    [
     "X.",
     "Pelorson"
    ],
    [
     "T.",
     "Lallouache"
    ],
    [
     "S.",
     "Tourret"
    ],
    [
     "C.",
     "Bouffartigue"
    ],
    [
     "Pierre",
     "Badin"
    ]
   ],
   "title": "Aerodynamical, geometrical and mechanical aspects of bilabial plosives production",
   "original": "i94_0599",
   "page_count": 4,
   "order": 152,
   "p1": "599",
   "pn": "602",
   "abstract": [
    "In a attempt to characterise the production of stop consonants, this paper presents a comprehensive study of voiceless bilabial plosive sounds production. The double issue of this research will be to develop realistic models to be integrated in articulatory models, and to provide means of studying control strategies for the command of these models. Geometrical and acoustical characterisation of bilabial plosives is achieved by means of a non-invasive method using video monitoring. The experimental system used makes possible to automatically extract parameters such as lip height, lip width, and intralabial lip area for front pictures, and of upper and lower lip protrusions for profile pictures. In order to catch more details of the upper and lower lip movements, the system has been extended to use a high speed camera providing 400 fields per second. This experimental technique also allows simultaneous aerodynamical measurements as well as some simple but illustrative unsteady-flow visualizations (using a smoke method). We then present a simple model for plosive sounds production. In this model, the mechanics of the upper and lower lips is represented by a mass-springs model while the time-varying geometry of the lips is described using a simple but realistic three-dimensional shape. Lastly the behaviour of the flow through the lips is described using a quasi stationary model which includes-the.effects of flow separation point. Some first simulation results will be shown, and compared with the experimental measurements and visualisations.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-152"
  },
  "dang94_icslp": {
   "authors": [
    [
     "Jianwu",
     "Dang"
    ],
    [
     "Kiyoshi",
     "Honda"
    ]
   ],
   "title": "Investigation of the acoustic characteristics of the velum for vowels",
   "original": "i94_0603",
   "page_count": 4,
   "order": 153,
   "p1": "603",
   "pn": "606",
   "abstract": [
    "Results obtained from a previous investigation of the acoustic characteristics of the velum using separate, simultaneous measurements of lip and nostril radiations suggested that nasal radiation was caused by the vibration of the velum. The current study used measurements of intraoral and intranasal sound pressures and velar movements to investigate the acoustic characteristics of the velum during vowel production. To observe changes in the acoustic properties of the velum, the transfer ratio of the section spanning the velum was analyzed and compared for nasalized vowels, non-nasalized vowels and nasal consonants. The results showed that the transfer ratio is larger in closed vowels than it is in open vowels.- The effect of the velum vibration on the transfer ratio was observed not only in non-nasalized vowels, but also in nasalized vowels. There is a consistent relationship between the height of the velar position, the transfer ratio and the nasal sound in a vowel section. Specifically, there is a higher velar position and larger transfer ratio and nasal sound for closed vowels, and a lower velar position and smaller transfer ratio and nasal sound for open vowels. The degree of velopharyngeal opening is strongly correlated to the stress position of a sentence. The velopharyngeal port opens more widely in the case of a nasal consonant following a syllable with a stress than it does in other cases.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-153"
  },
  "motoki94_icslp": {
   "authors": [
    [
     "Kunitoshi",
     "Motoki"
    ],
    [
     "Pierre",
     "Badin"
    ],
    [
     "Nobuhiro",
     "Miki"
    ]
   ],
   "title": "Measurement of acoustic impedance density distribution in the near field of the labial horn",
   "original": "i94_0607",
   "page_count": 4,
   "order": 154,
   "p1": "607",
   "pn": "610",
   "abstract": [
    "In this paper, a method for measuring the acoustic impedance density from sound pressure distribution is presented. Experiments were performed for a plaster replica of the lips together with the oral cavity and for a quasi-elliptical uniform tube with a wedged-shaped end imitating the lip horn. The change of frequency characteristics of the acoustic impedance density along the center-line inside the lips indicates that the labial horn acts as an acoustic wave guide. The theoretical characteristics of the acoustic impedance density derived from an idealized elliptical piston model and a pulsating sphere model are also shown for the better determination of the boundary condition for FEM computation.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-154"
  },
  "schoentgen94_icslp": {
   "authors": [
    [
     "Jean",
     "Schoentgen"
    ],
    [
     "S.",
     "Ciocea"
    ]
   ],
   "title": "Explicit relations between resonance frequencies and vocal tract cross sections in loss-less kelly-lochbaum and distinctive region vocal tract models",
   "original": "i94_0611",
   "page_count": 4,
   "order": 155,
   "p1": "611",
   "pn": "614",
   "abstract": [
    "The Kelly-Lochbaum (KL) [2] and Distinctive Region (DRM) [4] vocal tract models are concatenations of uniform loss-less tubelets of equal and unequal lengths respectively. The transfer matrixes of these models lead to resonance conditions that are polynomials F(x,Sj) in x = cos(2pirfl/c)f with / as the tubelet length, f the resonance frequency, c the sound speed and Sj the vocal tract cross sections. Usually, finding the resonance frequencies consists of numerically searching for the roots of F(x, Sj) and taking their arccosine to isolate the frequencies. In this article, we explore two methods for arriving instead at an explicit link fi = fi(Sj) between the cross sections Sj and resonance frequencies fi in the framework of KL and DRM vocal tract models. We show the following. Firstly, equation F(fi,Sj) = 0 can be solved analyti- cally provided that the number of tubelets is less than, or equal to, nine and provided that they are of equal length. Secondly, approximate analytical solutions of equation F(fi, Sj) = 0 can be obtained by representing the relations between log(fi) and log(Sj) by means of a Taylor expansion of the implicitly defined function F(fi(Sj), Sj) = 0. The results show that these approximations are valid for cross sections that typically vary between 1 or 2 cm2 and 10 cm2. Approximate analytical solutions of F(fi(Sj),Sj) = 0 can be arrived at for models with an arbitrary number of tubelets of unequal length.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-155"
  },
  "valimaki94_icslp": {
   "authors": [
    [
     "Vesa",
     "Välimäki"
    ],
    [
     "Matti",
     "Karjalainen"
    ]
   ],
   "title": "Improving the kelly-lochbaum vocal tract model using conical tube sections and fractional delay filtering techniques",
   "original": "i94_0615",
   "page_count": 4,
   "order": 156,
   "p1": "615",
   "pn": "618",
   "abstract": [
    "An articulatory model of speech production is usually constructed by approximating the profile of the vocal tract using cylindrical tube sections. This is implemented by a digital ladder filter that is called the Kelly-Lochbaum model. In this paper we propose an extended approach, where the tube sections approximating the profile of the tract are conical instead of cylindrical. Furthermore, the length of each tube section in our model can be accurately controlled using a novel fractional delay filtering scheme. These refinements result in an accurate and intuitively controllable vocal tract model that is well suited for articulatory speech synthesis.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-156"
  },
  "matsumura94_icslp": {
   "authors": [
    [
     "Masafumi",
     "Matsumura"
    ],
    [
     "Takuya",
     "Nukawa"
    ],
    [
     "Koji",
     "Shimizu"
    ],
    [
     "Yasuji",
     "Hashimoto"
    ],
    [
     "Tatsuya",
     "Morita"
    ]
   ],
   "title": "Measurement of 3d shapes of vocal tract, dental crown and nasal cavity using MRI: vowels and fricatives",
   "original": "i94_0619",
   "page_count": 4,
   "order": 157,
   "p1": "619",
   "pn": "622",
   "abstract": [
    "This paper deals with the measurement of three dimensional shapes of vocal tract, dental crown and nasal cavity using magnetic resonance imaging (MRI). 3D MR images of the vocal tract and nasal cavity of 3 adult males were obtained in the steady state productions of Japanese vowels and fricatives. MR images of the dental crown that contain a small amount of water were obtained using a dental crown plate that is shaped so as to tightly attach to the subject's dental crown by thermoforming and that contain a contrast medium for MR imaging. The measurement time was 186 second for 28 transverse sections from the larynx to the nasal cavity at a 6 mm interval, and was 186 second for 32 coronal sections from the tip of the nose to the atlas at a 4 mm interval. A computer algorithm for air-tissue boundary tracing of arbitrary sections of the vocal tract and nasal cavity from the 3D MR image was proposed. The algorithm is based on the gray level interpolation and the threshold operation. Images of arbitrary sections of the vocal tract and nasal cavity from 3D MR image were reconstructed using the gray level interpolation based on the sampling theorem. Thresholds of the gray level were computed from gray levels at air-tissue border points. Shapes of arbitrary sections of the vocal tract and nasal cavity were obtained using the present algorithm. The boundary tracing error of the dental crown from the 3D MR image was less than 5.7%, The 3D shape and area function of vocal tract during the productions of fricatives /s/ were obtained. The 3D shapes of sinus frontails, sinus spenoidalis and sinus maxillaris were observed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-157"
  },
  "yang94_icslp": {
   "authors": [
    [
     "Chang-Sheng",
     "Yang"
    ],
    [
     "Hideki",
     "Kasuya"
    ]
   ],
   "title": "Accurate measurement of vocal tract shapes from magnetic resonance images of child, female and male subjects",
   "original": "i94_0623",
   "page_count": 4,
   "order": 158,
   "p1": "623",
   "pn": "626",
   "abstract": [
    "We have developed an accurate method to measure vocal tract (VT) shape and length from magnetic resonance (MR) images acquired during sustained phonation of Japanese vowels. The 3-dimensional (3D) VT shape was reconstructed by using coronal MR images for the oral cavity and axial MR images for the pharyngeal and glottal regions. A mid-sagittal image was used as a reference for the reconstruction. MR images of dental impressions of a subject were also incorporated into the reconstruction. All the MR images were directly transferred to a workstation where an intercative program was used to measure the VT shapes. Three Japanese subjects, a child, a female and a male, participated in the experiment. Formant frequencies were computed from the 3D VT shapes which were measured for the three subjects assuming one-dimensional sound wave propagation through the vocal tract. The first three formant frequencies were compared with the ones measured directly from real voice. Except for a few cases, differences between the two measurements were all less than the difference limen (DL) of the formant frequencies.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-158"
  },
  "narayanan94_icslp": {
   "authors": [
    [
     "Shrikanth",
     "Narayanan"
    ],
    [
     "Abeer",
     "Alwan"
    ],
    [
     "Katherine",
     "Haker"
    ]
   ],
   "title": "An MRI study of fricative consonants",
   "original": "i94_0627",
   "page_count": 4,
   "order": 159,
   "p1": "627",
   "pn": "630",
   "abstract": [
    "Magnetic resonance images of the vocal tract during sustained production of the fricatives /s, ?, f, ?, z, ?, v, ?/ by four subjects are analyzed. Measurements of vocal tract lengths and area functions and analysis of the tongue shapes for different sounds are presented. Inter-speaker differences in area functions are found to be greater in the pharyngeal cavity than in the buccal cavity with the nonstrident fricatives exhibiting greater differences than the strident ones. The concave tongue shapes of /s/ and /z/ result in a more abrupt area function behind the constriction when compared to that of /?/ and /?/, Voiced fricatives show larger pharyngeal volumes than the unvoiced fricatives due to tongue-root retraction.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-159"
  },
  "vatikiotisbateson94b_icslp": {
   "authors": [
    [
     "Eric",
     "Vatikiotis-Bateson"
    ],
    [
     "Mark K.",
     "Tiede"
    ],
    [
     "Y.",
     "Wada"
    ],
    [
     "Vincent",
     "Gracco"
    ],
    [
     "Mitsuo",
     "Kawato"
    ]
   ],
   "title": "Phoneme extraction using via point estimation of real speech",
   "original": "i94_0631",
   "page_count": 4,
   "order": 160,
   "p1": "631",
   "pn": "634",
   "abstract": [
    "In order to complete a computational model of speech production incorporating the relations between neuromotor, biomechanical and aerodynamic levels of description, a means for assigning linguistically relevant input strings is needed. This paper describes a two-phase curve-fitting and template matching method for mapping between phonemes and articulator movement behavior based on via point estimation. Among other things via point estimation provides a good means of data compression and the estimation software has proved to be an excellent empirical tool.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-160"
  },
  "matsuzaki94_icslp": {
   "authors": [
    [
     "Hiroki",
     "Matsuzaki"
    ],
    [
     "Nobuhiro",
     "Miki"
    ],
    [
     "Nobuo",
     "Nagai"
    ],
    [
     "Tohru",
     "Hirohku"
    ],
    [
     "Yoshihiko",
     "Ogawa"
    ]
   ],
   "title": "3d FEM analysis of vocal tract model of elliptic tube with inhomogeneous-wall impedance",
   "original": "i94_0635",
   "page_count": 4,
   "order": 161,
   "p1": "635",
   "pn": "638",
   "abstract": [
    "Acoustic characteristics of sound tubes as a simplified model of vocal tracts are simulated by a three dimensional Finite Element Method(3D FEM). The results suggest some new facts as follows; if the area of the wall impedance distribution is spread or the cross section is flattened more, then the formant band widths are increase more, and the amplitude of sound pressure is decreased near formant frequencies. The sound wave propagates as a non-plane wave along the soft wall, and the inhomogeneous distribution causes distortion of surface wavefront. The pressure ampletude in the bent tube is reduced along the direction from the driving surface to the radiational surface. Moreover, as the effect of radiation, the sound wave is a non-plane wave at the radiational part.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-161"
  },
  "kakita94b_icslp": {
   "authors": [
    [
     "Yuki",
     "Kakita"
    ],
    [
     "Hitoshi",
     "Okamoto"
    ]
   ],
   "title": "Chaotic characteristics of voice fluctuation and its model explanation: normal and pathological voices",
   "original": "i94_0639",
   "page_count": 4,
   "order": 162,
   "p1": "639",
   "pn": "642",
   "abstract": [
    "This paper reports on an analysis of vocal irregularity in pitch and amplitude in terms of (1) the \"pseudo-phase-portrait,\" (2) the \"fractal dimension\" of the fluctuation, and (3) a model generating the fluctuation. A simple mathematical model generating the time series of fluctuation is proposed. Using this model, varying aspects of fluctuations as well as of the pseudo-phase-portraits are successfully described with a single control parameter. We also report that, by the simulation based on the model, a resultant intermittent type of fluctuation can manifest the chaotic behavior.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-162"
  },
  "ikeda94_icslp": {
   "authors": [
    [
     "Tadashige",
     "Ikeda"
    ],
    [
     "Yuji",
     "Matsuzaki"
    ]
   ],
   "title": "Flow theory for analysis of phonation with a membrane model of vocal cord",
   "original": "i94_0643",
   "page_count": 4,
   "order": 163,
   "p1": "643",
   "pn": "647",
   "abstract": [
    "A membrane model of the vocal cord is presented to shed light upon the mechanism of phonation from a mechanical point of view. The vocal cords are modeled by a pair of membranes supported by external distributed dampers and nonlinear springs, which represent the viscoelasticity of the vocal cord. A one-dimensional unsteady separated flow theory is proposed and is applied to the flow through the trachea, the bronchi, the glottis, and the false glottis. The theory successfully represents flow separation and reattachment with sufficient accuracy. Then, the flow in the vocal tract is represented by a one-dimensional compressible flow theory, which can include resonance. The equation of motion for the vocal cord is coupled with the governing equations of the flow. When voiced sound is evaluated numerically, the waveforms of pressure at the sub- and the supra-glottis agree qualitatively with data measured during phonation. Therefore, the present model is suitable for understanding of the mechanical phenomena of phonation.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-163"
  },
  "dickson94_icslp": {
   "authors": [
    [
     "B. Craig",
     "Dickson"
    ],
    [
     "John H.",
     "Esling"
    ],
    [
     "Roy C.",
     "Snell"
    ]
   ],
   "title": "Real-time processing of electroglottographic waveforms for the evaluation of phonation types",
   "original": "i94_0647",
   "page_count": 4,
   "order": 164,
   "p1": "647",
   "pn": "650",
   "abstract": [
    "A new process has been developed for automatically extracting and displaying selectable quotients of electroglottographic (EGG) waveforms in real time. The process relies upon the use of an algorithm that eliminates DC float and broadband noise from the captured EGG signal, without introducing phase distortion. The real-time capabilities facilitate visual feedback of the fundamental frequency and quotient results, both of which are reported as pitch-synchronous time-varying contours. Alternatively, a real-time scattergram mode that plots quotient values relative to fundamental frequency may be used to obtain visual feedback of phonation control.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-164"
  },
  "erickson94b_icslp": {
   "authors": [
    [
     "Donna",
     "Erickson"
    ],
    [
     "Kiyoshi",
     "Honda"
    ],
    [
     "Hiroyuki",
     "Hirai"
    ],
    [
     "Mary E.",
     "Beckman"
    ],
    [
     "Seiji",
     "Niimi"
    ]
   ],
   "title": "Global pitch range and the production of low tones in English intonation",
   "original": "i94_0651",
   "page_count": 4,
   "order": 165,
   "p1": "651",
   "pn": "654",
   "abstract": [
    "In addition to the well-studied high (H) targets, such as the H* peak accent in many declarative sentence, English intonation contours also include many low (L) tones, such as the L* target on the most stressed syllable in a canonical yes-no question contour. To understand better the behavior of such tone targets, we recorded three native speakers of English as they produced sentences containing various L types in soft, normal, and loud voice. For two speakers, we recorded activity from the infrahyoid strap muscles (SH), and for the third we recorded subglottal pressure (Ps). Results showed consistent variation in minimum fundamental frequency (F0) values associated with the L tone, and also consistent variation across the global pitch ranges for the three voice effort levels. Within a given pitch range, the level of associated SH activity was inversely correlated with the F0 value, but the correlation overall was less good because there is no overall variation in the SH level corresponding to the global range variation. Correlation improved when SH values for loud tokens were adjusted downward and those for soft tokens adjusted upward. The pattern of Ps values for the third speaker suggests an explanation. There were large differences in Ps value across the pitch ranges, against which speakers may adjust their SH activity to maintain a constant target F0.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-165"
  },
  "matsumura94b_icslp": {
   "authors": [
    [
     "Masafumi",
     "Matsumura"
    ],
    [
     "Kazuo",
     "Kimura"
    ],
    [
     "Katsumi",
     "Yoshino"
    ],
    [
     "Takashi",
     "Tachimura"
    ],
    [
     "Takeshi",
     "Wada"
    ]
   ],
   "title": "Measument of palatolingual contact pressure during consonant productions using strain gauge transducer mounted platal plate",
   "original": "i94_0655",
   "page_count": 4,
   "order": 166,
   "p1": "655",
   "pn": "658",
   "abstract": [
    "This paper deals with the measurement of palatolingual contact stress and pattern during consonant productions using a force sensor mounted palatal plate. The force sensor is composed of a strain gauge and a fixed beam that are mounted together in a palatal plate that is attached to the hard palate. The palatolingual contact stress is concentrated in a spot on the fixed beam by a protuberance bonded on the fixed beam. When the tongue touches the protuberance of the force sensor, the palatolingual contact stress on the force sensor is detected from the strain produced in the fixed beam. By mounting multiple force sensors, the palatolingual contact stress and pattern can be obtained. We have developed a prototype system, which consists of a five-force-sensor mounted palatal plate, a multi-channel amplifier for a strain gauge and a computer. The force sensor mounted palatal plate is LMM in thickness. The five-force-sensors are placed approximately at a 6mm interval along the midsagittal line of the hard palate. The most anterior sensor is located 5mm posterior to the alveolar ridge. Output of the force sensor was proportional to the stress applied to the fixed beam and had no hysteresis. Measurement error of the force sensor was less than 2% and an error by mechanical interferences among the sensors was less than 5.4%. The palatolingual contact stress and pattern for an adult male were measured during the productions of consonants /t/,/d/,/n/. Using the proposed system, dynamic aspect of the palatolingual contact stress in time-domain and space-domain were readily observed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-166"
  },
  "ogata94_icslp": {
   "authors": [
    [
     "Kohichi",
     "Ogata"
    ],
    [
     "Yorinobu",
     "Sonoda"
    ]
   ],
   "title": "A study of sensor arrangements for detecting movements and inclinations of tongue point during speech",
   "original": "i94_0659",
   "page_count": 4,
   "order": 167,
   "p1": "659",
   "pn": "662",
   "abstract": [
    "A magnetic sensing system has great advantages in data collection with safety, ease, and high accuracy of non-invasive and hazardless measurements of articulatory movements. In attempts to develop such a magnetic sensing system as to detect static magnetic field induced by a small magnetic rod on the tongue surface, serious problems with tilting and lateral movements of the tongue are encountered.\n",
    "This paper presents design improvements of articulatory measuring system and proposes a useful arrangement of sensors to reduce the measurement errors due to troublesome movements of the magnetic rod. For this purpose, several arrangements of the sensors were studied by computer simulation. Experimental results showed that errors can be effectively reduced by setting four sensors at each side of a subject's mandible where the sensors detect positions and inclination angles of the magnetic rod. At preliminary experiments, characteristic patterns of tongue movements including inclination due to deformation of tongue surface were observed on several test words.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-167"
  },
  "masaki94_icslp": {
   "authors": [
    [
     "Shinobu",
     "Masaki"
    ],
    [
     "Kiyoshi",
     "Honda"
    ]
   ],
   "title": "Estimation of temporal processing unit of speech motor programming for Japanese words based on the measurement of reaction time",
   "original": "i94_0663",
   "page_count": 4,
   "order": 168,
   "p1": "663",
   "pn": "666",
   "abstract": [
    "Sternberg et al. [6] analyzed the influence of the number of words, such as ascending numbers or a sequence of weekdays, on the reaction time (RT) to produce the words. They found that RT increased linearly as the number of words increased. Since a primary stress was included in each word used in their experiments, they concluded that a stress group comprises a programming unit in English speech production. In this paper, we modify and extend their study, and apply the RT measurement paradigm to the investigation of the process of Japanese speech production. As a first step, we investigated the influence of the number of Japanese syllables on RT. We used four material sets consisting of non-words having one to four syllables, where each syllable was [ka] or [ta]. We measured the RT for the production of the nonwords and analyzed the effect of the number of syllables on RT among each material set. Here, RT was defined as the time difference between visual presentation of the onset signal and release of the initial consonant. Analysis of variance (ANOVA) revealed a significant positive effect of the number of syllables on RT for 10 material sets out of 20 in all the five subjects, although the RT did not increase linearly as the number of syllables increased. No subject showed a significant decrease of RT for any material set. These results suggest that a syllable might be the smallest segment for a programming unit in Japanese speech production, while there remains another possibility that the programming unit might be a larger segment than a syllable. These results are discussed in terms of the production process model and difference between English and Japanese.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-168"
  },
  "wilpon94_icslp": {
   "authors": [
    [
     "Jay G.",
     "Wilpon"
    ],
    [
     "David B.",
     "Roe"
    ]
   ],
   "title": "Applications of speech recognition technology in telecommunications",
   "original": "i94_0667",
   "page_count": 4,
   "order": 169,
   "p1": "667",
   "pn": "670",
   "abstract": [
    "As the telecommunications industry evolves over the next decade to provide the products and services that people will desire, several key technologies will become commonplace. Two of these, automatic speech recognition and text-to-speech synthesis, will provide users more freedom on when, where and how they access information. While these technologies are currently in their infancy, their capabilities are rapidly increasing and their deployment in today's telephone network is expanding. The economic impact of just one application, automation of operator services, is well over $100 million per year. Yet, there still are many technical challenges that must be resolved before these technologies can be deployed ubiquitously in products and services through-out the wordwide telephone network. The current and future applications of speech recognition in the telecommunications industry will be examined in terms of their strengths, limitations and the degree to which user needs have been or have yet to be met.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-169"
  },
  "nitta94_icslp": {
   "authors": [
    [
     "Tsuneo",
     "Nitta"
    ]
   ],
   "title": "Speech recognition applications in Japan",
   "original": "i94_0671",
   "page_count": 4,
   "order": 170,
   "p1": "671",
   "pn": "674",
   "abstract": [
    "This paper introduces trends in speech recognition applications in Japan. Applications in the past several years are divided into three areas: application areas with actual results, new trends, and application areas under development. A focus on changes of application areas is given. Subsequently, the four year advancement in speech technologies after the first ICSLP in Kobe is investigated along with the key technologies that will realize next-generation products: various techniques for robust speech recognition and rapid-prototyping for a user interface.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-170"
  },
  "hirokawa94_icslp": {
   "authors": [
    [
     "Tomohisa",
     "Hirokawa"
    ]
   ],
   "title": "Trends in the applications of and market for speech synthesis technology",
   "original": "i94_0675",
   "page_count": 4,
   "order": 171,
   "p1": "675",
   "pn": "678",
   "abstract": [
    "Most conventional applications for speech synthesis employ digitized speech messages. The voice quality is high but these applications have several weak points. For instance, the output voice messages are limited and when a new service item is added, it is expensive to reconstruct the speech file. The more cost-effective alternative is text-to-speech(TTS) technology but several problems are obvious in current systems. First, the author overviews the speech synthesis applications currently active in Japan; digitized speech applications and TTS applications are covered. The size and trends of the speech synthesis market are examined. Voice response applications in which digitized speech is basically used, are investigated. Furthermore, applications of TTS are estimated from Japanese TTS synthesizer shipments. For both speech synthesis techniques the author indicates the problems to be solved and proposes the research direction. In particular, TTS technology is focused on because it seems capable of greatly expanding the speech synthesis market. At present, TTS quality is still insufficient compared to digitized sound and the applications are restricted to a small domain. However, TTS technology possesses many advantages, all of which are well known, so if the speech quality can be improved, the number of application domains will rapidly increase.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-171"
  },
  "mazor94_icslp": {
   "authors": [
    [
     "Baruch",
     "Mazor"
    ],
    [
     "Jerome",
     "Braun"
    ],
    [
     "Bonnie",
     "Zeigler"
    ],
    [
     "Solomon",
     "Lerner"
    ],
    [
     "Ming-Whei",
     "Feng"
    ],
    [
     "Han",
     "Zhou"
    ]
   ],
   "title": "OASIS - a speech recognition system for telephone service orders",
   "original": "i94_0679",
   "page_count": 4,
   "order": 172,
   "p1": "679",
   "pn": "682",
   "abstract": [
    "We have recently completed the development of a speech recognition system for telephone-service orders. The system handles American English speakers over common telephone links and does not require any enrollment. Our system is presently being deployed in a GTE service area for a field trial to take place during the second half of 1994. The initial application for the system is the automation of telephone-service-disconnect orders. In this paper, we describe the functional capabilities of the system and discuss some technical issues related to our development approach.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-172"
  },
  "cole94_icslp": {
   "authors": [
    [
     "Ronald",
     "Cole"
    ],
    [
     "David G.",
     "Novick"
    ],
    [
     "Mark",
     "Fanty"
    ],
    [
     "Pieter",
     "Vermeulen"
    ],
    [
     "Stephen",
     "Sutton"
    ],
    [
     "Dan",
     "Burnett"
    ],
    [
     "Johan",
     "Schalkwyk"
    ]
   ],
   "title": "A prototype voice-response questionnaire for the u.s. census",
   "original": "i94_0683",
   "page_count": 4,
   "order": 173,
   "p1": "683",
   "pn": "686",
   "abstract": [
    "This paper describes a study conducted to determine the feasibility of using a spoken questionnaire to collect information for the Year 2000 Census in the USA. To refine the dialogue and to train recognizers, we collected complete protocols from over 4000 callers. For the responses labeled (about half), over 99 percent of the answers contain the desired information. The recognizers trained so far range in performance from 75 percent correct on year of birth to over 99 percent for marital status. We developed a prototype system that engages the callers in a dialogue to obtain the desired information, reviews the recognized information at the end of the call, and asks the caller to identify the response categories that are incorrect.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-173"
  },
  "tsuboi94_icslp": {
   "authors": [
    [
     "Toshiaki",
     "Tsuboi"
    ],
    [
     "Shigeru",
     "Homma"
    ],
    [
     "Sho-ichi",
     "Matsunaga"
    ]
   ],
   "title": "A speech-to-text transcription system for medical diagnoses",
   "original": "i94_0687",
   "page_count": 4,
   "order": 174,
   "p1": "687",
   "pn": "690",
   "abstract": [
    "A prototype of a speech-to-text transcription system for medical diagnoses is described. This system recognizes continuous phrase speech and transcribes it into Japanese text. We devised a prototype based on consonant-vowel spotting using the DP matching technique, as demonstrated at ICSLP'90, and improved this system by using a Japanese character trigram model, a phrase syntax, and phoneme-based hidden Markov models (HMMs), as reported in ICSLP'92. This paper outlines recognition methods, and describes the system configuration and results of performance evaluation tests. The system consists of 2 main parts: specially designed speech recognition hardware and a SUN SPARC Station IPX. Using 2 RISC CPUs for the LR parser and 12 DSPs for HMM calculation, the processing time is reduced by 83%. A performance evaluation test was carried out for X-ray CT scanning reports. Before the test, a character trigram model was extracted by analyzing 1,500 scanning reports which contained a total of about 70,000 phrases. The current dictionary vocabulary size (about 3,600 words) was established at this time. Transcription accuracies of 91% and 85% were obtained for normal and abnormal CT reports, respectively, compared to 80% and 65% as reported at ICSLF90.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-174"
  },
  "dymetman94_icslp": {
   "authors": [
    [
     "Marc",
     "Dymetman"
    ],
    [
     "Julie",
     "Brousseau"
    ],
    [
     "George",
     "Foster"
    ],
    [
     "Pierre",
     "Isabelle"
    ],
    [
     "Yves",
     "Normandin"
    ],
    [
     "Pierre",
     "Plamondon"
    ]
   ],
   "title": "Towards an automatic dictation system for translators : the transtalk project",
   "original": "i94_0691",
   "page_count": 4,
   "order": 175,
   "p1": "691",
   "pn": "694",
   "abstract": [
    "Professional translators often dictate their translations orally and have them typed afterwards. The TransTalk project aims at automating the second part of this process. Its originality as a dictation system lies in the fact that both the acoustic signal produced by the translator and the source text under translation are made available to the system. Probable translations of the source text can be predicted and these predictions used to help the speech recognition system in its lexical choices. We present the results of the first prototype, which show a marked improvement in the performance of the speech recognition task when translation predictions are taken into account.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-175"
  },
  "grajski94_icslp": {
   "authors": [
    [
     "Kamil A.",
     "Grajski"
    ],
    [
     "Kurt",
     "Rodarmer"
    ]
   ],
   "title": "Real-time, speaker-independent, continuous Spanish speech recognition for personal computer desktop command & control",
   "original": "i94_0695",
   "page_count": 4,
   "order": 176,
   "p1": "695",
   "pn": "698",
   "abstract": [
    "We analyzed a speaker-independent, continuous Spanish speech recognition system for desktop command & control of a Macintosh personal computer. A word error rate of 0.6% was obtained for the System 7.1 Finder navigation task with \"clean\" speech recorded under semi-controlled conditions in Cupertino, California, and 2.8% for \"real world\" speech recorded in an open office environment in Mexico City. In obtaining these results, we demonstrated the utility of cepstral normalization as a means of pooling data across speech data collection sites. Finally, data was obtained showing that for the measured tasks, acoustic, phonetic and linguistic data could be leveraged across Spanish dialects.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-176"
  },
  "noguchi94_icslp": {
   "authors": [
    [
     "Jun",
     "Noguchi"
    ],
    [
     "Shinsuke",
     "Sakai"
    ],
    [
     "Kaichiro",
     "Hatazaki"
    ],
    [
     "Ken-Ichi",
     "Iso"
    ],
    [
     "Takao",
     "Watanabe"
    ]
   ],
   "title": "An automatic voice dialing system developed on PC speech i/o platform",
   "original": "i94_0699",
   "page_count": 4,
   "order": 177,
   "p1": "699",
   "pn": "702",
   "abstract": [
    "This paper presents an automatic voice dialing system developed on a speech I/O platform for personal computers. The voice dialing system recognizes a name spoken by the user, and dials the number registered in the system's database for the name uttered. Then, the line gets connected and the user can start talking with the party on the other end of the line. The system has been developed using a low-cost speech I/O platform software for personal computers with speaker-independent speech recognition capability. It has desirable capabilities to be used with application programs, such as dynamic modification of the speech recognition vocabulary from an application program. The voice dialing system has a user friendly interface, to prevent unexpected operation caused by misrecognition and to allow coexistence with other application programs. Four people actually used the voice dialing system in an office environment for a preliminary evaluation. Results were satisfactory.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-177"
  },
  "oerder94_icslp": {
   "authors": [
    [
     "Martin",
     "Oerder"
    ],
    [
     "Harald",
     "Aust"
    ]
   ],
   "title": "A realtime prototype of an automatic inquiry system",
   "original": "i94_0703",
   "page_count": 4,
   "order": 178,
   "p1": "703",
   "pn": "706",
   "abstract": [
    "We have developed an automatic system for train-timetable information over the telephone that provides accurate connections between 1200 German cities. The caller can talk to it in unrestricted, natural, and fluent speech, very much like he or she would communicate with a human operator, and is not given any instructions beforehand. The speech recognizer of this system creates a word graph and passes it on to the speech understanding component. It is then parsed with a stochastic attributed context-free grammar that is used as a language model, to identify the relevant parts, and to compute their meaning. A dialogue manager analyses the results and either comes up with a new question or accesses the timetable database. The answer to this query, as well as the questions, are converted to spoken language by replaying appropriate pre-recorded phrases.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-178"
  },
  "goddeau94_icslp": {
   "authors": [
    [
     "David",
     "Goddeau"
    ],
    [
     "Eric",
     "Brill"
    ],
    [
     "James R.",
     "Glass"
    ],
    [
     "Christine",
     "Pao"
    ],
    [
     "Michael",
     "Phillips"
    ],
    [
     "Joseph",
     "Polifroni"
    ],
    [
     "Stephanie",
     "Seneff"
    ],
    [
     "Victor W.",
     "Zue"
    ]
   ],
   "title": "GALAXY: a human-language interface to on-line travel information",
   "original": "i94_0707",
   "page_count": 4,
   "order": 179,
   "p1": "707",
   "pn": "710",
   "abstract": [
    "This paper introduces galaxy, a distributed system for providing access and intelligent mediation for on-line information and services via human-language technology. We have chosen to focus our initial implementation on the travel domain (including air travel, local navigation, and weather) largely to leverage off our past experience in the development of the VOYAGER [1], ATIS [2], and pegasus [3] systems. Galaxy is designed to access real databases that are distributed among various information networks (e.g., Internet, LAN, or commercial networks such as CompuServe). Our initial system utilizes American Airlines' eaasy sabre, the NYNEX Electronic Yellow Pages, the US Census Bureau's map database, and the National Weather Service database. Galaxy was developed as an environment for exploring important research issues in human language technology. Moreover, GALAXY will provide a demonstration of an interface that will enable users to efficiently access, process, and manipulate a vast amount of on-line information.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-179"
  },
  "horne94_icslp": {
   "authors": [
    [
     "Merle",
     "Horne"
    ],
    [
     "Marcus",
     "Filipsson"
    ]
   ],
   "title": "Generating prosodic structure for Swedish text-to-speech",
   "original": "i94_0711",
   "page_count": 4,
   "order": 180,
   "p1": "711",
   "pn": "714",
   "abstract": [
    "This article presents an outline of the prosodic constituent structure which will be incorporated in a linguistic preprocessor forming part of a text-to-speech system for generation of intonation in Swedish restricted texts. It further discusses the structure of a number of algorithms (including a word-class tagger, a complex-word identifier and a prosodic parser) which enable the generation of the assumed prosodic structure.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-180"
  },
  "black94_icslp": {
   "authors": [
    [
     "Alan W.",
     "Black"
    ],
    [
     "Paul",
     "Taylor"
    ]
   ],
   "title": "Assigning intonation elements and prosodic phrasing for English speech synthesis from high level linguistic input",
   "original": "i94_0715",
   "page_count": 4,
   "order": 181,
   "p1": "715",
   "pn": "718",
   "abstract": [
    "This paper describes a method for generating intonation  events and prosodic phrasing from a high level linguistic description.  Specifically, the input consists of information normally available from  linguistic processing: part of speech, constituent structure and,  importantly, speech act. The generated output contains explicit intonation  events from which an Fo contour may be generated. Prosody can be controlled  via features in the input describing the function of words and phrases  without direct reference to intonation. The results are evaluated against  natural spoken sentences.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-181"
  },
  "santen94_icslp": {
   "authors": [
    [
     "Jan P. H. van",
     "Santen"
    ],
    [
     "Julia",
     "Hirschberg"
    ]
   ],
   "title": "Segmental effects on timing and height of pitch contours",
   "original": "i94_0719",
   "page_count": 4,
   "order": 182,
   "p1": "719",
   "pn": "722",
   "abstract": [
    "Instantiations of a given type of pitch contour vary systematically as a function of the segmental material with which the contour is associated. In this study, we report on results of statistical analyses of 1,926 utterances collected from a single female speaker. Results show that the observed pitch contours can be quantitatively modeled as distortions in the temporal and frequency domains of a single, underlying contour. Distortions in the temporal domain can be described as non-linear rightward stretching of the contour as the durations of onset, vowel nucleus, and coda increase. Variations in the frequency domain include effects of vowel height and overall syllable duration. Frequency effects of onset voicing can be attributed to systematic differences in duration between voiced and voiceless consonants as well as to a small local perturbation confined to the first 50 ms after the vowel onset.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-182"
  },
  "fukada94_icslp": {
   "authors": [
    [
     "Toshiaki",
     "Fukada"
    ],
    [
     "Yasuhiro",
     "Komori"
    ],
    [
     "Takashi",
     "Aso"
    ],
    [
     "Yasunori",
     "Ohora"
    ]
   ],
   "title": "A study on pitch pattern generation using HMM-based statistical information",
   "original": "i94_0723",
   "page_count": 4,
   "order": 183,
   "p1": "723",
   "pn": "726",
   "abstract": [
    "This paper describes a novel pitch pattern generation method for speech synthesis using Hidden Markov Models (HMMs). In the proposed method, the F0 contours of minor phrase are modeled by HMMs (pitch-HMMs). The pitch-HMMs are trained using F0 and AF0considering phonetic environments (e.g. accent type, mora count, mora position, phonemic category, etc). To evaluate the pitch-HMMs, accent identification experiments are performed. The results indicate that the pitch-HMMs can capture the movement in F0 contours appropriately. In the F0 contour generation experiments, the proposed method yields an averaged root mean square error of 132cent (equivalent to 9.2Hz at 120Hz) between the original and the generated F0 contours. Furthermore, an application of the proposed method to text-to-speech system is also discussed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-183"
  },
  "boeffard94_icslp": {
   "authors": [
    [
     "O.",
     "Boeffard"
    ],
    [
     "F.",
     "Violaro"
    ]
   ],
   "title": "Using a hybrid model in a text-to-sppech system to enlarge prosodic modifications",
   "original": "i94_0727",
   "page_count": 4,
   "order": 184,
   "p1": "727",
   "pn": "730",
   "abstract": [
    "This paper describes a hybrid harmonic plus noise system developed for TTS applications in the CNET multilingual concatenation based environment. It was implemented in order to introduce more naturalness and variability in the speech synthesis, enabling greater prosodic modifications than is currently generated by PSOLA type systems. Another motivation for this research was the need to access the fine structure of the speech signal in order to perform voice transformations. An evaluation prototype was tested with a complete set of diphones extracted from a french male voice and the preliminary results are very encouraging.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-184"
  },
  "ando94b_icslp": {
   "authors": [
    [
     "Akio",
     "Ando"
    ],
    [
     "Eiichi",
     "Miyasaka"
    ]
   ],
   "title": "A new method for estimating Japanese speech rate",
   "original": "i94_0731",
   "page_count": 4,
   "order": 185,
   "p1": "731",
   "pn": "734",
   "abstract": [
    "This paper proposes a new method for estimating speaking rates of Japanese using a speech recognition technique. Taking a fact that a Japanese syllable generally contains one and only one vowel into consideration, the method estimates speaking rates based on a vowel recognition technique. The vowel recognition is performed using the LPC cepstral distance between the input and pre-stored vowel templates which have been trained for speaker-independent vowel recognition. The new method has been applied for two definitions of speaking rate: one is the number of moras in a certain time interval, and the other is the inverse of an average period of successive vowels. Experiments showed that the estimation errors of speaking rate with those two definitions are 3.0% and 8.7%, respectively.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-185"
  },
  "konst94_icslp": {
   "authors": [
    [
     "Emmy M.",
     "Konst"
    ],
    [
     "Louis",
     "Boves"
    ]
   ],
   "title": "Automatic grapheme-to-phoneme conversion of dutch names",
   "original": "i94_0735",
   "page_count": 4,
   "order": 186,
   "p1": "735",
   "pn": "738",
   "abstract": [
    "This paper describes the work on grapheme-to-phoneme conversion for Dutch proper names, carried out in the framework of the ONOMASTICA project. We give some basic statistics of Dutch names, describe the development of a set of letter-to-sound rules optimized for proper names, discuss the main reasons why a specific rule set for this purpose is necessary and finally give results of two tests of the performance of the optimized rule set.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-186"
  },
  "williams94_icslp": {
   "authors": [
    [
     "Briony",
     "Williams"
    ]
   ],
   "title": "Diphone synthesis for the welsh language",
   "original": "i94_0739",
   "page_count": 4,
   "order": 187,
   "p1": "739",
   "pn": "742",
   "abstract": [
    "The Welsh language is comparatively little researched, and this work represents the first attempt to develop speech technology for Welsh. A list of pseudo-Welsh nonsense words was generated. Certain linguistic features of Welsh, such as the relationship between stress location and phonological vowel length, made this task more complicated than for English. A native speaker was recorded reading this list. Over ten percent of the speech was segmented by hand. The segmentation was carried out at the \"demi-phoneme\" level, from the beginning of a phoneme to its midpoint, in order to train a segmenter to find plausible diphone boundaries automatically. The segmentations were used to train a set of Hidden Markov Models, which automatically segmented the rest of the recordings. The segmentations were corrected by hand, and pitchrnarking was carried out. An index of diphone locations was produced, together with a diphone dictionary. The resulting synthesised speech can be used either for Welsh, or for English spoken with a Welsh accent.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-187"
  },
  "doi94_icslp": {
   "authors": [
    [
     "Shinichi",
     "Doi"
    ],
    [
     "Kazuhiko",
     "Iwata"
    ],
    [
     "Kazunori",
     "Muraki"
    ],
    [
     "Yukio",
     "Mitome"
    ]
   ],
   "title": "Pause control in Japanese text-to-speech conversion system with lexical discourse grammar",
   "original": "i94_0743",
   "page_count": 4,
   "order": 188,
   "p1": "743",
   "pn": "746",
   "abstract": [
    "This paper presents a practical method for controlling prosodic information in Japanese text-to-speech conversion system based on Lexical Discourse Grammar(LDG). LDG is a method we have developed for providing a cheap and reliable parsing and global structure analyzing algorithm of Japanese long sentences with lexical information. LDG can presume the inter-clausal dependency within Japanese sentences prior to syntactic and semantic analyses by utilizing the differences of the encapsulating powers of each Japanese function words, such as conjunctive particles (postpositions) located at the end of each clause. Therefore, LDG can also produce prosodic information control parameters, especially the location and length of pauses, which are influenced by the sentence structure. We confirmed the correlation between the pause length inserted after the clause with a conjunctive particle and the particle's encapsulating power which is classified into 6 levels by analyzing the speech utterances of a male announcer.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-188"
  },
  "sakurai94_icslp": {
   "authors": [
    [
     "Naohiro",
     "Sakurai"
    ],
    [
     "Takerni",
     "Mochida"
    ],
    [
     "Tetsunori",
     "Kobayashi"
    ],
    [
     "Katsuhiko",
     "Shirai"
    ]
   ],
   "title": "Generation of prosody in speech synthesis using large speech data-base",
   "original": "i94_0747",
   "page_count": 4,
   "order": 189,
   "p1": "747",
   "pn": "750",
   "abstract": [
    "In order to improve the naturalness of synthetic speech in Japanese text-to-speech or concept-to-speech conversion, we introduce a new scheme to synthesize arbitrary speech sentences using the natural sentence speech data-base. In our synthesis method, a series of synthetic parameters is generated using patterns which are extracted from natural speech waveforms. In the first step, the basic sentence is selected from the data-base against a target sentence. The factors for the selection are phrase dependency structure(separation degree), number of mora, type of accent and phonemic labels. In the second step, if necessary, the basic accent-phrase is selected from the same data-base against the each target accent-phrase. The factors considered in selecting the each accent-phrase are the separation degree, the number of mora, the type of accent and the phonemic labels. In the third step, pitch pattern is generated from those waveform units selected in the first and the second step. In the last step, the phonemic parameters are generated. These phonemic parameters for several morae are extracted on the former three steps. Therefore, in this step, we only have to replace the phonemic parameters for ill-suited morae. As the pitch pattern is generated using patterns directly extracted from real speech, it is expected to be more natural than any other pattern which is estimated by any model. We have examined this method on Japanese sentence speech to the present and affirmed that the synthetic sound preserves human-like features fairly well.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-189"
  },
  "dyhr94_icslp": {
   "authors": [
    [
     "Niels-Jorn",
     "Dyhr"
    ],
    [
     "Marianne",
     "Elmlund"
    ],
    [
     "Carsten",
     "Henriksen"
    ]
   ],
   "title": "Preserving naturalness in synthetic voices while minimizing variation in formant frequencies and bandwidths",
   "original": "i94_0751",
   "page_count": 4,
   "order": 190,
   "p1": "751",
   "pn": "754",
   "abstract": [
    "As a preliminary to improving the naturalness of the synthetic male and female voices in a Danish text-to-speech system using a rule-driven formant synthesizer, the relative importance of the individual formant frequencies and bandwidths has been investigated. Recordings of a Danish compound word consisting entirely of voiced segments were analyzed. Based on these recordings and the analysis, a number of manipulated, synthetic stimuli were created and presented in two listening tests. The main results of these simplifications are: a) Bandwidths (B5-B8) are more sensitive to simplifications than formants (F5-F8). b) F5-F8 may be held constant throughout the utterance, and B1-B4 may be kept constant per segment without perceptible loss of naturalness, c) B5-B8 may also be held constant, though with a minor loss of naturalness. A similar approach has been tried with female synthetic voices, and preliminary results corroborate the results outlined above. Among the more comprehensive simplifications in the male voice a hierarchy of acceptability was established.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-190"
  },
  "takahashi94b_icslp": {
   "authors": [
    [
     "Kazuhiro",
     "Takahashi"
    ],
    [
     "Kazuhiko",
     "Iwata"
    ],
    [
     "Yukio",
     "Mitome"
    ],
    [
     "Keiko",
     "Nagano"
    ]
   ],
   "title": "Japanese text-to-speech conversion software for personal computers",
   "original": "i94_1743",
   "page_count": 4,
   "order": 191,
   "p1": "1743",
   "pn": "1746",
   "abstract": [
    "A Japanese text-to-speecli conversion software system has been developed. For that, two new techniques are proposed. One is a prosody generation model which utilizes the invariant prosodic structures against any speech tempo for generating natural rhythm and intonation. The other technique is the waveform concatenation method, using expanded CV-VC speech units. It runs on standard Personal Computers (PC) without any additional hardware, such as Digital Signal Processors (DSP), so that installation cost is very low. And application interfaces are provided, so that application programs can easily utilize the text-to-speech function.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-191"
  },
  "vorstermans94_icslp": {
   "authors": [
    [
     "Annemie",
     "Vorstermans"
    ],
    [
     "Jean-Pierre",
     "Martens"
    ]
   ],
   "title": "Automatic labeling of speech synthesis corpora",
   "original": "i94_1747",
   "page_count": 4,
   "order": 192,
   "p1": "1747",
   "pn": "1750",
   "abstract": [
    "In this paper, a new system for the automatic segmentation and labeling of speech is presented. The system comprises segmentation and broad phonetic classification neural networks which were originally trained on one task (Flemish continuous speech), and which were subsequently adapted to a new task. The adaptation is performed by an embedded training procedure requiring no hand labeled utterances representative for the new task. The system was evaluated on five isolated word corpora designed for the development of Dutch, French, American English, Spanish and Korean text-to-speech systems. Additional test were run on TIMIT utterances in order to provide segmentation and labeling results which can be compared to similar results reported in the literature.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-192"
  },
  "ishikawa94_icslp": {
   "authors": [
    [
     "Yasushi",
     "Ishikawa"
    ],
    [
     "Kunio",
     "Nakajima"
    ]
   ],
   "title": "On synthesis units for Japanese text-to-speech synthesis",
   "original": "i94_1751",
   "page_count": 4,
   "order": 193,
   "p1": "1751",
   "pn": "1754",
   "abstract": [
    "This paper describes on synthesis units for text-to-speech synthesis. A kind of synthesis unit and extraction of units are very important problems in speech synthesis by rule. In general, a synthesis method based on longer units reduces the difficulties in realization of coarticulation. VCV, CVC, demi-syllable, tri-phone are typical units for Japanese text-to-speech system. Recently, non-uniform units or context-dependent units are proposed, and good results are reported. However such a kind of unit is considered only phonetic context for realization of coarticulation. It is fact that phonetic context is one of the most important factor of variation of spectral feature, but prosodic features also are important. Our basic idea is introducing prosodic features into control of spectral features in order to realize natural sounded synthetic speech. In this paper, we report results of basic analytic experiments. The results show that there is obvious relation between prosodic features and spectral features, and that spectral control method considered not only phonetic context but also prosodic feature is able to improve quality of synthetic speech in text-to-speech system.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-193"
  },
  "klavans94_icslp": {
   "authors": [
    [
     "Judith L.",
     "Klavans"
    ],
    [
     "Evelyne",
     "Tzoukermann"
    ]
   ],
   "title": "Inducing concatenative units from machine readable dictionaries and corpora for speech synthesis",
   "original": "i94_1755",
   "page_count": 4,
   "order": 194,
   "p1": "1755",
   "pn": "1758",
   "abstract": [
    "The purpose of this research is to determine the best method for deciding on an optimal set of concatenative units for concatenative speech synthesis. Of the two main approaches to speech synthesis: segmental synthesis and rule-based synthesis, the former relies heavily on the successful choice of concatenative units. Segment al synthesis consists of concatenating segmental units (diphones, triphones, etc); rule-based synthesis consists of the computation of control parameters based on pre-established rules. Deciding on the set of diphones is quite straightforward in the sense that it suffices to take the phoneme inventory of a language, and simply combine each phoneme with every other one. For example, taking the approximately 35 French phonemes, 1225 phonemic pairs (35x35) constitute the complete and exhaustive starting diphone inventory. On the other hand, deciding on the set of triphones, quadriphones and larger units raises difficult questions about the nature of phonemes in a given language such as: (1) stability vs instability in a coarticulatory environment, (2) size of overall inventory, and (3) frequency of that unit in the language, in combination with factors (1) and (2).\n",
    "We report on experiments with four different databases, with comparisons between the resources regarding their n-gram frequency output. The first two databases consist of pronunciation field information from two dictionaries, the Encyclopedic Robert French dictionary [16] with 85,000 headwords, and the smaller Collins Gem [13] containing 15,000 words. For comparison, we use two text corpora, the Hansard (about 2.5 million words) and the smaller Tubach and Boe [31] corpus (80,000 words); both corpora were processed by a set of grapheme-to-phoneme rules [18]. A frequency extraction program was applied to all four resources to extract trigram phonemic frequencies; this serves as a basis for comparison between dictionary derived data and corpus derived, frequencies.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-194"
  },
  "portele94_icslp": {
   "authors": [
    [
     "Thomas",
     "Portele"
    ],
    [
     "Florian",
     "Höfer"
    ],
    [
     "Wolfgang J.",
     "Hess"
    ]
   ],
   "title": "Structure and representation of an inventory for German speech synthesis",
   "original": "i94_1759",
   "page_count": 4,
   "order": 195,
   "p1": "1759",
   "pn": "1762",
   "abstract": [
    "When generating synthetic speech by unit concatenation the structure and the representation of the unit inventory is a major issue. Therefore, a mixed inventory structure was developed and compared with a demisyllable and a diphone inventory in a perception experiment. The results show that indeed some of the disadvantages of the two standard methods vanish when the mixed inventory structure is used. The complete inventory consists of 2182 units for the German language. The inventory was recorded with 32 kHz sampling rate. A pair comparison test confirmed a noticeable increase in naturalness compared to synthetic speech sampled with 16 kHz. However, a segmental intelligibility test showed no differences between the two sampling rates. Both error rates are comparable to those obtained with natural speech. To find the representation and manipulation algorithm that yields the best overall quality of synthetic speech, TD-PSOLA, LP-PSOLA, and simple RELP were compared. Although only a very simple LP ana-lysis was performed, no perceptual differences between TD-PSOLA and simple RELP could be found, while LP-PSOLA speech was judged worse than the other versions.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-195"
  },
  "lacheretdujour94_icslp": {
   "authors": [
    [
     "Anne",
     "Lacheret-Dujour"
    ],
    [
     "Vincent",
     "Pean"
    ]
   ],
   "title": "Towards a prosodic cues-based modelling of phonological variability for text-to-speech synthesis",
   "original": "i94_1763",
   "page_count": 4,
   "order": 196,
   "p1": "1763",
   "pn": "1766",
   "abstract": [
    "The study presented in this paper has been carried out in the framework of phonological variability in French, with applications in automatic speech processing in mind. The specific aim of our study is to model inter-speaker phonemic variants at word boundaries: global strategies and strategies specific to different classes of speakers. Data have been collected from casual speech corpus. Variants are defined through their relationship with the prosodic structure of a given utterance, This structure depends on syntactic organization of discourse. One example of this phenomena is given here with the pronunciation of the schwa at words' frontiers.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-196"
  },
  "trancoso94_icslp": {
   "authors": [
    [
     "Isabel",
     "Trancoso"
    ],
    [
     "M. Ceu",
     "Viana"
    ],
    [
     "Fernando M.",
     "Silva"
    ],
    [
     "Goncalo C.",
     "Marques"
    ],
    [
     "Luis C.",
     "Oliveira"
    ]
   ],
   "title": "Rule-based vs neural network-based approaches to letter-to-phone conversion for portuguese common and proper names",
   "original": "i94_1767",
   "page_count": 4,
   "order": 197,
   "p1": "1767",
   "pn": "1770",
   "abstract": [
    "This paper presents some relevant aspects of the pronunciation of proper names and common lexica in European Portuguese. It presents statistical data concerning the occurrence and distribution of graphemes and phonemes for the two corpora and distinguishes between different subclasses found in proper names, namely first and last names, toponyms and acronyms. It also compares the performance of a rule-based method for letter-to-phone conversion for the two corpora with two self-learning methods, one based on a multi-layered neural network and another based on table look-up.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-197"
  },
  "ao94_icslp": {
   "authors": [
    [
     "Benjamin",
     "Ao"
    ],
    [
     "Chilin",
     "Shih"
    ],
    [
     "Richard",
     "Sproat"
    ]
   ],
   "title": "A corpus-based Mandarin text-to-speech synthesizer",
   "original": "i94_1771",
   "page_count": 4,
   "order": 198,
   "p1": "1771",
   "pn": "1774",
   "abstract": [
    "This paper describes the AT&T Bell Laboratories Mandarin Chinese concatenative text-to-speech (TTS) system, which is being constructed using the NewTTS architecture for multi-lingual TTS [12]. This Mandarin TTS system is 'corpus-based' in that important components of the system are developed on the basis of the analysis of large text corpora. We concentrate here primarily on two modules, namely the word segmentation module and the segmental duration module. We also briefly describe the intonation module and the concatenative unit inventory.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-198"
  },
  "hakoda94_icslp": {
   "authors": [
    [
     "Kazuo",
     "Hakoda"
    ],
    [
     "Tomohisa",
     "Hirokawa"
    ],
    [
     "Kenzo",
     "Itoh"
    ]
   ],
   "title": "Speech editor based on enhanced user-system interaction for high quality text-to-speech synthesis",
   "original": "i94_1775",
   "page_count": 4,
   "order": 199,
   "p1": "1775",
   "pn": "1778",
   "abstract": [
    "This paper describes a new speech editor based on enhanced user-system interaction that produces high quality synthesized speech by using an advanced text-to-speech synthesis method. A prototype system is constructed on a work station with the Open Window system. Features of the prototype are that the operator can correct the faults of the text-to-speech synthesis method and produce high quality synthesized speech from input Japanese text. System operation has been optimized by adopting a real-time synthesizer and a GUI design based on mouse operations. A system evaluation confirms that character level correction is very effective for improving synthesized speech quality. The proposed system can be used to provide voice messages for a conventional digital audio response unit at low cost.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-199"
  },
  "ljungqvist94_icslp": {
   "authors": [
    [
     "Mats",
     "Ljungqvist"
    ],
    [
     "Anders",
     "Lindstrom"
    ],
    [
     "Kjell",
     "Gustafson"
    ]
   ],
   "title": "A new system for text-to-speech conversion, and its application to Swedish",
   "original": "i94_1779",
   "page_count": 4,
   "order": 200,
   "p1": "1779",
   "pn": "1782",
   "abstract": [
    "High quality text-to-speech conversion requires robust and accurate text processing, well founded modelling of phonological/phonetic processes, and sound generation with high intelligibility and naturalness. The present paper describes a new text-to-speech system under development which provides a flexible framework for integrating these processing modules. We give an overview of the system design and describe some recent advances in the related fields: text processing centered around a large lemma-based lexicon, the rule formalism and data structures of the system, and some recent work on improving voice source modelling in a formant synthesizer.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-200"
  },
  "shiga94_icslp": {
   "authors": [
    [
     "Yoshinori",
     "Shiga"
    ],
    [
     "Yoshiyuki",
     "Hara"
    ],
    [
     "Tsuneo",
     "Nitta"
    ]
   ],
   "title": "A novel segment-concatenation algorithm for a cepstrum-based synthesizer",
   "original": "i94_1783",
   "page_count": 4,
   "order": 201,
   "p1": "1783",
   "pn": "1786",
   "abstract": [
    "We propose a segment-concatenation algorithm which reduce perceived distortion caused by segment concatenation for a segment-based speech synthesizer. This algorithm concatenates six types of phonetic segments along the transient part of speech rather than the steady part, where humans have a keen sense of spectral distortion. This concatenation method enables a segment-based synthesizer to produce a smooth sound with comparatively small required storage space for the segments. We apply the algorithm to a rule-based, cepstrum-based speech synthesizer for English words. We evaluate the intelligibility of the synthetic speech through the Modified Rhyme Test (MRT)[1]. The result proved that the speech has a high intelligibility ratio of 90 percent.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-201"
  },
  "koopmansvanbeinum94_icslp": {
   "authors": [
    [
     "Florien J.",
     "Koopmans-van Beinum"
    ],
    [
     "Louis C. W.",
     "Pols"
    ]
   ],
   "title": "Naturalness and intelligibility of rule-synthesized speech, supplied with specific spectro-temporal features derived from natural continuous speech",
   "original": "i94_1787",
   "page_count": 4,
   "order": 202,
   "p1": "1787",
   "pn": "1790",
   "abstract": [
    "This paper discusses several characteristics that might be responsible for the lack of naturalness we are faced with when listening to longer stretches of synthetic speech. By systematically manipulating these characteristics in the available Dutch allophone-based rule synthesis system, we evaluated their effects on naturalness and on generally perceived intelligibility. First results of our study show that 1) a speaker may use several tools to highlight some, informatively important, speech parts; 2) the effect of applying more spectral and temporal reduction in the synthesized speech yields an increase of perceived naturalness, without affecting the intelligibility as evaluated in a general way; 3) the effect of avoiding between-word coarticulation before words or word groups in focus, turned out to be overruled by the low quality of the synthesized speech, when presented in longer paragraphs and worked for some listeners negatively and for others positively when presented in one-sentence stimuli.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-202"
  },
  "patterson94_icslp": {
   "authors": [
    [
     "Karalyn",
     "Patterson"
    ],
    [
     "Karen",
     "Croot"
    ],
    [
     "John R.",
     "Hodges"
    ]
   ],
   "title": "Speech production: Insights from a study of progressive aphasia",
   "original": "i94_0755",
   "page_count": 4,
   "order": 203,
   "p1": "755",
   "pn": "758",
   "abstract": [
    "Neurodegenerative disease can result in a primary language disturbance, typically called primary progressive aphasia, of which two distinctly different patterns have been described. Non-fluent and fluent progressive aphasia [1,2] bear some similarities to non-fluent and fluent aphasia resulting from cerebro-vascular accident (CVA), but often reveal especially clear-cut, circumscribed language deficits. In this paper, we focus on the object naming performance of two patients, one with each form of progressive aphasia. Both patients' naming showed a marked decline over a two/three-year period. The anomia of the fluent case can be explained entirely by progressive loss of features of knowledge representation for objects and words. The deficit of the nonfluent case reflects progressive difficulty in access to and retrieval from phonological representations for speech production.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-203"
  },
  "iwata94_icslp": {
   "authors": [
    [
     "Makoto",
     "Iwata"
    ],
    [
     "Yasuhisa",
     "Sakurai"
    ],
    [
     "Toshimitsu",
     "Momose"
    ]
   ],
   "title": "Functional mapping of cerebral mechanism of reading in the Japanese language",
   "original": "i94_0759",
   "page_count": 4,
   "order": 204,
   "p1": "759",
   "pn": "762",
   "abstract": [
    "Japanese patients with alexia without aphasia caused by focal left hemispheric damage show the dissociation between Kanji and Kana. Recently discovered was that the patient with left posterior inferior temporal (PIT) lesion developed alexia without aphasia selective for Kanji. We carried out activation studies using H215O PET scan and confirmed the role of PIT area in semantic reading process. Although the role of left angular gyrus (AG) in reading process could not be confirmed by PET scan studies, the adjacent left lateral occipital area was found to be significantly activated in phonological reading process.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-204"
  },
  "boatman94_icslp": {
   "authors": [
    [
     "Dana F.",
     "Boatman"
    ],
    [
     "Ronald P.",
     "Lesser"
    ],
    [
     "Barry",
     "Gordon"
    ]
   ],
   "title": "Cortical representation of speech perception and production, as revealed by direct cortical electrical interference",
   "original": "i94_0763",
   "page_count": 3,
   "order": 205,
   "p1": "763",
   "pn": "766",
   "abstract": [
    "The relationship between speech perception and production was studied using deficits produced by direct cortical electrical stimulation. Stimulation was applied through indwelling subdural electrode grids in three patients with epilepsy. Without stimulation, patients performed at ceiling on tasks of auditory syllable discrimination, identification, comprehension, naming, reading, and repetition. With electrical interference, speech perception and production errors co-occurred at multiple sites on the lateral left temporal and inferior frontal cortex. These findings add to existing evidence that suggest that some aspects of speech perception and production share functional and neural resources. This in turn argues against classical notions of a strict functional and anatomic separation between speech perception and speech production.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-205"
  },
  "rugg94_icslp": {
   "authors": [
    [
     "Michael D.",
     "Rugg"
    ],
    [
     "Catherine J.C.",
     "Cox"
    ],
    [
     "Michael C.",
     "Doyle"
    ]
   ],
   "title": "Investigating word recognition and language comprehension with event-related brain potentials",
   "original": "i94_0767",
   "page_count": 4,
   "order": 206,
   "p1": "767",
   "pn": "780",
   "abstract": [
    "The applications of event-related brain potentials (ERPs) to the study of language are briefly outlined. Three studies of the effects of word repetition on ERPs evoked by spoken words are described. The results show that ERPs are sensitive to word repetition within-modality, but insensitive to variations in voice-specific acoustic features between first and second presentations. The studies also provide evidence that ERP repetition effects evoked by spoken words reflect both modality-and lexically-sensitive processes.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-206"
  },
  "franklin94_icslp": {
   "authors": [
    [
     "Sue",
     "Franklin"
    ],
    [
     "Julie",
     "Morris"
    ],
    [
     "Judy",
     "Turner"
    ]
   ],
   "title": "Dissociations in word deafness",
   "original": "i94_0771",
   "page_count": 4,
   "order": 207,
   "p1": "771",
   "pn": "774",
   "abstract": [
    "Four dysphasic patients were investigated who all had impairments in auditory word comprehension. Testing on auditory processing, phoneme discrimination and lexical tests suggested that each patient had a different pattern of deficit. JS was impaired at all auditory, non-speech tests as well as tests which required the processing of auditory speech sounds. TON's deficit was confined to speech sounds, but interpretation of this deficit was complicated by the fact that he had a mild hearing loss. MW was only impaired in the test that required semantic processing, and could thus be considered a word meaning deaf patient. DrO's comprehension deficit was similar to that of MW except for a specific problem in tasks requiring phonological processing. This research was supported by the Medical Research Council. We are also grateful to the British Aphasiology Society for their support.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-207"
  },
  "uno94_icslp": {
   "authors": [
    [
     "Akira",
     "Uno"
    ],
    [
     "Jun",
     "Tanemura"
    ],
    [
     "Koichi",
     "Higo"
    ]
   ],
   "title": "Recovery mechanism of naming disorders in aphasic patients: effects of different training modalities",
   "original": "i94_0775",
   "page_count": 4,
   "order": 208,
   "p1": "775",
   "pn": "778",
   "abstract": [
    "Recovery mechanism of naming disorders was investigated. Two kinds of naming training methods, namely, one with Kanji writing, and the other with initial syllable phonemic cue and repetition, were administered to two aphasic patients. As a result, both the improvement rate and the stability of performance were better after the training with Kanji writing than the other method. It suggests that the improvement of the namng performance with the training of Kanji writing was activated by the information processes of writing Kanji and reading them aloud.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-208"
  },
  "brown94b_icslp": {
   "authors": [
    [
     "Michael K.",
     "Brown"
    ],
    [
     "Stephen C.",
     "Glinski"
    ]
   ],
   "title": "Stochastic context-free language modeling with evolutional grammars",
   "original": "i94_0779",
   "page_count": 4,
   "order": 209,
   "p1": "779",
   "pn": "782",
   "abstract": [
    "We describe an efficient implementation of a stochastic context-free language model suitable for real-time processing of speech and handwriting. Several similar prior efforts have some overlap with the methods we present, but in each case some shortcoming was overcome. This paper describes the first integration of a stochastic context-free language model with a stochastic acoustic model in a way that makes the language modeling overhead very small.\n",
    "This is achieved through the use of an Evolutional Grammar (EG) consisting of both ephemeral (Markov) word models and grammatical rules which are dynamically expanded while processing speech input. The efficacy of this approach is tested with the ARPA Naval Resource Management speech application.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-209"
  },
  "ward94b_icslp": {
   "authors": [
    [
     "Nigel",
     "Ward"
    ]
   ],
   "title": "A lightweight parser for speech understanding",
   "original": "i94_0783",
   "page_count": 4,
   "order": 210,
   "p1": "783",
   "pn": "786",
   "abstract": [
    "To integrate well into a speech understanding system, a parser should be evidential. The current best evidential approach, stochastic context-free grammar parsing, is not completely satisfactory. This paper presents a new model. Here the parser's input is the raw lattice of word hypotheses, and its output is a set of scored clues to a semantic interpretation; and conversely for feedback. The parser is based on \"construction hypotheses\" each of which maps directly to word hypotheses, on the one hand, and conceptual hypotheses, on the other. These construction hypotheses are mutually independent, which simplifies the parser. These ideas are incorporated in a tiny prototype speech understanding system.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-210"
  },
  "kawabata94_icslp": {
   "authors": [
    [
     "Takeshi",
     "Kawabata"
    ]
   ],
   "title": "Dynamic probabilistic grammar for spoken language disambiguation",
   "original": "i94_0787",
   "page_count": 4,
   "order": 211,
   "p1": "787",
   "pn": "790",
   "abstract": [
    "The dynamic probabilistic grammar (DPG) is a context-free grammar whose rule probabilities are dynamically controlled by a hidden Markov model (HMM). This HMM receives a rule number as an input symbol, and outputs the probability of the rule number sequence. The DPG parser generates plural rule sequences representing different syntactic structures. The parser calculates the probability for each rule sequence, and selects the rule sequence (i.e. syntactic structure) which achieves maximum probability. This disambiguation mechanism is also effective for grammar-based speech recognition systems. The number of candidate words (called perplexity) can be reduced effectively using this mechanism. The DPG provides a stochastic framework for CFG-class spoken language processing.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-211"
  },
  "yamaguchi94_icslp": {
   "authors": [
    [
     "Kouichi",
     "Yamaguchi"
    ],
    [
     "Harald",
     "Singer"
    ],
    [
     "Shoichi",
     "Matsunaga"
    ],
    [
     "Shigeki",
     "Sagayama"
    ]
   ],
   "title": "Speaker-consistent parsing for speaker-independent continuous speech recognition",
   "original": "i94_0791",
   "page_count": 4,
   "order": 212,
   "p1": "791",
   "pn": "794",
   "abstract": [
    "This paper describes a novel speaker-independent speech recognition method, called \"speaker-consistent parsing\", which is based on an intra-speaker correlation called the speaker-consistency principle. We focus on the fact that a sentence or a string of words is tittered by an individual speaker even in a speaker-independent task. Thus, the proposed method searches through speaker variations in addition to the contents of utterances. As a result of the recognition process, an appropriate standard speaker is selected for speaker adaptation. This new method is experimentally compared with a conventional speaker-independent speech recognition method. Since the speaker-consistency principle best demonstrates its effect with a large number of training and test speakers, a small-scale experiment may not fully exploit this principle. Nevertheless, even the results of our small-scale experiment show that the new method significantly outperforms the conventional method. In addition, this framework's speaker selection mechanism can drastically reduce the likelihood map computation.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-212"
  },
  "nagata94_icslp": {
   "authors": [
    [
     "Masaaki",
     "Nagata"
    ]
   ],
   "title": "A stochastic morphological analyzer for spontaneously spoken languages",
   "original": "i94_0795",
   "page_count": 4,
   "order": 213,
   "p1": "795",
   "pn": "798",
   "abstract": [
    "We present a morphological analysis method for the phonetic transcription of spontaneous speech using a stochastic language modeling technique and an efficient two-pass N-best search strategy. It can segment a phonetically transcribed utterance into word, assign parts of speech to each segmented words, and convert the phonetic transcription into an orthographic transcription, which, in the case of Japanese, means the conversion from \"hiragana\" (phonogram) to \"kanji\" (ideogram). The morphological analyzer can handle pauses, interjections, restatements and chimings, all which are characteristics of spontaneous speech, by learning the parameters of the language model directly from the ¦phonetic transcription. The proposed morphological analyzer achieves 95.0% recall and 95.3% precision on closed text when it was trained and tested on a portion (containing 172,826 words) of the ATR Corpus, telephone dialogues in the conference registration domain.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-213"
  },
  "antoine94_icslp": {
   "authors": [
    [
     "Jean-Yves",
     "Antoine"
    ],
    [
     "Jean",
     "Caelen"
    ],
    [
     "Bertand",
     "Caillaud"
    ]
   ],
   "title": "Automatic adaptive understanding of spoken language by cooperation of syntactic parsing and semantic priming",
   "original": "i94_0799",
   "page_count": 4,
   "order": 214,
   "p1": "799",
   "pn": "802",
   "abstract": [
    "This paper focuses on the modelling of the linguistic level of MICRO, a multi-agents speech understanding system largely inspired by cognitive models. It describes the cooperation between, on the one hand a syntactic parser using a Lexical Functional Grammar, and on the other hand an semantic analyser. The semantic analysis is achieved through a mechanism semantic priming carried out by a incremental associative network. We emphasize the adaptive abilities of such a cooperation, particularly in case of ungrammatical utterances, which are very common in spoken language.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-214"
  },
  "ratnaparkhi94_icslp": {
   "authors": [
    [
     "Adwait",
     "Ratnaparkhi"
    ],
    [
     "Salim",
     "Roukos"
    ],
    [
     "R. Todd",
     "Ward"
    ]
   ],
   "title": "A maximum entropy model for parsing",
   "original": "i94_0803",
   "page_count": 4,
   "order": 215,
   "p1": "803",
   "pn": "806",
   "abstract": [
    "Traditional natural language parsers are based on rewrite rule systems developed in an arduous, time-consuming manner by grammarians. Most of the grammarian's work is devoted to the disambiguation process, first hypothesizing rules which dictate constituent categories and relationships among words in ambiguous sentences, and then seeking exceptions and corrections to these rules. This grammar refinement process is time-consuming and difficult, and has not yet resulted in a grammar which can be used by a parser to analyze accurately a large corpus of unrestricted text.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-215"
  },
  "kiyama94_icslp": {
   "authors": [
    [
     "Jiro",
     "Kiyama"
    ],
    [
     "Yoshiaki",
     "Itoh"
    ],
    [
     "Ryuichi",
     "Oka"
    ]
   ],
   "title": "Sentence spotting using continuous structuring method",
   "original": "i94_0807",
   "page_count": 4,
   "order": 216,
   "p1": "807",
   "pn": "810",
   "abstract": [
    "We have proposed Sentence Spotting[3] as one of the most suitable ways to handle spontaneous speech. In this paper, a novel sentence spotting algorithm called \"Continuous Structuring Method\"(CSM) is presented. This algorithm is able to spot word sequences that satisfy higher constraints such as syntax from the stream of input speech. This algorithm was evaluated in two experiments. In the first experiment, we used 110 sentences uttered continuously by 10 open male speakers. The task had a 113-word vocabulary. We used a phrase pair grammar for the syntactic constraint, with a phrase branching factor of 7.8. A sentence recognition rate of 70.0% and a phrase recognition rate of 89.0% were obtained. The second experiment was for simulated spontaneous speech containing a non-intensional phrase and a long pause and so on, spoken by 5 open male speakers. The syntax and vocabulary were the same as in the first one. We confirmed that CSM shows good performance throughout various utterance styles.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-216"
  },
  "sakamoto94_icslp": {
   "authors": [
    [
     "Hiroyuki",
     "Sakamoto"
    ],
    [
     "Shoichi",
     "Matsunaga"
    ]
   ],
   "title": "Continuous speech recognition using a dialog-conditioned stochastic language model",
   "original": "i94_0811",
   "page_count": 4,
   "order": 217,
   "p1": "811",
   "pn": "814",
   "abstract": [
    "This work attempts to improve recognition accuracy by predicting the next utterance in a dialog. We propose a dialog-conditioned stochastic language model that is applied to dialog speech recognition. Each dialog-conditioned stochastic language model has been constructed with text data of a certain situation and implemented using a Japanese syllable trigram. Each situation was denned in advance to predict next utterances effectively. Experiments in continuous speech recognition have shown that these models decrease perplexity and improve recognition accuracy in comparison with conventional dialog-uniform stochastic language models.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-217"
  },
  "kawahara94_icslp": {
   "authors": [
    [
     "Tatsuya",
     "Kawahara"
    ],
    [
     "Toshihiko",
     "Munetsugu"
    ],
    [
     "Norihide",
     "Kitaoka"
    ],
    [
     "Shuji",
     "Doshita"
    ]
   ],
   "title": "Keyword and phrase spotting with heuristic language model",
   "original": "i94_0815",
   "page_count": 4,
   "order": 218,
   "p1": "815",
   "pn": "818",
   "abstract": [
    "We discuss on a word spotting strategy that incorporates a language model as heuristics. It judges the word existence based on not only the matching score of the word itself but also the plausibility of the rest part as a sentence. Several language models are examined with respect to the accuracy and robustness, and the followings are conclusions: (1) Syllable-level knowledge is robust but insufficient. (2) Word-level knowledge is very effective and robust against spontaneous speech. (3) Word-pair constraint is most powerful but not robust. We further propose to incorporate phrase-level syntax into the spotting unit. The phrase-level syntax is rarely violated even in spontaneous utterances and significantly reduces the task perplexity. It turned out effective especially in getting a higher detection rate with a small number of false alarms.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-218"
  },
  "murakami94_icslp": {
   "authors": [
    [
     "Jin'ichi",
     "Murakami"
    ],
    [
     "Shoichi",
     "Matsunaga"
    ]
   ],
   "title": "A spontaneous speech recognition algorithm using word trigram models and filled-pause procedure",
   "original": "i94_0819",
   "page_count": 4,
   "order": 219,
   "p1": "819",
   "pn": "822",
   "abstract": [
    "This paper describes an effective recognition algorithm that uses word trigram models directly and a procedure for dealing with filled-pauses in spontaneous speech. This recognition algorithm greatly reduces the memory requirements and computational costs by employing two techniques: beam search and an improved Viterbi search. With these methods, execution can be performed in a 15M byte space for about a 1500-word vocabulary. The filled-pause procedure, capable of handing many filled-pauses in spontaneous speech, is then examined for spontaneous speech recognition. Even though the proposed algorithm employs a simple procedure, a 42% sentence recognition rate is obtained for spontaneous speech. Including the semantically correct sentences, the sentence recognition rate is about 15%.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-219"
  },
  "yamada94_icslp": {
   "authors": [
    [
     "Masayuki",
     "Yamada"
    ],
    [
     "Yasuhiro",
     "Komori"
    ],
    [
     "Yasunori",
     "Ohora"
    ]
   ],
   "title": "Active/non-active word control using garbage model, unknown word re-evaluation in speech conversation",
   "original": "i94_0823",
   "page_count": 4,
   "order": 220,
   "p1": "823",
   "pn": "826",
   "abstract": [
    "In this paper, we propose a new active/non-active word control algorithm, which we call unknown word re-evaluation algorithm. In the algorithm, we incorporate an unknown word detection using Garbage Models into a speech recognizer which dynamically changes its vocabulary. When the Garbage Models are perceived, non-active words suitable for the situation are activated and compared with the unknown part of the speech. Information for the re-evaluation is obtained from the speech containing the unknown part. Supplementary information may be obtained in the succeeding discourse in some cases. The algorithm makes a dynamic vocabulary recognizer more useful decreasing the risk of misprediction against the next utterance without spoiling its advantages in recognition speed and rate. In our preliminary experiment, 86.4% of proper nouns were correctly re-evaluated using the Garbage Models, while the false alarms of 5% were found.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-220"
  },
  "chase94_icslp": {
   "authors": [
    [
     "L.",
     "Chase"
    ],
    [
     "R.",
     "Rosenfeld"
    ],
    [
     "Wayne",
     "Ward"
    ]
   ],
   "title": "Error-responsive modifications to speech recognizers: negative n-grams",
   "original": "i94_0827",
   "page_count": 4,
   "order": 221,
   "p1": "827",
   "pn": "830",
   "abstract": [
    "We describe an error analysis technique that facilitates blame assignment among the various components of a speech recognizer and provides insight into their behavior. Tools are presented that help clarify how each of the component models and their interactions contribute to the bottom line performance. We use this technique to study the performance of the backoff [4] language model. The analysis highlights the significant effect of negative n-grams-sequences of words not seen in the training data. This leads to two modifications to the decoder, both of which are presented with experimental results. The first modification failed so far to improve recognition performance. The second yields up to 4% reduction in word error\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-221"
  },
  "suhm94_icslp": {
   "authors": [
    [
     "B.",
     "Suhm"
    ],
    [
     "Alex",
     "Waibel"
    ]
   ],
   "title": "Towards better language models for spontaneous speech",
   "original": "i94_0831",
   "page_count": 4,
   "order": 222,
   "p1": "831",
   "pn": "834",
   "abstract": [
    "In our effort to build a speech-to-speech translation system for spontaneous spoken dialogs we have developed several methods to improve the language models of the speech decoder of the system. We attempt to take advantage of natural equivalence word classes, frequently occuring word phrases, and discourse structure. Each of these methods was tested on spontaneous English, German and Spanish human-human dialogs.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-222"
  },
  "mccandless94_icslp": {
   "authors": [
    [
     "Michael K.",
     "McCandless"
    ],
    [
     "James R.",
     "Glass"
    ]
   ],
   "title": "Empirical acquisition of language models for speech recognition",
   "original": "i94_0835",
   "page_count": 4,
   "order": 223,
   "p1": "835",
   "pn": "838",
   "abstract": [
    "In this paper we describe an algorithm which automatically constructs a structured probabilistic language model for speech recognition in the atis air-travel domain [1]. Starting from unlabelled training sentences, the algorithm runs in two phases. In the first phase a simple context-free grammar is acquired which attempts to model the simple bottom-up structure (syntax and semantics) of the training sentences. This phase.is a form of grammar inference. In the second phase this grammar is incorporated into a phrase class w-gram formalism to assign probability to test sentences. We used this model to evaluate perplexity on an independent test set, and also to evaluate word error rate using the SUMMIT speech recognition system [2]. In these experiments the acquired model achieved a reduction in perplexity over a standard word trigram model by 6.7% (from 15.92 to 14.86), and a reduction in the number of free model parameters by 22.2%. We also observed a corresponding reduction in the word error rate of 5.5% (from 18.3% to 17.3%).\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-223"
  },
  "fujio94_icslp": {
   "authors": [
    [
     "Shigeru",
     "Fujio"
    ],
    [
     "Yoshinori",
     "Sagisaka"
    ],
    [
     "Norio",
     "Higuchi"
    ]
   ],
   "title": "Prediction of prosodic phrase boundaries using stochastic context-free grammar",
   "original": "i94_0839",
   "page_count": 4,
   "order": 224,
   "p1": "839",
   "pn": "842",
   "abstract": [
    "We propose a method of predicting prosodic phrase boundaries using a SCFG (stochastic context-free grammar) for non-bracketed input word attributes. In this paper, prosodic phrase boundaries are predicted using information generated by a SCFG trained using phrase dependency bracketings and prosody bracketings. Using the Inside-Outside algorithm for training, corpora with phrase dependency brackets are first used to train the SCFG from scratch. Next, this SCFG is re-trained using the same corpora with prosody brackets. Then, the probability of each bracketing structure is computed using the SCFG, and is used as a parameter in the prediction of the prosodic phrase boundaries. To examine the effect of these parameters, prosodic phrase boundaries were predicted. Held-out data: 92.9% of the prosodic phrase boundaries, and 83.1% of the non-prosodic phrase boundaries, were correctly predicted.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-224"
  },
  "giachin94_icslp": {
   "authors": [
    [
     "Egidio",
     "Giachin"
    ],
    [
     "Paolo",
     "Baggia"
    ],
    [
     "Giorgio",
     "Micca"
    ]
   ],
   "title": "Language models for spontaneous speech recognition: a bootstrap method for learning phrase digrams",
   "original": "i94_0843",
   "page_count": 4,
   "order": 225,
   "p1": "843",
   "pn": "846",
   "abstract": [
    "This study refers to the search for language models that are suitable for the recognition of spontaneous speech occurring in task-specific man-machine dialogue systems. Bigrams are an effective means for that purpose, however they only capture constraints between adjacent words. Task-specific training corpora are very expensive to collect and hence they are likely to be insufficient to reliably train trigrams. On the other hand, the type of sentences employed in these tasks are characterized by highly repetitive phrases that do occur enough times to suggest trying to automatically find and model them as if they were individual dictionary elements, so as to favor their recognition. The determination of the word sequences to model is accomplished according to a perplexity minimization criterion, thus it is optimal insofar perplexity is a reliable quality measure for a language model. The procedure is iterative: starting with the original 1-word elements, it finds the pair of words for which the perplexity reduction is higher and connects them into a 2-word element. By cyclically continuing this action it \"bootstraps\" to longer-span elements, until no more perplexity reduction is obtained. Some variants of the algorithm are discussed and compared. This model produced a more than 20% perplexity reduction over 1-word bigrams, which makes it favorably comparable to trigrams.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-225"
  },
  "woszczyna94_icslp": {
   "authors": [
    [
     "M.",
     "Woszczyna"
    ],
    [
     "Alex",
     "Waibel"
    ]
   ],
   "title": "Inferring linguistic structure in spoken language",
   "original": "i94_0847",
   "page_count": 4,
   "order": 226,
   "p1": "847",
   "pn": "850",
   "abstract": [
    "We demonstrate the applications of Markov Chains and HMMs to modeling of the underlying structure in spontaneous spoken language. Experiments with supervised train- ing cover the detection of the current dialog state and identification of the speech act as used by the speech translation component in our JANUS Speech-to-Speech Translation System. HMM training with hidden states is used to uncover other levels of structure in the task. The possible use of the model for perplexity reduction in a continuous speech recognition system is also demonstrated. To achieve improvement over a state independent bigram language model, great care must be taken to keep the number of model parameters small in the face of limited amounts of training data from transcribed spontaneous speech.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-226"
  },
  "bordel94_icslp": {
   "authors": [
    [
     "G.",
     "Bordel"
    ],
    [
     "I.",
     "Torrest"
    ],
    [
     "Enrique",
     "Vidal"
    ]
   ],
   "title": "Back-off smoothing in a syntactic approach to language modelling",
   "original": "i94_0851",
   "page_count": 4,
   "order": 227,
   "p1": "851",
   "pn": "854",
   "abstract": [
    "Statistical Language Modelling (N-grams) have been extensively used in Continuous Speech Recognition. However in practice only low values of N are used and as a consequence only very local constraints are represented. It has been recently shown that k-testable Stochastic Languages (k-TS) are strictly equivalent to N-grams so that choosing K-TS or N-grams could be just a matter of representation. A grammatical formalism of the N-gram Language Modelling is presented in this work as it present several advantages. Under the proposed syntactical approach new smoothing techniques can be considered. Alternative ways to obtain accurate probability distributions to be assigned to unseen N-grams are then proposed and the new redistribution formulae are established. Moreover, this new perspective suggests that a good inference ability does not need the symmetry principle to be globally applied but only locally. These proposals has been experimentally compared to the classical back-off method over a task-oriented spontaneous Spanish Speech corpus. A decrease in test-set perplexity of up to 6.5% was achieved when the new proposed approaches were used.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-227"
  },
  "shih94_icslp": {
   "authors": [
    [
     "H.-H.",
     "Shih"
    ],
    [
     "Steve J.",
     "Young"
    ]
   ],
   "title": "Computer assisted grammar construction",
   "original": "i94_0855",
   "page_count": 4,
   "order": 228,
   "p1": "855",
   "pn": "858",
   "abstract": [
    "This paper presents a system for computer assisted grammar construction (CAGC) and its application in speech processing. The CAGC system is designed to infer linguistically-motivated broad-coverage stochastic context-free grammars (SCFGs) for large corpora, without requiring significant manual contributions. Our approach utilizes an extended inside-outside learning algorithm [1] to train a hybrid SCFG [2] from a bracketed training set. The bracketing information is derived by an automatic surface bracketing system (AUTO) specifically designed for this purpose[3]. Experimental results, evaluated by using Parseval metrics [4], demonstrate that the CAGC system is capable of inferring a grammar from a subset of the Wall Street. Journal (WSJ) tagged text corpus and that the inferred grammar achieves high coverage and good precision. As an application, the inferred grammar acts as a language model for rescoring N-best outputs from a speech recognizer [5].\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-228"
  },
  "antoniol94_icslp": {
   "authors": [
    [
     "Giuliano",
     "Antoniol"
    ],
    [
     "Fabio",
     "Brugnara"
    ],
    [
     "Mauro",
     "Cettolo"
    ],
    [
     "Marcello",
     "Federico"
    ]
   ],
   "title": "Language model estimations and representations for real-time continuous speech recognition",
   "original": "i94_0859",
   "page_count": 4,
   "order": 229,
   "p1": "859",
   "pn": "862",
   "abstract": [
    "This paper compares different ways of estimating bigram language models and of representing them in a finite state network used by a beam-search based, continuous speech, and speaker independent HMM recognizer. Attention is focused on the n-gram interpolation scheme for which seven models are considered. Among them, the Stacked estimated linear interpolated model favourably compares with the best known ones. Further, two different static representations of the search space are investigated: \"linear\" and \"tree-based\". Results show that the latter topology is better suited to the beam-search algorithm. Moreover, this representation can be reduced by a network optimization technique, which allows the dynamic size of the recognition process to be decreased by 60%. Extensive recognition experiments on a 10,000-word dictation task with four speakers are described in which an average word accuracy of 93% is achieved with real-time response.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-229"
  },
  "jacob94_icslp": {
   "authors": [
    [
     "Bruno",
     "Jacob"
    ],
    [
     "Regine",
     "André-Obrecht"
    ]
   ],
   "title": "Sub-dictionary statistical modeling for isolated word recognition",
   "original": "i94_0863",
   "page_count": 4,
   "order": 230,
   "p1": "863",
   "pn": "866",
   "abstract": [
    "A lexical access using a two-stage algorithm based on the description of the lexicon into two types of units is proposed. To access to a sub-dictionnary, phonetic classes are used to build an HMM. Then this sub-dictionary is modeled by a temporary HMM, with pseudo-diphones units, to produce the recognized word. An acoustic pre-processing adapted to the pseudo-diphone decomposition and an adequate \"HMM Compiler\" toolkit are used in this application. Experiments are made to evaluate this approach with the 500 most frequents words in french language.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-230"
  },
  "jardino94_icslp": {
   "authors": [
    [
     "Michele",
     "Jardino"
    ]
   ],
   "title": "A class bigram model for very large corpus",
   "original": "i94_0867",
   "page_count": 4,
   "order": 231,
   "p1": "867",
   "pn": "870",
   "abstract": [
    "As pointed out by Jelinek the n-gram word model is a very efficient model but not well adapted for highly inflected languages such as French. So we have developed a class-based bigram model determined entirely automatically from written corpora. The classes are not predefined, the words are not tagged, the solely assumption is the number of classes. We get a robust model which insures a more complete coverage of the succession probabilities (the studied training text of 2 millions of French words gives a coverage rate of the class bigram model of 50% to be compared with 0.1% for the word bigram model).\n",
    "Here we present results on new classifications of the text defined above, obtained with more than one possible class for each word, as well as optimised combinations of word and class bigram models.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-231"
  },
  "amano94_icslp": {
   "authors": [
    [
     "Akio",
     "Amano"
    ],
    [
     "Toshiyuki",
     "Odaka"
    ]
   ],
   "title": "A spoken dialogue system based on hierarchical feedback mechanism",
   "original": "i94_0871",
   "page_count": 4,
   "order": 232,
   "p1": "871",
   "pn": "874",
   "abstract": [
    "This paper presents a \"Hierarchical Feedback Mechanism\" which improves user-friendliness of a spoken dialogue system. The authors have developed a spoken dialogue system whose application is a transportation guidance for several branches of Hitachi, Ltd[l]. The system consists of a speech recognition part, a speech synthesis part, an information retrieval part and a dialogue control part. Evaluation of this system reveals that the system has several problems such as, 1) Tt is not easy for users to speak to the system. 2) Users can not manage if a recognition error occurs. Main causes of these problems are the fact that users can not understand the status of the system behavior and that some system responses are not adequate for users to speak easily to the system. To solve these problems, the authors developed a \"Hierarchical Feedback Mechanism\". A basic idea is to show the status of the speech recognition part to the users. Since the speech recognition part has a hierarchical structure , showing the status of each unit in the speech recognition part is helpful information for the users. To achieve this, the authors introduced hierarchical feedback mechanism into the dialogue control part of the system. Trial use of 4 subjects confirmed the improvement of user-friendliness of the system.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-232"
  },
  "bernsen94_icslp": {
   "authors": [
    [
     "Niels Ole",
     "Bernsen"
    ],
    [
     "Laila",
     "Dybkjaer"
    ],
    [
     "Hans",
     "Dybkjaer"
    ]
   ],
   "title": "A dedicated task-oriented dialogue theory in support of spoken language dialogue systems design",
   "original": "i94_0875",
   "page_count": 4,
   "order": 233,
   "p1": "875",
   "pn": "878",
   "abstract": [
    "This paper presents steps towards an incremental dialogue theory in support of functional design of successive generations of spoken language dialogue systems. Dialogue functionality theory departs from a simple task taxonomy and develops a systematic set of concepts or dialogue elements and implementation strategies important to dialogue management. Increasingly complex tasks require the introduction of an increasing number of dialogue elements to ensure acceptable user-system interaction.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-233"
  },
  "ehsani94_icslp": {
   "authors": [
    [
     "Farzad",
     "Ehsani"
    ],
    [
     "Kaichiro",
     "Hatazaki"
    ],
    [
     "Jun",
     "Noguchi"
    ],
    [
     "Takao",
     "Watanabe"
    ]
   ],
   "title": "Interactive speech dialogue system using simultaneous understanding",
   "original": "i94_0879",
   "page_count": 4,
   "order": 234,
   "p1": "879",
   "pn": "882",
   "abstract": [
    "This paper describes a novel way to implement a speech dialogue system. This system can accept new inputs while simultaneously analyzing the previous utterance. Two ticket reservation speech dialogue systems were built; one of them was based on the above method, and the other was based on accepting new inputs after analyzing the previous utterance. It was found that the new method improved the total average utterance accuracy by 5.4% and the total time spent to solve tasks by 4.7%. This shows that the new method is promising for increasing the interactiveness for the speech dialogue system, and, as a result, for making the system more user-friendly.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-234"
  },
  "araki94_icslp": {
   "authors": [
    [
     "Masahiro",
     "Araki"
    ],
    [
     "Taro",
     "Watanabe"
    ],
    [
     "Felix Merida",
     "Quimbo"
    ],
    [
     "Shuji",
     "Doshita"
    ]
   ],
   "title": "A cooperative man-machine dialogue model for problem solving",
   "original": "i94_0883",
   "page_count": 4,
   "order": 235,
   "p1": "883",
   "pn": "886",
   "abstract": [
    "In this paper, we present a constructive model of man-machine dialogue for problem solving. Our spoken dialogue system aims to solve scheduling problems cooperatively by interacting user and system. An important feature of this model is that it separates the knowledge about dialogue structure (conversational space) and about problem (problem solving space) in order to make correspondence to utterances and actions. Conversational space is designed for identifying user's speech act and constructing dialogue segment. Problem solving space represents the structure of the problem and the most appropriate direction for solving problem. For incorporating probabilistic reasoning, each space is represented by network. This time, we adopt this model to keyword based plan recognition and topic prediction of next utterance. In small experiment, this system hit the topics of next user's utterance at 47% at the first candidate, and at 92% within the third candidates.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-235"
  },
  "yoshioka94_icslp": {
   "authors": [
    [
     "Osamu",
     "Yoshioka"
    ],
    [
     "Yasuhiro",
     "Minami"
    ],
    [
     "Kiyohiro",
     "Shikano"
    ]
   ],
   "title": "A multi-modal dialogue system for telephone directory assistance",
   "original": "i94_0887",
   "page_count": 4,
   "order": 236,
   "p1": "887",
   "pn": "890",
   "abstract": [
    "To evaluate our algorithm for large-vocabulary speaker-independent continuous speech recognition based on the HMM-LR technique and to collect dialogue data between computer and user, we have implemented a multi-modal dialogue system for telephone directory assistance. Our system uses three input devices (microphone, mouse, and keyboard) and two output devices (display and speaker). Twenty telephone number retrieval tasks were used to evaluate the system. In an evaluation involving twenty users, the task completion rate was 99.0%. About 75% of the users said they preferred this system to looking up telephone numbers in a telephone directory.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-236"
  },
  "terry94_icslp": {
   "authors": [
    [
     "Mark",
     "Terry"
    ],
    [
     "Randall",
     "Sparks"
    ],
    [
     "Patrick",
     "Obenchain"
    ]
   ],
   "title": "Automated query identification in English dialogue",
   "original": "i94_0891",
   "page_count": 4,
   "order": 237,
   "p1": "891",
   "pn": "894",
   "abstract": [
    "A question-acknowledgment statement classifier (QUASI) was built for use in an automated 'street-map directions' dialogue system. The system uses a cepstral pitch extractor to construct a pitch contour, which is then processed by a set of shape detectors. This information is sent to a rule component to classify the statement type. Using cellular telephone voice samples, basic query and statement types, where rising or falling pitch is the predominant cue, were classified with an accuracy of 90%. A Neural-Net classifier, trained with a back-propagation algorithm, gave a recognition accuracy of 85 % using the same pitch contours. Statements such as, 'okay' and 'uh-huh', which are employed to acknowledge correct receipt of an instruction and to signal readiness for further information, are commonly produced with a intonation rise. We tried to identify this statement type by utilizing a pitch rise shape cue. This cue proved unreliable and instead key-word spotting was employed to identify such statements.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-237"
  },
  "sakai94_icslp": {
   "authors": [
    [
     "Keiichi",
     "Sakai"
    ],
    [
     "Yuji",
     "Ikeda"
    ],
    [
     "Minoru",
     "Fujita"
    ]
   ],
   "title": "Robust discourse processing considering misrecognition in spoken dialogue system",
   "original": "i94_0895",
   "page_count": 4,
   "order": 238,
   "p1": "895",
   "pn": "898",
   "abstract": [
    "In this paper, we propose a new robust discourse processing compensating for misrecognition in a spoken dialogue system. As the current speech recognition is not able to obtain adequate accuracy, a robust discourse processing which compensates for misrecognition is required in a spoken dialogue system. Here, we developed some functions for the robust discourse processing to reduce extra interactions caused by misrecognition. We incorporated the robust discourse processing functions into a spoken dialogue system and evaluated the effectiveness. The experimental result showed 27% reduction of extra interactions.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-238"
  },
  "watanuki94_icslp": {
   "authors": [
    [
     "Keiko",
     "Watanuki"
    ],
    [
     "Kenji",
     "Sakamoto"
    ],
    [
     "Fumio",
     "Togawa"
    ]
   ],
   "title": "Analysis of multimodal interaction data in human communication",
   "original": "i94_0899",
   "page_count": 4,
   "order": 239,
   "p1": "899",
   "pn": "902",
   "abstract": [
    "We are developing multimodal man-machine interfaces through which users can communicate by integrating speech, gaze, facial expressions, and gestures such as nodding and finger pointing. Such multimodal interfaces are expected to provide more flexible, natural and productive communications between humans and computers. To achieve this goal, we have taken the approach of modeling human behaviors in the context of ordinary face-to-face conversations. As a first step, we have implemented a system which utilizes video and audio recording equipment to capture verbal and nonverbal information in interpersonal communications. Using this system, we have collected data from a task-oriented conversation between a guest (subject) and a receptionist at company reception, and quantitatively analyzed this data with respect to multi-modalities. This paper presents data showing that head nodding and gaze are related to speech content, acting to supplement speech information. We also discuss issues related to the timing of turn taking and listener responses, which yield a natural rhythm for human/computer interaction.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-239"
  },
  "arai94_icslp": {
   "authors": [
    [
     "Kazuhiro",
     "Arai"
    ]
   ],
   "title": "Changes in user's responses with use of a speech dialog system",
   "original": "i94_0903",
   "page_count": 4,
   "order": 240,
   "p1": "903",
   "pn": "906",
   "abstract": [
    "In assessing a speech dialog system, changes in the user's responses to exposure to the system must be considered. In this paper, the changes in user's responses with the repeated use of a speech dialog system are described focusing on different prompt styles. Experiments examine four different styles of prompts to compare their effectiveness. In the experiments, the dialogs between a user and the system are based on the Wizard-of-Oz method. The result shows that at least two back to back trials are necessary to achieve a stable response. This paper describes the experiments and its results.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-240"
  },
  "katunobu94_icslp": {
   "authors": [
    [
     "Itou",
     "Katunobu"
    ],
    [
     "Akiba",
     "Tomoyosi"
    ],
    [
     "Hasegawa",
     "Osamu"
    ],
    [
     "Hayamizu",
     "Satoru"
    ],
    [
     "Tanaka",
     "Kazuyo"
    ]
   ],
   "title": "Collecting and analyzing nonverbal elements for maintenance of dialog using a wizard of oz simulation",
   "original": "i94_0907",
   "page_count": 4,
   "order": 241,
   "p1": "907",
   "pn": "910",
   "abstract": [
    "This paper describes collection and analysis of nonverbal elements for maintenance of dialogue using a Wizard of Oz (WOZ) simulation. The WOZ technique is one in which a human (who is called the wizard) plays the role of the system in a simulated interaction between human and system. Many WOZ experiments have been reported to date, but they have been mainly designed to develop a specific spoken dialogue application. Our WOZ experiment is designed to collect data in order to analyze the elements which allow interaction between human and machine. We have done experiments for forty subjects. Each subject talked with the WOZ system from twenty to seventy minutes.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-241"
  },
  "flammia94_icslp": {
   "authors": [
    [
     "Giovanni",
     "Flammia"
    ],
    [
     "James R.",
     "Glass"
    ],
    [
     "Michael",
     "Phillips"
    ],
    [
     "Joseph",
     "Polifroni"
    ],
    [
     "Stephanie",
     "Seneff"
    ],
    [
     "Victor W.",
     "Zue"
    ]
   ],
   "title": "Porting the bilingual voyager system to Italian",
   "original": "i94_0911",
   "page_count": 4,
   "order": 242,
   "p1": "911",
   "pn": "914",
   "abstract": [
    "Voyager is a spoken language system that acts as a tourist guide for a limited geographical area. It was initially developed for English and subsequently for Japanese. The kernel of the system is now independent of the language used. In this paper we present the development of the Italian version of this system, stressing which parameters of the system are language independent and which ones are language specific. We present the three essential stages of the development of the Italian voyager. The first stage was data collection: 2367 read and spontaneous sentences have been recorded by 49 native speakers. In the second stage, we defined a set of acoustic models and a lexicon of pronunciations for the summit speech recognizer. The third stage consisted of developing grammar rules and constraints for the semantic parser TINA. The paper presents results in speech recognition, natural language and spoken language system evaluations on an independent test set of 252 sentences spoken by 7 speakers.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-242"
  },
  "kikui94_icslp": {
   "authors": [
    [
     "Gen-Ichiro",
     "Kikui"
    ],
    [
     "Tsuyoshi",
     "Morimoto"
    ]
   ],
   "title": "Similarity-based identification of repairs in Japanese spoken language",
   "original": "i94_0915",
   "page_count": 4,
   "order": 243,
   "p1": "915",
   "pn": "918",
   "abstract": [
    "The paper presents an algorithm for locating parts of an utterance that is canceled by the speaker using repairs, or self-correct ions. Especially, we concentrate on identification of the on-set of a canceled part. The underlying idea is that the repair and reparendum intervals are as similar to each other as are two coordinated phrases. We adopted the similarity-based algorithm for analyzing Japanese coordination structures proposed by Kurohashi and Nagao. The success rate of the algorithm is 92% for tagged utterances in ATR dialogue database.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-243"
  },
  "larsen94_icslp": {
   "authors": [
    [
     "Lars Bo",
     "Larsen"
    ],
    [
     "Anders",
     "Baekgaard"
    ]
   ],
   "title": "Rapid prototyping of a dialogue system using a generic dialogue development platform",
   "original": "i94_0919",
   "page_count": 4,
   "order": 244,
   "p1": "919",
   "pn": "922",
   "abstract": [
    "This paper presents the development of a simple, well-defined dialogue system, utilising a Generic Dialogue System Platform for prototyping. The application domain of the present system is an automated book club service.\n",
    "The main focus of the paper is the description of the dialogue design and implementation processes, together with a description of the underlying principles and architecture of the platform.\n",
    "The dialogue was designed, and represented as a set of dialogue graphs. A close correspondence between the graphs and the DDL (Dialogue Description Language) implementation is demonstrated, being one of the main reasons for the short implementation phase.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-244"
  },
  "naito94_icslp": {
   "authors": [
    [
     "Shozo",
     "Naito"
    ],
    [
     "Akira",
     "Shimazu"
    ]
   ],
   "title": "Heuristics for generating acoustic stress in dialogues and examination of their validity",
   "original": "i94_0923",
   "page_count": 4,
   "order": 245,
   "p1": "923",
   "pn": "926",
   "abstract": [
    "This paper examines the validity of five types of heuristics for generating acoustic stresses which had been proposed based on the analysis of cooperative dialogues. The five types of heuristics are, 1) noun that first appears in dialogues, 2) verb, adjective, or adjectival verb that first appears in dialogues, 3) new topics in dialogues, 4) sentence final particle of interrogative or rhetorical interrogative sentences, and 5) contrasting. These heuristics have been incorporated into an experimental natural language generation system.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-245"
  },
  "siroux94_icslp": {
   "authors": [
    [
     "Jacques",
     "Siroux"
    ],
    [
     "Mouloud",
     "Kharoune"
    ],
    [
     "Marc",
     "Guyomard"
    ]
   ],
   "title": "Application and dialogue in the sundial system",
   "original": "i94_0927",
   "page_count": 4,
   "order": 246,
   "p1": "927",
   "pn": "930",
   "abstract": [
    "This article deals with the modelling of what we call the application in person-machine oral dialogue systems. We define the application as being all the services (tasks) which the system can carry out for the user. The different roles that the application can play in a dialogue system are examined: the management of dialogue, the control of the coherence of exchanged information, the calculation of predictions for speech recognition and finally, the supplying of all information which allow a co-operative dialogue. We then display a modelling of application in the context of the European Project SUNDIAL. This modelling allows the principal roles described to be effectively put into practice. It is used with the help of a high level interpreted language. It includes three main sections: the description of application data, the description of dialogue knowledge which is necessary in order to manage a user-friendly interaction and the description, by way of scripts and actions, of the different tasks possible. Finally the article describes the different problems which remain unsolved so far.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-246"
  },
  "kamei94_icslp": {
   "authors": [
    [
     "Shin-ichiro",
     "Kamei"
    ],
    [
     "Shin-ichi",
     "Doi"
    ],
    [
     "Takako",
     "Komatsu"
    ],
    [
     "Susumu",
     "Akamine"
    ],
    [
     "Hitoshi",
     "Iida"
    ],
    [
     "Kazunori",
     "Muraki"
    ]
   ],
   "title": "A dialog analysis using information of the previous sentence",
   "original": "i94_0931",
   "page_count": 4,
   "order": 247,
   "p1": "931",
   "pn": "934",
   "abstract": [
    "This paper proposes a dialog analyzing method that stores and uses some pieces of information in a certain local context, in order to solve actual problems. First, the authors examined English dialog corpora in order to find constraints in natural conversations. It has been found that more than 70 percent of the content of ellipses in the corpora that must be recovered when translating into Japanese, can be extracted from the last sentence uttered by the other person just preceding the current sentence. The authors then implemented a dialog analyzing method on an English-to-Japanese translation system that handles written texts. The method used memorizes and utilizes a local context constructed by both the current sentence and the last sentence of just the previous utterance by the other person. Last, the authors examined the feasibility of the proposed model by translating other English dialog corpora, transcriptions of telephone conversations. It has been found that the model generates appropriate Japanese translations in some linguistic phenomena that are relating to discourse, such as Ellipsis, Polysemy disambiguation, and Selection of Japanese TOPIC/SUBJECT particles W and 'ga.'.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-247"
  },
  "kogure94_icslp": {
   "authors": [
    [
     "Kiyoshi",
     "Kogure"
    ],
    [
     "Akira",
     "Shimazu"
    ],
    [
     "Mikio",
     "Nakano"
    ]
   ],
   "title": "Recognizing plans in more natural dialogue utterances",
   "original": "i94_0935",
   "page_count": 4,
   "order": 248,
   "p1": "935",
   "pn": "938",
   "abstract": [
    "We propose a dialogue utterance analysis model that treats an utterance as a sequence of acts of uttering its constituent words and constructs plans that connect these acts. In this model, intra-utterance and inter-utterance plan structures are both treated uniformly, implying that the input utterance boundaries do not have to be predetermined. Moreover, this model constructs possible partial plan structures during utterances, making it possible to obtain the information needed to make an adequate response to the input utterance before it has finished. The model can effectively handle fragmental utterances and utterances that are syntactico-semantically ambiguous but pragmatically unambiguous. Based on this model, we have developed an experimental plan-analysis module that parses plan structures by using a constraint-based linguistic framework. This module is used as the pragmatic analysis module of an experimental system for a cooperative distributed natural language understanding model called \"Ensemble.\"\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-248"
  },
  "hildebrandt94_icslp": {
   "authors": [
    [
     "Bernd",
     "Hildebrandt"
    ],
    [
     "Gernot A.",
     "Fink"
    ],
    [
     "Franz",
     "Kummert"
    ],
    [
     "Gerhard",
     "Sagerer"
    ]
   ],
   "title": "Understanding of time constituents in spoken language dialogues",
   "original": "i94_0939",
   "page_count": 4,
   "order": 249,
   "p1": "939",
   "pn": "942",
   "abstract": [
    "The analysis and interpretation of time constituents is a rather complex enterprise, since diverse time constituents are distributable in variable positions within an utterance. The first step in order to manage this complexity and variability is the syntactic analysis at phrase structure level. Within a utterance each time constituent is analyzed independently and tested for its syntactic coherence. A semantic interpretation of the time constituent has to follow. The second step consists of the analysis and interpretation at sentence structure level. The time interpretations need to be tested for consistency and merged into a single representation. Here it is usually possible to resolve ambiguities. As a last step, the interpretations of time constituents have to be merged at dialogue level. Although the system asks the user for verification of the time computed, users do seldom just reply 'yes' or 'no'. Mostly they add new information about time, and sometimes users correct the system's interpretation without explicit negation. Thus, the merging of time constituents at dialogue level becomes a rather complex issue.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-249"
  },
  "kumamoto94_icslp": {
   "authors": [
    [
     "Tadahiko",
     "Kumamoto"
    ],
    [
     "Akira",
     "Ito"
    ],
    [
     "Tsuyoshi",
     "Ebina"
    ]
   ],
   "title": "An analysis of Japanese sentences in spoken dialogue and its application to communicative intention recognition",
   "original": "i94_0943",
   "page_count": 4,
   "order": 250,
   "p1": "943",
   "pn": "946",
   "abstract": [
    "The authors are developing a computer-based consultant system. A user can verbally ask this system for help when problems occur. The system understands the user's utterances and then makes appropriate responses. Our target language is spoken Japanese.\n",
    "To develop such a consultant system, recognizing communicative intention (CI) from a user's utterance is essential. CI recognition consists of two parts, i.e., speech and natural-language processing. In this paper, the latter is focused on and we propose a method of constructing a CI description from a sentence transcribed from the user's utterance.\n",
    "Our approach is based on an analysis of actual user utterances. These utterances were collected in XMH (X-window-based electronic mail handling program) [2] usage experiments. XMH has a visual interface operated using a mouse and a keyboard. Using this XMH, 16 novice XMH users performed a given task, e.g., arranging a tea party. They asked a human consultant for help when they were in trouble and the consultant helped them solve the problem. Based on our analysis of these user utterances, a description form of CI is defined and the CI recognition method is developed by which a CI description is constructed from a sentence.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-250"
  },
  "hockey94_icslp": {
   "authors": [
    [
     "Beth Ann",
     "Hockey"
    ]
   ],
   "title": "Extra propositional focus and belief revision",
   "original": "i94_0947",
   "page_count": 3,
   "order": 251,
   "p1": "947",
   "pn": "950",
   "abstract": [
    "This paper addresses the problem of accented occurrences of the verb believe that cannot be accounted for as instances of focus by either semantic or pragmatic accounts. In the natural interpretation of these accented believe utterances, the embedded sentence expresses the main proposition while the believe serves to reduce the speaker's accountability. I present an account of these problematic utterances which captures the accountability aspect of the interpretation and explains both similarities and differences between these cases and other instances of focus. Speaker accountability is modeled by ease of abandonment of a proposition within a belief revision system. I propose that the function of accented believe is to explicitly mark a proposition as more easily abandoned than the contextually determined default. Differences between accented believe and other instances of focus are accounted for by believe being external to the main proposition.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-251"
  },
  "schang94_icslp": {
   "authors": [
    [
     "Daniel",
     "Schang"
    ],
    [
     "Laurent",
     "Romary"
    ]
   ],
   "title": "Frames, a unified model for the representation of reference and space in a man-machine dialogue",
   "original": "i94_0951",
   "page_count": 4,
   "order": 252,
   "p1": "951",
   "pn": "954",
   "abstract": [
    "This paper aims at instanciating the notion of \"frames\" (from now on \"reference context\"1) which seems to emerge from recent work on space modeling but has until now been essentially seen as a too general concept to be usable within an understanding system. This notion, which may be subsumed by Langacker's \"cognitive domains\", still places itself at a more general level than what he introduces as \"search domains\". We will show that a reference context is essential for any general view of spatial reasoning, especially when considering the classical distinction between landmark and trajector which really takes its meaning within a surrounding \"portion\" of space such as the one we will present here.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-252"
  },
  "kawamori94_icslp": {
   "authors": [
    [
     "Masahito",
     "Kawamori"
    ],
    [
     "Akira",
     "Shimazu"
    ],
    [
     "Kiyoshi",
     "Kogure"
    ]
   ],
   "title": "Roles of interjectory utterances in spoken discourse",
   "original": "i94_0955",
   "page_count": 4,
   "order": 253,
   "p1": "955",
   "pn": "958",
   "abstract": [
    "Naturally occurring dialogues have features that are distinctively their own. The realization of a spontaneous dialogue understanding system presupposes a correct characterization and a way to treat successfully these features of natural dialogues. This paper describes an analysis of Japanese interjectory responses in everyday dialogue.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-253"
  },
  "ishikawa94b_icslp": {
   "authors": [
    [
     "Yukiko",
     "Ishikawa"
    ]
   ],
   "title": "Communicative mode dependent contribution from the recipient in information providing dialogue",
   "original": "i94_0959",
   "page_count": 4,
   "order": 254,
   "p1": "959",
   "pn": "962",
   "abstract": [
    "This paper claims that the contribution of the recipient in an information providing dialogue depends on the communicative mode. First, we analyze conversation data and show that the confirmations uttered by the information recipients and the response to them by the information giver differ in three different communicative modes: telephone conversation without map (tel-non-map), telephone conversation with maps (tel-map), and face-to-face conversation with map sharing (face-to-face). The results of an empirical study show that the recipient acknowledges the giver's utterance equally in all modes, but literal confirmation was most frequently used for tel-non-map while inferential confirmation was used for tel-map. The giver's response to the confirmation utterances also depends on the communicative mode. The giver responds more collaboratively in tel-non-map than the other two modes. Second, we discuss the empirical results by using two factors that characterize the three communicative modes; \"the communication channel\" and the \"information available to the participants\". Most of the results can be interpreted consistently by combining these two factors.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-254"
  },
  "cozannet94_icslp": {
   "authors": [
    [
     "Alain",
     "Cozannet"
    ],
    [
     "Jacques",
     "Siroux"
    ]
   ],
   "title": "Strategies for oral dialogue control",
   "original": "i94_0963",
   "page_count": 4,
   "order": 255,
   "p1": "963",
   "pn": "966",
   "abstract": [
    "This paper deals with the evaluation and the efficiency of different dialogue strategies for speech dialogue systems. The main aims of dialogue strategies are: to control the dialogue exchanges and to ensure information reliability, which depends on the performance of continuous speech recognition.\n",
    "A method of representation for dialogue is proposed, allowing to evaluate the dialogue strategy efficiency in terms of speech turns. Four dialogue strategies are described and evaluated when understanding failures occur. This rather theoretical approach is then used to analyse two corpora issued from the SUNDIAL project, where numerous communication failures are observed. The dialogue strategy follows mainly our second model, but as the verification constraint was not strictly followed, results are far from optimum. Our representation approach seems efficient and should provide a good tool for oral dialogue systems designers.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-255"
  },
  "brietzmann94_icslp": {
   "authors": [
    [
     "Astrid",
     "Brietzmann"
    ],
    [
     "Fritz",
     "Class"
    ],
    [
     "Ute",
     "Ehrlich"
    ],
    [
     "Paul",
     "Heisterkamp"
    ],
    [
     "Alfred",
     "Kaltenmeier"
    ],
    [
     "Klaus",
     "Mecklenburg"
    ],
    [
     "Peter",
     "Regel-Brietzmann"
    ]
   ],
   "title": "Robust speech understanding",
   "original": "i94_0967",
   "page_count": 4,
   "order": 256,
   "p1": "967",
   "pn": "970",
   "abstract": [
    "This paper describes a fully operational system for analyzing all aspects of continuous speech from word recognition up to linguistic representation. Many systems rely on fully grammatical speech input, others use only shallow analysis without using declarative linguistic knowledge. We propose a flexible processing, where the depth of the analysis varies according to internal (quality of the speech input) and/or external (limitations, e.g. of processing time) criteria. In architecture and system design, special effort was made to cover effects of spontaneous speech (e.g. unknown words, pauses, or sentence breaks). The described system is part of a speech dialog system not discussed in this paper.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-256"
  },
  "yamashita94_icslp": {
   "authors": [
    [
     "Yoichi",
     "Yamashita"
    ],
    [
     "Keiichi",
     "Tajima"
    ],
    [
     "Yasuo",
     "Nomura"
    ],
    [
     "Riichiro",
     "Mizoguchi"
    ]
   ],
   "title": "Dialog context dependencies of utterances generated from concept reperesentation",
   "original": "i94_0971",
   "page_count": 4,
   "order": 257,
   "p1": "971",
   "pn": "974",
   "abstract": [
    "In our framework of the speech output interface based on concept-to-speech conversion, sentence generation according to dialog context is a crucial issue to be discussed. Understanding of the diversity of surface sentences and relation between them and dialog context enables such an interface system to generate spoken utterances with high quality. This paper describes two experiments in order to investigate dialog context dependencies of utterances in written dialog. In the first experiment, sentences are twice generated from the same concept representation under two situations; sentences are generated isolatedly or generated by referring the preceding utterances in the dialog. A lot of differences are observed between a set of sentence pairs and classified into 22 categories. The second experiment examined preference of various expressions based on the categories obtained from the first experiment and verified substantiality of each category for sentence generation according to dialog context. Mechanisms for dialog processing in the interface system are also discussed at the last of the paper.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-257"
  },
  "nakazato94_icslp": {
   "authors": [
    [
     "Shu",
     "Nakazato"
    ],
    [
     "Katsuhiko",
     "Shirai"
    ]
   ],
   "title": "Effects on utterances caused by knowledge on the hearer",
   "original": "i94_0975",
   "page_count": 4,
   "order": 258,
   "p1": "975",
   "pn": "978",
   "abstract": [
    "This paper describes effects on utterances caused by knowledge about the hearer. Our study focuses on the difference of expressions in the dialogue of two groups. Two groups of six and seven subjects were asked to obtain traffic information and choose a route using our car navigation system. Subjects in the system responding (SR) group were led to believe they were talking to a machine. Subjects in the human responding (HR) group were told they were talking to a human. Both groups of subjects actually talked to the human operators. Our car navigation system displayed a map and proposed three routes, one at a time, to the subject. Operators responded to subjects by selecting sentences in the system which were shown as a synthetic voice. Every subjects took part in three sessions within two weeks. Utterances of the subjects in both groups declined to be more concise expression as the session proceeded. However, subjects in the HR group used more various expressions, with respect to \"intonation\" and \"accent\" than the SR group. We observed more natural expressions as \"filler\" in the HR group. And we found that subjects in HR group tended to make shorter pauses in their utterances and more often to utter ill-formed sentences than in the SR group. These differences in expressions between two groups were seen through the whole sessions.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-258"
  },
  "ferrieux94_icslp": {
   "authors": [
    [
     "A.",
     "Ferrieux"
    ],
    [
     "M. D.",
     "Sadek"
    ]
   ],
   "title": "An efficient data-driven model for cooperative spoken dialogue",
   "original": "i94_0979",
   "page_count": 4,
   "order": 259,
   "p1": "979",
   "pn": "982",
   "abstract": [
    "The task of the automatic dialogue system in many practical applications can be reduced to guiding the user towards a database query yielding a single nonempty answer. Starting from this remark, a generic dialogue strategy can be defined (and implemented in a modular way) which is neither structural nor Al-complete1. The kind of database application we are considering is characterized by a universal relation over a single parameter space, where the parameters represent various features of a common object the user and the system try to agree on, such as a seat on a given flight. The strategy is totally data-driven: no assumption is made about the a priori functional dependencies existing among these parameters. The generic part of the dialogue manager uses the description of the application to help the user specify a minimal set of constraints over the parameters yielding a unique solution. This is done incrementally, asking questions when the set of solutions is too large, and proposing a (possibly multiple) constraint relaxation when it is empty. The resulting dialogue type is a fluent combination of request negociations and corrective and suggestive answers. A complete system has been built atop this generic module, with separate modules for spoken input/output and natural language parsing and generation. It has been tested in a respectably sized flight reservation application (300,000 entries) on which it achieves real time on a workstation.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-259"
  },
  "glass94_icslp": {
   "authors": [
    [
     "James R.",
     "Glass"
    ],
    [
     "Joseph",
     "Polifroni"
    ],
    [
     "Stephanie",
     "Seneff"
    ]
   ],
   "title": "Multilingual language generation across multiple domains",
   "original": "i94_0983",
   "page_count": 4,
   "order": 260,
   "p1": "983",
   "pn": "986",
   "abstract": [
    "In this paper we describe a mechanism developed to generate well-formed sentences across multiple domains from a common semantic representation. This language generation component is embedded in conversational systems that permit users to query databases via spoken requests. The generation mechanism serves two distinct but overlapping roles: paraphrasing user utterances, and responding to user queries. The generation component is completely table-driven, with separate tables controlling the generation for each distinct language. Convenient mechanisms for specifying inflectional endings and for handling movement phenomena have been developed. The system can generate responses/paraphrases for a variety of languages and for several different database-query tasks. We report evaluation results for paraphrasing in an air-travel domain.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-260"
  },
  "mokbel94_icslp": {
   "authors": [
    [
     "C.",
     "Mokbel"
    ],
    [
     "R.",
     "Paches-Leal"
    ],
    [
     "D.",
     "Jouvet"
    ],
    [
     "J.",
     "Monné"
    ]
   ],
   "title": "Compensation of telephone line effects for robust speech recognition",
   "original": "i94_0987",
   "page_count": 4,
   "order": 261,
   "p1": "987",
   "pn": "990",
   "abstract": [
    "The work presented in this paper was devoted to the influence of telephone line distortions on speech recognition over the telephone network. It aimed at improving recognition performance by reducing telephone line effects in speech features to a minimum. Taking some measurements as a basis, we first show that for a given call, the telephone channel may be represented by a linear filter slowly varying with time. We then propose a theoretical framework showing that low frequencies of cepstral trajectories may help identify telephone channel effects. Identifying channel effects by means of low frequencies of cepstral trajectories confirms the interest of conventional techniques, such as high-pass filtering (RASTA) or cepstral subtraction, to increase the robustness of speech recognition over the telephone network. In this work, cepstral subtraction and high-pass filtering of the cepstral trajectories are experimented on a field database (recorded from a voice server in real use). All the experiments show that both techniques significantly improve recognition performance.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-261"
  },
  "takahashi94c_icslp": {
   "authors": [
    [
     "Jun-ichi",
     "Takahashi"
    ],
    [
     "Shigeki",
     "Sagayama"
    ]
   ],
   "title": "Telephone line characteristic adaptation using vector field smoothing technique",
   "original": "i94_0991",
   "page_count": 4,
   "order": 262,
   "p1": "991",
   "pn": "994",
   "abstract": [
    "This paper presents an adaptation method for telephone line characteristic variations in speech recognition across telephone lines. From the viewpoint of real world application, it is important to adapt several variability factors simultaneously, such factors include speakers, line characteristics, and telephone microphones. Our approach is to adapt models of high-quality speech to those of speech influenced by telephone line characteristics using the Vector Field Smoothing (VFS) technique as the adaptation method. The general framework of this technique is the training with a small amount of data. In this paper, the VFS technique is shown to be an effective method for simultaneously adapting speaker, line, and telephone microphone characteristics. Through some experiments in a speaker-independent recognizer using telephone-quality speech data collected through an actual telephone line, it was found that the VFS technique performed extremely well in the simultaneous adaptation of speaker and line characteristics. Furthermore, we introduce preliminary experimental results for telephone line characteristic adaptation using an advanced adaptation method, a combination of Maximum A Posteriori (MAP) estimation and the VFS technique. This method aims to achieve faster adaptation which will be helpful in achieving a comfortable human-machine interface in telephone network applications. Encouraging results were obtained in several experiments.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-262"
  },
  "chang94_icslp": {
   "authors": [
    [
     "Jane",
     "Chang"
    ],
    [
     "Victor W.",
     "Zue"
    ]
   ],
   "title": "A study of speech recognition system robustness to microphone variations: experiments in phonetic classification",
   "original": "i94_0995",
   "page_count": 4,
   "order": 263,
   "p1": "995",
   "pn": "998",
   "abstract": [
    "This paper presents experiments in phonetic classification conducted as part of a study on the effects of microphone variations on performance in speech recognition systems. The TIMIT corpus provides data recorded on a close-talking microphone, on a free field microphone and over telephone lines. The study focuses on the unmatched training and. testing conditions under which degradation is most severe. Analysis of baseline performance characterizes the effects of microphone variations. Downsampling is shown to significantly improve performance for bandlimited conditions at the cost of some degradation for non-bandlimited conditions. Comparative analysis of microphone independent preprocessing techniques, including cepstral mean normalization, RASTA processing, spectral subtraction and codebook dependent cepstral normalization, reveals the effects and tradeoffs of different compensation techniques.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-263"
  },
  "suzuki94b_icslp": {
   "authors": [
    [
     "Tadashi",
     "Suzuki"
    ],
    [
     "Kunio",
     "Nakqjima"
    ],
    [
     "Yoshiharu",
     "Abe"
    ]
   ],
   "title": "Isolated word recognition using models for acoustic phonetic variability by lombard effect",
   "original": "i94_0999",
   "page_count": 4,
   "order": 264,
   "p1": "999",
   "pn": "1002",
   "abstract": [
    "In noisy environment, performance of speech recognition system trained in quiet environment is degraded. We propose a new word recognition method using an acoustic phonetic variability model for Lombard effect that is one of the reasons for this degradation. In this method, difference between a spectral envelope of normal speech and that of Lombard speech is represented by the acoustic phonetic variability model, which are comprised of a non-linear warping function on spectral frequency domain for formant shift and spectral filters for changes of formant bandwidths and spectral tilt. Each model is trained with Lombard speech and provided for a sub-phoneme HMM. In Lombard speech recognition, the HMMs are modified with the acoustic-phonetic variability models, and the duration parameters are modified to compensate the word duration changes by Lombard effect. Recognition experiments without contamination-by-noise were conducted. The Lombard speech data was comprised of isolated 100 words spoken by 5 males hearing 90dB(SPL) pink noise through headphones. The recognition rate was 98.6% with this method, and 88.4% without the method.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-264"
  },
  "hansen94_icslp": {
   "authors": [
    [
     "John H. L.",
     "Hansen"
    ],
    [
     "Brian D.",
     "Womack"
    ],
    [
     "Levent M.",
     "Arslan"
    ]
   ],
   "title": "A source generator based production model for environmental robustness in speech recognition",
   "original": "i94_1003",
   "page_count": 4,
   "order": 265,
   "p1": "1003",
   "pn": "1006",
   "abstract": [
    "It is well known that the introduction of acoustic background distortion and the variability resulting from environmentally induced stress causes speech recognition algorithms to fail. In this paper, several causes for recognition performance degradation are explored. It is suggested that recent studies based on a source generator framework can provide the necessary foundation to establish robust speech recognition techniques. In addition, initial results from two studies are discussed which address both environmental noise and speaker perturbation due to stress for recognition. First, a novel constrained-iterative feature-estimation algorithm is considered which is shown to produce improved speech feature characterization in a wide variety of actual noise conditions (computer fan, large crowd, and voice communication channel noise using CCDATA). Second, a neural network based processing algorithm is formulated using one of several robust input feature sets, which detects and classifies the stressed speaker state. It is shown that a successful stress classification rate of 80.6% is possible when stress conditions are combined into groups of related domains. It is suggested that such knowledge could be used to monitor speaker state and direct feature estimation for improved robustness of speech recognizers. Further discussion of the overall source generator framework which models perturbation from vocal tract excitation to the environment is discussed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-265"
  },
  "matsumoto94_icslp": {
   "authors": [
    [
     "Hiroshi",
     "Matsumoto"
    ],
    [
     "Hiroyuki",
     "Imose"
    ]
   ],
   "title": "A frequency-weighted continuous density HMM for noisy speech recognition",
   "original": "i94_1007",
   "page_count": 4,
   "order": 266,
   "p1": "1007",
   "pn": "1010",
   "abstract": [
    "This paper presents a frequency-weighted Hidden Markov Model (HMM) for noisy speech recognition. In this HMM, the covariance matrices of Gaussian probability density functions are fixed to the inverse of frequency weithing matrices in order to utilize the robustness of quefrency weighted cepstrum and also to incorporate their relative perceptual importance in frequency domain into HMM. Two types of frequency weighting functions and the scaling methods of frequency weighting matrices are examined Using NOISEX-92 data base. As a result of ten digit word recognition tests, the 0.3 to 0.5th power of the smoothed power spectrum derived from each mean vector with a normalization factor are found to give the most robust HMM. A comparative experiments showed that the frequency-weighted HMM attained SNR gains of 12 dB, 6 dB, and 3 dB, 2 dB over a standard diagonal HMM for white, pink, car, and Linx noises. Furthermore, it was found that a duration control is important in the frequency-weighted HMM.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-266"
  },
  "lee94b_icslp": {
   "authors": [
    [
     "Lee-Min",
     "Lee"
    ],
    [
     "Hsiao-Chuan",
     "Wang"
    ]
   ],
   "title": "A study on adaptations of cepstral and delta cepstral coefficients for noisy speech recognition",
   "original": "i94_1011",
   "page_count": 4,
   "order": 267,
   "p1": "1011",
   "pn": "1014",
   "abstract": [
    "In this study, a family of coefficient adaptation methods for speech recognition under white noise environments is proposed. Based on the property of speech cepstral vector shrinking under white noise influence, the noisy speech reference cepstral vector can be approximated by a linear shrunk version of its clean counterpart. This approximation induces an affine transformation on delta cepstral vector to approximate its noisy version. Using these approximations, an adaptive HMM is proposed for noisy speech recognition. Three alternate adaptation schemes will be also investigated. The adaptation parameters can be determined by searching for optimal values such that the adapted reference is the closest to the test one. In addition, a bilinear function of log-signal-ratio is also proposed to determine the linear shrinking factor. The experimental results show that the proposed adaptation methods can compensate the noise effect.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-267"
  },
  "paliwal94_icslp": {
   "authors": [
    [
     "Kuldip K.",
     "Paliwal"
    ],
    [
     "Bishnu S.",
     "Atal"
    ]
   ],
   "title": "A comparative study of feature representations for robust speech recognition in adverse environments",
   "original": "i94_1015",
   "page_count": 4,
   "order": 268,
   "p1": "1015",
   "pn": "1018",
   "abstract": [
    "In this paper, a number of feature representations are studied as to their recognition performance in presence of additive noise and channel mismatch distortions. It is shown that 1) the linear prediction analysis technique provides more robust cepstral features than the homomorphic analysis technique, 2) the filter-bank power spectrum is capable of generating more robust cepstral features than the power spectrum derived through the fast Fourier transform algorithm, and 3) use of human auditory properties in an acoustic front-end makes it more robust.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-268"
  },
  "hamme94_icslp": {
   "authors": [
    [
     "Hugo Van",
     "Hamme"
    ]
   ],
   "title": "ARDOSS: autoregressive domain spectral subtraction for robust speech recognition in additive noise",
   "original": "i94_1019",
   "page_count": 4,
   "order": 269,
   "p1": "1019",
   "pn": "1022",
   "abstract": [
    "The first and second order statistics of the LPC parameters of speech corrupted by additive noise are predicted based on the first few lags of the autocorrelation of the noise. The computed mean allows a correction on the LPC parameters without reference to an assumed state and for any type of HMM emission models. This mean is equivalent to a 5 dB noise suppression. Additional robustness is gained when the predicted covariance in the AR-domain is transposed to the cepstral domain to correct the emission probabilities in a single-Gaussian HMM. These conclusions are drawn from speaker-dependent experiments on the NOISEX-92 database. For a p-th order LPC analysis, correction of the mean is accomplished in O(p2) floating point operations (flops). The full covariance correction requires O(/>3) flops. An O(p2)-approximation that yields comparable performance in practice is given.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-269"
  },
  "takagi94b_icslp": {
   "authors": [
    [
     "Keizaburo",
     "Takagi"
    ],
    [
     "Hiroaki",
     "Hattori"
    ],
    [
     "Takao",
     "Watanabe"
    ]
   ],
   "title": "Speech recognition with rapid environment adaptation by spectrum equalization",
   "original": "i94_1023",
   "page_count": 4,
   "order": 270,
   "p1": "1023",
   "pn": "1026",
   "abstract": [
    "This paper proposes a rapid environment adaptation algorithm based on spectrum equalization (REALISE). In practical speech recognition applications, differences between training and testing environments often seriously diminish recognition accuracy. These environmental differences can be classified into two types of difference: difference in additive noise and in multiplicative noise in the spectral domain. The proposed method calculates time-alignment between a testing utterance and the closest reference pattern to it, and then calculates the noise differences between the two according to the time-alignment. Then, we adapt all reference patterns to the testing environment using the differences. Finally, the testing utterance is recognized using the adapted reference patterns. In a 250 Japanese word recognition task, in which the training and testing microphones were of two different types, REALISE improved recognition accuracy from 87% to 96%.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-270"
  },
  "stern94_icslp": {
   "authors": [
    [
     "Richard M.",
     "Stern"
    ],
    [
     "Fu-Hua",
     "Liu"
    ],
    [
     "Pedro J.",
     "Moreno"
    ],
    [
     "Alejandro",
     "Acero"
    ]
   ],
   "title": "Signal processing for robust speech recognition",
   "original": "i94_1027",
   "page_count": 4,
   "order": 271,
   "p1": "1027",
   "pn": "1030",
   "abstract": [
    "This paper describes several new cepstral-based compensation procedures that render the SPHINX-II system more robust with respect to acoustical environment. The first algorithm, phone-dependent cepstral compensation, is simitar in concept to the previously-described MFCDCN method, except that cepstral compensation vectors are selected according to the current phonetic hypothesis, rather than on the basis of SNR or VQ codeword identity. We also describe two procedures to accomplish adaptation of the VQ codebook for new environments. Use of the various compensation algorithms in consort produces a reduction of error rates for SPHINX-II by as much as 40 percent relative to the rate achieved with cepstral mean normalization alone.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-271"
  },
  "siohan94_icslp": {
   "authors": [
    [
     "Olivier",
     "Siohan"
    ],
    [
     "Yifan",
     "Gong"
    ],
    [
     "Jean-Paul",
     "Haton"
    ]
   ],
   "title": "A comparison of three noisy speech recognition approaches",
   "original": "i94_1031",
   "page_count": 4,
   "order": 272,
   "p1": "1031",
   "pn": "1034",
   "abstract": [
    "We compare 3 recent approaches dealing with speech recognition in noisy environment. The first approach is based on stochastic model combination of noise and speech . Given a clean speech model based on speech trajectories and an HMM noise model, this method aims at deriving a noisy speech model, in order to recognise noisy speech. In the second approach, we perform a mapping between the noisy and the clean speech space. The noisy speech is recognised after mapping to the clean space, using clean speech models. In the last approach, LDA is used as a preprocessing, and the training and testing environmental conditions are identical. On a 206 isolated word recognition task under different noisy environment, LDA gave the best results. The model combination proved to be efficient at high SNR, but performances fell down at low SNR. The mapping approach showed to be very robust, but led to the lowest recognition rate at high SNR.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-272"
  },
  "cairns94_icslp": {
   "authors": [
    [
     "Douglas A.",
     "Cairns"
    ],
    [
     "John H. L.",
     "Hansen"
    ]
   ],
   "title": "Nonlinear speech analysis using the teager energy operator with application to speech classification under stress",
   "original": "i94_1035",
   "page_count": 4,
   "order": 273,
   "p1": "1035",
   "pn": "1038",
   "abstract": [
    "In this study, the problem of reliably classifying speech as normal or speech under stress is examined. It is hypothesized that speech production is composed of linear and nonlinear components, and that the nonlinear component changes measurably between normal and stressed speech. It is proposed that the Teager Energy operator could be utilized to quantify the change between normal and stressed speech. The stress styles considered are speech produced under loud, angry, Lombard effect and clear conditions. Results show that loud and angry speech can be reliably differentiated from neutral speech, while clear speech is difficult to differentiate from neutral speech. The results also show that Lombard effect speech can be reliably classified, but performance varies across speakers.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-273"
  },
  "moakes94_icslp": {
   "authors": [
    [
     "Paul A.",
     "Moakes"
    ],
    [
     "Steve W.",
     "Beet"
    ]
   ],
   "title": "Analysis of non-linear speech generating dynamics",
   "original": "i94_1039",
   "page_count": 4,
   "order": 274,
   "p1": "1039",
   "pn": "1042",
   "abstract": [
    "This paper demonstrates that the non-linear system dynamics generating speech can be embedded in a low dimensional Euclidean space which resembles a manifold. Phoneme manifolds are extracted from the speech time series and compared using a radial basis function network to attempt speaker independent phoneme classification. The initial results are inconclusive but show promising results. Phoneme manifolds are also used for speech prediction resulting in a predictor which is able to achieve noise reduction equal to that of a nonlinear predictor but exhibits an improvement in the signal quality.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-274"
  },
  "tokuda94_icslp": {
   "authors": [
    [
     "Keiichi",
     "Tokuda"
    ],
    [
     "Takao",
     "Kobayashi"
    ],
    [
     "Takashi",
     "Masuko"
    ],
    [
     "Satoshi",
     "Imai"
    ]
   ],
   "title": "Mel-generalized cepstral analysis - a unified approach to speech spectral estimation",
   "original": "i94_1043",
   "page_count": 4,
   "order": 275,
   "p1": "1043",
   "pn": "1046",
   "abstract": [
    "The generalized cepstral analysis method is viewed as a unified approach to the cepstral method and the linear prediction method, in which the model spectrum varies continuously from all-pole to cepstral according to the value of a parameter 7. Since the human ear has high resolution at low frequencies, introducing similar characteristics to the model spectrum, we can represent speech spectrum more efficiently. From this point of view, this paper proposes a spectral estimation method which uses the spectral model represented by mel-generalized cepstral coefficients. The effectiveness of mel-generalized cepstral analysis is demonstrated by an experiment of HMM-based isolated word recognition.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-275"
  },
  "gransden94_icslp": {
   "authors": [
    [
     "L. R.",
     "Gransden"
    ],
    [
     "Steve W.",
     "Beet"
    ]
   ],
   "title": "Combining auditory representations using fuzzy sets",
   "original": "i94_1047",
   "page_count": 4,
   "order": 276,
   "p1": "1047",
   "pn": "1050",
   "abstract": [
    "This paper addresses the problem of combining auditory representations of an acoustic signal based on a measure of the correlation of spectral events between representations in a way which attempts to preserve the detailed spatio-temporal events present in the auditory data. The fuzzy set theory class membership function is used to provide a measure of closeness of an observation vector to a set of centres positioned in the data space to characterise perceptually important spectral events. The technique provides a level of tolerance to the position of spectral events and allows a simple form of noise adaptation. A recognition experiment is described which demonstrates the improvement in noise robustness achieved for an isolated digit database at varying levels of signal to noise ratio (SNR) over a traditional MFCC pre-processor.-\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-276"
  },
  "kajita94_icslp": {
   "authors": [
    [
     "Shoji",
     "Kajita"
    ],
    [
     "Fumitada",
     "Itakura"
    ]
   ],
   "title": "Sbcor spectrum taking autocorrelation coefficients at integral multiples of 1/CF into account",
   "original": "i94_1051",
   "page_count": 4,
   "order": 277,
   "p1": "1051",
   "pn": "1054",
   "abstract": [
    "This paper describes an extension of snbband-autocorrelation (SBCOR) analysis that has been already proposed and shown to be robust against noise. The extended SBCOR analysis is defined by a weighted sum of the autocorrelation coefficients at the integral multiples of 1/CF (center frequency of band pass filter), in order to capture more information about the periodicity included in the speech signal. The experimental results performed by a DTW word recognition indicate that the extended SBCOR analysis is more robust against noise than the conventional SBCOR. The performance of the extended SBCOR is about 8% higher than that of the conventional SBCOR(when Q=1.5), and about 20% higher than that of the smoothed group delay spectrum under SNR OdB. The characteristics of the SB-COR and its extension are also described.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-277"
  },
  "murthy94_icslp": {
   "authors": [
    [
     "Hema A.",
     "Murthy"
    ]
   ],
   "title": "Pitch extraction from root cepstrum",
   "original": "i94_1055",
   "page_count": 4,
   "order": 278,
   "p1": "1055",
   "pn": "1058",
   "abstract": [
    "In this paper we propose a new method of extracting pitch from voiced speech using the root cepstrum. Most algorithms for estimating system and source parameters from a signal (where the signal is mathematically defined as x(n) = e(n)*h(n) (e(n) represents the source and h(n) represents the system) attempt to transform the signal to a domain in which simple filtering operations can be applied to separate the system from the source. The algorithm that is proposed in this paper is one such approach. By applying an appropriate transformation on the spectrum of the signal, the root cepstrum is obtained. If the parameters are appropriately chosen a simple gating function can be used to separate the system frcm the source in the root cepstrum. The algorithm is tested on natural (both male and female) and noisy speech (male).\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-278"
  },
  "hong94_icslp": {
   "authors": [
    [
     "Sunghoon",
     "Hong"
    ],
    [
     "Sangki",
     "Kang"
    ],
    [
     "Souguil",
     "Ann"
    ]
   ],
   "title": "Voice parameter estimation using sequential SVD and wave shaping filter bank",
   "original": "i94_1059",
   "page_count": 4,
   "order": 279,
   "p1": "1059",
   "pn": "1062",
   "abstract": [
    "In this paper, we propose new estimation technique of voice source and vocal-tract parameters in voiced speech. First, Kalman filtering technique which is a new approach to time-varying analysis is examined. Second, in order to overcome the problems of Kalman filtering approach, the error covariance matrix in Kalman filter is decomposed by reduced-rank SVD(singular value decomposition). Third, in order to estimate more accurate vocal-tract parameters we introduce robustness concept which assumes residual characteristics as normal mixture density. And, finally we propose wave-shaping filter bank approach as a new voice source modeling method which can represents and controls various source characteristics in frequency domain. The proposed methods give more accurate and more various time-varying voice parameters. These methods can be used to improve the speech quality in speech synthesis and coding techniques.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-279"
  },
  "schoentgen94b_icslp": {
   "authors": [
    [
     "Jean",
     "Schoentgen"
    ]
   ],
   "title": "Self excited threshold auto-regressive models of the glottal pulse and the speech signal",
   "original": "i94_1063",
   "page_count": 4,
   "order": 280,
   "p1": "1063",
   "pn": "1066",
   "abstract": [
    "We propose a composite signal model whose general form is valid for both the glottal pulse and the speech signal. The model consists of two linear autoregressive sub-models. The two submodels are respectively fitted to the open and return phase components of the glottal pulse or speech signal. In the case of the glottal pulse, the orders of the two sub-models are equal to 2 and 1 respectively. In the case of the speech signal the orders are higher in order to take into account the effects of vocal tract resonance. The switch from one sub-model to the next occurs when the signal crosses a critical threshold. The advantage is that the number and positions of these thresholds are independent of the position and length of the analysis window. As a result, the optimal threshold position, i.e. the best possible segmentation into the open and return phase components, can be found automatically by means of a conventional optimizer. Results show that the proposed model enables the glottal pulse to be segmented automatically and the sub-models to be fitted from within an excitation-asynchronously positioned analysis window. Similarly, when applied to the speech signal, the model automatically provides glottis cycle lengths, the open and return phase components of the speech signal and the open and return phase formant frequencies inside an excitation-asynchronously positioned analysis window.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-280"
  },
  "hess94_icslp": {
   "authors": [
    [
     "Wolfgang J.",
     "Hess"
    ]
   ],
   "title": "Determination of glottal excitation cycles for voice quality analysis",
   "original": "i94_1067",
   "page_count": 4,
   "order": 281,
   "p1": "1067",
   "pn": "1070",
   "abstract": [
    "The three-channel pitch determination algorithm (PDA) presented in this paper combines a short-term analysis PDA, which derives fundamental frequency via a period- icity criterion, and two time-domain PDAs that determine the instants of glottal closure according to local signal criteria. The first of these algorithms correlates the speech signal with an estimate of the impulse response of the vocal tract; the second one applies a neural network. The reliability of these time-domain PDAs is increased by constraints on the range of Fq imposed by the short-term analysis PDA. First results show that the algorithm can be applied for both accurate pitch period determination of running speech and voice quality measurements, particularly the measurement of voice jitter.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-281"
  },
  "cheveigne94_icslp": {
   "authors": [
    [
     "Alain de",
     "Cheveigné"
    ]
   ],
   "title": "Strategies for voice separation based on harmonicity",
   "original": "i94_1071",
   "page_count": 4,
   "order": 282,
   "p1": "1071",
   "pn": "1074",
   "abstract": [
    "A harmonic sound has a spectrum made of discrete components forming a harmonic series, and its wave form is periodic. Voiced speech is approximately harmonic, and this cue appears to be exploited by the auditory system for the perception of speech in a noisy background. This paper addresses the following question: does the auditory system exploit the harmonic structure of a target to segregate it from the background, or else that of a harmonic background to eliminate it.\n",
    "The issue was considered from two angles. First of all, both strategies were implemented to reduce voiced interference in a speech recognition experiment. The aim was to determine whether one \"works better\" than the other in this task; the results showed that harmonic cancellation was more effective. In a second experiment human listeners were presented with pairs of vowels, each of which was either harmonic or inharmonic, and requested to identify them both. The responses were scored according to whether the target vowel was harmonic, and whether the other (interfering) vowel was harmonic. It appears that the auditory system exploits harmonicity of the ground (cancellation strategy), a result that is coherent with the outcome of the first experiment.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-282"
  },
  "mitome94_icslp": {
   "authors": [
    [
     "Yukio",
     "Mitome"
    ]
   ],
   "title": "Speech analysis technique for PSOLA synthesis based on complex cepstrum analysis and residual excitation",
   "original": "i94_1075",
   "page_count": 4,
   "order": 283,
   "p1": "1075",
   "pn": "1078",
   "abstract": [
    "This paper presents a new speech analysis method for waveform segment concatenation synthesis or PSOLA ( Pitch Synchronous Over-Lap and Add) synthesis. In the proposed method, two techniques are employed: a new algorithm for calculating the complex cepstrum and a technique for extracting waveform segments from human speech. Although the complex cepstrum analysis can estimate not only spectral envelope but also phase characteristics, every usual algorithm has its own problem in the application to voiced speech analysis. The proposed algorithm can solve these problems, and the combination of this technique with a residual excitation technique can extract waveform segments as accurate approximation of original human speech. Evaluation tests for the proposed method was carried out, i.e. analysis-synthesis experiment and pitch modification synthesis. In the subjective listening tests for both synthesized speech and pitch modified speech, the listeners could not recognize the distortions. This shows the effectiveness of the proposed method.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-283"
  },
  "kiritani94_icslp": {
   "authors": [
    [
     "Shigeru",
     "Kiritani"
    ],
    [
     "Kikuo",
     "Maekawa"
    ],
    [
     "Hajime",
     "Hirose"
    ]
   ],
   "title": "Intonation pattern with focus and related muscle activities in tokyo dialect",
   "original": "i94_1079",
   "page_count": 4,
   "order": 284,
   "p1": "1079",
   "pn": "1082",
   "abstract": [
    "Electromyographic activities of the cricothyroid muscle (CT) and sternohyoid muscle (SH) in the production of intonation pattern in Tokyo dialect were analyzed with special reference to the effects of focus. Correlation analysis between F0 and CT activity showed that the effect of CT activity on F0 rise is greater for the first pitch peak within an utterance than for the later pitch peaks. This phenomenon may be related at least in part, to the so-called down-step phenomenon in Tokyo dialect. As for the SH, its activity varied with placement of focus. It is speculated that this change may be associated with a change in laryngeal state accompanying an expansion or reduction of pitch range due to focus.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-284"
  },
  "cao94_icslp": {
   "authors": [
    [
     "Jianfen",
     "Cao"
    ]
   ],
   "title": "The effects of contrastive accent and lexical stress upon temporal distribution in a sentence",
   "original": "i94_1083",
   "page_count": 4,
   "order": 285,
   "p1": "1083",
   "pn": "1086",
   "abstract": [
    "This paper describes how the accentual focus in sentence level and lexical stress in word level affect the temporal distribution of a sentence in Standard Chinese. To examine this effect, four clauses that consists of the same syllable string but with defferent assignment of contrastive accent were employed as the test object. The preliminary results show that the effects from accentual focus and lexical stress are different but not completely independent. They are related with and restricted to each other.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-285"
  },
  "cedergren94_icslp": {
   "authors": [
    [
     "Henrietta J.",
     "Cedergren"
    ],
    [
     "Hélène",
     "Perreault"
    ]
   ],
   "title": "Speech rate and syllable timing in spontaneous speech",
   "original": "i94_1087",
   "page_count": 4,
   "order": 286,
   "p1": "1087",
   "pn": "1090",
   "abstract": [
    "This paper bears on the issue of modelling timing in spontaneous speech. It presents the analysis of syllable timing in spontaneous speech corpora using three minutes excerpts of sociolinguistic interviews of two speakers of Montreal French. A speech rate transform of the data based on a measure of local tempo within each intonational phrase is obtained. A regression analysis of syllable timing is presented which compares two dependent variables, the first consists of measured syllable durations in milliseconds, the second consists of rate adjusted syllable durations in milliseconds. Although predicting syllable timing in spontaneous speech does not achieve the level of results achieved in reading paradigms as measured by R2, rate normalization provides improved results. We present and discuss the analyses of both speakers.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-286"
  },
  "lee94c_icslp": {
   "authors": [
    [
     "Hyun-bok",
     "Lee"
    ],
    [
     "Narn-taek",
     "Jin"
    ],
    [
     "Cheol-jae",
     "Seong"
    ],
    [
     "Il-jin",
     "Jung"
    ],
    [
     "Seung-mie",
     "Lee"
    ]
   ],
   "title": "An experimental phonetic study of speech rhythm in standard Korean",
   "original": "i94_1091",
   "page_count": 4,
   "order": 287,
   "p1": "1091",
   "pn": "1094",
   "abstract": [
    "The aim of this paper is to investigate the Korean speech rhythm by examining the pattern of the interstress time increases versus the number of interstress syllables. 16 informants, divided into four groups by age and sex, tape-recorded the carrier sentences consisting of two rhythmic units, each beginning with a stressed syllable. The first unit was gradually expanded by taking one to four weak syllables of CVC type. Two interesting results have emerged from this study' (1) the Korean speech rhythm is inbetween of the two opposite types like Greek; (2) two age groups differed in rhythmic type. The old group displayed the stress-timed rhythm like English whereas the young group the syllable-timed one like Spanish.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-287"
  },
  "umeda94_icslp": {
   "authors": [
    [
     "Noriko",
     "Umeda"
    ],
    [
     "Toby",
     "Wedmore"
    ]
   ],
   "title": "A rhythm theory for spontaneous speech: the role of vowel amplitude in the rhythmic hierarchy",
   "original": "i94_1095",
   "page_count": 4,
   "order": 288,
   "p1": "1095",
   "pn": "1098",
   "abstract": [
    "A conspicuous difference between spontaneous and read speech is that the former has a lively, rhythmical sound, while the latter sounds thin and flat. Music rhythms are created by intensifying certain beats to varying degrees, and grouping the beats together to form global patterns. It was hypothesized that the rhythms of spontaneous speech are created in a similar way. To test this hypothesis, two 1.5 minute monologous sections were chosen from recorded conversations of two male speakers. One is an English speaker with a slow, soothing rhythm while the other is a Japanese speaker with a fast, crisp rhythm. Raw amplitude plots reveal a lower-level structure built on energy clusters and beats. Most characteristics of this structure are shared by the speakers, though some language-specific features are seen. Plots of vowel peak amplitudes reveal a higher-level structure. Both speakers use accented beats to delimit rhythm intervals of approximately 1 second, 2.5 seconds and 4.5 seconds. The accented beats form waves of continuously varying amplitude, which last up to 30 seconds. Further, these waves correspond to different discourse sections and reflect the speaker's emotional state. The exact shape of the waves depends on the nature of the speaker's rhythm.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-288"
  },
  "bruce94_icslp": {
   "authors": [
    [
     "Gösta",
     "Bruce"
    ],
    [
     "Björn",
     "Granström"
    ],
    [
     "Kjell",
     "Gustafson"
    ],
    [
     "David",
     "House"
    ],
    [
     "Paul",
     "Touati"
    ]
   ],
   "title": "Modelling Swedish prosody in a dialogue framework",
   "original": "i94_1099",
   "page_count": 4,
   "order": 289,
   "p1": "1099",
   "pn": "1102",
   "abstract": [
    "The research reported here is conducted within the recently initiated project 'Prosodic Segmentation and Structuring of Dialogue'. The object of study in the project is the prosody of dialogue in a language technology framework. The specific goal of our research is to increase our understanding of how the prosodic aspects of speech are exploited interactively in dialogue - the genuine environment for prosody - and on the basis of this increased knowledge to be able to create a more powerful prosody model. In this paper we present an overview of project design and methods.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-289"
  },
  "fujisaki94b_icslp": {
   "authors": [
    [
     "Hiroya",
     "Fujisaki"
    ],
    [
     "Sumio",
     "Ohno"
    ],
    [
     "Masafumi",
     "Osame"
    ],
    [
     "Mayumi",
     "Sakata"
    ],
    [
     "Keikichi",
     "Hirose"
    ]
   ],
   "title": "Prosodic characteristics of a spoken dialogue for information query",
   "original": "i94_1103",
   "page_count": 4,
   "order": 290,
   "p1": "1103",
   "pn": "1106",
   "abstract": [
    "Prosodic characteristics of a simulated dialogue have been analyzed and compared with those of read speech. The simulated dialogue was produced by referring to a written text, while read speech was produced by reading individual sentences of the same text in isolation. Analysis of fundamental frequencies and speech rates indicated that dialogue-style utterances have larger mean value and standard deviation of fundamental frequencies as well as higher mean value of speech rates than reading-style utterances. Further analysis of the F0 contour parameters extracted by using a quantitative model has revealed that the differences in the F0 characteristics are caused by a higher baseline value of the F0 contours as well as an expanded range of variation in the amplitude of accent commands in the dialogue-style utterances.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-290"
  },
  "takeda94b_icslp": {
   "authors": [
    [
     "Shoichi",
     "Takeda"
    ],
    [
     "Yoshiyuki",
     "Itoh"
    ],
    [
     "Norifumi",
     "Sakuma"
    ],
    [
     "Kei",
     "Yokosato"
    ]
   ],
   "title": "Analysis of prosodic and linguistic features of spontaneous Japanese conversational speech",
   "original": "i94_1107",
   "page_count": 4,
   "order": 291,
   "p1": "1107",
   "pn": "1110",
   "abstract": [
    "Prosodic and linguistic features of spontaneous Japanese conversational speech are analyzed. Prosodic features, of prominence in particular, are first analyzed for a corpus consisting of 50 sentences in discussion on coeducation spoken by a male announcer as well as male and female high-school students. The analysis results show that speech-rate reduction in parts where prominence is placed is a conspicuous feature in spontaneous conversational speech spoken by the male high-school students. The mean mora duration for marked words is 130ms, whereas the mean mora duration for neutral words is 89ms. As for linguistic features, several habits of saying inherent in spontaneous speech are observed common to a plural number of speakers, and a speaker tends to speak to a specific person even in discussion among more than two people.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-291"
  },
  "campbell94_icslp": {
   "authors": [
    [
     "Nick",
     "Campbell"
    ]
   ],
   "title": "Combining the use of duration and F0 in an automatic analysis of dialogue prosody",
   "original": "i94_1111",
   "page_count": 4,
   "order": 292,
   "p1": "1111",
   "pn": "1114",
   "abstract": [
    "This paper evaluates the performance of two automatic labelling systems for intonation by using their output to predict the position and type of prominences and boundaries labelled in a ToBI transcription of twelve dialogues. It shows that they both model the prosodic information well, and that improvements in modelling are gained when including segmental information about the duration and energy profiles of the utterance. However, after parameter reduction, the features that survive are segmental rather than F0-related.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-292"
  },
  "bakenecker94_icslp": {
   "authors": [
    [
     "G.",
     "Bakenecker"
    ],
    [
     "U.",
     "Block"
    ],
    [
     "Anton",
     "Batliner"
    ],
    [
     "Ralf",
     "Kompe"
    ],
    [
     "Elmar",
     "Nöth"
    ],
    [
     "Peter",
     "Regel-Brietzmann"
    ]
   ],
   "title": "Improving parsing by incorporating 'prosodic clause boundaries into a grammar",
   "original": "i94_1115",
   "page_count": 4,
   "order": 293,
   "p1": "1115",
   "pn": "1118",
   "abstract": [
    "In written language, punctuation is used to separate main and subordinate clause. In spoken language, ambiguities arise due to missing punctuation, but clause boundaries are often marked prosodically and can be used instead. We detect PCBs (Prosodically marked Clause Boundaries) by using prosodic features (duration, intonation, energy, and pause information) with a neural network, achieving a recognition rate of 82%. PCBs are integrated into our grammar using a special syntactic category 'break' that can be used in the phrase-structure rules of the grammar in a similar way as punctuation is used in grammars for written language. Whereas punctuation in most cases is obligatory, PCBs are sometimes optional. Moreover, they can in principle occur everywhere in the sentence due e.g. to hesitations or misrecognition. To cope with these problems we tested two different approaches: A slightly modified parser for word chains containing PCBs and a word graph parser that takes the probabilities of PCBs into account. Tests were conducted on a subset of infinitive subordinate clauses from a large speech database containing sentences from the domain of train table inquiries. The average number of syntactic derivations could be reduced by about 70 % even when working on recognized word graphs.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-293"
  },
  "hunt94_icslp": {
   "authors": [
    [
     "Andrew",
     "Hunt"
    ]
   ],
   "title": "A prosodic recognition module based on linear discriminant analysis",
   "original": "i94_1119",
   "page_count": 4,
   "order": 294,
   "p1": "1119",
   "pn": "1122",
   "abstract": [
    "Prosodic features in continuous speech are a potential source of useful information for speech recognition and understanding. This paper presents a technique for using knowledge of the relationship between acoustic prosodic features and syntactic structure to resolve syntactic ambiguity. The technique uses Linear Discriminant Analysis (LDA) to determine the linear combination of prosodic features which provides maximum separation of syntactic contexts and then applies a correction factor to compensate for topological variation. The LDA model determines the probability of conformance of prosody and syntax for rescoring candidate sentence hypotheses, and achieves 74% accuracy in resolving syntactic ambiguity on a standard corpus. The model is also of phonetic interest because it reveals a direct relationship between acoustic prosodic features and syntactic structure.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-294"
  },
  "hirose94_icslp": {
   "authors": [
    [
     "Keikichi",
     "Hirose"
    ],
    [
     "Atsuhiro",
     "Sakurai"
    ],
    [
     "Hiroyuki",
     "Konno"
    ]
   ],
   "title": "Use of prosodic features in the recognition of continuous speech",
   "original": "i94_1123",
   "page_count": 4,
   "order": 295,
   "p1": "1123",
   "pn": "1126",
   "abstract": [
    "Two methods were proposed for the use of prosodic features in automatic speech recognition. One is to detect syntactic boundaries of input speech without information on the segmental level, which will be obtained by the ordinary speech recognition process. The other is to check the feasibility of recognition results. In the first method, both the microscopic and macroscopic features of fundamental frequency contours were taken into account, and 96 % of manually detectable boundaries were correctly extracted for the ATR continuous speech database. Several schemes were also proposed to reduce the insertion errors. As for the second method, a scheme of partial analysis-by-synthesis was developed, where fundamental frequency contours are generated using a functional model for the recognition hypotheses of the segmental level and are compared with the observed contour only for the part with recognition ambiguity. The hypothesis giving the best matching with the observation is the possible final recognition result. The proposed method was shown to be valid for recognition errors that include changes in the accent types and in the syntactic boundaries.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-295"
  },
  "wydell94_icslp": {
   "authors": [
    [
     "Taeko Nakayama",
     "Wydell"
    ],
    [
     "Brian",
     "Butterworth"
    ]
   ],
   "title": "The inconsistency of consistency effects in reading: the case of Japanese kanji phonology",
   "original": "i94_1127",
   "page_count": 4,
   "order": 296,
   "p1": "1127",
   "pn": "1130",
   "abstract": [
    "Most Japanese Kanji characters have several different pronunciations, at least one ON-reading (of Chinese origin) and a KUN-reading (of Japanese origin); the appropriate pronunciation is determined by intraword context. There are also Kanji characters which have a single ON-reading and no KUN-reading. With 2-character ON-reading Kanji words as stimuli, naming experiments were carried out to investigate print-to-sound consistency effects, as seen in studies of English. The consistent Kanji words were those where neither constituent character has an alternative ON-reading or a KUN-reading, hence there can be no pronunciation ambiguity for these words. The inconsistent items were ON-reading words composed of characters which have KUN-readings that are appropriate to other words in which the characters occur, hence there should be some ambiguity about the pronunciation of the constituent characters. Experiments reported here yielded reliable effects of both word and character frequency/familiarity on speed and accuracy of word naming, but virtually no evidence for consistency effects. It is concluded that for Kanji, phonology is computed dominantly at the word rather than the character level.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-296"
  },
  "ciocca94_icslp": {
   "authors": [
    [
     "Valter",
     "Ciocca"
    ],
    [
     "Livia",
     "Wong"
    ],
    [
     "Lydia K. H.",
     "So"
    ]
   ],
   "title": "An acoustic analysis of unreleased stop consonants in word-final position",
   "original": "i94_1131",
   "page_count": 4,
   "order": 297,
   "p1": "1131",
   "pn": "1134",
   "abstract": [
    "This study analysized the acoustic properties of the vocalic portions of CVC syllables (/sa__/) ending with the unreleased stops /p/, A/, or Ikf. The syllables were Cantonese words spoken in the High Level tone by four female and four male native speakers of Cantonese. The acoustic parameters investigated were the duration of the vowel segment, the fundamental frequency (F0), and the frequencies of the first three formants (Fl, F2,F3). Both the F0 and the formant frequencies were estimated from wide-band FFT spectra at the beginning, the middle and the end of the vowel segment. The results of the analysis showed a small but reliable difference in vowel segment duration among the final stops. The formant frequency pattern at the end of the vowel segment indicated that information about place of articulation is available in the frequency regions of Fl, F2 and F3. Moreover, the F2 and F3 frequencies in the middle of the vowel segment also separated the three stop consonants. The fundamental frequency measurement did not show any differential F0 pattern among the final stops.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-297"
  },
  "vroomen94_icslp": {
   "authors": [
    [
     "Jean",
     "Vroomen"
    ],
    [
     "Beatrice de",
     "Gelder"
    ]
   ],
   "title": "Speech segmentation in dutch: no role for the syllable",
   "original": "i94_1135",
   "page_count": 4,
   "order": 298,
   "p1": "1135",
   "pn": "1138",
   "abstract": [
    "Three monitoring experiments are reported investigating the role of the syllable in spoken Dutch. Subjects detected prespecified targets (e.g., daa or daal) that did or did not correspond to the initial syllable of a word {daa. ling and daal. def) or pseudoword (daa.les and daal.sel). In addition, carrier words varied in whether they had clear or ambisyllabic structure (e.g., kofrrjel versus kor.ting). No evidence for a syllabic segmentation routine was found. These findings are consistent with the absence of a syllable effect in English which is, like Dutch, another stress-based language.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-298"
  },
  "mcqueen94_icslp": {
   "authors": [
    [
     "James M.",
     "McQueen"
    ]
   ],
   "title": "Do ambiguous fricatives rhyme? lexical involvement in phonetic decision-making depends on task demands",
   "original": "i94_1139",
   "page_count": 4,
   "order": 299,
   "p1": "1139",
   "pn": "1142",
   "abstract": [
    "Lexical involvement in tasks which require phonetic decisions depends upon attentional factors. Attention can be manipulated by varying task demands. Tasks such as phonetic categorization may focus attention on acoustic-phonetic information but tasks such as rhyme decision may encourage use of the lexicon. An experiment is presented in which listeners made rhyme decisions about words and nonwords ending in ambiguous fricatives. Lexical effects were found, even though the materials had previously been used in a categorization experiment where no reliable effects were observed. The results suggest that the demands of the rhyme task do encourage lexical involvement. Attentional shifts due to task demands are more readily accounted for by autonomous than by interactive models of spoken word recognition.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-299"
  },
  "halle94_icslp": {
   "authors": [
    [
     "P. A.",
     "Halle"
    ],
    [
     "Juan",
     "Segui"
    ]
   ],
   "title": "Moraic segmentation in Japanese revisited",
   "original": "i94_1143",
   "page_count": 4,
   "order": 300,
   "p1": "1143",
   "pn": "1146",
   "abstract": [
    "Japanese naturally segment speech into moras. The possibility that the syllable also plays a role was investigated here with a syllable-monitoring task. The material was designed so as to discourage Japanese listeners from relying on orthographic representations. In experiment 1, Japanese subjects had to detect targets such as /ta/ vs. /tas/ in e.g. /ta-sa-ku/ vs. /tas-su-ru/. They could detect targets in all types of items with reasonably low error rates. CV targets were always detected faster than CVs targets such as /tas/. But CVs targets were detected faster in test items whose first syllable exactly matched the target. These results were replicated in experiment 2, where the targets used were CV vs. CVm. In experiment 3, French subjects were run with the Japanese material. They probably processed the material at an acoustic or phonetic level. Like Japanese, they also were slower to detect CVC than CV syllables but showed no 'syllabic effect1. We propose that, in addition to the mora, some closed CVC syllables, which do occur in Japanese, also have a psychological reality for Japanese listeners.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-300"
  },
  "venditti94_icslp": {
   "authors": [
    [
     "Jennifer J.",
     "Venditti"
    ],
    [
     "Hiroko",
     "Yamashita"
    ]
   ],
   "title": "Prosodic information and processing of temporarily ambiguous constructions in Japanese",
   "original": "i94_1147",
   "page_count": 4,
   "order": 301,
   "p1": "1147",
   "pn": "1150",
   "abstract": [
    "This study examines the contribution of acoustic information to distinguishing 'temporarily ambiguous structures in Japanese. In the written language, since there is no morphological marking on relative clauses, there is a consistent ambiguity (until the head noun is presented) as to whether a clause is a simplex sentence or a subordinate clause of a complex NP. Experiment 1 shows that, in the spoken language, there are differences in fundamental frequency, duration and amplitude in native speakers productions which distinguish these two constructions. Furthermore, experiments 2 and 3 show that this information is indeed used by listeners in identifying the structure of the utterance prior to the point of disambiguation (i.e., presentation of the head noun), and even prior to the completion of the verb in the relative clause.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-301"
  },
  "minematsu94b_icslp": {
   "authors": [
    [
     "Nobuaki",
     "Minematsu"
    ],
    [
     "Keikichi",
     "Hirose"
    ]
   ],
   "title": "Role of prosodic features in the human process of speech perception",
   "original": "i94_1151",
   "page_count": 4,
   "order": 302,
   "p1": "1151",
   "pn": "1154",
   "abstract": [
    "Prosodic features of speech are tightly related to the linguistic information of an utterance, such as the lexical meaning of a word and the syntactic structure of a sentence. Although we have conducted a series of perceptual experiments and constructed a model for the human process of speech recognition, only the segmental features have been taken into account. In order to quantitatively examine the role of prosodic features, two perceptual experiments were performed, where speech stimuli were synthesized by manipulating accent and phrase components of the fundamental frequency contour. Word stimuli were used in the first experiment to clarify the effects of word accent types on word perception. It was found that the words with type 1 accent were perceived differently from those with the other types. In the other experiment, sentence stimuli were used to examine the effects of phrase components on the perception of higher-order linguistic information. The results indicated that the phrase component, even if its command value was small, could work as a cue for detecting the syntactic structure in a sentence.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-302"
  },
  "hashimoto94_icslp": {
   "authors": [
    [
     "Masahiro",
     "Hashimoto"
    ],
    [
     "Hideaki",
     "Seki"
    ]
   ],
   "title": "Limitations of lip-reading advantage by desynchronizing visual and auditory information in speech",
   "original": "i94_1155",
   "page_count": 4,
   "order": 303,
   "p1": "1155",
   "pn": "1158",
   "abstract": [
    "Earlier research have demonstrated that visual information provided by the movement of a talker's lip helps in the perception of speech. The purpose of this paper was to explore the limits of the lip-reading effect by desynchronizing visual and auditory information in speech. In the experiment reported here, audio-visual identification of Japanese sentences was examined as a function of desynchronizations: audio-visually with either 0, 120, 240, 480 ms of audio delay or precedence, and audio-alone, . The results of this study indicated that all the increased desynchronizations in the range from 120-480 ms with one exception produced significant decrements in intelligibility when compared individually with the audio-visual insynchrony condition. On the other hand, when audio was delayed or preceded by 120 ms, subjects were better able to identify the sentences than when auditory information alone was provided. This indicated that subjects benefited from the visual information across such asynchronies. Furthermore, such asynchronous time coincided with the mean mora duration of 123 ms measured for sentences from test lists. Thus it appears that subjects might be attempting to integrate the visual and auditory information in speech at the level of individual morae. These results of experiment may serve to consider the linguistic units of auditory-visual integration during speech perception.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-303"
  },
  "franklin94b_icslp": {
   "authors": [
    [
     "Sue",
     "Franklin"
    ],
    [
     "Judy",
     "Turner"
    ],
    [
     "Julie",
     "Morris"
    ]
   ],
   "title": "Word meaning deafness: effects of word type",
   "original": "i94_1159",
   "page_count": 4,
   "order": 304,
   "p1": "1159",
   "pn": "1162",
   "abstract": [
    "We report the case of a patient, Dr O, who has impaired auditory comprehension, but intact written comprehension. His ability to repeat words that he cannot comprehend, along with good performance on lexical decision suggest that Dr O has an impairment of the mappings between lexical and semantic representations; a word meaning deafness. Dr O's ability to understand heard words depends on their imageability and their length, such that he is poor at comprehending words which are short and abstract. This is interpreted in terms of lexical and semantic distinctiveness.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-304"
  },
  "masukata94_icslp": {
   "authors": [
    [
     "Mikio",
     "Masukata"
    ],
    [
     "Seiichi",
     "Nakagawa"
    ]
   ],
   "title": "Concept and grammar acquisition based on combining with visual and auditory information",
   "original": "i94_1163",
   "page_count": 4,
   "order": 305,
   "p1": "1163",
   "pn": "1166",
   "abstract": [
    "We propose a system which acquires concepts and grammar from visual and acoustic information without a priori knowledge by comparing input information with acquired concepts. For the task domain, we choose the concept acquisition of simple figures. A concept consists of a set of relations between a figure feature and a speech event. And the system acquires the grammar for ordering the concepts. After acquiring the concepts and grammar, the system generates an utterance which explains input images by using acquired concepts and grammar. Furthermore, the system also generates image concepts corresponding to input speech by using acquired concepts. Our system acquired 11 out of 12 types of concepts from 100 pairs of utterance and images. Using the left-to-right HMM for grammar acquisition, the rate our system will generate correct sentences for input images is about 50%, We have realized the first stage of human's concept acquisition process on a computer system.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-305"
  },
  "dempster94_icslp": {
   "authors": [
    [
     "Gavin J.",
     "Dempster"
    ],
    [
     "Sheila M.",
     "Williams"
    ],
    [
     "Sandra P.",
     "Whiteside"
    ]
   ],
   "title": "The punch and judy man: a study of phonological / phonetic variation",
   "original": "i94_1167",
   "page_count": 4,
   "order": 306,
   "p1": "1167",
   "pn": "1170",
   "abstract": [
    "This paper describes an approach to the analysis of speaker variation through a contrastive study of the voices of characters in a puppet show, produced by a single speaker. A recording of an impromptu performance of an unscripted Punch and Judy show is analysed to compare variability within and between the three principle characters. Here we investigate some of the phonological and acoustic-phonetic factors that lead to perceptual discrimination between the character voices. The effects of context, including phonological boundaries, are considered in investigating variations in pronunciation of common words and phrases found within in each character. Acoustic correlates of some of these variants are also discussed. The distribution of phonemic realizations of word-final underlying N segments in a set of common words is found to vary substantially between two of the characters. Differences in the acoustic realizations of the same vowel from each of the two characters supports earlier evidence that voice quality is also a distinguishing factor. In addition, some evidence is presented that the need to emphasise such distinctions may influence lexical choice.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-306"
  },
  "traunmuller94_icslp": {
   "authors": [
    [
     "Hartmut",
     "Traunmüller"
    ],
    [
     "Renée van",
     "Bezooijen"
    ]
   ],
   "title": "The auditory perception of children's age and sex",
   "original": "i94_1171",
   "page_count": 4,
   "order": 307,
   "p1": "1171",
   "pn": "1174",
   "abstract": [
    "Data on the accuracy with which listeners rate the age of children and some knowledge about the relative contributions of underlying factors (Fo, formant frequencies, 'verbal maturation') have been obtained in perceptual experiments using excerpts from interviews with children in the age groups 5, 7, 9, and 11 years. Utterances containing verbal cues to age were avoided by presenting written versions to a panel of judges. The utterances were LPC-analyzed and resynthesized with modifications, to obtain whispered speech, speech of 9 year olds with F0, formants, and speech rate modified as at an age of 5, 7, 9, and 11 years, and whispered versions of these. The listeners agreed in their age ratings with an SD around 1.5 years. The differences between the four types of speech were small. The analysis showed the perceptual weight of the 'verbal maturation' factor to increase with experience, and that of F0to decrease. Data on sex recognition are presented without factor analysis.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-307"
  },
  "magnuson94_icslp": {
   "authors": [
    [
     "James S.",
     "Magnuson"
    ],
    [
     "Reiko A.",
     "Yamada"
    ],
    [
     "Howard C.",
     "Nusbaum"
    ]
   ],
   "title": "Are representations used for talker identification available for talker normalization?",
   "original": "i94_1175",
   "page_count": 4,
   "order": 308,
   "p1": "1175",
   "pn": "1178",
   "abstract": [
    "Contextual tuning theories of talker normalization state that listeners can use information about a talker's vocal characteristics stored in working memory to recognize that talker's speech [8], We investigated whether people can use information about a familiar talker's voice, stored in long-term memory [10], in the same way. That is, whether people can circumvent talker normalization processes when listening to familiar talkers by referencing the representations they use for talker identification. We presented subjects with stimuli produced by familiar and unfamiliar talkers in a monitoring paradigm that typically results in faster performance in a single-talker condition than a multiple-talker condition. We found the typical normalization effect for both familiar and unfamiliar talkers, suggesting that even if talker representations used for identification are compatible with those used for normalization, they cannot be retrieved more quickly than the representations used for normalization can be computed. We verified subjects ability to identify familiar talkers in a second experiment, and found that familiarity facilitated both accuracy and response time in the identification task. We discuss the implications of the results for theories of talker normalization and talker identification.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-308"
  },
  "hasegawa94_icslp": {
   "authors": [
    [
     "Yoko",
     "Hasegawa"
    ],
    [
     "Kazue",
     "Hata"
    ]
   ],
   "title": "Non-physiological differences between male and female speech: evidence from the delayed F0 fall phenomenon in Japanese",
   "original": "i94_1179",
   "page_count": 4,
   "order": 309,
   "p1": "1179",
   "pn": "1182",
   "abstract": [
    "There is a perceptual difference between male and female speech in fundamental frequency and, less significantly, in formant frequencies. These characteristics are physiologically determined to a great extent, and thus speakers exert little control over these characteristics in their normal utterances. The present study investigates whether some characteristics of fundamental frequency are also manipulated by speakers to make speech quality more feminine. The results of our experiment show that the delay of the F0 fall that signals a lexical accent is associated with femininity in Japanese.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-309"
  },
  "kitamura94_icslp": {
   "authors": [
    [
     "Tatsuya",
     "Kitamura"
    ],
    [
     "Masato",
     "Akagi"
    ]
   ],
   "title": "Speaker individualities in speech spectral envelopes",
   "original": "i94_1183",
   "page_count": 4,
   "order": 310,
   "p1": "1183",
   "pn": "1186",
   "abstract": [
    "Physical characteristics representing speaker individualities embedded in the spectral envelopes of vowels are investigated through four psychoacoustic experiments. The LMA analysis-synthesis system is used to prepare stimuli varying specific frequency bands in the spectral envelopes and the frequency bands having speaker individualities are estimated. The experimental results suggest that speaker individualities mainly exist at above the 23.5 ERB rate (2340 Hz) in the spectral envelopes and that they can be controlled without influencing vowel identification. More detailed information on the spectral envelopes is required for speaker identification than for vowel identification.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-310"
  },
  "markham94_icslp": {
   "authors": [
    [
     "Duncan",
     "Markham"
    ]
   ],
   "title": "Prosodic imitation: productional results",
   "original": "i94_1187",
   "page_count": 4,
   "order": 311,
   "p1": "1187",
   "pn": "1190",
   "abstract": [
    "Some of the results of an investigation of the imitation of auditory speech stimuli, and the identification of spontaneous and imitative speech are presented. In the investigation, listeners were found to have difficulty identifying non-spontaneous stimuli, and judgements of imitative quality were not found to correlate with naive spontaneous~non-spontaneous judgements. This paper presents the results of a further set of quality-judgements and discusses them in relationship to the productional characteristics of the imitations and previous parts of investigation.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-311"
  },
  "gibbon94_icslp": {
   "authors": [
    [
     "Fiona",
     "Gibbon"
    ],
    [
     "William J.",
     "Hardcastle"
    ]
   ],
   "title": "Articulatory description of affricate production in speech disordered children using electropalatography (EPG)",
   "original": "i94_1191",
   "page_count": 4,
   "order": 312,
   "p1": "1191",
   "pn": "1194",
   "abstract": [
    "This study used EPG to investigate some articulatory characteristics of affricates in ten children with functional articulation disorders. All the children had distorted productions of sibilant target ///. One assumption was that there would be perceptual and EPG similarities between the children's productions of /[/ and the fricative component of /tf/. The evidence from the majority of the children supported this view. Whilst the children showed considerable variability in their tongue-palate contact patterns for distorted productions, a general finding was that articulatory placement for the stop component of the affricate was closer to /?/ than to /t/. This was interpreted as suggesting that children may regard affricates as essentially fricatives with abrupt onsets. There was also a trend for increased tongue-palate contact for the stop component of /?/ compared to /?/.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-312"
  },
  "ujihira94_icslp": {
   "authors": [
    [
     "Akira",
     "Ujihira"
    ],
    [
     "Haruo",
     "Kubozono"
    ]
   ],
   "title": "A phonetic and phonological analysis of stuttering in Japanese",
   "original": "i94_1195",
   "page_count": 4,
   "order": 313,
   "p1": "1195",
   "pn": "1198",
   "abstract": [
    "This paper attempts to illuminate the phonetic and phonological patterns underlying stuttering in Japanese. To do this, the first part of the paper reports the results of a detailed analysis of over 700 stuttering samples produced by adult stutterers. This analysis has revealed two salient characteristics of adult stutterers' speech: (i) words beginning with a vowel trigger stuttering at a considerably higher rate than might be expected by chance; (ii) stuttering breaks long syllables (CVC or CW) into CV and the rest (C or V) rather than C and VC (or W). In the second half of the paper, these results will be compared with the previous analyses of speech errors in Japanese and stuttering in other languages, with a view to uncovering language-specific properties of speech production as against language-universal properties.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-313"
  },
  "jamieson94_icslp": {
   "authors": [
    [
     "Donald G.",
     "Jamieson"
    ],
    [
     "Susan",
     "Rvachew"
    ]
   ],
   "title": "Perception, production and training of new consonant contrasts in children with articulation disorders",
   "original": "i94_1199",
   "page_count": 4,
   "order": 314,
   "p1": "1199",
   "pn": "1202",
   "abstract": [
    "A substantial proportion of children who have normal oral and motor function, nevertheless demonstrate difficulty producing one or more sounds in their native language. Assessment and treatment of such children has traditionally focussed on speech production to the exclusion of speech perception. The possibility that speech production difficulties might be associated with atypical speech perception abilities has been discounted in traditional therapeutic approaches. In contradiction, we find that a subset of children who have a functional articulation disorder have a concurrent and correlated perceptual disorder and that in some children, a formal computer-based training technique produces striking improvements in speech perception abilities, and improvements in the quality of produced speech as well. A new computer-based system for IBM-PC compatible computers has been developed to implement this training strategy.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-314"
  },
  "nakakoshi94_icslp": {
   "authors": [
    [
     "Sachiko",
     "Nakakoshi"
    ],
    [
     "Atsushi",
     "Mizobuchi"
    ],
    [
     "Hiroto",
     "Katori"
    ]
   ],
   "title": "Cognitive processes of speech sounds in a brain-damaged patient",
   "original": "i94_1203",
   "page_count": 4,
   "order": 315,
   "p1": "1203",
   "pn": "1206",
   "abstract": [
    "The purpose of this study was to examine experimentally cognitive processes of serial speech sounds through a case having disturbances of speech sound perception after putamen hemorrhage. The patient was a right-handed and well-educated man in his forties. His scores on intelligence and language ability tests were very good, except for auditory comprehension tasks. His threshold of pure tones and perception of non-verbal auditory stimuli were normal. While his perception of single speech sounds was relatively good, perception of three to four syllable non-sense words was very poor. Specifically, correct responses steeply declined as the syllable position in the word presented moved toward the end. On the other hand, control subjects with peripheral hearing disorder did not show such tendency. It could be suggested, therefore, that the patient's type of deficits might underlie lowered speed of processing of speech sounds. To examine this hypothesis, syllable matching tasks with the different interval between syllables were performed. We found that the patient's scores at rear syllable positions improved only in the long presentation interval conditions. The results suggested that the main deficits in the patient might be a decreasing rate of processing of speech sounds.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-315"
  },
  "suzuki94c_icslp": {
   "authors": [
    [
     "N.",
     "Suzuki"
    ],
    [
     "H.",
     "Dent"
    ],
    [
     "M.",
     "Wakumoto"
    ],
    [
     "Fiona",
     "Gibbon"
    ],
    [
     "K.",
     "Michi"
    ],
    [
     "William J.",
     "Hardcastle"
    ]
   ],
   "title": "A cross-linguistic study of lateral /s/ using electropalatography (EPG)",
   "original": "i94_1207",
   "page_count": 4,
   "order": 316,
   "p1": "1207",
   "pn": "1210",
   "abstract": [
    "This paper reports on a collaborative cross- linguistic study which compares lateralised productions of /s/ in Japanese and British English. EPG recordings were made of four Japanese and three British patients. The EPG patterns were classified according to certain criteria , such as the presence/absence of complete constriction, and the area and location of the contact. It was found that, although lateralised productions varied between the speakers, Japanese and English productions were broadly similar. Acoustically, productions in both languages were characterised by a lower frequency of peak energy than would be expected in a normal speaker's productions. The implications of this work for the assessment of this type of speech disorder are discussed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-316"
  },
  "matsubara94_icslp": {
   "authors": [
    [
     "Junko",
     "Matsubara"
    ],
    [
     "Toshihiro",
     "Kashiwagi"
    ],
    [
     "Morio",
     "Kohno"
    ],
    [
     "Hirotaka",
     "Tanabe"
    ],
    [
     "Asako",
     "Kashiwagi"
    ]
   ],
   "title": "Prosody of recurrent utterances in aphasic patients",
   "original": "i94_1211",
   "page_count": 4,
   "order": 317,
   "p1": "1211",
   "pn": "1214",
   "abstract": [
    "Prosody of recurrent utterances (RU) in four aphasic patients was investigated focusing on a prosodic factor of fundamental frequency (F0). In addition, we assessed numbers and lengths of phrases within an utterance to determine the general aspects of RUs. The results demonstrated that (1) all cases had some variability in the numbers and lengths of phrases, (2) of the four subjects, three had only a limited number of F0 contour types, but (3) the other (Case 4) showed remarkably rich variability in the types of F0 contours. A hypothesis is proposed that F0 control in speaking is independent of other mechanisms involved in producing RUs.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-317"
  },
  "locastro94_icslp": {
   "authors": [
    [
     "Virginia",
     "LoCastro"
    ]
   ],
   "title": "Intonation and language teaching",
   "original": "i94_1215",
   "page_count": 3,
   "order": 318,
   "p1": "1215",
   "pn": "1218",
   "abstract": [
    "This paper argues that more attention be given to the teaching of prosodic features, in particular to intonation and stress patterns beyond the sentence level. There are two reasons. First, prosodic features are related to the presentation of information in spoken English language texts. Second, the ethnic stereotyping of Japanese speakers as \"emotionless\" and lifeless\" may result from their L2 intonation and stress patterns. An analysis of materials used in language classrooms in Japan indicates that the attention in pronunciation is focused on the teaching of segmentals. It will be argued that a syllabus of awareness-raising listening tasks preceding a production phase is necessary for the teaching of prosodic features.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-318"
  },
  "nara94_icslp": {
   "authors": [
    [
     "Tsuyoshi",
     "Nara"
    ],
    [
     "P.",
     "Bhaskararao"
    ]
   ],
   "title": "A computer-aided phonetic instruction system for south-asian languages",
   "original": "i94_1219",
   "page_count": 4,
   "order": 319,
   "p1": "1219",
   "pn": "1222",
   "abstract": [
    "A set of HyperCard stacks for Macintosh that explains and illustrates the sound system of South Asian languages is described. Some of the specific phonetic features of these languages are presented. This is followed by an illustration of the system. The system is built using HyperCard 2.2 using all the standard features like buttons, fields, sound resources etc. The system is expected to be useful in understanding the sound pattern of these languages as well as in teaching of pronunciation of South Asian languages.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-319"
  },
  "kohno94_icslp": {
   "authors": [
    [
     "Morio",
     "Kohno"
    ],
    [
     "Junko",
     "Matsubara"
    ],
    [
     "Katsuko",
     "Higuchi"
    ],
    [
     "Toshihiro",
     "Kashiwagi"
    ]
   ],
   "title": "Rhythm processing by a patient with pure anarthria: some suggestions on the role of rhythm in spoken language processing",
   "original": "i94_1223",
   "page_count": 4,
   "order": 320,
   "p1": "1223",
   "pn": "1226",
   "abstract": [
    "Rhythm processing consists of two neuropsychologically different works - holistic and analytic (Kohno, 1992). The former acts for the processing of fast rhythms and the latter for slow rhythms. A patient with pure anarthria (male, aged 45) (PA), nevertheless, dealt with all kinds of rhythm, including the fast one, by the analytic way alone. This extremely analytic idiosyncrasy may cause some of the typical symptoms of pure anarthria, such as motor incoordination of speech organs etc. In spite of this overly analytic way of processing, the patient clearly keeps the semantic unit which is generally perceived holistically, and shows the unit by pitch rising. This unit is called 'productive sense unit (PrSU) and it will be proven to be a counterpart of the 'perceptual sense unit* (PSU) whose existence was demonstrated by the authors' previous study on a split brain patient (Kohno, 1993). Judging from the fact that PA is a disease of speech production in motor programing, we may say that the PrSU, as well as the PSU, is rudimentarily made up in a deeper component than that of articulation programing. Finally, some evidence will be presented to show that speech disorders of PA may be brought about by the oppression of holistic processing, without destroying it, by the unusually analytic idiosyncrasy. These studies lead us to an important hypothesis that the holistic and analytic mechanisms may constitute a stratum - the holistic may make a basic structure and the analytic a hyperstructure.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-320"
  },
  "yamada94b_icslp": {
   "authors": [
    [
     "Nobuko",
     "Yamada"
    ]
   ],
   "title": "Japanese accentuation of foreign learners and its interlanguage",
   "original": "i94_1227",
   "page_count": 4,
   "order": 321,
   "p1": "1227",
   "pn": "1230",
   "abstract": [
    "This study investigates how non-native speakers of Japanese acquire Japanese accentuation, from the viewpoint of the location of the accent nucleus (AN). The data is analysed in regards to the following two features: (1) the acquisition of Japanese accentuation, and (2) the interim accentual system, which is predicted to be produced in the process of the acquisition, and called interlanguage strategies. The following conclusions are arrived at: (1) although the subjects seem to have acquired individual word accents and accentual rules to some extent and to be able to perceive accentual patterns, the pronunciation of correct accents appears to be difficult for them. Therefore, (2) they have created 10 interlanguage strategies under the strong influence of native Japanese accentuation, such as, for example, over-generalizations. (3) It appears likely that the subjects develop the applicaton of their own interlanguage strategies. (4) The application of interlanguage strategies tends to be variable even for a single word by the same subject.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-321"
  },
  "kaneko94_icslp": {
   "authors": [
    [
     "Masato",
     "Kaneko"
    ]
   ],
   "title": "Mechanisms producing recurring utterances in a patient with slowly progressive aphasia",
   "original": "i94_1231",
   "page_count": 4,
   "order": 322,
   "p1": "1231",
   "pn": "1234",
   "abstract": [
    "This paper discusses mechanisms of recurring utterances in a female patient with slowly progressive aphasia. The patient, TO, produced a meaningless recurring utterance [ko-no-shi:-no] in her spontaneous speech, but her ability to rewrite words written in kanji(Japanese ideograms) to kana(Japanese phonograms representing syllables or moras) sequences was relatively preserved. It was speculated that the production of spontaneous speech and writing kana sequences from kanji words share the phoneme level from speech output lexicon based on the cognitive model. Thus, The relatively preserved ability to rewrite kanji words to kana strongly suggests that the recurring utterance in this case was produced below the phoneme level as described by Blanken.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-322"
  },
  "katoh94b_icslp": {
   "authors": [
    [
     "Kiyokata",
     "Katoh"
    ],
    [
     "Takako",
     "Ayusawa"
    ],
    [
     "Yukihiro",
     "Nishinuma"
    ],
    [
     "Richard",
     "Harrison"
    ],
    [
     "Kikuko",
     "Yamashita"
    ]
   ],
   "title": "Hypermedia for spoken language education",
   "original": "i94_1235",
   "page_count": 4,
   "order": 323,
   "p1": "1235",
   "pn": "1238",
   "abstract": [
    "This paper describes a project for developing hypermedia software for spoken language education. The hardware is a combination of a Macintosh and a laser disk player, and the software is HyperCard stacks. The stacks are all interconnected and linked to the corresponding scenes in the video. Text stacks consist of the scenario in Japanese orthography, with translations into English, French, and other languages, as well as cultural or sociolinguistic explanations of certain words and expressions, and items in the scenes. Another text stack is for exercises based on expressions used in the video. The pitch data stack contains pitch curves for utterances in the video.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-323"
  },
  "bhaskararao94_icslp": {
   "authors": [
    [
     "P.",
     "Bhaskararao"
    ],
    [
     "Venkata N.",
     "Peri"
    ],
    [
     "Dr. Vishwas",
     "Udpikar"
    ]
   ],
   "title": "A text-to-speech system for application by visually handicapped and illiterate",
   "original": "i94_1239",
   "page_count": 4,
   "order": 324,
   "p1": "1239",
   "pn": "1242",
   "abstract": [
    "A practical text-to-speech system developed for Hindi language is discussed. The linguistic component includes a rule base and a segment base. The rule base converts a text input into an appropriate phoneme output of the language. For this purpose, various layers of rules are incorporated. The segment base consists of a library of digitized speech segments, of various lengths, demarcated on certain principles. Other part of the software includes modules for communication on serial link, database management and speech replay through the D/A converter. The hardware part of the system includes CPU, memory and the necessary I/O interfaces. Applications for the use of visually handicapped and illiterates are discussed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-324"
  },
  "giuliani94_icslp": {
   "authors": [
    [
     "Diego",
     "Giuliani"
    ],
    [
     "Maurizio",
     "Omologo"
    ],
    [
     "P.",
     "Svaizer"
    ]
   ],
   "title": "Talker localization and speech recognition using a microphone array and a cross-powerspectrum phase analysis",
   "original": "i94_1243",
   "page_count": 4,
   "order": 325,
   "p1": "1243",
   "pn": "1246",
   "abstract": [
    "Mismatch in training and testing conditions reduces considerably the performance of a speaker-independent HMM-based continuous speech recognizer. Compensation of this mismatch can avoid the complex and time-consuming retraining of the recognizer. This paper describes an acquisition system based on a four omnidirectional microphone array that was employed to reproduce a \"bearnformed\" version of the original acoustic messages acquired in a noisy and reverberant environment, with a talker-microphone distance of one meter. In this preliminary activity, some simple noise compensation techniques (i.e. a Mean Spectrum based Enhancement and a Cepstrum Mean Subtraction) were incorporated in this preprocessing stage to obtain an enhanced version of the given utterance. Feeding a clean-condition trained continuous speech recognizer with enhanced signals led to a significant improvement of performance, if compared to the use of unprocessed single-microphone signals as input.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-325"
  },
  "lin94b_icslp": {
   "authors": [
    [
     "Qiguang",
     "Lin"
    ],
    [
     "Ea-Ee",
     "Jan"
    ],
    [
     "Chi Wei",
     "Che"
    ],
    [
     "Bert de",
     "Vries"
    ]
   ],
   "title": "System of microphone arrays and neural networks for robust speech recognition in multimedia environments",
   "original": "i94_1247",
   "page_count": 4,
   "order": 326,
   "p1": "1247",
   "pn": "1250",
   "abstract": [
    "Hands-free operation of speech processing systems is sometimes desired to avoid encumbrance of the user by tethered microphone equipment. This paper explores the use of array microphones and neural networks (MANN) for robust speech recognition in real-world environments, such as large-group conferencing. Microphone arrays (MA) provide high-quality, hands-free sound pickup under severe acoustical conditions; and neural network (NN) processors \"learn\" the characteristics of environmental interference and transform features of MA-enhanced signal to those obtained under close-talking conditions. In this study, both realroom collected and computer-simulated reverberant speech signals are used to evaluate the power and advantages of MANN for direct deployment of speech recognition technology in adverse practical environments.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-326"
  },
  "rayner94_icslp": {
   "authors": [
    [
     "Manny",
     "Rayner"
    ],
    [
     "David",
     "Carter"
    ],
    [
     "Patti",
     "Price"
    ],
    [
     "Bertil",
     "Lyberg"
    ]
   ],
   "title": "Estimating performance of pipelined spoken language translation systems",
   "original": "i94_1251",
   "page_count": 4,
   "order": 327,
   "p1": "1251",
   "pn": "1254",
   "abstract": [
    "Most spoken language translation systems developed to date rely on a pipelined architecture, in which the main stages are speech recognition, linguistic analysis, transfer, generation and speech synthesis. When making projections of error rates for systems of this kind, it is natural to assume that the error rates for the individual components are independent, making the system accuracy the product of the component accuracies. The paper reports experiments carried out using the SRI-SICS-Telia Research Spoken Language Translator and a 1000-utterance sample of unseen data. The results suggest that the naive performance model leads to serious overestimates of system error rates, since there are in fact strong dependencies between the components. Predicting the system error rate on the independence assumption by simple multiplication resulted in a 16% proportional overestimate for all utterances, and a 19% overestimate when only utterances of length 1-10 words were considered.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-327"
  },
  "jo94_icslp": {
   "authors": [
    [
     "Cheol-Woo",
     "Jo"
    ],
    [
     "Kyung-Tae",
     "Kim"
    ],
    [
     "Yong-Ju",
     "Lee"
    ]
   ],
   "title": "Generation of multi-syllable nonsense words for the assessment of Korean text-to-speech system",
   "original": "i94_1255",
   "page_count": 4,
   "order": 328,
   "p1": "1255",
   "pn": "1258",
   "abstract": [
    "In this paper we propose a method to generate a multisyllable nonsense word set for the purpose of synthetic speech assessment and applies the wordset to assess one commercial text-to-speech system. Some results about the experiment is suggested and it is verified that the generated nonsense wordset can be used to assess the intelligibility of the synthesizer in phoneme level or in phonemic environmental level. From the experimental results it is verified that such multi-syllable nonsense wordset can be useful for the assessment of synthesized speech.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-328"
  },
  "bayya94_icslp": {
   "authors": [
    [
     "Aruna",
     "Bayya"
    ],
    [
     "Michael",
     "Durian"
    ],
    [
     "Lori",
     "Meiskey"
    ],
    [
     "Rebecca",
     "Root"
    ],
    [
     "Randall",
     "Sparks"
    ],
    [
     "Mark",
     "Terry"
    ]
   ],
   "title": "Voice map: a dialogue-based spoken language information access system",
   "original": "i94_1259",
   "page_count": 4,
   "order": 329,
   "p1": "1259",
   "pn": "1262",
   "abstract": [
    "In this paper we describe a system that provides speech-based access to complex information sources over a telephone. The main objective of this work is to provide information access with a simple spoken dialogue interface by integrating various technologies such as speech recognition, speech synthesis, natural language processing and dialogue management. The specific task chosen for feasibility testing of such an integration is getting street-map directions over the telephone. While the task presented in this paper is voice navigation, many of the concepts used in this system can be extended to other complex information access tasks. The prototype system, VoiceMap, requests the user's destination and starting location and provides directions for driving between these locations. All the components of the system are designed to run in a distributed architecture to increase performance. The job of controlling the interactions among the various components is an important aspect of the system. Since each component has different data requirements, a blackboard architecture is used to control flow of information. In this paper each of the components and the information flow among the components are discussed for the VoiceMap application domain.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-329"
  },
  "seto94_icslp": {
   "authors": [
    [
     "Shigenobu",
     "Seto"
    ],
    [
     "Kazuhiro",
     "Kimura"
    ]
   ],
   "title": "Development of a document preparation system with speech command using EDR electronic dictionaries",
   "original": "i94_1263",
   "page_count": 4,
   "order": 330,
   "p1": "1263",
   "pn": "1266",
   "abstract": [
    "This paper describes a document preparation system with speech commands which uses the EDR Electronic Dictionaries. The system employs a multimodal user interface, which allows keyboard inputs for writing documents and speech inputs for commanding the system to retrieve linguistic information from the EDR electronic dictionaries. The system consists of three components: a text editor, a speech processor, and a linguistic information retriever. It uses the EDR electronic dictionaries as a source of large scale linguistic knowledge that is referred to by the user preparing documents.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-330"
  },
  "angelini94_icslp": {
   "authors": [
    [
     "Bianca",
     "Angelini"
    ],
    [
     "Giuliano",
     "Antoniol"
    ],
    [
     "Fabio",
     "Brugnara"
    ],
    [
     "Mauro",
     "Cettolo"
    ],
    [
     "Marcello",
     "Federico"
    ],
    [
     "Roberto",
     "Fiutem"
    ],
    [
     "Gianni",
     "Lazzari"
    ]
   ],
   "title": "Radiological reporting by speech recognition: the a.re.s. system",
   "original": "i94_1267",
   "page_count": 4,
   "order": 331,
   "p1": "1267",
   "pn": "1270",
   "abstract": [
    "Radiological reporting has already been identified as a field in which voice technologies can prove to be very useful. Recent progress in automatic speech recognition and in hardware and software technology makes it possible to build large-vocabulary, continuous speech, speaker-independent, real-time systems. In this paper a dictation system for radiology reporting, the A.Re.S. system, is presented. A.Re.S. is a \"software only\" system which runs in real-time on an HP 715 workstation. It relies on an asynchronous and multi-process architecture in which speech decoding is performed by processes in pipeline. System requirements and architecture will be described, together with the results of a preliminary evaluation based on three months of on-site testing.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-331"
  },
  "bennacef94_icslp": {
   "authors": [
    [
     "S. K.",
     "Bennacef"
    ],
    [
     "H.",
     "Bonneau-Maynard"
    ],
    [
     "Jean-Luc",
     "Gauvain"
    ],
    [
     "Lori F.",
     "Lamel"
    ],
    [
     "Wolfgang",
     "Minker"
    ]
   ],
   "title": "A spoken language system for information retrieval",
   "original": "i94_1271",
   "page_count": 4,
   "order": 332,
   "p1": "1271",
   "pn": "1274",
   "abstract": [
    "Spoken language systems aim to provide a natural interface between humans and computers by using simple and natural dialogues to enable the user to access stored information. The LIMSI spoken language work is being pursued in several task domains. In this paper we present a system for vocal access to database for a French version of the Air Travel Information Services (ATIS) task. The ATIS task is a designated common task for data collection and evaluation within the ARPA Speech and Natural Language program. A complete spoken language system including a speech recognizer, a natural language component, a database query generator and a natural language response generator is described. The speaker independent continuous speech recognizer makes use of task-independent acoustic models trained on the BREF corpus and a task-specific language model. A case-frame approach is used for the natural language component. This component determines the meaning of the query and builds an appropriate semantic frame representation. The semantic frame is used to generate a database request to the ddatabase management system and the returned information is used to generate a response. First evaluation results for the ATIS task are given for the recognition and understanding components, as well as for the combined system.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-332"
  },
  "lindberg94_icslp": {
   "authors": [
    [
     "Borge",
     "Lindberg"
    ]
   ],
   "title": "Recogniser response modelling from testing on series of minimal word pairs",
   "original": "i94_1275",
   "page_count": 4,
   "order": 333,
   "p1": "1275",
   "pn": "1278",
   "abstract": [
    "The paper presents two approaches to recogniser word response modelling which rely on an estimation of interphoneme distances as a basis for modelling recogniser behaviour towards a particular vocabulary. The recogniser specific inter-phoneme distances are derived from testing on series of minimal word pairs in which only a single phoneme varies in fixed phonetic context. In the first approach an existing model of human word recognition is applied and modified to incorporate recogniser specific inter-phoneme distances rather than the original inter-phoneme distances obtained from human perception experiments. Experiments are conducted on both a DTW- and a CDHMM-recogniser architecture and the prediction capabilities are compared with \"live\" results from these two recognisers. These experiments lead to a formulation of the second approach, a new HMM-based response model.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-333"
  },
  "minowa94_icslp": {
   "authors": [
    [
     "Toshimitsu",
     "Minowa"
    ],
    [
     "Yasuhiko",
     "Arai"
    ],
    [
     "Hisanori",
     "Kanasashi"
    ],
    [
     "Tatsuya",
     "Kimura"
    ],
    [
     "Takuji",
     "Kawamoto"
    ]
   ],
   "title": "A study on the problems for apllication of voice interface based on ford recognition",
   "original": "i94_1279",
   "page_count": 4,
   "order": 334,
   "p1": "1279",
   "pn": "1282",
   "abstract": [
    "In this paper, we propose a criterion for designing practical voice recognition systems. In order to find out the criterion, psychological experiments were performed. In the experiments, subjects judged their degree of satisfaction with experimental voice recognition systems by 7 point scaling (from-3 to +3). It was found that the psychological satisfaction rate for the total system and the recognition capability had a positive corelation with the recognition accuracy. Moreover, it was observed that the satisfaction rate for the recognition capability constantly exceeded that for the total system. It seemed that the satisfaction rate for the total system was formed basically on the recognition capability and it was debased by other factors of the system. From these results, we thought that there was a possibility that if the system's recognition accuracy can be anticipated, the designer can foresee the user's satisfaction rate for the system. This can be a criterion for the recognition accuracy. This criterion can lead to a systematic method to design voice recognition systems. Some results of experiments with 2 types of systems were also described from the view point of this criterion.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-334"
  },
  "kamio94_icslp": {
   "authors": [
    [
     "Hiroyuki",
     "Kamio"
    ],
    [
     "Mika",
     "Koorita"
    ],
    [
     "Hiroshi",
     "Matsu'ura"
    ],
    [
     "Masafumi",
     "Tamura"
    ],
    [
     "Tsuneo",
     "Nitta"
    ]
   ],
   "title": "A UI design support tool for multimodal spoken dialogue system",
   "original": "i94_1283",
   "page_count": 4,
   "order": 335,
   "p1": "1283",
   "pn": "1286",
   "abstract": [
    "In this paper, we describe a user-interface ( UI ) design support tool to design and evaluate multimodal UI easily. Multimodal UI designers meet the various requirement of system engineers and design panels ( panel-data ) and describe plan-goal scenarios ( process-data ). In the UI design support tool, first, designers construct panel-data by setting Ul-objects on a card, and then describe plan-goal scenarios linking a Ul-object with other cards. Designers are also able to test their multimodal UI on the tool. The UI design support tool offers devicemanager to handle multiple inputs and transforms all the multimodal inputs into an X windows events. The UI design support tool can be applied for various types of multimodal dialogue systems.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-335"
  },
  "nishirnoto94_icslp": {
   "authors": [
    [
     "Takuya",
     "Nishirnoto"
    ],
    [
     "Nobutoshi",
     "Shida"
    ],
    [
     "Tetsunori",
     "Kobayashi"
    ],
    [
     "Katsuhiko",
     "Shirai"
    ]
   ],
   "title": "Multimodal drawing tool using speech, mouse and key-board",
   "original": "i94_1287",
   "page_count": 4,
   "order": 336,
   "p1": "1287",
   "pn": "1290",
   "abstract": [
    "In this paper, a multimodal drawing tool S-tgif is developed to verify the effectiveness of speech in human-machine interface. The system is designed in consideration of the basic principles and organization principles of interface that are required for comfortable input systems. Desired organization of interface using speech, mouse and key-board is discussed from the viewpoints of less physical movement, easiness to learn, and transparency of the system. As the results, the multimodal drawing tool with speech recognition reduces average operation time to 71%, the number of command operation to 77%, and movement of mouse pointer to 53% in evaluation with inexperienced users.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-336"
  },
  "arai94b_icslp": {
   "authors": [
    [
     "Yasuhiko",
     "Arai"
    ],
    [
     "Toshimitsu",
     "Minowa"
    ],
    [
     "Hiroko",
     "Yoshida"
    ],
    [
     "Hirofmi",
     "Nishimura"
    ],
    [
     "Hiroyvki",
     "Kamata"
    ],
    [
     "Takashi",
     "Honda"
    ]
   ],
   "title": "Generation of non-entry words from entries of the natural speech database",
   "original": "i94_1291",
   "page_count": 4,
   "order": 337,
   "p1": "1291",
   "pn": "1294",
   "abstract": [
    "In this paper, we describe a method to generate non-entry words from entries of the natural speech database which an automatic public announcing system is possessed of. Thereby, it becomes unnecessary to record new voices by a narrator. Non-entry words are generated by means of the waveform editing, that is, by the method of segmental speech sound concatenation. In case that there is no need to change the pitch pattern at editing, quality of the generated words is maintained to the level of natural speech sound. In case that the pitch pattern must be changed at editing, the zero-phased pitch waveform superposing method is used for pitch modification. In order to extract raw pitch waveforms, various windows including the Hanning and the Blackman-Harris whose length are proportional to the pitch period are tested. And, following results are obtained: (1)The Hanning window whose length is twice the pitch period is slightly superior to the Blackman-Harris windows. (2)Quality degradation of the generated words is a little bit.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-337"
  },
  "gomez94_icslp": {
   "authors": [
    [
     "Pedro",
     "Gomez"
    ],
    [
     "Daniel",
     "Martinez"
    ],
    [
     "Victor",
     "Nieto"
    ],
    [
     "Victoria",
     "Rodellar"
    ]
   ],
   "title": "MECALLSAT: a multimedia environment for computer-aided language learning incorporating speech assessment techniques",
   "original": "i94_1295",
   "page_count": 4,
   "order": 338,
   "p1": "1295",
   "pn": "1298",
   "abstract": [
    "This paper is focussed to the introduction and use of Speech Processing Techniques in the Assesment of the Speech Trace, with applications in Computer-Aided Language Learning (CALL). The general structure of a CALL Environment, and a Speech Interface to allow the spoken interaction of the student are presented. The system is intended to exploit and improve the speaking abilities of the student through a progressive Listening & Comprehension process supported by the registering and evaluation of the student's Speech in terms of Phonetics and Prosody. The general premises for a successful application of these techniques in Language Learning Laboratories are also presented.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-338"
  },
  "mcnair94_icslp": {
   "authors": [
    [
     "Arthur E.",
     "McNair"
    ],
    [
     "Alex",
     "Waibel"
    ]
   ],
   "title": "Improving recognizer acceptance through robust, natural speech repair",
   "original": "i94_1299",
   "page_count": 4,
   "order": 339,
   "p1": "1299",
   "pn": "1302",
   "abstract": [
    "Though large vocabulary speech recognition systems have improved greatly in recent years, usability of these systems in practical applications is still low, due to the ever-present errors. Adding a natural interface to these systems for users to correct errors should increase acceptance. This paper describes three methods for accomplishing speech-based repair of a misrecognition. The user must respeak or spell only an errorful subsection of the original utterance. A method is described to automatically locate the respoken subpiece in up to 90% of the instances. Once the subpiece has been respoken and located, another method is described which corrects the subpiece in up to 70% of the instances. If the location is known, and the subpiece is spelled, a third method is described which uses a single spelling utterance to correct the subpiece in up to 82% of the instances. The results indicate that these methods can decrease the error rate of a CSR by two thirds using only a single short repair utterance.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-339"
  },
  "fay94_icslp": {
   "authors": [
    [
     "David",
     "Fay"
    ]
   ],
   "title": "User acceptance of automatic speech recognition in telephone services",
   "original": "i94_1303",
   "page_count": 4,
   "order": 340,
   "p1": "1303",
   "pn": "1306",
   "abstract": [
    "This paper describes five experiments that compare consumer preferences for two types of user interface to interactive telephone services: TouchTone and Automatic Speech Recognition. The experiments measure consumers' subjective preferences after they interact with different versions of automated telephone services implemented using the Wizard-of-Oz technique. Versions of the interfaces differ in error rate, naturalness of vocabulary, interruptability of prompts, and so on. Two services were tested: a college course registration service and a telephone disconnect service. Consumers preferred TouchTone in the first, but Automatic Speech Recognition in the second. Several of the studies examine explanations for the results based on methodology, differences among consumer groups, and user interface design style. The paper concludes with some speculations about why Automatic Speech Recognition is more compatible with some services than others.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-340"
  },
  "love94_icslp": {
   "authors": [
    [
     "Stephen",
     "Love"
    ],
    [
     "R. T.",
     "Dutton"
    ],
    [
     "J. C.",
     "Foster"
    ],
    [
     "Mervyn A.",
     "Jack"
    ],
    [
     "F. W. M.",
     "Stentiford"
    ]
   ],
   "title": "Identifying salient usability attributes for automated telephone services",
   "original": "i94_1307",
   "page_count": 4,
   "order": 341,
   "p1": "1307",
   "pn": "1310",
   "abstract": [
    "This paper presents experimental results relating to the perceived usability of automated telephone services which incorporate a spoken language dialogue interface. Using a new Wizard of Oz experimental technique, users' attitudes towards automated telephone services are measured by a Likert attitude questionnaire. Factor Analysis performed on subjects' responses to the questionnaire produced a set of five underlying group factors which accounted for 74% of the variance in the questionnaire scores. On the basis of the factor analysis, a multiple regression model was constructed using the highest loading attributes on each of the five factors. The predictive power of this model was assessed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-341"
  },
  "mariniak94_icslp": {
   "authors": [
    [
     "Arnd",
     "Mariniak"
    ]
   ],
   "title": "Word complexity measures in the context of speech intelligibility tests",
   "original": "i94_1311",
   "page_count": 4,
   "order": 342,
   "p1": "1311",
   "pn": "1314",
   "abstract": [
    "This paper describes a study where the results of an intelligibility test (the Cluster Identification test) were compared to lexical information of words. Several parameters like word class, word frequency, cluster frequency and the transition probabilities of clusters were included, all obtained from a corpus of about 6000 monosyllabic German words. Different sets of words which were read aloud by a professional news reader were presented to subjects with and without masking noise. Investigations were carried out as to whether and to what extent the above mentioned parameters influence the word recognition process. The aim of the study was to gather information to clarify the as yet term 'complexity' so that it can be applied to characterize the degree of difficulty of a task in speech intelligibility tests. However, its understanding is extremely important for a valid interpretation and comparison of the results from different tests. It can be shown that the most influencing factors are word class and word frequency whereas such factors as cluster frequency, cluster transition probability or phoneme transition probability do not play an important role. The main influencing factors seem to dominate the others being processed in the background.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-342"
  },
  "wu94b_icslp": {
   "authors": [
    [
     "Frank H.",
     "Wu"
    ],
    [
     "Monica A.",
     "Maries"
    ]
   ],
   "title": "Recognition accuracy methods and measures",
   "original": "i94_1315",
   "page_count": 4,
   "order": 343,
   "p1": "1315",
   "pn": "1318",
   "abstract": [
    "This paper presents a standard data collection methodology and analysis measures which have been developed for reporting speech recognition accuracy. With this standard process, it is now possible to compare data collected at different times and on different systems. Moreover, other accuracy figures can now be interpreted based on knowing how their data were collected and analyzed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-343"
  },
  "jekosch94_icslp": {
   "authors": [
    [
     "Ute",
     "Jekosch"
    ],
    [
     "Louis C. W.",
     "Pols"
    ]
   ],
   "title": "A feature-profile for application-specific speech synthesis assessment and evaluation",
   "original": "i94_1319",
   "page_count": 4,
   "order": 344,
   "p1": "1319",
   "pn": "1322",
   "abstract": [
    "This paper is concerned with assessing the quality of speech synthesizers in application utilizing the conclusions made by subjects. Ultimately, the quality of speech becomes apparent in its usage, in this case the interaction between a machine and a biological system in a specific communicative context. As it is not possible to attain precise control over environmental circumstances of system application, however, a vast range of complicated human responses is possible. The approach introduced here is an attempt to bridge the gap between theory and experiment on the one hand, and, the actual performance of the system in application on the other hand, while still advocating the opinion that laboratory analogues of real applications are conceivable. What is still missing at present is a meaningful correlation between the properties of the speech signal and their relative relevance in an application-specific pragmatic context. Such a functionalized view of speech requires a unique formal description of both the speech signal properties and independently defined rudimentary variables characteristic of the situational communicative context.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-344"
  },
  "hegehofer94_icslp": {
   "authors": [
    [
     "Thomas",
     "Hegehofer"
    ]
   ],
   "title": "A description model for speech assessment tests with subjects",
   "original": "i94_1323",
   "page_count": 4,
   "order": 345,
   "p1": "1323",
   "pn": "1326",
   "abstract": [
    "This paper describes a data model for speech assessment tests with subjects. The aim of the proposed model is to obtain a description of the most important parameters which influence the outcome of speech assessment tests. Such a model enables a more controlled design of new test methods and the modification of existing methods. The proposed data model is divided into two parts: a description of the test methodology and a description of the used data. In order to verify the data model and to create software tools for speech assessment the proposed model was implemented as a software package.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-345"
  },
  "rodellar94_icslp": {
   "authors": [
    [
     "Victoria",
     "Rodellar"
    ],
    [
     "Antonio",
     "Diaz"
    ],
    [
     "Jose",
     "Gallardo"
    ],
    [
     "Virginia",
     "Peinado"
    ],
    [
     "Victor",
     "Nieto"
    ],
    [
     "Pedro",
     "Gomez"
    ]
   ],
   "title": "VLSI implementation of a robust hybrid parameter-extractor and neural network for speech decoding",
   "original": "i94_1327",
   "page_count": 4,
   "order": 346,
   "p1": "1327",
   "pn": "1330",
   "abstract": [
    "Through the present paper, the design and VLSI integration of an Application-Specific Inner-Product Processor is illustrated. This processor is especially suited to support Adaptive Parameter Extracting Algorithms and a Time-Delay Neural Network for their use in Speech Decoding. The main characteristics of the Integrated Device are its simplicity to program and implement Inner Products, and its capability to work in Real Time, facilitating the integration of a Speech Decoding Subsystem to be used in Speech Assesment Techniques for Computer-Aided Language Learning applications.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-346"
  },
  "watanabe94b_icslp": {
   "authors": [
    [
     "Toshiro",
     "Watanabe"
    ],
    [
     "Shinji",
     "Hayashi"
    ]
   ],
   "title": "An objective measure for qualitatively assessing low-bit-rate coded speech",
   "original": "i94_1331",
   "page_count": 4,
   "order": 347,
   "p1": "1331",
   "pn": "1334",
   "abstract": [
    "This paper reports an objective measure for assessing low-bit-rate coded speech. A model for this objective measure, in which several known features of the perceptual processing of speech sounds by the human ear are emulated, is based on the Hertz-to-Bark transformation, on critical-band filtering with a preemphasis for boosting higher frequencies, on nonlinear conversion for subjective loudness, and on temporal (forward) masking process. The Bark spectral distortion rating (BSDR) is computed for each 10-20 ms segment of the original and coded speech. The effectiveness of this measure was validated by regression analysis between the computed BSDR values and subjective MOS ratings obtained for a large number of utterances coded by several versions of a CELP coder and a VSELP coder under three degraded conditions: input speech levels, transmission error rates, and background noise levels. The BSDR values correspond better to MOS ratings than several commonly used measures. As a result, BSDR can be used to accurately predict subjective scores.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-347"
  },
  "ozeki94_icslp": {
   "authors": [
    [
     "Kazuhiko",
     "Ozeki"
    ]
   ],
   "title": "Performance comparison of recognition systems based on the akaike information criterion",
   "original": "i94_1335",
   "page_count": 4,
   "order": 348,
   "p1": "1335",
   "pn": "1338",
   "abstract": [
    "It is widely believed that we should apply a common set of test samples to all the recognition systems under test in order to make a reliable performance comparison. But how much is this true? We discuss this problem based on the Akaike Information Criterion (AIC). It becomes clear that by applying a common set of test samples, more discrimination power can be obtained, as has been believed, as to performance difference than by applying independent sets of samples to each system. The difference between them is, however, not so large as might be expected. The effect of applying a common set of test samples to two systems under test becomes prominent when we measure and utilize the number of samples recognized correctly by both systems in addition to the number of samples recognized correctly by each system.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-348"
  },
  "hanai94_icslp": {
   "authors": [
    [
     "Nobutoshi",
     "Hanai"
    ],
    [
     "Richard M.",
     "Stern"
    ]
   ],
   "title": "Robust speech recognition in the automobile",
   "original": "i94_1339",
   "page_count": 4,
   "order": 349,
   "p1": "1339",
   "pn": "1342",
   "abstract": [
    "In this paper we discuss a number of the ways in which the recognition accuracy of automatic speech recognition systems is affected by ambient noise in the automobile, along with the extent to which various techniques for robust speech recognition can provide for more robust recognition. We consider separately the effects of engine noise, interference by turbulent air outside the car, interference by sounds from the car's radio, and interference by the sounds of the car's windshield wipers. Recognition accuracy was compared using baseline processing, cepstral mean normalization (CMN), and codeword-dependent cepstral normalization (CDCN). The greatest degradation in recognition accuracy was produced by interference from AM-radio talk shows. The use of CMN and especially CDCN was found to be significantly improve recognition accuracy, except for the effects of interference from radio talk shows at low car speeds. This type of interference is effectively suppressed through the use of adaptive noise cancellation techniques.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-349"
  },
  "maciasguarasa94_icslp": {
   "authors": [
    [
     "Javier",
     "Macias-Guarasa"
    ],
    [
     "Manuel A.",
     "Leandro"
    ],
    [
     "Jose",
     "Colas"
    ],
    [
     "Alvaro",
     "Villegas"
    ],
    [
     "Santiago",
     "Aguilera"
    ],
    [
     "Jose M.",
     "Pardo"
    ]
   ],
   "title": "On the development of a dictation machine for Spanish: DIVO",
   "original": "i94_1343",
   "page_count": 4,
   "order": 350,
   "p1": "1343",
   "pn": "1346",
   "abstract": [
    "The first prototype of a low cost dictation machine for Spanish is described (DIVO). The main characteristics of our recognition approach are: bottom-up, hypothesis-verification strategy; large vocabulary, speaker dependent, isolated word recognition. Its modular structure is the cue for quick development and testing of different implementation alternatives. Two of them are presented: one is based in Static phoneme Modeling (SM) and the other uses Discrete Hidden Markov Modeling (DHMM). The system runs on a standard PC compatible (286 or higher) equipped with a DSP board and is fully voice controlled. This first version of the system can address multiple vocabulary sets of up to 2000 words each, with immediate response and reasonable performance. Modules for increasing vocabulary and performance are being developed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-350"
  },
  "ohshima94_icslp": {
   "authors": [
    [
     "Yoshiaki",
     "Ohshima"
    ],
    [
     "Richard M.",
     "Stern"
    ]
   ],
   "title": "Environmental robustness in automatic speech recognition using physiologic ally-motivated signal processing",
   "original": "i94_1347",
   "page_count": 4,
   "order": 351,
   "p1": "1347",
   "pn": "1350",
   "abstract": [
    "This paper examines methods by which speech recognition systems can be made more environmentally robust by analyzing the performance of Seneff' s model of auditory periphery [7]. The purpose of the paper is threefold. First, we document the extent to which the Seneff model reduces the degradation in speech recognition accuracy caused by additive noise and/or linear filtering. Second, we examine the extent to which individual components of the nonlinear neural transduction (NT) stage of the Seneff model contribute to recognition accuracy by evaluating the recognition accuracy with individual components of the model removed from the processing. Third, we determine the extent to which the robustness provided by the Seneff model is complementary to and independent of the improvement in recognition accuracy already provided by existing successful acoustical pre-processing algorithms such as codeword-dependent cepstral normalization (CDCN) [1]. Experimental techniques are proposed in the course of investigating the above issues. The results of speech recognition experiments using CMU's SPHINX [4] system under real and simulated degradation are reported.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-351"
  },
  "valtchev94_icslp": {
   "authors": [
    [
     "V.",
     "Valtchev"
    ],
    [
     "J. J.",
     "Odell"
    ],
    [
     "Phil C.",
     "Woodland"
    ],
    [
     "Steve J.",
     "Young"
    ]
   ],
   "title": "Recognition ********* a dynamic network decoder design for large vocabulary speech recognition",
   "original": "i94_1351",
   "page_count": 4,
   "order": 352,
   "p1": "1351",
   "pn": "1354",
   "abstract": [
    "Accuracy and speed are the main issues to consider when designing a large vocabulary speech recogniser. Recent experience with the Wall Street Journal (WSJ) corpus [5], has shown that high recognition accuracy requires the use of detailed acoustic models in conjunction with well-trained long span language models. In this paper we present a two-pass decoder architecture which extends an original [4] one-pass design. The initial pass consists of a time syn- chronous backward search in a pre-compiled network using simplified acoustic models and a null grammar. The forward pass can function as a stand-alone one-pass decoder capable of using cross-word context-dependent models and long span language models. The capabilities of this framework are empirically examined in terms of recognition accuracy vs speed on the Wall Street Journal database.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-352"
  },
  "ney94_icslp": {
   "authors": [
    [
     "Hermann",
     "Ney"
    ],
    [
     "Xavier",
     "Aubert"
    ]
   ],
   "title": "A word graph algorithm for large vocabulary, continuous speech recognition",
   "original": "i94_1355",
   "page_count": 4,
   "order": 353,
   "p1": "1355",
   "pn": "1358",
   "abstract": [
    "This paper describes a method for the construction of a word graph (or lattice) for large vocabulary, continuous speech recognition. The advantage of a word graph is that a fairly good degree of decoupling between acoustic recognition at the 10-ms level and the final search at the word level using a complicated language model can be achieved. The word graph algorithm is obtained as an extension of the one-pass beam search strategy using predecessor dependent word models. The method has been tested successfully on the 20,000-word Wall Street Journal task (American English, continuous speech, 20,000 words, speaker independent). The word graph density could be reduced to an average number of about 10 word hypotheses, i.e. word arcs in the graph, per spoken word without loss of performance.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-353"
  },
  "phillips94_icslp": {
   "authors": [
    [
     "Michael",
     "Phillips"
    ],
    [
     "David",
     "Goddeau"
    ]
   ],
   "title": "Fast match for segment-based large vocabulary continuous speech recognition",
   "original": "i94_1359",
   "page_count": 4,
   "order": 354,
   "p1": "1359",
   "pn": "1362",
   "abstract": [
    "This paper describes the use of a lexical tree based lookahead for reducing the computation needed by the lexical search stage of a segment-based speech recognition system. In the MIT SUMMIT system, a network representing possible phonetic interpretations of the signal is generated before the lexical search is performed. This allows the use of a fairly simple tree-based lookahead in order to find a reduced set of words which may be allowed to start at any point in time. The first N phones in the pronunciations of all words in the lexicon are collapsed into a tree and this tree is matched against the segment network using a pruning threshold to determine a subset of words which may start at each node in the segment network. We describe the computational needs of the system, explains the lookahead in more detail, and shows the tradeoffs between computation and accuracy.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-354"
  },
  "wooters94_icslp": {
   "authors": [
    [
     "Chuck",
     "Wooters"
    ],
    [
     "Andreas",
     "Stolcke"
    ]
   ],
   "title": "Multiple-pronunciation lexical modeling in a speaker independent speech understanding system",
   "original": "i94_1363",
   "page_count": 4,
   "order": 355,
   "p1": "1363",
   "pn": "1366",
   "abstract": [
    "One of the sources of difficulty in speech recognition and understanding is the variability due to alternate pronunciations of words. To address the issue we have investigated the use of multiple-pronunciation models (MPMs) in the decoding stage of a speaker-independent speech understanding system. In this paper we address three important issues regarding MPMs: (a) Model construction: How can MPMs be built from available data without human intervention? (b) Model embedding: How should MPM construction interact with the training of the sub-word unit models on which they are based? (c) Utility: Do they help in speech recognition? Automatic, data-driven MPM construction is accomplished using a structural HMM induction algorithm. The resulting MPMs are trained jointly with a multi-layer perceptron functioning as a phonetic likelihood estimator. The experiments reported here demonstrate that MPMs can significantly improve speech recognition results over standard single pronunciation models.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-355"
  },
  "normandin94_icslp": {
   "authors": [
    [
     "Yves",
     "Normandin"
    ],
    [
     "Roxane",
     "Lacouture"
    ],
    [
     "Regis",
     "Cardin"
    ]
   ],
   "title": "MMIE training for large vocabulary continuous speech recognition",
   "original": "i94_1367",
   "page_count": 4,
   "order": 356,
   "p1": "1367",
   "pn": "1370",
   "abstract": [
    "Over the past few years, there have been several published reports on the use of MMIE (Maximum Mutual Information Estimation) for training HMM parameters. Many of these reports clearly demonstrate MMIE's capability of substantially reducing the error rate in certain types speech recognition applications. The most convincing demonstrations, however, have usually been carried out on either small vocabulary tasks or on isolated speech. This is in large part a consequence of the fact that, for large vocabularies, MMIE training is much too computationally expensive and approximations must therefore be found in order to bring the task down to a manageable size. This paper looks at such approximations which can be applied in the context of large vocabulary continuous speech recognition and, in particular, proposes a technique based on the use of a compact looped word lattice. Experiments are used to demonstrate the effectiveness of the technique.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-356"
  },
  "yang94b_icslp": {
   "authors": [
    [
     "Yen-Ju",
     "Yang"
    ],
    [
     "Sung-Chien",
     "Lin"
    ],
    [
     "Lee-Feng",
     "Chien"
    ],
    [
     "Keh-Jiann",
     "Chen"
    ],
    [
     "Lin-Shan",
     "Lee"
    ]
   ],
   "title": "An intelligent and efficient word-class-based Chinese language model for Mandarin speech recognition with very large vocabulary",
   "original": "i94_1371",
   "page_count": 4,
   "order": 357,
   "p1": "1371",
   "pn": "1374",
   "abstract": [
    "This paper proposes a word-class-based Chinese language model for Mandarin speech recognition with very large vocabulary. The word classes used are developed based on the special structure of Chinese words. We have also developed some improved techniques. The ambiguous syllable filter can delete many confusion syllables and increase significantly the accuracy. The short-term cache memory can help the language model to adapt to the current application domain, and the learning module can significantly reduce the zero values in the language model.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-357"
  },
  "kosaka94_icslp": {
   "authors": [
    [
     "Tetsuo",
     "Kosaka"
    ],
    [
     "Shoichi",
     "Matsunaga"
    ],
    [
     "Shigeki",
     "Sagayama"
    ]
   ],
   "title": "Tree-structured speaker clustering for speaker-independent continuous speech recognition",
   "original": "i94_1375",
   "page_count": 4,
   "order": 358,
   "p1": "1375",
   "pn": "1378",
   "abstract": [
    "We have already proposed a tree-structured speaker clustering method and its application to supervised speaker adaptation. This adaptation method is based on the selection of a speaker cluster from among multiple reference speaker clusters. Since the adaptation method employs cluster selection rather than parameter training, it can adapt quickly using only a small amount of training data. In this paper, we extend this method for application to unsupervised speaker adaptation and speaker-independent speech recognition. The results show that the adaptation method using short calibration speech (less than 5 sec) outperforms a speaker-independent recognition system.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-358"
  },
  "kimura94_icslp": {
   "authors": [
    [
     "Tatsuya",
     "Kimura"
    ],
    [
     "Hiroyasu",
     "Kuwano"
    ],
    [
     "Akira",
     "Ishida"
    ],
    [
     "Taisuke",
     "Watanabe"
    ],
    [
     "Shoji",
     "Hiraoka"
    ]
   ],
   "title": "Compact-size speaker independent speech recognizer for large vocabulary using \"compats\" method",
   "original": "i94_1379",
   "page_count": 4,
   "order": 359,
   "p1": "1379",
   "pn": "1382",
   "abstract": [
    "This paper describes the development of a speaker-independent speech recognizer for large vocabulary which performs well in practical applications. The method named \"COMPATS(COntinu ous Matching of PArameters of Time-Spectral form)\"[l] is used for recognition accuracy. A combination of the segment-based parameter and the statistically based distance measure given by the linear discriminant function make it possible to use the dynamic features of speech effectively and precisely. To realize a small sized recognizer a two-stage matching procedure is developed. In the first stage, rough but high speed matching for all templates finds a small number of appropriate candidates. For noise robustness a word-spotting technique is used in this stage. In the second stage, precise match ing is performed for the candidates obtained in the first stage and then the final recognition result is determined. Using this technique we implemented a 500-word recognizer on a small board with a single DSP chip.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-359"
  },
  "masai94_icslp": {
   "authors": [
    [
     "Yasuyuki",
     "Masai"
    ],
    [
     "Jun'Ichi",
     "Iwasaki"
    ],
    [
     "Shin'Ichi",
     "Tanaka"
    ],
    [
     "Tsuneo",
     "Nitta"
    ],
    [
     "Masahiro",
     "Yao"
    ],
    [
     "Tomohiro",
     "Onogi"
    ],
    [
     "Akira",
     "Nakayama"
    ]
   ],
   "title": "A keyword-spotting unit for speaker-independent spontaneous speech recognition",
   "original": "i94_1383",
   "page_count": 4,
   "order": 360,
   "p1": "1383",
   "pn": "1386",
   "abstract": [
    "In this paper, we describe a real-time keyword-spotting unit (KeySpot) with an adaptive noise-canceller for speaker-independent, spontaneous speech recognition in noisy environments. KeySpot consists of a DSP (TMS320C30) for adaptive noise-cancellation and acoustic analysis, a special LSI for statistical matrix quantization (SMQ), two SPARC chips ('SPARC1' and 'SPARC2') for HMM based keyword-spotting, and a SPARC chip ('SPARC3') for syntactic analysis. KeySpot was tested under two conditions: a speaker-independent large-vocabulary isolated word recognizer, and a speaker-independent small-vocabulary word spotter. Evaluation results have shown that KeySpot can be used for the speaker-independent, 1000 isolated word recognizer with an accuracy of 96.3%, as well as the 90 word vocabulary word spotter with an accuracy of 94.4% with a response time of 0.3 sec.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-360"
  },
  "koo94_icslp": {
   "authors": [
    [
     "Myoung-Wan",
     "Koo"
    ],
    [
     "Sang-Kyu",
     "Park"
    ],
    [
     "Kyung-Tae",
     "Kong"
    ],
    [
     "Sam-joo",
     "Doh"
    ]
   ],
   "title": "KT-stock: a speaker-independent large-vocabulary speech recognition system over the telephone",
   "original": "i94_1387",
   "page_count": 4,
   "order": 361,
   "p1": "1387",
   "pn": "1390",
   "abstract": [
    "In this paper, we present a speaker-independent, large-vocabulary speech recognition system over the telephone (KT-STOCK) . If a user say isolated Korean company name over the telephone according to the voice instruction, the system announces real time stock quotes. The system can recognize 892 words consisting of 9 control words and 883 company names listed on the Korea Stock Exchange. The system is a HMM(hidden Markov model)-based isolated speech recognizer which uses phoneme-like unit as a basic unit. Currently, we have achieved recognition results of 92.0% over the telephone network.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-361"
  },
  "angelini94b_icslp": {
   "authors": [
    [
     "Bianca",
     "Angelini"
    ],
    [
     "Fabio",
     "Brugnara"
    ],
    [
     "D.",
     "Falavigna"
    ],
    [
     "Diego",
     "Giuliani"
    ],
    [
     "Roberto",
     "Gretter"
    ],
    [
     "Maurizio",
     "Omologo"
    ]
   ],
   "title": "Speaker independent continuous speech recognition using an acoustic-phonetic Italian corpus",
   "original": "i94_1391",
   "page_count": 4,
   "order": 362,
   "p1": "1391",
   "pn": "1394",
   "abstract": [
    "The objective of this paper is to describe the activity that is being carried out at IRST laboratories for the development of an HMM-based speaker independent continuous speech recognition system for the Italian language. The recognition system is trained and tested using the acoustic-phonetic continuous speech portion of the APASCI corpus. Acoustic modeling is based on the use of Continuous Density HMMs with gaussian mixture observation densities. As a baseline, a set of 38 Context Independent Units was evaluated using different numbers of mixture components. Then, two other classes of Context Dependent Unit sets were considered, that provide different performance and system complexity. Performance, expressed in terms of Phone loop recognition accuracy and Word loop recognition accuracy, shows an improvement using both of these classes of unit sets, with respect to the baseline.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-362"
  },
  "patterson94b_icslp": {
   "authors": [
    [
     "Roy D.",
     "Patterson"
    ],
    [
     "Timothy R.",
     "Anderson"
    ],
    [
     "Michael",
     "Allerhand"
    ]
   ],
   "title": "The auditory image model as a preprocessor for spoken language",
   "original": "i94_1395",
   "page_count": 4,
   "order": 363,
   "p1": "1395",
   "pn": "1398",
   "abstract": [
    "In the auditory system, the primary fibres that encode the mechanical motion of the basilar partition are phase locked to that motion, and auditory processing in the mid-brain preserves this information, to varying degrees, up to the level of the inferior colliculus. We know that this timing information is used in the localisation of point sources [1] and it is probably also used to separate point sources from more diffuse background noise. The time intervals in these neural patterns are on the order of milliseconds and so traditional speech preprocessors (like MCC and MFCC systems), with frames on the order of 15 milliseconds, remove the time-interval information from the representation. The performance of these systems deteriorates badly when the speaker is in a noisy environment with competing sources. This suggests that we will eventually need to incorporate time-interval processing into speech recognition systems if we are to achieve the kind of noise resistance characterisitic of human speech recognition. In this paper, we describe a) an auditory model designed to stabilise repeating time-interval patterns, b) the 'data-rate problem' associated with auditory models as speech preprocessors, c) a strategy for developing a noise resistant auditory spectrogram for speech recognition, and d) recent recognition results with a monaural auditory spectrogram.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-363"
  },
  "kawahara94b_icslp": {
   "authors": [
    [
     "HIdeki",
     "Kawahara"
    ]
   ],
   "title": "Effects of natural auditory feedback on fundamental frequency control",
   "original": "i94_1399",
   "page_count": 4,
   "order": 364,
   "p1": "1399",
   "pn": "1402",
   "abstract": [
    "This paper introduces a model of fundamental frequency control which includes auditory feedback as an indispensable component. Through the use of a new technique called Transformed Auditory Feedback (TAF), it is shown that there is a compensatory response with 100ms to 200ms latency to fundamental frequency fluctuations. However, it is not clear whether or not this response is an artifact caused by the artificial feedback conditions. Fundamental frequency trajectories produced without artificial feedback were analyzed to test whether the response found using TAF also exists under natural conditions. Spectral estimates of these trajectories obtained using an auto-regressive (AR) model extracted several sharp resonances inside 2Hz to 5Hz region, which correspond to compensatory responses with 100ms to 250ms latency. PARCOR parameters also showed that there are negative peaks in the same region. These observations were confirmed by Delayed Auditory Feedback (DAF) experiments. The predicted amount of shift was observed in both spectral peaks and peaks in PARCOR coefficients. These results illustrate that auditory perception plays the role F0 an automatic regulating system of fundamental frequencies. They also show that response parameters extracted by TAF experiments are consistent with parameters found under natural conditions.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-364"
  },
  "nakatani94_icslp": {
   "authors": [
    [
     "Tomohiro",
     "Nakatani"
    ],
    [
     "Takeshi",
     "Kawabata"
    ],
    [
     "Hiroshi G.",
     "Okuno"
    ]
   ],
   "title": "Unified architecture for auditory scene analysis and spoken language processing",
   "original": "i94_1403",
   "page_count": 4,
   "order": 365,
   "p1": "1403",
   "pn": "1406",
   "abstract": [
    "We proposes a unified architecture for auditory scene analysis (ASA) and spoken language processing (SLP). The unified system is expected to provide robust and friendly human-computer interface in the real acoustical environment. Because the number of auditory streams can be enormous, an adaptive stream attention mechanism is required. We consider adaptive understanding (behavior) emerges from competing goals in a multi-agent system. The behavior-based multi-agent system can explain various kinds of activities in human communications. In this paper, as the first stage of this approach, we design and implement a multi-agent based stream segregation system. The system dynamically generates stream segregation agents, which extract auditory streams incrementally. As clues to segregation, these agents use only simple sound attributes, harmonics and average spectrum intensity. To resolve stream interference, each agent communicates with other agents by signal subtraction and by common threshold modification. The resulting system, as a whole, segregates streams adaptively from the mixed sounds. The experimental results show that the system can segregate dual voice effectively even under noisy condition.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-365"
  },
  "cutler94_icslp": {
   "authors": [
    [
     "Anne",
     "Cutler"
    ],
    [
     "Duncan",
     "Young"
    ]
   ],
   "title": "Rhythmic structure of word blends in English",
   "original": "i94_1407",
   "page_count": 4,
   "order": 366,
   "p1": "1407",
   "pn": "1410",
   "abstract": [
    "Word blends combine fragments from two words, either in speech errors or when a new word is created. Previous work has demonstrated that in Japanese, such blends preserve moraic structure; in English they do not. A similar effect of moraic structure is observed in perceptual research on segmentation of continuous speech in Japanese; English listeners, by contrast, exploit stress units in segmentation, suggesting that a general rhythmic constraint may underlie both findings. The present study examined whether this parallel would also hold for word blends. In spontaneous English polysyllabic blends, the source words were significantly more likely to be split before a strong than before a weak (unstressed) syllable, ie. to be split at a stress unit boundary. In an experiment in which listeners were asked to identify the source words of blends, significantly more correct detections resulted when splits had been made before strong syllables. Word blending, like speech segmentation, appears to be constrained by language rhythm.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-366"
  },
  "kakehi94_icslp": {
   "authors": [
    [
     "Kazuhiko",
     "Kakehi"
    ],
    [
     "Kazumi",
     "Kato"
    ]
   ],
   "title": "Perception for VCV speech uttered simultaneously or sequentially by two talkers",
   "original": "i94_1411",
   "page_count": 4,
   "order": 367,
   "p1": "1411",
   "pn": "1414",
   "abstract": [
    "We investigated robust perception of speech using different talkers VCVs uttered simultaneously or sequentially. We conducted two experiments: one to investigate the perception of the number of talkers and the other to investigate the perception of intervocalic consonants. The experimental results are as follows : (1) Even when two talkers are perceived in a VCV stimulus, the extracted phoneme features are perceptually integrated regardless of the talker source information. (2) Phoneme features in a pre-closure part in double talk contribute to intervocalic stop consonant perception regardless of talker source information. This means that the talker source information is used to determine speech portion where phoneme features are extracted, and that these features are integrated to perceive a phoneme regardless of the talker source information.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-367"
  },
  "amano94b_icslp": {
   "authors": [
    [
     "Shigeaki",
     "Amano"
    ]
   ],
   "title": "Perception of time-compressed/expanded Japanese words depends on the number of perceived phonemes",
   "original": "i94_1415",
   "page_count": 4,
   "order": 368,
   "p1": "1415",
   "pn": "1418",
   "abstract": [
    "To investigate the relationship between phoneme perception and word perception, the relationships between the perception point of a word or a nonword and the perception point of a phoneme were measured using lexical decision and phoneme detection tasks for time-compressed stimuli, naturally spoken stimuli, and time-expanded stimuli. The perception point of a word/nonword was revealed to have a constant relationship to the perception point of a phoneme regardless of the time-expansion rate, that is, a constant number of phonemes was perceived before the perception point of a word/nonword. The results suggest that word/nonword perception depends on phoneme perception, not on the time positions of acoustical cues. The results also support the cohort model in which a word/nonword is perceived with the sequential processing of phonemes.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-368"
  },
  "radeau94_icslp": {
   "authors": [
    [
     "Monique",
     "Radeau"
    ],
    [
     "Juan",
     "Segui"
    ],
    [
     "Jose",
     "Morais"
    ]
   ],
   "title": "The effect of overlap position in phonological priming between spoken words",
   "original": "i94_1419",
   "page_count": 4,
   "order": 369,
   "p1": "1419",
   "pn": "1422",
   "abstract": [
    "Phonological priming between auditorily presented items was examined as a function of the early or late position of the overlap and of the prime-target relative frequency. While no effect occurred for beginning overlap, facilitation was consistently found between words with final overlap with both lexical decision and shadowing as task. It was not affected by relative frequency. The aim of the present experiments was to determine the locus of the effect. No effect was found, using a crossmodal (auditory-visual) priming procedure and the lexical decision task with ISIs of 0 ms and 50 ms. The effects of presenting either a word or a pseudoword prime sharing the rime with the word target were then compared. The items were presented auditorily and the task was shadowing. A facilitatory effect was again found but it was unaffected by the lexical status of the prime. These results suggest that the facilitatory effect of final-overlap is prelexical.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-369"
  },
  "yanagida94_icslp": {
   "authors": [
    [
     "Masuzo",
     "Yanagida"
    ]
   ],
   "title": "A cognitive model of inferring unknown words and uncertain sound sequence",
   "original": "i94_1423",
   "page_count": 4,
   "order": 370,
   "p1": "1423",
   "pn": "1426",
   "abstract": [
    "Proposed in this paper is a model of inferring the meanings of unknown words and rinding plausible words that fit to phoneme sequence containing inaudible/unsure articulation in dialog speech. The model is divided into two cases which correspond to particular mechanisms for 1) recognizing unknown words, and then inferring their meanings by looking for a coherent interpretation that satisfies semantic, syntactic and pragmatic constraints, and 2) finding the most plausible words to be assigned to some part of phoneme sequence containing inaudible sounds or unsure phoneme recognition.\n",
    "Case 1) appears when one clearly hears words out of his/her vocabulary, while case 2) usually occurs to aged persons having hearing loss. The latter sometimes happens even to persons of ordinary hearing particularly in case he/she holds preconceptions or preoccupations.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-370"
  },
  "otake94_icslp": {
   "authors": [
    [
     "Takashi",
     "Otake"
    ],
    [
     "Kiyoko",
     "Yoneyama"
    ]
   ],
   "title": "A moraic nasal and a syllable structure in Japanese",
   "original": "i94_1427",
   "page_count": 4,
   "order": 371,
   "p1": "1427",
   "pn": "1430",
   "abstract": [
    "In the previous study the recognition of a mora in Japanese during on-line speech perception was investigated independent of a syllable structure [1]. In this study the role of a syllable structure and the durational properties of a moraic nasal in Japanese during the off-line speech perception were explored. Three experiments regarding the recognition of a nasal were conducted. In the first experiment a reduced nasal duration in two positions, the onset and the coda were examined. The results have shown that the recognition of a nasal in the two positions can be accomplished irrelevant to the durational properties. In the second experiment, the relationship between the duration of a nasal and a syllable structure was examined. The results have shown that both the durational properties and a syllable structure played an important role in the recognition of a moraic nasal in Japanese. In the third experiment, it was tested how Dutch listeners recognized the nasal in the Japanese materials. The results have shown that their recognition system differed from that by the Japanese listeners.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-371"
  },
  "smeele94_icslp": {
   "authors": [
    [
     "Paula M. T.",
     "Smeele"
    ],
    [
     "Anne C.",
     "Sittig"
    ],
    [
     "Vincent J. van",
     "Heuven"
    ]
   ],
   "title": "Temporal organization of bimodal speech information",
   "original": "i94_1431",
   "page_count": 4,
   "order": 372,
   "p1": "1431",
   "pn": "1434",
   "abstract": [
    "We used the 'gating' method to study the perception of both auditory and visual speech on the phoneme level [4]. We investigated at which moments during the presentation of phonemes humans are able to correctly identify some or all of their phonetic features. The results indicated that the visual information about place of articulation is available for perceptual processing sooner than is the auditory information about place, manner of articulation and voicing. A plausible explanation seems to be that this difference is related to the difference in onset time between the visual and the acoustical speech signal. It does not seem necessary to assume substantial differences in processing times for the auditory and the visual modality.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-372"
  },
  "shigeno94_icslp": {
   "authors": [
    [
     "Sumi",
     "Shigeno"
    ]
   ],
   "title": "The use of auditory and phonetic memories in the discrimination of stop consonants under audio-visual presentation",
   "original": "i94_1435",
   "page_count": 4,
   "order": 373,
   "p1": "1435",
   "pn": "1438",
   "abstract": [
    "The use of short-term memory under audio-visual presentation was investigated. The experiment consisted of two tests. An identification test was carried out to provide a baseline for the influence of visual information on auditory judgment. An AX-discrimination test was conducted to examine the role of short-term memory of audio-visual stimuli. It was observed that (1) the timbre of the fused percept was judged to be modified by visual information, (2) stimulus order effects occurred under audio-visual presentation, (3) the inter-stimulus interval influenced the discrimination of audio-visual stimuli. The results suggest that two kinds of short-term memories are involved in the discrimination of audio-visual stimuli and that the auditory memory of the fused percept is influenced and modified by visual information.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-373"
  },
  "karlsson94b_icslp": {
   "authors": [
    [
     "Inger",
     "Karlsson"
    ]
   ],
   "title": "Controlling voice quality of synthetic speech",
   "original": "i94_1439",
   "page_count": 4,
   "order": 374,
   "p1": "1439",
   "pn": "1442",
   "abstract": [
    "The paper will give an short exposd of some of the experiments performed to produce good and different voice qualities through the history of speech synthesis. The experiments have mainly been concerned with creating one or more natural-sounding voices for a specific speech synthesiser. Some examples of productions of emotive speech and of changes of voice qualities between for example normal and breathy voices are also mentioned. Emphasis will be on recent synthesis experiments performed at KTH.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-374"
  },
  "pols94_icslp": {
   "authors": [
    [
     "Louis C. W.",
     "Pols"
    ]
   ],
   "title": "Voice quality of synthetic speech: representation and evaluation",
   "original": "i94_1443",
   "page_count": 4,
   "order": 375,
   "p1": "1443",
   "pn": "1446",
   "abstract": [
    "In most present-day rule synthesis systems (whether allophone-based or using concatenative units), the voice quality is generally limited to one voice and one speaking style. Although by now more knowledge is gathered about how to produce natural-sounding female voices, how to include some emotional elements in synthetic speech, and how to produce more acceptable prosody, a controlled and optimized voice quality of synthetic speech is so far much more a research goal than a reality. This does not preclude of course good use of present-day synthetic speech for specific applications. But even under those conditions, the presently available methods to evaluate speech quality, barely touch on voice quality. Most emphasis is on phonemic quality, which is expressed in phoneme, word, or sentence intelligibility measures, or on very global measures such as overall quality, naturalness, or acceptability scores, generally collected via scale judgments. We present some possibilities for diagnostic and functional evaluation of the voice quality of synthetic speech for specific applications.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-375"
  },
  "ofuka94_icslp": {
   "authors": [
    [
     "Etsuko",
     "Ofuka"
    ],
    [
     "Helene",
     "Valbret"
    ],
    [
     "Mitch",
     "Waterman"
    ],
    [
     "Nick",
     "Campbell"
    ],
    [
     "Peter",
     "Roach"
    ]
   ],
   "title": "The role of F0 and duration in signalling affect in Japanese: anger, kindness and Politeness",
   "original": "i94_1447",
   "page_count": 4,
   "order": 376,
   "p1": "1447",
   "pn": "1450",
   "abstract": [
    "This paper describes a study which is concerned with the effects of changing fundamental frequency (fO) and segmental duration, examining their interaction with different speaking styles (\"angry\" and \"kind\"). It was found that whereas appropriate settings of both duration and F0 can change the perception of these affects in the resultant speech, neither cue alone is strong enough to override the effect of the original voice source. The relative importance of these prosodic cues varied depending on the kind of affect studied.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-376"
  },
  "fant94_icslp": {
   "authors": [
    [
     "Gunnar",
     "Fant"
    ],
    [
     "Anita",
     "Kruckenberg"
    ],
    [
     "Johan",
     "Liljencrants"
    ],
    [
     "Mats",
     "Bavegdrd"
    ]
   ],
   "title": "Voice source parameters in continuous speech, transformation of LF-parameters",
   "original": "i94_1451",
   "page_count": 4,
   "order": 377,
   "p1": "1451",
   "pn": "1454",
   "abstract": [
    "This is a report on a data reduction scheme for studies of voice source characteristics in connected speech and some results from speech analysis within a segmental and prosodic frame. It is shown that essentials of glottal wave shape are included in the ratio of peak glottal volume velocity U0 to the negative peak Ee of glottal flow derivative. Default values of the complete set of LF-source parameters F0, Ee, Ra, Rk, Rg can be predicted from F0, Ee and U0/Eg. Of special importance is the relation of Ee and U0 to F0. Additional adjustments related to voice type and contextual demands are included. Simplified inverse filtering methods for extraction and direct recording of U0(t) and Ee(t) in synchrony with F0(t) are described. The dependency of source parameters on voice fundamental frequency F0, global phrase structure, loudness, stress and accents, and the influence of segmental type and supraglottal and subglottal interaction effects are briefly discussed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-377"
  },
  "abe94_icslp": {
   "authors": [
    [
     "Masanobu",
     "Abe"
    ],
    [
     "Hideyuki",
     "Mizuno"
    ]
   ],
   "title": "Speaking style conversion by changing prosodic parameters and formant frequencies",
   "original": "i94_1455",
   "page_count": 4,
   "order": 378,
   "p1": "1455",
   "pn": "1458",
   "abstract": [
    "The aim of this paper is to generate various speaking styles with the goal of enhancing the speech uttered by a person. Speaking style conversion is achieved by applying methods that we have proposed for text-to-speech systems and voice conversion. As the first trial, we selected three speaking styles appropriate for task specific texts, and analyzed their characteristics. Large differences were found in 1st and 3rd formant frequency, F0range and phrase height assignments, speaking rate, and segmental duration of phrases followed by pauses. Preliminary experiments of speaking style conversion are performed. Listening tests show that the proposed approach can freely change the speaking style.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-378"
  },
  "kasuya94_icslp": {
   "authors": [
    [
     "Hideki",
     "Kasuya"
    ],
    [
     "Xuan",
     "Tan"
    ],
    [
     "Chang-Sheng",
     "Yang"
    ]
   ],
   "title": "Voice source and vocal tract characteristics associated with speaker individuality",
   "original": "i94_1459",
   "page_count": 4,
   "order": 379,
   "p1": "1459",
   "pn": "1462",
   "abstract": [
    "The paper first discusses voice quality in terms of information conveyed by spoken language, primarily focusing on the relation between voice quality and speaker individuality. Experiments are performed on speaker identification by a computer and humans using short phrase utterances which are obtained under various vocal imitation conditions. Roles of both voice source characteristics associated with voice qualities and formant frequencies related to voice set in characterizing speaker individuality are investigated through the experiments. Results of the experiments show that formant frequencies are the significant parameters to represent speaker individuality.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-379"
  },
  "furui94_icslp": {
   "authors": [
    [
     "Sadaoki",
     "Furui"
    ],
    [
     "Tomoko",
     "Matsui"
    ]
   ],
   "title": "Phoneme-level voice individuality used in speaker recognition",
   "original": "i94_1463",
   "page_count": 4,
   "order": 380,
   "p1": "1463",
   "pn": "1466",
   "abstract": [
    "This paper reports our recent investigation on voice individuality (speaker characterization) at the phoneme level of speech. The first half of the paper reports our perceptual experiments on speaker characterizing information included in continuous speech. Experimental results show that speaker-characterizing vowels differ from speaker to speaker. The latter half of the paper introduces a text-prompted speaker recognition method that we have recently proposed as an automatic speaker recognition method using phoneme units. In this method, the recognition system prompts each user with a new key sentence every time the system is used, and accepts the input utterance only when it decides that the registered speaker has uttered the prompted sentence.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-380"
  },
  "imaizumi94_icslp": {
   "authors": [
    [
     "Satoshi",
     "Imaizumi"
    ],
    [
     "Hartono",
     "Abdoerrachman"
    ],
    [
     "Seiji.",
     "Niimi"
    ]
   ],
   "title": "Controllability of voice quality: evidence from physiological and acoustic observations",
   "original": "i94_1467",
   "page_count": 4,
   "order": 381,
   "p1": "1467",
   "pn": "1470",
   "abstract": [
    "\"Controllability\" is defined as the ability to produce desirable pitch, intensity and timbre according to the speaker's intention. As one component of the \"controllability\", the ability to keep the vocal fundamental frequency, F0, and intensity as constant and as close to a target as possible when instructed to produce a sustained vowel was tested. Using an object oriented acoustic analysis system, magnitude of the slow and fast fluctuations in F0, fractal characteristics and some other voice-quality-related parameters were analyzed for singing, normal/modal and pathological voices. All the pathological groups showed larger variations in F0, or lower controllability, than the normal controls. The ability to keep vocal F0 and intensity as constant as possible was dependent on the target conditions which speakers intended to produce particularly for the neurological disorder patients. These results suggest that vocal controllability can be assessed quantitatively by the method proposed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-381"
  },
  "krom94_icslp": {
   "authors": [
    [
     "Guus de",
     "Krom"
    ]
   ],
   "title": "Spectral correlates of breathiness and roughness for different types of vowel fragments",
   "original": "i94_1471",
   "page_count": 4,
   "order": 382,
   "p1": "1471",
   "pn": "1474",
   "abstract": [
    "Breathiness and roughness ratings were related to a number of spectral parameters, including, among others, the relative peak level of the first harmonic, Harmonics-to-Noise Ratios (HNR) in selected frequency bands, and level differences between these frequency bands. Analyses were performed for 200 ms vowel onset fragments, 200 ms midvowel (post-onset) fragments, and 1000 ms fragments covering both the onset and post-onset parts of a vowel. HNR in the main energy frequency band was the best single predictor of both breathiness and roughness, explaining up to 55% of the variance. A combination of predictors explained 70% of the breathiness variance for all three types of fragments. For the roughness data, the same combination of predictors explained most of the variance in vowel onset fragments (61%), and least in post-onset fragments (35%). Thus, the onset seems to contain more acoustic information relevant to the perception of roughness than the mid-vowel fragment.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-382"
  },
  "esling94_icslp": {
   "authors": [
    [
     "John H.",
     "Esling"
    ],
    [
     "Lynn Marie",
     "Heap"
    ],
    [
     "Roy C.",
     "Snell"
    ],
    [
     "B. Craig",
     "Dickson"
    ]
   ],
   "title": "Analysis of pitch dependence of pharyngeal, faucal, and larynx-height voice quality settings",
   "original": "i94_1475",
   "page_count": 4,
   "order": 383,
   "p1": "1475",
   "pn": "1478",
   "abstract": [
    "To examine the possibility that pharyngeal voice quality settings are not independent of pitch differences or of their effect on each other, pharyngealized voice, faucalized voice, raised-larynx voice and lowered-larynx voice were produced under controlled phonetic conditions at eight separate, incremental pitch intervals and analyzed using auditory, spectrographic, and vowel-formant tracking. Results suggest an interdependent relationship between raised-larynx voice and pharyngealization, and between lowered-larynx voice and faucalization. These relationships are realized at several pitch increments, and may reflect some pitch dependence; that is, pharyngealized voice may mask raised-larynx voice at low frequencies, and faucalization may mask lowered-larynx voice auditorily at high frequencies. At the highest frequencies (falsetto range), pharyngealized and faucalized voices, and to some extent lowered-larynx voice, cannot be distinguished. A close front vowel is differentiated the most consistently, while a close back vowel is most often undifferentiated for pharyngeal quality. To explain these relationships, it is posited that for raised-larynx voice at certain (low) frequencies, the vocal tract is configured in a manner that is the same as for pharyngealization; and that for faucalization at certain (low) frequencies, the vocal tract is configured in a manner that is the same as for lowered-larynx voice.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-383"
  },
  "na94_icslp": {
   "authors": [
    [
     "KyungMin",
     "Na"
    ],
    [
     "JaeYeol",
     "Rheem"
    ],
    [
     "SouGuil",
     "Ann"
    ]
   ],
   "title": "Minimum-error-rate training of predictive neural network models",
   "original": "i94_1479",
   "page_count": 4,
   "order": 384,
   "p1": "1479",
   "pn": "1482",
   "abstract": [
    "Recently, predictive neural network models (PNNM) have proven successful in various speech recognition tasks. But, they suffer from poor discrimination for acoustically similar speech signals. In this paper, a new discriminative training algorithm based on the minimum-error-rate decision rule is proposed. Experiments on the Korean digits recognition have shown 37.5 % reduction of the number of recognition errors.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-384"
  },
  "gorin94b_icslp": {
   "authors": [
    [
     "Allen L.",
     "Gorin"
    ],
    [
     "H.",
     "Hanek"
    ],
    [
     "R. C.",
     "Rose"
    ],
    [
     "L.",
     "Miller"
    ]
   ],
   "title": "Spoken language acquisition for automated call routing",
   "original": "i94_1483",
   "page_count": 4,
   "order": 385,
   "p1": "1483",
   "pn": "1486",
   "abstract": [
    "We investigate the task of Automated Call Routing in a telephone network. The goal is that when a person desires some service, they should proceed by dialing a single universal number, which prompts them with Hello, how may I help you? The person then responds to this prompt via unconstrained fluent speech, on which basis the call is automatically routed to an appropriate destination. We report on an on-line experimental system which explores the feasibility of this concept, comprising an information-theoretic connectionist network embedded in a feedback control system. Experimental results are reported for a database of recorded customer/operator dialogs.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-385"
  },
  "ambikairajah94_icslp": {
   "authors": [
    [
     "Eliathamby",
     "Ambikairajah"
    ],
    [
     "Owen",
     "Friel"
    ],
    [
     "William",
     "Millar"
    ]
   ],
   "title": "A speech recognition system using both auditory and afferent pathway signal processing",
   "original": "i94_1487",
   "page_count": 4,
   "order": 386,
   "p1": "1487",
   "pn": "1490",
   "abstract": [
    "This paper describes an implementation of a speech recognition system using a neural network, originally proposed by Kurogi [1], which is based on findings from the auditory mechanism, including both auditory signal processing and afferent pathway signal processing. A design procedure for a multi-rate cochlear model is described. The model uses a cascade of constant-Q filters. The model covers the bandwidth from 75Hz to 3.55kHz and is used as a pre-processor to the neural network. The neural network reproduces response patterns typical in the auditory system. The network does not use any time-warping and makes no use of the back-propagation algorithm for training. Preliminary results are presented for speaker independent speech recognition experiments carried out using a subset of the British Telecom Sl-Connex database.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-386"
  },
  "renals94_icslp": {
   "authors": [
    [
     "Steve J.",
     "Renals"
    ],
    [
     "Mike M.",
     "Hochberg"
    ]
   ],
   "title": "Using gamma filters to model temporal dependencies in speech",
   "original": "i94_1491",
   "page_count": 4,
   "order": 387,
   "p1": "1491",
   "pn": "1494",
   "abstract": [
    "Hybrid systems which use connectionist networks to estimate the output probabilities of a hidden Markov model represent time both at the network level and the Markov chain level. In this paper we discuss modelling time in connectionist networks, and introduce local recurrences in a feed-forward network in the form of an adaptive gamma filter. Using the Resource Management (RM) database, we have performed continuous speech recognition experiments comparing a gamma filtered input representation to a delay line. We have also performed speaker adaptation experiments using the speaker-dependent RM database. Our results have not indicated that gamma filters offer an appreciable modelling advantage on this task. However, the baseline speaker adaptation experiments have indicated that supervised adaptation over 100 sentences reduced the word error by an average of 40%.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-387"
  },
  "verhasselt94_icslp": {
   "authors": [
    [
     "Jan",
     "Verhasselt"
    ],
    [
     "Jean-Pierre",
     "Martens"
    ]
   ],
   "title": "Phone recognition using a transition-controlled, segment-based dp/mlp hybrid",
   "original": "i94_1495",
   "page_count": 4,
   "order": 388,
   "p1": "1495",
   "pn": "1498",
   "abstract": [
    "In this paper, we explore how 'phone transition' models can be efficiently integrated into a segment-based continuous speech recognizer using Multi Layer Perceptrons. In this transition-controlled recognition strategy, the acoustic continuum of speech is modeled as a sequence of transitions between phones, but the advantages of segment-based modeling are retained by performing a kind of non-linear interpolation with context-independent phone probabilities. Experiments show that the transition-based phone recognizer significantly outperforms the context-independent one.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-388"
  },
  "hochberg94_icslp": {
   "authors": [
    [
     "Mike M.",
     "Hochberg"
    ],
    [
     "Steve J.",
     "Renals"
    ],
    [
     "A. J.",
     "Robinson"
    ],
    [
     "D. J.",
     "Kershaw"
    ]
   ],
   "title": "Large vocabulary continuous speech recognition using a hybrid connectionist-HMM system",
   "original": "i94_1499",
   "page_count": 4,
   "order": 389,
   "p1": "1499",
   "pn": "1502",
   "abstract": [
    "Abbot is a hybrid connectionist-hidden Markov model (HMM) system for large vocabulary speech recognition which participated in the November 1993 ARPA Wall Street Journal benchmark tests. This system uses a recurrent network to estimate the acoustic observation probabilities within the HMM framework. Since the 1993 benchmark tests, a number of improvements have been made to the ABBOT system. These improvements have been gained through better phone-duration modeling and connectionist model combination. In addition, ABBOT has been extended to handle large vocabulary tasks with a trigram language model. Fast decoding is obtained using a pruning strategy particularly well-suited for the hybrid approach. This paper describes the recent modifications to the system and experimental results are reported for various test and development sets from the November 1993 ARPA evaluations.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-389"
  },
  "yu94_icslp": {
   "authors": [
    [
     "Dong",
     "Yu"
    ],
    [
     "Taiyi",
     "Huang"
    ],
    [
     "Dao Wen",
     "Chen"
    ]
   ],
   "title": "A multi-state NN/HMM hybrid method for high performance speech recognition",
   "original": "i94_1503",
   "page_count": 3,
   "order": 390,
   "p1": "1503",
   "pn": "1506",
   "abstract": [
    "This paper proposed a novel Multi-State NN/HMM Hybrid Method named Multi-State Gaussion Competitive Neural Network (MSGCNN) for High Performance Speech Recognition. The basic idea of this new approach is integrating the Viterbi algorithm into a Gaussion Competitive Neural Network (GCNN). Comparing with Self-Aligning Networkp] it is more successful for it has both the Time alignment ability of HMM and the strong discrimination capability of Neural Network (NN). Moreover, because GCNN has better performance than basic Multilayer Perceptron (MLP), the novel system has many strong points such as faster training, more robust and noise immunity. An all Chinese syllable recognition system have been established based on MSGCNN and the comparative experiments confirmed the good characters stated above.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-390"
  },
  "gurgen94_icslp": {
   "authors": [
    [
     "Fikret S.",
     "Gurgen"
    ],
    [
     "J. M.",
     "Song"
    ],
    [
     "R. W.",
     "King"
    ]
   ],
   "title": "A continuous HMM based preprocessor for modular speech recognition neural networks",
   "original": "i94_1507",
   "page_count": 4,
   "order": 391,
   "p1": "1507",
   "pn": "1510",
   "abstract": [
    "This paper uses a speech recognition method which combines a continuous density hidden Markov model (CDHMM)-based preprocessor with window-based neural network (WNN) architectures. The method also employs modularity of NNs. It removes fixed-sized input constraint of NN and improves recognition performance. The CDHMM preprocessor performs a priori Gaussian shaping and normalization using statistically modelled state vectors in contrast to simple distance metric between acoustic vectors in dynamic time warping (DTW) preprocessor. Then, normalized and a priori Gaussian shaped speech features are applied as input to WNN and modular WNN architectures. NIST TI-46 E-set experiments are performed and the results are compared with a baseline CDHMM results. The proposed system improves the recognition performance. Modular WNN provides further significant improvement on the performance.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-391"
  },
  "cheng94_icslp": {
   "authors": [
    [
     "Ying",
     "Cheng"
    ],
    [
     "Paul",
     "Fortier"
    ],
    [
     "Yves",
     "Normandin"
    ]
   ],
   "title": "System integrating connectionist and ibolic approaches for spoken language understanding",
   "original": "i94_1511",
   "page_count": 4,
   "order": 392,
   "p1": "1511",
   "pn": "1514",
   "abstract": [
    "In this paper, we describe a system which integrates connectionist and symbolic approaches (SICSA) for spoken language understanding. This system combines two connectionist parsers with a rule-based parser. The design of SICSA emphasizes portability issues within the domain of database interrogation. In SICSA, the meaning of a sentence is translated into a pragmatic representation which usually has two components: the first specifies the types of informations that must be presented to the user; the other specifies the constraints. SICSA is currently applied to the ATIS (Air Travel Information System) task?. Keywords: connectionist network, parser, natural language understanding.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-392"
  },
  "menendezpidal94_icslp": {
   "authors": [
    [
     "Xavier",
     "Menendez-Pidal"
    ],
    [
     "Javier",
     "Ferreiros"
    ],
    [
     "Ricardo de",
     "Cordoba"
    ],
    [
     "Josi M.",
     "Pardo"
    ]
   ],
   "title": "Recent work in hybrid neural networks and HMM systems in CSR tasks",
   "original": "i94_1515",
   "page_count": 4,
   "order": 393,
   "p1": "1515",
   "pn": "1518",
   "abstract": [
    "In this paper, we describe new hybrid combinations of neural networks with HMMs. These strategies, based on the inclusion of a probabilistic modelling of the ANN, make the hybrid system more independent from the neural network behavior. Using these methodologies, the hybrid system can now benefit from two distinct sources of improvement such as neural modelling and HMM modelling.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-393"
  },
  "mari94b_icslp": {
   "authors": [
    [
     "Jean-Frangois",
     "Mari"
    ],
    [
     "Dominique",
     "Fohr"
    ],
    [
     "Yolande",
     "Anglade"
    ],
    [
     "Jean-Claude",
     "Junqua"
    ]
   ],
   "title": "Hidden Markov models and selectively trained neural networks for connected confusable word recognition",
   "original": "i94_1519",
   "page_count": 4,
   "order": 394,
   "p1": "1519",
   "pn": "1522",
   "abstract": [
    "This paper presents a new method for connected-word recognition with confusable vocabularies, such as connected letters. The recognition process is performed in two steps. First, a second-order HMM provides N-best word strings. Then, the strings of confusable letters are discriminated by a procedure based on acoustic knowledge and artificial neural networks (ANN). This method has been tested on an American-English database containing spelled names collected through the telephone network. The results obtained with the first HMM pass and the improvements made with the ANN are presented and discussed. When a 3,300 name dictionary and a retrieval procedure based on a DTW alignment algorithm were used, 96% recognition accuracv was obtained.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-394"
  },
  "konig94_icslp": {
   "authors": [
    [
     "Yochai",
     "Konig"
    ],
    [
     "Nelson",
     "Morgan"
    ]
   ],
   "title": "Modeling dynamics in connectionist speech recognition - the time index model",
   "original": "i94_1523",
   "page_count": 4,
   "order": 395,
   "p1": "1523",
   "pn": "1526",
   "abstract": [
    "We are experimenting with an approach to connectionist speech recognition that models the dynamics within a speech segment using temporal position as an explicit variable. Currently, the most common model for human speech production that is used in speech recognition is the Hidden Markov Model (HMM). However, HMMs suffer from well known limitations; most notably, the assumption that the observations generated in a given state are independent and identically distributed (i.i.d.). As an alternative, we are developing a time index model that explicitly conditions the emission probability of a state on the time index, where time index is defined as the number of frames since entering a state till the current frame. Thus, the proposed model does not require the i.i.d. assumption. Our pilot results suggest that the time-index approach can greatly reduce error if we have good information about the phoneme boundary location.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-395"
  },
  "chen94b_icslp": {
   "authors": [
    [
     "Dao Wen",
     "Chen"
    ],
    [
     "Xiao Dong",
     "Li"
    ],
    [
     "San",
     "Zhu"
    ],
    [
     "Dongxin",
     "Xu"
    ],
    [
     "Taiyi",
     "Huang"
    ]
   ],
   "title": "Mandarin syllables recognition by subsyllables dynamic neural network",
   "original": "i94_1527",
   "page_count": 4,
   "order": 396,
   "p1": "1527",
   "pn": "1530",
   "abstract": [
    "This paper studied Mandarin syllables recognition dynamic neural network called state detector neural network (SDNN), which had minimum number of state detectors. We described the speech signal with a sequence of states and each state statistically characterized a period of speech vector. In each state detector which constituted the dynamic neural network, input vector nonlinearly approximated to a desired vector that was based on a forcing learning algorithm that has the merit of less training epoch and higher convergence accuracy. We compare this model to three other neural network (NN) models including the model of NN/HMM-based warping on the same database. Experiments show that our method has higher recognition accuracy than the others.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-396"
  },
  "okawa94_icslp": {
   "authors": [
    [
     "Shigeki",
     "Okawa"
    ],
    [
     "Christoph",
     "Windheuser"
    ],
    [
     "Frédéric",
     "Bimbot"
    ],
    [
     "Katsuhiko",
     "Shirai"
    ]
   ],
   "title": "Evaluation of phonetic feature recognition with a time-delay neural network",
   "original": "i94_1531",
   "page_count": 4,
   "order": 397,
   "p1": "1531",
   "pn": "1534",
   "abstract": [
    "In this paper we describe our experiments to evaluate the performance of a Time-Delay Neural Network recognizing binary phonetic features. We show that the error is dependent on the number of occurrence of the features in the test set and therefore must be normalized by the frequencies of the features. To get a more objective measure of the network performance, we propose the normalized mutual information calculated between the targets and the network outputs and we show that these two measures are equivalent. By evaluating the mutual information we can compare the different error rates of the features and we show that the network is a good classificator for the features with an error rate between 1% and 10%. Furthermore we observe, that the phonetic features which describe the kind of articulation are easier to recognize by the network than the features which describe the place of articulation.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-397"
  },
  "monte94_icslp": {
   "authors": [
    [
     "Enric",
     "Monte"
    ],
    [
     "Javier",
     "Hernando"
    ]
   ],
   "title": "A self organizing feature map based on the fisher discriminant",
   "original": "i94_1535",
   "page_count": 3,
   "order": 398,
   "p1": "1535",
   "pn": "1538",
   "abstract": [
    "In this paper we propose a modification of the Self Organising map, where we is train the units using a transformation related to the Fisher Discriminant, in order to maximise the discrimination between units. The algorithm integrates the recursion for updating the Self Organising Maps with the recursion for updating the discrimination matrix.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-398"
  },
  "favero94_icslp": {
   "authors": [
    [
     "Richard R.",
     "Favero"
    ],
    [
     "Fikret S.",
     "Gurgen"
    ]
   ],
   "title": "Using wavelet dyadic grids and neural networks for speech recognition",
   "original": "i94_1539",
   "page_count": 4,
   "order": 399,
   "p1": "1539",
   "pn": "1542",
   "abstract": [
    "This paper describes two multi-rate feature vectors derived from the wavelet transform coefficients for speech recognition. The feature vectors ensure time and frequency alignment across the dyadic grid. The first strategy to compose the feature vector is based on grouping by location in time. This produces frame synchronous data that can be applied to a recogniser without the addition of interpolated points on the dyadic grid. The second strategy is to compose groups of vectors according to frequency region and the sampling rate of each region. Then, the feature vectors are applied to a window based neural network (WNN) to assess speech recognition performance. The WNNs are designed to enhance the resolution of various frequency bands to improve speech recognition performance. Experiments are performed using the words /b,d,g/. The results show that the performance of the WNN using this wavelet based feature vector is comparable to that of the HMM based system reported in [3,4].\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-399"
  },
  "hattori94_icslp": {
   "authors": [
    [
     "Hiroaki",
     "Hattori"
    ]
   ],
   "title": "A normalization method of prediction error for neural networks",
   "original": "i94_1543",
   "page_count": 4,
   "order": 400,
   "p1": "1543",
   "pn": "1546",
   "abstract": [
    "This paper proposes a new prediction error normalization method for speaker verification using predictive neural networks. Predictive neural networks non-linearly predict a frame of an input from the preceding several frames and compute a prediction error; The error strongly depends on a particular input and the goodness of the fit is difficult to determine by comparing the value with a fixed threshold. We propose a normalization method which uses the prediction error obtained by a network trained for multiple speakers as a measurement of predictability of an input. The algorithm was evaluated in text-independent speaker verification. Without normalization, an equal error rate of 41.2% was achieved for 12 male speaker verification, using the normalization, the equal error rate was improved drastically to 1.5%. The proposed algorithm is also applicable to other speech processing areas which involve comparison with a threshold such as word spotting and rejection of unknown words.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-400"
  },
  "cerf94_icslp": {
   "authors": [
    [
     "Philippe Le",
     "Cerf"
    ],
    [
     "Dirk Van",
     "Compernolle"
    ]
   ],
   "title": "Recurrent neural network word models for small vocabulary speech recognition",
   "original": "i94_1547",
   "page_count": 4,
   "order": 401,
   "p1": "1547",
   "pn": "1550",
   "abstract": [
    "In this paper, we describe the use of recurrent Multilayer Perceptrons (MLP's) as state probability estimators for word models. The advantage over conventional word Hidden Markov Models (HMM's) is the ease of discriminative training of the models. We find that a minimal state duration is useful. The results on a telephone quality, speaker independent digit recognition task compare favorably with the results of the approach presented by us earlier this year (Le Cerf et al [4]).\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-401"
  },
  "koto94_icslp": {
   "authors": [
    [
     "Yoshinaga",
     "Koto"
    ],
    [
     "Shigeru",
     "Katagiri"
    ]
   ],
   "title": "A novel fuzzy partition model architecture for classifying dynamic patterns",
   "original": "i94_1551",
   "page_count": 4,
   "order": 402,
   "p1": "1551",
   "pn": "1554",
   "abstract": [
    "In speech pattern recognition, there is a clear need to appropriately model the dynamics (variable durational nature) of pattern. This paper discusses a novel neural network solution to this requirement by proposing State-Transition Fuzzy Partition Model (STFPM). STFPM uses an HMM-like state transition structure, of which each state corresponds to one FPM network. The proposed network accordingly inherits all the advantages, such as a fast training and a robust decision, from the original FPM. Evaluations in speaker-dependent phoneme classification tasks clearly demonstrate the utility of this new network classifier.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-402"
  },
  "cooke94_icslp": {
   "authors": [
    [
     "Martin",
     "Cooke"
    ],
    [
     "Phil",
     "Green"
    ],
    [
     "Malcolm",
     "Crawford"
    ]
   ],
   "title": "Handling missing data in speech recognition",
   "original": "i94_1555",
   "page_count": 4,
   "order": 403,
   "p1": "1555",
   "pn": "1558",
   "abstract": [
    "In this paper, we propose a new paradigm for robust ASR based on auditory scene analysis. In previous work, we have shown how models of auditory processing and grouping principles can be used to separate the evidence for a speech signal from arbitrary intrusions. However, this evidence will generally be incomplete since some spectro-temporal regions will be dominated by the other sources. Here, we address the problem of recognising such 'occluded' speech. Two investigations are reported: the first applies unsupervised learning and subsequent recognition to spectral vectors with missing components. The second adapts the Viterbi algorithm for HMM-based ASR to the occluded speech case. Both techniques are encouragingly robust: for instance, more than half of the observation vector can be obscured without appreciable deterioration in recognition performance. Additionally, our demonstration that it is possible to learn to recognise speech from partial information suggests a model for the formation of auditory-phonetic representations by infants in natural (i.e. cluttered) acoustic environments.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-403"
  },
  "haffner94_icslp": {
   "authors": [
    [
     "Patrick",
     "Haffner"
    ]
   ],
   "title": "A new probabilistic framework for connectionist time alignment",
   "original": "i94_1559",
   "page_count": 4,
   "order": 404,
   "p1": "1559",
   "pn": "1562",
   "abstract": [
    "To build optimally effective word classifiers, one research direction in speech recognition is to train a connectionist architecture with a gradient back-propagation procedure that minimises the word error rate directly. The first step was the integration of the DTW alignment procedure into the architecture: the Multi-State Time Delay Neural Network (MS-TDNN[6]) architecture was successfully demonstrated on several large speech recognition tasks. In this paper, we provide an HMM probabilistic framework for the alignment procedure, with improved experimental results. Moreover, applying a unified HMM/connectionist formalation to global speech recognition systems suggests ways to exchange expertise between both fields.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-404"
  },
  "iso94_icslp": {
   "authors": [
    [
     "Ken-ichi",
     "Iso"
    ]
   ],
   "title": "A speech recognition model using internal degrees of freedom",
   "original": "i94_1563",
   "page_count": 4,
   "order": 405,
   "p1": "1563",
   "pn": "1566",
   "abstract": [
    "We present a speech recognition model using continuous internal degrees of freedom (IDF) between acoustic observation and phonetic units. Trajectories in the internal space are assumed to play an important role in the synthetic transformations from phonetic transcriptions to observations. The phonetic transcriptions define a series of target points for the trajectory. Moreover, the trajectories are constrained to be as continuous as possible. As a result, it is shown that the model represents the long term correlation in the observation, that is not found in HMMs. We also examine a discrete model approximating the internal space as a set of finite number of representative points. The resultant model is based on two HMMs mutually coupled. One is a discrete HMM (HMM-I) for a phonetic unit that outputs a trajectory in the internal space. The other is a shared ergodic HMM (HMM-II) with states labeled by the output symbols of HMM-I. The HMM-II implies the acoustic constraints in the observation and accounts for the interactions between neighboring phones through its state transitions.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-405"
  },
  "xu94_icslp": {
   "authors": [
    [
     "Dongxin",
     "Xu"
    ],
    [
     "Dao Wen",
     "Chen"
    ],
    [
     "Qian",
     "Ma"
    ],
    [
     "Bo",
     "Xu"
    ],
    [
     "Taiyi",
     "Huang"
    ]
   ],
   "title": "Adaptation of neural network model: comparison of multilayer perceptron and LVQ",
   "original": "i94_1567",
   "page_count": 4,
   "order": 406,
   "p1": "1567",
   "pn": "1570",
   "abstract": [
    "This paper explores the adaptation features of Multilayer Perceptron and LVQ3 under the speech phoneme recognition experiment. The result shows that Multilayer Perceptron can adapt to novel data easily but may lose the high performance on old data, and by contrast LVQ3 can maintain what it has learnt in the past but is hard to adjust itself to fit the novel data* if a relative great difference lies between the old and novel data. In order to gain a better adaptation algorithm, some modifications are added to LVQ3 and better results are achieved under the same experiment. A method to add in new code vectors for LVQ3 is also examined and a better result is also achieved. Finally, Some problems of the proposed algorithms are discussed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-406"
  },
  "koizumi94_icslp": {
   "authors": [
    [
     "Takuya",
     "Koizumi"
    ],
    [
     "Shuji",
     "Taniguchi"
    ],
    [
     "Ken-ichi",
     "Hattori"
    ],
    [
     "Mikio",
     "Mori"
    ]
   ],
   "title": "Simplified sub-neural-networks for accurate phoneme recognition",
   "original": "i94_1571",
   "page_count": 4,
   "order": 407,
   "p1": "1571",
   "pn": "1574",
   "abstract": [
    "This paper deals with a new phoneme recognition system comprising a set of a simplified version of the sub-neural network and the hidden Markov models (HMMs). The ability of this system has been investigated by a phoneme recognition experiment using a number of Japanese words uttered by a native male speaker in a quiet environment. The result of the experiment shows that recognition rates achieved with this system is higher than those attained with the LVQ-HMM system which is known to be one of the most reliable phoneme recognizers.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-407"
  },
  "rodellar94b_icslp": {
   "authors": [
    [
     "Victoria",
     "Rodellar"
    ],
    [
     "Victor",
     "Nieto"
    ],
    [
     "Pedro",
     "Gomez"
    ],
    [
     "Daniel",
     "Martinez"
    ],
    [
     "M. Mercedes",
     "Perez"
    ]
   ],
   "title": "A neural network for phonetically decoding the speech trace",
   "original": "i94_1575",
   "page_count": 4,
   "order": 408,
   "p1": "1575",
   "pn": "1578",
   "abstract": [
    "A Time-Delay Neural Network to decode Speech Traces into Phoneme Codes in Real time is presented. Such network associates traces of LPC Spectral Parameters with strings of 8-bit Phonetic Codes from a Phonetic Encoding Scheme, which is also presented. The LPC Adaptive Parameter Extractor and the Time-Delay Neural Network may be easily integrated under a common recursive loop in the Time Domain, allowing a fast implementation in VLSI with small memory requirements, which convert this hybrid structure in a powerful Phoneme Spotter, to be used in Speech Recognition or in the Assesment of the Speech Trace. A set of experiments is presented to show the ability of the Neural Network to determine the most significant elements in the LPC vectors from several speakers.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-408"
  },
  "aikawa94_icslp": {
   "authors": [
    [
     "Kiyoaki",
     "Aikawa"
    ],
    [
     "Tsuyoshi",
     "Saito"
    ]
   ],
   "title": "Noise robust speech recognition using a dynamic-cepstrum",
   "original": "i94_1579",
   "page_count": 4,
   "order": 409,
   "p1": "1579",
   "pn": "1582",
   "abstract": [
    "Noise robust speech recognition is achieved using a dynamic-cepstrum. The dynamic-cepstrum is a new spectral representation incorporating time-frequency forward masking. The time-frequency masking can suppress the spectral components commonly included in the current spectrum and in the preceding spectra. This feature suggests the applicability of the dynamic-cepstrum to noisy speech recognition. Speaker-dependent and speaker-independent phoneme recognition experiments are conducted using hidden Markov models. Experimental results demonstrate that the dynamic-cepstrum outperforms the conventional cepstrum on robustness against stationary noise and amplitude-modulated noise. The dynamic-cepstrum is also found to be superior to the conventional cepstrum combined with a delta-cepstrum.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-409"
  },
  "aritsuka94_icslp": {
   "authors": [
    [
     "Toshiyuki",
     "Aritsuka"
    ],
    [
     "Yoshito",
     "Nejime"
    ]
   ],
   "title": "Telephone-band speech enhancement based on the fundamental frequency component compensation",
   "original": "i94_1583",
   "page_count": 4,
   "order": 410,
   "p1": "1583",
   "pn": "1586",
   "abstract": [
    "To raise the audibility of telephone speech for elderly listeners, a telephone speech enhancement method is proposed. The method is based on the compensation of a fundamental frequency (F0) component eliminated or attenuated in the telephone speech. A function of F0 width enhances the F0 component in the spectrum of each F0 estimated frame. In this paper, the enhancement procedure was applied to the speech whose pass- band was restricted to the telephone bandwidth. Sixteen elderly listeners of ordinary hearing levels assessed the enhanced sentence speech quality by the method of paired comparisons. The results confirmed that the enhanced speech was preferred more than untreated telephone-band speech when presented at normal conversational level.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-410"
  },
  "kunieda94_icslp": {
   "authors": [
    [
     "Nobuyuki",
     "Kunieda"
    ],
    [
     "Tetsuya",
     "Shimamura"
    ],
    [
     "Jouji",
     "Suzuki"
    ],
    [
     "Hiroyuki",
     "Yashima"
    ]
   ],
   "title": "Reduction of noise level by SPAD (speech processing system by use of auto-difference function)",
   "original": "i94_1587",
   "page_count": 4,
   "order": 411,
   "p1": "1587",
   "pn": "1590",
   "abstract": [
    "This paper proposes a new speech enhancement system named SPAD (Speech Processing system by use of Auto-Difference function). SPAD is a non-parametric speech enhancement system and a modified version of SPAC(Speech Processing system by use of Autocorrelation function). The algorithm of SPAD is very simple, but SPAD can reduce white noise and pink noise as large as 19 dB (at accumulation time is 20 ms). Results of computer simulation and hearing tests reveal that SPAD can reduce noise level as same as SPAC does.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-411"
  },
  "yoshida94_icslp": {
   "authors": [
    [
     "Yuki",
     "Yoshida"
    ],
    [
     "Masanobu",
     "Abe"
    ]
   ],
   "title": "An algorithm to reconstruct wideband speech from narrowband speech based on codebook mapping",
   "original": "i94_1591",
   "page_count": 4,
   "order": 412,
   "p1": "1591",
   "pn": "1594",
   "abstract": [
    "This paper proposes a new algorithm to reconstruct wideband speech from its narrow version. The algorithm has two novel points. The first is spectrum envelope reconstruction based on codebook mapping. The other is speech signal reconstruction using the reconstructed spectrum envelope. Because the algorithm makes it possible to generate high quality speech without the use of any additional transmitted information, it is applicable for any network, such as the existing telephone network, networks supporting analog and ISDN services, and so on. The algorithm is applied to 20 speakers. Evaluation by the acoustic distance measure and by listening tests confirms the good performance of the algorithm.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-412"
  },
  "seymour94_icslp": {
   "authors": [
    [
     "C. W.",
     "Seymour"
    ],
    [
     "M.",
     "Niranjan"
    ]
   ],
   "title": "An hmm-based cepstral-domain speech enhancement system",
   "original": "i94_1595",
   "page_count": 4,
   "order": 413,
   "p1": "1595",
   "pn": "1598",
   "abstract": [
    "This paper describes a method of enhancing speech corrupted by additive uncorrelated noise. The approach adopted is to use cepstral-domain hidden Markov models to determine statistics of the clean speech and noise processes. A compensated model of speech corrupted by noise is generated using parallel model combination. MMSE and linear non-homogeneous estimators of the clean speech signal are derived. The enhancement system gives natural-sounding speech without the artifacts introduced by systems such as spectral subtraction. HMM recognition tests performed on the enhanced speech using the NOISEX-92 database show a significant reduction in error rate.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-413"
  },
  "iwahashi94_icslp": {
   "authors": [
    [
     "Naoto",
     "Iwahashi"
    ],
    [
     "Yoshinori",
     "Sagisaka"
    ]
   ],
   "title": "Voice adaptation using multi-functional transformation with weighting by radial basis function networks",
   "original": "i94_1599",
   "page_count": 4,
   "order": 414,
   "p1": "1599",
   "pn": "1602",
   "abstract": [
    "This paper describes a spectral transformation method for voice conversion, using multiple linear functions with weighting by Radial Basis Function (RBF) networks. Spectral transformation by speaker interpolation with single linear function is the method which can obtain a moderate mapping by using a small amount of training data. However, even if larger amounts of data could be used, a more precise mapping can not be obtained. To cope with this, multiple linear functions with weighting are used. The weight value is decided by a weighting function represented by RBF networks. Parameters of both the linear functions and the weighting function are simultaneously adapted. The reduction rate of the spectral distance from the generated spectrum to the target speaker, compared with the distance from the interpolated speaker closest to the target, was calculated. It was shown that while the distance reduction rate was about 42 % using the single linear function, the rate increased to 48 % using the multi-functional transformation, which includes two linear functions.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-414"
  },
  "tang94_icslp": {
   "authors": [
    [
     "Hong",
     "Tang"
    ],
    [
     "Xiaoyuan",
     "Zhu"
    ],
    [
     "Iain",
     "Macleod"
    ],
    [
     "Bruce",
     "Millar"
    ],
    [
     "Michael",
     "Wagner"
    ]
   ],
   "title": "A dynamic-window weighted-RMS averaging filter applied to speaker identification",
   "original": "i94_1603",
   "page_count": 4,
   "order": 415,
   "p1": "1603",
   "pn": "1606",
   "abstract": [
    "Based on concepts which are known to work well with 2-D images, this paper explores the application of a statistical noise reduction method to improving the robustness of speaker identification against background noise. A dynamic-window weighted-rms averaging criterion is used to detect noise and preserve the speech quality. Various forms of noise from the NOISEX-92 database were added to digitised speech to test the enhancement properties of our methods. Experimental results show that the proposed filter leads to improved speaker identification performance, even with certain of our supportedly \"clean\" speech data sets which were contaminated by high-level background noise. Keywords: Noise Reduction, Speaker Identification.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-415"
  },
  "yasukawa94_icslp": {
   "authors": [
    [
     "Hiroshi",
     "Yasukawa"
    ]
   ],
   "title": "Quality enhancement of band limited speech by filtering and multirate techniques",
   "original": "i94_1607",
   "page_count": 4,
   "order": 416,
   "p1": "1607",
   "pn": "1610",
   "abstract": [
    "This paper describes the quality enhancement of band limited speech signals. In speech transmission the quality of the received speech signals is degraded by severe band limitation. We propose a spectrum widening method that utilizes aliasing effects in sampling rate conversion and digital filtering for spectrum shaping. Implementation procedures are clarified, and its performance is discussed. Furthermore, a subjective evaluation is obtained. It is shown that the proposed method effectively enhances speech quality.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-416"
  },
  "le94_icslp": {
   "authors": [
    [
     "T. T.",
     "Le"
    ],
    [
     "J. S.",
     "Mason"
    ],
    [
     "T.",
     "Kitamura"
    ]
   ],
   "title": "Characteristics of multi-layer perceptron models in enhancing degraded speech",
   "original": "i94_1611",
   "page_count": 4,
   "order": 417,
   "p1": "1611",
   "pn": "1614",
   "abstract": [
    "A multi-layer perceptron (MLP) acting directly in the time-domain is applied as a speech signal enhancer, and the performance examined in the context of three common classes of degradation, namely non-linear system degradation (introduced by a low-bit rate CELP coder), additive Gaussian white noise, and convolution by a linear system. The investigation focuses on two topics: (i) net topology, comparing single and multiple output structures, and (ii) the influence of non-linearities within the net. Experimental results confirm the importance of matching the enhancer to the class of degradation. In the case of the CELP coder the standard MLP with its inherently non-linear characteristics is consistently better than any equivalent linear structure. In contrast, when the degradation is from additive noise, a linear enhancer is always superior. Interestingly in both cases nets with multiple outputs give significantly better performance than single-output structures.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-417"
  },
  "fineberg94_icslp": {
   "authors": [
    [
     "Adam B.",
     "Fineberg"
    ],
    [
     "Kevin C.",
     "Yu"
    ]
   ],
   "title": "A time-frequency analysis technique for speech recognition signal processing",
   "original": "i94_1615",
   "page_count": 4,
   "order": 418,
   "p1": "1615",
   "pn": "1618",
   "abstract": [
    "The most widely used technique to estimate the time-frequency representation of a discrete speech signal is the spectrogram. A technique based upon Cohen's class of generalized time frequency representations (TFR) is proposed herein and a technique for using this representation in a speech recognition system is described. The kernel design considerations used for analyzing speech signals and their rationale are detailed. Several well-known kernel functions as well as several novel kernel functions are studied. The TFR based coefficients are used to train and test the Apple Computer PlainTalk (TM) speech recognition system. A significant reduction in both the sentence and word error rates are shown.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-418"
  },
  "alku94_icslp": {
   "authors": [
    [
     "Paavo",
     "Alku"
    ],
    [
     "Erkki",
     "Vilkman"
    ]
   ],
   "title": "Estimation of the glottal pulseform based on discrete all-pole modeling",
   "original": "i94_1619",
   "page_count": 4,
   "order": 419,
   "p1": "1619",
   "pn": "1622",
   "abstract": [
    "A new inverse filtering technique for the estimation of the glottal excitation is presented in this study. The method is based on the application of a new algorithm, called Discrete All-pole Modeling (DAP), used in the estimation of the vocal tract transfer function. The paper presents results that were obtained from natural and synthetic vowels of different phonation types and fundamental frequencies. It was found that estimated glottal waveforms comprised less formant ripple when the inverse filtering analysis was performed with the DAP-technique instead of conventional linear prediction. Glottal waveforms computed from natural female voices were also characterized by longer glottal closed phases when the DAP-technique was used.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-419"
  },
  "nishi94_icslp": {
   "authors": [
    [
     "H.",
     "Nishi"
    ],
    [
     "M.",
     "Kitai"
    ]
   ],
   "title": "Analysis and detection of double talk in telephone dialogs",
   "original": "i94_1623",
   "page_count": 4,
   "order": 420,
   "p1": "1623",
   "pn": "1626",
   "abstract": [
    "This paper proposes a new double talk detection method for improving the man-machine interface of speech dialog systems. Echo cancelers are well known to be useful in detecting double talk. However, in order to use an echo canceler effectively, correct learning data which contain original speech data and echo speech data with-out double talk, are necessary. If other speech data is included in the echo speech data, correct learning is obstructed. In order to distinguish between correct learning data and incorrect data, the difference in the logarithmic power values of the output speech and the input speech, Q, is introduced. Firstly, the two main parameters are defined. They are the attenuation factor (a) from the system out-put to the system input through the hybrid circuit and the attenuation factor (/3) from the telephone line to the system input through the hybrid circuit. Secondly, the formula of Q in each state is described using a\\ and /3. Then the discrimination probability functions are introduced using the difference of Q distributions in each state. Thirdly, the experiment and the results of Q distributions in each state are described. Finally, the evaluation results of the new method are shown. The method can select learning speech data for the echo canceler learning at 94 % accuracy from a 3-second speech input.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-420"
  },
  "andersen94_icslp": {
   "authors": [
    [
     "Ove",
     "Andersen"
    ],
    [
     "Paul",
     "Dalsgaard"
    ]
   ],
   "title": "A self-learning approach to transcription of danish proper names",
   "original": "i94_1627",
   "page_count": 4,
   "order": 421,
   "p1": "1627",
   "pn": "1630",
   "abstract": [
    "This paper addresses the development of a Self-Learning system for Grapheme to Phoneme conversion, dubbed SELEGRAPH, which is applied for transcribing ordinary words and proper names. The system learns the conversion from graphemes-to-phonemes during a training session in which a large number of pairs of grapheme strings and their corresponding manually verified phonemic transcription strings are presented to the system. The two main components of the SELEGRAPH software system are the Viterbi module and the grapheme-to-phoneme conversion module. During training the Viterbi module is used to align corresponding strings of grapheme and phoneme pairs by inserting \"nulls\" into the strings. The information given by the set of aligned pairs is stored in a tree structure during training of the conversion module. An evaluation is carried out using three independent databases, one English and two Danish, containing ordinary words as well as proper names. The use of these databases allows for conclusions to be drawn on testing the relative complexity of transcribing ordinary English and Danish words and selected categories of Danish proper names. The best phoneme transcription results obtained are 87.5% for the NETtalk data, 94,9% for Ordinary Danish and 92.0% for Danish family names.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-421"
  },
  "horita94_icslp": {
   "authors": [
    [
     "Eisuke",
     "Horita"
    ],
    [
     "Yoshikazu",
     "Miyanaga"
    ],
    [
     "Koji",
     "Tochinai"
    ]
   ],
   "title": "A time-varying analysis based on analytic speech signals",
   "original": "i94_1631",
   "page_count": 4,
   "order": 422,
   "p1": "1631",
   "pn": "1634",
   "abstract": [
    "This paper proposes an adaptive spectral estimation method for analytic speech signals. The method decreases the calculation errors of finite precision which happen in a method estimating real coefficients. A pre-processing is applied to compensate the discontinuity of a spectrum on decimated analytic signals. Furthermore, the tracking ability of the method estimating complex coefficients is discussed. Its speed is approximately as fast as a conventional method. It is shown from the results of experiments that the proposed method estimates more accurate spectra of speech signals than conventional adaptive methods estimating real coefficients.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-422"
  },
  "endo94_icslp": {
   "authors": [
    [
     "Takashi",
     "Endo"
    ],
    [
     "Shun'ichi",
     "Yajima"
    ]
   ],
   "title": "New spectrum interpolation method for improving quality of synthesized speech",
   "original": "i94_1635",
   "page_count": 4,
   "order": 423,
   "p1": "1635",
   "pn": "1638",
   "abstract": [
    "This paper proposed a new spectrum interpolation method for improving quality of synthesized speech. This method named SSFL(Spectrum Sliding on Formant Loci) overcomes one of the most serious problems in the OLA(Overlapping Addition) method that the continuity of formant loci at a transition part between phonemes is not guaranteed. This discontinuity of formant loci decreases quality of synthesized speech. In the SSFL method, waves are transformed into spectra to interpolate in the spectrum domain and then spectra interpolated are transformed into waves to produce synthesized speech. We have evaluated the proposed SSFL method by synthesizing continuous Japanese speech /aiueo/. The evaluation results have confirmed that the proposed SSFL improved the quality of synthesized speech because it guaranteed the continuity of formant loci.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-423"
  },
  "johnson94_icslp": {
   "authors": [
    [
     "Mark",
     "Johnson"
    ]
   ],
   "title": "Automatic context-sensitive measurement of the acoustic correlates of distinctive features at landmarks",
   "original": "i94_1639",
   "page_count": 4,
   "order": 424,
   "p1": "1639",
   "pn": "1642",
   "abstract": [
    "This paper models speech recognition as the estimation of distinctive feature values at articulatory landmarks [8]. Toward this end, we propose modeling each distinctive feature as a table containing phonetic contexts, a list of signal measurements (acoustic correlates) which provide information about the feature in each context, and, for each context, a statistical model for evaluating the feature given the measurements. The model of a distinctive feature may include several sets of acoustic correlates, each indexed by a different set of context features. Context features are typically lower-level features of the same segment, e.g. manner features ([continuant, sonorant]) provide context for the identification of articulator-bound features ([lips, blade]). The acoustic correlates of a feature can be any static or dynamic spectral measurements defined relative to the time of the landmark. The statistical model is a simple N-dimensional Gaussian hypothesis test. A measurement program has been developed to test the usefulness of user-defined acoustic correlates in user-defined phonetic contexts. Measures of voice onset time and formant locus classification are presented as examples.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-424"
  },
  "soquet94_icslp": {
   "authors": [
    [
     "Alain",
     "Soquet"
    ],
    [
     "Marco",
     "Saerens"
    ]
   ],
   "title": "A comparison of different acoustic and articulatory representations for the determination of place of articulation of plosives",
   "original": "i94_1643",
   "page_count": 4,
   "order": 425,
   "p1": "1643",
   "pn": "1646",
   "abstract": [
    "A problem of long-standing interest in speech processing concerns the most appropriate representation for recognition purposes. The objective of this paper is to compare 6 acoustic and 4 articulatory representations in a task of determination of place of articulation of inter-vocalic plosives. Place of articulation recognition results were obtained based on linear discriminant analysis with the \"jackknife\" method in which the tokens from each individual are successively removed from the training set, and used as a test set. Systematic comparisons were performed under 3 different sets of conditions depending on whether or not the information about the end of the transition, the transition and the stable part of the vowel are integrated. The LPC cepstrum and two articulatory representations (DRM and Maeda's model) achieved the best recognition rate (86%). However, the 2 articulatory representations appeared to be more stable in terms of inter-speaker variability. The performances of the 7 others representations were found to be significantly lower (74% for formants, and 60% for LPC area).\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-425"
  },
  "osaka94b_icslp": {
   "authors": [
    [
     "Naotoshi",
     "Osaka"
    ]
   ],
   "title": "An analysis of voice quality using sinusoidal model",
   "original": "i94_1647",
   "page_count": 4,
   "order": 426,
   "p1": "1647",
   "pn": "1650",
   "abstract": [
    "Voice quality control technology is useful for flexible speech synthesis systems, such as expressive speech synthesis and voice individuality control. So far, most voice quality analysis has focused on spectral envelope. However, voiced sounds consist of several harmonics with time varying magnitudes and frequencies. This paper analizes voice quality by focusing on the harmonics of actual voiced speech. Five Japanese vowels uttered by two female speakers were examined. Harmonic extraction used a sinusoidal model in which M & Q algorithm was used to estimate frequency domain trajectory for each harmonic. Two voice quality related results were acqired. First, the increased rate of standard deviation of instantaneous frequencies depeneds on speakers. Second, mean magnitude of harmonics in the lower frequencies has more voice quality in comparison with LPC spectral envelopes. These results suggest avenues to improve voice quality control in speech synthesis.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-426"
  },
  "wrench94_icslp": {
   "authors": [
    [
     "Alan A.",
     "Wrench"
    ],
    [
     "M. M.",
     "Watson"
    ],
    [
     "D. S.",
     "Soutar"
    ],
    [
     "A. G.",
     "Robertson"
    ],
    [
     "John",
     "Laver"
    ]
   ],
   "title": "Fast formant estimation of children's speech",
   "original": "i94_1651",
   "page_count": 4,
   "order": 427,
   "p1": "1651",
   "pn": "1654",
   "abstract": [
    "Constrained Multiple Centroid Analysis is a method of form ant estimation already used successfully in the analysis of adult speech. In this paper we examine the performance of this approach when applied to the speech of children. The accuracy of the estimated formant frequency is measured for a range of excitation frequencies. In order to gauge absolute performance, an acoustic analogue of the vocal tract is used. Speed and accuracy of the Constrained Multiple Centroid Analysis are examined in a comparative study with a Linear Prediction based formant estimation algorithm. Results show that Multiple Centroid Analysis provides less bias towards the nearest harmonic than the established method based on Linear Prediction. Furthermore, computational efficiency makes it an attractive alternative to Linear Prediction.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-427"
  },
  "salavedra94_icslp": {
   "authors": [
    [
     "Josep",
     "Salavedra"
    ],
    [
     "Enrique",
     "Masgrau"
    ],
    [
     "Asuncidn",
     "Moreno"
    ],
    [
     "Joan",
     "Estarellas"
    ],
    [
     "Javier",
     "Hernando"
    ]
   ],
   "title": "Some fast higher order AR estimation techniques applied to parametric wiener filtering",
   "original": "i94_1655",
   "page_count": 4,
   "order": 428,
   "p1": "1655",
   "pn": "1658",
   "abstract": [
    "Some Speech Enhancement algorithms based on the iterative Wiener filtering Method due to Lim-Oppenheim [2] are presented. In the original Lim-Oppenheim algorithm, speech AR estimation is carried out using classic second-order analysis, but our algorithms consider a more robust AR modelling. Two different strategies of speech AR estimation are presented and both estimators are tiying to see as less amount of noise as possible. First one uses a previous One-Sided Autocorrelation computation, that is a pole-preserving function, and the actual SNR in the second-order LPC analysis is increased. Second one combines advantages of Higher-Order Statistics [1] with a linear combination of AR coefficients, belonging to two consecutive overlapped frames, to assess a less disturbed speech estimation.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-428"
  },
  "yamaguchi94b_icslp": {
   "authors": [
    [
     "Mikio",
     "Yamaguchi"
    ],
    [
     "Shigeharu",
     "Toyoda"
    ],
    [
     "Katsuhiro",
     "Yada"
    ]
   ],
   "title": "Applications of a rule-based speech synthesizer module",
   "original": "i94_1659",
   "page_count": 4,
   "order": 429,
   "p1": "1659",
   "pn": "1662",
   "abstract": [
    "Units and boards of text-to-speech synthesis (or rule-based speech synthesis) have been commercially available for many years. In order to explore new application fields, the author developed a module of Japanese rule-based speech synthesis in 1990, and improved the module in 1993. This paper discusses the new applications concerning the module. Using the module, applications of portable equipment or in moving vehicles can be realized. Important points of specific applications are also reported in this paper. Assuming small scale in-house applications, a general purpose portable speech terminal using the module has been developed on trial. The terminal is equipped with rule-based speech synthesis, spoken word recognition and wireless data transmission. The terminal can be used for picking, inspection and so on. Size of the terminal is 156x92x46 mm (excluding antenna), and weight is 585 grams. The headset which has a speaker and a microphone are used to communicate with the user. Consequently, the fact that such a speech terminal is made portable indicates and encourages new fields of application.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-429"
  },
  "iles94_icslp": {
   "authors": [
    [
     "Jon",
     "Iles"
    ],
    [
     "William",
     "Edmondson"
    ]
   ],
   "title": "Quasi-articulatory formant synthesis",
   "original": "i94_1663",
   "page_count": 4,
   "order": 430,
   "p1": "1663",
   "pn": "1666",
   "abstract": [
    "This paper describes work leading from discussion of hybrid articulatory/formant synthesis [3,4]. We briefly outline the problems that we consider are facing both formant and articulatory synthesis techniques, and how we overcome these using a hybrid solution. Construction of a prototype hybrid model is discussed, the intelligibility of the resulting synthetic speech, and the benefits of this type of approach are examined.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-430"
  },
  "kvale94_icslp": {
   "authors": [
    [
     "Knut",
     "Kvale"
    ]
   ],
   "title": "On the connection between manual segmentation conventions and \"errors\" made by automatic segmentation",
   "original": "i94_1667",
   "page_count": 4,
   "order": 431,
   "p1": "1667",
   "pn": "1670",
   "abstract": [
    "Segmentation of speech signals into non-overlapping, directly following phoneme-sized segments is considered. An automatic segmentation algorithm is applied on several different languages and then compared with the corresponding manual segmentations of the same speech material. This investigation shows that many of the suggestions made by the automatic segmentation algorithm can be argued for being phonetically as \"correct\" as the original manual segmentation. Thus, the automatically placed segment boundaries were acceptable although they were reported as errors by an automatic evaluation program.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-431"
  },
  "tomokiyo94_icslp": {
   "authors": [
    [
     "Mutsuko",
     "Tomokiyo"
    ]
   ],
   "title": "Natural utterance segmentation and discourse label assignment",
   "original": "i94_1671",
   "page_count": 4,
   "order": 432,
   "p1": "1671",
   "pn": "1674",
   "abstract": [
    "This paper proposes an approach based on pragmatics of spontaneously-spoken Japanese dialogue. Input to the system presented here appears as an unbroken stream of \"utterance\" i.e, a sequence of clauses or clause fragments separated by particles and other transitional elements, uttered by one speaker. To cope with data of this sort, a three-step procedure is employed. The aim of the present paper is motivate and discribe step 1 and 2 of this three-step process. As step 1, the input stream is automatically segmented into discourse units and in step 2, discourse labels are assigned. Test results of 1742 utterances show that the procedures presented here effected discourse segmentation correctly on average of 95.32% of the time, and they assigned discourse labels correctly on average of 85.12% of the time.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-432"
  },
  "yumoto94_icslp": {
   "authors": [
    [
     "Satoshi",
     "Yumoto"
    ],
    [
     "Jouji",
     "Suzuki"
    ],
    [
     "Tetsuya",
     "Shimamura"
    ]
   ],
   "title": "Possibility of speech synthesis by common voice source",
   "original": "i94_1675",
   "page_count": 4,
   "order": 433,
   "p1": "1675",
   "pn": "1678",
   "abstract": [
    "In order to improve naturalness of synthesized speech, this paper intend to utilize common residual wave as voice source of Japanese typical sentences which have the same intonation or accent pattern. An analysis-synthesis with 14 coefficients are employed in this experiment. The synthetic experiments are conducted on some words. At first, a residual wave extracted from a word, are used as excitation source waves for other words. Secondly, fundamental frequency and intensity extracted by the original words are used to generate source waves. This synthesized sounds are less noisy than aforesaid sounds. In the last, fundamental frequency is generated by Fujisaki-model. This paper suggests that there are some possibilities to use common voice source for synthesis.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-433"
  },
  "wang94b_icslp": {
   "authors": [
    [
     "Changfu",
     "Wang"
    ],
    [
     "Wenshen",
     "Yue"
    ],
    [
     "Keikichi",
     "Hirose"
    ],
    [
     "Hiroya",
     "Fujisaki"
    ]
   ],
   "title": "A scheme for Chinese speech synthesis by rule based on pitch-synchronous multi-pulse excitation LP method",
   "original": "i94_1679",
   "page_count": 4,
   "order": 434,
   "p1": "1679",
   "pn": "1682",
   "abstract": [
    "A pitch-synchronous analysis and synthesis method has been developed for Chinese speech synthesis by rule. This method is based on the multi-pulse excitation linear predictive (MELP) analysis. The wavelet transform is adopted to detect the instant of glottal closure (GCI) from the speech signal. Since the analysis is carried out pitch-synchronously within the interval of a pitch period bounded by two adjacent GCI's for a voiced speech segment, the major excitation occurring at a GCI is excluded from the analysis interval. Thus higher accuracy is achieved for vocal tract and excitation source parameters than that obtainable by conventional fixed frame methods. Since, in the synthesis process, the speech segments of one pitch period each are used as the units of synthesis for voiced speech, the pitch-synchronous scheme also simplifies the process of control parameter generation and increases the flexibility and controllability of the synthesizer.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-434"
  },
  "lindstrom94_icslp": {
   "authors": [
    [
     "Anders",
     "Lindstrom"
    ],
    [
     "Mats",
     "Ljungqvist"
    ]
   ],
   "title": "Text processing within a speech synthesis system",
   "original": "i94_1683",
   "page_count": 4,
   "order": 435,
   "p1": "1683",
   "pn": "1686",
   "abstract": [
    "The first stage of text-to-speech (tts) conversion involves analyzing the text to determine the correct pronunciation of the individual words or word groups. Building upon a previously devised modular architecture for text processing within the context of a TTS system, we describe recent development in each of the areas \"Word Pronunciation\" and \"Text Analysis\", and describe the different modules involved. We describe how the tokeniser interacts with the lexicon in handling word groups (collocations), abbreviations and typing case. We also describe and give further references to on-going work, where the output of a probabilistic part-of-speech tagger will be used as input to a prosodic phrasing algorithm.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-435"
  },
  "carvalho94_icslp": {
   "authors": [
    [
     "P.",
     "Carvalho"
    ],
    [
     "P.",
     "Lopes"
    ],
    [
     "Isabel",
     "Trancoso"
    ],
    [
     "Luis C.",
     "Oliveira"
    ]
   ],
   "title": "E-mail to voice-mail conversion using a portuguese text-to-speech system",
   "original": "i94_1687",
   "page_count": 4,
   "order": 436,
   "p1": "1687",
   "pn": "1690",
   "abstract": [
    "This paper describes the application of a text-to-speech system to electronic mail, effectively converting its messages into speech. E-mail to voice-mail conversion is a complex process involving the development of special applications such as e-mail filtering and the integration of orthographic correction and language identification.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-436"
  },
  "kitazawa94_icslp": {
   "authors": [
    [
     "Shigeyoshi",
     "Kitazawa"
    ],
    [
     "Satoshi",
     "Kobayashi"
    ],
    [
     "Takao",
     "Matsunaga"
    ],
    [
     "Hideya",
     "Ichikawa"
    ]
   ],
   "title": "Tempo estimation by wave envelope for recognition of paralinguistic features in spontaneous speech",
   "original": "i94_1691",
   "page_count": 4,
   "order": 437,
   "p1": "1691",
   "pn": "1694",
   "abstract": [
    "We analyze speech rate through an envelope extraction process. The process is low-pass filtering of rectified speech wave to eliminate ripples caused from pitch and vocal resonances. Speech wave is amplitude modulated about 8 mora/sec. Dips of the envelope correspond to consonants or phonemic boundaries, therefore dips within a unit time is correlated with the rate of speech. We measured the rate of speech from an interviewing between a female interviewer and a male interviewee. Speech data analysed consists of 7 utterances of the man and 6 utterances of the lady with durations of 2 to 7 seconds. Same utterances were labeled manually for locations of individual phonemes. Manually computed rate excluding pauses is faster than averaged one. By DFT of the envelope, a frequency component of the rate of speech is avilable and have shown to be correlated with the manual rate at the coefficient of 0.57.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-437"
  },
  "tsushima94_icslp": {
   "authors": [
    [
     "Teruaki",
     "Tsushima"
    ],
    [
     "Osamu",
     "Takizawa"
    ],
    [
     "Midori",
     "Sasaki"
    ],
    [
     "Satoshi",
     "Shiraki"
    ],
    [
     "Kanae",
     "Nishi"
    ],
    [
     "Morio",
     "Kohno"
    ],
    [
     "Paula",
     "Menyuk"
    ],
    [
     "Catherine",
     "Best"
    ]
   ],
   "title": "Discrimination of English /r-l/ and /w-y/ by Japanese infants at 6-12 months: language-specific developmental changes in speech perception abilities",
   "original": "i94_1695",
   "page_count": 4,
   "order": 438,
   "p1": "1695",
   "pn": "1698",
   "abstract": [
    "In this study, we investigated language-specific developmental changes in Japanese infants' perceptual discrimination of English approximants, /r-l/ and /w-y/ at 6-12 months. It was found that both /r-l/ and /w-y/ were discriminated at 6-8 months. At 10-12 months, however, /w-y/ was discriminated, whereas /r-l/ was not. The present findings support the following conclusions; 1) a developmental decline in discrimination of non-native approximants occurs toward the end of the first year, consistent with earlier reports on non-native stop consonant contrasts; 2) phonemic factors are crucial in accounting for the observed decline, given that discrimination of a native approximant contrast remains good throughout the first year.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-438"
  },
  "kojima94_icslp": {
   "authors": [
    [
     "Hiroaki",
     "Kojima"
    ],
    [
     "Kazuyo",
     "Tanaka"
    ],
    [
     "Satoru",
     "Hayamizu"
    ]
   ],
   "title": "Generating phoneme models for forming phonological concepts",
   "original": "i94_1699",
   "page_count": 4,
   "order": 439,
   "p1": "1699",
   "pn": "1702",
   "abstract": [
    "The aim of this work is to improve speech recognition performance by forming \"phonological concepts\". In order to improve speech recognition performance, the phoneme models or phone models of the system need to satisfy following two properties: l)preciseness and 2)robustness. These two properties usually trade off against each other in traditional stochastic models. In order to satisfy the both simultaneously, we propose to simulates situations of a human infant learning a language. We call this \"phonological concept formation\", which is a task of acquiring knowledge of phonological system from spoken word samples without using any transcriptions except for the identification of each word in a lexicon. This knowledge includes what is the set of the whole phonemes, how the acoustic phonetic features of each phoneme are described, and how they are appropriately discriminated. The basis of this idea is specifying essential situations of speech communication instead of providing all encompassing universal samples of a spoken language. Based on this framework, this paper discusses about the method for generating both precise and robust models.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-439"
  },
  "shimura94_icslp": {
   "authors": [
    [
     "Yoko",
     "Shimura"
    ],
    [
     "Satoshi",
     "Imaizumi"
    ]
   ],
   "title": "Infant's expression and perception of emotion through vocalizations",
   "original": "i94_1703",
   "page_count": 4,
   "order": 440,
   "p1": "1703",
   "pn": "1706",
   "abstract": [
    "This paper examined two hypotheses; (1) infants produce voices necessary for emotional communication; and (2) Infants interpret emotions contained in their own vocalizations. Voice samples produced by six infants at 6, 9, 12 and 17 months of age were perceptually evaluated by 79 adults (aged 20-22) and 31 children (aged 2-6) using rating scales representing emotions. The following results were obtained. 1) Even six month old infants who have not yet developed a language could produce various voices necessary for emotional communication through nonlinguistic aspects of voice. 2) Both the adult and child listeners perceived rich contents of emotions from the voice samples recorded even at 6 month of age. 3) More factors were needed to account for the rating scores given by the children than by the adults via factor analyses. 4) Children seem to be more sensitive in perception of emotions from infants' vocalization than the adults. These results support the hypothesis that \"infants begin to communicate through nonlinguistic aspects of voice at very early stage of their life.\"\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-440"
  },
  "ito94_icslp": {
   "authors": [
    [
     "Tomohiko",
     "Ito"
    ]
   ],
   "title": "Transition from two-word to multiple-word stage in the course of language acquisition",
   "original": "i94_1707",
   "page_count": 4,
   "order": 441,
   "p1": "1707",
   "pn": "1710",
   "abstract": [
    "This is a study of the problem of the transition from the two-word stage to the multiple-word stage in the course of language acquisition. The longitudinal speech data of six normal Japanese children were examined. The results showed a close relationship between the state of the knowledge of the projection of lexical category V and N and the appearance of multiple-word utterances. We proposed the hypothesis that it is the complete realization of the knowledge of the projection of lexical category V and N that enables children to break through the constraint on the length of utterances and makes them produce multiple-word utterances.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-441"
  },
  "rao94_icslp": {
   "authors": [
    [
     "P. V. S.",
     "Rao"
    ],
    [
     "Nandini",
     "Bondale"
    ]
   ],
   "title": "BSLP based language grammars for child speech",
   "original": "i94_1711",
   "page_count": 3,
   "order": 442,
   "p1": "1711",
   "pn": "1714",
   "abstract": [
    "We report in this paper our investigations aimed at evolving an approach to develop a grammar model for the speech of a child acquiring English as the first language. We use the occurrence of words in similar syntac-to-semantic environment as the criterion for classifying words into equivalence classes. Consequently, words in the same class are interchangeable in a phrase or larger context. We identify phrases on the basis of n-gram frequencies of 'class exemplars' in the corpus and their meaningfulness. We group them into a small number of interchangeable phrase types. It turns out that the corpus of 723 sentences consists of 161 distinct phrase type sequence patterns and that of these, just 70 patterns have a frequency more than one and account for as many as 632 sentences in the original corpus. We use an algorithmic procedure to build a 'State Transition Network' which accounts for the sentence patterns. The STN consists of 11 states and generates most of the sentences in the corpus. This indicates that the approach is effective for modeling the grammar of the child.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-442"
  },
  "nienart94_icslp": {
   "authors": [
    [
     "John",
     "Nienart"
    ],
    [
     "J. Devin",
     "McAuley"
    ]
   ],
   "title": "Using prediction to learn pre-linguistic speech characteristics: a connectionist model",
   "original": "i94_1715",
   "page_count": 4,
   "order": 443,
   "p1": "1715",
   "pn": "1718",
   "abstract": [
    "We describe initial results in the development of a connectionist model of pre-linguistic categorization in infant language acquisition. We assume that early listeners are sensitized to salient contour properties of natural sounds. To model this learning process, we trained a recurrent connectionist network to predict its future inputs, under the assumption that rising or falling sounds generate an expectation in the hearer that they will continue in the given direction. The training and testing sets represented rising and falling tone sequences, as found in formant transitions. To learn the task, the network developed attractors for each input and positioned these attractors tonotopically in state space. Principal components analysis of the memory-layer activations showed that this positioning of attractors enabled the network to implicitly categorize sweeps as high or low, and rising or falling.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-443"
  },
  "mochizukisudo94_icslp": {
   "authors": [
    [
     "Michiko",
     "Mochizuki-Sudo"
    ],
    [
     "Shigeru",
     "Kiritani"
    ]
   ],
   "title": "Naturalness judgments for stressed vowel duration in second language acquisition",
   "original": "i94_1719",
   "page_count": 4,
   "order": 444,
   "p1": "1719",
   "pn": "1722",
   "abstract": [
    "This study examines the naturalness threshold of English stressed vowels for Americans and Japanese learners of English. Conversely, the naturalness threshold of Japanese accented vowels for Japanese speakers is compared to that of Japanese for American learners. Perception experiments demonstrated Americans are more sensitive to the shortening of a vowel both in English and Japanese than Japanese. As for the lengthening of a vowel, Japanese learners of English were found to be less tolerant of lengthening in English vowels. Likewise, in Japanese stimuli, the Japanese were less tolerant of lengthening than the Americans when a vowel was followed by a voiced consonant. In Japanese vowels followed by either a voiceless consonant or a flap, the Japanese were conversely more tolerant of lengthening than the Americans. We found certain factors, such as the phonemic vowel length distinction and a voice-conditioned effect, which exerted an influence on these judgments of naturalness.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-444"
  },
  "maeda94_icslp": {
   "authors": [
    [
     "Margaret",
     "Maeda"
    ]
   ],
   "title": "Pre-nuclear intonation in questions of Japanese students in English",
   "original": "i94_1723",
   "page_count": 4,
   "order": 445,
   "p1": "1723",
   "pn": "1726",
   "abstract": [
    "Intonation contours in questions in English of nineteen Japanese students are examined for their shape, placement of prominence and relative speeds of different parts of the contour, concentrating on the part before the nucleus. The students' prosody in English is compared with that of eight native speakers living in Britain, with their own prosody in Japanese, and with the prosody of two native-speaking English teachers in Japan. It is noted that the students prosody shows evidence of native-language transfer, learning strategies and teaching-induced error, and that the teachers' prosody shows acquisition of their students' interlanguage. It is questioned whether native-speaking teachers who acquire their students' interlanguage can be effective as native models for pronunciation.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-445"
  },
  "tsumaki94_icslp": {
   "authors": [
    [
     "Junko",
     "Tsumaki"
    ]
   ],
   "title": "Intonational properties of adverbs in tokyo Japanese",
   "original": "i94_1727",
   "page_count": 4,
   "order": 446,
   "p1": "1727",
   "pn": "1730",
   "abstract": [
    "In the canonical word order for Japanese, adverbial modifiers precede head - constituents. Analysis of fo contours of recorded utterances reveals that constructions with adverbs can be classified into two types: Type 1: adverbs which require focus; the pitch range of the modified predicate is notably compressed and lowered. Type 2: adverbs which do not require focus, rather, the predicate requires focus; the pitch range of the predicate is expanded or reset at a new value.\n",
    "Different prosodic features should be attributed to type 1 and type 2 adverbs.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-446"
  },
  "miura94_icslp": {
   "authors": [
    [
     "Ichiro",
     "Miura"
    ]
   ],
   "title": "Production and perception of English sentences spoken by Japanese university students",
   "original": "i94_1731",
   "page_count": 4,
   "order": 447,
   "p1": "1731",
   "pn": "1734",
   "abstract": [
    "In Experiment I, English sentences pronounced by Japanese university students (both English majors and non-English majors) as well as Americans were analysed acoustically. The results showed that in interrogative and complex sentences, longer duration and/or smaller F0 ranges were observed in Japanese than in American speakers. In Experiment II, perceptual judgment of naturalness was conducted, changing either F0 or duration. The results showed that raising the sentence-final F0 facilitated naturalness whereas lengthening or shortening vowel durations did not.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-447"
  },
  "kikuchi94_icslp": {
   "authors": [
    [
     "Atsuko",
     "Kikuchi"
    ],
    [
     "Wayne",
     "Lawrence"
    ]
   ],
   "title": "Using morphological analysis to improve Japanese pronunciation",
   "original": "i94_1735",
   "page_count": 4,
   "order": 448,
   "p1": "1735",
   "pn": "1738",
   "abstract": [
    "This paper argues that correcting mispronunciations of words in a case-by-case fashion only serves to improve the pronunciation of the one particular form. The paper offers an alternative approach where (i) the morphological make-up of words, (ii) regular processes which apply at morpheme boundaries, and (iii) restrictions on the length of morphemes are taught to prevent common pronunciation mistakes such as *itoo (for ittoo 'one head'), *hokaidoo (for hokkaidoo), and *shupatsu, *shuupatsu, *shuuppatsu (for shuppatsu 'departure').\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-448"
  },
  "nishinuma94_icslp": {
   "authors": [
    [
     "Yukihiro",
     "Nishinuma"
    ]
   ],
   "title": "How do the French perceive tonal accent in Japanese? experimental evidence",
   "original": "i94_1739",
   "page_count": 4,
   "order": 449,
   "p1": "1739",
   "pn": "1742",
   "abstract": [
    "We studied how French subjects learning Japanese perceive tonal accent under different experimental conditions. The perceptual experiment included 3 tests using 3-, 4-, and 5-syllable words with different tonal accents. In Test 1, the stimuli were presented in isolation, in Test 2, words were extracted from short sentences, and in Test 3, the target stimuli were embedded in a carrier sentence with 4 syllables preceding and following the target word. Ten French students participated were asked to detect the \"high-low\" tone change in the words. The average score was 66% for Test 1, 49% for Test 2, and 45% for Test 3. Japanese subjects obtained an average score of 86%. The results suggest that the perception of tonal accent is language-dependent.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-449"
  },
  "morimoto94_icslp": {
   "authors": [
    [
     "Tsuyoshi",
     "Morimoto"
    ],
    [
     "Noriyoshi",
     "Uratani"
    ],
    [
     "Toshiyuki",
     "Takezawa"
    ],
    [
     "Osamu",
     "Furuse"
    ],
    [
     "Yasuhiro",
     "Sobashima"
    ],
    [
     "Hitoshi",
     "Iida"
    ],
    [
     "Atsushi",
     "Nakamura"
    ],
    [
     "Yoshinori",
     "Sagisaka"
    ],
    [
     "Norio",
     "Higuchi"
    ],
    [
     "Yasuhiro",
     "Yamazaki"
    ]
   ],
   "title": "A speech and language database for speech translation research",
   "original": "i94_1791",
   "page_count": 4,
   "order": 450,
   "p1": "1791",
   "pn": "1794",
   "abstract": [
    "This paper describes a new database that we are constructing for speech translation research. A goal of the research is to develop a system capable of handling spontaneous utterances. The database is composed of three parts. The main one is an integrated speech and language database, which this paper mainly focuses. The basic approach for the database and its specific features are described. The other two, a speech database and a language database, supplement the main one. Brief descriptions about them also appear.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-450"
  },
  "lamel94_icslp": {
   "authors": [
    [
     "Lori F.",
     "Lamel"
    ],
    [
     "Florian",
     "Schiel"
    ],
    [
     "Adrian",
     "Fourcin"
    ],
    [
     "Joseph",
     "Mariani"
    ],
    [
     "Hans G.",
     "Tillmann"
    ]
   ],
   "title": "The translanguage English database (TED)",
   "original": "i94_1795",
   "page_count": 4,
   "order": 451,
   "p1": "1795",
   "pn": "1798",
   "abstract": [
    "The Translanguage English Database is a corpus of recordings made of oral presentations at Eurospeech93 in Berlin. The corpus name derives from the high percentage of presentations given in English by non-native speakers of English. 224 oral presentations at the conference were successfully recorded, providing a total of about 75 hours of speech material. These recordings provide a relatively large number of speakers speaking a variant of the same language (English) over a relatively large amount of time (15 min each + 5 min discussion) on a specific topic. A subset of speakers were recorded with a laryngograph in addition to the standard microphone. A set of Polyphone-like recordings were made, for which a subset also had a laryngograph signal recorded. These recordings were made in English and in the speaker's mother language.\n",
    "In addition to the spoken material, associated text materials are being collected. These include written versions of the proceedings papers and any oral preparations texts which were made available. The text materials will provide vocabulary items and data for language modeling. Speakers were also asked to complete a short questionnaire regarding their mother language, any other languages they speak, as well as their knowledge of English.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-451"
  },
  "kudo94_icslp": {
   "authors": [
    [
     "Ikuo",
     "Kudo"
    ],
    [
     "Takao",
     "Nakama"
    ],
    [
     "Nozomi",
     "Arai"
    ],
    [
     "Nahoko",
     "Fujimura"
    ]
   ],
   "title": "The data collection of voice across Japan (VAJ) project",
   "original": "i94_1799",
   "page_count": 4,
   "order": 452,
   "p1": "1799",
   "pn": "1802",
   "abstract": [
    "The Voice Across Japan (VAJ) speech data collection started from September, 1993, and is now in progress. VAJ is a large-scale telephone database to support the creation of continuous, speaker-independent speech recognition systems. The goal of the effort is to obtain speech from 10,000 speakers with broad dialect coverage, then use the database for developing recognition systems ranging from digit recognition system to large vocabulary systems based on phonetic recognition units. In collecting a 10,000 speaker corpus in all Japan through telephone line, both data quality and data sampling become very important problems. In order to improve the problem, this paper describe our method of data collection and reports the current status of our database such as balanced data set. keyword Speech Database, Voice Across Japan (VAJ), Telephone speech, Speaker-independent speech recognition, robustness, Digit recognition, Tri-phone.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-452"
  },
  "damhuis94_icslp": {
   "authors": [
    [
     "M.",
     "Damhuis"
    ],
    [
     "T.",
     "Boogaart"
    ],
    [
     "C. in't",
     "Veld"
    ],
    [
     "M.",
     "Versteijlen"
    ],
    [
     "W.",
     "Schelvis"
    ],
    [
     "L.",
     "Bos"
    ],
    [
     "Louis",
     "Boves"
    ]
   ],
   "title": "Creation and analysis of the dutch polyphone corpus",
   "original": "i94_1803",
   "page_count": 4,
   "order": 453,
   "p1": "1803",
   "pn": "1806",
   "abstract": [
    "In this paper the linguistic design, speaker selection, and the recording and transliteration procedures for the Dutch POLYPHONE corpus are described in some detail. Over 5,000 have been recorded. The paper gives details of the distributions of the speakers according to regional and socio-economic background, sex and age of the speakers. Also, first results of the analysis of the linguistic contents of the recordings are reported.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-453"
  },
  "rosenbeck94_icslp": {
   "authors": [
    [
     "Per",
     "Rosenbeck"
    ],
    [
     "Bo",
     "Baungaard"
    ],
    [
     "Claus",
     "Jacobsen"
    ],
    [
     "Dan-Joe",
     "Barry"
    ]
   ],
   "title": "The design and efficient recording of a 3000 speaker scandinavian telephone speech database: rafael.0",
   "original": "i94_1807",
   "page_count": 4,
   "order": 454,
   "p1": "1807",
   "pn": "1810",
   "abstract": [
    "This paper presents experience from a new efficient concept for establishing a large speech database for Spoken Language research and new Advanced Telephone Services (ATS). This Scandinavian telephone speech database, entitled Rafael.0, contains speech material from 3000 speakers in total, from Denmark, Norway and Sweden. The speakers for Rafael.0 have been carefully selected according to a predetermined distribution based on age, sex and regional language. A number of regional languages, comprising dialects and the standard language, have been defined using linguistic knowledge. The speakers in Rafael.0 have spoken connected digits and whole sentences with emphasis on obtaining realistic and spontaneous telephone speech. A dedicated PC-based recording system with an ISDN interface board connected to a digital exchange, has been developed for this project. The recording system proved to be very efficient and flexible for the recording of the speakers via domestic telephone lines. Labelling of Rafael.0 is done at sentence level and for a specific part also at word level.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-454"
  },
  "tapias94_icslp": {
   "authors": [
    [
     "D.",
     "Tapias"
    ],
    [
     "Alejandro",
     "Acero"
    ],
    [
     "J.",
     "Esteve"
    ],
    [
     "J. C.",
     "Torrecilla"
    ]
   ],
   "title": "The VESTEL telephone speech database",
   "original": "i94_1811",
   "page_count": 4,
   "order": 455,
   "p1": "1811",
   "pn": "1814",
   "abstract": [
    "VESTEL is a telephone speech corpus collected at the Speech Technology Division of Telefonica Investigacidn y Desarrollo. The data base was designed to support research in speaker-independent automatic speech recognition (ASR) based on word and subword units. Over sixteen thousand people called in response to newspaper advertisements. They were prompted by a recorded voice to say digits, numbers and commands, and to answer questions asking them the city where they lived and they were born, their name and surnames, a yes/no question and to spell some words. The utterances were spoken over commercial telephone lines, and each call was composed by twenty five separate utterances. Spain was divided into ten dialectical regions in order to take into account the main Spanish dialects of Castilian (usually known as \"Spanish\"). Each call was checked and transcribed by two people. In this report we describe the system implemented to record the data base, the publicity campaign, the recording protocol, the regions in which Spain was divided, and the statistical information of the tasks that were carried out.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-455"
  },
  "cole94b_icslp": {
   "authors": [
    [
     "Ronald",
     "Cole"
    ],
    [
     "Mark",
     "Fanty"
    ],
    [
     "Mike",
     "Noel"
    ],
    [
     "Terri",
     "Lander"
    ]
   ],
   "title": "Telephone speech corpus development at CSLU1",
   "original": "i94_1815",
   "page_count": 4,
   "order": 456,
   "p1": "1815",
   "pn": "1818",
   "abstract": [
    "This paper describes eight telephone-speech corpora at various stages of development at the Center for Spoken Language Understanding. For each corpus we describe data collection procedures, methods of soliciting callers, protocol used to collect the data, transcriptions that accompany the speech data, and the expected release date. The corpora are (or will be) available at no charge to academic institutions.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-456"
  },
  "kenne94_icslp": {
   "authors": [
    [
     "P. E.",
     "Kenne"
    ],
    [
     "Hamish",
     "Pearcy"
    ],
    [
     "Mary",
     "O'Kane"
    ]
   ],
   "title": "Derivation of a large speech and natural language database through alignment of court recordings an their transcripts",
   "original": "i94_1819",
   "page_count": 4,
   "order": 457,
   "p1": "1819",
   "pn": "1822",
   "abstract": [
    "A major difficulty for both speech recognition systems and natural language systems is the large effort required to port such systems to a new application. Both speech and NL systems require large amounts of training data. The data collection and annotation is generally a labour-intensive activity. All court proceedings in Australia are recorded, and transcripts are produced for over 95% of them. The recordings, together with the transcripts, provide a rich source of data for speech and NL training. The court recordings are examples of spontaneous speech. Training using spontaneous speech (as opposed to read speech) can significantly improve performance for recognising spontaneous speech [1]. A major difficulty in using these data to derive a speech recognition training database is that the transcripts are not in any way time-aligned with the audio data. We describe how we are deriving a large speech recognisor training database from Australian court recordings in a semi-automatic manner through aligning the court recordings and their transcripts using a successive refinement bootstrap procedure which relies particularly on speaker-dependent word-spotting of common words.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-457"
  },
  "lin94c_icslp": {
   "authors": [
    [
     "Qiguang",
     "Lin"
    ],
    [
     "ChiWei",
     "Che"
    ],
    [
     "Joe",
     "French"
    ]
   ],
   "title": "Description of the caip speech corpus",
   "original": "i94_1823",
   "page_count": 4,
   "order": 458,
   "p1": "1823",
   "pn": "1826",
   "abstract": [
    "As part of our effort in developing a synergistically-integrated system of microphone arrays and neural networks (MANN) for robust large-vocabulary continuous speech recognition in variable acoustical environments, we recently collected a sizable amount of \"hands-free\" speech data. This database comprises stereo recordings of read speech using a head-mounted microphone, a desk-mounted microphone, and a 1-dimensional beamforming line array of 29-microphones. Both the desk-mounted microphone and the array were positioned 3 meters from the subject. This speech corpus has been utilized to evaluate the capability of the MANN system for robust speech/speaker recognition under adverse conditions. The purpose of this paper is to document the data collection process and present some results of acoustic analyses of collected data with an emphasis on array speech.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-458"
  },
  "kassel94_icslp": {
   "authors": [
    [
     "Rob",
     "Kassel"
    ]
   ],
   "title": "Automating the design of compact linguistic corpora",
   "original": "i94_1827",
   "page_count": 4,
   "order": 459,
   "p1": "1827",
   "pn": "1830",
   "abstract": [
    "In this paper we address two aspects of linguistic corpus construction. First we examine the process of selecting the units to be covered in our design. Rather than enumerating a set of fixed-length units, we derive variable-length units based on a measure of cohesiveness. Next we consider the selection of material to cover efficiently these, or other, units. Our scoring procedure takes into account frequency distributions to improve the result's compactness. The proposed techniques have been successfully applied to the design of a handwriting corpus at MIT and a speech corpus elsewhere.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-459"
  },
  "tanaka94b_icslp": {
   "authors": [
    [
     "Kazuyo",
     "Tanaka"
    ],
    [
     "Kanae",
     "Kinebuchi"
    ],
    [
     "Naoko",
     "Houra"
    ],
    [
     "Kazuyuki",
     "Takagi"
    ],
    [
     "Shuichi",
     "Itahashi"
    ],
    [
     "Katsunobu",
     "Itou"
    ],
    [
     "Satoru",
     "Hayamizu"
    ]
   ],
   "title": "Annotating illocutionary force types and phonological features into a spontaneous dialogue corpus: an experimental study",
   "original": "i94_1831",
   "page_count": 4,
   "order": 460,
   "p1": "1831",
   "pn": "1834",
   "abstract": [
    "The purpose of this paper is to present an experimental study for constructing an annotated spoken dialogue corpus. Labels annotated on the corpus characterize surface forms and intentions of speaker's utterances. The first half of the paper deals with: (l)defining categories for the labels that represent illocutionary force types(IFTs) and phonological physical features(P/PFs) of utterances, and (2)howIFTs and P/PFs are annotated to a spoken dialogue corpus. In the latter half, the feasibility of and difficulties in carrying out this annotation process are discussed and some statistical data obtained from a spoken dialogue corpus selected from ASJ Continuous Speech Corpus, Volume 7 are shown.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-460"
  },
  "rosenberg94_icslp": {
   "authors": [
    [
     "Aaron E.",
     "Rosenberg"
    ],
    [
     "Chin-Hui",
     "Lee"
    ],
    [
     "Frank K.",
     "Soong"
    ]
   ],
   "title": "Cepstral channel normalization techniques for HMM-based speaker verification",
   "original": "i94_1835",
   "page_count": 4,
   "order": 461,
   "p1": "1835",
   "pn": "1838",
   "abstract": [
    "Mismatched recording and channel conditions for training sessions and verification trials can lead to serious performance degradations for speaker verification systems. The effect of linear channel distortions can be compensated by subtracting the cepstrum attributable to the distortion from the cepstrum of the observed signal. Three cepstral normalization techniques have been studied to evaluate their effect on performance of a speaker verification system with a telephone network database of connected digit password utterances. The three techniques represent cepstral distortion as a long term cepstral average, short term cepstral average, and as a maximum likelihood estimate of the observed cepstrum with respect to HMM parameters. Overall, verification performance improves 30 to 45% with cepstral normalization over a baseline condition. The greater improvements are obtained for longer utterances. No significant differences in performance are found for the three techniques.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-461"
  },
  "raman94_icslp": {
   "authors": [
    [
     "Vijay",
     "Raman"
    ],
    [
     "Jayant M.",
     "Naik"
    ]
   ],
   "title": "Noise reduction for speech recognition and speaker verification in mobile telephony",
   "original": "i94_1839",
   "page_count": 4,
   "order": 462,
   "p1": "1839",
   "pn": "1842",
   "abstract": [
    "Noise classification and reduction is used in this work to improve speech recognition and speaker verification performance for utterances received over the mobile network. Automatic noise classification is achieved with high accuracy in conjunction with spectral subtraction methods. The results audibly improve signal-to-noise ratio and significantly improve recognizer performance in low SNR situations as well as poorly modeled environments. The process is fully automatic, from implementation considerations.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-462"
  },
  "parris94b_icslp": {
   "authors": [
    [
     "Eluned S.",
     "Parris"
    ],
    [
     "Michael J.",
     "Carey"
    ]
   ],
   "title": "Discriminative phonemes for speaker identification",
   "original": "i94_1843",
   "page_count": 4,
   "order": 463,
   "p1": "1843",
   "pn": "1846",
   "abstract": [
    "This paper describes experiments in speaker identification using the discriminatory ability of subword models based on phonemes. Speaker identification is performed using a free match technique where test speech is matched to a combination of speaker independent subword Hidden Markov Models (HMMs) and speaker dependent subword HMMs. Errors occur when the true speaker's phonemes are not consistently matched with the speaker dependent models or when other speakers match to the true speaker's models. Using Bayesian statistics we show that the discriminative power of a phoneme is related to the log likelihood ratio of the frequencies of true speaker to impostor recognitions. Reduction in error rates of 33% have been achieved over a simpler technique for a ten speaker identification task. The phonemes z, i, e, m? v, N, { and s were found to be the most discriminative. More generally front vowels, voiced fricatives and nasals give the best performance.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-463"
  },
  "hernando94_icslp": {
   "authors": [
    [
     "Javier",
     "Hernando"
    ],
    [
     "Climent",
     "Nadeu"
    ],
    [
     "Carlos",
     "Villagrasa"
    ],
    [
     "Enric",
     "Monte"
    ]
   ],
   "title": "Speaker identification in noisy conditions using linear prediction of the one-sided autocorrelation sequence",
   "original": "i94_1847",
   "page_count": 4,
   "order": 464,
   "p1": "1847",
   "pn": "1850",
   "abstract": [
    "The OSALPC (One-Sided Autocorrelation Linear Predictive Coding) representation of the speech signal has shown to be attractive for speech recognition because of its simplicity and its high recognition performance with respect to the standard LPC in severe noisy conditions. In this paper the OSALPC technique is applied to the problem of speaker identification in noisy conditions. As shown with experimental results, using additive white noise, that technique also achieves much better results than both LPC and mel-cepstrum parameterizations in this task.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-464"
  },
  "he94_icslp": {
   "authors": [
    [
     "Jialong",
     "He"
    ],
    [
     "Li",
     "Liu"
    ],
    [
     "Günther",
     "Palm"
    ]
   ],
   "title": "A text-independent speaker identification system based on neural networks",
   "original": "i94_1851",
   "page_count": 4,
   "order": 465,
   "p1": "1851",
   "pn": "1854",
   "abstract": [
    "A text-independent automatic speaker identification system was constructed and evaluated with the TIMIT database. All voiced parts of speech signals were automatically located by measuring the short-term energy of the signals. For each segment of the voiced signals LPC based cepstrum were calculated to compose a feature vector. Multilayer perceptron (MLP) and learning vector quantization (LVQ) networks were used as classifiers. The codebooks of the LVQ classifiers were initialized by the LBG algorithm and then were trained by the LVQ3 algorithm. The MLP classifiers were standard feed forward networks with one hidden layer and were trained in two steps by the conjugate gradient method. Speech data from 112 male speakers in the test subdivision of the TIMIT database were used to evaluate our system. For each speaker, we randomly selected eight sentences as training data and the remaining two as the testing ones. The results showed that the best correct identification rates were 88.4% by LVQ classifiers and 99.1% by MLP classifiers for a population of 112 speakers.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-465"
  },
  "chen94c_icslp": {
   "authors": [
    [
     "Fangxin",
     "Chen"
    ],
    [
     "Bruce",
     "Millar"
    ],
    [
     "Michael",
     "Wagner"
    ]
   ],
   "title": "Hybrid threshold approach in text-independent speaker verification",
   "original": "i94_1855",
   "page_count": 4,
   "order": 466,
   "p1": "1855",
   "pn": "1858",
   "abstract": [
    "In this paper we suggest a hybrid threshold approach for text-independent speaker verification. A maximum distortion (or minimum likelihood) threshold is set for the claimed speaker to perform pre-filtering of the highly dissimilar impostors. Cohort normalisation then is applied to further separate those impostors who are acoustically similar to the claimed speaker. A VQ-distortion based text-independent speaker verification system using this approach achieves better results than the conventional absolute threshold or cohort normalisation methods.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-466"
  },
  "ariki94_icslp": {
   "authors": [
    [
     "Y.",
     "Ariki"
    ],
    [
     "K.",
     "Doi"
    ]
   ],
   "title": "Speaker recognition based on subspace methods",
   "original": "i94_1859",
   "page_count": 4,
   "order": 467,
   "p1": "1859",
   "pn": "1862",
   "abstract": [
    "This paper proposes a method to separate speaker information from phonetic information included in speech data. A speaker recognition method using the separated speaker information is also proposed and shown to be equivalent with a method based on speaker subspace. The validity of this proposed method was verified by earring out simple speaker recognition experiments.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-467"
  },
  "yun94_icslp": {
   "authors": [
    [
     "Seong Jin",
     "Yun"
    ],
    [
     "Yung Hwan",
     "Oh"
    ]
   ],
   "title": "Performance improvement of speaker recognition system for small training data",
   "original": "i94_1863",
   "page_count": 4,
   "order": 468,
   "p1": "1863",
   "pn": "1866",
   "abstract": [
    "Presently many speaker recognition algorithms can provide high accuracy. They need reliable samples of the parameter space, long utterances and large amount of training data for each speaker. In practical applications, data acquisition constraints thus limit their domain of application. This paper proposes a speaker recognition method that creates models to specify speaker information accurately by using only a small amount of training data and very short utterance for each speaker, both for training and recognition. The proposed methods based on a discrete hidden Markov model(HMM) improves modeling of output probability estimation. The basic idea is that the proposed hidden Markov VQ model(HMVQM) uses the state dependent codebook, and each state represents a partition of speaker information. This method can be considered as multisection codebook model with stochastic transitions between section. Speaker identification experiments based on single syllable word tests give a 1.5% error rate for the proposed HMVQM method, whereas the discrete HMM method gives error rate of 24.12%. In this experiment, We use only two training data for each speaker.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-468"
  },
  "yegnanarayana94_icslp": {
   "authors": [
    [
     "B.",
     "Yegnanarayana"
    ],
    [
     "S. P.",
     "Wagh"
    ],
    [
     "S.",
     "Rajendran"
    ]
   ],
   "title": "A speaker verification system using prosodic features",
   "original": "i94_1867",
   "page_count": 4,
   "order": 469,
   "p1": "1867",
   "pn": "1870",
   "abstract": [
    "This paper describes a technique for automatic speaker verification based on prosodic knowledge in Hindi using neural networks. Properties of intonation patterns (changes in F0 as a function of time) and duration were exploited to extract speaker specific information from natural speech utterances, which were used for fixed text speaker verification task. A set of twenty-three features (fifteen pitch features and eight durational features) were extracted from a fixed natural utterance, using a word boundary hypothesization algorithm. A neural network model based on adaptive resonance theory (ART2) was used to verify speaker from the input feature set. The system was trained for twenty five speakers and tested with twenty seven impostors. The results show that the overall percentage of correct acceptance and correct rejection was found to be about 98%.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-469"
  },
  "goldenthal94_icslp": {
   "authors": [
    [
     "William D.",
     "Goldenthal"
    ],
    [
     "James R.",
     "Glass"
    ]
   ],
   "title": "Statistical trajectory models for phonetic recognition",
   "original": "i94_1871",
   "page_count": 4,
   "order": 470,
   "p1": "1871",
   "pn": "1874",
   "abstract": [
    "In this work, we apply statistical trajectory models (STM's) to the task of phonetic recognition. STM's attempt to capture the dynamic characteristics and statistical dependencies of acoustic attributes in a segment-based framework. The approach is based on the creation of a track, fa, for each phonetic unit a. The track serves as a model of the dynamic trajectories of the acoustic attributes over the segment. The statistical framework for scoring incorporates the auto- and cross-correlation properties of the track error over time, within a segment. This paper presents the results of a series of phonetic recognition experiments using the timit acoustic-phonetic corpus [1]. Using the NIST train and core test sets we obtained context-independent and context-dependent recognition accuracies of 64.0% and 69.0% respectively.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-470"
  },
  "blomberg94_icslp": {
   "authors": [
    [
     "Mats",
     "Blomberg"
    ]
   ],
   "title": "A common phone model representation for speech recognition and synthesis",
   "original": "i94_1875",
   "page_count": 4,
   "order": 471,
   "p1": "1875",
   "pn": "1878",
   "abstract": [
    "A combined representation of context-dependent phones at the production parametric and the spectral level is described. The phones are trained in the production domain using analysis-by-synthesis and piece-wise linear approximation of parameter trajectories. For recognition, this representation is transformed to spectral subphones, using a cascade formant synthesis procedure. In a connected-digit recognition task, 99.1% average correct digit rate was achieved in a group of seven male speakers when, for each test speaker, training was done on the other six speakers. Simple rules for male-to-female transformation of the male phone library increased the performance for six female speakers from 88.9% without transformation to 96.3%. In informal listening tests of resynthesised digit strings, the speech has been judged as intelligible, however far from natural.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-471"
  },
  "kadambe94_icslp": {
   "authors": [
    [
     "Shubha",
     "Kadambe"
    ],
    [
     "James L.",
     "Hieronymus"
    ]
   ],
   "title": "Spontaneous speech language identification with a knowledge of linguistics",
   "original": "i94_1879",
   "page_count": 4,
   "order": 472,
   "p1": "1879",
   "pn": "1882",
   "abstract": [
    "A task independent spoken Language Identification (LID) system for telephone speech is described. This system is based on continuos density second-order ergodic variable duration Hidden Markov phoneme models and trigram phonemotactic models. The language specific phoneme models are trained using \"High accuracy phoneme recognition system\" [1]. A trigram phonemotactic model for each language is trained using a text corpus of about 10 million words and a grapheme to phoneme converter. The language Li of an incoming speech signal x is hypothesized as the one that produced the highest likelihood P(x\\fii)P(fii\\Li) for all the phonemic models fit of a given set of phonemes per language. The LID results for three languages are presented. The effect of the phonemotactic model in distinguishing languages is demonstrated by comparing the LID results obtained with and without phonemotactic models.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-472"
  },
  "hazen94_icslp": {
   "authors": [
    [
     "Timothy J.",
     "Hazen"
    ],
    [
     "Victor W.",
     "Zue"
    ]
   ],
   "title": "Recent improvements in an approach to segment-based automatic language identification",
   "original": "i94_1883",
   "page_count": 4,
   "order": 473,
   "p1": "1883",
   "pn": "1886",
   "abstract": [
    "In 1993, a segment-based system for Automatic Language Identification (ALI) was developed and introduced. The system incorporates phonetic, acoustic, and prosodic information within a probabilistic framework. The original system was trained and tested using the OGI Multi-Language Telephone Speech Corpus and achieved an accuracy of 57.3% in identifying the language of test utterances from the OGI corpus. Recent improvements to the system have included the addition of channel normalization during preprocessing, the utilization of the recently transcribed utterances from the OGI corpus for phonetic recognition training, the use of mixture Gaussian density functions for the modeling of prosodic information, and the development of a hill-climbing optimization procedure for determining the scaling factors used when combining the scores from different models. The current system has achieved an accuracy of 79.7% in identifying the language of test utterances.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-473"
  },
  "ramesh94_icslp": {
   "authors": [
    [
     "Padma",
     "Ramesh"
    ],
    [
     "David B.",
     "Roe"
    ]
   ],
   "title": "Language identification with embedded word models",
   "original": "i94_1887",
   "page_count": 4,
   "order": 474,
   "p1": "1887",
   "pn": "1890",
   "abstract": [
    "This paper presents results on acoustic identification of which language is being spoken. Conventional approaches to spoken language identification are aimed at determining the language spoken by any speaker, on any subject, over any transmission channel. Typically, such systems have achieved accuracies around 80% on identification of 10 languages. The new feature of this work is the use of embedded models of frequently occurring words and phrases, in addition to a conventional Hidden Markov Model of the language to be recognized. The experimental results on four languages indicate a substantial improvement in accuracy, especially when one or more of the key words is present in the speech to be identified. For phrases that contain one of the key words that are explicitly modeled, the correct language is identified 93% of the time, with utterances that have an average length of 2 seconds. We also report results on language identification in cross-channel conditions - models of language created over one channel, and test data collected over a different channel. Adaptive background modeling and spectral correction can raise the accuracy in these challenging conditions.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-474"
  },
  "berkling94_icslp": {
   "authors": [
    [
     "Kay M.",
     "Berkling"
    ],
    [
     "Etienne",
     "Barnard"
    ]
   ],
   "title": "Language identification of six languages based on a common set of broad phonemes",
   "original": "i94_1891",
   "page_count": 4,
   "order": 475,
   "p1": "1891",
   "pn": "1894",
   "abstract": [
    "We describe a system designed to recognize the language of an utterance spoken by any native speaker over the telephone. Our previous work based on language-specific phonemes [5] is extended to include sequences of all lengths of language-independent speech units. These units are derived by clustering phonemes across all languages in the system (Hindi, Spanish, English, German, Japanese, and Mandarin). Our language-identification results based on broad-phoneme occurrence statistics indicate 90% accurate distinction between English and Japanese, which is comparable to results obtained when using language-specific phonemes. By relaxing the precision of language-dependent phonemes into language-independent broad phonemes we thus retain language discriminative power. The degree to which the precision can be relaxed while retaining sequences of broad phonemes that can discriminate between languages is an indication of the accuracy with which the phoneme segmenter and recognizer have to recognize the incoming speech.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-475"
  },
  "reyes94_icslp": {
   "authors": [
    [
     "Allan A.",
     "Reyes"
    ],
    [
     "Takashi",
     "Seino"
    ],
    [
     "Seiichi",
     "Nakagawa"
    ]
   ],
   "title": "Three language identification methods based on HMMs",
   "original": "i94_1895",
   "page_count": 4,
   "order": 476,
   "p1": "1895",
   "pn": "1898",
   "abstract": [
    "This paper describes three methods of language identification, all of which are based on HMMs(Hidden Markov Models). Here, we focused mainly on language identification for English and Japanese. In the first method,a fully-structured(ergodic) HMM was trained for each language using text-independent speech samples from 10 native speakers. The likelihood for each language is calculated for the input speech using this HMM. In the second method, a universal ergodic HMM is trained using all the language data and with it, the most likely state sequence is computed for each language. The state sequence derived is processed and is used in the construction of trigram models for each language. The trigram model was used for modeling the phonotactics for each language. For the third method, a set of phonemic/syllabic HMMs are trained: 60 phonemic HMMs for English and 113 syllabic HMMs for Japanese. With this system, the most likely sequence of phonemes/syllables of each language and its likelihood are determined for the speech input. Separate test data were provided. For these test data, the identification for the first method was 96.5% on English-Japanese identification. The second method gave a performance of 85.6%, while for the third method, it was 93.9%. Language identification tests were also performed on a 4-language(English, Japanese, Chinese and Indonesian) database. Combining the first and second methods gave a best performance of 98.4% for utterances lasting 10 seconds.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-476"
  },
  "itahashi94_icslp": {
   "authors": [
    [
     "Shuichi",
     "Itahashi"
    ],
    [
     "Jian Xiong",
     "Zhou"
    ],
    [
     "Kimihito",
     "Tanaka"
    ]
   ],
   "title": "Spoken language discrimination using speech fundamental frequency",
   "original": "i94_1899",
   "page_count": 4,
   "order": 477,
   "p1": "1899",
   "pn": "1902",
   "abstract": [
    "This paper describes classification methods of spoken languages based on fundamental frequency (F0) contours of speech. Speech data were taken from language learning tapes. Six languages including Japanese, Korean, Chinese, English, French and German were used. First, F0 contour was approximated by a set of polygonal lines so that the mean square error between the lines and F0 values was minimized; the optimum boundaries of the lines were determined using a dynamic programming procedure. The starting frequency, slope and duration of each line were calculated. Seventeen parameters including mean values and standard deviations of the above parameters were used for the analysis. Then parameters derived from F0pattern were analyzed by using principal component analysis. We also tried discriminant analysis of these parameters. Results show that the six languages can be classified based on these parameters. Keywords: Slope of F0 contour, Principal Component analysis, Discriminant analysis.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-477"
  },
  "dalsgaard94_icslp": {
   "authors": [
    [
     "Paul",
     "Dalsgaard"
    ],
    [
     "Ove",
     "Andersen"
    ]
   ],
   "title": "Application of inter-language phoneme similarities for language identification",
   "original": "i94_1903",
   "page_count": 4,
   "order": 478,
   "p1": "1903",
   "pn": "1906",
   "abstract": [
    "The results of a language identification experiment employing speech material covering four European languages - Danish, English, German and Italian - are presented and discussed.\n",
    "The methodology applied presupposes that the languages under test have some speech sounds which are similar enough to be equated across these languages. These speech sounds are the cross-language polyphonemes. Contrasting them there exists a set of non-combinable language dependent monophonemes for each of the languages. The poly- and monophonemes are separated on the basis of a combined database covering training speech material from all four languages. In the language identification experiment each of the four languages is modelled by a set of CDHMMs covering the combined set of language-specific monophonemes and the cross-language polyphonemes. The results show an average language identification score of 88.1% covering the range from 77.9% for the lowest to 95.5% for the highest identification score.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-478"
  },
  "hamme94b_icslp": {
   "authors": [
    [
     "Hugo Van",
     "Hamme"
    ],
    [
     "Guido",
     "Gallopyn"
    ],
    [
     "Ludwig",
     "Weynants"
    ],
    [
     "Bart",
     "D'hoore"
    ],
    [
     "Hervé",
     "Bourlard"
    ]
   ],
   "title": "Comparison of acoustic features and robustness tests of a real-time recogniser using a hardware telephone line simulator",
   "original": "i94_1907",
   "page_count": 4,
   "order": 479,
   "p1": "1907",
   "pn": "1910",
   "abstract": [
    "Three feature extraction methods for a recogniser for telephone speech are compared : LPC-cepstra, RASTA-PLP and LPC with cepstral mean subtraction (CMS). Training of the discrete-density HMM's happened on a database collected over the telephone, while recognition was done on TI46-Word, played back via a hardware telephone line emulator. The robustness against realistic variations in the telephone line transfer function and in the signal-to-noise ratio is tested.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-479"
  },
  "okawa94b_icslp": {
   "authors": [
    [
     "Shigeki",
     "Okawa"
    ],
    [
     "Tetsunori",
     "Kobayashi"
    ],
    [
     "Katsuhiko",
     "Shirai"
    ]
   ],
   "title": "Phoneme recognition in various styles of utterance based on mutual information criterion",
   "original": "i94_1911",
   "page_count": 4,
   "order": 480,
   "p1": "1911",
   "pn": "1914",
   "abstract": [
    "This paper discusses a highly reliable phoneme recognition method in various styles of utterance based on mutual information criterion. Mutual information is a good measure to build an effective phoneme dictionary in the process of optimal selection of acoustic features and integration of clusters. Using VQ code sequences organized by the hierarchical clustering method, phonemic likelihoods for ea,ch frame can be calculated. Phoneme recognition is performed with applying phonemic duration and bigram constraints of phonemes. Also, we cover an iterative training mechanism of the phoneme dictionary. The correct rate for phoneme is improved to 90.5% (8.4% insertion, 7.0% deletion) in the speaker independent recognition experiment for the continuous utterance.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-480"
  },
  "hoshimi94_icslp": {
   "authors": [
    [
     "Masakatsu",
     "Hoshimi"
    ],
    [
     "Maki",
     "Yamada"
    ],
    [
     "Katsuyuki",
     "Niyada"
    ]
   ],
   "title": "Speaker independent speech recognition method using phoneme similarity vector",
   "original": "i94_1915",
   "page_count": 4,
   "order": 481,
   "p1": "1915",
   "pn": "1918",
   "abstract": [
    "We are developing a speaker independent speech recognition method using the similarity vectors as feature parameters. In this paper we present the feasibility of the technique in practical usage by downsizing the algorithm and improving the word spotting technique. When tested with 100 Japanese city names in noisy environments, a word recognition rate 95.9% was obtained with reduced memory size and computation amount. We expect that the method can be implemented in a small hardware.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-481"
  },
  "hubener94_icslp": {
   "authors": [
    [
     "Kai",
     "Hübener"
    ],
    [
     "Julie",
     "Carson-Berndsen"
    ]
   ],
   "title": "Phoneme recognition using acoustic events",
   "original": "i94_1919",
   "page_count": 4,
   "order": 482,
   "p1": "1919",
   "pn": "1922",
   "abstract": [
    "This paper presents a new approach to phoneme recognition using nonsequential sub-phoneme units. These units are called acoustic events and are phonologically meaningful as well as recognizable from speech signals.\n",
    "Acoustic events form a phonologically incomplete representation as compared to distinctive features. This problem may partly be overcome by incorporating phonological constraints. Currently, 24 binary events describing manner and place of articulation, vowel quality and voicing are used to recognize all German phonemes. Phoneme recognition in this paradigm consists of two steps: After the acoustic events have been determined from the speech signal, a phonological parser is used to generate syllable and phoneme hypotheses from the event lattice. Results obtained on a speaker-dependent corpus are presented.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-482"
  },
  "mokhtari94_icslp": {
   "authors": [
    [
     "Parham",
     "Mokhtari"
    ],
    [
     "Frantz",
     "Clermont"
    ]
   ],
   "title": "Contributions of selected spectral regions to vowel classification accuracy",
   "original": "i94_1923",
   "page_count": 4,
   "order": 483,
   "p1": "1923",
   "pn": "1926",
   "abstract": [
    "In this paper we describe the results of several vowel classification experiments, which shed light on the relative importance of certain spectral regions for achieving optimum classification accuracy in either a speaker-dependent or a speaker-independent task. The methodology adopted to conduct this investigation is based on a new formulation [3] of the quefrency-weighted cepstral distance measure, which allows specification of any frequency band within the available spectral range. This more flexible approach to distance computation was used to study the behaviour of classification accuracy as a function of increasing spectral range, and to identify spectral regions which predominantly carry either phonetic or speaker-specific information.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-483"
  },
  "nadeu94_icslp": {
   "authors": [
    [
     "Climent",
     "Nadeu"
    ],
    [
     "Biing-Hwang",
     "Juang"
    ]
   ],
   "title": "Filtering of spectral parameters for speech recognition",
   "original": "i94_1927",
   "page_count": 4,
   "order": 484,
   "p1": "1927",
   "pn": "1930",
   "abstract": [
    "The time sequences of speech parameters resulting from current short-time spectral estimators show a tradeoff between estimation error variance and time and frequency resolution. In this paper, we apply frequency analysis and linear filtering to these sequences to gain insights into their limitations and to provide an interpretation framework for several parameter processing techniques proposed in the past. Particularly, the observation of their long-term spectrum reveals the importance of band equalization for improving discrimination in speech recognition. Based on that, we propose a method of filtering the sequences that includes an explicit equalization and incorporates a bandwidth parameter. By using Slepian sequences in the design of the filters, good results were obtained in our preliminary word recognition tests.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-484"
  },
  "arons94_icslp": {
   "authors": [
    [
     "Barry",
     "Arons"
    ]
   ],
   "title": "Pitch-based emphasis detection for segmenting speech recordings",
   "original": "i94_1931",
   "page_count": 4,
   "order": 485,
   "p1": "1931",
   "pn": "1934",
   "abstract": [
    "This paper describes a technique to automatically locate emphasized segments of a speech recording based on pitch. These salient portions can be used in a variety of applications, but were originally designed to be used in an interactive system that enables high-speed skimming and browsing of speech recordings. Previous techniques to detect emphasis have used Hidden Markov Models; emphasized regions in close temporal proximity were found to successfully create useful summaries of the recordings. The new research described herein presents a simpler technique to detect salient segments and summarize a recording without using statistical models that require large amounts of training data. The algorithm adapts to the pitch range of a speaker, then automatically selects the regions of highest pitch activity as a measure of emphasis.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-485"
  },
  "li94_icslp": {
   "authors": [
    [
     "Z.",
     "Li"
    ],
    [
     "P.",
     "Kenny"
    ]
   ],
   "title": "Overlapping phone segments",
   "original": "i94_1935",
   "page_count": 4,
   "order": 486,
   "p1": "1935",
   "pn": "1938",
   "abstract": [
    "We present a new approach to acoustic-phonetic modelling in which the acoustic segments which correspond to the phones in a given phonetic transcription are allowed to overlap with each other. We show how any type of phone segment model including conventional HMMs can be accommodated in this framework by matching transcriptions to data in a phone-synchronous rather than the conventional frame-synchronous manner. We present a new and efficient algorithm which can be used to carry out phone-synchronous scoring with overlapping phone segment models in both training and recognition. We give some preliminary experimental results obtained using conventional HMMs to model overlapping phone segments.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-486"
  },
  "wong94_icslp": {
   "authors": [
    [
     "Maurice K.",
     "Wong"
    ]
   ],
   "title": "Clustering triphones by phonological mapping",
   "original": "i94_1939",
   "page_count": 4,
   "order": 487,
   "p1": "1939",
   "pn": "1942",
   "abstract": [
    "One of the most important issues in large-vocabulary continuous speech recognition is the modeling of subword units. To model context-dependent acoustic-phonetic variations, typically a large number of units such as triphones are used. Given a finite amount of training data, many triphones are underrepresented and remain undertrained or even untrained. This paper proposes an algorithm for mapping underrepresented triphones to adequately represented ones that are phonetically similar. First, all triphones are categorized according to their places and manner of articulation. Each triphone that needs to be mapped is compared to other triphones, and the candidates are ranked according to whether the left contexts and/or the right contexts are in the same phonetic class, as determined by acoustic-phonetic variations due to context. Second, if a good candidate has not been found, each candidate triphone is analyzed as phonological feature vectors, and the ranking of similarity is determined by the dot product of the vectors. The best candidate for mapping is chosen on the basis of phonetic similarity as well as the frequency of occurrence of the candidate triphones. In a recognition test of the Resource Management task, using this phonological mapping reduces the word error rate significantly.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-487"
  },
  "morgan94_icslp": {
   "authors": [
    [
     "Nelson",
     "Morgan"
    ],
    [
     "Herve",
     "Bourlard"
    ],
    [
     "Steven",
     "Greenberg"
    ],
    [
     "Hynek",
     "Hermansky"
    ]
   ],
   "title": "Stochastic perceptual auditory-event-based models for speech recognition",
   "original": "i94_1943",
   "page_count": 4,
   "order": 488,
   "p1": "1943",
   "pn": "1946",
   "abstract": [
    "We have developed a statistical model of speech that incorporates certain temporal properties of human speech perception. The primary goal of this work is to avoid a number of current constraining assumptions for statistical speech recognition systems, particularly the model of speech as a sequence of stationary segments consisting of uncorrelated acoustic vectors. A focus on perceptual models may in principle allow for statistical modeling of speech components that are more relevant for discrimination between candidate utterances during speech recognition. In particular, we hope to develop systems that have some of the robust properties of human audition for speech collected under adverse conditions. The outline of this new research direction is given here, along with some preliminary theoretical work.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-488"
  },
  "tatsumi94_icslp": {
   "authors": [
    [
     "Itaru E.",
     "Tatsumi"
    ],
    [
     "Hiroya",
     "Fujisaki"
    ]
   ],
   "title": "Auditory perception of filled and empty time intervals, and mechanism of time discrimination",
   "original": "i94_1947",
   "page_count": 4,
   "order": 489,
   "p1": "1947",
   "pn": "1950",
   "abstract": [
    "The discrimination ability of filled (1 kHz pure tones) and empty (gaps bounded by two clicks) intervals ranging from 50 msec to 12 sec was investigated. The relative accuracy of discrimination showed U-shaped curves for both intervals. A model, comprising a logarithmic conversion process of physical time to sensory quantity, a short-term memory and two internal noise sources, was proposed. It was found that the model was adequate for intervals shorter than 1 or 2 sec. Participation of other coding mechanisms was implied for time intervals beyond this range.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-489"
  },
  "cheesman94_icslp": {
   "authors": [
    [
     "Margaret F.",
     "Cheesman"
    ],
    [
     "Jennifer C.",
     "Armitage"
    ],
    [
     "Kimberley",
     "Marshall"
    ]
   ],
   "title": "Speech perception and growth of masking in younger and older adults",
   "original": "i94_1951",
   "page_count": 4,
   "order": 490,
   "p1": "1951",
   "pn": "1954",
   "abstract": [
    "Speech discrimination abilities and growth of masking in the 1000-Hz region were measured in younger and older listeners. All listeners had hearing thresholds equal to or better than 30 dB HL at audiometric frequencies < 2000 Hz and equal or better than 40 dB HL at audiometric frequencies from 3000 to 6000 Hz. In the first experiment, listeners were tested on a modified version [1] of the distinctive feature difference test [2] presented (a) in quiet, (b) in wide band noise, (c) high-pass filtered at 2000 Hz, and (d) low-pass filtered at 1000 Hz. Despite their good hearing thresholds, the older listeners made significantly more errors in all four test conditions and were more adversely affected by the degraded listening conditions than the younger listeners. In a second experiment, masked thresholds for signal frequencies of 750, 1000, and 1500 Hz in the presence of a 1/3-octave band of noise centred at 1000 Hz were obtained with four noise levels from 50 to 80 dB SPL. The older listeners had higher masked thresholds, however the slope of the growth of masking functions did not differ with age.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-490"
  },
  "toshio94_icslp": {
   "authors": [
    [
     "Irino",
     "Toshio"
    ],
    [
     "Roy D.",
     "Patterson"
    ]
   ],
   "title": "A theory of asymmetric intensity enhancement around acoustic transients",
   "original": "i94_1955",
   "page_count": 4,
   "order": 491,
   "p1": "1955",
   "pn": "1958",
   "abstract": [
    "A theory of asymmetric intensity enhancement around acoustic transients is presented. Experiments with ramped and damped tones (Patterson 1994a,b) have shown that reversing an asymmetric envelope changes the timbre of the sound. A 'delta-gamma' filter with thresholding produces asymmetric intensity enhancement around transients that can explain the experimental results obtained with damped and ramped tones. If we assume that the onset of a sound is more important perceptually than what comes shortly thereafter, then this onset information can be enhanced by the delta-gamma process. Nonsimultaneous masking experiments were also performed but between subject variability precluded explanation by the delta-gamma theory.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-491"
  },
  "javkin94_icslp": {
   "authors": [
    [
     "Hector",
     "Javkin"
    ],
    [
     "Elizabeth",
     "Keate"
    ],
    [
     "Norma",
     "Antonanzas"
    ],
    [
     "Ranjun",
     "Zou"
    ],
    [
     "Karen",
     "Youdelman"
    ]
   ],
   "title": "Text-to-speech in the speech training of the deaf: adapting models to individual speakers",
   "original": "i94_1959",
   "page_count": 4,
   "order": 492,
   "p1": "1959",
   "pn": "1962",
   "abstract": [
    "Computer-based speech training systems for deaf children provide, in addition to feedback from the children's speech production, acoustic and articulatory models for the children to imitate, either from pre-recorded utterences or from the teacher's on-line speech. It would be beneficial if such training systems functioned without a teacher and beyond what can be taught with pre-recorded models. Speech training might also be more successful if the models were adapted to the child's individual mode of producing speech sounds. Our system is designed to supplement the time that students have with teachers and to provide models optimized for a student's preferred mode of articulation by using a form of text-to-speech (TTS) to synthesize teaching models that are based on the student's own production. Although we are currently focusing on customizing tongue-palate contact data, it is applicable to other aspects of computer-based teaching.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-492"
  },
  "holton94_icslp": {
   "authors": [
    [
     "Thomas",
     "Holton"
    ]
   ],
   "title": "Robust pitch and voicing detection using a model of auditory signal processing",
   "original": "i94_1963",
   "page_count": 4,
   "order": 493,
   "p1": "1963",
   "pn": "1966",
   "abstract": [
    "We present a theory for the detection and identification of pitch and voicing based on a comprehensive physiological model of auditory signal processing. Our approach is based on building detectors of spatially and temporally local patterns of response phase from a number of parallel channels. Using this approach, we have designed computationally simple, physiologically reasonable algorithms for pitch and voicing that are robust in noise and selective for speech.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-493"
  },
  "imaizumi94b_icslp": {
   "authors": [
    [
     "Satoshi",
     "Imaizumi"
    ],
    [
     "Akiko",
     "Hayashi"
    ],
    [
     "Toshisada",
     "Deguchi"
    ]
   ],
   "title": "Listener adaptive characteristics in dialogue speech effects of temporal adjustment on emotional aspects of speech",
   "original": "i94_1967",
   "page_count": 4,
   "order": 494,
   "p1": "1967",
   "pn": "1970",
   "abstract": [
    "Effects of listener adaptive temporal adjustment in dialogue on emotional characteristics of speech were investigated by analyzing the speech of teachers directed to hearing-impaired (HI) or normal-hearing (NH) children. 1) Four factors were extracted which accounted for 81% of the total variance in perceptual rating scores on emotions. 2) Factor 1 represented the emotional differences between the read tokens (RD) and the dialogue speech (NH and HI), which were the contrasts between pleasant versus discomfort. 3) Factor 3 represented the differences between HI and the others (RD and NH), which was the contrast between \"Slow, Stiff, Intelligible, Strong\" versus \"Busy, Tense, Rough, Dull\". 4) The measures of the temporal structure of speech significantly accounted for the above-mentioned emotional contrasts. These results suggest that the listener-adaptive temporal adjustment of dialogue speech affects not only the segmental characteristics of speech such as devoicing but also prosody-related characteristics of speech such as the emotional profiles.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-494"
  },
  "tsuzaki94_icslp": {
   "authors": [
    [
     "Minoru",
     "Tsuzaki"
    ],
    [
     "Hiroaki",
     "Kato"
    ],
    [
     "Masako",
     "Tanaka"
    ]
   ],
   "title": "Effects of acoustic discontinuity and phonemic deviation on the apparent duration of speech segments",
   "original": "i94_1971",
   "page_count": 4,
   "order": 495,
   "p1": "1971",
   "pn": "1974",
   "abstract": [
    "To investigate the relation between a scene analytic process and a schema-based process in the perception of speech duration, the perceptual duration of restored speech segments whose duration was manipulated experimentally was measured in psychophysical experiments. In Experiment 1, the vowel /i/in the 3rd mora of the Japanese word, /shima»bi»ru/f was modified and replaced by a noise burst. The duration of the noise-replaced speech was perceived to be significantly shorter than that of the intact speech. Besides the effect of acoustic interruption, a significant effect of the durational modification was observed. However, there was no clear evidence that these two main factors interacted with each other. This suggests that a scene analytic process responsible for handling acoustic discontinuity is independent of the speech module responsible for the anchoring to the speech template. In Experiments 2 and 3,. a wider continuum of the standard stimulus, covering the Japanese word, /da*i*chi/, and another word, /da«i#i»chi/, was tested to investigate whether the cyclic pattern of the underestimation and overestimation could be observed owing to the double anchoring points. However, no clear cyclic pattern was found. This indicates that the anchoring effects by these two points, /daichi/ and /daiichi/, were not equivalent. Although no independent relation between the acoustic interruption and the durational modification was replicated because of the effect of presentation order, when the standard stimulus did precede the comparison stimulus, the noise replaced speech was perceived to be shorter than the intact one in most cases.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-495"
  },
  "craig94_icslp": {
   "authors": [
    [
     "Chie H.",
     "Craig"
    ],
    [
     "Richard. M.",
     "Warren"
    ],
    [
     "Tricia B. K.",
     "Chirillo"
    ]
   ],
   "title": "The influence of context on spoken language perception and processing among elderly and hearing impaired listeners",
   "original": "i94_1975",
   "page_count": 4,
   "order": 496,
   "p1": "1975",
   "pn": "1978",
   "abstract": [
    "In real-world speech communication, listener-based factors influence the timing and nature of fluent spoken language understanding. Such human factors might include the listener's knowledge base, language experience and auditory sensitivity. This study examined how aging and diminished hearing sensitivity influence a listener's use of context to perceive and process time-gated monosyllabic words. The results indicate that age, hearing sensitivity, and contextual factors significantly (p. <. .0001) influenced the timing and nature of the spoken word recognition process.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-496"
  },
  "kato94_icslp": {
   "authors": [
    [
     "Hiroaki",
     "Kato"
    ],
    [
     "Minoru",
     "Tsuzaki"
    ],
    [
     "Yoshinori",
     "Sagisaka"
    ]
   ],
   "title": "Acceptability of temporal modification in consonant and vowel onsets",
   "original": "i94_1979",
   "page_count": 4,
   "order": 497,
   "p1": "1979",
   "pn": "1983",
   "abstract": [
    "To specify the location of perceptually dominant markers for temporal structures of speech, the acceptability or detectability of the modification of segmental duration is measured. The modification is carried out in a complementary way, i.e., two successive segments are lengthened or shortened, and have the same absolute duration and opposite directions of change. The first experiment using 15 four-mora word stimuli shows that a vowel (V) duration and its adjacent consonant (C) duration can perceptually compensate each other. This compensation is found to not depend on the temporal order of target pairs (C-to-V or V-to-C), but rather on the loudness difference between V and C; the acceptability decreases when the loudness difference between V and C becomes high. This suggests that perceptually dominant markers locate around major loudness jumps; this finding is supported by the second experiment using non-speech stimuli replicating the loudness contours of speech stimuli. The detectability of temporal displacement is higher at large loudness jumps than at small loudness jumps.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-497"
  },
  "zhu94_icslp": {
   "authors": [
    [
     "Weizhong",
     "Zhu"
    ],
    [
     "Yoshinobu",
     "Kikuchi"
    ],
    [
     "Yasuo",
     "Endo"
    ],
    [
     "Hideki",
     "Kasuya"
    ],
    [
     "Minoru",
     "Hirano"
    ],
    [
     "Masanao",
     "Ohashi"
    ]
   ],
   "title": "An integrated acoustic evaluation system of pathologic voice",
   "original": "i94_1983",
   "page_count": 4,
   "order": 498,
   "p1": "1983",
   "pn": "1986",
   "abstract": [
    "The paper presents an integrated acoustic evaluation system of pathologic voice which has been designed for the use at voice clinics and research laboratories. The key features of the system are implementation of a sophisticated fundamental period detection algorithm, various jitter and shimmer measurements, accurate laryngeal turbulence noise measurements, voice range profile, fast computation of the voice evaluation parameters and many other measurements of basic voice parameters, such as formant values. The system is a Windows application. It can runs on any personal computer with a MS-Windows platform. The proposed evaluation system is expected to be useful at voice clinics and research laboratories.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-498"
  },
  "fukuda94_icslp": {
   "authors": [
    [
     "Yumiko",
     "Fukuda"
    ],
    [
     "Wako",
     "Ikehara"
    ],
    [
     "Erniko",
     "Kamikubo"
    ],
    [
     "Shizuo",
     "Hiki"
    ]
   ],
   "title": "An electronic dictionary of Japanese sign language: design of system and organization of database",
   "original": "i94_1987",
   "page_count": 4,
   "order": 499,
   "p1": "1987",
   "pn": "1990",
   "abstract": [
    "The design of a system and the organization of a database for an electronic dictionary of Japanese sign language is discussed, as part of a study concerned with language supplements to compensate for disabilities in using speech. The sign language vocabulary used in this dictionary is a set of about 900 sign words which are highly frequent in daily use of Japanese sign language. Each of the selected sign words was labeled with a descriptive system which consisted of a new combination of symbols for shape, position and movement of hands and arms, as well as phonetic and orthographic descriptions of corresponding speech. Pictures of these words, signed by a female native signer, were recorded on a laser disc. These recorded pictures were linked to a computer program through a computer controlled laser disc player, so that the corresponding pictures in the sign language vocabulary could be retrieved instantaneously from any of the descriptive items in the database. The system is being used to analyze details of sign gestures, and to refine the descriptions. The possibility of coordinating traditionally used descriptive symbols in different sign languages with kinematic properties of hand and arm movements is also investigated.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-499"
  },
  "endo94b_icslp": {
   "authors": [
    [
     "Yasuo",
     "Endo"
    ],
    [
     "Hideki",
     "Kasuya"
    ]
   ],
   "title": "Synthesis of pathological voice based on a stochastic voice source model",
   "original": "i94_1991",
   "page_count": 4,
   "order": 500,
   "p1": "1991",
   "pn": "1994",
   "abstract": [
    "This paper proposes a stochastic voice source model to synthesize pathological voice as well as normal voice. The voice signal is assumed to consist of a harmonic signal and an additive laryngeal noise signal. Perturbation of fundamental period, which is the key characteristic of pathological voice, is represented by an autoregressive moving average (ARMA) model. Suitability of the model is tested based on a spectral flatness measure by applying it to the normalized period sequence of 52 pathologic voice samples. Relationship between the model parameters and perceived roughness quality is also investigated. Based on this model, we construct an analysis-conversion-synthesis system of pathological voice. The system is applicable not only to perceptual experiments to explore acoustic correlates of pathological voice qualities, but also to the simulation of the voice quality variations of the voice treatment at voice clinics.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-500"
  },
  "hosoi94_icslp": {
   "authors": [
    [
     "Hiroshi",
     "Hosoi"
    ],
    [
     "Yoshiaki",
     "Tsuta"
    ],
    [
     "Takashi",
     "Nishida"
    ],
    [
     "Kiyotaka",
     "Murata"
    ],
    [
     "Fumihiko",
     "Ohta"
    ],
    [
     "Tsuyoshi",
     "Mekata"
    ],
    [
     "Yumiko",
     "Kato"
    ]
   ],
   "title": "Hearing aid evaluation using variable - speech - rate audiometry",
   "original": "i94_1995",
   "page_count": 4,
   "order": 501,
   "p1": "1995",
   "pn": "1998",
   "abstract": [
    "A new hearing aid evaluation method using variable-speech-rate audioraetry (VSRA) was recently developed. VSRA was newly created based on the Japanese speech audiometry authorized by the Japan Audiological Society. The ordinary speech audiometry can not reveal a temporal factor in word discrimination ability of the hearing impaired. Since, with VSRA, we can compare several performance-intensity curves obtained from different speech-rate speech audiometries, the impact on the auditory system of each patient by the fast or slow speech rate could be easily determined. Taking the temporal factor of the auditory systems into consideration by using VSRA, hearing aid evaluation was performed for a master hearing aid with three types of signal processings and fittings for 36 hearing impaired subjects. VSRA was useful for hearing aid evaluation, in particular, for cases when ordinary normal speech rate audiometry does not provide a significant difference in word discrimination scores. In addition, using VSRA revealed that amplitude compression is more effective for improvement of word discrimination than linear amplification.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-501"
  },
  "minifie94_icslp": {
   "authors": [
    [
     "Fred D.",
     "Minifie"
    ],
    [
     "Daniel Z.",
     "Huang"
    ],
    [
     "Jordan",
     "Green"
    ]
   ],
   "title": "Relationship between acoustic measures of vocal perturbation and perceptual judgments of breathiness, harshness, and hoarseness",
   "original": "i94_1999",
   "page_count": 4,
   "order": 502,
   "p1": "1999",
   "pn": "2002",
   "abstract": [
    "This paper compares the effects of acoustic perturbations on perceptual judgments of voice quality in disordered and synthesized vowels. Five levels of jitter, shimmer, and glottal noise, respectively, were systematically added to synthesized vowels. Perceptual judgments of breathiness, hoarseness and harshness showed that glottal noise was correlated more strongly with perceptual judgments of Breathiness and Hoarseness than with Harshness; and was a better predictor of all three perceptual dimensions that were jitter and shimmer.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-502"
  },
  "ikeda94b_icslp": {
   "authors": [
    [
     "Takashi",
     "Ikeda"
    ],
    [
     "Kouji",
     "Tasaki"
    ],
    [
     "Akira",
     "Watanabe"
    ]
   ],
   "title": "A hearing aid by single resonant analysis for telephonic speech",
   "original": "i94_2003",
   "page_count": 4,
   "order": 503,
   "p1": "2003",
   "pn": "2006",
   "abstract": [
    "This paper proposes a hearing aid which works by a new analysis-synthesis technique for telephonic speech. The software simulation will be introduced first and the hardware realization with multi-DSP units will be described. The developed system has two strong points in comparison with the other digital hearing aids. One is to use the special analysis-synthesis technique, and the other is in the amplitude compression method to reduce harmonic distortion which cannot be removed in instantaneous compression. Since the compression coefficients in this system change with rms level in a frame of signals, non-linear distortion little causes. The real-time system has been designed and produced with four DSP boards which have a quite same structure. As each DSP unit takes charge of independent program such as formant extraction, pitch extraction or amplitude compression etc., computer simulation programs can be transferred basically as they are. Assuming the auditory characteristics of sensorineural hearing loss, the effect of the processing by the DSP system has been investigated. The processing results show to be same as those by computer simulation. Furthermore, the preliminary listening tests indicate usefulness in this processing technique.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-503"
  },
  "yamada94c_icslp": {
   "authors": [
    [
     "Tsuneo",
     "Yamada"
    ],
    [
     "Reiko A.",
     "Yamada"
    ],
    [
     "Winifred",
     "Strange"
    ]
   ],
   "title": "Perceptual learning of Japanese mora syllables by native speakers of american English an analysis of acquisition processes of speech perception in second language learning",
   "original": "i94_2007",
   "page_count": 4,
   "order": 504,
   "p1": "2007",
   "pn": "2010",
   "abstract": [
    "Fifteen native speakers of American English ( AE ), who had no Japanese language experience, were trained to identify Japanese short vowels, long vowels and special phoneme /Q/. Stimulus materials were two- or three-mora syllables which constituted a minimal triplet, /C1V1C2V2/-/C1V1V1C2V2/-/C1V1QC2V2/. By changing the combinations of C2 and VI, two sets of training stimuli, which were different in phonemic contexts, were prepared and their effects on the training were compared. In both training groups, generalization was extensively observed in most of phonemic contexts. Acquisition processes in each training group were described by using two kinds of achievement tests. Though terminal states were very similar, acquisition processes were different between training groups. The effectiveness of various training methods and the significance of the learning paradigm in speech perception studies were discussed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-504"
  },
  "veda94_icslp": {
   "authors": [
    [
     "Yuichi",
     "Veda"
    ],
    [
     "Takayuki",
     "Agawa"
    ],
    [
     "Akira",
     "Watanabe"
    ]
   ],
   "title": "A DSP-based amplitude compressor for digital hearing AIDS",
   "original": "i94_2011",
   "page_count": 4,
   "order": 505,
   "p1": "2011",
   "pn": "2014",
   "abstract": [
    "Amplitude compression techniques have been used in the hearing aids for the sensory-neural impaired. In this paper, we present a DSP-based amplitude compressor which is based on the conventional filter-bank method, but can realize the relatively smoothed compression characteristics without the FFT method. The system has two kinds of filter-banks. The speech spectrum is estimated using the first bank and a desired compressor is generated at the second one which is similar to FIR filter used in the wide-band compression. Moreover, by estimating the spectrum using the output of the first filter-bank, it is possible to compress adaptively the signal according to the temporal variations of the speech features. We describe the principles of such compression system and its DSP implementation, and show that it is the simplified compression for the DSP realization, in this paper.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-505"
  },
  "sarabasa94_icslp": {
   "authors": [
    [
     "Amalia",
     "Sarabasa"
    ]
   ],
   "title": "Perception and production saturation of spoken English as a first phase in reducing a foreign accent",
   "original": "i94_2015",
   "page_count": 4,
   "order": 506,
   "p1": "2015",
   "pn": "2018",
   "abstract": [
    "To improve perception and production of spoken English prosody in students of English Phonetics and Phonology I and II at the School of Modern Languages of Central University of Venezuela, 84 audio recorded tapes of 43 interviewed subjects from Canada, UK and USA were the sources for the perception saturation exercises (repeated listening of a selection, orthographic transcription of its utterances marking pause, stress, and final intonation), and the production saturation ones (imitation, memorization and oral production of the native speaker's pronunciation in the transcribed selection). The hypothesis was that saturated learning of the different accents at the level of perception and later production would give the students a mental and articulatory pronunciation template that would help them reduce their foreign accent. The results of the longitudinal studies seem to display the positive effect of perception saturation and production saturation in the students1 transcriptions of prosodic elements and their oral production of them since they show an apparent reduction in their foreign accent.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-506"
  },
  "rooney94_icslp": {
   "authors": [
    [
     "Edmund",
     "Rooney"
    ],
    [
     "Fabrizio",
     "Carraro"
    ],
    [
     "Will",
     "Dempsey"
    ],
    [
     "Katie",
     "Robertson"
    ],
    [
     "Rebecca",
     "Vaughan"
    ],
    [
     "Mervyn A.",
     "Jack"
    ],
    [
     "Jonathan",
     "Murray"
    ]
   ],
   "title": "Harp: an autonomous speech rehabilitation system for hearing-impaired people",
   "original": "i94_2019",
   "page_count": 4,
   "order": 507,
   "p1": "2019",
   "pn": "2022",
   "abstract": [
    "This paper describes the implementation and evaluation of the HARP system, a PC-based visual speech training aid for hearing-impaired people. The system offers teaching modules for a range of speech parameters, including intonation, rhythm, vowel quality and consonant production, and provides evaluative and diagnostic feedback to users. The development of the system is being assisted by consultation with hearing-impaired groups and speech and hearing therapists, and initial evaluations of a prototype aid have been completed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-507"
  },
  "yamada94d_icslp": {
   "authors": [
    [
     "Reiko A.",
     "Yamada"
    ],
    [
     "Winifred",
     "Strange"
    ],
    [
     "James S.",
     "Magnuson"
    ],
    [
     "John S.",
     "Pruitt"
    ],
    [
     "William D.",
     "Clarke"
    ]
   ],
   "title": "The intelligibility of Japanese speakers' production of american English /r/, /i/, and /w/, as evaluated by native speakers of american English",
   "original": "i94_2023",
   "page_count": 4,
   "order": 508,
   "p1": "2023",
   "pn": "2026",
   "abstract": [
    "We studied the effects of immersion in an American English (AE) language environment on the intelligibility of the productions of English /r/, /I/, and /w/ in CV syllables by native speakers of Japanese, as judged by native speakers of American English. Stimuli were produced by subjects in 3 language groups: 144 native speakers of Japanese with no experience living in an AE language environment (J subjects), 144 native speakers of Japanese with experience living in an AE speaking environment (JE subjects), and a control group of 24 native speakers of AE (AE subjects). The intelligibility patterns of the 3 groups differed significantly from each other, and the intelligibility of the JE subjects' productions ranged from native-AE-speaker levels to the level of Japanese speakers who had never lived abroad. A comparison of the Japanese subjects' production performance with performance in a perception test revealed a positive correlation between production and perception abilities. There was a negative correlation between production scores and the amount of variance in perception scores, suggesting that in early stages of learning, subjects may acquire articulation more easily than perception. There were also tendencies for age of initial experience in the AE environment to be negatively correlated with production ability, and for duration of experience to be positively correlated with production ability.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-508"
  },
  "nagayama94_icslp": {
   "authors": [
    [
     "Itaru",
     "Nagayama"
    ],
    [
     "Norio",
     "Akamatsu"
    ],
    [
     "Toshiki",
     "Yoshino"
    ]
   ],
   "title": "Phonetic visualization for speech training system by using neural network",
   "original": "i94_2027",
   "page_count": 4,
   "order": 509,
   "p1": "2027",
   "pn": "2030",
   "abstract": [
    "This paper describes an attempt to develop a new phonetic visualization method of speech training system for the speech impairment. A problem associated with the conventional speech training system is briefly reviewed. Reasons for attempting to develop the new method are given. The proposed method uses neural network approach to visualize phonetic patterns. The method and its display capabilities are described. Also the experimental result of visualizing five Japanese vowels /A/,/I/,/U/,/E/,/O/ by using the proposed method is described.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-509"
  },
  "slawinski94_icslp": {
   "authors": [
    [
     "Elzbieta B.",
     "Slawinski"
    ]
   ],
   "title": "Perceptual and productive distinction between the English [r] and [l] in prevocalic position by English and Japanese speakers",
   "original": "i94_2031",
   "page_count": 4,
   "order": 510,
   "p1": "2031",
   "pn": "2034",
   "abstract": [
    "The present study examined the usage of spectral and temporal acoustical features in the production of the phonemic contrast between the [r] and [1] sounds in the prevocalic position by Japanese speakers with different perceptual skills in English. The acoustical analysis of recorded tokens demonstrated that Japanese speakers, who perceptually were able to integrate information carried by the spectral and temporal acoustical cues, were also able to use both of those cues in their productive distinction between the [r] and [1] sounds. A lack of ability to integrate the temporal and spectral cues of the perceptual task demonstrated by the second group of Japanese listeners was reflected in their poor productive distinction between the [r] and [1] sounds.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-510"
  },
  "naito94b_icslp": {
   "authors": [
    [
     "Yasushi",
     "Naito"
    ],
    [
     "Hidehiko",
     "Okazawa"
    ],
    [
     "Iwao",
     "Honjo"
    ],
    [
     "Yosaku",
     "Shiomi"
    ],
    [
     "Haruo",
     "Takahashi"
    ],
    [
     "Waka",
     "Hoji"
    ],
    [
     "Michio",
     "Kawano"
    ],
    [
     "Hiroshi",
     "Ishizu"
    ],
    [
     "Sadahiko",
     "Nishizawa"
    ],
    [
     "Yoshiharu",
     "Yonekura"
    ],
    [
     "Junji",
     "Konishi"
    ]
   ],
   "title": "Cortical activation with speech in cochlear implant users: a study with positron emission tomography",
   "original": "i94_2035",
   "page_count": 4,
   "order": 511,
   "p1": "2035",
   "pn": "2038",
   "abstract": [
    "The objective of this study is to investigate the pattern of cortical activation associated with speech sound stimulation in cochlear implant (CI) users. Six postlingually deaf patients using CIs were examined with positron emission tomography (PET) using intravenous injection of 15-O labeled water. PET activation studies were performed in two sets of three different conditions; (1) no sound stimulation as control, (2) hearing white noise and (3) hearing sequential words. Under sequential words' stimulation, regional cerebral blood flow (rCBF) in the primary auditory cortex, the auditory integration region and the auditory association area increased from the base line value. In contrast, white noise stimulation resulted in an increase of rCBF only in the primary auditory and auditory integration regions, and the increase was less than that observed in word stimulation. These results suggest that, in spite of non-physiological stimulation of the auditory primary afferents, speech being input through CIs are processed in the ordinary auditory cortices.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-511"
  },
  "aikawa94b_icslp": {
   "authors": [
    [
     "Kiyoaki",
     "Aikawa"
    ],
    [
     "Reiko A.",
     "Yamada"
    ]
   ],
   "title": "Comparative study of spectral representations in measuring the English /r/-/l/ acoustic-perceptual dissimilarity",
   "original": "i94_2039",
   "page_count": 4,
   "order": 512,
   "p1": "2039",
   "pn": "2042",
   "abstract": [
    "The talker dependency of the correct response rate (CRR) on English /r/-/l/ identification by Japanese listeners has already been reported. This paper shows that the talker dependency of the CRR can be explained by the acoustical dissimilarity (ADS) between an /r/ and an /I/ measured by the dynamic-cepstrum. The dynamic-cepstrum is a new spectral representation which simulates time-frequency forward masking. Nine spectral representations including weighted-cepstrum, mel-cepstrum, and delta-cepstrum were compared in terms of correlation between the CRR and the ADS. The ADS measured by the dynamic-cepstrum showed the best correlation with the CRR. The experimental results imply that Japanese listeners tend to identify /r/ or /I/ using the succeeding vowels affected by co-articulations.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-512"
  },
  "kitazawa94b_icslp": {
   "authors": [
    [
     "Shigeyoshi",
     "Kitazawa"
    ],
    [
     "Kazuyuki",
     "Muramoto"
    ],
    [
     "Juichi",
     "Ito"
    ]
   ],
   "title": "Acoustic simulation of auditory model based speech processor for cochlear implant system",
   "original": "i94_2043",
   "page_count": 4,
   "order": 513,
   "p1": "2043",
   "pn": "2046",
   "abstract": [
    "We propose a speech processor based on the auditory model to generate stimulus pattern similar to the auditory system. Our model consists of the auditory filter to simulate the tuning curve and the hair-cell model to simulate the non-linear characteristics of the auditory nerve firing. The model consists of 28 separate channels placed equidistant along the basilar membrane. Assuming the implanted receiver stimulator, the stimulation is possible sequentially at the interval greater than 0.8 ms. The fundamental period was obtained from the summation of lower three channels. During every period, four most frequent firing channels were selected to stimulate- We estimated the quality of electric sensation by acoustic simulation. We carried out listening experiment by normal hearing subjects without visual presentation. The subjects have got improvements in perception of voiced stops, voiced fricatives, voiceless fricatives, voiced affricates, and nasals.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-513"
  },
  "kashino94_icslp": {
   "authors": [
    [
     "Makio",
     "Kashino"
    ],
    [
     "Chie H.",
     "Craig"
    ]
   ],
   "title": "The influence of knowledge and experience during the processing of spoken words: non-native listeners",
   "original": "i94_2047",
   "page_count": 4,
   "order": 514,
   "p1": "2047",
   "pn": "2050",
   "abstract": [
    "This paper examines the effect of prior language knowledge and experience on English as a Second Language (ESL) listeners' use of acoustic-phonetic and linguistic-contextual information during the real-time spoken word recognition process. A set of prerecorded compound words called \"spondees\" was applied to study the recognition of time-gated words and syllables. Performance measures were obtained from thirty Japanese ESL listeners. Their response accuracy and confidence were monitored as the amount and nature of gated-in acoustic-phonetic and contextual information changed. Also an item analysis was performed to observe how the listeners' intermediate and final word representations reflected important features of their lexical searching processes. The results indicate that ESL listeners' knowledge and experience strongly influences the timing and nature of their real-time spoken word recognition process as well as the interaction of acoustic-phonetic and linguistic-contextual information. These findings are discussed in terms of native language effects and translanguage influences.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-514"
  },
  "house94_icslp": {
   "authors": [
    [
     "David",
     "House"
    ]
   ],
   "title": "Perception and production of mood in speech by cochlear implant users",
   "original": "i94_2051",
   "page_count": 4,
   "order": 515,
   "p1": "2051",
   "pn": "2054",
   "abstract": [
    "This paper presents results from a study where speaker mood identification performance of 17 cochlear implant users at two weeks following processor activation was compared to the performance of 13 and 11 members of the same group at 6 months and at more than one year after processor activation respectively. The cochlear implant users as a group showed difficulties confusing happy with angry and sad. Group performance improved only slightly over time while individual differences were considerable, ranging from 25% to 90% correct identification. Changes in production performance were also studied in three of the implant users. In the production study, the most dramatic improvements occurred in recordings made six months after activation. The length of time needed to regain voice control after deafness may indicate a difference between reacquisition speed in perception and production in cochlear implant users.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-515"
  },
  "nejime94_icslp": {
   "authors": [
    [
     "Yoshito",
     "Nejime"
    ],
    [
     "Toshiyuki",
     "Aritsuka"
    ],
    [
     "Toshiki",
     "Imamura"
    ],
    [
     "Tohru",
     "Ifukube"
    ],
    [
     "Jun'ichi",
     "Matsushima"
    ]
   ],
   "title": "A portable digital speech rate converter and its evaluation by hearing-impaired listeners",
   "original": "i94_2055",
   "page_count": 4,
   "order": 516,
   "p1": "2055",
   "pn": "2058",
   "abstract": [
    "A new portable device that reduces speech speed in real time has been developed to aid hearing-impaired listeners. The device employs realtime digital signal processing to expand the time scale of the speech signal without changing the pitch. Seven out often elderly hearing-impaired listeners have shown improvements in word discrimination tests when using this speech-rate conversion. The subjects' temporal resolution showed a correlation with the observed improvement, with a coefficient value of 0.859. The results suggest that this device provides support at the cognitive level of the auditory system.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-516"
  },
  "funaki94_icslp": {
   "authors": [
    [
     "Keiichi",
     "Funaki"
    ],
    [
     "Kazunaga",
     "Yoshida"
    ],
    [
     "Kazunori",
     "Ozawa"
    ]
   ],
   "title": "4kb/s speech coding with small computational amount and memory requirement: ULCELP",
   "original": "i94_2059",
   "page_count": 4,
   "order": 517,
   "p1": "2059",
   "pn": "2062",
   "abstract": [
    "This paper proposes a 4 kb/s speech coding method with extremely small computational amount and memory requirements, ULCELP (Ultra small computational amount JLearned CELP). ULCELP is based on the M-LCELP (Multi-mode Learned CELP) method, which was submitted to the North-America TIA halfrate digital cellular telephone standardization contest. In order to drastically reduce the computation and memory requirements, while maintaining good quality synthetic speech at 4 kb/s, the following techniques are developed; (l)Codebooks are changed in accord with voiced and unvoiced segments, (2)Two-stage closed-closed adaptive codebook search with a small computational amount and pitch lag differential coding, (3)A small computational amount and memory requirement excitation codebook search. ULCELP requires less than 3 MOPS of computation amount, and less than 2k word memory (RAM) amount. The MOS subjective evaluation results show that 4 kb/s ULCELP speech quality is close to that of G.726 AD-PCM at 24 kb/s. The result demonstrates that the ULCELP provides sufficient speech quality for voice storage applications.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-517"
  },
  "ferrerballester94_icslp": {
   "authors": [
    [
     "Miguel A.",
     "Ferrer-Ballester"
    ],
    [
     "Anibal R.",
     "Figueiras-Vidal"
    ]
   ],
   "title": "Improving CELP voice quality by projection similarity measure",
   "original": "i94_2063",
   "page_count": 4,
   "order": 518,
   "p1": "2063",
   "pn": "2066",
   "abstract": [
    "It is well-known the limited quality of the reconstructed speech by Code-Excited Linear Prediction (CELP) speech coder at low bit-rate (2400-8000 bps). The major source of audible distortion has been attributed to an inaccurate degree of periodicity of the voiced speech signal. In the present paper we alleviate this drawback by modifying the MSE distance measure, which is not able to capture the periodicity adequately. The new error criterion proposed is a projection similarity measure, which computes a projection distance of original onto coded perceptually weighted voice on a point to point basis. The improvement of the quality of the speech reconstructed by the CELP with projection distance measure has been checked by subjective A-B test. This result emphasizes the perceptual importance of an adequate description of the pitch-pulse waveform of the original (uncoded) LP residue by the CELP-coded one.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-518"
  },
  "ohmuro94_icslp": {
   "authors": [
    [
     "Hitoshi",
     "Ohmuro"
    ],
    [
     "Kazunori",
     "Mano"
    ],
    [
     "Takehiro",
     "Moriya"
    ]
   ],
   "title": "Variable bit-rate speech coding based on PSI-CELP",
   "original": "i94_2067",
   "page_count": 4,
   "order": 519,
   "p1": "2067",
   "pn": "2070",
   "abstract": [
    "This paper proposes a new variable bit-rate PSI-CELP speech coding method that switches four modes roughly corresponding to silence, unvoiced regions, voiced transient regions, and voiced stationary regions. The coder modes are determined by an open-loop procedure every two subframes (20 ms) using the feature parameters extracted from the input speech. The proposed method is based on the algorithms used in the PDC half-rate standard, but the LSP coder uses inter-frame predictive vector quantization every two subframes instead of matrix quantization every four subframes, and pitch parameters are coded by using finite-state intersubframe prediction in voiced stationary regions to achieve good quality with fewer bits. We examined the performance using Japanese speech data which was approximately 83% active. A listening test using non-specialist subjects showed that the proposed method achieves much better quality at average bit-rate of 2.53 kbit/s over all speech data or at an average bit-rate of 2.88 kbit/s without silence than the fixed bit-rate PDC half-rate standard (3.45 kbit/s). Even when the input speech was noisy, the proposed method still achieved better quality with fewer bits than the PDC standard. This method considerably reduces the bit-rate not only in silence and unvoiced regions but also in voiced regions, so it achieves high-quality variable-low-bit-rate speech coding.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-519"
  },
  "kim94b_icslp": {
   "authors": [
    [
     "Sung Joo",
     "Kim"
    ],
    [
     "Seung Jong",
     "Park"
    ],
    [
     "Yung Hwan",
     "Oh"
    ]
   ],
   "title": "Complexity reduction methods for vector sum excited linear prediction coding",
   "original": "i94_2071",
   "page_count": 4,
   "order": 520,
   "p1": "2071",
   "pn": "2074",
   "abstract": [
    "The vector sum excited linear prediction (VSELP) coding gives high quality of synthetic speech at bit rates as low as 4.8kbps, but its computational complexity is prohibitive for real-time applications. In this paper, we propose three methods to reduce the computations in VSELP coding. First, we use the overlapped sparse codebook for the basis vectors. Second, we introduce the preprocessing step to the stochastic codebook search procedure. It decides some combination coefficients of basis vectors using heuristics so that the search space decreases. Third, some candidates are preselected before the adaptive codebook search procedure by comparing them with the ideal excitation sequence. We develop a 4.8kbps coder using all the proposed methods and perform the quality test. It has been shown that the proposed coder retains good quality of synthetic speech and it is more than twice as fast as the original coder.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-520"
  },
  "rao94b_icslp": {
   "authors": [
    [
     "Preeti",
     "Rao"
    ],
    [
     "Yoshiaki",
     "Asakawa"
    ],
    [
     "Hidetoshi",
     "Sekine"
    ]
   ],
   "title": "8 kb/s low-delay speech coding with 4 ms frame size",
   "original": "i94_2075",
   "page_count": 4,
   "order": 521,
   "p1": "2075",
   "pn": "2078",
   "abstract": [
    "This paper describes modifications to a previously proposed 8 kb/s 4 ms-delay CELP speech coding algorithm with a view to improving the speech quality while maintaining the low delay and with only moderate increases in complexity. The modifications are based on improving the effectiveness of interframe pitch lag prediction as well as the level of sub-optimality of the coding of the excitation to the backward adapted synthesis filter by delayed decision and joint optimization techniques. Results of subjective listening tests using Japanese speech indicate that the coded speech quality is significantly superior to that of the 8 kb/s VSELP coder with 20 ms delay. A method that reduces the computational complexity of closed-loop 3-tap pitch prediction with no perceptible degradation in speech quality is proposed, based on representing the pitch-tap vector as the product of a scalar pitch gain and a normalized shape codevector.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-521"
  },
  "yao94_icslp": {
   "authors": [
    [
     "Jey-Hsin",
     "Yao"
    ],
    [
     "Yoshinori",
     "Tanaka"
    ]
   ],
   "title": "Low-bit-rate speech coding with mixed-excitation and interpolated LPC coefficients",
   "original": "i94_2079",
   "page_count": 4,
   "order": 522,
   "p1": "2079",
   "pn": "2082",
   "abstract": [
    "A very low-bit-rate speech codec combining mixed excitation technique and interpolation of LPC coefficients has been proposed. This coder is based on LPC vocoder structure and the target bit rate is 1.9 kb/s. Interframe coding techniques were used to achieve the low rate but at the same time, measures were taken to prevent error propagation. Informal listening tests showed that the proposed coder produces good quality speech.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-522"
  },
  "chan94_icslp": {
   "authors": [
    [
     "Cheung-Fat",
     "Chan"
    ]
   ],
   "title": "Multi-band excitation coding of speech at 960 bps using split residual VQ and v/UV decision regeneration",
   "original": "i94_2083",
   "page_count": 4,
   "order": 523,
   "p1": "2083",
   "pn": "2086",
   "abstract": [
    "This paper describes a method to achieve high-quality coding of speech signals at 960 bps. The method employs the multiband excitation (MBE) model together with very efficient quantization schemes for coding the spectrum magnitudes and the voiced/unvoiced (V/UV) decisions. The spectrum magnitudes arc coded using a novel vector quantization (VQ) scheme which has both the structural characteristics of multistage VQ and split VQ. The V/UV decisions for pitch harmonics are not transmitted to the decoder but are regenerated from the spectrum information available in the decoder. It was demonstrated that the proposed speech coder is capable of generating speech of good quality.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-523"
  },
  "koishida94_icslp": {
   "authors": [
    [
     "Kazuhito",
     "Koishida"
    ],
    [
     "Keiichi",
     "Tokuda"
    ],
    [
     "Takao",
     "Kobayashi"
    ],
    [
     "Satoshi",
     "Imai"
    ]
   ],
   "title": "Speech coding based on adaptive MEL-cepstral analysis for noisy channels",
   "original": "i94_2087",
   "page_count": 4,
   "order": 524,
   "p1": "2087",
   "pn": "2090",
   "abstract": [
    "In this paper, we examine the robustness of an ADPCM coder based on adaptive mel-cepstral analysis. To improve the noisy channel performance, we use traditional techniques: the leakage factor and sign function. The subjective speech quality of the proposed 16kb/s coder and G.726 coder in terms of the opinion equivalent Q is measured and compared. It is shown that the proposed coder produces much higher quality speech than that of 16kb/s G.726 at BER(Bit Error Rate)=0 and BER=10~3. Although the coder scores 4dB lower than 32kb/s G.726 at BER=0, the improvement of more than 5dB is achieved by the proposed coder over G.726 at BER=10~3.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-524"
  },
  "jean94_icslp": {
   "authors": [
    [
     "Fu-Rong",
     "Jean"
    ],
    [
     "Hsiao-Chuan",
     "Wang"
    ]
   ],
   "title": "A two-stage coding of speech LSP parameters based on KLT transform and 2d-prediction",
   "original": "i94_2091",
   "page_count": 4,
   "order": 525,
   "p1": "2091",
   "pn": "2094",
   "abstract": [
    "In this paper, a two-stage approach based on KLT transform and 2D prediction is proposed for encoding the LSP parameters. At the first stage, KLT transform is applied to the even part of LSP parameters which are mean-removed. Then DPCM coding is performed on the transformed parameters. At the second stage, the even part of LSP parameters is reconstructed, and a third-order two-dimensional prediction is used to estimate the odd part of LSP parameters. This approach considers both interframe and intraframe correlations simultaneously. The simulation on the database provided by different speakers shows that 1 dB2 difference limen of spectral distortion can be achieved at 19 bits per frame by using the partitioned vector quantization (PVQ) to the residuals of LSP parameters. The residuals of LSP parameters are partitioned into even part and odd part according to the processing of stage sequence. The outlier frames which have spectral distortion greater than 2 dB are 3.63 %. A switched classifier is built to reduce the outlier frames down to about 0.27 % and make no frames with spectral distortion greater than 4 dB at an average bit-rate below 19 bits per frame.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-525"
  },
  "levitt94_icslp": {
   "authors": [
    [
     "Harry",
     "Levitt"
    ]
   ],
   "title": "on communication disabilities ********* technologies for signal processing hearing AIDS",
   "original": "i94_2095",
   "page_count": 4,
   "order": 526,
   "p1": "2095",
   "pn": "2098",
   "abstract": [
    "Hearing aids that are currently in use can be subdivided into three basic types. In a traditional linear hearing aid, the gain of the instrument is adjusted manually and remains fixed regardless of signal level. In a compression hearing aid, gain is dependent on signal levels. In an automatic frequency response (AFR) hearing aid the frequency-gain characteristic (i.e., both gain and frequency response) are dependent on signal level. Hybrid circuitry in which a digital unit controls analog audio components is being used increasingly in modern hearing aids. This technology allows for the hearing aid to be programmed by an external computer. There are many different kinds of experimental hearing aids. These include complex multi-channel compression amplifiers as well as systems with advanced signal-processing capabilities for noise/reverberation reduction, feedback control and novel techniques for enhancing those features of the speech signal that are either inaudible or misheard as a result of the hearing impairment.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-526"
  },
  "asano94_icslp": {
   "authors": [
    [
     "Futoshi",
     "Asano"
    ],
    [
     "Yoiti",
     "Suzuki"
    ],
    [
     "Toshio",
     "Sone"
    ]
   ],
   "title": "Signal processing techniques applicable to hearing aids",
   "original": "i94_2099",
   "page_count": 4,
   "order": 527,
   "p1": "2099",
   "pn": "2102",
   "abstract": [
    "In this paper, signal processing techniques for environmental noise reduction and howling suppression, which have mainly been developed for other applications but are also applicable to hearing aids, are discussed. Among noise reduction techniques, adaptive signal processing including the adaptive beamformer and speech spectrum/parameter estimator is discussed. Especially, in the adaptive beamformer, an effort to reduce impulsive noise, which severely reduces intelligibility in daily communication but is difficult to deal with by conventional adaptive schemes, is described. In howling suppression, two approaches, i.e., the adaptive feedback canceler and the adaptive notch filter, are discussed.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-527"
  },
  "blarney94_icslp": {
   "authors": [
    [
     "Peter",
     "Blarney"
    ],
    [
     "Gary",
     "Dooley"
    ],
    [
     "Elvira",
     "Parisi"
    ]
   ],
   "title": "Combination and comparison of electric stimulation and residual hearing",
   "original": "i94_2103",
   "page_count": 4,
   "order": 528,
   "p1": "2103",
   "pn": "2106",
   "abstract": [
    "Speech processing for cochlear implant users has now reached a level where some severely hearing-impaired hearing aid users may be better aided by a cochlear implant, or a hearing aid and implant together. This paper reviews studies comparing the loudness, pitch, and vowel perception in opposite ears of adults using cochlear implants and hearing aids. A study of nine subjects showed narrow dynamic ranges and steep loudness growth in both ears. Mismatches in aided thresholds and dynamic ranges at different frequencies resulted in highly variable loudness differences between the ears for some subjects. A comparison using pure tones showed that the electric pitch depended on both rate and electrode site. Pitch of electrodes was lower than expected from the characteristic frequency distribution in a normal cochlea. Synthetic vowels were used to show that signals presented via the implant and hearing aid may be perceived as different vowels in the two ears.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-528"
  },
  "funasaka94_icslp": {
   "authors": [
    [
     "Sotaro",
     "Funasaka"
    ],
    [
     "Masae",
     "Shiroma"
    ],
    [
     "Kumiko",
     "Yukawa"
    ]
   ],
   "title": "Analysis of consonants perception of Japanese 22-channel cochlear implant patients",
   "original": "i94_2107",
   "page_count": 4,
   "order": 529,
   "p1": "2107",
   "pn": "2110",
   "abstract": [
    "Consonant confusion matrices were analyzed to deduce transfer ratios of each consonant features for Japanese 22-channel (F0/F1/F2 strategy) cochlear implantees. All patients were post-lingually deafened adults, and average scores of correct response for 14 Japanese consonants were 50% for the cochlear implant alone condition and 85% for the cochlear implant plus lipreading condition. This analysis revealed that average information transfer ratios were the highest for voicing and semivowels and the lowest for nasality in the cochlear implant alone condition, while the ratio was the highest for the place of articulation and the lowest for nasality in the cochlear implant plus lipreading condition. In addition, these results showed that information transfer ratios were improved by 25 to 35%, when aided by lipreading. Multiple regression analysis indicated that nasality and semivowels were more contributing features for consonant recognition with cochlear implant alone, and the place of articulation was most influential under conditions of cochlear implant plus lipreading.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-529"
  },
  "hohmann94_icslp": {
   "authors": [
    [
     "Volker",
     "Hohmann"
    ],
    [
     "Birger",
     "Kollmeier"
    ]
   ],
   "title": "Digital hearing aid techniques employing a loudness model for recruitment compensation",
   "original": "i94_2111",
   "page_count": 4,
   "order": 530,
   "p1": "2111",
   "pn": "2114",
   "abstract": [
    "A multichannel dynamic compression algorithm for use in hearing aids is introduced which uses a loudness estimation model to adapt the frequency-gain characteristic to signals of arbitrary spectral shape and level. The model was incorporated into the algorithm described in [2] and has been compared with an optimally fitted linear system (frequency shaping only) in terms of subjective transmission quality and speech intelligibility in quiet and in noise. The results show an advantage of the proposed algorithm in comparison to linear frequency shaping, especially when the broad dynamic range of signals in everyday life is considered.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-530"
  },
  "nakamura94_icslp": {
   "authors": [
    [
     "Akira",
     "Nakamura"
    ],
    [
     "Nobumasa",
     "Seiyama"
    ],
    [
     "Atsushi",
     "Imai"
    ],
    [
     "Tohru",
     "Takagi"
    ],
    [
     "Eiichi",
     "Miyasaka"
    ]
   ],
   "title": "A new approach to compensate degeneration of speech intelligibility for elderly listeners",
   "original": "i94_2115",
   "page_count": 4,
   "order": 531,
   "p1": "2115",
   "pn": "2118",
   "abstract": [
    "This paper presents a new hearing aid system intended to compensate the degradation of perceptual functions in the central auditory pathways which can be found typically with elderly people, while conventional hearing aid systems are effective only for conductive hearing impairments. A typical type of such functional deterioration is the decrease of rate of processing speed for identification of speech and effective cognitive capacity. The new system is designed to compensate it by slowing the input speaking rate instead of a direct processing of the related auditory functions. This system enables a user to convert a speaking rate as desired by him/her-self on real time, with invariance in pitch as well as small impairments in quality.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-531"
  },
  "mekata94_icslp": {
   "authors": [
    [
     "Tsuyoshi",
     "Mekata"
    ],
    [
     "Yoshiyuki",
     "Yoshizwrti"
    ],
    [
     "Yumiko",
     "Koto"
    ],
    [
     "Etji",
     "Noguchi"
    ],
    [
     "Yoshinori",
     "Yamada"
    ]
   ],
   "title": "Development of a portable multi-function digital hearing aid",
   "original": "i94_2119",
   "page_count": 4,
   "order": 532,
   "p1": "2119",
   "pn": "2122",
   "abstract": [
    "We have developed a portable multi-function digital hearing aid and its fitting system. The size of the hearing aid is 59 x 63 x 26 mm and the weight is 98 grams. The processing speed of its DSP chip is 6.6 MIPS. It works for 20 hours continuously on one Lithium-ion rechargeable battery. We introduced four kinds of basic software modules which can be combined: impulsive sound suppression, 3 channel compression, temporal consonant enhancement and enhancement of spectral contrast By combination of modules, 14 programs are installed in DSP on-chip ROM. The fitting system based on a personal computer measures the residual hearing ability of the hearing impaired and sends fitting parameters to customize the hearing aid. We will evaluate and improve the hearing aid this year.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-532"
  },
  "jamieson94b_icslp": {
   "authors": [
    [
     "Donald G.",
     "Jamieson"
    ]
   ],
   "title": "The use of spoken language in the evaluation of assistive listening devices",
   "original": "i94_2123",
   "page_count": 4,
   "order": 533,
   "p1": "2123",
   "pn": "2126",
   "abstract": [
    "The primary objective of modern approaches to selecting a hearing aid or other assistive listening device for a person is to optimize speech intelligibility. To achieve this objective, researchers and clinicians are required to make certain assumptions about the nature of the speech signal and how speech can and should be used to evaluate hearing aid processing. This paper addresses three aspects of this issue: (1) alternative definitions of \"the speech signal\" as they are applied to establish amplification targets for hearing aid prescription; (2) measurement of the electroacoustic effects of hearing aid processing on speech; and (3) prediction and measurement of the intelligibility of hearing aid processed speech.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-533"
  },
  "gauvain94_icslp": {
   "authors": [
    [
     "Jean-Luc",
     "Gauvain"
    ],
    [
     "Lori F.",
     "Lamel"
    ],
    [
     "Gilles",
     "Adda"
    ],
    [
     "Martine",
     "Adda-Decker"
    ]
   ],
   "title": "Continuous speech dictation in French",
   "original": "i94_2127",
   "page_count": 4,
   "order": 534,
   "p1": "2127",
   "pn": "2130",
   "abstract": [
    "A major research activity at LIMSI is multilingual, speaker-independent, large vocabulary speech dictation. In this paper we report on efforts in large vocabulary, speaker-independent continuous speech recognition of French using the BREF corpus. Recognition experiments were carried out with vocabularies containing up to 20k words. The recognizer makes use of continuous density HMM with Gaussian mixture for acoustic modeling and n-gram statistics estimated on 38 million words of newspaper text from Le Monde for language modeling. The recognizer uses a time-synchronous graph-search strategy. When a bigram language model is used, recognition is carried out in a single forward pass. A second forward pass, which makes use of a word graph generated with the bigram language model, incorporates a trigram language model. Acoustic modeling uses cepstrum-based features, context-dependent phone models and phone duration models. An average phone accuracy of 86% was achieved. A word accuracy of 84% has been obtained for an unrestricted vocabulary test and 95% for a 5k vocabulary test.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-534"
  },
  "cole94c_icslp": {
   "authors": [
    [
     "Ronald",
     "Cole"
    ],
    [
     "Beatrice T.",
     "Oshika"
    ],
    [
     "Mike",
     "Noel"
    ],
    [
     "Terri",
     "Lander"
    ],
    [
     "Mark",
     "Fanty"
    ]
   ],
   "title": "Labeler agreement in phonetic labeling of continuous speech",
   "original": "i94_2131",
   "page_count": 4,
   "order": 535,
   "p1": "2131",
   "pn": "2134",
   "abstract": [
    "This paper analyzes inter-labeler agreement of label choice and boundary placement for human phonetic transcriptions of continuous telephone speech in different languages. In experiment one, English, German, Mandarin and Spanish are labeled by fluent speakers of the languages. In experiment two, German and Hindi are labeled by linguists who do not speak the languages. Experiment two uses a somewhat finer phonetic transcription set than experiment one. We compare the transcriptions of the utterances in terms of the minimum number of substitutions, insertions and deletions needed to map one transcription to the other. Native speakers agree on the average 67.52% of the time at the finest level of labeling, including diacritics. Non-native linguists agree 34.41% of the time. The implications of the results are discussed for evaluation of phonetic recognition algorithms.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-535"
  },
  "juang94_icslp": {
   "authors": [
    [
     "Biing-Hwang",
     "Juang"
    ],
    [
     "Jay G.",
     "Wilpon"
    ]
   ],
   "title": "Recent technology developments in connected digit speech recognition",
   "original": "i94_2135",
   "page_count": 4,
   "order": 536,
   "p1": "2135",
   "pn": "2138",
   "abstract": [
    "The use of automatic speech recognition (ASR) over telephone lines has increased tremendously over the past several years. One of the most important vocabularies for ASR technology today is the digit set. The ability to recognize a string of fluently spoken digits is essential to many applications. Over the past year, our experience with connected digit recognition in real-world settings has steadily increased. We have learned that current connected digit recognition algorithms are not robust to - (1) the increased variation in pronunciation and articulation, and (2) the variety of transducer, transmission and background conditions that exist in the telephone network. Hence, if improvements in recognition performance are to be achieved these issues have to explicitly considered. In this paper, we report results of several directions of research aiming at increasing the robustness and performance of connected digit recognition over the telephone network.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-536"
  },
  "jurafsky94_icslp": {
   "authors": [
    [
     "Daniel",
     "Jurafsky"
    ],
    [
     "Chuck",
     "Wooters"
    ],
    [
     "Gary",
     "Tajchman"
    ],
    [
     "Jonathan",
     "Segal"
    ],
    [
     "Andreas",
     "Stolcke"
    ],
    [
     "Eric",
     "Fosler"
    ],
    [
     "Nelson",
     "Morgan"
    ]
   ],
   "title": "The berkeley restaurant project",
   "original": "i94_2139",
   "page_count": 4,
   "order": 537,
   "p1": "2139",
   "pn": "2142",
   "abstract": [
    "This paper describes the architecture and performance of the Berkeley Restaurant Project (BeRP), a medium-vocabulary, speaker-independent, spontaneous continuous speech understanding system currently under development at ICSI. BeRP serves as a testbed for a number of our speech-related research projects, including robust feature extraction, connectionist phonetic likelihood estimation, automatic induction of multiple-pronunciation lexicons, foreign accent detection and modeling, advanced language models, and lip-reading. In addition, it has proved quite usable in its function as a database frontend, even though many of our subjects are non-native speakers of English.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-537"
  },
  "steinbiss94_icslp": {
   "authors": [
    [
     "Volker",
     "Steinbiss"
    ],
    [
     "Bach-Hiep",
     "Tran"
    ],
    [
     "Hermann",
     "Ney"
    ]
   ],
   "title": "Improvements in beam search",
   "original": "i94_2143",
   "page_count": 4,
   "order": 538,
   "p1": "2143",
   "pn": "2146",
   "abstract": [
    "Time-synchronous beam search has successfully been employed in the Philips continuous-speech recognizer for several years now, handling a vocabulary of 20 000 words and more. We have now improved the search procedure with two robust pruning methods that drastically reduce both average and peak search effort.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-538"
  },
  "johnson94b_icslp": {
   "authors": [
    [
     "Kevin",
     "Johnson"
    ],
    [
     "Roberto",
     "Garigliano"
    ],
    [
     "Russell J.",
     "Collingham"
    ]
   ],
   "title": "Data-based control of the search space generated by multiple knowledge bases for speech recognition",
   "original": "i94_2147",
   "page_count": 4,
   "order": 539,
   "p1": "2147",
   "pn": "2150",
   "abstract": [
    "Automatic speech recognition ( ASR ) systems are made up of a number of different knowledge sources (KSs) which combine to solve the overall problem of speech recognition. An investigation into the benefits of each KS and the benefits of combining the KSs has been undertaken. Test sentences, taken from real data, have been processed by different versions of a speech recognition sub-system. The system performance was measured accurately by identifying the exact location of the required hypothesis (RH) in the hypothesis list (HL), at all cycles in the processing. This paper describes the analysis carried out and presents the results of this analysis and shows that each KS does play a role, though their importance varies considerably. The overall aim of this work is to aid in the development of a speech recognition system being produced at the University of Durham.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-539"
  },
  "kai94_icslp": {
   "authors": [
    [
     "Atsuhiko",
     "Kai"
    ],
    [
     "Seiichi",
     "Nakagawa"
    ]
   ],
   "title": "Evaluation of unknown word processing in a spoken word recognition system",
   "original": "i94_2151",
   "page_count": 4,
   "order": 540,
   "p1": "2151",
   "pn": "2154",
   "abstract": [
    "Detection of an unknown word or non-vocabulary word uttered by the user is necessary in realizing an effective spoken language user-interface. This paper describes the evaluation of an unknown word processing method for a sub word unit based spoken word recognizer. We have assessed the relationship between the word recognition accuracy of a system and the detection rate of unknown words both by simulation and by experiment of the unknown word processing method. We have seen that the resultant detection accuracies using the unknown word processing are significantly influenced by the original word recognition accuracy.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-540"
  },
  "araki94b_icslp": {
   "authors": [
    [
     "Tetsuo",
     "Araki"
    ],
    [
     "Satoru",
     "Ikehara"
    ],
    [
     "Hideto",
     "Yokokawa"
    ]
   ],
   "title": "Using accent information to correctly select Japanese phrases made of strings of syllables",
   "original": "i94_2155",
   "page_count": 4,
   "order": 541,
   "p1": "2155",
   "pn": "2158",
   "abstract": [
    "This paper proposes a method to determine the most suitable string of syllable candidates using 2nd-order Markov model of accent information added to syllable characters, assuming that the correct positions of accents can be obtained from speech by acoustic processing. From the experiment which uses the statistical data for 5 issues of a daily Japanese newspaper, the accuracy rate that the first candidate of syllable strings are correct was shown to be improved by 6.4 %.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-541"
  },
  "young94_icslp": {
   "authors": [
    [
     "Sheryl R.",
     "Young"
    ]
   ],
   "title": "Estimating recognition confidence: methods for conjoining acoustics, semantics, pragmatics and discourse",
   "original": "i94_2159",
   "page_count": 4,
   "order": 542,
   "p1": "2159",
   "pn": "2162",
   "abstract": [
    "This paper describes and evaluates a new technique for measuring confidence in word strings produced by speech recognition systems. It detects misrecognized and out-of-yocabulary words in spontaneous spoken utterances and dialogs using multiple stochastic and symbolic knowledge sources including acoustics, semantics, pragmatics and discourse structure. The work is part of a larger effort to automatically recognize and understand new words when spoken. The system described combines newly developed acoustic confidence measures with the semantic, pragmatic and discourse structure knowledge embodied in the MENDS-n system. The acoustic confidence metrics output independent probabilites that a word is recognized correctly and measure how reliably we can estimate if a word is wrong. The acoustic confidence metrics are derived from normalized acoustic recognition scores. Acoustic scores are normalized by estimates of the denomiator of Bayes equation. To evaluate the utility of using the acoustic techniques together with higher-level constraints, the preliminary system restricted component interaction. Words with normalized acoustic scores that had a 95% or greater probability of being incorrect were flagged prior to being input to the mnds-ii analysis module. For this study, MNDS-n independently used its higher-level knowledge to detect recognition errors that were semantically or contextually inappropriate. Misrecognized word strings were then re-recognized using an RTN-based speech decoder and dynamically derived language model that biases against recognition of illogical and highly improbable content. The dynamically derived grammars restrict the words that can be matched during recognition, reducing perplexity by defining a set of semantic content predictions for the word string. A grammar is derived for each misrecognized word string encountered within an utterance. Speaker goals and plans, contextual appropriateness and structural characteristics of discourse and spontaneous speech are all considered in the derivation of grammars. The results indicate that the conjoined usage of acoustic confidence measures of accuracy and higher-level constraints increased ability to detect misrecognitions by 36% and enabled the larger system to overcomes the weaknesses of the individual techniques. The techniques detect complementary phenomena. The acoustic methods detect important, misrecognized content words. They cannot reliably estimate recognition accuracy for most small or confusible words. The higher-level constraint methods cannot detect contextually consistent misrecogntions, but can detect errors caused by confusible content words, restarts and mid-utterance corrections. Current work focuses upon development of more sophisticated techniques for conjoining these two methods and techniques to use acoustic confidence measures during decoding.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-542"
  },
  "mcdonough94_icslp": {
   "authors": [
    [
     "John",
     "McDonough"
    ],
    [
     "Herbert",
     "Gish"
    ]
   ],
   "title": "Issues in topic identification on the switchboard corpus",
   "original": "i94_2163",
   "page_count": 4,
   "order": 543,
   "p1": "2163",
   "pn": "2166",
   "abstract": [
    "Topic identification (TID) is the automatic classification of speech messages into one of a known set of possible topics. The TID task can be view as having three principal components: 1) event generation, 2) keyword event selection, and 3) topic modeling. Using data from the Switchboard corpus, we present experimental results for various approaches to the TID problem and compare the relative effectiveness of each. In particular, we examine issues in topic modeling and keyword selection.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-543"
  },
  "deng94_icslp": {
   "authors": [
    [
     "L.",
     "Deng"
    ],
    [
     "H.",
     "Sameti"
    ]
   ],
   "title": "Automatic speech recognition using dynamically defined speech units",
   "original": "i94_2167",
   "page_count": 4,
   "order": 544,
   "p1": "2167",
   "pn": "2170",
   "abstract": [
    "Dynamically-defined, sub-phonemic units of speech are proposed and constructed by way of phonological assimilation of one or more major articulatory features. Assimilation of these features spans several phonetic segments, accounting for cross-phone context dependence in speech. A statistical theory is developed that endows these new speech units with computational and parameter-learning capability sufficiently powerful for their use as the primitive units in designing a feature-based automatic speech recognition system. The effectiveness of a feature-based speech recognition system exploiting the dynamically-defined speech units is demonstrated in preliminary evaluation experiments using a standard phonetic recognition task designed from TIMIT database.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-544"
  },
  "jones94_icslp": {
   "authors": [
    [
     "M.",
     "Jones"
    ],
    [
     "Phil C.",
     "Woodland"
    ]
   ],
   "title": "Modelling syllable characteristics to improve a large vocabulary continuous speech recogniser",
   "original": "i94_2171",
   "page_count": 4,
   "order": 545,
   "p1": "2171",
   "pn": "2174",
   "abstract": [
    "The acoustic-phonetic modelling used in state-of-the-art large vocabulary continuous speech recognisers (LVCSR) cannot effectively exploit the prosody based distinctions known to exist at the syllable level. These distinctions are between the strength of the syllable (strong or weak) and the stress (stressed or unstressed) it is given. This paper shows how a small set of syllable-sized Hidden Markov Models (HMMs) can model syllable type effectively. These models have been applied to a large vocabulary continuous speech recogniser and a 23% reduction in word error rate was achieved.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-545"
  },
  "prieto94_icslp": {
   "authors": [
    [
     "Natividad",
     "Prieto"
    ],
    [
     "Emilio",
     "Sanchis"
    ],
    [
     "Luis",
     "Palmero"
    ]
   ],
   "title": "Continuous speech understanding based on automatic learning of acoustic and semantic models",
   "original": "i94_2175",
   "page_count": 4,
   "order": 546,
   "p1": "2175",
   "pn": "2178",
   "abstract": [
    "In this paper, we present a Continuous Speech Understanding (CSU) system directed by semantics, in which all the required knowledge sources are automatically learnt from training data. In particular, we use an inductive learning technique in order to obtain structural models both at the acoustic-phonetic level and the semantic level. The system which we propose assumes that understanding is the ultimate goal of the system performance. Therefore, the search should mainly be constrained by the semantic relations rather than by the word relations of language, allowing for a relaxed syntax. Preliminary experiments have been carried out with a semantic constrained task consisting of the understanding of queries to a database with information about Spanish geography in natural language, using two different system architectures.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-546"
  },
  "kondo94d_icslp": {
   "authors": [
    [
     "Kazuhiro",
     "Kondo"
    ],
    [
     "Yu-Hung",
     "Kao"
    ],
    [
     "Barbara",
     "Wheatley"
    ]
   ],
   "title": "On inter-phrase context dependencies in continuously read Japanese speech",
   "original": "i94_2179",
   "page_count": 4,
   "order": 547,
   "p1": "2179",
   "pn": "2182",
   "abstract": [
    "This paper investigates methods to model inter-phrase or word context for continuous Japanese speech recognition. It was found that by compiling a network of context-dependent phonetic models which models the inter-word or inter-phrase context, recognition error reduction by 32% can be achieved compared to models which do not account for inter-word context. However, this will significantly increase the number of phonetic models required to model the vocabulary. To overcome this increase, we clustered the inter-word/phrase context into only a few classes. Using one class for consonant inter-word context and two classes for vowel context, the recognition accuracy on digit string recognition was found to be virtually equal to the accuracy with unclustered models, while the number of phonetic models required was reduced by more than 50%.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-547"
  },
  "fink94_icslp": {
   "authors": [
    [
     "Gernot A.",
     "Fink"
    ],
    [
     "Franz",
     "Kummert"
    ],
    [
     "Gerhard",
     "Sagerer"
    ]
   ],
   "title": "A close high-level interaction scheme for recognition and interpretation of speech",
   "original": "i94_2183",
   "page_count": 4,
   "order": 548,
   "p1": "2183",
   "pn": "2186",
   "abstract": [
    "The vast majority of speech understanding systems suffers from a bottleneck between the recognition and the interpretation components. Normally, only a relatively small set of word hypotheses is passed from the recognizer and no flow of information in the opposite direction is even possible. We propose an interaction scheme that tries to overcome many of the disadvantages of traditional systems. It makes use of the possibility to process abstract constituents in our word recognizer and pass them back as complex hypotheses. Predictions that define the complex analysis goal of the recognition can be derived dynamically during the interpretation of an utterance. A left-to-right processing in both recognition and interpretation makes an incremental analysis possible.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-548"
  },
  "costemarquis94_icslp": {
   "authors": [
    [
     "Sylvie",
     "Coste-Marquis"
    ]
   ],
   "title": "Interaction between most reliable acoustic cues and lexical analysis",
   "original": "i94_2187",
   "page_count": 4,
   "order": 549,
   "p1": "2187",
   "pn": "2190",
   "abstract": [
    "A model for lexical analysis based on reliable information stemming from the acoustic-phonetic decoder is presented in this paper. Reliable information is conveyed by some of the acoustic cues used for the acoustic-phonetic decoding which are both particularly discriminant and well pronounced at the time of realization. Lexical access and lexical analysis we put forward also aim at maintaining the consistency of the overall process keeping track of the context in which identification hypotheses have been made. We describe the lexical module of the system DAPHNE designed for French sentences made of stops and vowels. We also present acoustic cues used on stops followed by back vowels and give preliminary results obtained on these sounds.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-549"
  },
  "ariki94b_icslp": {
   "authors": [
    [
     "Y.",
     "Ariki"
    ],
    [
     "T.",
     "Kawamura"
    ]
   ],
   "title": "Simultaneous spotting of phonemes and words in continuous speech",
   "original": "i94_2191",
   "page_count": 4,
   "order": 550,
   "p1": "2191",
   "pn": "2194",
   "abstract": [
    "In word spotting on continuous speech, powerful discrimination of known words from unknown words is required. In this paper, two word spotting techniques are proposed which discriminate known words using phoneme information obtained through Viterbi decoding. One is to select known words at every frame and extract a sequence of known words and phonemes by backtrace. The other is to locally extract known words without backtrace.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-550"
  },
  "siu94_icslp": {
   "authors": [
    [
     "Man-hung",
     "Siu"
    ],
    [
     "Herbert",
     "Gish"
    ],
    [
     "Robin",
     "Rohlicek"
    ]
   ],
   "title": "Predicting word spotting performance",
   "original": "i94_2195",
   "page_count": 4,
   "order": 551,
   "p1": "2195",
   "pn": "2198",
   "abstract": [
    "To use a word spotting system efficiently, it is helpful to be able to predict the performance of the system accurately. In this paper, we investigate performance prediction under different conditions. First, we discuss how to use statistical techniques to predict performance, and its variability on new unseen testing data. Second, we show that classification trees can be used to estimate the posterior probability of putative hits and that posterior probability can predict performance of unlabeled test data. Thirdly, we show that the classification tree method can generalize to predict spotting performance on new keywords.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-551"
  },
  "ohno94_icslp": {
   "authors": [
    [
     "Sumio",
     "Ohno"
    ],
    [
     "Hiroya",
     "Fujisaki"
    ],
    [
     "Keikichi",
     "Hirose"
    ]
   ],
   "title": "A method for word spotting in continuous speech using both segmental and contextual likelihood scores",
   "original": "i94_2199",
   "page_count": 4,
   "order": 552,
   "p1": "2199",
   "pn": "2202",
   "abstract": [
    "In conventional systems for word spotting or speech recognition, individual frames or segments of the input speech are assigned labels and local likelihood scores solely on the basis of their own acoustic characteristics. On the other hand, human perception of speech segments depends heavily on their acoustic and linguistic context. The present paper presents a new method of word spotting in continuous speech based on template matching where the likelihood score of each segment of a word is determined not only by its own characteristics but also by the likelihood of its context within the frame-work of a word. The advantage of the proposed method over conventional methods is demonstrated by a word-spotting experiment on a limited number of samples of connected speech of Japanese.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-552"
  },
  "mori94_icslp": {
   "authors": [
    [
     "Renato De",
     "Mori"
    ],
    [
     "Diego",
     "Giuliani"
    ],
    [
     "Roberto",
     "Gretter"
    ]
   ],
   "title": "Phone-based prefiltering for continuous speech recognition",
   "original": "i94_2203",
   "page_count": 4,
   "order": 553,
   "p1": "2203",
   "pn": "2206",
   "abstract": [
    "An architecture for speech recognition is proposed, based on four stages: (1) recognition of the most likely phone sequence using centisecond Hidden Markov Models (HMMs); (2) phone-based lexical and syntactical forward decoding; (3) A*phone-based backward pass, producing a Word Hypothesis Structure (WHS); (4) accurate rescoring of the search sub-space represented by the WHS using centisecond HMMs. Experiments carried out on two different tasks show that a recognizer based on the proposed four-stage architecture is able to achieve comparable performance respect to a classic one-stage recognizer. Experimental results show also that the same recognition performance can be obtained with WHSs built with this approach and WHSs built using centisecond HMMs with a potential speed-up, in WHS generation, proportional to the average phoneme duration in centiseconds.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-553"
  },
  "singer94_icslp": {
   "authors": [
    [
     "Harold",
     "Singer"
    ],
    [
     "Jun-ichi",
     "Takami"
    ]
   ],
   "title": "Speech recognition without grammar or vocabulary constraints",
   "original": "i94_2207",
   "page_count": 4,
   "order": 554,
   "p1": "2207",
   "pn": "2210",
   "abstract": [
    "Out-of-vocabulary words and ungrammatical utterances are two major problems in speech recognition. We believe that improving the acoustic model is essential in dealing with these problems. We propose to use a 'phonetic typewriter' as an evaluation method. Unlike common approaches, which evaluate acoustic and language model together, this allows direct evaluation of the acoustic model. A comparison of context-independent phone models based on continuous mixture HMM (20 mixtures per state) with context-dependent phone models based on HMnet[4] (3 mixtures per state) showed that phoneme error rate can be halved by using the latter models. The same 'phonetic typewriter' paradigm can also be used directly as a speech recognition method, in which speech is recognized as a string of phonemes without constraints on vocabulary or grammar. We show that over 97 % phoneme recognition accuracy can be achieved if our best acoustic model is used.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-554"
  },
  "maciasguarasa94b_icslp": {
   "authors": [
    [
     "Javier",
     "Macias-Guarasa"
    ],
    [
     "Manuel A.",
     "Leandro"
    ],
    [
     "Xavier",
     "Menendez-Pidal"
    ],
    [
     "Jose",
     "Colas"
    ],
    [
     "Ascension",
     "Gallardo"
    ],
    [
     "Jose M.",
     "Pardo"
    ],
    [
     "Santiago",
     "Aguilera"
    ]
   ],
   "title": "Comparison of three approaches to phonetic string generation for large vocabulary speech recognition",
   "original": "i94_2211",
   "page_count": 4,
   "order": 555,
   "p1": "2211",
   "pn": "2214",
   "abstract": [
    "We are building a large vocabulary, isolated word preselection system according to a bottom-up design strategy. It will be used in the development of a dictation machine for Spanish and it is composed of three main modules: feature extraction, phonetic string build up and lexical access. In the second one, we are considering three different technological approaches based on static modeling (SM), Hidden Markov Models (HMM) and Neural Networks (NN). This paper will compare these three alternatives in terms of recognition performance, training complexity and computational load, and will conclude with the results of the comparison in order to adopt the most suitable approach depending on the task.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-555"
  },
  "laface94_icslp": {
   "authors": [
    [
     "Pietro",
     "Laface"
    ],
    [
     "Lorenzo",
     "Fissore"
    ],
    [
     "F.",
     "Ravera"
    ]
   ],
   "title": "Automatic generation of words toward flexible vocabulary isolated word recognition",
   "original": "i94_2215",
   "page_count": 4,
   "order": 556,
   "p1": "2215",
   "pn": "2218",
   "abstract": [
    "The paper deals with flexible and very large vocabulary isolated word recognition systems. In particular it discusses two main topics referring to a speaker independent isolated word recognizer: the evaluation of the recognizer performance with vocabularies of different size and with different entries, and the generation of \"artificial\" word utterances for fast lexical access to very large vocabularies.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-556"
  },
  "choi94_icslp": {
   "authors": [
    [
     "H. C.",
     "Choi"
    ],
    [
     "R. W.",
     "King"
    ]
   ],
   "title": "Fast speaker adaptation through spectral transformation for continuous speech recognition",
   "original": "i94_2219",
   "page_count": 4,
   "order": 557,
   "p1": "2219",
   "pn": "2222",
   "abstract": [
    "In this paper, the use of spectral transformation to perform speaker adaptation for continuous speech recognition is presented. Speaker adaptation is performed by transforming the feature vectors of the speech of a new speaker to the spectral space of a reference speaker who has participated in the training of a reference recognition system. The same reference recognition system is used for all new speakers but the individual transformation for each new speaker is estimated accordingly by canonical correlation analysis (CCA). Since this approach does not require any modifications to the reference system, itis especially attractive for the development of on-line adaptive recognition systems which require fast adaptation. The effectiveness of this approach is investigated by performing adaptation experiments using the DARPA 1000-word Resource Management (RM1) continuous speech corpus.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-557"
  },
  "datta94_icslp": {
   "authors": [
    [
     "Sekharjit",
     "Datta"
    ]
   ],
   "title": "Dynamic machine adaptation in a multi-speaker isolated word recognition system",
   "original": "i94_2223",
   "page_count": 4,
   "order": 558,
   "p1": "2223",
   "pn": "2226",
   "abstract": [
    "This paper investigates a dynamic adaptation technique applied to a real-time isolated word recognition system which continually adapts to the speaker's characteristics and improves the performance of the system. The technique initially allows the system to compensate for inadequate training of the classifier and then continues to track changes in the speaker's voice. The overall results show that a recognition rate of over 96% is achievable when tested on 12 unknown speakers. The study also reveals that the crucial factors in a dynamically adaptive speech recognition are: (a) knowing when to adapt and (b) deciding by how much to adapt.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-558"
  },
  "young94b_icslp": {
   "authors": [
    [
     "Sheryl R.",
     "Young"
    ]
   ],
   "title": "Discourse structure for spontaneous spoken interactions: multi-speaker vs. human-computer dialogs",
   "original": "i94_2227",
   "page_count": 4,
   "order": 559,
   "p1": "2227",
   "pn": "2230",
   "abstract": [
    "In real spoken language applications, speakers interact spontaneously and frequently diverge from the task at hand by initiating various types of sub-dialogs. Multi-speaker cooperative problem-solving dialogs evidence significantly more spontaneous phenomena than human-computer interactions. We claim that unconstrained, task-oriented spontaneous spoken dialog is structured and predictable in spite of such phenomena.\n",
    "The discourse structure observed for any specific dialog is derived from the structure of the task, contextual constraints derived from prior interaction and the characteristics of a finite set of domain-independent discourse plans for communicating, including subdialogs and topic changes. There are oasically four algorithms for traversing domain plan trees and computing, for each potential application, the amount of perplexity reduction that will be achieved by applying these algorithms. However, when people interact spontaneously and speak freely, they often digress from strict verbal problem solving. To account for these behaviors, we modify our algorithms to allow for discourse structure effects including subdialog and topic change behaviors. The new algorithms maintain our abilities to constrain recognition and interpretation of spontaneous spoken utterances and account for subdialog phenomena as well as the meta-planning and initiative issues found in multiple speaker dialogs. This paper focuses on the distinctions in discourse structure that appear when two persons try to jointly and cooperatively solve a problem verbally, in contrast to a human - computer interaction. Multi-speaker, mutual problem solving dialogs exhibit significant initiative-based effects, where initiative for accomplishing the task can vary from utterance to utterance. Initiative is constrained in human-computer interaction - usually only one participant has the capability to initiate solutions to problems. Similarly, meta-planning or discussions of general problem solving constraints and ordering of problem solving or plan steps is only observed in multiple speaker interactions. We describe each of these, our algorithms for processing the phenomena and the resulting constraints that result from exploiting the structural regularities in multi-speaker, cooperative problem solving dialogs. The basic model of discourse structure and plan recognition for spontaneous spoken dialog has been implemented and evaluated on a 10,000 utterance corpora in the ARPA ATIS domain, a 3,000 utterance test corpora in a lunch-ordering domain and are being applied to a 700 dialog meeting scheduling application. The model dynamically constrains a speech recognizer, simplifies the process of inferring meaning from a spontaneous spoken utterance and accounts for the subdialojg phenomena observed. We describe these discourse plans, constraints on their occurrence and content, and their representation and processing. The model processes all subdialog phenomena using a domain plan tree, a current focus stack and a set of domain tree traversal algorithms. This paper describes a set of domain independent discourse structure algorithms for spontaneous spoken interaction. The paper overviews algorithms for traversing domain trees, or the set of potential plans that can be executed to solve problems in a specific application domain. It enumerates types of discourse plans and specifies how they interact with domain plans. The interaction results in constraints upon when specific types of discourse plans can occur and constraints upon their content. The paper then focuses upon unique properties of multi-speaker discourse.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-559"
  },
  "mixdorff94_icslp": {
   "authors": [
    [
     "Hansjörg",
     "Mixdorff"
    ],
    [
     "Hiroya",
     "Fujisaki"
    ]
   ],
   "title": "Analysis of voice fundamental frequency contours of German utterances using a quantitative model",
   "original": "i94_2231",
   "page_count": 4,
   "order": 560,
   "p1": "2231",
   "pn": "2234",
   "abstract": [
    "In German, as in many other languages, the fundamental frequency contour (henceforth the F0 contour) is an important acoustic correlate of intonation. The present paper adopts a quantitative model originally developed for Japanese to analyze German declarative sentences with statement intonation. As far as the current data is concerned the analysis proves that the model is directly applicable to German. The position of the accent command assigned to an accentable constituent depends on whether the item exhibits a terminal or non-terminal accentuation. Phrasing is found to occur at the boundary between clauses but also after larger noun phrases, for instance.\n",
    ""
   ],
   "doi": "10.21437/ICSLP.1994-560"
  }
 },
 "sessions": [
  {
   "title": "Plenary Lectures",
   "papers": [
    "lehiste94_icslp",
    "hiki94_icslp",
    "levelt94_icslp"
   ]
  },
  {
   "title": "Integration of Speech and Natural Language Processing",
   "papers": [
    "takezawa94_icslp",
    "kim94_icslp",
    "wallerstein94_icslp",
    "kita94_icslp",
    "plannerer94_icslp",
    "brown94_icslp",
    "nagao94_icslp",
    "edmondson94_icslp"
   ]
  },
  {
   "title": "Articulatory Motion",
   "papers": [
    "erickson94_icslp",
    "lee94_icslp",
    "ostry94_icslp",
    "tiede94_icslp",
    "stone94_icslp",
    "hoole94_icslp",
    "hawkins94_icslp",
    "kondo94_icslp"
   ]
  },
  {
   "title": "Cognitive Models for Spoken Language Processing",
   "papers": [
    "jusczyk94_icslp",
    "norris94_icslp"
   ]
  },
  {
   "title": "Semantic Interpretation of Spoken Messages",
   "papers": [
    "kuhn94_icslp",
    "gorin94_icslp",
    "ward94_icslp",
    "kameyama94_icslp",
    "shirotsuka94_icslp",
    "nagai94_icslp",
    "shimazu94_icslp",
    "okada94_icslp",
    "eckert94_icslp",
    "kanazawa94_icslp"
   ]
  },
  {
   "title": "Prosody",
   "papers": [
    "kieling94_icslp",
    "maekawa94_icslp",
    "pitrelli94_icslp",
    "todd94_icslp",
    "kakita94_icslp",
    "lyberg94_icslp",
    "behne94_icslp",
    "takagi94_icslp",
    "ichikawa94_icslp"
   ]
  },
  {
   "title": "Towards Natural Sounding Synthetic Speech",
   "papers": [
    "karlsson94_icslp",
    "strik94_icslp",
    "ding94_icslp",
    "badin94_icslp",
    "miki94_icslp",
    "suzuki94_icslp",
    "honda94_icslp",
    "honda94b_icslp",
    "stevens94_icslp"
   ]
  },
  {
   "title": "Statistical Methods for Speech Recognition",
   "papers": [
    "minematsu94_icslp",
    "osaka94_icslp",
    "wakita94_icslp",
    "hayamizu94_icslp",
    "hu94_icslp",
    "wu94_icslp",
    "wang94_icslp",
    "parris94_icslp",
    "wolfertstetter94_icslp",
    "watanabe94_icslp",
    "roe94_icslp",
    "zhao94_icslp",
    "takara94_icslp",
    "johansen94_icslp",
    "sun94_icslp",
    "mari94_icslp",
    "chen94_icslp",
    "gales94_icslp",
    "puel94_icslp",
    "katoh94_icslp",
    "takahashi94_icslp",
    "beyerlein94_icslp",
    "cremelie94_icslp",
    "wesenick94_icslp",
    "jouvet94_icslp",
    "windheuser94_icslp",
    "afify94_icslp",
    "lubensky94_icslp",
    "takeda94_icslp",
    "yi94_icslp",
    "lin94_icslp"
   ]
  },
  {
   "title": "Phonetics & Phonology I, II",
   "papers": [
    "kondo94b_icslp",
    "tanaka94_icslp",
    "potapova94_icslp",
    "jun94_icslp",
    "hata94_icslp",
    "itoh94_icslp",
    "jun94b_icslp",
    "gussenhoven94_icslp",
    "belotelgrenie94_icslp",
    "nakai94_icslp",
    "derwing94_icslp",
    "fujisaki94_icslp",
    "farnetani94_icslp",
    "grover94_icslp",
    "kurisu94_icslp",
    "sagawa94_icslp",
    "cleirigh94_icslp",
    "reddy94_icslp",
    "mcrobbieutasi94_icslp",
    "sekimoto94_icslp",
    "muranaka94_icslp",
    "ran94_icslp",
    "delogu94_icslp",
    "heuft94_icslp",
    "huang94_icslp",
    "usuki94_icslp",
    "junqua94_icslp",
    "coile94_icslp",
    "kohler94_icslp",
    "jun94c_icslp",
    "yoon94_icslp",
    "ohala94_icslp",
    "ohala94b_icslp",
    "ingram94_icslp",
    "ghitza94_icslp",
    "akagi94_icslp",
    "rossi94_icslp",
    "abubakar94_icslp",
    "hardcastle94_icslp",
    "fujimura94_icslp",
    "slamacazacu94_icslp"
   ]
  },
  {
   "title": "Adaption and Training for Speech Recognition",
   "papers": [
    "tsurumi94_icslp",
    "shinoda94_icslp",
    "chou94_icslp",
    "shen94_icslp",
    "fissore94_icslp",
    "leggetter94_icslp",
    "ohkura94_icslp",
    "strom94_icslp",
    "chiang94_icslp",
    "nakahashi94_icslp",
    "kong94_icslp",
    "torre94_icslp"
   ]
  },
  {
   "title": "Science and Technology for Multimodal Interfaces",
   "papers": [
    "vatikiotisbateson94_icslp",
    "massaro94_icslp",
    "kondo94c_icslp",
    "kuhl94_icslp",
    "pentland94_icslp",
    "duchnowski94_icslp",
    "oviatt94_icslp",
    "berkley94_icslp",
    "bertelson94_icslp",
    "junqua94b_icslp",
    "ando94_icslp",
    "lokenkim94_icslp"
   ]
  },
  {
   "title": "Measurements and Models of Speech Production",
   "papers": [
    "rose94_icslp",
    "kaburagi94_icslp",
    "george94_icslp",
    "hirayama94_icslp",
    "hiraike94_icslp",
    "jospa94_icslp",
    "pelorson94_icslp",
    "dang94_icslp",
    "motoki94_icslp",
    "schoentgen94_icslp",
    "valimaki94_icslp",
    "matsumura94_icslp",
    "yang94_icslp",
    "narayanan94_icslp",
    "vatikiotisbateson94b_icslp",
    "matsuzaki94_icslp",
    "kakita94b_icslp",
    "ikeda94_icslp",
    "dickson94_icslp",
    "erickson94b_icslp",
    "matsumura94b_icslp",
    "ogata94_icslp",
    "masaki94_icslp"
   ]
  },
  {
   "title": "Applications of Spoken Language Processing",
   "papers": [
    "wilpon94_icslp",
    "nitta94_icslp",
    "hirokawa94_icslp",
    "mazor94_icslp",
    "cole94_icslp",
    "tsuboi94_icslp",
    "dymetman94_icslp",
    "grajski94_icslp",
    "noguchi94_icslp",
    "oerder94_icslp",
    "goddeau94_icslp"
   ]
  },
  {
   "title": "Speech Synthesis I, II",
   "papers": [
    "horne94_icslp",
    "black94_icslp",
    "santen94_icslp",
    "fukada94_icslp",
    "boeffard94_icslp",
    "ando94b_icslp",
    "konst94_icslp",
    "williams94_icslp",
    "doi94_icslp",
    "sakurai94_icslp",
    "dyhr94_icslp",
    "takahashi94b_icslp",
    "vorstermans94_icslp",
    "ishikawa94_icslp",
    "klavans94_icslp",
    "portele94_icslp",
    "lacheretdujour94_icslp",
    "trancoso94_icslp",
    "ao94_icslp",
    "hakoda94_icslp",
    "ljungqvist94_icslp",
    "shiga94_icslp",
    "koopmansvanbeinum94_icslp"
   ]
  },
  {
   "title": "New Approach for Brain Function Research in Speech Perception and Production/",
   "papers": [
    "patterson94_icslp",
    "iwata94_icslp",
    "boatman94_icslp",
    "rugg94_icslp",
    "franklin94_icslp",
    "uno94_icslp"
   ]
  },
  {
   "title": "Language Modeling for Speech Recognition",
   "papers": [
    "brown94b_icslp",
    "ward94b_icslp",
    "kawabata94_icslp",
    "yamaguchi94_icslp",
    "nagata94_icslp",
    "antoine94_icslp",
    "ratnaparkhi94_icslp",
    "kiyama94_icslp",
    "sakamoto94_icslp",
    "kawahara94_icslp",
    "murakami94_icslp",
    "yamada94_icslp",
    "chase94_icslp",
    "suhm94_icslp",
    "mccandless94_icslp",
    "fujio94_icslp",
    "giachin94_icslp",
    "woszczyna94_icslp",
    "bordel94_icslp",
    "shih94_icslp",
    "antoniol94_icslp",
    "jacob94_icslp",
    "jardino94_icslp"
   ]
  },
  {
   "title": "Models and Systems for Spoken Dialogue",
   "papers": [
    "amano94_icslp",
    "bernsen94_icslp",
    "ehsani94_icslp",
    "araki94_icslp",
    "yoshioka94_icslp",
    "terry94_icslp",
    "sakai94_icslp",
    "watanuki94_icslp",
    "arai94_icslp",
    "katunobu94_icslp",
    "flammia94_icslp",
    "kikui94_icslp",
    "larsen94_icslp",
    "naito94_icslp",
    "siroux94_icslp",
    "kamei94_icslp",
    "kogure94_icslp",
    "hildebrandt94_icslp",
    "kumamoto94_icslp",
    "hockey94_icslp",
    "schang94_icslp",
    "kawamori94_icslp",
    "ishikawa94b_icslp",
    "cozannet94_icslp",
    "brietzmann94_icslp",
    "yamashita94_icslp",
    "nakazato94_icslp",
    "ferrieux94_icslp",
    "glass94_icslp"
   ]
  },
  {
   "title": "Speech Recognition in Adverse Environments",
   "papers": [
    "mokbel94_icslp",
    "takahashi94c_icslp",
    "chang94_icslp",
    "suzuki94b_icslp",
    "hansen94_icslp",
    "matsumoto94_icslp",
    "lee94b_icslp",
    "paliwal94_icslp",
    "hamme94_icslp",
    "takagi94b_icslp",
    "stern94_icslp",
    "siohan94_icslp"
   ]
  },
  {
   "title": "Speech Analysis",
   "papers": [
    "cairns94_icslp",
    "moakes94_icslp",
    "tokuda94_icslp",
    "gransden94_icslp",
    "kajita94_icslp",
    "murthy94_icslp",
    "hong94_icslp",
    "schoentgen94b_icslp",
    "hess94_icslp",
    "cheveigne94_icslp",
    "mitome94_icslp"
   ]
  },
  {
   "title": "Prosody of Discourse and Dialogue",
   "papers": [
    "kiritani94_icslp",
    "cao94_icslp",
    "cedergren94_icslp",
    "lee94c_icslp",
    "umeda94_icslp",
    "bruce94_icslp",
    "fujisaki94b_icslp",
    "takeda94b_icslp",
    "campbell94_icslp",
    "bakenecker94_icslp",
    "hunt94_icslp",
    "hirose94_icslp"
   ]
  },
  {
   "title": "Spoken Language Cognition and Its Disorders",
   "papers": [
    "wydell94_icslp",
    "ciocca94_icslp",
    "vroomen94_icslp",
    "mcqueen94_icslp",
    "halle94_icslp",
    "venditti94_icslp",
    "minematsu94b_icslp",
    "hashimoto94_icslp",
    "franklin94b_icslp",
    "masukata94_icslp",
    "dempster94_icslp",
    "traunmuller94_icslp",
    "magnuson94_icslp",
    "hasegawa94_icslp",
    "kitamura94_icslp",
    "markham94_icslp",
    "gibbon94_icslp",
    "ujihira94_icslp",
    "jamieson94_icslp",
    "nakakoshi94_icslp",
    "suzuki94c_icslp",
    "matsubara94_icslp",
    "locastro94_icslp",
    "nara94_icslp",
    "kohno94_icslp",
    "yamada94b_icslp",
    "kaneko94_icslp",
    "katoh94b_icslp",
    "bhaskararao94_icslp"
   ]
  },
  {
   "title": "Spoken Language Systems and Assessments",
   "papers": [
    "giuliani94_icslp",
    "lin94b_icslp",
    "rayner94_icslp",
    "jo94_icslp",
    "bayya94_icslp",
    "seto94_icslp",
    "angelini94_icslp",
    "bennacef94_icslp",
    "lindberg94_icslp",
    "minowa94_icslp",
    "kamio94_icslp",
    "nishirnoto94_icslp",
    "arai94b_icslp",
    "gomez94_icslp",
    "mcnair94_icslp",
    "fay94_icslp",
    "love94_icslp",
    "mariniak94_icslp",
    "wu94b_icslp",
    "jekosch94_icslp",
    "hegehofer94_icslp",
    "rodellar94_icslp",
    "watanabe94b_icslp",
    "ozeki94_icslp",
    "hanai94_icslp",
    "maciasguarasa94_icslp",
    "ohshima94_icslp"
   ]
  },
  {
   "title": "Large Vocabulary/Speaker Independent Speech",
   "papers": [
    "valtchev94_icslp",
    "ney94_icslp",
    "phillips94_icslp",
    "wooters94_icslp",
    "normandin94_icslp",
    "yang94b_icslp",
    "kosaka94_icslp",
    "kimura94_icslp",
    "masai94_icslp",
    "koo94_icslp",
    "angelini94b_icslp"
   ]
  },
  {
   "title": "Perception and Structure of Spoken Language",
   "papers": [
    "patterson94b_icslp",
    "kawahara94b_icslp",
    "nakatani94_icslp",
    "cutler94_icslp",
    "kakehi94_icslp",
    "amano94b_icslp",
    "radeau94_icslp",
    "yanagida94_icslp",
    "otake94_icslp",
    "smeele94_icslp",
    "shigeno94_icslp"
   ]
  },
  {
   "title": "Voice Quality",
   "papers": [
    "karlsson94b_icslp",
    "pols94_icslp",
    "ofuka94_icslp",
    "fant94_icslp",
    "abe94_icslp",
    "kasuya94_icslp",
    "furui94_icslp",
    "imaizumi94_icslp",
    "krom94_icslp",
    "esling94_icslp"
   ]
  },
  {
   "title": "Neural Network and Connectionist Approaches",
   "papers": [
    "na94_icslp",
    "gorin94b_icslp",
    "ambikairajah94_icslp",
    "renals94_icslp",
    "verhasselt94_icslp",
    "hochberg94_icslp",
    "yu94_icslp",
    "gurgen94_icslp",
    "cheng94_icslp",
    "menendezpidal94_icslp",
    "mari94b_icslp",
    "konig94_icslp",
    "chen94b_icslp",
    "okawa94_icslp",
    "monte94_icslp",
    "favero94_icslp",
    "hattori94_icslp",
    "cerf94_icslp",
    "koto94_icslp",
    "cooke94_icslp",
    "haffner94_icslp",
    "iso94_icslp",
    "xu94_icslp",
    "koizumi94_icslp",
    "rodellar94b_icslp",
    "aikawa94_icslp"
   ]
  },
  {
   "title": "Speech Analysis and Enhancement",
   "papers": [
    "aritsuka94_icslp",
    "kunieda94_icslp",
    "yoshida94_icslp",
    "seymour94_icslp",
    "iwahashi94_icslp",
    "tang94_icslp",
    "yasukawa94_icslp",
    "le94_icslp",
    "fineberg94_icslp",
    "alku94_icslp",
    "nishi94_icslp",
    "andersen94_icslp",
    "horita94_icslp",
    "endo94_icslp",
    "johnson94_icslp",
    "soquet94_icslp",
    "osaka94b_icslp",
    "wrench94_icslp",
    "salavedra94_icslp",
    "yamaguchi94b_icslp",
    "iles94_icslp",
    "kvale94_icslp",
    "tomokiyo94_icslp",
    "yumoto94_icslp",
    "wang94b_icslp",
    "lindstrom94_icslp",
    "carvalho94_icslp",
    "kitazawa94_icslp"
   ]
  },
  {
   "title": "Acquisition of Spoken Language",
   "papers": [
    "tsushima94_icslp",
    "kojima94_icslp",
    "shimura94_icslp",
    "ito94_icslp",
    "rao94_icslp",
    "nienart94_icslp"
   ]
  },
  {
   "title": "Education of Spoken Language",
   "papers": [
    "mochizukisudo94_icslp",
    "maeda94_icslp",
    "tsumaki94_icslp",
    "miura94_icslp",
    "kikuchi94_icslp",
    "nishinuma94_icslp"
   ]
  },
  {
   "title": "Speech/Language Database",
   "papers": [
    "morimoto94_icslp",
    "lamel94_icslp",
    "kudo94_icslp",
    "damhuis94_icslp",
    "rosenbeck94_icslp",
    "tapias94_icslp",
    "cole94b_icslp",
    "kenne94_icslp",
    "lin94c_icslp",
    "kassel94_icslp",
    "tanaka94b_icslp"
   ]
  },
  {
   "title": "Speaker, Language and Phoneme Recognition",
   "papers": [
    "rosenberg94_icslp",
    "raman94_icslp",
    "parris94b_icslp",
    "hernando94_icslp",
    "he94_icslp",
    "chen94c_icslp",
    "ariki94_icslp",
    "yun94_icslp",
    "yegnanarayana94_icslp",
    "goldenthal94_icslp",
    "blomberg94_icslp",
    "kadambe94_icslp",
    "hazen94_icslp",
    "ramesh94_icslp",
    "berkling94_icslp",
    "reyes94_icslp",
    "itahashi94_icslp",
    "dalsgaard94_icslp",
    "hamme94b_icslp",
    "okawa94b_icslp",
    "hoshimi94_icslp",
    "hubener94_icslp",
    "mokhtari94_icslp",
    "nadeu94_icslp",
    "arons94_icslp",
    "li94_icslp",
    "wong94_icslp"
   ]
  },
  {
   "title": "Speech Perception and Speech Related Disorders",
   "papers": [
    "morgan94_icslp",
    "tatsumi94_icslp",
    "cheesman94_icslp",
    "toshio94_icslp",
    "javkin94_icslp",
    "holton94_icslp",
    "imaizumi94b_icslp",
    "tsuzaki94_icslp",
    "craig94_icslp",
    "kato94_icslp",
    "zhu94_icslp",
    "fukuda94_icslp",
    "endo94b_icslp",
    "hosoi94_icslp",
    "minifie94_icslp",
    "ikeda94b_icslp",
    "yamada94c_icslp",
    "veda94_icslp",
    "sarabasa94_icslp",
    "rooney94_icslp",
    "yamada94d_icslp",
    "nagayama94_icslp",
    "slawinski94_icslp",
    "naito94b_icslp",
    "aikawa94b_icslp",
    "kitazawa94b_icslp",
    "kashino94_icslp",
    "house94_icslp",
    "nejime94_icslp"
   ]
  },
  {
   "title": "Speech Coding",
   "papers": [
    "funaki94_icslp",
    "ferrerballester94_icslp",
    "ohmuro94_icslp",
    "kim94b_icslp",
    "rao94b_icslp",
    "yao94_icslp",
    "chan94_icslp",
    "koishida94_icslp",
    "jean94_icslp"
   ]
  },
  {
   "title": "The Impact of Signal Processing Technologies",
   "papers": [
    "levitt94_icslp",
    "asano94_icslp",
    "blarney94_icslp",
    "funasaka94_icslp",
    "hohmann94_icslp",
    "nakamura94_icslp",
    "mekata94_icslp",
    "jamieson94b_icslp"
   ]
  },
  {
   "title": "Continuous Speech Recognition",
   "papers": [
    "gauvain94_icslp",
    "cole94c_icslp",
    "juang94_icslp",
    "jurafsky94_icslp",
    "steinbiss94_icslp",
    "johnson94b_icslp",
    "kai94_icslp",
    "araki94b_icslp",
    "young94_icslp",
    "mcdonough94_icslp",
    "deng94_icslp",
    "jones94_icslp",
    "prieto94_icslp",
    "kondo94d_icslp",
    "fink94_icslp",
    "costemarquis94_icslp",
    "ariki94b_icslp",
    "siu94_icslp",
    "ohno94_icslp",
    "mori94_icslp",
    "singer94_icslp",
    "maciasguarasa94b_icslp",
    "laface94_icslp",
    "choi94_icslp",
    "datta94_icslp",
    "young94b_icslp",
    "mixdorff94_icslp"
   ]
  }
 ],
 "doi": "10.21437/ICSLP.1994"
}