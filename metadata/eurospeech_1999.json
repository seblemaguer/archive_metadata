{
 "title": "6th European Conference on Speech Communication and Technology (Eurospeech 1999)",
 "location": "Budapest, Hungary",
 "startDate": "5/9/1999",
 "endDate": "9/9/1999",
 "chair": "General Chair: Géza Gordos",
 "conf": "Eurospeech",
 "year": "1999",
 "name": "eurospeech_1999",
 "series": "Eurospeech",
 "SIG": "",
 "title1": "6th European Conference on Speech Communication and Technology",
 "title2": "(Eurospeech 1999)",
 "date": "5-9 September 1999",
 "booklet": "eurospeech_1999.pdf",
 "papers": {
  "jelinek99_eurospeech": {
   "authors": [
    [
     "Frederick",
     "Jelinek"
    ],
    [
     "Ciprian",
     "Chelba"
    ]
   ],
   "title": "Putting language into language modeling",
   "original": "e99_kn01",
   "page_count": 5,
   "order": 1,
   "p1": "keynote paper 1",
   "pn": "",
   "abstract": [
    "In this paper we describe the statistical Structured Language Model (SLM) that uses grammatical analysis of the hypothesized sentence segment (prefix) to predict the next word. We first describe the operation of a basic, completely lexicalized SLM that builds up partial parses as it proceeds left to right. We then develop a chart parsing algorithm and with its help a method to compute the prediction probabilities P(wi+1jWi): We suggest useful computational shortcuts followed by a method of training SLM parameters from text data. Finally, we introduce more detailed parametrization that involves non-terminal labeling and considerably improves smoothing of SLM statistical parameters. We conclude by presenting certain recognition and perplexity results achieved on standard corpora.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-1"
  },
  "gosy99_eurospeech": {
   "authors": [
    [
     "Mária",
     "Gósy"
    ]
   ],
   "title": "The controversial connection between speech production and perception: theories vs. facts",
   "original": "e99_kn02",
   "page_count": 8,
   "order": 2,
   "p1": "keynote paper 2",
   "pn": "",
   "abstract": [
    "Neurolinguistic investigations support the dissociation between speech production and speech perception in the case of aphasia. This means that an aphasic patient may be able to understand words and sentences while he is not able to produce them or, he may be able to produce certain linguistic forms properly while he is not able to detect the semantics of verbal utterances. Empirical data, however, seem to contradict the supposed excellent co-operation of speaking and perceiving in normal subjects as well. Various phenomena such as slips of the ear, violations of co-operation strategy of communication as well as the controversial connections of speech production and perception during language acquisition show dissociations of the two mechanisms. The question is how the properly working speech production co-occurs with inappropriate speech perception and what is the strategy where appropriate speech perception is escorted by inappropriate speech production.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-2"
  },
  "maybury99_eurospeech": {
   "authors": [
    [
     "Mark T.",
     "Maybury"
    ]
   ],
   "title": "Multimedia interaction for the new millennium",
   "original": "e99_kn03",
   "page_count": 8,
   "order": 3,
   "p1": "keynote paper 3",
   "pn": "",
   "abstract": [
    "Spoken language processing has created value in multiple application areas such as document transcription, data base entry, and command and control. Recently scientists have been focusing on a new class of application that promises on-demand access to multimedia information such as radio and broadcast news. In separate research, augmenting traditional graphical interfaces with additional modalities of interaction, such as spoken language, gesture, or eye tracking, promises to enhance human computer interaction. In this address I discuss the synergy of speech, language and image processing, introduce a new idea for corpus based multimedia interfaces, and identify some remaining challenging research areas.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-3"
  },
  "lindblom99_eurospeech": {
   "authors": [
    [
     "Björn",
     "Lindblom"
    ]
   ],
   "title": "How speech works - questions and preliminary answers",
   "original": "e99_kn04",
   "page_count": 2,
   "order": 4,
   "p1": "keynote paper 4",
   "pn": "",
   "abstract": [
    "In this presentation I attempt to discuss research from a wide spectrum of topics all relevant to the study of speech communication. It is my hope that placing more narrowly focused work in a common, broader context will produce a kind of symbiosis effect that will help us see the individual subtopics in perhaps new and insightful ways. Admittedly, such an undertaking might seem ambitious and even ill-advised. However, as our field is put under increasing pressures to deliver clinical and technological applications of greater real-life relevance, it does appear useful to try to assess what we know and do not know about speech processes. Also, as is sometimes the case, the whole tends to be more than the sum of its parts.\n",
    ""
   ]
  },
  "chou99_eurospeech": {
   "authors": [
    [
     "Wu",
     "Chou"
    ]
   ],
   "title": "Maximum a posterior linear regression with elliptically symmetric matrix variate priors",
   "original": "e99_0001",
   "page_count": 4,
   "order": 5,
   "p1": "1",
   "pn": "4",
   "abstract": [
    "In this paper, elliptic symmetric matrix variate distribution is proposed as the prior distribution for maximum a posterior linear regression (MAPLR) based model adaptation. The exact close form solution of MAPLR with elliptically symmetric matrix variate priors is obtained. The effects of the proposed prior in MAPLR are characterized and compared with conventional maximum likelihood linear regression (MLLR). The proposed priors are significant informative priors, through which a well-founded Bayesian theoretical framework is formulated to incorporate prior information in model adaptation. Moreover, an efficient approach of hyperparameter estimation in MAPLR is described. Experimental results indicate that significant gain can be obtained when adaptation data are sparse.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-4"
  },
  "goronzy99_eurospeech": {
   "authors": [
    [
     "Silke",
     "Goronzy"
    ],
    [
     "Ralf",
     "Kompe"
    ]
   ],
   "title": "A MAP-like weighting scheme for MLLR speaker adaptation",
   "original": "e99_0005",
   "page_count": 4,
   "order": 6,
   "p1": "5",
   "pn": "8",
   "abstract": [
    "This paper presents an approach for fast, unsupervised, on-line MLLR speaker adaptation using two MAP-like weighting schemes, a static and a dynamic one. While for the standard MLLR approach several sentences are necessary before a reliable estimation of the transformations is possible, the weighted approach shows good results even if adaptation is conducted after only a few short utterances. Experimental results show that using the static approach can improve the word error rate by approx. 27% if adaptation is conducted after every 4 utterances (single words or short phrases). Using the dynamic approach, results can be improved by 28%. The most important advantage of the dynamic weight is that it is rather insensitive with respect to the initial weight whereas for the static approach it is very critical which initial weight to chose. Moreover, useful values for the weights in the static case depend very much on the corpus. If the standard MLLR approach is used, even a drastic increase in sentence error rate can be observed for these small amounts of adaptation data.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-5"
  },
  "hirsch99_eurospeech": {
   "authors": [
    [
     "Hans-Günter",
     "Hirsch"
    ]
   ],
   "title": "HMM adaptation for telephone applications",
   "original": "e99_0009",
   "page_count": 4,
   "order": 7,
   "p1": "9",
   "pn": "12",
   "abstract": [
    "The presence of background noise and the frequency response of a transmission line like in telephone applications have a major influence on the performance of speech recognition systems. An approach is presented in this paper to cope with both effects. It is based on an estimation of the stationary noise spectrum and an estimation of the mismatch between the frequency responses present during training and during recognition. These estimations are used in combination with the PMC scheme [1] to adapt the whole word HMMs for a speaker independent recognition of connected words. A considerable improvement can be achieved on recognizing distorted speech data. The technique is also used as part of a complete speech dialogue system over the telephone network where it could also proof its beneficial usability.\n",
    "Keywords: robust speech recognition, HMM adaptation\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-6"
  },
  "huang99_eurospeech": {
   "authors": [
    [
     "Jing",
     "Huang"
    ],
    [
     "Mukund",
     "Padmanabhan"
    ]
   ],
   "title": "A study of adaptation techniques on a voicemail transcription task",
   "original": "e99_0013",
   "page_count": 4,
   "order": 8,
   "p1": "13",
   "pn": "16",
   "abstract": [
    "Speaker adaptation techniques have emerged as very effective and practical methods to improve ASR performance on a test speaker with only limited speech data from the speaker. We explore the use of adaptation techniques on a new Voicemail database and present some adaptation techniques on a new Voicemail database and present some theoretical extensions of the Cluster Transformation (CT) technique. Our experiments on 40 hours of voicemail data and four clusters shows that using cluster information with MLLR improves over baseline MLLR by 2.2% (relative). When the amount of adaptation data in a short message is insufficient to reliably decide its cluster, higher improvements result when we use MLLR for the very short messages and CT on longer ones.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-7"
  },
  "logan99_eurospeech": {
   "authors": [
    [
     "Beth",
     "Logan"
    ]
   ],
   "title": "Maximum likelihood sequential adaptation",
   "original": "e99_0017",
   "page_count": 4,
   "order": 9,
   "p1": "17",
   "pn": "20",
   "abstract": [
    "We develop a new sequential adaptation technique for HMMs based on an incremental variant of the EM algorithm. The approach has little impact on the speed of normal Viterbi decoding and in the case of mean adaptation only, is equivalent to incremental MAP adaptation for a certain choice of priors. We apply the technique to the ARPA HUB4 broadcast news task. Here since the acoustic conditions change frequently, it is advisable to `reset' the adaptation process periodically. However, for this task, the acoustic conditions change so rapidly that it is difficult to obtain enough information for adaptation between model resets. Many existing adaptation schemes tackle this problem of data sparsity by cleverly updating unseen mixture components. We investigate an orthogonal strategy in which a set of models, each representing a different acoustic condition, is maintained and adapted. We show that small improvements in performance are possible using this approach.\n",
    "Keywords: sequential adaptation, online adaptation, speech recognition, HMM.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-8"
  },
  "brinckmann99_eurospeech": {
   "authors": [
    [
     "Caren",
     "Brinckmann"
    ],
    [
     "Ralf",
     "Benzmüller"
    ]
   ],
   "title": "The relationship between utterance type and F0 contour in German",
   "original": "e99_0021",
   "page_count": 4,
   "order": 10,
   "p1": "21",
   "pn": "24",
   "abstract": [
    "In this study we investigate the intonational characteristics of the four utterance types statement, wh-question, yes/no-question and declarative question. Readings of two German scripted dialogues were examined to ascertain characteristic features of the F0 contour for each utterance type. Final boundary tone, nuclear pitch accent, F0 offset, F0 onset, F0 range, and the slopes of a topline and a bottomline were determined for each utterance and compared for the four utterance types. Results show that for an average speaker, the final boundary tone, the F0 range, and the slope of the topline can be used to distinguish between the four utterance types. However, speakers may deviate from this pattern and exploit other intonational means to distinguish certain utterance types or choose not to mark a syntactic difference at all.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-9"
  },
  "grigorova99_eurospeech": {
   "authors": [
    [
     "Evelina",
     "Grigorova"
    ],
    [
     "Vladimir",
     "Filipov"
    ],
    [
     "Bistra",
     "Andreeva"
    ]
   ],
   "title": "A contrastive investigation of discourse intonational characteristic features of sofia bulgarian and hamburg German in MAP task dialogues",
   "original": "e99_0025",
   "page_count": 4,
   "order": 11,
   "p1": "25",
   "pn": "28",
   "abstract": [
    "Ten MAP Task dialogues for Sofia Bulgarian (SB) and six for Hamburg German (HG) are recorded and analyzed by means of X-Waves Software Package. The discourse intonation features focused on are denial and convergence. It has been observed that for German denial can be integrated into discourse-listing through intonation: Ja-acknowledge and Nein-/Ne-denial moves are both manifested by intonation rises. For Bulgarian, intonation rises in answering moves occur only in the acknowledge subtype: rises in denials (Ne-) are associated with uncertainty and surprise. The HG Ne- and SB Ne-moves are resynthesized by means of PSOLA, twelve stimuli being obtained for SB and sixteen for HG. Two appropriate contexts marked for discourse-listing and follow-up moves are excerpted from the MAP Task and are included in perceptual tests whereby native speakers are asked to determine the appropriateness of each stimulus in relation to each context. 'The results for Bulgarian contradict our preliminary observations. Convergence is defined as the matching of corresponding movements in pitch ranges and signals sympathetic agreement with the other speakers point of view. The check: answer move sequence can be viewed as instantiating convergence and exemplifies both lexical and Fo movement repetition, especially where ellipted moves are concerned. The two resynthesized sequences for HG and SB respectively are \"Im Westen\" and \"Pravo nagore\" as manifested in check and answering contexts. As above, native speakers are expected to determine the appropriateness of each stimulus in relation to each context. It has been observed that the differences between checks and answering moves for both HG and SB are phonetically manifested and are also established as being relevant by the perceptual tests, yet they cannot be accounted for phonologically by tone alignment: convergence seems to attenuate the phonological differentiation between checks and answering moves.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-10"
  },
  "horne99_eurospeech": {
   "authors": [
    [
     "Merle",
     "Horne"
    ],
    [
     "Petra",
     "Hansson"
    ],
    [
     "Gösta",
     "Bruce"
    ],
    [
     "Johan",
     "Frid"
    ]
   ],
   "title": "Prosodic correlates of information structure in Swedish human-human dialogues",
   "original": "e99_0029",
   "page_count": 4,
   "order": 12,
   "p1": "29",
   "pn": "32",
   "abstract": [
    "Investigation of travel agent (~System) Task-related utterances reveals intonational contours where focal accents occur on information given previously in the dialogue by the client (~User). The accentuation can be related to the interactive nature of dialogue where the information holder picks up on a Task-related topic introduced by the client and comments on it in an engaged manner.\n",
    "Keywords: prosody, accentuation, information structure, dialogue, speech synthesis, Given-New\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-11"
  },
  "kitazawa99_eurospeech": {
   "authors": [
    [
     "S.",
     "Kitazawa"
    ],
    [
     "S.",
     "Kobayashi"
    ]
   ],
   "title": "Paralinguistic features as suprasegmental acoustics observed in natural Japanese dialogue",
   "original": "e99_0033",
   "page_count": 4,
   "order": 13,
   "p1": "33",
   "pn": "36",
   "abstract": [
    "The paralinguistic features, however this conference classifies the Paralinguistic analysis\" as Speaker identification, Keyword/topic spotting, and Language identification, include emotional aspects of voice, which is focused recently on interpersonal communications. Study starts from description and statistics of those features. Among many acoustic characteristics, we investigated features represented in pitch raising and lowering, loudness of speech, and rate of talking; these are not represented in textual meaning of speech that is the content of conventional speech recognition. The paralinguistic features are distinguished from prosodic features, however these two are often confused.\n",
    "Keywords: paralanguage, prosody, spontaneous speech, transcription\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-12"
  },
  "tamoto99_eurospeech": {
   "authors": [
    [
     "Masafumi",
     "Tamoto"
    ],
    [
     "Masahito",
     "Kawamori"
    ],
    [
     "Takeshi",
     "Kawabata"
    ]
   ],
   "title": "Integrating prosodic features in dialogue understanding",
   "original": "e99_0037",
   "page_count": 4,
   "order": 14,
   "p1": "37",
   "pn": "40",
   "abstract": [
    "We report our studies on functions of prosodic information in dialogue and on the result of our experiment in dialogue using a speech understanding system that incorporates a discrimination schema for illocutionary acts using prosodic features obtained from human-human dialogs.For constructing a speech understanding system with an `effortless' interface, it is necessary to model coordination in dialogue. This model needs to capture such sequential constraints of illocutionary acts as answers following questions, acceptance or rejections following requests, acknowledgements following assertions, and so on. However, recognizing these speech acts by simple, superficial analysis of dialogue is often difficult because of such disfluencies as omission and interruption that abound in spontaneous dialogs. Prosodic features are important in this respect because they often contribute to identifying speech acts of utterance when explicit linguistic information is missing. In order to investigate how prosodic information is utilized, along with other linguistic information, to identify speech acts, we performed a series of experiments. We collected task oriented spontaneous dialogs between human subjects, and extracted sentences that represent dialogue control structure. A different set of subjects were chosen for an experiment in which they were asked to identify the sentence type and intonation contour of these sentences. Given the transcription of these extracted sentences with contextual information, the subjects were able to identify the speech act types of about 85$ the 290 sentences. The subjects were then asked to identify the intonation contour of the same sentences by listening to the utterance modified in such a way that all voiced sounds were replaced by sinusoid so that only the fundamental frequency of the utterance can be heard. We observed syntactic and prosodic properties of those utterances Speech acts were represented as three basic categories, the illocutions of assertion, question and request. Similarly, sentence types are represented as declarative, interrogative and imperative. Intonations are classified into rise-up, fall-down and neutral pitch contour. We then made a simulation task of dialogue understanding system to incorporate the results of the human-human dialogue experiment. A sentence type was identified using human subjects. An intonation contour was identified using an algorithm that calculates the range and slope of the upper and lower bounds of unwarped segmental contour, and matches these against predefined contour templates.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-13"
  },
  "cox99_eurospeech": {
   "authors": [
    [
     "Stephen",
     "Cox"
    ],
    [
     "Srinandan",
     "Dasmahapatra"
    ]
   ],
   "title": "A high-level approach to confidence estimation in speech recognition",
   "original": "e99_0041",
   "page_count": 4,
   "order": 15,
   "p1": "41",
   "pn": "44",
   "abstract": [
    "Errors in the output of a speech recogniser can be said to be due to the interaction of inadequate phonetic and language modelling components. We investigate an approach to estimating confidence scores for the words output by a recogniser in which the language modelling and acoustic modelling are decoupled by the use of a phone recogniser working in parallel with the word recogniser. An advantage of such an approach is that it avoids techniques which rely on the use of side-information derived from the decoder: such information may not always be available and/or may depend on the type and configuration of the decoder used. We have investigated two ways of using the additional information provided by the phone-loop recogniser. One is based on correlating the phone strings from the two recognisers; the other is based on using the phone-loop recogniser output to construct hypotheses for the utterance and correlating these hypotheses with the word recogniser output.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-14"
  },
  "jia99_eurospeech": {
   "authors": [
    [
     "Bin",
     "Jia"
    ],
    [
     "Xiaoyan",
     "Zhu"
    ],
    [
     "Yupin",
     "Luo"
    ],
    [
     "Dongcheng",
     "Hu"
    ]
   ],
   "title": "Utterance verification using modified segmental probability model",
   "original": "e99_0045",
   "page_count": 4,
   "order": 16,
   "p1": "45",
   "pn": "48",
   "abstract": [
    "Today speech recognition is requested not only to decode utterances into transcriptions, but also to determine the reliabilities of the result, by Utte-rance Verification (UV). With the conventional HMM, the measure of reliabilities can not be determined directly by the likelihoods of models. Whereas, Modified Segmental Probability Model (MSPM), suggested in this paper, with its norma-lized likelihood, facilitates rendering UV and speech recognition at the same time and as a whole. In the paper, Integrated Anti-word Model (IAM) is suggested, which is used to advance the measure of UV likelihood of MSPM. Some experiments show high perfor-mance and moderate computa-tion with IAM.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-15"
  },
  "klakow99_eurospeech": {
   "authors": [
    [
     "Dietrich",
     "Klakow"
    ],
    [
     "Georg",
     "Rose"
    ],
    [
     "Xavier",
     "Aubert"
    ]
   ],
   "title": "OOV-detection in large vocabulary system using automatically defined word-fragments as fillers",
   "original": "e99_0049",
   "page_count": 4,
   "order": 17,
   "p1": "49",
   "pn": "52",
   "abstract": [
    "The problem of unknown words has been addressed using automatically generated filler fragments which augment the lexicon and are incorporated in the language model. These fragments are used to reduce the damage on in-vocabulary words, to detect OOV regions and to provide a phonetic transcription for these regions. The performance of this technique has been evaluated in terms of damage reduction error rate and OOV tagging rate. Significant improvements are reported on both measures. In particular, the influence of an appropriate tuning of the language model factor and word penalties is demonstrated as well as the usefulness of using cross-word triphones over fragments boundaries.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-16"
  },
  "lin99_eurospeech": {
   "authors": [
    [
     "Qiguang",
     "Lin"
    ],
    [
     "David",
     "Lubensky"
    ],
    [
     "Salim",
     "Roukos"
    ]
   ],
   "title": "Use of recursive mumble models for confidence measuring",
   "original": "e99_0053",
   "page_count": 4,
   "order": 18,
   "p1": "53",
   "pn": "56",
   "abstract": [
    "In many speech recognition applications such as name dialing, it is necessary to have the ability to know when a recognition error has occurred so that undesired or unpredicted system behavior can be minimized. Confidence measure is usually used for detection of probable errors. In this paper, a new method for measuring confidence is presented. The method is based on use of recursive mumble models. During a regular decoding from which word hypotheses and word boundaries are known, the score of recursive mumble models is then determined. The (weighted) difference between the word detail-match score and the mumble score is used as the confidence measure. It is next compared to a predefined threshold to decide whether the decoded result is confidently correct or not. The method has been evaluated with two different databases. The results show that the new method outperforms our previous method solely based on the word detail-match scores. In particular, the results show that the new method is able to reduce the equal error rate from 32% to 23% and that it rejects far more (78% versus 35%) out-of-domain sentences at the fixed 5% false rejection rate.\n",
    "Keywords: confidence measure, mumble model, equal error rate\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-17"
  },
  "rahim99_eurospeech": {
   "authors": [
    [
     "Mazin",
     "Rahim"
    ]
   ],
   "title": "Utterance verification for the numeric language in a natural spoken dialogue",
   "original": "e99_0057",
   "page_count": 4,
   "order": 19,
   "p1": "57",
   "pn": "60",
   "abstract": [
    "The defines the set of words or phrases that are relevant to the task of understanding and interpreting number strings, such as credit cards, telephone numbers, zip codes, etc. This paper addresses the issues involved in designing an utterance verification system for the numeric language in a natural spoken dialogue. The objective is to associate each utterance with a verification measure that reflects the confidence in recognizing its numeric language. A hierarchical mixture-of-experts is used to verify whether an utterance, being encoded using different features, is correctly recognized and interpreted by the understanding module. Experimental results on a field-trial study conclude that the proposed verification system outperforms the classical likelihood ratio distance and does significantly better than a standard Gaussian mixture model.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-18"
  },
  "chengalvarayan99_eurospeech": {
   "authors": [
    [
     "Rathinavelu",
     "Chengalvarayan"
    ]
   ],
   "title": "Robust energy normalization using speech/nonspeech discriminator for German connected digit recognition",
   "original": "e99_0061",
   "page_count": 4,
   "order": 20,
   "p1": "61",
   "pn": "64",
   "abstract": [
    "The addition of a word normalized energy contour uniformly improves performance of the HMM recognizer and makes it more robust to difference in talker populations. This kind of normalization generally requires some information on the statistics of energy features over the whole utterance, which is not a feasible solution in real-time applications due to the unnecessary long processing delay. In this paper, we propose a more efficient implementation approach for energy feature normalization where the normalization coefficients are computed using a look-a-head delay of fixed length. The experimental results on German connected digit recognition task show that a 12% string error rate reduction is obtained by using a look-a-head delay energy normalization scheme when compared to without using the energy feature. Further reduction of 10% string error rate is achieved by integrating the speech/nonspeech decision mechanism.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-19"
  },
  "veth99_eurospeech": {
   "authors": [
    [
     "Johan de",
     "Veth"
    ],
    [
     "Bert",
     "Cranen"
    ],
    [
     "Febe de",
     "Wet"
    ],
    [
     "Louis",
     "Boves"
    ]
   ],
   "title": "Acoustic pre-processing for optimal effectivity of missing feature theory",
   "original": "e99_0065",
   "page_count": 4,
   "order": 21,
   "p1": "65",
   "pn": "68",
   "abstract": [
    "In this paper we investigate acoustic backing-off as an operationalization of Missing Feature Theory with the aim to increase recognition robustness. Acoustic backing-off effectively diminishes the detrimental influence of outlier values by using a new model of the probability density function of the feature values. The technique avoids the need for explicit outlier detection. Situations that are handled best by Missing Feature Theory are those where only part of the coefficients are disturbed and the rest of the vector is unaffected. Consequently, one may predict that acoustic feature representations that smear local spectro-temporal distortions over all feature vector elements are inherently less suitable for automatic speech recognition. Our experiments seem to confirm this prediction. Using additive band limited noise as a distortion and comparing four different types of feature representations, we found that the best recognition performance is obtained with recognizers that use acoustic backing-off and that operate on feature types that minimally smear the distortion.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-20"
  },
  "heracleous99_eurospeech": {
   "authors": [
    [
     "Panikos",
     "Heracleous"
    ],
    [
     "Takeshi",
     "Yamada"
    ],
    [
     "Satoshi",
     "Nakamura"
    ],
    [
     "Kiyohiro",
     "Shikano"
    ]
   ],
   "title": "Simultaneous recognition of multiple sound sources based on 3-d n-best search using microphone array",
   "original": "e99_0069",
   "page_count": 4,
   "order": 22,
   "p1": "69",
   "pn": "72",
   "abstract": [
    "The recognition of distant talking speech in a noisy and reverberant environments is key issue in any speech recognition system. A so-called hands-free speech recognition system plays an important role in the natural and friendly human-machine interface. Considering the practical use of a speech recognition system, we realize that such a system has to deal, also, with the case of the presence of multiple sound sources, including multiple talkers, as well as other noise sources. This paper proposes a novel method which recognizes multiple talkers simultaneously in real environments by extending the 3-D Viterbi search to a 3-D N-best search algorithm. While the 3-D Viterbi method finds the most likely path in the 3-D trellis space, the proposed method considers multiple hypotheses for each direction in every frame. Combinations of the direction sequence and the phoneme sequence of multiple sources are included in the N-best list. The paper investigates the performance of the proposed method through experiments using real utterances of multiple talkers.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-21"
  },
  "hermansky99_eurospeech": {
   "authors": [
    [
     "Hynek",
     "Hermansky"
    ],
    [
     "Pratibha",
     "Jain"
    ]
   ],
   "title": "Down-sampling speech representation in ASR",
   "original": "e99_0073",
   "page_count": 3,
   "order": 23,
   "p1": "73",
   "pn": "76",
   "abstract": [
    "Features for automatic speech recognition (ASR) are typically sampled at about 100 Hz (10 ms analysis step). Recent experiments indicate that the most efficient components of the modulation spectrum of speech for ASR are up to about 16 Hz. Consequently, RASTA processing attenuates modulation frequencies higher than 16 Hz and should in principle allow for a subsequent down-sampling of the features. It has been shown earlier that in a Gaussian mixture model based speaker recognition system(which uses single state HMM, thus not requiring any time alignments of the incoming speech) one could down-sample the speech representation after RASTA filtering without any significant loss of performance. However since ASR uses Viterbi time alignment, reduced number of time samples due to down-sampling, although justified by Nyquist criteria after the low-pass filtering, could create problems. In this paper we experimentally show that the down-sampling of features after RASTA filtering is feasible and could result in considerable computational or at least storage/transmission savings.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-22"
  },
  "macho99_eurospeech": {
   "authors": [
    [
     "Dusan",
     "Macho"
    ],
    [
     "Climent",
     "Nadeu"
    ],
    [
     "Peter",
     "Jancovic"
    ],
    [
     "Gregor",
     "Rozinaj"
    ],
    [
     "Javier",
     "Hernando"
    ]
   ],
   "title": "Comparison of time & frequency filtering and cepstral-time matrix approaches in ASR",
   "original": "e99_0077",
   "page_count": 4,
   "order": 24,
   "p1": "77",
   "pn": "80",
   "abstract": [
    "In current speech recognition systems, speech is represented by a 2-D sequence of parameters that model the temporal evolution of the spectral envelope of speech. Linear transformation or filtering along both time and frequency axes of that 2-D sequence are used to enhance the discriminative ability and robustness of speech parameters in the HMM pattern-matching formalism. In this paper, we compared two recently reported approaches which operate on the sequence of logarithmically compressed mel-scaled filter-bank energies: the first approach - TIFFING (TIme and Frequency FilterING) - applies FIR filters to that 2-D sequence along both axes, while the second one - CTM (Cepstral Time Matrix) - uses the DCT to compute a set of parameters in the 2-D transformed domain. They are compared in several ways: (1) analytically, using Fourier transformation, (2) statistically and (3) performing recognition tests with clean and noisy speech.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-23"
  },
  "meinedo99_eurospeech": {
   "authors": [
    [
     "Hugo",
     "Meinedo"
    ],
    [
     "Joao P.",
     "Neto"
    ],
    [
     "Luis B.",
     "Almeida"
    ]
   ],
   "title": "Syllable onset detection applied to the portuguese language",
   "original": "e99_0081",
   "page_count": 4,
   "order": 25,
   "p1": "81",
   "pn": "84",
   "abstract": [
    "Recent developments have suggested that the use of syllables as the basic unit in a speech recognition system could be very usefull. Since syllable boundaries are more precise and well defined than phoneme ones there is a large scope for their application on the continuous speech recognition process. In this work we developed different methods of syllable segmentation in continuous speech. These methods are based on perceptually oriented feature extraction techniques. These features were post-processed through simple threshold mechanisms or by an artificial neural network based classifier in order to estimate the syllable boundaries. These systems were trained and evaluated using a Portuguese database with continuous speech. The results show that large context input windows (260 ms) are the most appropriate, achieving results of 93% detection of onsets with insertion rates of only 15%.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-24"
  },
  "paliwal99_eurospeech": {
   "authors": [
    [
     "Kuldip K.",
     "Paliwal"
    ]
   ],
   "title": "Decorrelated and liftered filter-bank energies for robust speech recognition",
   "original": "e99_0085",
   "page_count": 4,
   "order": 26,
   "p1": "85",
   "pn": "88",
   "abstract": [
    "Though Mel frequency cepstral coefficients (MFCCs) have been very successful in speech recognition, they have the following two problems: 1) They do not have any physical interpretation, and 2) Liftering of cepstral coefficients, found to be highly useful in the earlier dynamic warping-based speech recognition systems, has no effect in the recognition process when used with continuous observation Gaussian density hidden Markov models. In this paper, we propose to use the filter-bank energies (FBEs) as features. The FBEs are physically meaningful quantities and amenable for applying human auditory processing such as masking. We describe procedures to decorrelate and lifter the FBEs and show that the FBEs perform at least as good as (and sometimes even better than) the MFCCs for robust speech recognition.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-25"
  },
  "pachesleal99_eurospeech": {
   "authors": [
    [
     "Pau",
     "Paches-Leal"
    ],
    [
     "Richard C.",
     "Rose"
    ],
    [
     "Climent",
     "Nadeu"
    ]
   ],
   "title": "Optimization algorithms for estimating modulation spectrum domain filters",
   "original": "e99_0089",
   "page_count": 4,
   "order": 27,
   "p1": "89",
   "pn": "92",
   "abstract": [
    "The goal of the work described in this paper is to develop and evaluate procedures for automatic estimation of modulation spectrum filters to compensate for distortions in the modulation spectrum domain. The modulation spectrum (MS) is often used to describe the time sequence of spectral parameters (TSSPs) that are derived from the speech waveform, and is thought to be a good representation of many sources of variability in speech. These procedures will be used in the context of automatic speech recognition (ASR) applications where there is likely to be a significant mismatch in the MS characteristics that exist for system training and evaluation. Results are presented describing application of the algorithm to one task involving an artificially introduced MS distortion and to another task involving differences in speaking styles for training and testing. It is shown in the paper that these techniques are able to compensate for the effects of artificially introduced distortions that appear in testing. It is also shown that a small degree of compensation is obtained for speaking style mismatch, and this result is compared with the measured effects of the speaking style differences in the MS domain. An algorithm is presented for automatic estimation of the An algorithm to estimate automatically filters in the modulation spectrum domain. These are used to compensate for distortions in this domain or to obtain the difference coefficients that are a part of the acoustic vector handed over to the HMM-based speech model. The mathematical properties of the new algorithm are analyzed. Its performance is studied in two different experiments: in the first the goal is to alleviate an artificially simulated distortion while in the second we try to compensate for speaking rate distortions in a database which has two distinct parts differing significantly in speaking rate.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-26"
  },
  "sansegundo99_eurospeech": {
   "authors": [
    [
     "R.",
     "San-Segundo"
    ],
    [
     "R.",
     "Córdoba"
    ],
    [
     "J.",
     "Ferreiros"
    ],
    [
     "A.",
     "Gallardo"
    ],
    [
     "J.",
     "Colás"
    ],
    [
     "J.",
     "Pastor"
    ],
    [
     "Y.",
     "López"
    ]
   ],
   "title": "Efficient vector quantization using an n-path binary tree search algorithm",
   "original": "e99_0093",
   "page_count": 4,
   "order": 28,
   "p1": "93",
   "pn": "96",
   "abstract": [
    "We propose the utilization of a new n-path binary tree search algorithm for vector quantization. Our target is to reduce the complexity (time processing) of the vector quantizer maintaining the quantization distortion. The algorithm has been applied to an isolated digit recognizer by telephone based on DHMM and to a speaker dependent continuous speech system based on SCHMM, so we will also give the recognition results for both of them. We have tested several alternatives to calculate the centroids of the higher levels of the tree. In all the experiments we have considered the following parameters for the evaluation: average distortion, same choice percentage, average distortion for the mistakes and processing time. Our reference has been the standard quantization (computing the distance with all centroids). In this reference case the distortion was 220.9 and the processing time was 2.1 seconds. With the n-path binary tree search algorithm, we have obtained a 0.7 seconds processing time with a similar distortion: 226.4. In the semicontinuous system, we have obtained a reduction of 71 % in vector quantization processing time, maintaining the word accuracy.\n",
    "Keywords: vector quantization, binary tree search, CPU time reduction\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-27"
  },
  "warakagoda99_eurospeech": {
   "authors": [
    [
     "Narada D.",
     "Warakagoda"
    ],
    [
     "Magne H.",
     "Johnsen"
    ]
   ],
   "title": "Neural network based optimal feature extraction for ASR",
   "original": "e99_0097",
   "page_count": 4,
   "order": 29,
   "p1": "97",
   "pn": "100",
   "abstract": [
    "The procedure of calculating Mel Frequency based Cepstral Coefficients (MFCC) is shown to resemble a three layer Multilayer Perceptron (MLP) like structure. Such an MLP is employed as a preprocessor in a hybrid HMM-MLP system, and the possibility of optimizing the whole system as a single entity, with respect to a suitable criterion, is pointed out. This system, to-gether with the Maximum Mutual Information (MMI) criterion was tested on a speaker independent, five broad class, isolated phoneme recognition task. Results of these preliminary experi-ments, which clearly indicate the advantage of optimizable pre-processing, are reported.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-28"
  },
  "yato99_eurospeech": {
   "authors": [
    [
     "Fumihiro",
     "Yato"
    ],
    [
     "Naomi",
     "Inoue"
    ],
    [
     "Kazuo",
     "Hashimoto"
    ]
   ],
   "title": "A study of speech recognition for the elderly",
   "original": "e99_0101",
   "page_count": 4,
   "order": 30,
   "p1": "101",
   "pn": "104",
   "abstract": [
    "The elderly have needs for voice operated interfaces to manipulate various systems. However, only few results were reported about the performance of the current speech recognition technology with the elderly. In this paper we report our speech recognition experiments for 469 Japanese elderly persons belong to the age range 59 to 85. The speech recognition is executed on PC by using IBM Via Voice. The utterances of 600 place names are used for our speech recognition experiments. The average error rate becomes about 7 %, which seems to be a little bit lower than that of young adults. The paper presents the detailed analysis of the results of our experiments.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-29"
  },
  "zhu99_eurospeech": {
   "authors": [
    [
     "Jie",
     "Zhu"
    ],
    [
     "Fei-li",
     "Chen"
    ]
   ],
   "title": "The analysis and application of a new endpoint detection method based on distance of autocorrelated similarity",
   "original": "e99_0105",
   "page_count": 4,
   "order": 31,
   "p1": "105",
   "pn": "108",
   "abstract": [
    "Endpoints detection play the important role in speech recognition. But the performance of some often used algorithms will decrease under low SNR conditions. A new concept as distance between autocorrelated functions is brought fully, and a new endpoint detecting method based on it is discussed in this paper. Results from experiment show that this new method has higher performance in endpoint detection. Especially in low SNR circumstance, it still can detect endpoint of speech accurately.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-30"
  },
  "beautemps99_eurospeech": {
   "authors": [
    [
     "Denis",
     "Beautemps"
    ],
    [
     "Pascal",
     "Borel"
    ],
    [
     "Sébastien",
     "Manolios"
    ]
   ],
   "title": "Hyper-articulated speech: auditory and visual intelligibility",
   "original": "e99_0109",
   "page_count": 4,
   "order": 32,
   "p1": "109",
   "pn": "112",
   "abstract": [
    "In the field of speech adaptability to environmental conditions, the intelligibility of hyper auditory and visual speech versus normal speech was investigated. The « Lombard » reflex was used to obtain auditory sequences produced with vocal effort for a set of French /b, d, g, p, t, k/ plosives. Analysis of plosive identifications showed an improvement with the auditory hyper condition in the plosive place of articulation for dentals and velars. Instruction to produce hyper-visual speech provided hyper-articulated labial movements for a set of French /a, i, y/ vowels and /b, v, z, J, l, r/ consonants. Analysis of visual identification scores showed a gain for the hyper-visual condition on the /i/ versus /a/ contrast, but to the detriment of identification of lip movement for consonants - results which were moreover confirmed by the lip shape analysis.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-31"
  },
  "engwall99_eurospeech": {
   "authors": [
    [
     "Olov",
     "Engwall"
    ]
   ],
   "title": "Modeling of the vocal tract in three dimensions",
   "original": "e99_0113",
   "page_count": 4,
   "order": 33,
   "p1": "113",
   "pn": "116",
   "abstract": [
    "This paper describes the development of a three-dimensional articulatory vocal tract model at KTH. The model represents vocal and nasal tract walls, lips, teeth and tongue as parameterised polygon surfaces. This allows the geometry of the vocal tract to be set with a small number of articulatory parameters. As the cross-sectional areas in addition are given directly from the vocal tract geometry, the model is suitable for articulatory synthesis in 3D. The second field of application is pronunciation training, where the model can provide visual feedback to hearing-impaired children and adult second language learners. A 3D model can improve both articulatory and visual speech synthesis as it provides information lacking in the 2D models traditionally used. Correctness of the model will increase with the amount of articulatory data incorporated, as exemplified by this papers description of the method to improve the tongue model.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-32"
  },
  "kienast99_eurospeech": {
   "authors": [
    [
     "Miriam",
     "Kienast"
    ],
    [
     "Astrid",
     "Paeschke"
    ],
    [
     "Walter",
     "Sendlmeier"
    ]
   ],
   "title": "Articulatory reduction in emotional speech",
   "original": "e99_0117",
   "page_count": 4,
   "order": 34,
   "p1": "117",
   "pn": "120",
   "abstract": [
    "The present study aims at examining vocal expression of emotion. Emotionally loaded speech material produced by actors was analyzed with reference to the accuracy of articulation as well as to the duration of syllables and segments. It was investigated whether the vocal expression of several emotions (anger, happiness, fear and sadness) differ from one another and from neutral speech with respect to the durational and qualitative reduction of articulation. The obtained results suggest that accuracy of articulation and durational aspects are useful parameters to characterize especially the emotions anger, fear and sadness.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-33"
  },
  "kaburagi99_eurospeech": {
   "authors": [
    [
     "Tokihiko",
     "Kaburagi"
    ],
    [
     "Masaaki",
     "Honda"
    ],
    [
     "Takeshi",
     "Okadome"
    ]
   ],
   "title": "A trajectory formation model of articulatory movements using a multidimensional phonemic task",
   "original": "e99_0121",
   "page_count": 4,
   "order": 35,
   "p1": "121",
   "pn": "124",
   "abstract": [
    "This paper presents a model for representing context-dependent variation of articulator movements. Our model explains the contextual effect based on a multidimensional phonemic task and dynamic constraints of movements. The task determines the articulatory target so that invariant features of phoneme articulation are achieved. The dynamic constraints represent smoothly moving behavior of the articulators. Because the dimension of the task is smaller than that of the articulator variables, there are unconstrained degrees-of-freedom of the articulator variables. These redundant components are used to represent the contextual effect by smoothly interpolating the adjacent tasks. The phonemic invariant feature is defined as a linear transformation that minimizes a normalized articulatory variation. Simulation of articulatory movements is performed and the results are compared with actual movements.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-34"
  },
  "krstulovic99_eurospeech": {
   "authors": [
    [
     "Sacha",
     "Krstulovic"
    ]
   ],
   "title": "LPC-based inversion of the DRM articulatory model",
   "original": "e99_0125",
   "page_count": 4,
   "order": 36,
   "p1": "125",
   "pn": "128",
   "abstract": [
    "Articulatory representations are expected to bring better speech recognition results. This requires to estimate the parameters of a speech production model from the speech sound, problem known as acoustico-articulatory inversion. Known methods to solve this problem usually introduce a heavy computational cost. Alternately, it is known that Linear Prediction analysis offers an analogy with acoustic filtering. This analogy had been exploited to develop a less expensive analytic method applicable to the estimation of tube shapes discretized in equal-length sections. We have extended the method to the DRM case, where the tube is made of unequal-length sections. The proposed DRM inversion scheme is thus simpler and faster. Furthermore, it shows good performance in terms of low residual modeling error. It also enhances speech recognition results when used to compute Log Area Ratios.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-35"
  },
  "miki99_eurospeech": {
   "authors": [
    [
     "Nobuhiro",
     "Miki"
    ],
    [
     "Thoru",
     "Yokoyama"
    ],
    [
     "Takeshi",
     "Ohtani"
    ],
    [
     "Shinobu",
     "Masaki"
    ],
    [
     "Ikuhiro",
     "Shimada"
    ],
    [
     "Ichiro",
     "Fujimoto"
    ],
    [
     "Yuji",
     "Nakamura"
    ]
   ],
   "title": "A vocal tract model using multi-line equivalent circuits",
   "original": "e99_0129",
   "page_count": 4,
   "order": 37,
   "p1": "129",
   "pn": "132",
   "abstract": [
    "In this paper we show a speech production model of the vocal tract analogue, The 3-D vocal tract shape has information for phonetic or personal characteristics, and it has an important meaning to study for the relation between the vocal tract shape and the acoustic characteristics. The FEM analysis is a useful tool for the acoustic analysis, but the computational cost is a problem. On the contrary to the FEM modeling, the transmission line model is convenient to evaluate the impedance function without high computational cost. We propose a new multi-line model which is available to represent the muli-channel as liquid sound. but the involving the 3-D parameters is Difficult because of the one-dimensional model essentially.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-36"
  },
  "matsuda99_eurospeech": {
   "authors": [
    [
     "Masahiro",
     "Matsuda"
    ],
    [
     "Hideki",
     "Kasuya"
    ]
   ],
   "title": "Acoustic nature of the whisper",
   "original": "e99_0133",
   "page_count": 4,
   "order": 38,
   "p1": "133",
   "pn": "136",
   "abstract": [
    "The lower formant frequencies of the whispery vowel are known to be slightly higher than those of the modal vowel. This paper attempts to interpret this phenomenon acoustically, based on an electrical circuit model of the vocal tract, taking into account acoustic coupling with the subglottal system. A three-dimensional vocal tract shape was measured from a magnetic resonance image (MRI). It was found that the narrowing of the tract in the false vocal fold regions and weak acoustic coupling with the subglottal system are primary causes of the rise of the lower formant frequencies.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-37"
  },
  "okadome99_eurospeech": {
   "authors": [
    [
     "Takeshi",
     "Okadome"
    ],
    [
     "Tokihiko",
     "Kaburagi"
    ],
    [
     "Masaaki",
     "Honda"
    ]
   ],
   "title": "Relations between utterance speed and articulatory movements",
   "original": "e99_0137",
   "page_count": 4,
   "order": 39,
   "p1": "137",
   "pn": "140",
   "abstract": [
    "A relation between utterance speed and the amount articulatory behavior for each phoneme reduces is investigated on the basis of the articulatory data taken by using a magnetic sensory system. The relation between the utterance speed and the change of the utterance timing for each phoneme is also focused on. Two linear-regression models are proposed: one predicts reduced articulatory movements and the other adjusts the utterance timing of each phoneme according to the utterance speed.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-38"
  },
  "laprie99_eurospeech": {
   "authors": [
    [
     "Slim Ouni And Yves",
     "Laprie"
    ]
   ],
   "title": "Design of hypercube codebooks for the acoustic-to-articulatory inversion respecting the non-linearities of the articulatory-to-acoustic mapping",
   "original": "e99_0141",
   "page_count": 4,
   "order": 40,
   "p1": "141",
   "pn": "144",
   "abstract": [
    "This paper presents a new method to generate a codebook that best represents the mapping between articulatory and acoustic domains. Existing codebook generation methods are relatively inefficient because of the non-linearities between the two domains. In our method the articulatory space is considered as composed of several hypercubes where the mapping is linear. The advantages of such a representation are the lower number of vectors which compose the codebook and the hierarchical organization of the codebook in a manner that respects the structure of the two domains.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-39"
  },
  "owens99_eurospeech": {
   "authors": [
    [
     "Marie",
     "Owens"
    ],
    [
     "Anja",
     "Krüger"
    ],
    [
     "Paul",
     "Donnelly"
    ],
    [
     "F J",
     "Smith"
    ],
    [
     "Ji",
     "Ming"
    ]
   ],
   "title": "A missing-word test comparison of human and statistical language model performance",
   "original": "e99_0145",
   "page_count": 4,
   "order": 41,
   "p1": "145",
   "pn": "148",
   "abstract": [
    "A suite of missing-word tests based on text extracts selected randomly from two different text corpora provided a metric which was used in an evaluation of human performance, an evaluation of language model performance and a cross-comparison of the performances. The effects of providing different sizes of context for the missing word (ranging from two words to three sentences) were examined and two main patterns became clear from the results: - surprisingly, for tests where the language model was able to take advantage of all the context information provided (i.e. where the context consisted of just a few words) it outperformed humans; - conversely, humans outperformed the language model when the size of context given for the missing word exceeded the size, which the language model could usefully, employ in its probability calculations (typically more than six words).\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-40"
  },
  "richmond99_eurospeech": {
   "authors": [
    [
     "Korin",
     "Richmond"
    ]
   ],
   "title": "Estimating velum height from acoustics during continuous speech",
   "original": "e99_0149",
   "page_count": 4,
   "order": 42,
   "p1": "149",
   "pn": "152",
   "abstract": [
    "This paper reports on present work, in which a recurrent neural network is trained to estimate velum height during continuous speech. Parallel acoustic-articulatory data comprising more than 400 read TIMIT sentences is obtained using electromagnetic articulography (EMA). This data is processed and used as training data for a range of neural network sizes. The network demonstrating the highest accuracy is identified. This performance is then evaluated in detail by analysing the networks output for each phonetic segment contained in 50 hand-labelled utterances set aside for testing purposes.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-41"
  },
  "silva99_eurospeech": {
   "authors": [
    [
     "C.",
     "Silva"
    ],
    [
     "S.",
     "Chennoukh"
    ],
    [
     "Isabel",
     "Trancoso"
    ]
   ],
   "title": "On improving the decision algorithm for articulatory codebook search",
   "original": "e99_0153",
   "page_count": 4,
   "order": 43,
   "p1": "153",
   "pn": "156",
   "abstract": [
    "This paper describes our progress on articulatory voice mimic. The objective is to achieve an articulatory voice mimic system as a basis for low bit-rate speech coding using articulatory codebooks. The articulatory codebook uses a suitable vocal tract model for generating shapes for all possible speech sounds. When building a codebook, unrealistic vocal tract shapes may be generated. In this paper, we describe the used filter based on physiological assumptions to avoid populating the codebook with such shapes. We also propose a new method for the codebook search. This method weights the prediction error for each area function parameter as a function of its contribution to match the input acoustic signal parameters. An interaction between the variations of the area function parameter is created to constraint the articulatory trajectory, which improves the search and increases the output speech quality of the voice mimic system.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-42"
  },
  "thimm99_eurospeech": {
   "authors": [
    [
     "G.",
     "Thimm"
    ],
    [
     "J.",
     "Luettin"
    ]
   ],
   "title": "Extraction of articulators in x-ray image sequences",
   "original": "e99_0157",
   "page_count": 4,
   "order": 44,
   "p1": "157",
   "pn": "160",
   "abstract": [
    "We describe a method for tracking tongue, lips, and throat in X-ray _lms showing the side-view of the vocal tract. The technique uses specialized histogram normalization techniques and a new tracking method that is robust against occlusion, noise, and spontaneous, non-linear deformations of articulators. The tracking results characterize the con_guration of the vocal tract over time and can be used in different areas of speech research.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-43"
  },
  "teixeira99_eurospeech": {
   "authors": [
    [
     "António",
     "Teixeira"
    ],
    [
     "Francisco",
     "Vaz"
    ],
    [
     "José Carlos",
     "Príncipe"
    ]
   ],
   "title": "Effects of source-tract interaction in perception of nasality",
   "original": "e99_0161",
   "page_count": 4,
   "order": 45,
   "p1": "161",
   "pn": "164",
   "abstract": [
    "In this paper we study the effect of source changes, caused by vocal tract load, in perception of nasality. For that we have developed an articulatory speech synthesizer, includ-ing a comprehensive nasal tract model and an interactive glottal source model. Our main objective was to inves-tigate to what extent is necessary, in systems aimed to produce high quality synthetic sounds, to include the ef-fect of source-tract interaction in the glottal source model when synthesizing nasal vowels. In our studies we used Portuguese nasal vowels. Portuguese uses nasalization of vowels in its phonological inventory. Changes in glottal wave, caused by the additional load of the nasal tract are more significant in vowels like [i] with low F1 and high F2 . Effects are more dramatic in time rather frequency domain. Perception tests favor the idea that listener arent able to detect the perceptual effect of source-tract inter-action changes caused by the additional coupling of the nasal tract. More tests are needed to support, or reject, this.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-44"
  },
  "vaxelaire99_eurospeech": {
   "authors": [
    [
     "Béatrice",
     "Vaxelaire"
    ],
    [
     "Rudolph",
     "Sock"
    ],
    [
     "Véronique",
     "Hecker"
    ]
   ],
   "title": "Perceiving anticipatory phonetic gestures in French",
   "original": "e99_0165",
   "page_count": 4,
   "order": 46,
   "p1": "165",
   "pn": "168",
   "abstract": [
    "Positing that listeners exploit early motoric cues and vocal tract shapes related to upcoming speech elements, this research examines the perceptual effects of anticipatory gestures and vocal tract configurations in the production of a French rounded vowel-like consonant: the so-called semivowel [á]. The paradigm consists in generating speech samples by a representative French speaker, then segments are \"gated-out\" and listeners are asked to judge what the \"gated-out\" segments were. The robustness of the perceptual effects and extent of these anticipatory gestures and vocal tract shapes are evaluated under increased speaking rate. Relationships between coarticulatory production strategies and perceptual mechanisms are explained in terms of sensory-motor constraints.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-45"
  },
  "vilain99_eurospeech": {
   "authors": [
    [
     "Anne",
     "Vilain"
    ],
    [
     "Christian",
     "Abry"
    ],
    [
     "Pierre",
     "Badin"
    ]
   ],
   "title": "Motor equivalence evidenced by articulatory modelling",
   "original": "e99_0169",
   "page_count": 4,
   "order": 47,
   "p1": "169",
   "pn": "172",
   "abstract": [
    "We present a method for the analysis of motor equivalence on two French corpuses, evidenced by articulatory modelling. This processing enables us to make out the individual actions of each degree of freedom of the vocal tract. We intend to define the phonetic types we are studying with a combination of degrees of freedom recruited. Such a characterisation makes it possible to account for coarticulatory processes in a coherent way. This method brings to light compensatory strategies, striking strategies of preservation of vocalic configurations in some consonants by one speaker only, and a large divergence in the strategies used by the two speakers.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-46"
  },
  "wet99_eurospeech": {
   "authors": [
    [
     "Febe de",
     "Wet"
    ],
    [
     "Catia",
     "Cucchiarini"
    ],
    [
     "Helmer",
     "Strik"
    ],
    [
     "Lou",
     "Boves"
    ]
   ],
   "title": "Using likelihood ratios to perform utterance verification in automatic pronunciation assessment",
   "original": "e99_0173",
   "page_count": 4,
   "order": 48,
   "p1": "173",
   "pn": "176",
   "abstract": [
    "The aim of our current research is to investigate the possibility of using likelihood ratios to perform utterance verification within the context of automatic oral proficiency assessment. The likelihood ratios under investigation have the appealing feature that they may be computed simply by using an off-the-shelf automatic speech recognition system in two different recognition modes (forced and free phone) instead of using a system with specifically trained anti-models. We achieved 93% correct classification for 10 phonetically rich sentences uttered by 60 non-native language students.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-47"
  },
  "kawai99_eurospeech": {
   "authors": [
    [
     "Goh",
     "Kawai"
    ],
    [
     "Carlos Toshinori",
     "Ishi"
    ]
   ],
   "title": "A system for learning the pronunciation of Japanese pitch accent",
   "original": "e99_0177",
   "page_count": 4,
   "order": 49,
   "p1": "177",
   "pn": "182",
   "abstract": [
    "We propose a technique that associates the F0 of Japa-nese words with native-speaker perceptions of pitch ac-cents. Up till now, computer-aided prosody learning sys-tems that measure fundamental frequency (F0) have not mapped F0 values to perceptual thresholds of native speakers. Existing learning systems often instruct their students to fit F0 contours within a band designated by the system, but the band's boundaries do not to corre-spond to measures of acceptance by native speakers. Our method estimates intelligibility of pitch accent patterns based on perception experiments. Intelligibility scores motivate learners by guiding them towards meaningful goals.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-48"
  },
  "nouza99_eurospeech": {
   "authors": [
    [
     "Jan",
     "Nouza"
    ]
   ],
   "title": "Computer-aided spoken-language training with enhanced visual and auditory feedback",
   "original": "e99_0183",
   "page_count": 5,
   "order": 50,
   "p1": "183",
   "pn": "186",
   "abstract": [
    "A tool developed for computer-aided training of spoken language is presented in the paper. The tools envi-ronment utilizes both visual and auditory feedback information to help a user in learning pronunciation and intonation in L1 or L2. The learning is supported by displaying the users speech and its relevant pa-rameters (volume, F0 and spectrum) in parallel with multiple reference templates. The templates may be-long to the same utterance or make a minimum pair that can be used for contrastive training. The time plots are accompanied by textual labels (phonemes, syllables or words) that are automatically aligned to the users utterance and by plots that identify the regions with major deviations with respect to the reference tem-plates. The tool has been tested in two tasks: a) speech training of a deaf person and b) learning pronunciation and intonation in a foreign language.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-49"
  },
  "song99_eurospeech": {
   "authors": [
    [
     "Zhanjiang",
     "Song"
    ],
    [
     "Fang",
     "Zheng"
    ],
    [
     "Mingxing",
     "Xu"
    ],
    [
     "Wenhu",
     "Wu"
    ]
   ],
   "title": "An effective scoring method for speaking skill evaluation system",
   "original": "e99_0187",
   "page_count": 4,
   "order": 51,
   "p1": "187",
   "pn": "190",
   "abstract": [
    "The Speaking Skill Evaluation (SSE) technologies are derived from speech recognition technologies and are used for language learning and instructing. In this paper, an effective automatic pronunciation scoring method for SSE systems is proposed. The Center-Distance Continuous Probability Model (CDCPM) is incorporated to model the speech. The Merging-Based Syllable Detection Automaton (MBSDA) and the Non-Linear Partition (NLP) method are used to perform the time alignment. And the Critical Area Percentage (CAP) based scoring method is used to score the learners pronunciations or reject invalid utterances. Subjective assessments show that this method is concise, fast, and effective. The SSE system based on it has achieved a satisfying performance.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-50"
  },
  "santiagooriola99_eurospeech": {
   "authors": [
    [
     "Conception",
     "Santiago-Oriola"
    ]
   ],
   "title": "Vocal synthesis in a computerized dictation exercise",
   "original": "e99_0191",
   "page_count": 4,
   "order": 52,
   "p1": "191",
   "pn": "194",
   "abstract": [
    "Dictation can be considered as a transcription exercise of an utterance. The main difficulty for the elaboration of a dictation system consists in modeling the errors and the associated explanations provided to the learner. On these bases, an experimental DICTOR system is being developed as an assistant tool to learn the French language spelling. DICTOR includes an automatic checking tool based on a stochastic alignment algorithm and French written linguistic knowledge. This paper focus on both aspects. We discuss the spelling learning issues due to the no one-to-one correspondence between the utterance and the written text. Then the principles of the spelling difficulty groups (sdg) and associated explanation rules are presented. Finally, we report an observation of the DICTOR system in a school environment for the French language spelling. We then analyze how the different vocal synthesis influence the exercise and are perceived by the pupils in this new generation of language learning systems.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-51"
  },
  "trancoso99_eurospeech": {
   "authors": [
    [
     "Isabel",
     "Trancoso"
    ],
    [
     "Céu",
     "Viana"
    ],
    [
     "Isabel",
     "Mascarenhas"
    ],
    [
     "Carlos",
     "Teixeira"
    ]
   ],
   "title": "On deriving rules for nativised pronunciation in navigation queries",
   "original": "e99_0195",
   "page_count": 4,
   "order": 53,
   "p1": "195",
   "pn": "198",
   "abstract": [
    "Navigation queries are typical examples of contexts in which a recognizer may have to deal with non-native names. In order to build a pronunciation lexicon with these names, special GtoP rules may be derived. The paper addresses this problem in the context of naviga-tion queries in French including German names and vice-versa. The special GtoP rules were mostly based on statis-tics derived from cross-lingual spoken corpora.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-52"
  },
  "yvon99_eurospeech": {
   "authors": [
    [
     "Francois",
     "Yvon"
    ]
   ],
   "title": "Pronouncing unknown words using multi-dimensional analogies",
   "original": "e99_0199",
   "page_count": 4,
   "order": 54,
   "p1": "199",
   "pn": "202",
   "abstract": [
    "In this paper, a model of analogy-based learning is pre-sented, whose main novelty is the crucial ability to pro-duce analogies in multi-dimensional input and output spaces. Evaluations are performed on various word pro-nunciation tasks, revealing the effectiveness of such joint learning strategies.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-53"
  },
  "laczko99_eurospeech": {
   "authors": [
    [
     "Maria",
     "Laczko"
    ]
   ],
   "title": "Characteristics features of planning of speech and production of secondary schoolchildren's spontaneous speech",
   "original": "e99_0202",
   "page_count": 0,
   "order": 55,
   "p1": "202",
   "pn": "",
   "abstract": [
    "In this paper we analyse adolescents' speech production by comparing their use of planning strategies in terms of hesitation phenomena occurring in adolescents' spontaneous speech and their articulation rates and pauses. Our preliminary assumption was that students with a weak ability of speech as a poor linguistic skill is characterized by more pauses and hesitation phenomena occurring in their speech because the poor speaker does not exactly know what he will say consequently more number of their hesitation in the first part of the speech planning process can frequently appear in the second part of it as in the production. Furthermore we try to discuss: i) whether it is possible to demonstrate the existence of any interrelation between articulation rate and pauses in their spontaneous speech in terms of their abilty of speech as a characteristic feature; ii) can articulete rate and hesitation phenomena be used as objective indicators of their linguistic skill. A series of experiments has been carried out with the participation of secondary school students as 'good' and 'poor' speakers. In order to discuss the questions mentioned above 5-5 minutes of spontaneous speech samples of 17 year old students (number of them was 10 ) were recorded for analyses.\n",
    ""
   ]
  },
  "byrne99_eurospeech": {
   "authors": [
    [
     "William",
     "Byrne"
    ],
    [
     "Asela",
     "Gunawardana"
    ]
   ],
   "title": "Discounted likelihood linear regression for rapid adaptation",
   "original": "e99_0203",
   "page_count": 4,
   "order": 56,
   "p1": "203",
   "pn": "206",
   "abstract": [
    "Rapid adaptation schemes that employ the EM algorithm may suffer from overtraining problems when used with small amounts of adaptation data. An algorithm to alleviate this problem is derived within the information geometric framework of Csiszár and Tusnády, and is used to improve MLLR adaptation on NAB and Switchboard adaptation tasks. It is shown how this algorithm approximately optimizes a discounted likelihood cri-terion.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-54"
  },
  "chien99_eurospeech": {
   "authors": [
    [
     "Jen-Tzung",
     "Chien"
    ],
    [
     "Jean-Claude",
     "Junqua"
    ],
    [
     "Philippe",
     "Gelin"
    ]
   ],
   "title": "Extraction of reliable transformation parameters for unsupervised speaker adaptation",
   "original": "e99_0207",
   "page_count": 4,
   "order": 57,
   "p1": "207",
   "pn": "210",
   "abstract": [
    "Adaptation of speaker-independent hidden Markov models (HMMs) to a new speaker using speaker-specific data is an effective approach to reinforce speech recognition performance for the enrolled speaker. Practically, it is desirable to flexibly perform the adaptation without any knowledge or limitation on the enrolled adaptation data (e.g. data transcription, length and content). However, the inevitable transcription errors on adaptation data may cause unreliability in model adaptation. The variable amount and content of adaptation data require the algorithm to dynamically control the degrees of sharing in transformation-based adaptation. This paper presents an unsupervised hierarchical adaptation algorithm where a tree structure of HMMs is incorporated to control the transformation sharing. To extract reliable transformation parameters, we exploit the reliability assessment criteria using the confidence measure and description length. Experiments show that the unsupervised speaker adaptation with reliability assessment can significantly improve the recognition performance for any lengths of adaptation data.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-55"
  },
  "chesta99_eurospeech": {
   "authors": [
    [
     "Cristina",
     "Chesta"
    ],
    [
     "Olivier",
     "Siohan"
    ],
    [
     "Chin-Hui",
     "Lee"
    ]
   ],
   "title": "Maximum a posteriori linear regression for hidden Markov model adaptation",
   "original": "e99_0211",
   "page_count": 4,
   "order": 58,
   "p1": "211",
   "pn": "214",
   "abstract": [
    "In the past few years, transformation-based model adaptation techniques have been widely used to help reducing acoustic mismatch between training and testing conditions of automatic speech recognizers. The estimation of the transformation parameters is usually carried out using estimation paradigms based on classical statistics such as maximum likelihood, mainly because of their conceptual and computational simplicity. However, it appears necessary to introduce some constraints on the possible values of the transformation parameters to avoid getting unreasonable estimates that might perturb the underlying structure of the acoustic space. In this paper, we propose to introduce such constraints using Bayesian statistics, where a prior distribution of the transformation parameters is used. A Bayesian counter-part of the well known maximum likelihood linear regression (MLLR) adaption is formulated based on maximum a posteriori (MAP) estimation. Supervised, unsupervised and incremental non-native speaker adaptation experiments are carried out to compare the proposed MAPLR approach to MLLR. Experimental results show that MAPLR outperforms MLLR.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-56"
  },
  "hariharan99_eurospeech": {
   "authors": [
    [
     "Ramalingam",
     "Hariharan"
    ],
    [
     "Olli",
     "Viikki"
    ]
   ],
   "title": "On combining vocal tract length normalisation and speaker adaptation for noise robust speech recognition",
   "original": "e99_0215",
   "page_count": 4,
   "order": 59,
   "p1": "215",
   "pn": "218",
   "abstract": [
    "This paper investigates the combination of vocal tract length normalisation and speaker adaptation in con-nected digit recognition. In particular, we focus on performing this task under a continuously varying car noise environment. Continuous supervised speaker and environment adaptation is carried out on the test data according to the Bayesian framework. The paper also evaluates various approaches to implement vocal tract length normalisation. The best performance was obtained when the normalisation was performed during both initial speaker-independent training and testing. It was also noticed that, during testing, speaker specific normalisation produced better results than utterance specific normalisation. Our experimental results on the connected digit database show that the joint approach outperforms the system in which on-line Bayesian speaker adaptation is performed on HMM mean parameters. The performance gain was particularly high with so called outlier speakers for whom adaptation is truly needed.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-57"
  },
  "rottland99_eurospeech": {
   "authors": [
    [
     "Jörg",
     "Rottland"
    ],
    [
     "Christoph",
     "Neukirchen"
    ],
    [
     "Daniel",
     "Willett"
    ],
    [
     "Gerhard",
     "Rigoll"
    ]
   ],
   "title": "Speaker adaptation using regularization and network adaptation for hybrid MMI-NN/HMM speech recognition",
   "original": "e99_0219",
   "page_count": 4,
   "order": 60,
   "p1": "219",
   "pn": "222",
   "abstract": [
    "This paper describes, how to perform speaker adaptation for a hybrid large vocabulary speech recognition system. The hybrid system is based on a Maximum Mutual Information Neural Net-work (MMINN), which is used as a Vector Quantizer (VQ) for a discrete HMM speech recognizer. The combination of MMINNs and HMMs has shown good performance on several large vocabulary speech recognition tasks like RM and WSJ. This paper now presents two approaches to perform speaker adaptation with this hybrid system. The first approach is a trans-formation of the feature space, which is performed by a neural network with maximum likelihood (ML) as objective function for the complete system, which means, that the parameters of the NN are estimated in order to match the HMM-parameters of the pretrained speaker independent system. The second approach is to adapt the HMM parameters depending on the amount of training data available per HMM, using a regularization approach. Both approaches can be applied jointly, which further improves the recognition accuracy.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-58"
  },
  "alter99_eurospeech": {
   "authors": [
    [
     "Kai",
     "Alter"
    ],
    [
     "Annett",
     "Schirmer"
    ],
    [
     "Sonja A.",
     "Kotz"
    ],
    [
     "Angela D.",
     "Friederici"
    ]
   ],
   "title": "Prosodic phrasing and accentuation in speech production of patients with right hemisphere lesions",
   "original": "e99_0223",
   "page_count": 4,
   "order": 61,
   "p1": "223",
   "pn": "226",
   "abstract": [
    "While the left hemispheric role in speech production has been extensively studied the role of the right hemisphere is less explored. Therefore, this paper addressed the question to what extend the right hemisphere is involved in prosodic production in an intonational language such as German. In particular, we explored the speech production of three patients with right hemisphere damage and compared these data to that of matched controls. We used a question-answer test of 48 German sentences with different verb-argument structures and varying focus positions. Healthy subjects correctly differentiated between the syntactic conditions and between the wide and narrow focused sentences by correlating prosodic parameters with cues for Intonational Phrasing as well as for accent distribution and accent types. In comparison, patients had difficulties in realizing prominence/accent distribution. Additionally, they seem to have a decreasing capacity for durational control as indicated by missing prefinal lengthening in prosodic domains.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-59"
  },
  "goto99_eurospeech": {
   "authors": [
    [
     "Masataka",
     "Goto"
    ],
    [
     "Katunobu",
     "Itou"
    ],
    [
     "Satoru",
     "Hayamizu"
    ]
   ],
   "title": "A real-time filled pause detection system for spontaneous speech recognition",
   "original": "e99_0227",
   "page_count": 4,
   "order": 62,
   "p1": "227",
   "pn": "230",
   "abstract": [
    "This paper describes a method for automatically detecting filled (vocalized) pauses, which are one of the hesitation phenomena that current speech recognizers typically cannot handle. The detection of these pauses is important in spontaneous speech dialogue systems because they play valuable roles, such as helping a speaker keep a conversational turn, in oral communication. Although a few speech recognition systems have processed filled pauses within subword-based connected word recognition or word-spotting frameworks, they did not detect the pauses indi-vidually and consequently could not consider their roles. In this paper we propose a method that detects filled pauses and word lengthening on the basis of small fundamental frequency transition and small spectral envelope deformation under the assumption that speakers do not change articulator parameters during filled pauses. Experimental results for a Japanese spoken dialogue corpus show that our real-time filled-pause-detection system yielded a recall rate of 84.9% and a precision rate of 91.5%.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-60"
  },
  "iwano99_eurospeech": {
   "authors": [
    [
     "Koji",
     "Iwano"
    ]
   ],
   "title": "Prosodic word boundary detection using mora transition modeling of fundamental frequency contours -speaker independent experiments-",
   "original": "e99_0231",
   "page_count": 4,
   "order": 63,
   "p1": "231",
   "pn": "234",
   "abstract": [
    "We have been developing a reliable method for prosodic word boundary detection for Japanese continuous speech based on the discrete hidden Markov modeling of fundamental frequency (F0 ) contours in mora unit. Although a favorable result was obtained for ATR continuous speech corpus as reported already, experiments were done only on closed conditions. This paper reports the results on open and speaker-independent cases using database by two speakers. On average, detection rate reached around 76.0% with insertion error rate of 18.4%. Degradation from the closed condition experiment was only a little, showing the validity of the method for open conditions.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-61"
  },
  "warnke99_eurospeech": {
   "authors": [
    [
     "Volker",
     "Warnke"
    ],
    [
     "Florian",
     "Gallwitz"
    ],
    [
     "Anton",
     "Batliner"
    ],
    [
     "Jan",
     "Buckow"
    ],
    [
     "R.",
     "Huber"
    ],
    [
     "Elmar",
     "Nöth"
    ],
    [
     "A.",
     "Höthker"
    ]
   ],
   "title": "Integrating multiple knowledge sources for word hypotheses graph interpretation",
   "original": "e99_0235",
   "page_count": 4,
   "order": 64,
   "p1": "235",
   "pn": "238",
   "abstract": [
    "We present an integrated approach for the interpretation of word hypotheses graphs (WHGs) using multiple knowledge sources. Commonly, different knowledge sources in speech understanding are applied sequentially. Typically, speech understanding systems, such as the {\\vm} speech-to-speech translation system, first use a word recognizer to determine word hypotheses, only based on acoustic and language model (LM) information. The resulting word sequences or WHGs are then segmented according to syntactic and/or prosodic information. Finally, these segments are interpreted by a parser or a stochastic process. Thus, it is impossible to use the knowledge of the syntactic-prosodic process, the parser or any other subsequent process to find the best word sequence. In our new approach we use acoustic, prosodic and LM information to determine the best word chain, to detect syntactic/prosodic/pragmatic phrase boundaries and to classify dialog acts in one integrated search procedure, based on a WHG or a word lattice.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-62"
  },
  "yang99_eurospeech": {
   "authors": [
    [
     "Li-chiung",
     "Yang"
    ]
   ],
   "title": "Prosodic correlates of interruptions in spoken dialogue",
   "original": "e99_0239",
   "page_count": 4,
   "order": 65,
   "p1": "239",
   "pn": "242",
   "abstract": [
    "In this paper, we investigate the prosody and functions of interruptions in natural dialogue, and the role of interruption-initiated actions in spoken dialogue systems. The specific forms of prosody are both an indicator of cognitive state and a reflection of negotiation strategies to mutually guide a dialogue, with pitch and amplitude values signaling the degree of concordance with the current topics. These prosodic forms are potentially usable in spoken dialogue system to porvide intelligent responding systems that are responsive to human motivations in dialogues.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-63"
  },
  "constantinides99_eurospeech": {
   "authors": [
    [
     "Paul C.",
     "Constantinides"
    ],
    [
     "Alexander I.",
     "Rudnicky"
    ]
   ],
   "title": "Dialog analysis in the carnegie mellon communicator",
   "original": "e99_0243",
   "page_count": 4,
   "order": 66,
   "p1": "243",
   "pn": "246",
   "abstract": [
    "In this paper, we present a formative evaluation procedure that we have applied to the Communicator dialog system. In the system improvement process, we have recognized the need to identify interaction failures through passive observation of system use. By systematizing the process of dialog evaluation, we hope to gain a mechanism for effectively communicating descriptions of interaction failures, specifically for use in system improvement. Additionally, we argue that this process can be taught to and executed by an evaluator external to the system development process, with the same proficiency as someone intimately familiar with the mechanics of the system components.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-64"
  },
  "fiscus99_eurospeech": {
   "authors": [
    [
     "Jon",
     "Fiscus"
    ],
    [
     "George",
     "Doddington"
    ],
    [
     "John",
     "Garofolo"
    ],
    [
     "Alvin",
     "Martin"
    ]
   ],
   "title": "NIST's 1998 topic detection and tracking evaluation (TDT2)",
   "original": "e99_0247",
   "page_count": 4,
   "order": 67,
   "p1": "247",
   "pn": "250",
   "abstract": [
    "This paper presents a summary of the 1998 Topic Detection and Tracking (TDT) tasks and the results of the 1998 TDT evaluation. The purpose of TDT is to develop technologies for retrieval and automatic organization of Broadcast News and Newswire stories and to evaluate the performance of those technologies. The TDT project builds on and extends the technologies of Automatic Speech Recognition and Document Retrieval with three tasks: 1) Story Segmentation, 2) Topic Detection and 3) Topic Tracking. Each of the tasks simulates a hypothetical operational system that requires incoming data to be processed time synchronously. The 1998 TDT evaluation (TDT2) continues the work of the TDT pilot study conducted in 1997 (TDT1) and is the first open evaluation of TDT tasks.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-65"
  },
  "sonntag99_eurospeech": {
   "authors": [
    [
     "Gerit P.",
     "Sonntag"
    ],
    [
     "Thomas",
     "Portele"
    ],
    [
     "Felicitas",
     "Haas"
    ],
    [
     "Joachim",
     "Köhler"
    ]
   ],
   "title": "Comparative evaluation of six German TTS systems",
   "original": "e99_0251",
   "page_count": 4,
   "order": 68,
   "p1": "251",
   "pn": "254",
   "abstract": [
    "An application-specific perceptual evaluation was carried out in order to compare six high-quality German text-to-speech systems. Subjects judged the systems reading of an email message and a newspaper article according to four application-specific questions and six voice quality attributes. The results indicate significant differences between the systems. Possible applications of the systems were judged rather unfavourably. The main reasons for this proved to be the synthetic prosody and voice quality. Errors concerning text conversion were less important.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-66"
  },
  "steeneken99_eurospeech": {
   "authors": [
    [
     "Herman J. M.",
     "Steeneken"
    ]
   ],
   "title": "Standardisation of ergonomic assessment of speech communication",
   "original": "e99_0255",
   "page_count": 4,
   "order": 69,
   "p1": "255",
   "pn": "258",
   "abstract": [
    "The purpose of standardisation of the ergonomic assessment of speech communications is to assure a certain level of speech communication quality for various applications. The quality of speech communications is assessed in case of warning, danger, or information messages for work places, public areas, meeting rooms, and auditoria. In many applications direct communication between humans is considered while in other applications the use of electro-acoustic systems (e.g., PA systems) will be the most convenient means of informing and instructing people present. A standard on this subject, under the responsibility of ISO and CEN, is in preparation. The draft version of this standard covers criteria for speech communication qual-ity in various applications, methods to predict the speech transmission quality, and methods to assess the quality (subjective and objective). Examples of several applica-tions are given in annexes of that standard.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-67"
  },
  "suzuki99_eurospeech": {
   "authors": [
    [
     "Noriko",
     "Suzuki"
    ],
    [
     "Yugo",
     "Takeuchi"
    ],
    [
     "Kazuo",
     "Ishii"
    ],
    [
     "Michio",
     "Okada"
    ]
   ],
   "title": "Evaluation of affiliation in interaction with autonomous creatures",
   "original": "e99_0259",
   "page_count": 4,
   "order": 70,
   "p1": "259",
   "pn": "262",
   "abstract": [
    "The paper presents an evaluation method to assess impressions toward a computer-generated creature through human-computer interaction. The aim of our study is to construct a mechanism on a computer to realize empathic interaction between humans and creatures. In this paper, we examine the effect on personality cognition of a virtual creature when the creature replies with different timing to a human utterances. The creature has an abstract appearance and a non-linguistic voice. We control the following three conditions for timing of the creature's response: a) alternation, b) overlap, and c) neutral condition. As a result, we found that the evaluation value of the alternation condition shows more positive behavior than that of the overlap and neutral conditions when the creature has the above features.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-68"
  },
  "bauer99_eurospeech": {
   "authors": [
    [
     "Josef G.",
     "Bauer"
    ],
    [
     "Jochen",
     "Junkawitsch"
    ]
   ],
   "title": "Accurate recognition of city names with spelling as a fall back strategy",
   "original": "e99_0263",
   "page_count": 4,
   "order": 71,
   "p1": "263",
   "pn": "266",
   "abstract": [
    "Entering city names is an important issue for various speech driven applications such as telephone directory assistance. This paper proposes a system that combines word recognition with utterance verification and spelling as a fall back strategy. Word recognition experiments show that the use of a medium size vocabulary yields the lowest error rate when only permitting very few false ac-ceptances. For letter recognition discriminatively trained letter models are applied. In order to shorten the spelling procedure two abort conditions are introduced which re-duce the number of letters that have to be spelled. The system handles 96.5% of all calls correctly while less than 45% of all callers must spell the name.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-69"
  },
  "bartkova99_eurospeech": {
   "authors": [
    [
     "Katarina",
     "Bartkova"
    ],
    [
     "Denis",
     "Jouvet"
    ]
   ],
   "title": "Selective prosodic post-processing for improving recognition of French telephone numbers",
   "original": "e99_0267",
   "page_count": 4,
   "order": 72,
   "p1": "267",
   "pn": "270",
   "abstract": [
    "This study describes a selective prosodic post-processing procedure for improving the recognition of telephone numbers in French. The aim of the post-processing procedure is to recover recognition errors made by an HMM based ASR system. Instead of a global post-processing, this paper proposes a selective one. Post-processing is carried out only on some recognised numbers and only if its associated frequent confusion is also present in the N-best candidates. In such a case the discrimination between the solutions is carried out by checking the duration of a specific segment in a pertinent prosodic position. On the different data used in this study, about 23 % of the substitution errors are considered as being possible to recover with the selective duration post-processing and of this amount about 40 % of the errors are actually recovered.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-70"
  },
  "chang99_eurospeech": {
   "authors": [
    [
     "Eric I.",
     "Chang"
    ]
   ],
   "title": "Improving rejection with semantic slot-based confidence scores",
   "original": "e99_0271",
   "page_count": 12,
   "order": 73,
   "p1": "271",
   "pn": "274",
   "abstract": [
    "This paper introduces a new confidence scoring mechanism which takes advantage of the semantic parsing provided by a natural language understanding system. As a result, different segments of the user input which fill individual semantic slots are identified and individual semantic slot confidence scores are generated. With slot-based confidence scores, each individual slot receives a confidence score generated by combining the confidence scores of the individual words which fill that particular slot. With judicious use of confidence scores for individual slots, better rejection and more natural interaction with the system are achieved.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-71"
  },
  "davies99_eurospeech": {
   "authors": [
    [
     "K.",
     "Davies"
    ],
    [
     "R.",
     "Donovan"
    ],
    [
     "M.",
     "Epstein"
    ],
    [
     "Martin",
     "Franz"
    ],
    [
     "Abraham",
     "Ittycheriah"
    ],
    [
     "E. E.",
     "Jan"
    ],
    [
     "J. M.",
     "LeRoux"
    ],
    [
     "David",
     "Lubensky"
    ],
    [
     "Chalapathy",
     "Neti"
    ],
    [
     "Mukund",
     "Padmanabhan"
    ],
    [
     "K.",
     "Papineni"
    ],
    [
     "Salim",
     "Roukos"
    ],
    [
     "A.",
     "Sakrajda"
    ],
    [
     "Jeffrey S.",
     "Sorensen"
    ],
    [
     "B.",
     "Tydlitat"
    ],
    [
     "T.",
     "Ward"
    ]
   ],
   "title": "The IBM conversational telephony system for financial applications",
   "original": "e99_0275",
   "page_count": 4,
   "order": 74,
   "p1": "275",
   "pn": "278",
   "abstract": [
    "We describe our development work on a telephone-based conversational system in the domain of mutualfund transactions. This system uses several components including robust large vocabulary continuous speech recognition, natural language understanding, dialog management, and text-to-speech synthesis technologies.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-72"
  },
  "elmeliani99_eurospeech": {
   "authors": [
    [
     "Rachida",
     "El Méliani"
    ],
    [
     "Douglas",
     "OShaughnessy"
    ]
   ],
   "title": "Error spotting using syllabic fillers in spontaneous conversational speech recognition",
   "original": "e99_0279",
   "page_count": 4,
   "order": 75,
   "p1": "279",
   "pn": "282",
   "abstract": [
    "Spontaneous conversational phone-call speech databases are difficult to recognize because of the large variation of speech rates, of pronunciations as well as noises, of acoustic degradations from the telephone channel, and of an unpredictible non-grammatical language structure including many random phenomena. Each cause of mis-recognition can be addressed separately; however there is still no satisfying solution. As a misrecognition is considered by the system as being a kind of new word, we propose to apply here our keyword spotting and new-word detection technology. However because of the large variety of types of misrecognitions and of the lack of information on where, why and how they occur, we had to define a different language model from those used in previous work. Results show a noticeable recognition improvement, often associated with a decrease in the number of substitutions and a slight increase in the number of the deletions.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-73"
  },
  "jouvet99_eurospeech": {
   "authors": [
    [
     "Denis",
     "Jouvet"
    ],
    [
     "Jean",
     "Monné"
    ]
   ],
   "title": "Recognition of spelled names over the telephone and rejection of data out of the spelling lexicon",
   "original": "e99_0283",
   "page_count": 4,
   "order": 76,
   "p1": "283",
   "pn": "286",
   "abstract": [
    "This paper deals with the recognition of spelled names over the telephone. It introduces an efficient way of handling the spelling grammar, that is the lexicon of the allowed spelled names. The proposed approach is based on a forward-backward algorithm. The constraints on the sequences of letters are derived from the lexicon and are used by the A* algorithm in the backward pass. This forward-backward approach is compared to a 2-pass ap-proach, which relies on a discrete HMM based retrieval procedure. The rejection of incorrect data is also investi-gated, based on the comparison of a lexicon constrained solution with an unconstrained decoding. The ap-proaches are compared on field data collected from a vocal directory service. Results are presented for the recognition of valid spelled names and for the rejection of incorrect data (non-spelling and noise tokens and spellings not in the lexicon). The results show the efficiency of the proposed forward-backward procedure.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-74"
  },
  "koo99_eurospeech": {
   "authors": [
    [
     "Myoung-Wan",
     "Koo"
    ],
    [
     "Sun-Jeong",
     "Lee"
    ]
   ],
   "title": "An utterance verification system based on subword modeling for a vocabulary independent speech recognition system",
   "original": "e99_0287",
   "page_count": 4,
   "order": 77,
   "p1": "287",
   "pn": "290",
   "abstract": [
    "This paper describes a Korean utterance verification system based on subword modeling for a vocabulary independent speech recognition system. We deploy strategy consisting of two modules: recognition and verification, for utterance verification. In the stage of recognition, multiple hypotheses with hypothesized word boundaries obtained through Viterbi segmentation of the utterance are obtained. And likelihood ratio is used as a post-processor for rejecting unlikely hypothesis in the stage of verification. Our study is focused on the verification module. First, we make a comparative study on averaging methods for obtaining the confidence measure for words from the log likelihood ratio based on phone. Three kinds of average techniques were investigated as arithmetic, geometric, and harmonic averages. Second, we study the effect of cohort set, which is the most competitive units to subword units. One cohort set model is trained for each subword. We found out the size of the cohort set for best recognition result. Finally, we present how to model anti-models for each context-dependent units. Three kinds of approaches are studied. The first one is to use cohort set based on context-independent unit to simplify the calculation. The second one is use cohort set based on context-dependent unit, which is obtained by phone recognizer based on context-dependent units. The final one is to use cohort set based on hybrid units. We make a comparative study on each approach.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-75"
  },
  "moreau99_eurospeech": {
   "authors": [
    [
     "Nicolas",
     "Moreau"
    ],
    [
     "Denis",
     "Jouvet"
    ]
   ],
   "title": "Use of a confidence measure based on frame level likelihood ratios for the rejection of incorrect data",
   "original": "e99_0291",
   "page_count": 4,
   "order": 78,
   "p1": "291",
   "pn": "294",
   "abstract": [
    "Interactive vocal services are based on speech recognition systems which must be able to reject efficiently incorrect utterances such as out-of-vocabulary or noise tokens. A possible approach is a post-processing of the hypotheses delivered by the recogniser, based on the computation of a confidence measure (CM). A recognition hypothesis is rejected if its CM is below a chosen threshold. This paper presents a new way of computing a CM on a recognition hypothesis, based on the calculation of a likelihood ratio for each acoustic frame of the utterance. Promising results are reported on a large vocabulary of a telephone directory task. Significant falls in the error rates are observed, compared to a reference system which include a garbage model, with no post-processing of the recognised words.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-76"
  },
  "maciasguarasa99_eurospeech": {
   "authors": [
    [
     "J.",
     "Macías-Guarasa"
    ],
    [
     "J.",
     "Ferreiros"
    ],
    [
     "A.",
     "Gallardo"
    ],
    [
     "R.",
     "San-Segundo"
    ],
    [
     "Juan Manuel",
     "Pardo"
    ],
    [
     "L.",
     "Villarrubia"
    ]
   ],
   "title": "Variable preselection list length estimation using neural networks in a telephone speech hypothesis-verification system",
   "original": "e99_0295",
   "page_count": 4,
   "order": 79,
   "p1": "295",
   "pn": "298",
   "abstract": [
    "At ICSLP98 we presented some preliminary results on automatic preselection list length estimation using parametric and non-parametric techniques, for a flexible large and very large vocabulary, speaker independent, isolated-word hypothesis generation system in a telephone environment, with vocabularies of up to 10000 words. In the baseline system, the preselection module generates a fixed-length list of candidate words, to be given to the verification stage. Our idea is making this length variable, depending on any known-in-advance system parameter, to allow decreasing computational demands. In this paper we present a novel approach to preselection list length estimation. A neural network is used to give an initial estimate of the required length, which is further processed to obtain a final value. The key factor to evaluate different methods is calculating the average preselection list length (effort) while keeping the required error rate.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-77"
  },
  "pfau99_eurospeech": {
   "authors": [
    [
     "Thilo",
     "Pfau"
    ],
    [
     "Robert",
     "Faltlhauser"
    ],
    [
     "Günther",
     "Ruske"
    ]
   ],
   "title": "Speaker normalization and pronunciation variant modeling: helpful methods for improving recognition of fast speech",
   "original": "e99_0299",
   "page_count": 4,
   "order": 80,
   "p1": "299",
   "pn": "302",
   "abstract": [
    "The presented paper addresses the problem of creating hidden Markov models for fast speech. The major issues discussed are robust parameter estimation and reducing within-model variations. Regarding the first issue, the use of the maximum a posteriori parameter estimation is discussed. To reduce within-model variations, a maximum likelihood based vocal tract length normalization procedure and a statistical approach to model pronunciation variants are applied. Experiments with a large vocabulary continuous speech recognition system were carried out on the German spontaneous scheduling task (Verbmobil) to prove the effectiveness of the investigated methods. The results show that a combination of pronunciation variant modeling and vocal tract length normalization is most effective. On fast speech, a relative improvement of 16.3% compared to the baseline models was achieved. Pronunciation variant modeling combined with the maximum a posteriori reestimation proved to be the second best method resulting in a 14.9% relative improvement. In addition, this combination does not cause any additional computational load during recognition.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-78"
  },
  "rose99_eurospeech": {
   "authors": [
    [
     "Richard C.",
     "Rose"
    ],
    [
     "Giuseppe",
     "Riccardi"
    ]
   ],
   "title": "Automatic speech recognition using acoustic confidence conditioned language models",
   "original": "e99_0303",
   "page_count": 4,
   "order": 81,
   "p1": "303",
   "pn": "306",
   "abstract": [
    "A modified decoding algorithm for automatic speech recognition (ASR) will be described which facilitates a closer coupling between the acoustic and language modeling components of a speech recognition system. This closer coupling is obtained by extracting word level measures of acoustic confidence during decoding, and making coded representations of these confidence measures available to the ASR network during decoding. A simulation of this decoding strategy is implemented using a word lattice rescoring paradigm. A joint acoustic{language model will be described where linguistic context is augmented to include the encoded values of acoustic confidence. Finally, the performance of the word lattice based implementation of the decoding algorithm will be evaluated on a large vocabulary natural language understanding task.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-79"
  },
  "strom99_eurospeech": {
   "authors": [
    [
     "Volker",
     "Strom"
    ],
    [
     "Henrik",
     "Heine"
    ]
   ],
   "title": "Utilizing prosody for unconstrained morpheme recognition",
   "original": "e99_0307",
   "page_count": 4,
   "order": 82,
   "p1": "307",
   "pn": "310",
   "abstract": [
    "Speech recognition systems for languages with a rich in ectional morphology (like German) suffier from the limitations of a word-based full-form lexicon. Although the morphological and acoustical knowledge about words is coded implicitly within the lexicon entries (which are usually closely related to the orthography of the language at hand) this knowledge is usually not explicitly available for other tasks (e.g. detecting OOV words). This paper presents an HMM-based `word' recognizer that uses morphemes on the string level for recognizing spontaneous German conversational speech ( Verbmobil corpus). The system has no explicit word knowledge but uses a morpheme{bigram to capture the German word and sentence structure to some extent. The morpheme recognizer is tightly coupled with a prosodic classifier in order to compensate for some of the additional ambiguity introduced by using morphemes instead of words. Although the recognizer's morpheme accuracy of 85:3% is comparable to that of our word-based decoder (word accuracy 86%) until now the benefit of introducing the prosodic classifier is not yet clear.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-80"
  },
  "stolcke99_eurospeech": {
   "authors": [
    [
     "Andreas",
     "Stolcke"
    ],
    [
     "Elizabeth",
     "Shriberg"
    ],
    [
     "Dilek",
     "Hakkani-Tür"
    ],
    [
     "Gökhan",
     "Tür"
    ]
   ],
   "title": "Modeling the prosody of hidden events for improved word recognition",
   "original": "e99_0311",
   "page_count": 4,
   "order": 83,
   "p1": "311",
   "pn": "314",
   "abstract": [
    "We investigate a new approach for using speech prosody as a knowledge source for speech recognition. The idea is to penal­ize word hypotheses that are inconsistent with prosodic features such as duration and pitch. To model the interaction between words and prosody we modify the language model to represent hidden events such as sentence boundaries and various forms of disfluency, and combine with it decision trees that predict such events from prosodic features. N­best rescoring experiments on the Switchboard corpus show a small but consistent reduction of word error as a result of this modeling. We conclude with a preliminary analysis of the types of errors that are corrected by the prosodically informed model.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-81"
  },
  "wessel99_eurospeech": {
   "authors": [
    [
     "Frank",
     "Wessel"
    ],
    [
     "Klaus",
     "Macherey"
    ],
    [
     "Hermann",
     "Ney"
    ]
   ],
   "title": "A comparison of word graph and n-best list based confidence measures",
   "original": "e99_0315",
   "page_count": 4,
   "order": 84,
   "p1": "315",
   "pn": "318",
   "abstract": [
    "In this paper we present and compare several confidence mea-sures for large vocabulary continuous speech recognition. We show that posterior word probabilities computed on word graphs and N-best lists clearly outperform non-probabilistic confidence measures, e.g. the acoustic stability and the hypothesis density. In addition, we prove that the estimation of posterior word prob-abilities on word graphs yields better results than their estimation on N-best lists and discuss both methods in detail. We present experimental results on three different corpora, the English NAB 94 20k development corpus, the German VERBMOBIL 96 evaluation corpus and a Dutch corpus, which has been recorded with a train timetable information system in the ARISE project.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-82"
  },
  "pratzas99_eurospeech": {
   "authors": [
    [
     "Marcus M.",
     "Prätzas"
    ],
    [
     "Ulrich",
     "Balss"
    ],
    [
     "Herbert",
     "Reininger"
    ],
    [
     "Harald",
     "Wüst"
    ]
   ],
   "title": "C++ software environment for speech signal processing",
   "original": "e99_0319",
   "page_count": 4,
   "order": 85,
   "p1": "319",
   "pn": "322",
   "abstract": [
    "Here we present the C++ library SPC (Speech Signal Processing Classes) as development tool for assembling of speech processing applications. SPC offers real-time processing, batch processing of large databases, visualization, and analysis of signals between processing steps. In SPC the data stream occurring in speech processing is partitioned in three different information flows: signal data, control information and visualization data. Because hardware dependent program code is limited exclusively to some special methods, SPC can be adapted to different hardware environments easily. System specific code is encapsulated in low level parts of SPC and SPC user programs can be compiled on various plat-forms without any changes in source code. Up to now SPC supports Windows 95/98/NT, IBM AIX and LINUX.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-83"
  },
  "ma99_eurospeech": {
   "authors": [
    [
     "Kun",
     "Ma"
    ],
    [
     "Pelin",
     "Demirel"
    ],
    [
     "Carol",
     "Espy-Wilson"
    ],
    [
     "Joel",
     "MacAuslan"
    ]
   ],
   "title": "Improvement of electrolaryngeal speech by introducing normal excitation information",
   "original": "e99_0323",
   "page_count": 4,
   "order": 86,
   "p1": "323",
   "pn": "326",
   "abstract": [
    "In electrolaryngeal speech, an excitation signal is provided by means of a buzzer held against the neck which is usually operated at a constant frequency rate. While such Transcutaneous Artificial Larynges (TALs) provide a means for verbal communication for people who are unable to use their own, the monotone F0 pattern results in poor speech quality. In the present study, cepstral analysis was used to replace the original F0 contour of the TAL speech with a normal F0 pattern. Spectral analysis shows that this substitution results in two changes: (a) a varying F0 contour and (b) removal of steady background noise due to the leakage of acoustic energy. Perceptual tests were conducted to assess speech, before and after cepstral processing, produced by four laryngectomized speakers (2 males and 2 females) All speakers used the Servox TAL. The results indicate a clear preference for the processed speech.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-84"
  },
  "ittycheriah99_eurospeech": {
   "authors": [
    [
     "Abraham",
     "Ittycheriah"
    ],
    [
     "Richard J.",
     "Mammone"
    ]
   ],
   "title": "Detecting user speech in barge-in over prompts using speaker identification methods",
   "original": "e99_0327",
   "page_count": 4,
   "order": 87,
   "p1": "327",
   "pn": "330",
   "abstract": [
    "In this paper, we investigate the use of a speaker identification technique to solve the bargein speechdetection problem. This scenario is a very simpleapplication of speaker identification since only twousers are involved. This is further simplified by thefact that the prompt speaker can be modelled apriori. Additionally, the user can be modelled as wellimproving the performance of the system on subsequent utterances. In the system described below, weexplicitly model several non-speech sounds such aslaughter, coughs and breath noises. We show thatthis technique is generally better than that of currentmethods which measure the ratio of incoming speechenergy to that of the prompt signal being played.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-85"
  },
  "lobanov99_eurospeech": {
   "authors": [
    [
     "Boris",
     "Lobanov"
    ],
    [
     "T.",
     "Levkovskaya"
    ],
    [
     "Igor E.",
     "Kheidorov"
    ]
   ],
   "title": "Speaker and channel-normalized set of formant parameters for telephone speech recognition",
   "original": "e99_0331",
   "page_count": 4,
   "order": 88,
   "p1": "331",
   "pn": "334",
   "abstract": [
    "The speech parameters, most commonly used nowadays, are Cepstral coefficients derived from FFT or LPC Spectrum. An alternative approach that can potentially provide maximum speaker and channel independence is estimation of articulatory based features such as formant frequencies, amplitudes and voicing degree. A present report describes a new method and algorithm of robust estimation of F1(t), F2(t), F3(t), A1(t),A2(t), A3(t), V(t) from telephone speech signal, and also the procedures of their normalization against speaker and channel variability. The results obtained from the experiments confirm the efficiency of the suggested set of formant parameters in a view of speech signal speaker  and channel variability resistance. According to the experiments it gives significant improvement in the recognition performance as compared with cepstral parameters use.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-86"
  },
  "liew99_eurospeech": {
   "authors": [
    [
     "Alan W.C.",
     "Liew"
    ],
    [
     "K. L.",
     "Sum"
    ],
    [
     "S. H.",
     "Leung"
    ],
    [
     "Wai H.",
     "Lau"
    ]
   ],
   "title": "Fuzzy segmentation of lip image using cluster analysis",
   "original": "e99_0335",
   "page_count": 4,
   "order": 89,
   "p1": "335",
   "pn": "338",
   "abstract": [
    "A clustering-based lip segmentation algorithm is described here. The fuzzy clustering algorithm presented here is able to take into account the local smoothness property of image data. The objective functional of our algorithm utilizes a new distance metric that takes into account the influence of the neighboring pixels on the centre pixel in a 3 by 3 window. Computational steps involved in the segmentation of color lip image and segmentation result for a set of color lip images are given.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-87"
  },
  "mctear99_eurospeech": {
   "authors": [
    [
     "Michael F.",
     "McTear"
    ]
   ],
   "title": "Software to support research and development of spoken dialogue systems",
   "original": "e99_0339",
   "page_count": 4,
   "order": 90,
   "p1": "339",
   "pn": "342",
   "abstract": [
    "The development of a spoken dialogue system requires the integration of the various components of spoken language technology, such as speech recognition, natural language processing, dialogue modelling, and speech synthesis. Recently several toolkits have been developed that provide support for this process, enabling developers who have no specialist knowledge of the component technologies to produce working spoken dialogue systems with relative ease. This paper reports on the use of CSLUs RAD (Rapid Application Developer) to provide practical experience for undergraduate students taking courses in spoken dialogue systems. Two groups of students were involved - students of linguistics, speech and language therapy, and communication, on the one hand, and students of computational linguistics and computing science. The paper describes the use of the toolkit for students with these different degrees of competence in computing and reports on plans for future work with the toolkit.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-88"
  },
  "kajarekar99_eurospeech": {
   "authors": [
    [
     "Sachin",
     "Kajarekar"
    ],
    [
     "Narendranath",
     "Malayath"
    ],
    [
     "Hynek",
     "Hermansky"
    ]
   ],
   "title": "Analysis of sources of variability in speech",
   "original": "e99_0343",
   "page_count": 4,
   "order": 91,
   "p1": "343",
   "pn": "346",
   "abstract": [
    "The variability in the speech signal can be attributed to the following sources: (a) Phonetic content, (b) Speaker and Channel, and (c) Coarticulation or context. In this paper, the variability in speech is decomposed using Two Factor Analysis of Variance (ANOVA) with the above mentioned sources as factors. The speech variability is decomposed in temporal and spectral domain separately and structure of these sources of variability in time-frequency plane is described. Although these factors are not indepdendent, it is shown that they can be studied independently after modeling the interaction between the factors.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-89"
  },
  "shimamura99_eurospeech": {
   "authors": [
    [
     "Tetsuya",
     "Shimamura"
    ],
    [
     "Haruko",
     "Hayakawa"
    ]
   ],
   "title": "Adaptive nonlinear prediction based on order statistics for speech signals",
   "original": "e99_0347",
   "page_count": 4,
   "order": 92,
   "p1": "347",
   "pn": "350",
   "abstract": [
    "This paper proposes a novel adaptive algorithm for nonlinear prediction of speech signals, which turns out to be the adaptation procedure for an order statistic LMS predictor. The LMS-L filter Pitas et al. addressed is modified to preserve the time information in the input vector for the adaptation, in which a coeficient matrix is utilized to update the predictor coeficients. Computer simulations demonstrate that the novel nonlinear predictor provides better performance than the Volterra quadratic predictor as well as the linear predictor.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-90"
  },
  "souza99_eurospeech": {
   "authors": [
    [
     "M. N.",
     "Souza"
    ],
    [
     "E. J.",
     "Caprini"
    ],
    [
     "C. G.",
     "Machado"
    ],
    [
     "M. V.",
     "Ludolf"
    ],
    [
     "L. P.",
     "Calôba"
    ],
    [
     "J. M.",
     "Seixas"
    ],
    [
     "F. G.",
     "Resende"
    ],
    [
     "S. L.",
     "Netto"
    ],
    [
     "Diamantino R.",
     "Freitas"
    ],
    [
     "Joao Paulo",
     "Teixeira"
    ],
    [
     "C.",
     "Espain"
    ],
    [
     "V.",
     "Pera"
    ],
    [
     "F.",
     "Moreira"
    ]
   ],
   "title": "Developing a voiced information retrieval system for the portuguese language capable to handle both brazilian and portuguese spoken versions",
   "original": "e99_0351",
   "page_count": 4,
   "order": 93,
   "p1": "351",
   "pn": "354",
   "abstract": [
    "The two versions of the Portuguese language, the one spoken in Portugal and the one spoken in Brazil differ both phonetically and lexically. The paper reports on a menu driven voiced information retrieval system, BomdePapo, which can handle both versions of the language. The system was developed in Delphi during a research project joining teams from the Universidade do Porto, Portugal, and the Universidade Federal do Rio de Janeiro, Brazil. Information retrieval is processed through a tree menu. The system, BomdePapo, is currently being tested on an application for accessing information on sports, culture and politics news. The paper also reports on the main phonetic differences between the two versions.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-91"
  },
  "soraghan99_eurospeech": {
   "authors": [
    [
     "John J.",
     "Soraghan"
    ],
    [
     "Amir",
     "Hussain"
    ],
    [
     "Ivy",
     "Shim"
    ]
   ],
   "title": "Real-time speech modeling using computationally efficient locally recurrent neural networks (CERNs)",
   "original": "e99_0355",
   "page_count": 4,
   "order": 94,
   "p1": "355",
   "pn": "358",
   "abstract": [
    "A general class of Computationally Efficient locally Recurrent Networks (CERN) is described for real-time adaptive signal processing. The structure of the CERN is based on linear-in-the-parameters single-hidden-layered feedforward neural networks such as the Radial Basis Function (RBF) network, the Volterra Neural Network (VNN) and the recently developed Functionally Expanded Neural Network (FENN), adapted to employ local output feedback. The corresponding learning algorithms are described and key structural and computational complexity comparisons are made between the CERN and conventional Recurrent Neural Networks. A speech signal is used, which shows that a Recurrent FENN based adaptive CERN predictor can significantly outperform the corresponding feedforward FENN and conventionally employed linear adaptive filtering models.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-92"
  },
  "tokuhira99_eurospeech": {
   "authors": [
    [
     "M.",
     "Tokuhira"
    ],
    [
     "Y.",
     "Ariki"
    ]
   ],
   "title": "Effectiveness of KL-transformation in spectral delta expansion",
   "original": "e99_0359",
   "page_count": 4,
   "order": 95,
   "p1": "359",
   "pn": "362",
   "abstract": [
    "MFCC is widely used together with its delta and delta-delta features in the field of speech recognition based on HMM. MFCC is designed to apply DCT to the MF output. We propose in this paper to employ KL transformation instead of DCT, because it can reflect the statistics of speech data more precisely. MFCCis the compressed feature of the log MFso that some detailed features seem to be lost. In this sense, we propose to compute the delta and delta-delta feature on the MF, and apply the KL transformation to a set of MF, its delta and delta-delta features.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-93"
  },
  "berkling99_eurospeech": {
   "authors": [
    [
     "Kay",
     "Berkling"
    ],
    [
     "Douglas A.",
     "Reynolds"
    ],
    [
     "Marc",
     "Zissman"
    ]
   ],
   "title": "Evaluation of confidence measures for language identification",
   "original": "e99_0363",
   "page_count": 4,
   "order": 96,
   "p1": "363",
   "pn": "366",
   "abstract": [
    "In this paper we examine various ways to derive confidence measures for a language identification system [3], using phone recognition followed by language models, and describe the application of an evaluation metric [1] for measuring the goodness\" of the different confidence measures. Experiments are conducted on the 1996 NIST Language Identification Evaluation corpus (derived from the Callfriend corpus of conversational telephone speech). The system is trained on the NIST 96 development data and evaluated on the NIST 96 evaluation data. Results indicate that we are able to predict the performance of a system and quantitatively evaluate how well the prediction holds on new data.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-94"
  },
  "tsai99_eurospeech": {
   "authors": [
    [
     "Wuei-He",
     "Tsai"
    ],
    [
     "Wen-Whei",
     "Chang"
    ]
   ],
   "title": "Chinese dialect identification using an acoustic-phonotactic model",
   "original": "e99_0367",
   "page_count": 4,
   "order": 97,
   "p1": "367",
   "pn": "370",
   "abstract": [
    "In this paper we develop hidden Markov model (HMM) based approaches to identify Chinese dialects spoken in Taiwan. This task can be aided by exploiting various characteristic features of Chinese spoken languages. The baseline system performs phonotactic analysis after the speech utterance is tokenized into a sequence of five broad phonetic classes. The sequential statistics of the resulting symbols are then used to distinguish one dialect from another. The second approach we tested is to incor-porate dialect-dependent phonotactic constraints into the phonetic tokenization rather than applying these con-straints after the broad phonetic classification is complete. These algorithms were evaluated using a multi-speaker speech corpus of text-independent spontaneous speech data. Simulation results indicate that the acoustic-phonotactic approach to dialect identification yields better performance with an average identification rate of 89.6%, compared to 70% for the baseline system.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-95"
  },
  "cummins99_eurospeech": {
   "authors": [
    [
     "Fred",
     "Cummins"
    ],
    [
     "Felix",
     "Gers"
    ],
    [
     "Jürgen",
     "Schmidhuber"
    ]
   ],
   "title": "Language identification from prosody without explicit features",
   "original": "e99_0371",
   "page_count": 4,
   "order": 98,
   "p1": "371",
   "pn": "374",
   "abstract": [
    "Most current language identification (LID) systems make little or no use of prosodic information, despite the importance of prosody in LID by humans. The greatest obstacle has been that of finding an appropriate feature set which captures linguistically relevant prosodic information. The only system to attempt LID entirely on the basis of prosodic variables uses a set of over 200 features which are selected and combined in a task-specific manner [12]. We apply a novel recurrent neural network model to the task of pairwise discrimination among languages. Network inputs are limited to delta-F0 and the first difference of the band limited amplitude envelope. Initial results are based on all pairwise combinations of English, German, Japanese, Mandarin and Spanish, with 90 speakers per language.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-96"
  },
  "harbeck99_eurospeech": {
   "authors": [
    [
     "Stefan",
     "Harbeck"
    ],
    [
     "Uwe",
     "Ohler"
    ]
   ],
   "title": "Multigrams for language identification",
   "original": "e99_0375",
   "page_count": 4,
   "order": 99,
   "p1": "375",
   "pn": "378",
   "abstract": [
    "In our paper we present two new approaches for language identification. Both of them are based on the use of so-called multigrams, an information theoretic based observation representation. In the first approach we use multigram models for phonotactic modeling of phoneme or codebook sequences. The multigram model can be used to segment the new observation into larger units (e.g. something like words) and calculates a probability for the best segmentation. In the second approach we build a fenon recognizer using the segments of the best segmentation of the training material as \\words\" inside the recognition vocabulary. On the OGI test corpus and on the NIST'95 evaluation corpus we got significant improvements with this second approach in comparison to the unsupervised codebook approach when discriminating between English and German utterances.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-97"
  },
  "hombert99_eurospeech": {
   "authors": [
    [
     "Jean-Marie",
     "Hombert"
    ],
    [
     "Ian",
     "Maddieson"
    ]
   ],
   "title": "The use of 'rare' segments for language identification",
   "original": "e99_0379",
   "page_count": 4,
   "order": 100,
   "p1": "379",
   "pn": "382",
   "abstract": [
    "Knowledge of the distribution of rare segments across the languages of the world might be used in identifying languages within an open set. Segments which are both discriminatory (i.e. rare) and robust (i.e. easy to identify) are the best targets for efficient language identification. Considering several properties at the same time allows to use more common segments and/or features in a still very discriminatory way.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-98"
  },
  "itahashi99_eurospeech": {
   "authors": [
    [
     "Shuichi",
     "Itahashi"
    ],
    [
     "Toshikazu",
     "Kiuchi"
    ],
    [
     "Mikio",
     "Yamamoto"
    ]
   ],
   "title": "Spoken language identification utilizing fundamental frequency and cepstra",
   "original": "e99_0383",
   "page_count": 4,
   "order": 101,
   "p1": "383",
   "pn": "386",
   "abstract": [
    "This paper describes a combined method of spoken language identification, which utilizes speech fundamental frequency (Fo) and mel cepstral coefficients. In the first method, the Fo contour was used as prosodic information; its trajectory was approximated by polygonal lines or exponential functions, their parameters were used for discrimination. The second method is based on an ergodic HMM using cepstra as segmental information. The number of states of the HMM was varied from 4 to 64. Speech data of 40-seconds spontaneous uttereances were used, spoken by 50 male speakers for each of the 10 languages considered in this study. The results show the effectiveness of the two proposed methods, and that better indentification rate is obtained by combining the two methods.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-99"
  },
  "matrouf99_eurospeech": {
   "authors": [
    [
     "D.",
     "Matrouf"
    ],
    [
     "Martine",
     "Adda-Decker"
    ],
    [
     "Jean-Luc",
     "Gauvain"
    ],
    [
     "Lori",
     "Lamel"
    ]
   ],
   "title": "Comparing different model configurations for language identification using a phonotactic approach",
   "original": "e99_0387",
   "page_count": 4,
   "order": 102,
   "p1": "387",
   "pn": "390",
   "abstract": [
    "In this paper different model configurations for language identification using a phonotactic approach are explored. Identification experiments were carried out on the 11-language telephone speech corpus OGI-TS, containing calls in French, English, German, Spanish, Japanese, Korean, Mandarin, Tamil, Farsi, Hindi, and Vietnamese. Phone sequences output by one or multiple phone recognizers are rescored with language-dependent phonotactic models approximated by phone bigrams. The parameters of different sets of acoustic phone models were estimated using the 4-language IDEAL corpus. Sets of language-specific phonotactic models were trained using the training portion of the OGI-TS corpus. Error rates are significantly reduced by combining language-dependent and language-independent acoustic decoders, especially for short segments. A 9.9% LID error rate was obtained on the 11-language task using phonotactic models trained on spontaneous speech data. These results show that the phonotactic approach is relative insensitive to an acoustic mismatch between training and test conditions.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-100"
  },
  "mori99_eurospeech": {
   "authors": [
    [
     "K.",
     "Mori"
    ],
    [
     "N.",
     "Toba"
    ],
    [
     "T.",
     "Harada"
    ],
    [
     "T.",
     "Arai"
    ],
    [
     "M.",
     "Komatsu"
    ],
    [
     "M.",
     "Aoyagi"
    ],
    [
     "Y.",
     "Murahara"
    ]
   ],
   "title": "Human language identification with reduced spectral information",
   "original": "e99_0391",
   "page_count": 4,
   "order": 103,
   "p1": "391",
   "pn": "394",
   "abstract": [
    "We conducted human language identification (LID) experiments using signals with reduced segmental information in pursuit of cues that humans use in their remarkable LID ability, which may be applicable to the development of robust automatic LID. American English and Japanese excerpts from the OGI-TS were processed by (1) spectral-envelope removal (SER) and (2) temporal-envelope modulation. With the SER signal, where the spectral-envelope is eliminated, humans could still identify the languages fairly successfully (85.2%). With the TEM signal, composed of white-noise driven, combined intensity envelopes from several frequency bands, the identification rate rose from 62.5% to 93.8% corresponding to the increasing number of bands from 1 to 4. These results, though with a limited number of languages, indicate that humans can identify languages using signal with its segmental information much reduced  in acoustic terms much reduced in spectral information.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-101"
  },
  "barkat99_eurospeech": {
   "authors": [
    [
     "Melissa",
     "Barkat"
    ],
    [
     "John",
     "Ohala"
    ],
    [
     "François",
     "Pellegrino"
    ]
   ],
   "title": "Prosody as a distinctive feature for the discrimination of arabic dialects",
   "original": "e99_0395",
   "page_count": 3,
   "order": 104,
   "p1": "395",
   "pn": "398",
   "abstract": [
    "The aim of the work to be reported here is to explore the utility of prosodic information in language identification and discrimination tasks. The purpose of this study is to see whether prosodic patterns can be considered as reliable acoustic cues for the discrimination of Arabic dialects by investigating, via a perceptual experiment, if listeners are successful in identifying the Arabic dialect used by a speaker when they only have access to fundamental frequency, amplitude and some rhythmic characteristics of the original voice signal. Results show that prosodic cues alone can distinguish between dialect pairs, since native Arabic listeners are significantly more successful in identifying the Arabic dialectal varieties both in their natural and synthesized forms and that listeners identification rate are higher for the discrimination of their own dialectal variety when presented under its processed form. This perceptual study must be regarded as a first step towards the determination of a set of reliable cues for the Automatic Identification of Arabic Dialects.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-102"
  },
  "pellegrino99_eurospeech": {
   "authors": [
    [
     "François",
     "Pellegrino"
    ],
    [
     "Jérôme",
     "Farinas"
    ],
    [
     "Régine",
     "André-Obrecht"
    ]
   ],
   "title": "Comparison of two phonetic approaches to language identification",
   "original": "e99_0399",
   "page_count": 4,
   "order": 105,
   "p1": "399",
   "pn": "402",
   "abstract": [
    "This paper presents two unsupervised approaches to Automatic Language Identification (ALI) based on a segmental preprocessing. In the Global Segmental Model approach, the language system is modeled by a Gaussian Mixture Model (GMM) trained with automatically detected segments. In the Phonetic Differentiated Model approach, an unsupervised detection vowel/non vowel is performed and the language model is defined with two GMMs, one to model the vowel segments and a second one to model the others segments. For each approach, no labeled data are required. GMMs are initialized using an efficient data-driven variant of the LBG algorithm: the LBG-Rissanen algorithm. With 5 languages from the OGI MLTS corpus and in a closed set identification task, we reach 85 % of correct identification with each system using 45 second duration utterances for the male speakers. We increase this performance (91%) when we merge the two systems.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-103"
  },
  "anderson99_eurospeech": {
   "authors": [
    [
     "Stephen",
     "Anderson"
    ],
    [
     "Natalie",
     "Liberman"
    ],
    [
     "Larry",
     "Gillick"
    ],
    [
     "Stephen",
     "Foster"
    ],
    [
     "Sahoko",
     "Hama"
    ]
   ],
   "title": "The effects of speaker training on ASR accuracy",
   "original": "e99_0403",
   "page_count": 4,
   "order": 106,
   "p1": "403",
   "pn": "406",
   "abstract": [
    "Do experienced speech recognition users achieve high accuracy rates because their systems have taught them successful speaking styles? We report an experiment to quantify this \"speaker training\" effect. In our experiment, 30 computer-literate elderly speakers (15 male, 15 female) with no previous ASR experience were given 2 hours of intensive training in using a speech recognition system. Before and after this training session, they were asked to read separate 520-word texts. Measuring the word error rates (WERs) on these \"before training\" and \"after training\" recordings, we find a small but statistically significant improvement. Before training, speakers had an average WER of 20.9%, and after training, 19.8%. We examine changes in speaking rate, phrase length, and SNR and their impact on WER. This improvement is surprisingly small; anecdotal evidence suggests that experienced ASR users have substantially higher accuracy than novices. The effect may be larger for more extensive training.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-104"
  },
  "faltlhauser99_eurospeech": {
   "authors": [
    [
     "Robert",
     "Faltlhauser"
    ],
    [
     "Thilo",
     "Pfau"
    ],
    [
     "Günther",
     "Ruske"
    ]
   ],
   "title": "Creating hidden Markov models for fast speech by optimized clustering",
   "original": "e99_0407",
   "page_count": 4,
   "order": 107,
   "p1": "407",
   "pn": "410",
   "abstract": [
    "Previous studies have shown that the recognition accuracy often severely degrades at higher speech rates, which can basically be traced back to two main dimensions: acoustic and phonemic. Reasons for this effect can be found in the phonemic field (e.g. elisions) as well as on the acoustic level: with increasing rates of speech the spectral characteristics are changing. A main obstacle in this context is the training data, consisting of only a small fraction of samples, which can be labeled as 'fast'. Therefore, the effects caused by an increased speech rate often cannot be completely covered. To meet this problem, in this paper an optimized clustering process is presented making eficient use of the available data. Our modified mixture splitting algorithm with an incorporated cross-validation step aims at increasing the generalization of Hidden Markov Models, especially with respect to fast speech. Experimental results showed a relative decrease in word error rate of 7.6% for fast speech.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-105"
  },
  "richardson99_eurospeech": {
   "authors": [
    [
     "M.",
     "Richardson"
    ],
    [
     "M.",
     "Hwang"
    ],
    [
     "Alex",
     "Acero"
    ],
    [
     "Xuedong",
     "Huang"
    ]
   ],
   "title": "Improvements on speech recognition for fast talkers",
   "original": "e99_0411",
   "page_count": 4,
   "order": 108,
   "p1": "411",
   "pn": "414",
   "abstract": [
    "The accuracy of a speech recognition (SR) system depends on many factors, such as the presence of background noise, mismatches in microphone and language models, variations in speaker, accent and even speaking rates. In addition to fast speakers, even normal speakers will tend to speak faster when using a speech recognition system in order to get higher throughput. Unfortunately, state-of-the-art SR systems perform significantly worse on fast speech. In this paper, we present our efforts in making our system more robust to fast speech. We propose cepstrum length normalization, applied to the incoming testing utterances, which results in a 13% word error rate reduction on an independent evaluation corpus. Moreover, this improvement is additive to the contribution of Maximum Likelihood Linear Regression (MLLR) adaptation. Together with MLLR, a 23% error rate reduction was achieved.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-106"
  },
  "saul99_eurospeech": {
   "authors": [
    [
     "Lawrence",
     "Saul"
    ],
    [
     "Mazin",
     "Rahim"
    ]
   ],
   "title": "Modeling the rate of speech by Markov processes on curves",
   "original": "e99_0415",
   "page_count": 4,
   "order": 109,
   "p1": "415",
   "pn": "418",
   "abstract": [
    "We propose a statistical model for automatic speech recognition that relates variations in speaking rate to nonlinear warpings of time. The model describes a discrete random variable, s(t), that evolves as a function of the arc length traversed along a curve, parameterized by x(t). Since arc length does not depend on the rate at which a curve is traversed, this evolution gives rise to a family of Markov processes whose predictions, Pr[s|x], are invariant to nonlinear warpings of time. We describe the use of such models, known as Markov processes on curves (MPCs), for automatic speech recognition, where x are acoustic feature trajectories and s are phonetic transcriptions. On two tasks|recognizing New Jersey town names and connected alpha-digits|we find that MPCs yield lower word error rates than comparably trained hidden Markov models.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-107"
  },
  "tuerk99_eurospeech": {
   "authors": [
    [
     "Andreas",
     "Tuerk"
    ],
    [
     "Steve",
     "Young"
    ]
   ],
   "title": "Modelling speaking rate using a between frame distance metric",
   "original": "e99_0419",
   "page_count": 4,
   "order": 110,
   "p1": "419",
   "pn": "422",
   "abstract": [
    "It is well known [5] that variations in speaking rate can account for a significant percentage of errors in practical speech recognition tasks. This is the result of the dynamic nature of speech which is not modelled properly by the standard HMM structure. This paper proposes an extension to the standard HMM that takes advantage of the information about the rate of speech that is contained in inter-frame transitions. The new model can be seen as a combination of Moore and Mealy type HMM's that has output probabilities attached to the transitions between states in addition to the conventional output probabilities attached to states. In this model fast and slow transitions are associated with additional hidden parameters. The output probabilities of the transitions are modelled with gamma distributions.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-108"
  },
  "bloothooft99_eurospeech": {
   "authors": [
    [
     "Gerrit",
     "Bloothooft"
    ],
    [
     "Peter",
     "Pabon"
    ]
   ],
   "title": "Vocal registers revisited",
   "original": "e99_0423",
   "page_count": 5,
   "order": 111,
   "p1": "423",
   "pn": "426",
   "abstract": [
    "The paper concerns the quest to find acoustical parameters that describe properties of vocal fold functioning from professional singing to voice pathologies. Improved data handling within the framework of phonetogram recordings has been used to arrive at a better understanding of what acoustical parameters could tell us about vocal registers.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-109"
  },
  "bouteille99_eurospeech": {
   "authors": [
    [
     "Franck",
     "Bouteille"
    ],
    [
     "Pascal",
     "Scalart"
    ],
    [
     "Michel",
     "Corazza"
    ]
   ],
   "title": "Pseudo affine projection algorithm new solution for adaptive identication",
   "original": "e99_0427",
   "page_count": 4,
   "order": 112,
   "p1": "427",
   "pn": "430",
   "abstract": [
    "In this paper, we present a new approach for adaptive echo cancellation: Pseudo Affine projection algorithm. From the original update equation of the filter coefficients of the Affine Projection algorithms, which are based on multiple dimension projection of the input signal vector, we derive a simplified solution under some realistic hypothesis. We show that our solution possesses a complexity figure close to the NLMS one but with the same convergence characteristics of the original sample-by-sample AP algorithm. Implementation details are presented in the last part of the paper.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-110"
  },
  "jesus99_eurospeech": {
   "authors": [
    [
     "Luis M. T.",
     "Jesus"
    ],
    [
     "Christine H.",
     "Shadle"
    ]
   ],
   "title": "Acoustic analysis of a speech corpus of european portuguese fricative consonants",
   "original": "e99_0431",
   "page_count": 4,
   "order": 113,
   "p1": "431",
   "pn": "434",
   "abstract": [
    "The study of Portuguese fricatives is a complex problem, which has not been explored fully. As partof a larger study of the acoustic properties of Portuguese fricatives, corpora of Portuguese words containing /f, v, s, z, A, O/, nonsense words with Por-tuguese phonology, and sustained Portuguese frica-tives have been recorded and analysed. Results showthat more than half of the voiced fricatives devoice.Averaged power spectra were computed for all fricatives; differences between fricatives, in particular related to syllable stress, are described.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-111"
  },
  "petlyuchenko99_eurospeech": {
   "authors": [
    [
     "Natalia",
     "Petlyuchenko"
    ]
   ],
   "title": "Acoustic characteristics of plosives in consonant-consonant sequences at word boundaries",
   "original": "e99_0435",
   "page_count": 4,
   "order": 114,
   "p1": "435",
   "pn": "438",
   "abstract": [
    "In this paper we describe acoustic characteristics of aspi-ration, explosiveness and tension of plosives within the framework of consonant sequences at boundaries of two words extracted from German read speech. In these sequences (66 types) the prejuncture consonants (in final position in the preceding word) were limited to the plosives p/t/k, whereas the post-juncture consonants (in initial position in the following word) could consist of the entire set of German consonants. The proposed order of sounds at word boundaries enables to trace modifications of the plosives in question and degree of their correlation with a number of positional-combinatory and prosodic factors, namely type of following consonant, presence of pause inside the sequence, position of plosives in rhyth-mic groups with regards to tone syllable.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-112"
  },
  "son99_eurospeech": {
   "authors": [
    [
     "Rob J. J. H. van",
     "Son"
    ],
    [
     "Louis C. W.",
     "Pols"
    ]
   ],
   "title": "Effects of stress and lexical structure on speech efficiency",
   "original": "e99_0439",
   "page_count": 4,
   "order": 115,
   "p1": "439",
   "pn": "442",
   "abstract": [
    "It is proposed that some of the variation in speech is the result of an effort to communicate efficiently. Speaking is considered efficient if the speech sound contains only the information needed to understand it. This efficiency is tested by means of a corpus of spontaneous and matched read speech, and syllable, word, and N-gram frequencies as measures of information content (1582 intervocalic consonants, and 2540 vowels). It is indeed found that the duration and spectral reduction of consonants and vowels from stressed syllables correlate with syllable and word frequencies, as does consonant intelligibility. Correlations for phonemes from unstressed syllables are generally weaker or absent. N-gram models of word predictability did not correlate with any of the factors investigated. Simple N-grams seem to be a poor model for human word prediction. It is concluded that the principle of efficient communication organizes at least some aspects of speech production.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-113"
  },
  "abe99_eurospeech": {
   "authors": [
    [
     "Yoshiharu",
     "Abe"
    ],
    [
     "Hiroyasu",
     "Itsui"
    ],
    [
     "Yuzo",
     "Maruta"
    ],
    [
     "Kunio",
     "Nakajima"
    ]
   ],
   "title": "A two-stage speech recognition method with an error correction model",
   "original": "e99_0443",
   "page_count": 4,
   "order": 116,
   "p1": "443",
   "pn": "446",
   "abstract": [
    "Anovel multi-pass speech recognition method is presented. The method is organized as two stages. The first stage decodes the input speech based on an acoustic model and outputs the most probable sequence of basic units. The second stage searches for the most probable word sequence in the decoding output of the first stage. The novel point is use of an error correction model (ECM) in the second stage. With the ECM the second stage can recover decoding errors in the first stage. The ECM is realized as a statistical model, whose parameters are estimated from training data. The first stage is realized by a one-pass DP algorithm with triphone models. The second stage is realized by a best-first search algorithm with the ECM and a N-gram language model. The presented method was evaluated with large vocabulary continuous speech recognition. When we used N-best decoding outputs of the first stage and a 64K word trigram language model we achieved the word accuracy of 89.1% for open data with test-set perplexity of 129.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-114"
  },
  "chen99_eurospeech": {
   "authors": [
    [
     "C. Julian",
     "Chen"
    ]
   ],
   "title": "Speech recognition with automatic punctuation",
   "original": "e99_0447",
   "page_count": 4,
   "order": 117,
   "p1": "447",
   "pn": "450",
   "abstract": [
    "We present a method of speech recognition with automatic punctuation based on a combination of acoustic and lexical evidence. In the recognizer vocabulary, punctuation marks are treated as word entries. By assigning the acoustic baseforms of silence, breath, and other non-speech sounds to punctuation marks, and using a properly processed N-gram language model, unpronounced punctuation marks of various types (commas, periods, etc.) appear naturally in the recognizer output. This technology can be used in dictation systems to improve usability, in commercial broadcast transcription systems to reduce editing time, and in information retrieval systems to provide phrasing information to facilitate natural language understanding.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-115"
  },
  "eide99_eurospeech": {
   "authors": [
    [
     "Ellen",
     "Eide"
    ]
   ],
   "title": "Automatic modeling of pronunciation variations",
   "original": "e99_0451",
   "page_count": 3,
   "order": 118,
   "p1": "451",
   "pn": "454",
   "abstract": [
    "We report on an automatic method for discovering an appropriate model topology for each context-dependent phoneme, allowing for such phenomena as reduced pronunciations and substituted phonemes. The method leads to a reduction in the word error rate on both the Wall Street Journal and Broadcast News databases.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-116"
  },
  "franz99_eurospeech": {
   "authors": [
    [
     "Martin",
     "Franz"
    ],
    [
     "Miroslav",
     "Novak"
    ]
   ],
   "title": "Reducing search complexity in low perplexity tasks",
   "original": "e99_0455",
   "page_count": 3,
   "order": 119,
   "p1": "455",
   "pn": "458",
   "abstract": [
    "In this paper we present a new method for improving the throughput of an asynchronous stack search based speech recognition system in the low perplexity applications. The algorithm reduces the acoustic fast match use in the cases where the word context information represented by the language model is sufficient to provide a reliable list of word candidates for the detailed match processing. The proposed technique improves the throughput of the system by reducing the number of fast match calls and by shortening the list of candidate words to be processed by the detailed match. Tested on the set of 3400 sentences, the new method reduces the CPU requirements of the search part of the speech recognition system by 47.7%, increasing the throughput of the entire speech system by 30.8% without degrading the recognition accuracy.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-117"
  },
  "coletti99_eurospeech": {
   "authors": [
    [
     "Paolo",
     "Coletti"
    ],
    [
     "Marcello",
     "Federico"
    ]
   ],
   "title": "A two-stage speech recognition method for information retrieval applications",
   "original": "e99_0459",
   "page_count": 4,
   "order": 120,
   "p1": "459",
   "pn": "462",
   "abstract": [
    "This paper presents a two-stage approach to speech recognition that is suited for information retrieval tasks, e.g. accessing a large telephone directory. The first stage performs a Viterbi beam search to decode the speech input into a sequence of phonemes. The second stage performs a graph search to match the phoneme sequence with a large list of keywords. The key issue is that the first step employs a syllable based language model that does not necessarily depend on the application domain. Experimental results are shown for a telephone directory access task of one million of entries.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-118"
  },
  "foslerlussier99_eurospeech": {
   "authors": [
    [
     "Eric",
     "Fosler-Lussier"
    ]
   ],
   "title": "Multi-level decision trees for static and dynamic pronunciation models",
   "original": "e99_0463",
   "page_count": 4,
   "order": 121,
   "p1": "463",
   "pn": "466",
   "abstract": [
    "We have been focusing on improving pronunciation models for automatic transcription of television and radio news reports by modeling phone, syllable, and word pronunciation distributions with decision trees. These models were employed in two sep-arate sets of experiments. First, decision trees facilitated selection of word pronunciations derived automatically from data for use in a standard speech recognizer dictionary. We have seen a small but significant improvement with these automatically con-structed dictionaries in our onepass decoding system. In a sec-ond set of experiments, we allowed decision tree models to de-termine the probability of word pronunciations dynamically, de-pendent on the linguistic context of the word during recognition. Dynamic models provided an additional insignificant decrease in error, but improvements were focused within the spontaneous speech portion of the test set.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-119"
  },
  "finke99_eurospeech": {
   "authors": [
    [
     "Michael",
     "Finke"
    ],
    [
     "Jürgen",
     "Fritsch"
    ],
    [
     "Detlef",
     "Koll"
    ],
    [
     "Alex",
     "Waibel"
    ]
   ],
   "title": "Modeling and efficient decoding of large vocabulary conversational speech",
   "original": "e99_0467",
   "page_count": 4,
   "order": 122,
   "p1": "467",
   "pn": "470",
   "abstract": [
    "Capturing the large variability of conversational speechin the framework of purely phone based speech recog-nizers is virtually impossible. It has been shown earlier that suprasegmental features such asspeaking rate,duration and syllabic, syntactic and semantic structureare important predictors of pronunciation variation. Inorder to allow for a tighter coupling of these predictorsof pronunciation, duration and acoustic modeling a newrecognition toolkit has been developed. The phonetictranscription of speech has been generalized to an attribute based representation, thus enabling the integra-tion of suprasegmental, non-phonetic features. A pronunciation model is trained to augment the attribute tran-scription to mark possible pronunciation effects which arethen taken into account by the acoustic model induction algorithm. A finite state machine single-prefix-tree,one-pass, time-synchronous decoder is presented that efficiently decodes highly spontaneous speech within thisnew representational framework.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-120"
  },
  "husson99_eurospeech": {
   "authors": [
    [
     "Jean-Luc",
     "Husson"
    ]
   ],
   "title": "Evaluation of a segmentation system based on multi-level lattices",
   "original": "e99_0471",
   "page_count": 4,
   "order": 123,
   "p1": "471",
   "pn": "474",
   "abstract": [
    "This paper addresses the problem of the evaluation of an automatic continuous speech segmentation system based on multi-level lattices called dendrograms previously described in [2] [3]. After a short overview of the segmentation framework, we briefly discuss the difficulties inherent to such an evaluation. We first state quantitative results based on the usual quality coefficient and oversegmentation rate measures obtained for the automatic paths selection yielded by our system and we estimate the potential improvement margin of our approach. These quantitative results are completed by a qualitative analysis of the system performance by examining the detection accuracy of all possible ransitions between 6 broad phonetic classes. The results are judged encouraging given the restricted number of phonetic likelihood criteria used at this stage of development and demonstrates the feasibility of the segmentation process.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-121"
  },
  "hanna99_eurospeech": {
   "authors": [
    [
     "Philip",
     "Hanna"
    ],
    [
     "Darryl",
     "Stewart"
    ],
    [
     "Ji",
     "Ming"
    ]
   ],
   "title": "The application of an improved DP match for automatic lexicon generation",
   "original": "e99_0475",
   "page_count": 5,
   "order": 124,
   "p1": "475",
   "pn": "478",
   "abstract": [
    "A number of automatic lexicon construction methods have been proposed in recent years. Such approaches employ a dynamic programming (DP) match to collect statistics concerning differences between the observed phone sequence and that which was predicted by a standard lexicon. A more expressive lexicon is then constructed based upon the collected statistics, offering a more accurate phone-to-word mapping for use within speech recognition systems. We show that the standard DP procedure leads to the introduction of spurious matches, which reduces the quality of any subsequent processing based upon the DP provided matches. In order to remove this deficiency, an iterative DP match procedure, using individual phone confusion probabilities is outlined. It was found that the iterative DP match significantly reduced the number of equi-probable matches, to the extent that for the vast majority of utterances, only one possible DP mapping resulted, thereby improving the quality of generated statistics.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-122"
  },
  "iyer99_eurospeech": {
   "authors": [
    [
     "Rukmini",
     "Iyer"
    ],
    [
     "Owen",
     "Kimball"
    ],
    [
     "Herbert",
     "Gish"
    ]
   ],
   "title": "Modeling trajectories in the HMM framework",
   "original": "e99_0479",
   "page_count": 4,
   "order": 125,
   "p1": "479",
   "pn": "482",
   "abstract": [
    "Most state-of-the-art statistical speech recognition systems use hidden Markov models (HMM) for modeling the speech signal. However, limited by the assumption of conditional independence of observations given the state se-quence, current HMM's poorly model the trajectory con-straints in speech. In [1], we introduced the parallel path HMM, where each phonetic unit is represented by a parallel collection of HMM's that model the phone trajectory variability. The trajectory constraint is imposed by disallowing transitions across parallel paths. In this paper,we investigate improvements to two critical components ofthis new framework: (i) initializing the sets of trajectoriesper phone that will form the basis of the parallel collection of HMM's, and (ii) evaluating alternative parameter shar-ing strategies related to distributing the number of model parameters. Recognition results on Switchboard, a large vocabulary conversational speech recognition task, demonstrate 0.7-1.0% absolute performance improvements with the parallel path HMM in the N-best rescoring paradigm.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-123"
  },
  "kwon99_eurospeech": {
   "authors": [
    [
     "Oh-Wook",
     "Kwon"
    ],
    [
     "Kyuwoong",
     "Hwang"
    ],
    [
     "Jun",
     "Park"
    ]
   ],
   "title": "Korean large vocabulary continuous speech recognition using pseudomorpheme units",
   "original": "e99_0483",
   "page_count": 4,
   "order": 126,
   "p1": "483",
   "pn": "486",
   "abstract": [
    "This paper presents a Korean large vocabulary continuous speech recognition system based on pseudomorpheme units. In Korean, an eojeol (word phrase) is a unit for spacing and a morpheme is the smallest unit with semantic meaning. If the eojeol is used as the dictionary and language modeling unit, the number of the unit becomes enormous. Instead we propose to use modified morpheme or pseudomorpheme as the basic recognition unit. We can recover the original eojeol by concatenating graphemes of pseudomorpheme components. We used a dictionary and language model with pseudomorpheme/part-of-speech entries where each entry can have multiple pronunciations according to the morphology rule. With 32k-word vocabulary, the speaker-independent character, pseudomorpheme, and eojeol recognition accuracies on economy article database were 90.8%, 84.5%, and 81.3%, respectively.\n",
    "Keywords: continuous speech recognition\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-124"
  },
  "kabre99_eurospeech": {
   "authors": [
    [
     "Harouna",
     "Kabré"
    ],
    [
     "Alexander",
     "Waibel"
    ]
   ],
   "title": "Navigating German cities by spontaneous French queries",
   "original": "e99_0487",
   "page_count": 4,
   "order": 127,
   "p1": "487",
   "pn": "490",
   "abstract": [
    "This paper reports our efforts on the adaptation of a baseline system trained on clean speech to a task for which French native speakers uttered some Spontaneous French queries while driving a car. When the system is retrained on the new task acoustic data the Word Error Rate (WER) is decreased by 60% compared to our baseline system initial performance on the new task. We show that on spontaneous queries, L of this improvement could be achieved without prior system retraining by a more accurate Language Modelling which takes into account the noises and spontaneous speech effects and by a carefull grapheme/phoneme transcription of foreign words. We also describe the integration of this French system in our Multilingual Navigation System.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-125"
  },
  "korkmazskiy99_eurospeech": {
   "authors": [
    [
     "Filipp",
     "Korkmazskiy"
    ],
    [
     "Chin-Hui",
     "Lee"
    ]
   ],
   "title": "Generating alternative pronunciations from a dictionary",
   "original": "e99_0491",
   "page_count": 4,
   "order": 128,
   "p1": "491",
   "pn": "494",
   "abstract": [
    "We propose a language independent method for alterna-tive word pronunciation generation using a language specific dictionary. A set of optimal alternative word pronunciations can be generated from a word spelling by usingstatistically significant associations between strings of let-ters and strings of phonemes extracted from a dictionary.The proposed method does not require any prior knowledge about the language nor does it need a collection ofthe speech training data. The alternative pronunciationswere used in the recognition experiments. Even thoughthe experiments showed comparable to a baseline systemrecognition performance they indicated that produced alternative pronuncations could be used for a pronunciationnetwork initialization. This pronunciation network can befurther adjusted by a small amount of speech adaptationdata. The important advantage of the proposed method isits ability to automatically learn about a language phono-logical structure. This knowledge can be used while designing complex multilingual systems when informationabout the languages is limited and speech data for a specific language are unavailable or restricted.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-126"
  },
  "mangu99_eurospeech": {
   "authors": [
    [
     "Lidia",
     "Mangu"
    ],
    [
     "Eric",
     "Brill"
    ],
    [
     "Andreas",
     "Stolcke"
    ]
   ],
   "title": "Finding consensus among words: lattice-based word error minimization",
   "original": "e99_0495",
   "page_count": 4,
   "order": 129,
   "p1": "495",
   "pn": "498",
   "abstract": [
    "We describe a new algorithm for finding the hypothesis in a recognition lattice that is expected to minimize the word error rate (WER). Our approach thus overcomes the mismatch between the word-based performance metric and the standard MAP scoring paradigm that is sentence-based, and that can lead to sub-optimal recognition results. To this end we first find a complete alignment of all words in the recognition lattice, identifying mutually supporting and competing word hypotheses. Finally, a new sentence hypothesis is formed by concatenating the words with maximal posterior probabilities. Experimentally, this approach leads to a significant WER reduction in a large vocabulary recognition task.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-127"
  },
  "ortmanns99_eurospeech": {
   "authors": [
    [
     "Stefan",
     "Ortmanns"
    ],
    [
     "Wolfgang",
     "Reichl"
    ],
    [
     "Wu",
     "Chou"
    ]
   ],
   "title": "An efficient decoding method for real time speech recognition",
   "original": "e99_0499",
   "page_count": 4,
   "order": 130,
   "p1": "499",
   "pn": "502",
   "abstract": [
    "In this paper, we describe approaches for improving the search efficiency of a dynamic programming based one-pass decoder for dialogue applications. In order to allow the use of long-term language models (LM) and cross-word acoustic models, efficient pruning techniques and fast methods for the calculation of emission probability density functions (pdfs) are required. This is particularly important for real-time and memory constrained applications such as dialogue systems involving automatic speech recognition (ASR) and natural-language understanding. We propose an effective pruning technique exploiting the LM and cross-word context. We also present a fast distance calculation method to reduce the cost of state likelihood calculations in HMM-based systems. Experimental results on a natural language call routing task indicate that the proposed techniques speeded up the search process by a factor of 4 without loss in the recognition accuracy. In addition, we present a technique for generating word graphs incorporating cross-word context.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-128"
  },
  "padmanabhan99_eurospeech": {
   "authors": [
    [
     "Mukund",
     "Padmanabhan"
    ],
    [
     "G.",
     "Saon"
    ],
    [
     "S.",
     "Basu"
    ],
    [
     "Jing",
     "Huang"
    ],
    [
     "Geoffrey",
     "Zweig"
    ]
   ],
   "title": "Recent improvements in voicemail transcription",
   "original": "e99_0503",
   "page_count": 4,
   "order": 131,
   "p1": "503",
   "pn": "506",
   "abstract": [
    "In this paper we report recent improvements in voicemail transcription. Last year, the speaker independent and speaker adapted word error rates (WER) on the Voicemail Transcription task were reported at 41.94% and 38.18% respectively. This year, we report a relative improvement of 18% in the speaker independent performance and 11% in the speaker adapted performance over last year. This improvement is a result of some new algorithms and an increase in the amount of training data. In the following sections, we describe the contribution of several components to improving the word error rate.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-129"
  },
  "ramabhadran99_eurospeech": {
   "authors": [
    [
     "Bhuvana",
     "Ramabhadran"
    ],
    [
     "Sabine",
     "Deligne"
    ],
    [
     "Abraham",
     "Ittycheriah"
    ]
   ],
   "title": "Acoustics-based baseform generation with pronunciation and/or phonotactic models",
   "original": "e99_0507",
   "page_count": 4,
   "order": 132,
   "p1": "507",
   "pn": "510",
   "abstract": [
    "In this paper, we describe a method to derive a phonetic pronunciation of a word using only an acoustic utterance of that word without a priori knowledge of the spelling of the word. In [5] and [6], we used a pronunciation model based on bigram statistics. Bi-gram statistics only constrain the left neighbor phone and results in phone sequences that are only pairwise appropriate. Here, we apply a pronunciation model in combination with a phonotactic model that serves the purpose of a language model to constrain the phone sequences produced. Error rates with and without the phonotactic model are presented.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-130"
  },
  "shirosaki99_eurospeech": {
   "authors": [
    [
     "Yasuo",
     "Shirosaki"
    ],
    [
     "Hideaki",
     "Kikuchi"
    ],
    [
     "Katsuhiko",
     "Shirai"
    ]
   ],
   "title": "Improving recognition correct rate of important words in large vocabulary speech recognition",
   "original": "e99_0511",
   "page_count": 4,
   "order": 133,
   "p1": "511",
   "pn": "514",
   "abstract": [
    "The speech recognition technique has used in the various fields, with the progress of it and the needed functions are different for purposes. For example, speech dictators need to recognize whole sentences, but spoken dialogue systems need to recognize important words rather than whole sentences. In this paper, we propose a method of modifying language model for improving recognition correct rate of important words. This method multiplied probabilities of unigram and bigram of important words by constant weight and made important words to easy to be recognized. And we examine differences of recognition result in changes of a set of important words. As a result, the correct rate of important words increased when we multiplied probabilities of important words by weight.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-131"
  },
  "saraclar99_eurospeech": {
   "authors": [
    [
     "Murat",
     "Saraclar"
    ],
    [
     "Harriet",
     "Nock"
    ],
    [
     "Sanjeev",
     "Khudanpur"
    ]
   ],
   "title": "Pronunciation modeling by sharing gaussian densities across phonetic models",
   "original": "e99_0515",
   "page_count": 4,
   "order": 134,
   "p1": "515",
   "pn": "518",
   "abstract": [
    "Conversational speech exhibits considerable pronunciation vari-ability, which has been shown to have a detrimental effect on the accuracy of automatic speech recognition. There have been many attempts to model pronunciation variation, including the use of decision-trees to generate alternate word pronunciations from phonemic baseforms. Use of such pronunciation models during recognition is known to improve accuracy. This paper describes the use of such pronunciation models during acous-tic model training. Subtle difficulties in the straightforward use of alternatives to canonical pronunciations are first illustrated: it is shown that simply improving the accuracy of the phonetic transcription used for acoustic model training is of little benefit. Analysis of this paradox leads to a new method of accommodat-ing nonstandard pronunciations: rather than allowing a phoneme in the canonical pronunciation to be realized as one of a few distinct alternate phones predicted by the pronunciation model, the HMM states of the phonemes model are instead allowed to share Gaussian mixture components with the HMM states of the model of the alternate realization. Qualitatively, this amounts to making a soft decision about which surface-form is realized. Quantitative experiments on the Switchboard corpus show that this method improves accuracy by 1.7% (absolute).\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-132"
  },
  "aubert99_eurospeech": {
   "authors": [
    [
     "Xavier L.",
     "Aubert"
    ]
   ],
   "title": "One pass cross word decoding for large vocabularies based on a lexical tree search organization",
   "original": "e99_1559",
   "page_count": 4,
   "order": 135,
   "p1": "1559",
   "pn": "1562",
   "abstract": [
    "This paper describes the new Philips Research decoder that performs large vocabulary continuous speech recognition in a single pass for cross-word acoustic models and an m-gram language model (with m up to 4) as opposed to our previous technique of multiple passes. The decoder is based on a time-synchronous beam search and a prefix tree structure of the lexicon. Cross-word transitions are treated dynamically. A language-model look-ahead technique is applied on the bigram probabilities. On a variety of speech data, reduced error rates are obtained together with significant speed-ups confirming the advantage of an early use of all available knowledge sources. In particular, the search effort of a one-pass trigram decoding is only marginally increased compared to bigram and the integration of cross-word triphones improves the overall accuracy by typically 10% relative.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-133"
  },
  "batliner99_eurospeech": {
   "authors": [
    [
     "Anton",
     "Batliner"
    ],
    [
     "M.",
     "Nutt"
    ],
    [
     "Volker",
     "Warnke"
    ],
    [
     "Elmar",
     "Nöth"
    ],
    [
     "Jan",
     "Buckow"
    ],
    [
     "R.",
     "Huber"
    ],
    [
     "Heinrich",
     "Niemann"
    ]
   ],
   "title": "Automatic annotation and classification of phrase accents in spontaneous speech",
   "original": "e99_0519",
   "page_count": 4,
   "order": 136,
   "p1": "519",
   "pn": "522",
   "abstract": [
    "During the last years, we have been working on the automatic classification of boundaries and accents in the German VERBMOBIL (VM) project (human-human communication, appointment scheduling dialogues). A sub-corpus was annotated manually with prosodic boundary and accent labels, and neural networks (NN) trained with a large set of prosodic features were used for automatic classification. The classification of boundaries could be improved markedly with a combination of the NN with a language model (LM) that was trained with manually annotated syntactic-prosodic boundary labels in a much larger sub-corpus. Here we show how a combination of NN with LM along similar lines can be used for an improvement of accent classification as well. For the training of the LM, accents are annotated automatically in the transliteration with the help of a rule-based system that uses part{of{speech (POS) as well as other linguistic/phonological information.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-134"
  },
  "conkie99_eurospeech": {
   "authors": [
    [
     "Alistair",
     "Conkie"
    ],
    [
     "Giuseppe",
     "Riccardi"
    ],
    [
     "Richard C.",
     "Rose"
    ]
   ],
   "title": "Prosody recognition from speech utterances using acoustic and linguistic based models of prosodic events",
   "original": "e99_0523",
   "page_count": 4,
   "order": 137,
   "p1": "523",
   "pn": "526",
   "abstract": [
    "A system for automatic recognition of prosodic events in speech utterances has been developed and applied to recognizing accent tones as defined by the tone and break index (ToBI) prosodic labeling standard. Both the acoustic and syntactic modeling portions of the system are described in the paper. The acoustic modeling portion of the system involves representation of ToBI labeled events using hidden Markov models (HMMs) that are defined over a set of prosodic features. The syntactic modeling component involves the prediction of prosodic events based on a stochastic finite state model defined over input labels obtained from a part-of-speech (POS) tagger. The system was evaluated in terms of its ability to recognize pitch accents in a single speaker read speech corpus when the orthographic transcription of the utterance was assumed to be known. It was shown to improve average labeling accuracy over a baseline text{only prosodic labeling system from 84.8% to 88.3%.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-135"
  },
  "fach99_eurospeech": {
   "authors": [
    [
     "Marcus L.",
     "Fach"
    ]
   ],
   "title": "A comparison between syntactic and prosodic phrasing",
   "original": "e99_0527",
   "page_count": 4,
   "order": 138,
   "p1": "527",
   "pn": "530",
   "abstract": [
    "This study presents a comparison between syntactic and prosodic phrasing. A parser is used to calculate the syntactic structures from the orthographic text the prosodic structures of which are given by means of ToBI label files. For the automatic evaluation the prosodic break indices \"3\" (intermediate phrase boundary) and \"4\" (intonation phrase boundary) are compared with the terminals extracted from the extensive syntactic structures generated by the parser. These terminals are assumed to be the carriers of the phrase boundaries.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-136"
  },
  "fivela99_eurospeech": {
   "authors": [
    [
     "Barbara Gili",
     "Fivela"
    ]
   ],
   "title": "The prosody of left-dislocated topic constituents in italian read speech",
   "original": "e99_0531",
   "page_count": 5,
   "order": 139,
   "p1": "531",
   "pn": "534",
   "abstract": [
    "The prosody of the left periphery of the sentence is investigated by means of acoustic analysis of read speech data. In Italian, syntactic constituents can be left dislocated when they represent the topic or focus part in the sentence. In this paper, the characteristics of topic and focus constituents are then compared, in particular when different types of prosodic focalization affect them. Results of the analysis of the corpus data are presented. On the basis of this analysis, left dislocated constituents appear always to be prosodically separated from the following material. In case of multiple left dislocation, differences in the prosodic realization are found, and they relate to the marked or unmarked order of the left dislocated constituents.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-137"
  },
  "haas99_eurospeech": {
   "authors": [
    [
     "Jürgen",
     "Haas"
    ],
    [
     "Volker",
     "Warnke"
    ],
    [
     "Heinrich",
     "Niemann"
    ],
    [
     "M.",
     "Cettolo"
    ],
    [
     "A.",
     "Corazza"
    ],
    [
     "D.",
     "Falavigna"
    ],
    [
     "G.",
     "Lazzari"
    ]
   ],
   "title": "Semantic boundaries in multiple languages",
   "original": "e99_0535",
   "page_count": 4,
   "order": 140,
   "p1": "535",
   "pn": "538",
   "abstract": [
    "This paper presents the results obtained for the task of detecting Semantic Boundaries (SBs) in spoken language using two different methods on the same data set. Hence we first introduce the two approaches developed by ITC-Irst in Trento (Italy) and the LME of the University Erlangen (Germany) and discuss the individually obtained results. The basis for the decision upon SBs in both cases are textual and prosodic features. The LME has already worked for several years on the computation and application of prosodic features in automatic speech processing within the -\\verbmobil-\\footnote{This work was funded by the German Federal Ministry of Education, Science, Research and Technology (BMBF) in the framework of the -\\verbmobil- Project under Grant 01 IV 102 H/0. The responsibility for the contents lies with the authors.- project. The approaches developed in that project were adapted to work on the data collected at IRST in the Italian language. Finally we compare the results we obtain with the German SB detection against the Italian result with regard to precision and recall.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-138"
  },
  "kim99_eurospeech": {
   "authors": [
    [
     "Yeon-Jun",
     "Kim"
    ],
    [
     "Heo-Jin",
     "Byeon"
    ],
    [
     "Yung-Hwan",
     "Oh"
    ]
   ],
   "title": "Prosodic phrasing in korean, determine governor, and then split or not",
   "original": "e99_0539",
   "page_count": 4,
   "order": 141,
   "p1": "539",
   "pn": "542",
   "abstract": [
    "This paper introduces a prosodic phrasing method in Korean to improve the naturalness of speech synthesis, especially in text-tospeech conversion. In prosodic phrasing, it is necessary to understand the structure of a sentence through a language pro-cessing procedure, such as POS tagging and parsing, since syn-tactic structure correlates better with the prosodic structure of speech than with other factors. In this paper, the prosodic phrasing procedure is treated from two perspectives: dependency parsing and prosodic phrasing using dependency relations. This is appropriate for Ural-Altaic, since a prosodic boundary in speech usually concurs with a gov-ernor of dependency relation. From experimental results, using the proposed method achieved 12% improvement in prosody boundary prediction accuracy with a speech corpus consisting 300 sentences uttered by 3 speakers.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-139"
  },
  "mersdorf99_eurospeech": {
   "authors": [
    [
     "Joachim J.",
     "Mersdorf"
    ],
    [
     "Kai U.",
     "Schmidt"
    ],
    [
     "Stefanie",
     "Köster"
    ]
   ],
   "title": "Linear prediction coding of individual pitch accent shapes",
   "original": "e99_0543",
   "page_count": 4,
   "order": 142,
   "p1": "543",
   "pn": "546",
   "abstract": [
    "The LPC-Intonation model presented here is dedicated to integrate speaker-dependent features into F0 modeling and analysis. It has been developed for an additional prosodic speaker-transformation based on the command-response approach for intonation features [1,2,3]. The basic principles of this integrated method for automatic analysis, coding, prediction and generation of F0 contours have already been presented in [7,9]. In this paper we are focusing on individual aspects of the resulting LPC filter parameters. Therefore we analyzed F0 contours of the 16 speakers in the German Phondat II Corpus. We found some individual differences of global parameters in speakers accent shapes that can be also modeled in synthesis. It can be shown that the resulting parameters are necessarily independent of linguistic information. Moreover, there are significant differences for each speaker. When transforming one speaker into another, a set of individual pitch tones for accents (peaks and valleys) and the resynthesis filter must be changed.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-140"
  },
  "nakatani99_eurospeech": {
   "authors": [
    [
     "Christine H.",
     "Nakatani"
    ]
   ],
   "title": "Prominence variation beyond given/new",
   "original": "e99_0547",
   "page_count": 4,
   "order": 143,
   "p1": "547",
   "pn": "550",
   "abstract": [
    "Prominence variation is known to be determined in part by discourse factors, such as the givenness or newness of the discourse entity being realized to the discourse. However, few empirical studies have been carried out to explain a wider range of phenomena occurring in natural speech corpora. In this study, corpus linguistics methods are applied to a task-oriented monologue corpus to show that the given/new dichotomy does not explain the data at hand. Rather, there are marked linguistic configurations in which discourse focusing status combines with other linguistic cues to meaning - such as grammatical function and form of referring expression - to convey subtle shades of given/new that can be captured by using a computational discourse model to define attentional focusing status.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-141"
  },
  "streefkerk99_eurospeech": {
   "authors": [
    [
     "Barbertje M.",
     "Streefkerk"
    ],
    [
     "Louis C. W.",
     "Pols"
    ],
    [
     "Louis F. M. ten",
     "Bosch"
    ]
   ],
   "title": "Acoustical features as predictors for prominence in read aloud dutch sentences used in ANN's",
   "original": "e99_0551",
   "page_count": 4,
   "order": 144,
   "p1": "551",
   "pn": "554",
   "abstract": [
    "In this paper we present several acoustical features, which are used as predictors for prominence. A set of 1244 sentences from 273 different speakers is selected from the Dutch Polyphone Corpus. Via listening experiments the subjective prominence markers are obtained. Several acoustical features concerning F 0 , energy and duration are derived and used as predictors for prominence. The sentences are divided in a test and a training set, to test and train neural networks with different topologies and different input features. The first results show that a classification of prominent and non-prominent words is possible with 82.1% correct for an independent test set.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-142"
  },
  "theune99_eurospeech": {
   "authors": [
    [
     "Mariet",
     "Theune"
    ]
   ],
   "title": "Parallelism, coherence, and contrastive accent",
   "original": "e99_0555",
   "page_count": 4,
   "order": 145,
   "p1": "555",
   "pn": "558",
   "abstract": [
    "This paper discusses an experiment concerning the assignment of contrastive accents, i.e., accents used to indicate the presence of contrastive information. The experiment tested which of two existing approaches to the determination of contrastive information gives the most natural results. According to one approach the presence of `alternative items' is the only condition for the assignment of contrastive accent; according to the other the presence of parallelism between sentences also is a condition. The experimental results indicate that the presence of alternative items combined with parallelism always triggers a preference for contrastive accent, whereas in the absence of parallelism, accent assignment seems to depend on the degree of coherence of the utterance.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-143"
  },
  "bonneau99_eurospeech": {
   "authors": [
    [
     "Anne",
     "Bonneau"
    ],
    [
     "Parham",
     "Mokhtari"
    ]
   ],
   "title": "A phonetically-guided diagnosis of auditory deficiency based on synthetic speech stimuli",
   "original": "e99_0559",
   "page_count": 4,
   "order": 146,
   "p1": "559",
   "pn": "562",
   "abstract": [
    "We propose a phonetically-guided diagnosis of auditory deficiency, which hinges on a carefully constructed cor-pus of synthetic sounds. Our aim is to complement the diagnosis of sensorineural hearing deficiencies, in order to improve the correction afforded by auditory prosthesis. We first design a vowel corpus, based upon pairs of two-formant, steady-state synthetic French vowels, where the vowels of each pair are chosen to differ only in the fre-quency of one of their two formants. To test our method, we simulate a frequency-selective loss of audibility, by specifying a piece-wise linear audibility curve with a minimum of -40 dB at a given centre-frequency (1.3, 1.6, and 1.9 kHz). Results of perceptual experiments with normal hearing people tend to show that our synthetic data set is amenable to the diagnosis of the frequency region where the simulated hearing problem is most acute.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-144"
  },
  "godinollorente99_eurospeech": {
   "authors": [
    [
     "Juan I.",
     "Godino-Llorente"
    ],
    [
     "Santiago",
     "Aguilera-Navarro"
    ],
    [
     "Carlos",
     "Hernández-Espinosa"
    ],
    [
     "Mercedes",
     "Fernández-Redondo"
    ],
    [
     "Pedro",
     "Gómez-Vilda"
    ]
   ],
   "title": "On the selection of meaningful speech parameters used by a pathologic/non pathologic voice register classifier",
   "original": "e99_0563",
   "page_count": 4,
   "order": 147,
   "p1": "563",
   "pn": "566",
   "abstract": [
    "Most of vocal and voice diseases cause changes in the voice. These diseases have to be diagnosed and treated during an early stage. There is an increased risk for vocal and voice diseases due to the modern way of life. Acoustic voice analysis is an effective and non-invasive tool due to: a) Objective support of the diagnostics. b) Screening the vocal and voice diseases and especially their early detection. c) Objective determination of the impairment of the vocal function. d) Objective evaluation of the effect of the air pollution on the voice. e) Evaluation of surgical and pharmacological treatments. f) Evaluation of the rehabilitation. Many algorithms to calculate acoustic parameters have been developed and it is demonstrated that there is a great correlation between deviations of parameters and pathologies. The effectiveness and importance of the acoustic analysis of pathological voices has been proven by many experimental researches demonstrating that acoustic parameters of pathologic voices are deviated from the mean. The authors have focused their task in separation of pathologic/non pathologic voices, and evaluating the meaningful acoustic parameters by means of neural network technology and pruning methods.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-145"
  },
  "harborg99_eurospeech": {
   "authors": [
    [
     "Erik",
     "Harborg"
    ],
    [
     "Trym",
     "Holter"
    ],
    [
     "Magne Hallstein",
     "Johnsen"
    ],
    [
     "Torbjon",
     "Svendsen"
    ]
   ],
   "title": "On-line captioning of TV-programs for the hearing impaired",
   "original": "e99_0567",
   "page_count": 4,
   "order": 148,
   "p1": "567",
   "pn": "570",
   "abstract": [
    "A system for on-line generation of closed captions for broadcast of live TV-programs is described. During broadcast, a commentator formulates a possibly condensed, but semantically correct version of the original speech. These compressed phrases are recognized by a continuous speech recognizer, and the resulting captions are directly fed into the teletext system. This application will provide the hearing impaired with an option to read captions for live broadcast programs, i.e., when off-line captioning is not feasible. The main advantage in using a speech recognizer rather than a stenography-based system (e.g., Velotype) is the relaxed requirements for operator training. Also, the amount of text generated by a system based on stenography tends to be large, thus making it harder to read.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-146"
  },
  "jo99_eurospeech": {
   "authors": [
    [
     "Cheol-Woo",
     "Jo"
    ],
    [
     "Dae-Hyun",
     "Kim"
    ]
   ],
   "title": "Classification of pathological voice into normal/benign/malignant state",
   "original": "e99_0571",
   "page_count": 4,
   "order": 149,
   "p1": "571",
   "pn": "574",
   "abstract": [
    "In this paper we propose a new method to classify the pathological voice into normal, benign and malignant cases. New parameter is proposed to discriminate each class. New parameter is based on cepstral analysis technique. Pathological speech signal is collected at the hospital. Normal speech signal is also contained at the same database and analyzed as well. Then the results are compared to find the differences between normal and pathological speech. Source components are separated using cepstrum after obtaining residual signal from speech. Then the ratios between harmonic components and noise components are obtained from the original signal and residual signal. Finally a neural network is used to train and classify normal, benign and malignant states of speech.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-147"
  },
  "maruyama99_eurospeech": {
   "authors": [
    [
     "Ichiro",
     "Maruyama"
    ],
    [
     "Yoshiharu",
     "Abe"
    ],
    [
     "Eiji",
     "Sawamura"
    ],
    [
     "Tetsuo",
     "Mitsuhashi"
    ],
    [
     "Terumasa",
     "Ehara"
    ],
    [
     "Katsuhiko",
     "Shirai"
    ]
   ],
   "title": "Cognitive experiments on timing lag for superimposing closed captions",
   "original": "e99_0575",
   "page_count": 4,
   "order": 150,
   "p1": "575",
   "pn": "578",
   "abstract": [
    "This paper describes cognitive characteristics of timing difference for closed captions superimposed onto TV news programs. It was reported that timing delays for superimposing disrupts hearing impaired people's enjoyment and intelligibility of TV, but nobody has yet investigated the permissible limit for timing difference. This study presents subjects' permissible and preferable limits of the timing differences and also the correlation between the limits and the four factors: subject groups, news contents, caption formats and the direction of timing difference. Significant difference between the hearing impaired and the hearing was recognized for those limits. For the permissible limit of the hearing impaired, significant difference was recognized in subject groups and caption formats. For captions outside the picture, the limits for the deaf and late-deafened were 3.95 and 4.94 s, respectively. For caption inside the picture, the permissible limit for the late-deafened was 1.63 s, but that for the deaf was not recognized. For the permissible limit of the hearing impaired, significant difference was recognized only in caption formats and the limits by caption formats were 5.38 and 1.39 s for the outside and inside formats, respectively.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-148"
  },
  "ogner99_eurospeech": {
   "authors": [
    [
     "Marcel",
     "Ogner"
    ],
    [
     "Zdravko",
     "Kacic"
    ]
   ],
   "title": "Speaker normalization for audio-visual articulation training",
   "original": "e99_0579",
   "page_count": 4,
   "order": 151,
   "p1": "579",
   "pn": "582",
   "abstract": [
    "The paper describes formant based speaker zation method suitable for speech visualization and articulation training systems. The method estimates the error function obtained from speaker formant characteristics for a given vowel. Estimated error function gives information for critical band filter shifting on mel-warped frequency scale. The paper also describes accurate technique for formant tracking.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-149"
  },
  "prizljakovac99_eurospeech": {
   "authors": [
    [
     "Tatjana",
     "Prizl-Jakovac"
    ]
   ],
   "title": "Vowel production in aphasia",
   "original": "e99_0583",
   "page_count": 3,
   "order": 152,
   "p1": "583",
   "pn": "586",
   "abstract": [
    "This study explored a voice characteristics in aphasic patients suffering from the damage of the left and right cerebral hemispheres regardless of the aphasia type. A series of acoustic analyses were conducted including fundamental frequency in Hz (Fo), duration of the vowel \"a\" phonation, jitter and shimmer. A pattern of 20 male aphasic patients was included in the research. At the aphasics with the damage of the right cerebral hemisphere means show higher value of the basic laryngeal tone (Fo) but shorter fonation. Respondents with aphasia caused by a left hemisphere damage achieved better scores on variable duration of the vowel \"a\". It seems that disorder in aphasics with right lesion primarily reflects an inability to implement particular types of articulatory gestures or articulatory parameters rather than an inability to implement particular vowel.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-150"
  },
  "cerisara99_eurospeech": {
   "authors": [
    [
     "Christophe",
     "Cerisara"
    ],
    [
     "Jean-Paul",
     "Haton"
    ],
    [
     "Dominique",
     "Fohr"
    ]
   ],
   "title": "Towards a global optimization scheme for multi-band speech recognition",
   "original": "e99_0587",
   "page_count": 4,
   "order": 153,
   "p1": "587",
   "pn": "590",
   "abstract": [
    "In this paper, we deal with a new method to globally optimize a Multi-Band Speech Recognition (MBSR) system. We have tested our algorithm with the TIMIT database and obtained a significant improvement in the accuracy over a basic HMM system for clean speech. The goal of this work is not to prove the effectiveness of MBSR, what has yet been done, but to improve the training scheme by introducing a global optimization procedure. A consequence of this method is that the models are no longer phone-models, but define new classes in the phonetic space which might better model the acoustic information carried by each band.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-151"
  },
  "janin99_eurospeech": {
   "authors": [
    [
     "Adam",
     "Janin"
    ],
    [
     "Dan",
     "Ellis"
    ],
    [
     "Nelson",
     "Morgan"
    ]
   ],
   "title": "Multi-stream speech recognition: ready for prime time?",
   "original": "e99_0591",
   "page_count": 4,
   "order": 154,
   "p1": "591",
   "pn": "594",
   "abstract": [
    "Multi-stream and multi-band methods can improve the accuracy of speech recognition systems without overly increasing the complexity. However, they cannot be applied blindly. In this paper, we review our experience applying multi-stream and multi-band methods to the Broadcast News corpus. We found that multi-stream systems using different acoustic front-ends provide a significant improvement over single stream systems. However, despite the fact that they have been successful on smaller tasks, we have not yet been able to show any improvement using multi-band methods. We report various insights gained from the experience in applying these methods in a large-vocabulary task.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-152"
  },
  "mirghafori99_eurospeech": {
   "authors": [
    [
     "Nikki",
     "Mirghafori"
    ],
    [
     "Nelson",
     "Morgan"
    ]
   ],
   "title": "Sooner or later: exploring asynchrony in multi-band speech recognition",
   "original": "e99_0595",
   "page_count": 4,
   "order": 155,
   "p1": "595",
   "pn": "598",
   "abstract": [
    "Multi-band speech recognition is an exploratory paradigm in which each frequency region is treated as a distinct source of information and the streams are combined after each is processed independently. A number of researchers have hypothesized that it is advantageous to combine the sub-frequency information in an asynchronous manner. This paper examines this hypothesis, using two different approaches in relaxing synchrony constraints: HMM decomposition/recombination [19] and two-level dynamic programming (DP) [16]. Drawing on this work and those of others [2, 18], we conclude that relaxing the synchrony constraints indiscriminately for all phone-to-phone transitions does not consistently and significantly reduce the word error rate. The optimal permissible asynchrony must depend on both the phone-class transitions and the training-data statistics.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-153"
  },
  "morris99_eurospeech": {
   "authors": [
    [
     "Andrew",
     "Morris"
    ],
    [
     "Astrid",
     "Hagen"
    ],
    [
     "Hervé",
     "Bourlard"
    ]
   ],
   "title": "The full combination sub-bands approach to noise robust HMM/ANN based ASR",
   "original": "e99_0599",
   "page_count": 4,
   "order": 156,
   "p1": "599",
   "pn": "602",
   "abstract": [
    "The performance of most ASR systems degrades rapidly with data mismatch relative to the data used in training. Under many realistic noise conditions a significant proportion of the spectral representation of a speech signal, which is highly redundant, remains uncorrupted. In the \"missing feature\" approach to this problem mismatching data is simply ignored, but the need to base recognition on unorthogonalised spectral features results in reduced performance in clean speech. In multiband ASR the results from independent recognition on a number of within-band orthogonalised sub-bands are combined. This approach more accurately reflects the uncertainty in mismatch detection, but loss of joint information due to independent sub-band processing can also result in reduced performance with clean speech. In this article the \"full combination\" approach to noise robust ASR is presented in which multiple data streams are associated not with individual sub-bands but with sub-band combinations. In this way no assumption of sub-band independence is required. Initial tests show some improved robustness to noise with no significant loss of performance with clean speech. orthogonalisation prior to recognition, which results in unacceptably low performance in clean speech.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-154"
  },
  "okawa99_eurospeech": {
   "authors": [
    [
     "Shigeki",
     "Okawa"
    ],
    [
     "Takehiro",
     "Nakajima"
    ],
    [
     "Katsuhiko",
     "Shirai"
    ]
   ],
   "title": "A recombination strategy for multi-band speech recognition based on mutual information criterion",
   "original": "e99_0603",
   "page_count": 4,
   "order": 157,
   "p1": "603",
   "pn": "606",
   "abstract": [
    "This paper presents a recombination strategy for multi-band automatic speech recognition (MB-ASR). Several recent works have suggested that MB-ASR gives more accurate recognition, especially in noisy acoustic environments. The main issue in this study concerns the sub-band score recombination in MB-ASR framework. Intuitively, it seems very improbable that all sub-band features have the same amount of information for speech recognition. We therefore investigate to weight the contribution from each band at the recombination process by using a strategy derived from the information theory. The quantity of information is well determined by the mutual information between band features and target phoneme categories to be recognized. The experimental results show that the recognition accuracy improves for noisy speech by using three and six stream systems with the proposed approach.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-155"
  },
  "beutnagel99_eurospeech": {
   "authors": [
    [
     "Mark",
     "Beutnagel"
    ],
    [
     "Mehryar",
     "Mohri"
    ],
    [
     "Michael",
     "Riley"
    ]
   ],
   "title": "Rapid unit selection from a large speech corpus for concatenative speech synthesis",
   "original": "e99_0607",
   "page_count": 4,
   "order": 158,
   "p1": "607",
   "pn": "610",
   "abstract": [
    "Concatenative Text-to-Speech (TTS) systems such as those described by Hunt and Black [6] can select at synthesis time from a very large number of recorded units. The selected units are chosen to minimize a combination of target and join costs for a given sentence. However, the join costs, in particular, can be quite expensive to com-pute, even when this computation has been optimized. If possible, we would avoid this computation by precomputing and caching all the possible join costs, but their number is prohibitive. Although the search space of possible joins is large, we have found that only a small fraction are selected in practice. By synthesizing a large quantity of text and logging the units actually selected, we were able to gather usage statistics and construct a practical and efficient cache of concatenation costs. Use of this cache dramatically decreased the runtime of the AT&T Next-Generation TTS system [1] with negligible effect on speech quality. Experiments show that by caching 0.7% of the possible joins, 99% of the join cost computations can be avoided.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-156"
  },
  "chen99b_eurospeech": {
   "authors": [
    [
     "Jing-Dong",
     "Chen"
    ],
    [
     "Nick",
     "Campbell"
    ]
   ],
   "title": "Objective distance measures for assessing concatenative speech synthesis",
   "original": "e99_0611",
   "page_count": 4,
   "order": 159,
   "p1": "611",
   "pn": "614",
   "abstract": [
    "Several different acoustic transforms of the speech signal are compared for use in the assessment and evaluation of concatenative speech synthesis. The transforms tested include LPC, LSP, MFCC, bispectrum, Mellin transform of the log spectrum, Wigner-Ville distribution (WVD), etc. The computed distances between a synthesised utterance and a naturally spoken version of the same sentence are compared by correlation with perceptually-based scores obtained from a MOS evaluation. The results show that the distances computed using the bispectrum have the highest degree of correlation with the MOS score. Both the RMFCC and the LPC outperform the MFCC and the LPCC. The WVD-based cepstrum is found to behave poorly in this task.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-157"
  },
  "lewis99_eurospeech": {
   "authors": [
    [
     "Eric",
     "Lewis"
    ],
    [
     "Mark",
     "Tatham"
    ]
   ],
   "title": "Word and syllable concatenation in text-to-speech synthesis",
   "original": "e99_0615",
   "page_count": 4,
   "order": 160,
   "p1": "615",
   "pn": "618",
   "abstract": [
    "MeteoSPRUCE is a database of 2000 words re-lating to weather forecasting. While such a database is clearly not large enough to be definitive, its usability can be greatly extended by excising syllables from polysyllabic words in its inventory and recombining them to form new words [1], [2], [3]. The authors believe that it provides sufficient data to start to enable conclusions to be drawn as to how syllables should be modified for concatenation in contexts other than those in which they were recorded. A classification scheme for syllables, based on the class of their initial and final segments, has been defined and used to determine a set of rules for making modifications to syllables so that when concatenated the joins are perceptually not noticeable.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-158"
  },
  "stober99_eurospeech": {
   "authors": [
    [
     "Karlheinz",
     "Stöber"
    ],
    [
     "Thomas",
     "Portele"
    ],
    [
     "Petra",
     "Wagner"
    ],
    [
     "Wolfgang",
     "Hess"
    ]
   ],
   "title": "Synthesis by word concatenation",
   "original": "e99_0619",
   "page_count": 4,
   "order": 161,
   "p1": "619",
   "pn": "622",
   "abstract": [
    "Verbmobil is a speaker-independent system that offers transla-tion assistance in dialogue situations. In cooperation with other institutes we are developing the speech synthesis module within Verbmobil for German and American English. Current priority is given to an enhancement of naturalness of our PSOLA based concatenative synthesis of German. Due to a tight schedule we investigated alternative methods to our traditional approach. In our opinion, quality enhancement of PSOLA based concatenative synthesis has reached its limits. We decided to avoid concatenation points and prosodic ma-nipulations as much as possible. Our new approach obtains prosodic diversity by using those synthesis units which inherently possess the necessary prosodic features. To get fast results we started with words as primary synthesis units. The outcome is encouraging. Even a first version of our system frequently succeeds in synthesising utterances with close to natural quality.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-159"
  },
  "taylor99_eurospeech": {
   "authors": [
    [
     "Paul",
     "Taylor"
    ],
    [
     "Alan W.",
     "Black"
    ]
   ],
   "title": "Speech synthesis by phonological structure matching",
   "original": "e99_0623",
   "page_count": 4,
   "order": 162,
   "p1": "623",
   "pn": "626",
   "abstract": [
    "This paper presents a new technique for speech synthesis by unit selection. The technique works by specifying the synthesis target and the speech database as phonological trees, and using a selection algorithm which finds the largest parts of trees in the database which match parts of the target tree. The technique avoids many of the errors made by prosody generation modules by incorporating their operation in the selection implicitly. A technique for using signal processing only when it is needed most is also described. The technique produces better quality speech than previous approaches and is also significantly faster.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-160"
  },
  "bloothooft99b_eurospeech": {
   "authors": [
    [
     "Gerrit",
     "Bloothooft"
    ]
   ],
   "title": "The implementation of a european masters in language and speech",
   "original": "e99_0627",
   "page_count": 4,
   "order": 163,
   "p1": "627",
   "pn": "630",
   "abstract": [
    "The study of spoken language communication and the development of applications ask for students that are educated and trained in both speech sciences and natural language processing. The paper describes the model, structure, procedures and contents of a European Masters in Language and Speech that opens the possibility to realise this in a truly international way. The implementations at the eleven universities where the scheme will start in 1999-2000 are described and show a rich variety of curricula all matching the same qualifications.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-161"
  },
  "cooke99_eurospeech": {
   "authors": [
    [
     "Martin",
     "Cooke"
    ],
    [
     "Helen",
     "Parker"
    ],
    [
     "Guy J.",
     "Brown"
    ],
    [
     "Stuart N.",
     "Wrigley"
    ]
   ],
   "title": "The interactive auditory demonstrations project",
   "original": "e99_0631",
   "page_count": 3,
   "order": 164,
   "p1": "631",
   "pn": "634",
   "abstract": [
    "Topics in speech and hearing are well-suited to demonstrations using media other than the printed word. Currently, educators rely largely on passive formats such as the CD collections for general auditory psychophysics [6], auditory scene analysis [2] and cochlear damage [10]. Progress in programming tools and cheap, multimedia hardware now presents the potential to go much further. The interactive auditory demonstrations project aims to provide the user with an environment in which to explore the many phenomena and processes associated with speech and hearing. This promotes a much richer space of parameter manipulation than is possible via passive media. Further, the ability to initiate actions, repeat procedures and benefit from practically any kind of multimodal feedback enables a much wider range of learning possibilities. This paper focusses on the interface issues which are revealed by interactive exploration of the domain. A demonstration of linear prediction is presented to illustrate these issues.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-162"
  },
  "mctear99b_eurospeech": {
   "authors": [
    [
     "Michael F.",
     "McTear"
    ]
   ],
   "title": "Curricula and courseware in spoken language engineering in europe: a critical appraisal",
   "original": "e99_0635",
   "page_count": 4,
   "order": 165,
   "p1": "635",
   "pn": "638",
   "abstract": [
    "This paper summarises work by the Spoken Language Engineering (SLE) Working Group of the Socrates Thematic Network in Speech Communication Sciences. The thematic network has shown that computer-based teaching aids are vital to the future development of SLE education. This follows from the multidisciplinary and technical nature of SLE, which requires novel ways of presenting unfamiliar material. The paper analyses software resources available in relation to curricular requirements and educational criteria and makes recommendations for modules in an SLE curriculum. In addition, we identify areas for which high-quality courseware is, to our knowledge, unavailable and identify actions to fill these remaining gaps.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-163"
  },
  "hoffmann99_eurospeech": {
   "authors": [
    [
     "Rüdiger",
     "Hoffmann"
    ],
    [
     "Bettina",
     "Ketzmerick"
    ],
    [
     "Ulrich",
     "Kordon"
    ],
    [
     "Steffen",
     "Kürbis"
    ]
   ],
   "title": "An interactive tutorial on text-to-speech synthesis from diphones in time domain",
   "original": "e99_0639",
   "page_count": 4,
   "order": 166,
   "p1": "639",
   "pn": "642",
   "abstract": [
    "We are presenting an interactive course on speech synthesis which is designed to support the education in speech communication. In the basic section, the fundamental principles of speech synthesis are explained. To explore a complete text-to-speech (TTS) system, the user is provided with access to the Dresden Speech Synthesizer DreSS. The user may type any text, and he may observe how the system processes the text from the first linguistic preprocessing until the acoustic synthesis. A further section is devoted to the crucial problem of correct segmentation of the speech elements used for the concatenative synthesis. The user may select his own diphone segments from a given speech data base. The quality of the segments may be evaluated acoustically, and hints are given to avoid errors in cutting. Thus, the user will learn how to select the segments with good quality. The course is written in HTML and Java and is designed for Internet application.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-164"
  },
  "qvarfordt99_eurospeech": {
   "authors": [
    [
     "Pernilla",
     "Qvarfordt"
    ],
    [
     "Arne",
     "Jönsson"
    ]
   ],
   "title": "Evaluating the dialogue component in the GULAN educational system",
   "original": "e99_0643",
   "page_count": 4,
   "order": 167,
   "p1": "643",
   "pn": "646",
   "abstract": [
    "In this paper we present results on the learning effects of using the gulan educational system to understand spoken dialogue systems. The investigation is restricted to the dialogue management component, which uses a subset of the linlin dialogue manager. The results are based on an evaluation of questionnaires given to the students and of tutors assessment of the students knowledge before and after using the educational system. The study shows that the students knowledge on focus and dialogue structure for dialogue systems improved after using the system. We also gained insights on further development of the system.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-165"
  },
  "harris99_eurospeech": {
   "authors": [
    [
     "Matthew",
     "Harris"
    ],
    [
     "Xavier",
     "Aubert"
    ],
    [
     "Reinhold",
     "Haeb-Umbach"
    ],
    [
     "Peter",
     "Beyerlein"
    ]
   ],
   "title": "A study of broadcast news audio stream segmentation and segment clustering",
   "original": "e99_1027",
   "page_count": 4,
   "order": 168,
   "p1": "1027",
   "pn": "1030",
   "abstract": [
    "In transcription of broadcast news, dividing the signal into homogeneous segments, and clustering to-gether similar segments is important. Decoding a complete broadcast news program in one chunk is technically dificult. Also, through creation of homogeneous clusters of segments, improvement from adaptation can be increased. Two systems of segmentation and clustering are compared. The best system used the BIC algorithm to produce long, homogeneous segments, and a nearest neighbour bottom-up agglomerative clustering algo-rithm to produce homogeneous clusters. Adaptation brought aword error rate (WER) improvement from 23:4% to 21:0% using the automatic segmentation and clustering, compared to an improvement from 21:8% to 20:0% using a handmade \\correct\" segmentation and clustering.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-166"
  },
  "liu99_eurospeech": {
   "authors": [
    [
     "Daben",
     "Liu"
    ],
    [
     "Francis",
     "Kubala"
    ]
   ],
   "title": "Fast speaker change detection for broadcast news transcription and indexing",
   "original": "e99_1031",
   "page_count": 4,
   "order": 169,
   "p1": "1031",
   "pn": "1034",
   "abstract": [
    "In this paper, we describe a new speaker change detection algorithm designed for fast transcription and audio indexing of spoken broadcast news. We have designed a two-stage algorithm that begins with a gender-independent phone-class recognition pass. We collapse the phoneme inventory to only 4 broad classes and include 4 different models for non-speech, resulting in a small fast decoder that runs in less than 0.1 times real-time. The second stage of the SCD algorithm hypothesizes a speaker change boundary between every phone in the labeled input. The phone level time resolution in our approach permits the algorithm to run quickly while maintaining the same accuracy as a frame level approach. Applying the new algorithms to a large sample of broadcast news programs resulted in improvements in speaker change detection accuracy, speech recognition accuracy, and speed.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-167"
  },
  "palmery99_eurospeech": {
   "authors": [
    [
     "David D.",
     "Palmery"
    ],
    [
     "Mari",
     "Ostendorf"
    ],
    [
     "John D.",
     "Burgerz"
    ]
   ],
   "title": "Robust information extraction from spoken language data",
   "original": "e99_1035",
   "page_count": 4,
   "order": 170,
   "p1": "1035",
   "pn": "1038",
   "abstract": [
    "In this paper we address the problem of information extraction from speech data, particularly improving robustness to automatic recognition errors. We describe a baseline probabilistic model that uses wordclass smoothing in a phrase n-gram language model. The model is adjusted to the error characteristics of a speech recognizer by inserting error tokens in the training data and by using word confidences in decoding to account for possible errors in the recognition output. Experiments show improved performance when training and test conditions are matched.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-168"
  },
  "renals99_eurospeech": {
   "authors": [
    [
     "Steve",
     "Renals"
    ],
    [
     "Yoshihiko",
     "Gotoh"
    ]
   ],
   "title": "Integrated transcription and identification of named entities in broadcast speech",
   "original": "e99_1039",
   "page_count": 4,
   "order": 171,
   "p1": "1039",
   "pn": "1042",
   "abstract": [
    "This paper presents an approach to integrating functions for both transcription and named entity (NE) identification into a large vocabulary continuous speech recognition system. It builds on NE tagged language modelling approach, which was recently applied for development of the statistical NE annotation system. We also present results for proper name identification experiment using the Hub-4 evaluation data.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-169"
  },
  "woodland99_eurospeech": {
   "authors": [
    [
     "P. C.",
     "Woodland"
    ],
    [
     "J. J.",
     "Odell"
    ],
    [
     "T.",
     "Hain"
    ],
    [
     "G. L.",
     "Moore"
    ],
    [
     "T. R.",
     "Niesler"
    ],
    [
     "Andreas",
     "Tuerk"
    ],
    [
     "E. W. D.",
     "Whittaker"
    ]
   ],
   "title": "Improvements in accuracy and speed in the HTK broadcast news transcription system",
   "original": "e99_1043",
   "page_count": 4,
   "order": 172,
   "p1": "1043",
   "pn": "1046",
   "abstract": [
    "This paper describes a number of recent improvements to the HTK Broadcast News Transcription System. Changes to the system include the use of more acoustic training data; use of cluster-based variance normalisation and vocal tract length normalisation; the use of interpolated language models and enhanced adaptation using a full variance transform. These changes produce an reduction in word error rate of 13%. A simplified version of the system has also been constructed that runs in less than 10 times real-time and gives a 2.3% absolute higher error rate than the 300xRT full system.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-170"
  },
  "brindopke99_eurospeech": {
   "authors": [
    [
     "Christel",
     "Brindöpke"
    ],
    [
     "Gernot A.",
     "Fink"
    ],
    [
     "Franz",
     "Kummert"
    ]
   ],
   "title": "A comparative study of HMM-based approaches for the automatic recognition of perceptually relevant aspects of spontaneous German speech melody",
   "original": "e99_0699",
   "page_count": 4,
   "order": 173,
   "p1": "699",
   "pn": "710",
   "abstract": [
    "Three approaches to the speaker independent automatic recognition of melodic aspects of spontaneous German are presented. All systems are based on Hidden Markov Models. Their input is restricted to the speech signal from which a feature extraction component derives eleven prosodic features. No additional information -- as commonly used for prosody recognition -- like word chains, word hypotheses, further segmental or lexical prosodic information (e.g. stress placement) is required. The three systems are tested and compared with respect to their performance on a speaker-independent recognition task on spontaneous German speech focusing on three functional aspects of speech melody (accent lending, boundary signalling, concatenating pitch movements) and the pause as a fourth category.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-171"
  },
  "demenko99_eurospeech": {
   "authors": [
    [
     "Grazyna",
     "Demenko"
    ],
    [
     "Wiktor",
     "Jassem"
    ]
   ],
   "title": "Modelling intonational phrase structure with artificial neural networks",
   "original": "e99_0711",
   "page_count": 4,
   "order": 174,
   "p1": "711",
   "pn": "714",
   "abstract": [
    "A model of intonation for Polish has been created on the basis of a general theory of suprasegmentals and on experiments using isolated utterances as well as continuous speech. An intonational phrase consists of an optional prenuclear tune and an obligatory nuclear tune. A training of a three-layer MLP network was performed distinguishing 9 nuclear accents: HL, ML, LL, HM, LH, LM, MH, MM, LHL and 2 secondary prenuclear accents: High (H) and Low (L). A total of 1600 structures (in constructed phrases) were used for training, and 430 for verification. The average score for training and testing was 82 percent. In continuous speech the following structures were postulated: H and L for prenuclear intonation and for nuclear intonations: R (rising), F (falling), MM (level), LHL (rising-falling). For the testing set, a score between 79 and 83 per cent was obtained. In both classifications, an 11-element vector was used to describe the intonational structures under analysis.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-172"
  },
  "duez99_eurospeech": {
   "authors": [
    [
     "Danielle",
     "Duez"
    ]
   ],
   "title": "Effects of articulation rate on duration in read French speech",
   "original": "e99_0715",
   "page_count": 4,
   "order": 175,
   "p1": "715",
   "pn": "718",
   "abstract": [
    "The present analysis examined the interactive effects of articulation rate and position in phrases and utterances on the duration of syllables, consonants, and vowels in read French speech. Three major tendencies emerged. 1) Rate changes primarily affected vocalic segments. 2) Non-prominent (penultimate) syllables, consonants and vowels were more stable than their prominent (final) counterparts. This finding, which differs from that obtained for lexical-stress languages, is consistent with the claim that French is not a stress language but a boundary language. 3) At the normal rate, there was no significant difference in mean final-syllable, consonant and vowel duration in the utterance-internal and utterance-final positions. This finding suggests that utterance-final lengthening varies with articulation rate, speech style and speaker, and points out the need for a more comprehensive study of lengthening effects.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-173"
  },
  "gurlekian99_eurospeech": {
   "authors": [
    [
     "Jorge A.",
     "Gurlekian"
    ],
    [
     "Marcela Leticia",
     "Riccillo"
    ],
    [
     "Alejandro",
     "Renato"
    ],
    [
     "Jose",
     "Alvarez"
    ]
   ],
   "title": "A semi automatic method for the characterization of Spanish intonation contours",
   "original": "e99_0719",
   "page_count": 4,
   "order": 176,
   "p1": "719",
   "pn": "722",
   "abstract": [
    "The purpose of this work is to present a method to characterize Spanish Intonation Patterns (SIP) and to describe its principal features. A semi automatic method was developed, which consisted in the stylization of SIP and coding of pitch movements. A preliminary corpus of 120 sentences were collected and recorded by two native speakers of Buenos Aires (one female, one male). In the stylization process, smoothing of the row F0 contours in two stages and a polynomial approximation by straight-line segments were applied. A perceptual test assessed the intonation quality obtained for sentences using the stylized f0 contours. Subjects reported no differences with natural ones. In the coding process, the micro and macro prosodic features of the SIP were represented. Spanish ToBI notations were defined and combined with a hierarchy structure according to the ERB-rate scale. This notation strategy was applied to the corpus, resulting in a set of sentence labels. We applied this set of sentence labels to an equivalent text corpus and tested the quality of the predicted F0 contours by concatenating interpolated demi-syllables. The quality of the synthetic speech obtained, suggest the convenience to extend the method to a larger corpus.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-174"
  },
  "tolba99_eurospeech": {
   "authors": [
    [
     "Hesham",
     "Tolba"
    ],
    [
     "Douglas",
     "O'Shaughnessy"
    ]
   ],
   "title": "Towards recognizing \"non-lexical\" words in spontaneous conversational speech",
   "original": "e99_0723",
   "page_count": 4,
   "order": 177,
   "p1": "723",
   "pn": "726",
   "abstract": [
    "The purpose of this paper is to study and analyze both the -\\it non-lexical- filled pauses and intended responses in conversational spontaneous speech, and how this can be useful in both automatic speech recognition and speaker identification systems. Through experiments, it was found that we are able to distinguish between words and non-lexical words in spontaneous speech using prosodic features. Consequently, a pre-recognition of such pauses using a decision-tree based CART classifier is evident. Thus, for ASR of spontaneous speech, such pauses can be either totally omitted or considered as words to be added to the dictionary of the ASR system, consequently improving the performance of such an ASR system.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-175"
  },
  "isogai99_eurospeech": {
   "authors": [
    [
     "Mitsuaki",
     "Isogai"
    ],
    [
     "Hideyuki",
     "Mizuno"
    ]
   ],
   "title": "A new F0 contour control method based on vector representation of F0 contour",
   "original": "e99_0727",
   "page_count": 4,
   "order": 178,
   "p1": "727",
   "pn": "730",
   "abstract": [
    "This paper proposes a new fundamental frequency(F0) con-tour control method based on vector representation of F0 contour. The main points of the proposed method are as follows; (1) Desired F0 contours are created by selecting or modifying natural F0 contours held in a speech database. (2) F0 contour selection is based on statistical estimation using a vector representation of F0 contour (3) The selected F0 contour is modified to match the target context according to rules produced by statistical learning. An evaluation by listening tests confirms the superior performance of our proposed over the conventional method approach to F0 modeling.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-176"
  },
  "kleckova99_eurospeech": {
   "authors": [
    [
     "Jana",
     "Kleckova"
    ]
   ],
   "title": "Developing the database of the spontaneous speech prosody characteristics",
   "original": "e99_0731",
   "page_count": 4,
   "order": 179,
   "p1": "731",
   "pn": "734",
   "abstract": [
    "The presentation deals with an experimental database of spontaneous-speech characteristics. The database is intended for a dialog system which have been developed in the Department of Computer Science at the University of West Bohemia within the framework of the Copernicus project. This system is assembled with a speech prosody module for processing the output of the acoustic phonetic module.. Proceeding that analysis the features obtained are evaluated using a neural network so that a type of the sentence can be determined. These results were verified in a number of experiments (1000 sentences). Having analysed the linguistic model we proposed further to extend the types of prosody characteristics and to create a database. Thus, besides the intonation analysis the prosody module also involves subroutines which can evaluate the pitch, both in a sentence and in a word, and the pause. The prosodic characteristics including the sentence are stored in the database and consequently exploited by the linguistic module as an additional information used for recognizing and understanding the spontaneous speech. Processing the characteristics by usual methods of statistics the database can also be used to generate answers in the dialog system. The module was implemented in the C language an supported by the ORACLE database. For the user interface the environment SQL is suggested.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-177"
  },
  "mohler99_eurospeech": {
   "authors": [
    [
     "Gregor",
     "Möhler"
    ],
    [
     "Jörg",
     "Mayer"
    ]
   ],
   "title": "A method for the analysis of prosodic registers",
   "original": "e99_0735",
   "page_count": 4,
   "order": 180,
   "p1": "735",
   "pn": "738",
   "abstract": [
    "The position of a speaker's pitch range conveys important information about the structure of the discourse. In this study we extract the pitch range of a speech database and compare the results with a register description of global prominence. The automatic extraction of the pitch range uses a parametric description of pitch accents and boundaries. We focused on the analysis of the pitch range properties during a sub-topic change, and found that the phonetic properties extracted from a database confirmed the predictions of the phonological model of global prominence: In phrases introducing a sub-topic, the lower margin is found to be at an average level, whereas the upper margin is clearly raised (extra-high register). In messages concluding a sub-topic, the pitch range is compressed and at a low level (low-compressed register).\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-178"
  },
  "smirnova99_eurospeech": {
   "authors": [
    [
     "Natalia",
     "Smirnova"
    ]
   ],
   "title": "Whole tunes, nuclear and pre-nuclear patterns and prosodic features in the perception of interrogativity and non-finality in dutch.",
   "original": "e99_0739",
   "page_count": 4,
   "order": 181,
   "p1": "739",
   "pn": "742",
   "abstract": [
    "The paper deals with some aspects of production and perception of yes-no questions and non-final clauses in Dutch. The analysis of the prosodic characteristics of naturally produced utterances and the subjects responses obtained in the course a series of perception experiments with naturally produced and modified utterances yielded the following conclusions: · the identification of utterance type is to the largest extent determined by the presence of relevant prosodic cues in the nuclear accent · cues to utterance type in the pre-nuclear pattern appear to be used by listeners when the nuclear accent type allows of several possible interpretations · timing of the nuclear accent may be considered a relevant cue to utterance type, «early» vs. «late» rise accounting for «non-final» vs. «interrogative» preferences, «early» vs. «late» fall eliciting «statement» vs. «question/statement» reactions.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-179"
  },
  "wang99_eurospeech": {
   "authors": [
    [
     "Wern-Jun",
     "Wang"
    ],
    [
     "Yuan-Fu",
     "Liao"
    ],
    [
     "Sin-Horng",
     "Chen"
    ]
   ],
   "title": "Prosodic modeling of Mandarin speech and its application to lexical decoding",
   "original": "e99_0743",
   "page_count": 4,
   "order": 182,
   "p1": "743",
   "pn": "746",
   "abstract": [
    "In this paper, a new RNN-based prosodic modeling method for Mandarin speech recognition is proposed. It is performed in the post-processing stage of the acoustic decoding aiming at detecting word boundaries for assisting in the lexical decoding. It employs a simple RNN to learn the relationship between input prosodic features, extracted from the input utterance with syllable boundaries provided by the preceding acoustic decoding, and output information related to word boundaries. Simulations on a large single-speaker database were performed to evaluate the proposed method. Experimental results showed that 71.9% of word tags and 95.3% of punctuation mark (PM) tags could be correctly detected. By incorporating the prosodic model into an HMM-based continuous Mandarin speech recognition system, the character recognition rate increased from 73.6% to 74.7% with a reduction of 17% on the computational complexity. So the proposed prosodic modeling method is helpful for speech recognition.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-180"
  },
  "zhang99_eurospeech": {
   "authors": [
    [
     "Jin-Song",
     "Zhang"
    ],
    [
     "Hiromichi",
     "Kawanami"
    ]
   ],
   "title": "Modeling carryover and anticipation effects for Chinese tone recognition",
   "original": "e99_0747",
   "page_count": 4,
   "order": 183,
   "p1": "747",
   "pn": "750",
   "abstract": [
    "This paper presents our new approach to model tone coarticulation of Chinese continuous speech for tone recognition. We suggest that coarticulation effects between two neighboring tones are rather unstable, since they may be uni-directional, bi-directional, or none despite of the same phonetic contexts. Instability is suggested due to non-local prosodic events like prosodic phrase boundaries or stress effects. Hence, we propose that context dependent tone models should be estimated according to the exact underlying coarticulation effects. To simplify label work for coarticulation effects, label sets as few as 3 labels are adopted. Also F0 contours of tone nuclei are used to facilitate human to discriminate tone coarticulation effects. A new search algorithm for the output candidates was also proposed to adopt the new modeling method of tone coarticulation effects. Preliminary experiments on a females utterances of data corpus HKU96 showed the effectiveness of the new approach.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-181"
  },
  "besacier99_eurospeech": {
   "authors": [
    [
     "Laurent",
     "Besacier"
    ],
    [
     "J.",
     "Luettin"
    ],
    [
     "G.",
     "Maitre"
    ],
    [
     "E.",
     "Meurville"
    ]
   ],
   "title": "Experimental evaluation of text-independent speaker verification on laboratory and field test databases in the M2VTS project",
   "original": "e99_0751",
   "page_count": 4,
   "order": 184,
   "p1": "751",
   "pn": "754",
   "abstract": [
    "This paper describes experiments of a text-independent speaker verification method that has been evaluated on two laboratory databases (M2VTS and XM2VTS) and on one field test database (LoCoMic). This work has been performed within the European ACTS-M2VTS project (Multi-Modal Verification for Teleservices and Security Applications) which is concerned with person authentication using multiple modalities. The system achieved good performance on the M2VTS and XM2VTS databases whereas the performance decreased on the LoCoMic database which is more representative of real conditions. In fact, we have shown that in text independent mode, the performance differs significantly according to the test item considered (digits, name, and sentence). This shows that, even in text independent mode, the access control system should propose a structure for the uttered sentence.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-182"
  },
  "balchandran99_eurospeech": {
   "authors": [
    [
     "Rajesh",
     "Balchandran"
    ],
    [
     "Vidhya",
     "Ramanujam"
    ],
    [
     "Richard J.",
     "Mammone"
    ]
   ],
   "title": "Channel estimation and normalization by coherent spectral averaging for robust speaker verification",
   "original": "e99_0755",
   "page_count": 4,
   "order": 185,
   "p1": "755",
   "pn": "758",
   "abstract": [
    "In real-world speech and speaker recognition systems, data is often recorded over commercial telephone lines. Consequently, differing transmission channels cause mismatch between training and testing conditions resulting in significant performance loss. This paper presents a new technique that uses complex spectral averaging to estimate the channel accurately. The estimated channel is used as an inverse filter for normalization. This technique being speech-in speech-out, can be used as the preprocessing stage in any automatic speech processing system. A refinement process is also presented that further improves the channel estimate. The combined technique is evaluated on a speaker verification task where the training and testing data were convolved with different telephone channels. The new technique provides excellent channel estimates and nearly restores performance back to that of clean conditions.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-183"
  },
  "magrinchagnolleau99_eurospeech": {
   "authors": [
    [
     "Ivan",
     "Magrin-Chagnolleau"
    ],
    [
     "Geoffrey",
     "Durou"
    ]
   ],
   "title": "Time-frequency principal components of speech: application to speaker identification",
   "original": "e99_0759",
   "page_count": 4,
   "order": 186,
   "p1": "759",
   "pn": "762",
   "abstract": [
    "In this paper, we propose a formalism, called vector filtering of spectral trajectories, which allows to integrate under a common formalism a lot of speech parameterization approaches. We then propose a new filtering in this framework, called time-frequency principal components (TFPC) of speech. We apply this new filtering in the framework of speaker identification, using a subset of the POLYCOST database. The results show an improvement of roughly 20 % compared to the use of the classical cepstral coefficients augmented by their coefficients.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-184"
  },
  "faundezzanuy99_eurospeech": {
   "authors": [
    [
     "Marcos",
     "Faúndez-Zanuy"
    ]
   ],
   "title": "Speaker recognition by means of a combination of linear and nonlinear predictive models",
   "original": "e99_0763",
   "page_count": 4,
   "order": 187,
   "p1": "763",
   "pn": "766",
   "abstract": [
    "This paper deals the combination of nonlinear predictive models with classical LPCC parameterization for speaker recognition. It is shown that the combination of both a measure defined over LPCC coefficients and a measure defined over predictive analysis residual signal gives rise to an improvement over the classical method that considers only the LPCC coefficients. If the residual signal is obtained from a linear prediction analysis, the improvement is 2.63% (error rate drops from 6.31% to 3.68%) and if it is computed through a nonlinear predictive neural nets based model, the improvement is 3.68%. An efficient algorithm for reducing the computational burden is also proposed.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-185"
  },
  "jang99_eurospeech": {
   "authors": [
    [
     "Gil-Jin",
     "Jang"
    ],
    [
     "Seong-Jin",
     "Yun"
    ],
    [
     "Yung-Hwan",
     "Oh"
    ]
   ],
   "title": "Feature vector transformation using independent component analysis and its application to speaker identification",
   "original": "e99_0767",
   "page_count": 4,
   "order": 188,
   "p1": "767",
   "pn": "770",
   "abstract": [
    "This paper presents a feature parameter transformation method using ICA (independent component analysis) for text independent speaker identification of telephone speech. ICA is a sig-nal processing technique which can separate linearly mixed sig-nals into statistically independent signals. The proposed method transforms them into new vectors using ICA assuming that the cepstrum vectors of the telephone speech collected from various kinds of channel conditions are linear combinations of some characteristic functions with random noise added. The performance of the proposed method was compared to the original cepstrum for the HMM-based speaker identification system. Experiments were done in equal and different channel conditions on SPIDRE, a real telephone speech database for text independent speaker identification. The identification rates increased from about 1~13% most cases, so it was confirmed that the proposed method is effective for speaker identification systems, and more effective in adverse environments.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-186"
  },
  "lavner99_eurospeech": {
   "authors": [
    [
     "Yizhar",
     "Lavner"
    ],
    [
     "Judith",
     "Rosenhouse"
    ],
    [
     "Isak",
     "Gath"
    ]
   ],
   "title": "The prototype model in speaker identification",
   "original": "e99_0771",
   "page_count": 4,
   "order": 189,
   "p1": "771",
   "pn": "774",
   "abstract": [
    "Little is known on the perceptual processes of speaker identification and its relations to the acoustic features of the speakers voice. A study of speaker perception and identification by psycho-acoustic experiments was carried out. Statistical analysis of the results suggests that the prototype model is appropriate for explaining the process of speaker identification. It has been also found that the most important features for speaker identification were the fundamental frequency, the third and fourth formants and the closing phase of the glottal wave. For different listeners, different sets of features were found to be significant for coding speaker identity.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-187"
  },
  "lo99_eurospeech": {
   "authors": [
    [
     "T. F.",
     "Lo"
    ],
    [
     "M. W.",
     "Mak"
    ],
    [
     "K. K.",
     "Yiu"
    ]
   ],
   "title": "A new cepstrum-based channel compensation method for speaker verification",
   "original": "e99_0775",
   "page_count": 4,
   "order": 190,
   "p1": "775",
   "pn": "778",
   "abstract": [
    "A new cepstrum-based channel compensation method is proposed for speaker verification over the telephone network. The method consists of intra-frame and inter-frame cepstral processing. For the former, a pole-removed cepstrum is derived, where the LP poles with frequency higher than a certain threshold are removed. For the latter, we introduce a novel way of cepstral mean subtraction called differential-partial cepstral mean subtraction (DPCMS). The main idea is that the cepstral mean of clean speech is not necessarily zero and that the cepstral difference between clean and channel-corrupted speech is mainly contributed by the channel effects on LP poles within a certain frequency range. A speaker verification system based on radial basis function networks was used to evaluate the proposed approach. Clean speech was used to train the networks and telephone speech was used to evaluate their performance. Experimental results show that the proposed method reduces verification error rate significantly.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-188"
  },
  "miyajima99_eurospeech": {
   "authors": [
    [
     "Chiyomi",
     "Miyajima"
    ],
    [
     "Hideyuki",
     "Watanabe"
    ],
    [
     "Tadashi",
     "Kitamura"
    ],
    [
     "Shigeru",
     "Katagiri"
    ]
   ],
   "title": "Speaker recognition based on discriminative feature extraction - optimization of mel-cepstral features using second-order all-pass warping function",
   "original": "e99_0779",
   "page_count": 4,
   "order": 191,
   "p1": "779",
   "pn": "782",
   "abstract": [
    "This paper describes a new framework for designing speaker recognition systems based on the discriminative feature extraction (DFE) method. We apply a mel-cepstral estimation technique to the feature extractor in a Gaussian mixture model (GMM)­based text­independent speaker identification system. The mel­cepstral estimation technique uses the second­order all­pass warping function for frequency transformation. We jointly optimize the frequency warping parameters of the feature extractor and the GMM parameters of the classifier based on a minimum classification error (MCE) criterion. Experimental results show that the frequency warped scale after optimization is different from traditional linear/mel scales; moreover, the proposed system outperforms conventional systems trained with the generalized probabilistic descent (GPD) method in which only the classifier is optimized.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-189"
  },
  "ortegagarcia99_eurospeech": {
   "authors": [
    [
     "Javier",
     "Ortega-Garcia"
    ],
    [
     "Santiago",
     "Cruz-Llanas"
    ],
    [
     "Joaquin",
     "Gonzalez-Rodriguez"
    ]
   ],
   "title": "Facing severe channel variability in forensic speaker verification conditions",
   "original": "e99_0783",
   "page_count": 4,
   "order": 192,
   "p1": "783",
   "pn": "786",
   "abstract": [
    "It is becoming increasingly usual to find audio physical traces (telephone calls, recorded tapes, security surveillance recordings, etc.) while committing crimes, forcing in consequence speech research community to find reliable methods that allow the association of an unknown voice sample with a determined person identity. Regarding speech variability in forensic approaches, some of these variability sources highly degrade the speaker verification process, namely: channel influence, inter-session variability and emotional state. In this contribution, channel and inter-session variability will be explored in order to accomplish real automatic systems for forensic speaker recognition.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-190"
  },
  "quatieri99_eurospeech": {
   "authors": [
    [
     "Thomas F.",
     "Quatieri"
    ],
    [
     "E.",
     "Singer"
    ],
    [
     "R. B.",
     "Dunn"
    ],
    [
     "Douglas A.",
     "Reynolds"
    ],
    [
     "J. P.",
     "Campbell"
    ]
   ],
   "title": "Speaker and language recognition using speech codec parameters",
   "original": "e99_0787",
   "page_count": 4,
   "order": 193,
   "p1": "787",
   "pn": "790",
   "abstract": [
    "In this paper, we investigate the effect of speech coding on speaker and language recognition tasks. Three coders were selected to cover a wide range of quality and bit rates: GSM at 12.2 kb/s, G.729 at 8 kb/s, and G.723.1 at 5.3 kb/s. Our objective is to measure recognition performance from either the synthesized speech or directly from the coder parameters themselves. We show that using speech synthesized from the three codecs, GMM-based speaker verification and phone-based language recognition performance generally degrades with coder bit rate, i.e., from GSM to G.729 to G.723.1, relative to an uncoded baseline. In addition, speaker verification for all codecs shows a performance decrease as the degree of mismatch between training and testing conditions increases, while language recognition exhibited no decrease in performance. We also present initial results in determining the relative importance of codec system components in their direct use for recognition tasks. For the G.729 codec, it is shown that removal of the postfilter in the decoder helps speaker verification performance under the mismatched condition. On the other hand, with use of G.729 LSF-based mel-cepstra, performance decreases under all conditions, indicating the need for a residual contribution to the feature representation.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-191"
  },
  "ramanujam99_eurospeech": {
   "authors": [
    [
     "Vidhya",
     "Ramanujam"
    ],
    [
     "Rajesh",
     "Balchandran"
    ],
    [
     "Richard J.",
     "Mammone"
    ]
   ],
   "title": "Robust speaker verification in noisy conditions by modification of spectral time trajectories",
   "original": "e99_0791",
   "page_count": 4,
   "order": 194,
   "p1": "791",
   "pn": "794",
   "abstract": [
    "Real-world speech and speaker recognition systems are often subject to ambient noise which results in significant performance loss, more so when the noise types and noise levels are different between training and testing. This paper presents a new preprocessing technique, Coherent Spectral Modification, that aims to reduce distortion due to noise by modifying the complex speech spectrum using information from non-speech regions of the spectral time trajectories. A refinement process is also proposed that further reduces noise mismatch. This combined technique was evaluated on a speaker verification task where the test data was corrupted with varying levels of white noise and pink noise. The new method yielded significant reduction in error rates and performed better than conventional spectral subtraction, particularly at moderate SNRs.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-192"
  },
  "vergin99_eurospeech": {
   "authors": [
    [
     "Rivarol",
     "Vergin"
    ],
    [
     "Douglas",
     "O'Shaughnessy"
    ],
    [
     "Pierre",
     "Dumouchel"
    ]
   ],
   "title": "Toward parametric representation of speech for speaker recognition systems",
   "original": "e99_0795",
   "page_count": 4,
   "order": 195,
   "p1": "795",
   "pn": "798",
   "abstract": [
    "The front-end used in many speaker recognition systems extracts, from the input speech signal, a set of coeficients based on a mel-cepstrum technique. This paper addresses the problem of efficiency of melcepstrum coeficients in a speaker recognition system and suggests a technique permitting an appropriate choice of these coeficients. It is shown, by the results obtained, that this technique can significantly increase the performance of a speaker recognition system.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-193"
  },
  "zilca99_eurospeech": {
   "authors": [
    [
     "R. D.",
     "Zilca"
    ],
    [
     "Y.",
     "Bistritz"
    ]
   ],
   "title": "Text independent speaker identification using LSP codebook speaker models and linear discriminant functions",
   "original": "e99_0799",
   "page_count": 4,
   "order": 196,
   "p1": "799",
   "pn": "802",
   "abstract": [
    "The popularity of Line Spectra Pairs (LSP) in speech processing has been supported recently by theoretical studies of their statistical properties. It has been shown that LSP frequencies are uncorrelated, and have a diagonal sensitivity matrix with respect to spectral distortion. Therefore, LSP is suitable for Vector Quantization (VQ) schemes with simple weighted Euclidean distance measures. This was further supported by an analysis of the LSP probability density function, shown to be appropriate for distance-based recognition frameworks. This paper reports our study on developing improved methods for using LSP in VQ based speaker recognition. We used Linear Discriminant Analysis (LDA) to explore the speaker-discrimination statistics of LSP, and to transform them into a speaker-discriminative space. Further enhancements include the use of special VQ distance measures such as F-ratio weighting and Inverse Harmonic Measure (IHM). Performance evaluation experiments were conducted on very short speech sessions, using a database of 32 male speakers, taken from the TIMIT and NTIMIT. Identification results of 100% for clean speech and 70% for telephone speech were achieved.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-194"
  },
  "che99_eurospeech": {
   "authors": [
    [
     "Chiwei",
     "Che"
    ],
    [
     "Nick",
     "Wang"
    ],
    [
     "Max",
     "Huang"
    ],
    [
     "Hank",
     "Huang"
    ],
    [
     "Frank",
     "Seide"
    ]
   ],
   "title": "Development of the philips 1999 taiwan Mandarin benchmark system",
   "original": "e99_0803",
   "page_count": 4,
   "order": 197,
   "p1": "803",
   "pn": "806",
   "abstract": [
    "This paper describes the Philips Large Vocabulary Continuous Mandarin speech recognition system for the 1999 Taiwan benchmark. The basic system architecture is based on the Philips LVCSR technology developed for Western languages. However, several modifications are made in order to better suitted processing Chinese spoken languages. In the paper, we present some experimental results on the two tasks we participated in this benchmark: digit and continuous syllable. For the development set, we were able to obtain a digit/syllable error rate of 2.9%/23.9%. At the final evaluation, our system achieves the lowest error rate of 3.1%/24.3% among all participating sites.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-195"
  },
  "ljolje99_eurospeech": {
   "authors": [
    [
     "Andrej",
     "Ljolje"
    ],
    [
     "Michael D.",
     "Riley"
    ],
    [
     "Donald M.",
     "Hindle"
    ]
   ],
   "title": "The AT&t large vocabulary conversational speech recognition system",
   "original": "e99_0807",
   "page_count": 4,
   "order": 198,
   "p1": "807",
   "pn": "810",
   "abstract": [
    "We describe the AT&T recognition system used in the DARPA Large Vocabulary Conversational Speech Recognition (LVCSR-98) evaluation. It is based on multi-pass rescoring of weighted Finite State Machines (FSMs) using progressively more accu-rate acoustic models. Acoustic models used in the system are all gender independent. They are based on three state context-dependent hidden Markov models using Gaussian mixtures. The recognition paradigm uses the baseline system to generate a set of word lattices. Subsequent passes use Vocal Tract Normaliza-tion (VTN), Maximum Likelihood Linear Regression (MLLR) adaptation and ROVER to further refine the recognition output. All the acoustic models (except for one of the additional models used in the ROVER experiments) employed models of alterna-tive pronunciations to improve recognition performance. The overall recognition word error rate on the LVCSR-98 evaluation set was 44.1 %.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-196"
  },
  "mohri99_eurospeech": {
   "authors": [
    [
     "Mehryar",
     "Mohri"
    ],
    [
     "Michael",
     "Riley"
    ]
   ],
   "title": "Integrated context-dependent networks in very large vocabulary speech recognition",
   "original": "e99_0811",
   "page_count": 4,
   "order": 199,
   "p1": "811",
   "pn": "814",
   "abstract": [
    "All the components used in the search stage of speech recognition systems  language model, pronunciation dictionary, context-dependent network, HMM model  can be represented by finitestate labeled networks. To construct real-time recog-nition systems, it is important to optimize these networks and to efficiently combine them. We present newmethods that substantially improve these steps. We show that an efficient recognition network including context-dependent and HMM models can be built using weighted determinization of transducers [6]. We report experiments with a 463,331-word vocabulary North American Business News Task that show a substantial improvement of the recognition speed over our previous method [9]. Further-more, the size of the integrated context-dependentnetworks constructed can be dramatically reduced using a factoring algorithm that we briefly describe. With our construction, the integrated NAB network contains only about 1:3 times as many arcs as the language model it is constructed from.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-197"
  },
  "reichert99_eurospeech": {
   "authors": [
    [
     "J.",
     "Reichert"
    ],
    [
     "Tanja",
     "Schultz"
    ],
    [
     "Alex",
     "Waibel"
    ]
   ],
   "title": "Mandarin large vocabulary speech recognition using the globalphone database",
   "original": "e99_0815",
   "page_count": 4,
   "order": 200,
   "p1": "815",
   "pn": "818",
   "abstract": [
    "This paper presents our recent efforts in developing a speaker independent LVCSR engine for Mandarin Chinese using our multilingual database GlobalPhone. We describe a two pass approach, in which the recognition first generates phoneme hypotheses and second transform these into Chinese character hypotheses. We show how this approach can reduce complexity and increase flexibility. We evaluate and compare different systems including different base units for speech recognition as phoneme units versus syllables. Furthermore we analyze the influence of tonal information. Our currently bestsystem shows very promising results achieving 15.0 % character error rate.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-198"
  },
  "zheng99_eurospeech": {
   "authors": [
    [
     "Fang",
     "Zheng"
    ],
    [
     "Zhanjiang",
     "Song"
    ],
    [
     "Mingxing",
     "Xu"
    ],
    [
     "Jian",
     "Wu"
    ],
    [
     "Yinfei",
     "Huang"
    ],
    [
     "Wenhu",
     "Wu"
    ],
    [
     "Cheng",
     "Bi"
    ]
   ],
   "title": "Easytalk: a large-vocabulary speaker-independent Chinese dictation machine",
   "original": "e99_0819",
   "page_count": 4,
   "order": 201,
   "p1": "819",
   "pn": "822",
   "abstract": [
    "The EasyTalk application is a large-vocabulary speaker-independent continuous Chinese speech recognition system, i.e. Chinese dictation machine (CDM), under the WINTEL environment. Addressed in this paper are a number of novel techniques adopted in the CDM engine which is the basis of EasyTalk, including the merging-based syllable detection automaton (MBSDA) and the statistical knowledge based frame synchronous search (SKB-FSS) algorithms in the acoustic processing stage, the percentage in critical area (CAP) and recognition score gap (RSG) methods for the acceptation and rejection decision, the word search tree (WST), the N-Gram, and the syllable synchronous network search (SSNS) algorithm in the language processing stage, the embedded multiple model sheme (EMM) and the fuzzy syllable set (FSS) for the robustness purpose.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-199"
  },
  "fitt99_eurospeech": {
   "authors": [
    [
     "Susan",
     "Fitt"
    ],
    [
     "Stephen",
     "Isard"
    ]
   ],
   "title": "Synthesis of regional English using a keyword lexicon",
   "original": "e99_0823",
   "page_count": 4,
   "order": 202,
   "p1": "823",
   "pn": "826",
   "abstract": [
    "We discuss the use of an accent-independent keyword lexicon to synthesise speakers with different regional accents. The paper describes the system architecture and the transcription system used in the lexicon, and then focuses on the construction of word-lists for recording speakers. We illustrate by mentioning some of the features of Scottish and Irish English, which we are currently synthesising, and describe how these are captured by keyword synthesis.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-200"
  },
  "maeda99_eurospeech": {
   "authors": [
    [
     "Noriyasu",
     "Maeda"
    ],
    [
     "Banno",
     "Hideki"
    ],
    [
     "Shoji",
     "Kajita"
    ],
    [
     "Kazuya",
     "Takeda"
    ],
    [
     "Fumitada",
     "Itakura"
    ]
   ],
   "title": "Speaker conversion through non-linear frequency warping of straight spectrum",
   "original": "e99_0827",
   "page_count": 4,
   "order": 203,
   "p1": "827",
   "pn": "830",
   "abstract": [
    "A parametric conversion of speech individuality is proposed based on STRAIGHT speech representation. STRAIGHT speech analysis-synthesis can produce high quality speech for various kinds of transformations by using 1) pitch synchronous windowing, 2) time-frequency spectrum interpolating and 3) randomized all-pass filtering for shaping phase spectrum. In order to utilize the smoothness of STRAIGHT spectrum, speech conversion is accomplished by warping the frequency axis. The warping functions are trained for each class of the predetermined spectrum shape grouping. The evaluation test is performed to compare the proposed method and VQ prototype mapping or linear transformation of cepstrum vectors. As a measure of converted speech quality, the MOS score of 6 subjects is calculated and is found to be better than conventional methods by about 1.5 point without degrading the accuracy of speech individuality discrimination.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-201"
  },
  "mcinnes99_eurospeech": {
   "authors": [
    [
     "F. R.",
     "McInnes"
    ],
    [
     "D. J.",
     "Attwater"
    ],
    [
     "Michael D.",
     "Edgington"
    ],
    [
     "Mark S.",
     "Schmidt"
    ],
    [
     "Mervyn A.",
     "Jack"
    ]
   ],
   "title": "User attitudes to concatenated natural speech and text-to-speech synthesis in an automated information service",
   "original": "e99_0831",
   "page_count": 4,
   "order": 204,
   "p1": "831",
   "pn": "834",
   "abstract": [
    "Todays automated telephone services generally use recorded speech from one speaker for all output. In applications with large and varying output vocabularies, such as place names, it may be necessary to employ a second speaker to provide new vocabulary items if the original speaker is not available, or to use text-tospeech (TTS) synthesis for the whole or parts of the output. This paper reports a comparison of 10 schemes for the generation of spoken output in a travel information service, ranging from natural speech from a single speaker, through combinations of different voices and of natural and synthetic speech, to TTS synthesis throughout. The results show strong preferences for concatenated speech over TTS and for quality recordings over amateur ones, and a weaker preference for a single speaker over two speakers.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-202"
  },
  "traber99_eurospeech": {
   "authors": [
    [
     "Christof",
     "Traber"
    ],
    [
     "Karl",
     "Huber"
    ],
    [
     "Karim",
     "Nedir"
    ],
    [
     "Beat",
     "Pfister"
    ],
    [
     "Eric",
     "Keller"
    ],
    [
     "Brigitte",
     "Zellner"
    ]
   ],
   "title": "From multilingual to polyglot speech synthesis",
   "original": "e99_0835",
   "page_count": 4,
   "order": 205,
   "p1": "835",
   "pn": "838",
   "abstract": [
    "This paper proposes a distinction between existing multilingual synthesis systems and mixed-lingual or polyglot synthesis systems. The latter should be capable of synthesising with the same voice utterances which contain foreign language words or word groups. As a first step towards polyglot synthetic speech, the design and realisation of a 4-lingual single-speaker diphone inventory is detailed. The first results show that mixed-lingual sentences can be synthesised using this inventory. Further work will focus on multilingual text analysis and prosodic modelling in order to create a complete polyglot TTS system.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-203"
  },
  "tanaka99_eurospeech": {
   "authors": [
    [
     "Kimihito",
     "Tanaka"
    ],
    [
     "Hideyuki",
     "Mizuno"
    ],
    [
     "Masanobu",
     "Abe"
    ],
    [
     "Shin'ya",
     "Nakajima"
    ]
   ],
   "title": "A Japanese text-to-speech system based on multi-form units with consideration of frequency distribution in Japanese",
   "original": "e99_0839",
   "page_count": 4,
   "order": 206,
   "p1": "839",
   "pn": "842",
   "abstract": [
    "This paper proposes our new text-to-speech (TTS) system that concatenates large numbers of speech segments to produce very natural and intelligible synthetic speech. One novel point of our system is its new synthesis unit, which is has three remarkable characteristics as follows; - The synthesis units contain all Japanese syllables together with all possible vowel sequences, so very smooth synthetic speech is produced. - Both previous and succeeding phoneme environments are considered when speech segments are concatenated, so natural sounding transients from a vowel to a consonant, which is the only concatenation point with the proposed unit, are present in the synthetic speech. - Each unit has various fundamental frequency (F0 ) contours. Therefore, F0 modification rates are very small in any synthesis event, and the F0 modification process causes only minor distortion. To develop a unit database efficiently and effectively, we analyzed 4,850,000 Japanese phrases (breath-group) containing 87,810,000 phonemes and ranked them in order of appearance frequency. Listening tests confirm the high intelligibility and naturalness of speech produced by our new TTS system. It uses the 50,000 highest frequency units that cover over 77% of Japanese texts.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-204"
  },
  "deville99_eurospeech": {
   "authors": [
    [
     "G.",
     "Deville"
    ],
    [
     "O.",
     "Deroo"
    ],
    [
     "Henri",
     "Leich"
    ],
    [
     "S.",
     "Gielen"
    ],
    [
     "J.",
     "Vanparys"
    ]
   ],
   "title": "Automatic detection and correction of pronunciation errors for foreign language learners: the demosthenes application",
   "original": "e99_0843",
   "page_count": 4,
   "order": 207,
   "p1": "843",
   "pn": "846",
   "abstract": [
    "This paper accounts for the DEMOSTHENES application, an interactive tool for the correction of foreign language learners' pronunciation. After giving the didactic and technical arguments advocating for DEMOSTHENES, the authors describe the phase of creation and labelling of a sound database for the application language (Dutch). The methodolgy for the training of the recognizer is then discussed, as well as the pronunciation scoring paradigm. To conclude, the final application is described, together with preliminary but promising results.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-205"
  },
  "eskenazi99_eurospeech": {
   "authors": [
    [
     "Maxine",
     "Eskenazi"
    ],
    [
     "Scott",
     "Hansma"
    ],
    [
     "John",
     "Corwin"
    ],
    [
     "Jordi",
     "Albornoz"
    ]
   ],
   "title": "User adaptation in the fluency pronunciation trainer",
   "original": "e99_0847",
   "page_count": 4,
   "order": 208,
   "p1": "847",
   "pn": "850",
   "abstract": [
    "Adaptation to the user is of great importance in a language training system since the user, making unfamiliar, socially unacceptable sounds, must be made to feel self-confident in order to obtain best results. The Fluency project aims at creating a pronunciation trainer for foreign language learning. It uses automatic speech recognition (CMUs SPHINX II) and is guided by basic principles in foreign language learning research. We describe efforts in this project to make the interface adaptable concerning learning strategies and \"golden voices\".\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-206"
  },
  "franco99_eurospeech": {
   "authors": [
    [
     "Horacio",
     "Franco"
    ],
    [
     "Leonardo",
     "Neumeyer"
    ],
    [
     "María",
     "Ramos"
    ],
    [
     "Harry",
     "Bratt"
    ]
   ],
   "title": "Automatic detection of phone-level mispronunciation for language learning",
   "original": "e99_0851",
   "page_count": 4,
   "order": 209,
   "p1": "851",
   "pn": "854",
   "abstract": [
    "We are interested in automatically detecting specific phone seg-ments that have been mispronounced by a nonnative student of a foreign language. The phone-level information allows a language instruction system to provide the student with feedback about specific pronunciation mistakes. Two approaches were evaluated; in the first approach, log-posterior probability-based scores [1] are computed for each phone segment. These probabilities are based on acoustic models of native speech. The sec-ond approach uses a phonetically labeled nonnative speech database to train two different acoustic models for each phone: one model is trained with the acceptable, or correct native-like pronunciations, while the other model is trained with the incorrect, strongly nonnative pronunciations. For each phone seg-ment, a log-likelihood ratio score is computed using the incorrect and correct pronunciation models. Either type of score is compared with a phone dependent threshold to detect a mispronunciation. Performance of both approaches was evaluated in a phonetically transcribed database of 130,000 phones uttered in continuous speech sentences by 206 nonnative speakers.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-207"
  },
  "herron99_eurospeech": {
   "authors": [
    [
     "Daniel",
     "Herron"
    ],
    [
     "Wolfgang",
     "Menzel"
    ],
    [
     "Eric",
     "Atwell"
    ],
    [
     "Roberto",
     "Bisiani"
    ],
    [
     "Fabio",
     "Daneluzzi"
    ],
    [
     "Rachel",
     "Morton"
    ],
    [
     "Juergen A.",
     "Schmidt"
    ]
   ],
   "title": "Automatic localization and diagnosis of pronunciation errors for second-language learners of English",
   "original": "e99_0855",
   "page_count": 4,
   "order": 210,
   "p1": "855",
   "pn": "858",
   "abstract": [
    "An automatic system for detection of pronunciation errors by adult learners of English is embedded in a language learning package. Four main features are: (1) a recognizer robust to nonnative speech; (2) localization of phone and wordlevel errors; (3) diagnosis of what sorts of phonelevel errors took place; and (4) a lexical stress detector. These tools together allow robust, consistent, and specific feedback on pronunciation errors, unlike many previous systems that provide feedback only at a more general level. The diagnosis technique searches for errors expected based on the students mother tongue and uses a separate bias for each error in order to maintain a particular desired global false alarm rate. Results are presented here for nonnative recognition on tasks of differing complexity and for diagnosis, based on a data set of artificial errors, showing that this method can detect many contrasts with a high hit rate and a low false alarm rate.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-208"
  },
  "vicsi99_eurospeech": {
   "authors": [
    [
     "Klára",
     "Vicsi"
    ],
    [
     "Peter",
     "Roach"
    ],
    [
     "Annemarie",
     "Öster"
    ],
    [
     "Zdravko",
     "Kacic"
    ],
    [
     "P.",
     "Barczikay"
    ],
    [
     "I.",
     "Sinka"
    ]
   ],
   "title": "SPECO - a multimedia multilingual teaching and training system for speech handicapped children",
   "original": "e99_0859",
   "page_count": 4,
   "order": 211,
   "p1": "859",
   "pn": "862",
   "abstract": [
    "In the frame of the INCO-Copernicus program of European Commission we have started to develop an audio-visual pronunciation teaching and training method and software system for hearing and speech-handicapped persons to help them to control their speech production. A teaching method is drawn up for progression from the individual sound preparation to practice of the sounds in sentences. The main aim is to develop an audio-visual articulation training and teaching system for all participant languages, these being English, Swedish, Slovenian and Hungarian. The basic part is a general language-independent measuring system and database editor. This database editor makes it possible to construct modules for all participant languages and for different speech disabilities. Two modules are under development for its construction in all languages, one of them being for teaching and training vowels for hearing-impaired children, while the other one is for correction of misarticulated fricative sounds.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-209"
  },
  "ahadi99_eurospeech": {
   "authors": [
    [
     "S. M.",
     "Ahadi"
    ]
   ],
   "title": "Recognition of continuous persian speech using a medium-sized vocabulary speech corpus",
   "original": "e99_0863",
   "page_count": 4,
   "order": 212,
   "p1": "863",
   "pn": "866",
   "abstract": [
    "Speech recognition in Persian (Farsi) has recently been addressed by a few native speaking researchers and some approaches to isolated word and phoneme recognition have been reported. A main bottleneck in this research field is the lack of a recognition-specific speech corpus. In this work, a phonetically balanced speech database of Persian has been modified and used in continuous speech recognition. A basic continuous speech recognizer using HMMs has been designed for this language and recognition tests have been performed. Using mixture-Gaussian monophone models, a word recognition rate of about 68% in no-grammar tests were obtained while word-pair grammar tests increased this rate to an unexpectedly high value of 99.5%. The reason is found to be the low grammar perplexity of the database which is not suitable for recognition applications. This obviates the need for a Persian speech corpus specifically designed for such tasks.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-210"
  },
  "fegyo99_eurospeech": {
   "authors": [
    [
     "Tibor",
     "Fegyó"
    ],
    [
     "Péter",
     "Tatai"
    ]
   ],
   "title": "Multi-lingual speech recognition based on demi-syllable subword units",
   "original": "e99_0867",
   "page_count": 5,
   "order": 213,
   "p1": "867",
   "pn": "870",
   "abstract": [
    "Hungarian, unlike English, is an agglutinating language, so new, special methods are needed for speech recognition. The word dictionary could become very large due to its complex morphological system, so a suitable approach could be to use subword units as, eg., half (demi) syllables and language models. In this way most of the natural languages can be described, therefore this method can be applied in a multi lingual recognition system. We describe in the article a method to represent the demi-syllable segments efficiently. With this method the computation of the distortion measurement becomes fast. Our recognition results in speaker dependent case proved that the chosen method performs slightly better than DTW and requires sharply less computation time.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-211"
  },
  "fung99_eurospeech": {
   "authors": [
    [
     "Pascale",
     "Fung"
    ],
    [
     "Chi Yuen",
     "Ma"
    ],
    [
     "Wai Kat",
     "Liu"
    ]
   ],
   "title": "MAP-based cross-language adaptation augmented by linguistic knowledge: from English to Chinese",
   "original": "e99_0871",
   "page_count": 4,
   "order": 214,
   "p1": "871",
   "pn": "874",
   "abstract": [
    "Construction of a recognizer in a new target language usually involves collection of a comprehensive database in that language as well as manual annotation and model training. For rapid development of new language recognizer, we propose (1) substituting target languagephoneme models by source language phoneme models trained previously; and (2) adapting target language phoneme models from source language phoneme mod-els using maximum a posteriori method. Evaluations show that adaptation method results in an error reduction rate of 51% from using substitution, and an error reduction rate of 10% from language-dependent training. We also propose using data-driven method with linguistic knowledge, instead of heuristic rules, to align source language phonemes to target language phonemes, both for substitution and adaptation. Evalution shows that using linguistic rules brings a 2% in-crease in recognition rate.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-212"
  },
  "grocholewski99_eurospeech": {
   "authors": [
    [
     "Stefan",
     "Grocholewski"
    ]
   ],
   "title": "Analysis of HMM models in alphabet letters recognition",
   "original": "e99_0875",
   "page_count": 4,
   "order": 215,
   "p1": "875",
   "pn": "878",
   "abstract": [
    "In the paper we consider the problems concerning the speaker independent alphabet letters recognition by using the common CDHMMs along with standard cepstral feature vectors. The propositions of new phonetically motivated features are discussed.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-213"
  },
  "hirose99_eurospeech": {
   "authors": [
    [
     "Keikichi",
     "Hirose"
    ],
    [
     "Jin-song",
     "Zhang"
    ]
   ],
   "title": "Tone recognition of Chinese continuous speech using tone critical segments",
   "original": "e99_0879",
   "page_count": 4,
   "order": 216,
   "p1": "879",
   "pn": "882",
   "abstract": [
    "This paper presents our approach to automatically detect tone nuclei, and to use their features for recognizing lexical tones of Chinese continuous speech. We have suggested that Fundamental frequency (F0) contour of a syllable usually consists of three segments: onset course, tone nucleus and offset course. Among them, only tone nucleus contains key features for tone discrimination, hence the tone critical segment of a syllable. The other two segments result from physiological transition effect of human vocal cords, and are affected largely by adjacent tones in continuous speech. The tone nucleus can be detected out by a two-process scheme; the first process segments a syllable F0 contour by Segmental Clustering algorithm, and the second one finds tone nucleus according to knowledge rules on supra-segmental features. Tone recognition performance can be improved by using tone nucleus features and discarding others. Tone recognition experimental results proved the advantage of our method over the conventional ones.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-214"
  },
  "ho99_eurospeech": {
   "authors": [
    [
     "Tai-Hsuan",
     "Ho"
    ],
    [
     "Chin-Jung",
     "Liu"
    ],
    [
     "Herman",
     "Sun"
    ],
    [
     "Ming-Yi",
     "Tsai"
    ],
    [
     "Lin-Shan",
     "Lee"
    ]
   ],
   "title": "Phonetic state tied-mixture tone modeling for large vocabulary continuous Mandarin speech recognition",
   "original": "e99_0883",
   "page_count": 4,
   "order": 217,
   "p1": "883",
   "pn": "886",
   "abstract": [
    "This paper presents a new approach to tone modeling for continuous Mandarin speech recognition. Mandarin tones provide rich information for speech recognition. In this paper, we treat the tone as an attribute of the final vowel part of a Mandarin syllable. Separate distributions are estimated for cepstral coefficients and pitch features respectively, and the phonetic state tied-mixture technique is exploited to achieve improved modeling. Several tying structures are investigated, and the results are compared with that without using tonal parameters. After integrating tone models, decent improvements can be achieved in large vocabulary continuous Mandarin speech recognition. Besides, this approach can be easily incorporated into the one-pass Viterbi search framework for practical implementation of Mandarin dictation system.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-215"
  },
  "imperl99_eurospeech": {
   "authors": [
    [
     "Bojan",
     "Imperl"
    ],
    [
     "Bogomir",
     "Horvat"
    ]
   ],
   "title": "The clustering algorithm for the definition of multilingual set of context dependent speech models",
   "original": "e99_0887",
   "page_count": 4,
   "order": 218,
   "p1": "887",
   "pn": "890",
   "abstract": [
    "The paper addresses the problem of designing a language independent phonetic inventory for the speech recognisers with multilingual vocabulary. A new clustering algorithm for the definition of multilingual set of triphones is proposed. The clustering algorithm bases on a definition of a distance measure for triphones defined as a weighted sum of explicit estimates of the context similarity on a monophone level. The monophone similarity estimation method based on the algorithm of Houtgast. The clustering algorithm is integrated in a multilingual speech recognition system based on HTK V2.1.1. The ongoing experiments are based on the SpeechDat II databases 1 . So far, experiments included the Slovenian, Spanish and German 1000 FDB SpeechDat (II) database. Current results are very promising. The use of clustering algorithm resulted in a significant reduction of the number of triphones at acceptable level of word and language identification accuracy degradation.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-216"
  },
  "liu99b_eurospeech": {
   "authors": [
    [
     "Jian",
     "Liu"
    ],
    [
     "Xiaodong",
     "He"
    ],
    [
     "Fuyuan",
     "Mo"
    ],
    [
     "Tiecheng",
     "Yu"
    ]
   ],
   "title": "Study on tone classification of Chinese continuous speech in speech recognition system",
   "original": "e99_0891",
   "page_count": 4,
   "order": 219,
   "p1": "891",
   "pn": "894",
   "abstract": [
    "In this paper, we first introduce the use of Gaussian mixture models (GMM) for Chinese tone classification in continuous speech. Then, we explain how to integrate it with the HMM-based speech recognition system. Finally, we provide the tone classification accuracy of this probabilistic method which is tested with Chinese continuous speech database of national 863\" project.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-217"
  },
  "liu99c_eurospeech": {
   "authors": [
    [
     "Yi",
     "Liu"
    ],
    [
     "Pascale",
     "Fung"
    ]
   ],
   "title": "Decision tree-based triphones are robust and practical for mandarian speech recognition",
   "original": "e99_0895",
   "page_count": 4,
   "order": 220,
   "p1": "895",
   "pn": "898",
   "abstract": [
    "In large-vocabulary, speaker-independent speech recognition systems, modeling of vocabulary words by subword units is mandatory. This paper studies the use of triphone units for Mandarin speech recognition compared to biphone and context-independent phonetic units. In order to solve unseen triphones in speech recognition, decision-tree based clustering is used in triphone units. This method achieves high recognition performance with limited training data and also reduces the model training time. The robustness and effectiveness of the cross-word, tree-based triphone units have been proved by the speaker-independent continuous Mandarin speech recognition task. The training computation time reduces by about 2.3 times after tying states for triphone models, the recognition syllable accuracy increases 28.7% compared to monophone units and by 13.5% compared to biphone units.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-218"
  },
  "lopezdeipina99_eurospeech": {
   "authors": [
    [
     "K.",
     "López de Ipiña"
    ],
    [
     "A.",
     "Varona"
    ],
    [
     "I.",
     "Torres"
    ],
    [
     "L. J.",
     "Rodríguez"
    ]
   ],
   "title": "Decision trees for inter-word context dependencies in Spanish continuous speech recognition tasks",
   "original": "e99_0899",
   "page_count": 5,
   "order": 221,
   "p1": "899",
   "pn": "902",
   "abstract": [
    "Context Dependent Units are broadly used in Continuous Speech Recognition (CSR) system, being decision trees a suitable clustering technique to obtain this kind of units. This work was aimed to extend the decision tree based clustering to model inter-word context dependencies in Spanish CSR tasks. We first used a set of previously defined context dependent units to model word boundaries. A decision tree derived pair grammar was then used at decoding time to prune each network connecting pairs of words. Then, specific sets of decision tree based inner context dependent units were obtained to model word boundaries. Both approaches were experimentally evaluated and compared to classical approaches over a Spanish CSR task. Experimental results showed the potential contribution of modelling between-word contexts to CSR systems. These units were selected by decision trees and provided full coverage while keeping a suitable computational cost.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-219"
  },
  "nassar99_eurospeech": {
   "authors": [
    [
     "Amin M.",
     "Nassar"
    ],
    [
     "Nemat S.",
     "Abdel Kader"
    ],
    [
     "Amr M.",
     "Refat"
    ]
   ],
   "title": "End points detection for noisy speech using a wavelet based algorithm",
   "original": "e99_0903",
   "page_count": 4,
   "order": 222,
   "p1": "903",
   "pn": "906",
   "abstract": [
    "This paper represents a way for the detection of start-end boundaries of a speech segment in the presence of noise using wavelet transform. The technique is based on generating a certain mathematical function derived from the wavelet parameters that can keep track with the energy changes along the speech duration. The problem in end points detection always appears in words that begin or end in low-energy phonemes. This problem can be entirely eliminated by this technique. Examples are given to show how the algorithm performs under different signal-to-noise ratios.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-220"
  },
  "nieuwoudt99_eurospeech": {
   "authors": [
    [
     "C.",
     "Nieuwoudt"
    ],
    [
     "E. C.",
     "Botha"
    ]
   ],
   "title": "Adaptation of acoustic models for multilingual recognition",
   "original": "e99_0907",
   "page_count": 4,
   "order": 223,
   "p1": "907",
   "pn": "910",
   "abstract": [
    "This paper evaluates the recognition performance of a system using acoustic models transformed across language boundaries. Parameters of hidden Markov models (HMMs) trained on speaker independent English data are adapted using Afrikaans adaptation data to realise speaker dependent, multispeaker and speaker independent Afrikaans models. Adaptation is performed using maximum a posteriori probability (MAP) and maximum likelihood linear regression (MLLR) methods on context independent and context dependent phones. Results show that MLLR transformation of English models using Afrikaans adaptation data significantly improves model performance and for context dependent models achieves better performance on speaker independent tests than achievable by direct training on the adaptation data.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-221"
  },
  "uebler99_eurospeech": {
   "authors": [
    [
     "Ulla",
     "Uebler"
    ],
    [
     "Manuela",
     "Boros"
    ]
   ],
   "title": "Recognition of non-native German speech with multilingual recognizers",
   "original": "e99_0911",
   "page_count": 4,
   "order": 224,
   "p1": "911",
   "pn": "914",
   "abstract": [
    "In this study we present different approaches to the recognition of non-natives. With a corpus in German spoken by speakers with 56 different first languages, the Strange Corpus, we perform recognition experiments with monolingual and multilingual recognizers. Among other, we compared two German recognizers, one that was trained in addition with non-native (Italian) speech and the other trained with German speakers only. We found that best performance is achieved with the recognizer trained with German including non-native speech, followed by a bilingual recognizer and an Italian recognizer trained with German and Italian natives.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-222"
  },
  "altosaar99_eurospeech": {
   "authors": [
    [
     "Toomas",
     "Altosaar"
    ],
    [
     "Bruce",
     "Millar"
    ],
    [
     "Martti",
     "Vainio"
    ]
   ],
   "title": "Relational vs. object-oriented models for representing speech: a comparison using ANDOSL data",
   "original": "e99_0915",
   "page_count": 4,
   "order": 225,
   "p1": "915",
   "pn": "918",
   "abstract": [
    "Much effort has been expended by the speech community in designing and producing speech corpora and databases. However, considerably less thought has been given to the type of database model that best represents speech for optimal database access. This paper reviews an established database model, the relational database management system (RDBMS), as well as an emerging one, the object-oriented database management system (OODBMS). Both model types are compared by addressing identical queries to the same data that is separately represented under these respective systems. This is accomplished using an identical source of richly described speech data from ANDOSL (Australian National Database of Spoken Language). By identifying the strengths and weaknesses of both approaches new models can be defined that incorporate the best of both existing systems enabling more fluent and extensive extraction of knowledge from speech databases.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-223"
  },
  "draxler99_eurospeech": {
   "authors": [
    [
     "Christoph",
     "Draxler"
    ],
    [
     "Robert",
     "Grudszus"
    ],
    [
     "Stephan",
     "Euler"
    ],
    [
     "Klaus",
     "Bengler"
    ]
   ],
   "title": "First experiences of the German speechdat-car database collection in mobile environments",
   "original": "e99_0919",
   "page_count": 5,
   "order": 226,
   "p1": "919",
   "pn": "922",
   "abstract": [
    "In SpeechDat-Car, speech databases for speech driven devices and services for mobile environments are collected for nine European languages. The German SpeechDat-Car installation was the first fully equipped platform within the project. It has served as a testbed for the recording software for the entire project, and as an opportunity to perform technical and organizational feasibility tests for the German data collection. The main results are: i) drivers feel comfortable with the prompts displayed on an LCD screen attached to the dashboard, ii) the audio-visual mode of prompting allowed the elicitation of speech even under difficult traffic conditions, and iii) the original synchronization protocol could be made more efficient by 20%, so that the 129 items of one recording session can now be recorded in 45-50 minutes.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-224"
  },
  "edgington99_eurospeech": {
   "authors": [
    [
     "Mike",
     "Edgington"
    ],
    [
     "David",
     "Attwater"
    ],
    [
     "Peter",
     "Durston"
    ]
   ],
   "title": "OASIS - a framework for spoken language call steering",
   "original": "e99_0923",
   "page_count": 4,
   "order": 227,
   "p1": "923",
   "pn": "926",
   "abstract": [
    "OASIS is a research project at BT Labs investigating practical large-scale spoken language automation of call steering (call routing). BTs own operator assistance service is used as an initial trial domain. Spoken language call steering requires understanding of both users language and behaviour. Therefore, the OASIS project makes extensive use of corpus-based machine learning techniques and iterative Wizard-of-Oz simulations to verify and refine the system design. We also describe a simple four layer model of general telephone call-handling, developed to characterise services in order to identify appropriate automation technology. We argue that for many helpdesk applications, a high proportion of callers will just describe the nature of their problem, rather than refer to particular services offered. For these calls, language use is highly unpredictable, and corpus based learning techniques are essential for speech recognition and understanding.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-225"
  },
  "gegenmantel99_eurospeech": {
   "authors": [
    [
     "Eike",
     "Gegenmantel"
    ]
   ],
   "title": "VOCAPI - small standard API for command & control",
   "original": "e99_0927",
   "page_count": 4,
   "order": 228,
   "p1": "927",
   "pn": "930",
   "abstract": [
    "Some speech APIs are becoming standards to ac-cess speech functionality. They aim to be highly flexible to avoid restrictions. Consequently, corre-sponding interface specifications are extensive and implementations have high demands on the performance of processing platforms. Resulting prices are unacceptable for consumer devices like phones, car navigation, audio, TV, etc. Here, the state of the art requires low resource consumption while the range of functionality can be focused on command and control dialogs. A consortium of major companies has come together to develop and standardize a small and handy speech API, the so-called Voice Control API (VOCAPI). Being a German draft standard [1] now, VOCAPI will be proposed as European standard in the near future.\n",
    "Keywords: speech API, standardization, command and control, consumer devices, real-time platforms\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-226"
  },
  "muller99_eurospeech": {
   "authors": [
    [
     "Christel",
     "Müller"
    ],
    [
     "Karsten",
     "Schröder"
    ]
   ],
   "title": "Standardised speech interfaces - key for objective evaluation of recognition accuracy",
   "original": "e99_0931",
   "page_count": 4,
   "order": 229,
   "p1": "931",
   "pn": "934",
   "abstract": [
    "Today existing speech recognisers differ not only in their technology from single word to natural language understanding, from hardware based and software-only solutions but also in the recognition accuracy under real-time conditions in the whole variety of environments and networks. Standardised APIs for a defined state diagram of recognition process are the key requirement for integrating different technologies and their evaluation. First time evaluation tests based on standardised interfaces and optimised vocabularies were carried out in an open system environment at Deutsche Telekoms Research Lab. The approach of a unique lexicon development system enhances the whole process of vocabulary generation. Optimising the phoneme transcription is one of the main future issues at the lab.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-227"
  },
  "matsunaga99_eurospeech": {
   "authors": [
    [
     "Shoichi",
     "Matsunaga"
    ],
    [
     "Yoshiaki",
     "Noda"
    ],
    [
     "Katsutoshi",
     "Ohtsuki"
    ],
    [
     "Eiji",
     "Doi"
    ],
    [
     "Tomio",
     "Itoh"
    ]
   ],
   "title": "A medical rehabilitation diagnoses transcription method that integrates continuous and isolated word recognition",
   "original": "e99_0935",
   "page_count": 4,
   "order": 230,
   "p1": "935",
   "pn": "938",
   "abstract": [
    "This paper describes a practical dictation system that is able to compensate for the lack of a large text database and reports on the results of field tests in which the system was used to make medical rehabilitation diagnosis reports in a hospital. Our dictation system uses two recognition engines: continuous speech recognition and isolated word (including connected words) recognition engines. When a user makes a report, the two recognition engines can be selectively operated in a single window through voice commands or mouse click. The system operates on common personal computers under the Windows O.S. A field test conducted in noisy therapeutic work rooms that compared the performance of speech input system comparing to conventional keyboard input system for the medical rehabilitation field, demonstrated the effectiveness of the dictation system.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-228"
  },
  "nemeth99_eurospeech": {
   "authors": [
    [
     "Géza",
     "Németh"
    ],
    [
     "Csaba",
     "Zainkó"
    ],
    [
     "Gábor",
     "Olaszy"
    ],
    [
     "Gábor",
     "Prószéky"
    ]
   ],
   "title": "Problems of creating a flexible e-mail reader for hungarian",
   "original": "e99_0939",
   "page_count": 4,
   "order": 231,
   "p1": "939",
   "pn": "942",
   "abstract": [
    "The problems found during the development of a Hungarian e-mail reader are reported in this paper. Hungarian is special on one hand because of the use of diacritics for several vowels (á, é, í, ó, ö, õ, ú, ü, û), on the other hand because of the ag-glutinative nature of the language, which greatly increases the number of possible valid word forms. Emphasis is placed on text processing related issues, e.g. language detection, dia-critic regeneration from stripped down 7bit ASCII forms, etc. Test results for different solutions on real-life e-mail data are also presented.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-229"
  },
  "olaszy99_eurospeech": {
   "authors": [
    [
     "Gábor",
     "Olaszy"
    ],
    [
     "Géza",
     "Németh"
    ],
    [
     "Péter",
     "Olaszi"
    ],
    [
     "Géza",
     "Gordos"
    ]
   ],
   "title": "Interactive, TTS supported speech message composer for large, limited vocabulary, but open information systems",
   "original": "e99_0943",
   "page_count": 4,
   "order": 232,
   "p1": "943",
   "pn": "946",
   "abstract": [
    "In limited vocabulary speaking systems where several pre-recorded speech items are concatenated to generate the message one of the main problems is to add new items to the message set recorded formerly. This procedure is complicated, expensive and the new message(s) cannot be integrated into the original system without leaving the impression that the new items were generated during a later process. In most cases the new items have different tempo, voice timbre and fundamental frequency. Application users would like to have tools which eliminate these problems. To meet these requirements a method and a tool (for Hungarian) has been developed which gives the possibility to create limited vocabulary but open systems. Using this method new speech items can be generated (without recording) with the same voice as that of the original system, so the user will not feel that new message items are combined with the originally recorded items.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-230"
  },
  "penn99_eurospeech": {
   "authors": [
    [
     "Gerald",
     "Penn"
    ],
    [
     "Bob",
     "Carpenter"
    ]
   ],
   "title": "ALE for speech: a translation prototype",
   "original": "e99_0947",
   "page_count": 4,
   "order": 233,
   "p1": "947",
   "pn": "950",
   "abstract": [
    "In this paper, we describe The Attribute Logic Engine (ALE) and enhancements to it that enable it to serve as a complete grammatical infrastructure for applications such as spoken language translation. We indicate howALE was expanded and combined with off-the-shelf speech com-ponents to develop an application that translates English speech to German speech. The translation operates by way of a semantic representation based on typed feature structures, with information on thematic roles (\"who did what to whom\") and agreement information that can be used to guide search in less restricted domains and map expressions to more felicitous (but semantically equivalent) constructions in the target language than a more literal, surface-oriented method would admit.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-231"
  },
  "rodriguez99_eurospeech": {
   "authors": [
    [
     "L.J.",
     "Rodríguez"
    ],
    [
     "M. I.",
     "Torres"
    ],
    [
     "J. M.",
     "Alcaide"
    ],
    [
     "A.",
     "Varona"
    ],
    [
     "K.",
     "López de Ipina"
    ],
    [
     "M.",
     "Penagarikano"
    ],
    [
     "G.",
     "Bordel"
    ]
   ],
   "title": "An integrated system for Spanish CSR tasks",
   "original": "e99_0951",
   "page_count": 4,
   "order": 234,
   "p1": "951",
   "pn": "954",
   "abstract": [
    "This paper presents a new system for the continuous speech recognition of Spanish, integrating previous works in the fields of acoustic-phonetic decoding and language modelling. Acoustic and language models -separately trained with speech and text samples, respectivelyare integrated into one single automaton, and their probabilities combined according to a standard beam search procedure. Two key issues were to adequately adjust the beam parameter and the weight affecting the language model probabilities. For the implementation, a client-server arquitecture was selected, due to the desirable working scene where one or more simple machines in the client side make the speech analysis task, and a more powerful workstation in the server side looks for the best sentence hypotheses. Preliminary experimentation gives promising results with around 90% word recognition rates in a medium size word speech recognition task 1 .\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-232"
  },
  "sanderman99_eurospeech": {
   "authors": [
    [
     "Angelien",
     "Sanderman"
    ],
    [
     "Ellen",
     "Bosgoed"
    ],
    [
     "Hans de",
     "Graaff"
    ],
    [
     "Peter van",
     "Splunder"
    ]
   ],
   "title": "Use of speech synthesis in an application",
   "original": "e99_0955",
   "page_count": 4,
   "order": 235,
   "p1": "955",
   "pn": "958",
   "abstract": [
    "In this paper the use of speech synthesis in applications is investigated. It is of high importance to know if speech synthesis needs to be improved to achieve acceptable quality for applications like email reading. Also, it is important to know how user interfaces influence the use of speech synthesis in application. To gain insight into the use of speech synthesis in an application, two available email reading systems are tested. The results of an expert and user test are presented in this paper. The mean judgment of speech synthesis in these email reading applications is insufficient. The intelligibility is mostly scored as not quite sufficient. Especially, email headers and very short messages are difficult to understand. Also, it appears that the user interface influences the perception of quality of the speech synthesis.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-233"
  },
  "tamura99_eurospeech": {
   "authors": [
    [
     "Masatsune",
     "Tamura"
    ],
    [
     "Shigekazu",
     "Kondo"
    ],
    [
     "Takashi",
     "Masuko"
    ],
    [
     "Takao",
     "Kobayashi"
    ]
   ],
   "title": "Text-to-audio-visual speech synthesis based on parameter generation from HMM",
   "original": "e99_0959",
   "page_count": 4,
   "order": 236,
   "p1": "959",
   "pn": "962",
   "abstract": [
    "This paper describes a technique for synthesizing auditory speech and lip motion from an arbitrary given text. The technique is an extension of the visual speech synthesis technique based on an algorithm for parameter generation from HMM with dynamic features. Audio and visual features of each speech unit are modeled by a single HMM. Since both audio and visual parameters are generated simultaneously in a unified framework, auditory speech with synchronized lip movements can be generated automatically. We train both syllable and triphone models as the speech synthesis units, and compared their performance in text-to-audio-visual speech synthesis. Experimental results show that the generated audio-visual speech using triphone models achieved higher performance than that using syllable models.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-234"
  },
  "wouters99_eurospeech": {
   "authors": [
    [
     "Johan",
     "Wouters"
    ],
    [
     "Brian",
     "Rundle"
    ],
    [
     "Michael W.",
     "Macon"
    ]
   ],
   "title": "Authoring tools for speech synthesis using the sable markup standard",
   "original": "e99_0963",
   "page_count": 4,
   "order": 237,
   "p1": "963",
   "pn": "966",
   "abstract": [
    "In text-to-speech (TTS) synthesis, input text is automatically analyzed. This involves prediction of pronunciation, intonation, and timing at segmental and phrase level. In the design of dialog applications, developers need more control over the text-to-speech conversion. While the automatic analysis is often unsatisfactory, the developer can easily provide hints that improve the synthetic speech. The Sable markup language, which has been proposed as a standard for TTS, includes tags to indicate emphasis, speaking rate, phrase breaks, and other properties. We extend this work as follows. First, we describe a graphical editor (GUI) for Sable. An interesting challenge is to find intuitive mappings between the visual representation of the text and the attached markup properties. Next, we discuss the addition of several new markup commands and the implementation in Festival, the TTS platform we use. Finally, we describe our experiences using the authoring tools in a language training project for profoundly deaf children. The authoring tools are made freely available viahttp://cslu.cse.ogi.edu/tts.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-235"
  },
  "ariyaeeinia99_eurospeech": {
   "authors": [
    [
     "A. M.",
     "Ariyaeeinia"
    ],
    [
     "P.",
     "Sivakumaran"
    ],
    [
     "M.",
     "Pawlewski"
    ],
    [
     "M. J.",
     "Loomes"
    ]
   ],
   "title": "Dynamic weighting of the distortion sequence in text-dependent speaker verification",
   "original": "e99_0967",
   "page_count": 4,
   "order": 238,
   "p1": "967",
   "pn": "970",
   "abstract": [
    "A new dynamic weighting method for robust text-dependent speaker verification is proposed and investigated. The main attraction of the proposed approach is that it is equally effective under both uniform and non-uniform mismatched conditions. It involves estimating the mismatch associated with each feature vector in the test token and using this to weight the respective vector distortion appropriately, prior to the computation of the overall degree of dissimilarity. The experiments were conducted using a subset of the Brent speech database consisting of repetitions of isolated digit utterances zero to nine spoken by native English speakers. Based on the experimental results it is shown that the use of the proposed approach leads to a considerably higher accuracy in speaker verification than that obtainable with conventional score normalisation methods.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-236"
  },
  "altincay99_eurospeech": {
   "authors": [
    [
     "Hakan",
     "Altincay"
    ],
    [
     "Mübeccel",
     "Demirekler"
    ]
   ],
   "title": "On the use of supra model information from multiple classifiers for robust speaker identification",
   "original": "e99_0971",
   "page_count": 4,
   "order": 239,
   "p1": "971",
   "pn": "974",
   "abstract": [
    "In this paper, we propose a text-independent speaker identification (SI) scheme under uncertainty. In this scheme, extraction of supra model information about probability distributions in the feature space is proposed. Supra modeling is a model cluster-ing technique which groups the speaker models into model sets where the speakers in these sets have similar properties. The scheme uses the Dempster-Shafer (D-S) theory of evidence to combine the model sets of two classifiers which are thought to provide complementary information about the speaker identity. A dependency analysis of classifiers to be combined is presented and it is shown to be effective in avoiding wrong decisions. Ex-perimental results of the classifier combination system is given at the end of the paper.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-237"
  },
  "elmaliki99_eurospeech": {
   "authors": [
    [
     "Mounir",
     "El-Maliki"
    ],
    [
     "Andrzej",
     "Drygajlo"
    ]
   ],
   "title": "Missing features detection and handling for robust speaker verification",
   "original": "e99_0975",
   "page_count": 4,
   "order": 240,
   "p1": "975",
   "pn": "978",
   "abstract": [
    "This paper addresses the problem of robust text-independent speaker verification in the presence of missing (masked by noise) features. It presents and assesses several missing feature handling approaches. In these approaches, the speech enhancement and missing feature detection are based on the minimum mean-square error (MMSE) spectral amplitude estimator of Ephraim and Malah [1].\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-238"
  },
  "fakotakis99_eurospeech": {
   "authors": [
    [
     "Nikos",
     "Fakotakis"
    ],
    [
     "John",
     "Sirigos"
    ],
    [
     "George",
     "Kokkinakis"
    ]
   ],
   "title": "High performance text-independent speaker recognition system based on voiced/unvoiced segmentation and multiple neural nets",
   "original": "e99_0979",
   "page_count": 4,
   "order": 241,
   "p1": "979",
   "pn": "982",
   "abstract": [
    "This paper presents a text-independent speaker recognition system based on the voiced segments of the speech signal. The proposed system uses feedforward MLP classification with only a limited amount of training and testing data and gives a comparatively high accuracy. The techniques employed are: the Rasta-PLP speech analysis for parameter estimation, a feedforward MLP for voiced/unvoiced segmentation and a large number (equal to the number of speakers) of simple MLPs for the classification procedure. The system has been trained and tested using TIMIT and NTIMIT databases. The verification experiments presented a high accuracy rate: above 99% for clean speech (TIMIT) and 74.7%, for noisy speech (NTIMIT). Additional experiments were performed comparing the proposed approach of using voiced segments with only vowels and all phonetic categories with results favorable to the use of voiced segments.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-239"
  },
  "fredouille99_eurospeech": {
   "authors": [
    [
     "Corinne",
     "Fredouille"
    ],
    [
     "Jean-François",
     "Bonastre"
    ],
    [
     "Teva",
     "Merlin"
    ]
   ],
   "title": "Similarity normalization method based on world model and a posteriori probability for speaker verification",
   "original": "e99_0983",
   "page_count": 4,
   "order": 242,
   "p1": "983",
   "pn": "986",
   "abstract": [
    "For the task of speaker verification, similarity measure normalization methods are relevant to cope with variability problems and with data and/or decision fusion issues. The aim of this paper is to suggest a new normalization method, which combines classical world model-based normalization techniques with a posteriori probability-based ones. This method presents the well-known advantages of the a posteriori probability-based methods without requiring data and speaker specific processing. Here, it is experimented through a temporal-segmental, multi-recognizer speaker verification system. The results obtained on a subset of the Switchboard-Nist98 database demonstrate the ability of this method to normalize similarity measures (in probability domain) without decreasing performance. The second advantage of this method is borne out by the performance of the multi-recognizer system, which reveals that this normalization is able to make the fusion step easier without requiring any weighting function even if individual recognizer performance is dissimilar.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-240"
  },
  "isobe99_eurospeech": {
   "authors": [
    [
     "Toshihiro",
     "Isobe"
    ],
    [
     "Jun-ichi",
     "Takahashi"
    ]
   ],
   "title": "Text-independent speaker verification using virtual speaker based cohort normalization",
   "original": "e99_0987",
   "page_count": 5,
   "order": 243,
   "p1": "987",
   "pn": "990",
   "abstract": [
    "In this paper, we propose a new score normalization method for text-independent speaker verification using GMM (Gaussian Mixture Model). In the proposed method, cohort model is designed as virtual speaker model based on the similarity of local acoustic information between the reference speaker and other customers. The similarity is determined using statistical distance between model components such as the Gaussian distributions. Therefore, synthesized cohort model is statistically close to the reference speaker model, and can provide an effective normalizing score for various observed measurements. The experimental results using telephone speech of 60 speakers showed that the proposed method is superior to the typical methods with cohort speaker model or pooled model. Equal Error Rate (EER) when using common posteriori-defined threshold value for every speakers was drastically reduced from 3.82 % (for the conventional normalization with cohort speaker model) or 10.3 % (for normalization with pooled model) to 2.50 % (for the proposed method) when cohort size is equal to three.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-241"
  },
  "luettin99_eurospeech": {
   "authors": [
    [
     "J.",
     "Luettin"
    ],
    [
     "S.",
     "Ben-Yacoub"
    ]
   ],
   "title": "Robust person verification based on speech and facial images",
   "original": "e99_0991",
   "page_count": 4,
   "order": 244,
   "p1": "991",
   "pn": "994",
   "abstract": [
    "This paper describes a multi-modal person verification system using speech and frontal face images. We consider two different speaker verification algorithms, a text-independent method using a second-order statistical measure and a text-dependent method based on hidden Markov modelling, as well as a face verification technique using a robust form of corellation. Fusion of the different recognition modules is performed by a Support Vector Machine classifier. Experimental results obtained on the audio-visual database XM2VTS for individual modalities and their combinations show that multimodal systems yield better performances than individual modules for all cases.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-242"
  },
  "mathew99_eurospeech": {
   "authors": [
    [
     "M.",
     "Mathew"
    ],
    [
     "B.",
     "Yegnanarayana"
    ],
    [
     "R.",
     "Sundar"
    ]
   ],
   "title": "A neural network-based text-dependent speaker verification system using suprasegmental features",
   "original": "e99_0995",
   "page_count": 4,
   "order": 245,
   "p1": "995",
   "pn": "998",
   "abstract": [
    "In this paper, we propose two neural network-based approaches, namely, One Speaker One Network and One Speaker Multiple Networks, for text-dependent speaker verification using suprasegmental features. The suprasegmental features used for this study are pitch accent and durational features. These features are extracted using properties of intonation patterns and duration. We have proposed an approach to combine evidence present at the segmental and suprasegmental levels to improve the performance of the verification system.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-243"
  },
  "pelecanos99_eurospeech": {
   "authors": [
    [
     "Jason",
     "Pelecanos"
    ],
    [
     "Sridha",
     "Sridharan"
    ]
   ],
   "title": "Modelling output probability distributions for enhancing speaker recognition",
   "original": "e99_0999",
   "page_count": 4,
   "order": 246,
   "p1": "999",
   "pn": "1002",
   "abstract": [
    "This paper discusses the use of a secondary likeli-hood classifier scheme for improving speaker recognition performance. The system models the out-put likelihoods of a typical Gaussian Mixture Model system across multiple speakers. The Out-put Probability Distributions (OPD) of the primary classifiers contain information on inter-speaker relationships, and are modelled by secondary classifiers to improve recognition accuracies. A com-parison of the OPD system with the traditional likelihood ratio and maximum likelihood scoring schemes for verification and identification is performed. Fusion of traditional measures with OPDs is shown to enhance overall recognition performance.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-244"
  },
  "rodriguezlinares99_eurospeech": {
   "authors": [
    [
     "L.",
     "Rodríguez-Linares"
    ],
    [
     "C.",
     "García-Mateo"
    ],
    [
     "J. L.",
     "Alba-Castro"
    ]
   ],
   "title": "On the use of neural networks to combine utterance and speaker verification systems in a text-dependent speaker verification task",
   "original": "e99_1003",
   "page_count": 4,
   "order": 247,
   "p1": "1003",
   "pn": "1006",
   "abstract": [
    "Speaker Verification and Utterance Verification are examples of techniques that can be used for Speaker Authentication purposes. Speaker Verification consists of accepting or rejecting the claimed identity of a speaker by processing samples of his/her voice. Utterance Verification systems make use of a set of speaker-independent speech models to recognize a certain utterance and decide whether a speaker has uttered it or not. If the utterances consist of passwords, this technique can be used for identity verification purposes. Up to now, both techniques have been used separately. We propose an architecture consisting of both systems working in parallel with a novel output combination technique. Thus, a Neural Network is designed to learn from the data how to balance the influence of both outputs in order to jointly minimize the False Acceptance and False Rejection rates. The better performance of this architecture is compared with those of the individual systems in an over the phone speaker recognition task.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-245"
  },
  "ruizmezcua99_eurospeech": {
   "authors": [
    [
     "B.",
     "Ruiz-Mezcua"
    ],
    [
     "R.",
     "Rodríguez-Galán"
    ],
    [
     "Luis A.",
     "Hernández-Gómez"
    ],
    [
     "Paloma",
     "Domingo-García"
    ],
    [
     "Enrique",
     "Bailly-Baillicre Gutiérrez"
    ]
   ],
   "title": "Genesys: a neural network model for speaker identification",
   "original": "e99_1007",
   "page_count": 4,
   "order": 248,
   "p1": "1007",
   "pn": "1010",
   "abstract": [
    "Mathematical models have been extensively used to shape living organism behaviour. These models are based on the N-dimensional space classification for those in which the patterns may have been defined. GeNeSys neural network family has been postulated as a global, comprehensive solution that shapes an individual behaviour. This article describes the GeNeSys family and presents some theoretical results of the researches in speaker recognition. An identification/verification system voice based is proposed. This implementation can identify or verify a speaker from 30 speakers contained in a multisession database. In this paper, a speaker verification system is presented and the tasks related to the speaker verification through the speech are developed. This system is applied to multimedia database access, services and applications. To achieve this goal a previous learning process is necessary. After the training phase is finished, the speaker model is calculated and stored in a database. A speaker recognition task using the database M2VTS from ElRA is about 88%.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-246"
  },
  "sabac99_eurospeech": {
   "authors": [
    [
     "Bogdan",
     "Sabac"
    ],
    [
     "Inge",
     "Gavat"
    ]
   ],
   "title": "Speaker verification with growing cell structures",
   "original": "e99_1011",
   "page_count": 4,
   "order": 249,
   "p1": "1011",
   "pn": "1014",
   "abstract": [
    "We present a new self-organizing neural network which performs unsupervised learning and can be used for vector quantization. The main advantage over existing approaches, e.g., the Kohonen feature map, is the ability of the model to automatically find a suitable network structure and size. This is achieved through a controlled growth process which also includes occasional removal of units. The algorithm is evaluated on a database that includes 25 speakers each of them recorded in 12 diffrent sesions. The overall performance was 99.5%. That is, in 99.5% of the trials, the right speaker was correctly accepted and the impostor speaker correctly rejected.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-247"
  },
  "tadj99_eurospeech": {
   "authors": [
    [
     "Chakib",
     "Tadj"
    ],
    [
     "Pierre",
     "Dumouchel"
    ],
    [
     "Mohamed",
     "Mihoubi"
    ],
    [
     "Pierre",
     "Ouellet"
    ]
   ],
   "title": "Environment adaptation and long term parameters in speaker identification",
   "original": "e99_1015",
   "page_count": 4,
   "order": 250,
   "p1": "1015",
   "pn": "1018",
   "abstract": [
    "In this paper, we have integrated in a GMM based speaker identification system two different techniques: a) Maximum Likelihood Linear Regression (MLLR) transformation which adapts the system to the new environment based on modifying the continuous densities of the GMM mixtures. We apply the MLLR to perform environmental compensation by reducing a mismatch due to channel or additive noise effects, b) Linear Discriminant Analysis (LDA) applied on sequences of acoustic vectors. LDA extracts, from these sequences, a set of discriminant parameters maximizing the class separability by designing a linear transformation. Previous works have shown that application of LDA to speech recognition problem increases performance of speech recognition system. We use this approach to extract features that are more invariant to non-speakers-related conditions such as handset types and channel effects. Experiments are done on 45 speaker's Spidre database.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-248"
  },
  "yoshida99_eurospeech": {
   "authors": [
    [
     "K.",
     "Yoshida"
    ],
    [
     "K.",
     "Takagi"
    ],
    [
     "K.",
     "Ozeki"
    ]
   ],
   "title": "Speaker identification using subband HMMS",
   "original": "e99_1019",
   "page_count": 4,
   "order": 251,
   "p1": "1019",
   "pn": "1022",
   "abstract": [
    "This paper is concerned with optimum band splitting and optimum recombination weights in subband HMM-based speaker identification. In the first experiment, the full frequency band (8kHz) was split into two subbands, and speaker identification rate was measured for various splitting frequencies and recombination weights. It was found that subbands 0-2kHz and 2-8kHz with equal recombination weights gave the best identification rate, outperforming a baseline method without band-splitting. In the second experiment, the full-band was split into three subbands with various splitting frequencies. Splitting into 0-2kHz, 2-6kHz, and 6-8kHz gave the best result, slightly outperforming the two-subband case. Finally, four-subband experiment was conducted, the result of which suggests that the speaker information and the phonemic information are complementary to a considerable degree in the spectral domain.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-249"
  },
  "zhang99b_eurospeech": {
   "authors": [
    [
     "W. D.",
     "Zhang"
    ],
    [
     "K. K.",
     "Yiu"
    ],
    [
     "M. W.",
     "Mak"
    ],
    [
     "C. K.",
     "Li"
    ],
    [
     "M. X.",
     "He"
    ]
   ],
   "title": "A priori threshold determination for phrase-prompted speaker verification",
   "original": "e99_1023",
   "page_count": 4,
   "order": 252,
   "p1": "1023",
   "pn": "1026",
   "abstract": [
    "This paper presents a novel method to determine the decision thresholds of speaker verification systems using enrollment data only. In the method, a speaker model is trained to differentiate the voice of the corresponding speaker and that of a general population. This is accomplished by using the speaker's utterances and those of some other speakers (denoted as anti-speakers) as the training set. Then, an operation environment is simulated by presenting the utterances of some pseudo-impostors (none of them is an anti-speaker) to the speaker model. The threshold is adjusted until the chance of falsely accepting a pseudo-impostor falls below an application dependent level. Experimental evaluations based on 138 speakers of the YOHO corpus suggest that with a simulated operation environment, it is able to determine the best compromise between false acceptance and false rejection.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-250"
  },
  "acero99_eurospeech": {
   "authors": [
    [
     "Alex",
     "Acero"
    ]
   ],
   "title": "Formant analysis and synthesis using hidden Markov models",
   "original": "e99_1047",
   "page_count": 4,
   "order": 253,
   "p1": "1047",
   "pn": "1050",
   "abstract": [
    "This paper describes a unifying framework for both formant tracking and speech synthesis using Hidden Markov Models (HMM). The feature vector in the HMM is composed by the first three formant frequencies, their bandwidths and their delta with time. Speech is synthesized by generating the most likely sequence of feature vectors from a HMM, trained with a set of sentences from a given speaker. Higher formant tracking accuracy can be achieved by finding the most likely formant track given a distribution of the formants of every sound. This data-driven formant synthesizer bridges the gaps between rule-based formant synthesizers and concatenative synthesizers by synthesizing speech that is both smooth and resembles the speaker in the training data.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-251"
  },
  "bailly99_eurospeech": {
   "authors": [
    [
     "Gérard",
     "Bailly"
    ]
   ],
   "title": "Accurate estimation of sinusoidal parameters in an harmonic+noise model for speech synthesis",
   "original": "e99_1051",
   "page_count": 4,
   "order": 254,
   "p1": "1051",
   "pn": "1054",
   "abstract": [
    "We present here an Harmonic+Noise Model (HNM) for speech synthesis. The noise part is represented by an autoregressive model whose output is pitch-synchronously modulated in energy. The harmonic part of the signal is represented by a sinusoidal model. This paper compares different methods for separating these two components. We then propose a method for the estimation of the sinusoidal parameters derived from the ABS model [8] and evaluate different models for the analysis/synthesis of the stochastic part proposed in the literature.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-252"
  },
  "laine99_eurospeech": {
   "authors": [
    [
     "Unto K.",
     "Laine"
    ]
   ],
   "title": "Modal synthesis and modeling of vowels",
   "original": "e99_1055",
   "page_count": 5,
   "order": 255,
   "p1": "1055",
   "pn": "1058",
   "abstract": [
    "A new simple model for vocal tract (VT) acoustics is presented. The model is based on a systems approach applied to the modes of the VT and the subglottal cavities. Glottal interaction with the ability to produce pitch-synchronous modulation effects is included as well as the lip radiation impedance. The synthesized vowels have spectral and temporal features close to natural ones. They are almost free of the nasal quality present in many vowels synthesized by conventional methods.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-253"
  },
  "obrien99_eurospeech": {
   "authors": [
    [
     "Darragh",
     "O'Brien"
    ],
    [
     "Alex I. C.",
     "Monaghan"
    ]
   ],
   "title": "Shape invariant pitch modification of speech using a harmonic model",
   "original": "e99_1059",
   "page_count": 4,
   "order": 256,
   "p1": "1059",
   "pn": "1062",
   "abstract": [
    "We present a simple but effective approach to pitch modification of speech based on a harmonic model. Building on our time-scaling algorithm [1], pitchmodification applies to a harmonically coded glottal wave estimate derived via a simple inverse filtering technique [3]. The modified glottal wave subsequently serves as input to an LPC vocal tract filter and the pitch-scaled speech is generated. Shape invariance is maintained in the glottal wave by exploiting the harmonic nature of the sine waves used to code each frame thus avoiding the need for \\pitch pulse onset time\" estimation. Furthermore, given its smooth shape it is not necessary to resample the glottal wave spectrum at the new harmonic frequencies. The original spectrum is merely compressed/expanded to produce the desired pitch change.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-254"
  },
  "beutnagel99b_eurospeech": {
   "authors": [
    [
     "Mark",
     "Beutnagel"
    ],
    [
     "Alistair",
     "Conkie"
    ]
   ],
   "title": "Interaction of units in a unit selection database",
   "original": "e99_1063",
   "page_count": 4,
   "order": 257,
   "p1": "1063",
   "pn": "1066",
   "abstract": [
    "The purpose of this paper is to examine some aspects of unit selection for Text to Speech synthesis (TTS). We use Unit Selection as described in [2],[3]. The approach taken was to synthesize a large number of sentences and capture information about the selected units. We used approximately 25 million phonemes resulting from 10,000 files of AP newswire text. Given these statistics we looked at the units selected in order to try to analyse how unit selection works from a statistical point of view. Results of our analysis are presented.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-255"
  },
  "garciagomez99_eurospeech": {
   "authors": [
    [
     "Ramón",
     "García Gómez"
    ],
    [
     "Ricardo",
     "López Barquilla"
    ],
    [
     "José Ignacio",
     "Puertas Tera"
    ],
    [
     "José",
     "Parera Bermudez"
    ],
    [
     "Marie-Christine",
     "Haton"
    ],
    [
     "Jean-Paul",
     "Haton"
    ],
    [
     "Pierre",
     "Alinat"
    ],
    [
     "Sofia",
     "Moreno"
    ],
    [
     "Wolfgang",
     "Hess"
    ],
    [
     "Ma Araceli",
     "Sanchez Raya"
    ],
    [
     "Eduardo Alberto",
     "Martínez Gual"
    ],
    [
     "Juan Luis",
     "Navas-Chaveli Daza"
    ],
    [
     "Christophe",
     "Antoine"
    ],
    [
     "Marie-Madeleine",
     "Durel"
    ],
    [
     "Genevieve",
     "Maugin"
    ],
    [
     "Silke",
     "Hohmann"
    ]
   ],
   "title": "Speech training for deaf and hearing-impaired people",
   "original": "e99_1067",
   "page_count": 4,
   "order": 258,
   "p1": "1067",
   "pn": "1070",
   "abstract": [
    "In this paper we will describe the results of the ISAEUS project (TIDE DE 3004) achieved until now. The objective of this project is to develop a state of the art, cost effective, prototype for training deaf people in German, French and Spanish languages. We describe the user needs as understood for the users of ISAEUS: deaf persons and speech therapist. The paper also includes a brief description of the main blocks of the system: the acoustic-phonetic analyser in which will be based the exercises, the set of exercises that will be included, other features of the system and the real implementation. The paper will finalise with the description of the strategies for evaluating the effectiveness of this tool.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-256"
  },
  "hoshino99_eurospeech": {
   "authors": [
    [
     "Shinichi",
     "Hoshino"
    ],
    [
     "Itaru",
     "Kaneko"
    ],
    [
     "Hideaki",
     "Kikuchi"
    ],
    [
     "Katsuhiko",
     "Shirai"
    ]
   ],
   "title": "A post-processing of speech for hearing impaired integrate into standard digital audio decoders",
   "original": "e99_1071",
   "page_count": 4,
   "order": 259,
   "p1": "1071",
   "pn": "1074",
   "abstract": [
    "This paper describes the simple method of post processing for hearing impaired integrate into standard digital audio decoder such as AAC, AC3 or other codecs based on time/frequency transform. The results of evaluation, employing emulated hearing impairment, will be shown.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-257"
  },
  "imatomi99_eurospeech": {
   "authors": [
    [
     "Setsuko",
     "Imatomi"
    ],
    [
     "Takayuki",
     "Arai"
    ],
    [
     "Yuko",
     "Mimura"
    ],
    [
     "Masako",
     "Kato"
    ]
   ],
   "title": "Effects of hoarseness on hypernasality ratings",
   "original": "e99_1075",
   "page_count": 4,
   "order": 260,
   "p1": "1075",
   "pn": "1078",
   "abstract": [
    "We investigated how hoarseness effects perceived hypernasality ratings in order to make a rating scale for hoarse voices. Thirty stimuli, of which 18 were target stimuli (three each for voices with and without hoarseness at three levels of hypernasality), and of which 12 were foil stimuli. These voices were listened to by four experienced speech pathologists. They were asked to rate hypernasality on 5-point scale. Perceived hypernasality decreased with hoarseness for a severely hypernasal voice, but results varied among the listeners for voices with no and moderate hypernasality.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-258"
  },
  "rezaeiaghbash99_eurospeech": {
   "authors": [
    [
     "N.",
     "Rezaei-Aghbash"
    ],
    [
     "S. P.",
     "Whiteside"
    ],
    [
     "P. A.",
     "Cudd"
    ]
   ],
   "title": "Cross-language analysis of voice onset time in stuttered speech",
   "original": "e99_1079",
   "page_count": 4,
   "order": 261,
   "p1": "1079",
   "pn": "1082",
   "abstract": [
    "This study compares the VOT measurements between Persian and English stutterers. The speech samples of five adult stutterers (four males and one female) of each language were recorded during a reading task. 98 stuttering events in words beginning with plosives and 34 fluent plosives (i.e., final fluent iterations following the stuttered portions of speech) were identified in the data of both groups. The VOT measurements were obtained from both stuttered as well as fluent productions of speech for voiceless plosives /p, t, k/ and voiced plosives /b, d, J/ respectively.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-259"
  },
  "deng99_eurospeech": {
   "authors": [
    [
     "Li",
     "Deng"
    ],
    [
     "Jeff",
     "Ma"
    ]
   ],
   "title": "A statistical coarticulatory model for the hidden vocal-tract-resonance dynamics",
   "original": "e99_1499",
   "page_count": 4,
   "order": 262,
   "p1": "1499",
   "pn": "1502",
   "abstract": [
    "A statistical coarticulatory model is presented for spontaneous speech recognition, where knowledge of the dynamic, target-directed behavior in the vocal tract resonance responsible for the production of highly coarticulated speech is incorporated into the recognizer design, training, and in likelihood computation. The principal advantage of the new speech model over the conventional HMM is the use of a compact, internal structure that parsimoniously represents long-span context dependence in the observable domain of speech acoustics without using additional, contextdependent model parameters. The new model is formulated mathematically as a constrained, nonstationary, and nonlinear dynamic system, for which aversion of the generalized EM algorithm is developed and implemented for automatically learning the compact set of model parameters. Experiments for speech recognition using spontaneous speech data from SWITCHBOARD corpus are reported.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-260"
  },
  "albesano99_eurospeech": {
   "authors": [
    [
     "D.",
     "Albesano"
    ],
    [
     "R. De",
     "Mori"
    ],
    [
     "R.",
     "Gemello"
    ],
    [
     "F.",
     "Mana"
    ]
   ],
   "title": "A study on the effect of adding new dimensions to trajectories in the acoustic space",
   "original": "e99_1503",
   "page_count": 4,
   "order": 263,
   "p1": "1503",
   "pn": "1506",
   "abstract": [
    "The paper discusses the use, in a hybrid recognizer, of gravity centers (gc) in spectral subbands as features to be used in addition to Mel Scaled Cepstral Coefficients (MFCC) and their time derivatives. Results on noisy telephone speech show that gc computed after the nonlinear processing of an ear model increase the word accuracy from 72.63% to 78.13% .\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-261"
  },
  "gales99_eurospeech": {
   "authors": [
    [
     "M. J. F.",
     "Gales"
    ],
    [
     "P. A.",
     "Olsen"
    ]
   ],
   "title": "Tail distribution modelling using the richter and power exponential distributions",
   "original": "e99_1507",
   "page_count": 4,
   "order": 264,
   "p1": "1507",
   "pn": "1510",
   "abstract": [
    "The vast majority of HMMbased speech recognition systems use Gaussian mixture models as the state distribution model. The use of these distributions is motivated more by ease of training, decod-ing and the fact that a sufficient number of Gaussian components may be used to approximate any distribution, than some underlying aspect of the data being modelled. If distributions were se-lected that better modelled the observed data, fewer components should be required and recognition accuracy should improve. This paper examines two distributions for improving the modelling of the tails of the densities. The first distribution, the Richter distribution, fits within the general framework of Gaussian component tying, but has some attractive attributes for decoding. The second distribution, the power exponential, does not fit within a tying framework. Despite gains in likelihood, indicating that the Gaus-sian components are suboptimal in a likelihood sense, only small gains in recognition performance were observed on a large vocabulary speech recognition task.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-262"
  },
  "zhao99_eurospeech": {
   "authors": [
    [
     "Qingwei",
     "Zhao"
    ],
    [
     "Zuoying",
     "Wang"
    ],
    [
     "Dajin",
     "Lu"
    ]
   ],
   "title": "A study of duration in continuous speech recognition based on DDBHMM",
   "original": "e99_1511",
   "page_count": 4,
   "order": 265,
   "p1": "1511",
   "pn": "1514",
   "abstract": [
    "DDBHMM solved the defects of traditional HMM. Based on DDBHMM, the problem of how to effectively utilize the duration information is studied in detail. The approach on estimating the duration distribution is introduced firstly, then the data file is classified according to the speak rate. The recognition experiment shows that, the duration information behaves best on the data of low speak rate, behaves normal on the data of medium speak rate and has little effect on the data of fast speak rate. Therefore, the most importance of duration is that by it the more accurate state segmentation point could be obtained and then the recognition rate can be improved. At the same time, the robustness of the system to speaking rate is improved with the employment of the duration information. Furthermore, the method of classified duration and normalized duration is also put forward and studied in detail, it shows that both of the two method can improve the effect. In order to study the dependency between the duration, the method of using the Bigram of the duration is proposed and analyzed. At last, the approach of post processing duration is studied, it shows that nly based on DDBHMM, and utilizing the duration information synchronously in the recognition process, then the performance can be improved greatly.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-263"
  },
  "vaich99_eurospeech": {
   "authors": [
    [
     "T.",
     "Vaich"
    ],
    [
     "A.",
     "Cohen"
    ]
   ],
   "title": "Comparison of continuous-density and semi-continuous HMM in isolated words recognition systems",
   "original": "e99_1515",
   "page_count": 4,
   "order": 266,
   "p1": "1515",
   "pn": "1518",
   "abstract": [
    "Several types of Semi-Continuous HMM (SC-HMM) have been compared with the Continuous Density HMM (CD-HMM) in the context of Speaker Independent Isolated Words Recognition (SI-IWR). It is demonstrated that for the ten-digit vocabulary (TIDIGITS), the SC-HMM outperforms the CD-HMM when memory constraints are imposed on the system. SC-HMMs demonstrate recognition rate of about 95% with a total of 59 Gaussians, while under similar conditions the CD-HMM yield recognition rates of well below 80%. An algorithm for optimal selection of Gaussian functions for SC-HMM is presented.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-264"
  },
  "asoh99_eurospeech": {
   "authors": [
    [
     "Hideki",
     "Asoh"
    ],
    [
     "Toshihiro",
     "Matsui"
    ],
    [
     "John",
     "Fry"
    ],
    [
     "Futoshi",
     "Asano"
    ],
    [
     "Satoru",
     "Hayamizu"
    ]
   ],
   "title": "A spoken dialog system for a mobile office robot",
   "original": "e99_1139",
   "page_count": 4,
   "order": 267,
   "p1": "1139",
   "pn": "1142",
   "abstract": [
    "A spoken dialog interface of a mobile office robot is described. To realize robust speech recognition in noisy office environments, a microphone array system and a technique of switching multiple speech recognition proc-esses with different dictionaries are introduced. To real-ize flexible and natural dialog, task dependent semantic frames and keeping track of attentional state of dialog are used. The system is implemented on a real mobile robot and evaluated with sample dialogs.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-265"
  },
  "bell99_eurospeech": {
   "authors": [
    [
     "Linda",
     "Bell"
    ],
    [
     "Joakim",
     "Gustafson"
    ]
   ],
   "title": "Interaction with an animated agent in a spoken dialogue system",
   "original": "e99_1143",
   "page_count": 4,
   "order": 268,
   "p1": "1143",
   "pn": "1146",
   "abstract": [
    "The study reported in this paper is based on results from a Swedish database of spontaneous computer-directed speech. This database was investigated to determine how people adapt their language when they interact with computers. A spoken dialogue system with an animated agent, August, was installed in a public location in downtown Stockholm. Members of the general public were invited to interact with the system and dialogues were recorded. The database was collected during a period of six months and consists of transcriptions of more than ten thousand spontaneous utterances. The domain restrictions in this spoken dialogue system were minimal, and the users were not explicitly told what they could expect the system to understand. In this paper the users communicative strategies, as they are manifested in the input utterances, are studied. The influence of the interface design on user expectations is also discussed. Results indicate that user adaptation, as reflected in the corpus, comprises lexical as well as syntactical aspects\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-266"
  },
  "bernsen99_eurospeech": {
   "authors": [
    [
     "Niels Ole",
     "Bernsen"
    ],
    [
     "Laila",
     "Dybkjaer"
    ],
    [
     "Ulrich",
     "Heid"
    ]
   ],
   "title": "Current practice in the development and evaluation of spoken language dialogue systems.",
   "original": "e99_1147",
   "page_count": 4,
   "order": 269,
   "p1": "1147",
   "pn": "1150",
   "abstract": [
    "The growing industrial take-up of spoken language dialogue systems (SLDSs), their constantly increasing sophistication, and the scarcity of teams which master the full system complexity as well as all the necessary steps in the SLDSs life-cycle, has created a felt need for a best practice model for development and evaluation of SLDSs. An obvious first step towards establishing a best practice model is to build a solid overview of current practice. This paper presents a model for the description of SLDS current practice with particular focus on dialogue management and human factors.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-267"
  },
  "gustafson99_eurospeech": {
   "authors": [
    [
     "Joakim",
     "Gustafson"
    ],
    [
     "Nikolaj",
     "Lindberg"
    ],
    [
     "Magnus",
     "Lundeberg"
    ]
   ],
   "title": "The august spoken dialogue system",
   "original": "e99_1151",
   "page_count": 4,
   "order": 270,
   "p1": "1151",
   "pn": "1154",
   "abstract": [
    "This paper describes the Swedish spoken dialogue system August. This system has been used to collect spontaneous speech data, largely from people with no previous experience of speech technology or computers. The aim was to be able to analyse how novice users interact with a multi-modal information kiosk, placed without supervision in a public location. The system described in this paper featured an animated talking agent, August. Speech data was collected during the six months that the system was exposed to the general public. The system and its components are briefly described, with references to more detailed papers.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-268"
  },
  "grisvard99_eurospeech": {
   "authors": [
    [
     "Olivier",
     "Grisvard"
    ],
    [
     "Bertrand",
     "Gaiffe"
    ]
   ],
   "title": "An event-based dialogue model and its implementation in multidial2",
   "original": "e99_1155",
   "page_count": 4,
   "order": 271,
   "p1": "1155",
   "pn": "1158",
   "abstract": [
    "In this paper we present and justify the design and implementation choices we have made in order to build our dialogue system MultiDial2. We propose an event-based representation that enables us to structure the dialogue data upon several levels but within a single representation space. We show how this provides us with the necessary flexibility for a proper management of the dialogue. We describe the implementation of our model which uses the theory of Mental Representations, a formalism developed for referential resolution purposes.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-269"
  },
  "huang99b_eurospeech": {
   "authors": [
    [
     "Chao",
     "Huang"
    ],
    [
     "Peng",
     "Xu"
    ],
    [
     "Xin",
     "Zhang"
    ],
    [
     "Shubin",
     "Zhao"
    ],
    [
     "Taiyi",
     "Huang"
    ],
    [
     "Bo",
     "Xu"
    ]
   ],
   "title": "LODESTAR: a Mandarin spoken dialogue system for travel information retrieval",
   "original": "e99_1159",
   "page_count": 4,
   "order": 272,
   "p1": "1159",
   "pn": "1162",
   "abstract": [
    "In this paper, we present LOADSTAR, a Mandarin Spoken Dialogue system developed for travel information retrieval, which could provide clients tourism information and plan proper itinerary according to clients favor. In addition to description of the system architecture and individual modules, several significant strategies such as mixed class-based and word-based language model, robust semantic-based hierarchical parsing paradigm, dynamically refreshed tracing of dialog, unified scoring schema integrating multiple information sources, and mixed-initiatives control mechanism are proposed and applied to realize efficient interaction. Evaluation of system performance on 13 dialog include 120 utterances, which contains ample spontaneous speech phenomena is reported. The response accuracy of system achieved 90.9%.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-270"
  },
  "sasajima99_eurospeech": {
   "authors": [
    [
     "Munehiko",
     "Sasajima"
    ],
    [
     "Takehide",
     "Yano"
    ],
    [
     "Yasuyuki",
     "Kono"
    ]
   ],
   "title": "EUROPA: a generic framework for developing spoken dialogue systems",
   "original": "e99_1163",
   "page_count": 4,
   "order": 273,
   "p1": "1163",
   "pn": "1166",
   "abstract": [
    "Voice interfaces are not popular since they are neither useful nor user-friendly for non-specialist users. In this paper, EUROPA, a new framework for developing spoken dialogue systems, is introduced. In developing EUROPA, the authors focused on three points : (1) acceptance of spoken language, (2) portability in terms of domain and task, and (3) practical performance of the applied system. The framework is applied to prototyping a car navigation system called MINOS. MINOS is built on a portable PC, can process over 700 words of recognition vocabulary, and is able to respond to a user's question within a few seconds.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-271"
  },
  "nakano99_eurospeech": {
   "authors": [
    [
     "Mikio",
     "Nakano"
    ],
    [
     "Kohji",
     "Dohsaka"
    ],
    [
     "Noboru",
     "Miyazaki"
    ],
    [
     "Jun-ichi",
     "Hirasawa"
    ],
    [
     "Masafumi",
     "Tamoto"
    ],
    [
     "Masahito",
     "Kawamori"
    ],
    [
     "Akira",
     "Sugiyama"
    ],
    [
     "Takeshi",
     "Kawabata"
    ]
   ],
   "title": "Handling rich turn-taking in spoken dialogue systems",
   "original": "e99_1167",
   "page_count": 4,
   "order": 274,
   "p1": "1167",
   "pn": "1170",
   "abstract": [
    "This paper discusses how to build a system that can engage in a mixed-initiative human-machine spoken dia-logue in which system utterances sometimes overlap with user utterances and vice versa. In the method, a module that incrementally understands user utterances and another module that incrementally generates system utterances work in parallel, and the timing of taking and releasing the dialogue initiative is decided according to the understanding of user utterances and the content of the system utterances. This method enables the system to respond when the user holds the dialogue initiative and is speaking, and enables the system to react to the users bargeins when it holds the initiative and is speaking. An experimental system called DUG-1 is also presented.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-272"
  },
  "pirker99_eurospeech": {
   "authors": [
    [
     "Hannes",
     "Pirker"
    ],
    [
     "Georg",
     "Loderer"
    ],
    [
     "Harald",
     "Trost"
    ]
   ],
   "title": "Thus spoke the user to the wizard",
   "original": "e99_1171",
   "page_count": 4,
   "order": 275,
   "p1": "1171",
   "pn": "1174",
   "abstract": [
    "Wizard-of-Oz (WOZ) simulations are a popular means for investigating the properties of humancomputer interaction. In this paper the findings from a WOZ experiment for evaluating different design options for a spoken dialogue system are presented. In addition to the documentation of the outcomes of this evaluation in terms of standard quantitative measures we also present findings from a more qualitative analysis of the speech data collected throughout this experiment. It is argued that such a combined analysis of all aspects of the human-computer interaction allows for a correct interpretation of the results and their fruitful application in the context of system prototyping.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-273"
  },
  "pargellis99_eurospeech": {
   "authors": [
    [
     "Andrew",
     "Pargellis"
    ],
    [
     "Hon-Kwang Jeff",
     "Kuo"
    ],
    [
     "Chin-Hui",
     "Lee"
    ]
   ],
   "title": "Automatic dialogue generator creates user defined applications",
   "original": "e99_1175",
   "page_count": 4,
   "order": 276,
   "p1": "1175",
   "pn": "1178",
   "abstract": [
    "We report on the development of an Automatic Dialogue Generator (ADG), a software engine with associated library files, that simplifies the generation of new applications requiring a speech interface. A key feature of the ADG is that, given any task description specified in tables, the ADG can automatically generate a finite-state dialogue for that task, in a uniform and consistent fashion. The ADG generates a connected graph structure that provides the general dialogue flow between dialogue states, as well as all the dialogue and action components within each state. Our design philosophy is to build an Application Generator (AG) that is highly portable. The AG contains four modular pieces, of which one is the ADG described here. The ADG consists of domain independent modules used to create a unique dialogue for each individual. The ADG accomplishes this by combining information from a users profile with information from external sources to generate a dynamic and user-specific dialogue flow. The user specifies types of services, such as Sports and Email, and keywords are defined to prune and organize the information for each service.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-274"
  },
  "relanogil99_eurospeech": {
   "authors": [
    [
     "José",
     "Relaño Gil"
    ],
    [
     "Daniel",
     "Tapias"
    ],
    [
     "Juan Manuel",
     "Villar-Navarro"
    ],
    [
     "Maria C.",
     "Gancedo"
    ],
    [
     "Luis A.",
     "Hernández-Gómez"
    ]
   ],
   "title": "Flexible mixed-initiative dialogue for telephone services",
   "original": "e99_1179",
   "page_count": 4,
   "order": 277,
   "p1": "1179",
   "pn": "1182",
   "abstract": [
    "In this work, we present an experimental analysis of a Dialogue System for the automatization of simple telephone services. A first evaluation of a preliminary version of the system was done based on the Speech Recognizer error rate and on the identification of two groups of users, that we refer to as group A and B. From this evaluation we conclude the necessity to design a robust and exible system suitable to have different dialogue control strategies depending on the characteristics of the user and the performance of the speech recognition module. A system adaptation procedure combining a normalized average number of utterances per task, the amount ofinformation in some particular utterances, and an estimate of the recognition error rate. Our adaptation procedure defines two different control management strategies for a exible mixed-initiative strategy: _xed for high recognition errors and Group B users, and mixed for low error rates and A users. Experimental results following the PARADISE framework showed an important improvement.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-275"
  },
  "veldhuijzenvanzanten99_eurospeech": {
   "authors": [
    [
     "Gert",
     "Veldhuijzen van Zanten"
    ]
   ],
   "title": "User modelling in adaptive dialogue management",
   "original": "e99_1183",
   "page_count": 5,
   "order": 278,
   "p1": "1183",
   "pn": "1186",
   "abstract": [
    "This paper describes an adaptive approach to dialogue management in spoken dialogue systems. The system maintains a user model, in which assumptions about the users expectations of the system are recorded. Whenever recognition errors occur, the dialogue manager drops assumptions from the user model, and adapts its behaviour accordingly. The system uses a hierarchical slot structure that allows generation and interpretation of utterances at various levels of generality. This is essential for ex-ploiting mixed-initiative to optimise effectiveness. The hierarchical slot structure is also a source of variation. This is important because it is ineffective to repeat prompts in cases of speech recognition errors, for in such cases users are likely to repeat their responses also, and thus the same speech recognition problems reoccur.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-276"
  },
  "ashour99_eurospeech": {
   "authors": [
    [
     "Gal",
     "Ashour"
    ],
    [
     "Isak",
     "Gath"
    ]
   ],
   "title": "Characterization of speech during imitation",
   "original": "e99_1187",
   "page_count": 4,
   "order": 279,
   "p1": "1187",
   "pn": "1190",
   "abstract": [
    "The present work analyzes some effects of imitation on the produced voice, focusing on both macro-level parameters such as the pitch contour, as well as on micro-level acoustical structures such as the formant frequencies. The following speech production characteristics are addressed: (1) Tracking of pitch contours is carried out using a super-resolution algorithm for pitch detection. (2) Formant frequencies are estimated for the vowel phonemes using a linear prediction technique, approximating the vocal tract by an all-pole model. (3) Cross-sectional tube areas of the vocal tract are estimated using inverse Levinson algorithm. A strong tendency of pitch contours during imitation to resemble those of the target speaker was found. A significant tendency of the second and third formants during imitation to resemble those of the target person was identified. A tendency of the cross sectional areas to be smaller during imitation was found.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-277"
  },
  "bovbel99_eurospeech": {
   "authors": [
    [
     "Evgeny I.",
     "Bovbel"
    ],
    [
     "Polina P.",
     "Tkachova"
    ],
    [
     "Igor E.",
     "Kheidorov"
    ]
   ],
   "title": "The analysis of speaker individual features based on autoregressive hidden Markov models",
   "original": "e99_1191",
   "page_count": 4,
   "order": 280,
   "p1": "1191",
   "pn": "1194",
   "abstract": [
    "The speech-based analysis of speaker individual features has found wide application area. In order to analyse the speaker individual features it is necessary to use high frequencies and accurate spectrum estimation methods. It was found out that the best way to analyse the personal voice individuality is to use bark scaled spectrum estimation based on arithmetic Fourier transform. For each of 10 speakers the autoregressive hidden Markov model was trained. The experiments show that such models provide high accuracy of person identification based on bark- cepstrum analysis and high sampling frequency.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-278"
  },
  "delacourt99_eurospeech": {
   "authors": [
    [
     "Perrine",
     "Delacourt"
    ],
    [
     "David",
     "Kryze"
    ],
    [
     "Christian J.",
     "Wellekens"
    ]
   ],
   "title": "Detection of speaker changes in an audio document",
   "original": "e99_1195",
   "page_count": 4,
   "order": 281,
   "p1": "1195",
   "pn": "1198",
   "abstract": [
    "This paper addresses the problemof speaker-based segmentation. The aim is to segment the audio data with respect to the speakers. In our study, we assume that no prior information on speakers is available and that people do not speak simultaneously. Our segmentation technique is operated in two passes: first, the most likely speaker changes are detected and then, they are validated or discarded during the second pass. The practical significance of this study is illustrated by applying our technique to synthesized and real data to show its efficiency and to compare its performances with another segmentation technique.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-279"
  },
  "glaeser99_eurospeech": {
   "authors": [
    [
     "Axel",
     "Glaeser"
    ]
   ],
   "title": "Dynamic test durations for text-independent speaker verification systems",
   "original": "e99_1199",
   "page_count": 4,
   "order": 282,
   "p1": "1199",
   "pn": "1202",
   "abstract": [
    "In the past few years, phone banking and phone shopping have become more and more popular. These new needs have mainly been initiated by the service providers (banks, etc) which are looking for cost-efficient and secure solutions to enable their clients a flexible and comfortable service access. In this context, the assessment of a speaker recognition system must take into account subjective factors that are directly linked to user acceptance criteria. During field tests we found out that the main point of criticism was the test duration. While the enrollment duration was mainly accepted as a single, initial effort, the test duration of 10 seconds was not accepted as feature of a user-friendly system. Therefore, we developed a new approach based on Dynamic Test Durations (DTD) by taking the quality of speech and special characteristics of the classification process into account. This approach results in a decrease of the test down to 2,4 seconds (average).\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-280"
  },
  "kolano99_eurospeech": {
   "authors": [
    [
     "Guido",
     "Kolano"
    ],
    [
     "Peter",
     "Regel-Brietzmann"
    ]
   ],
   "title": "Combination of vector quantization and gaussian mixture models for speaker verification with sparse training data",
   "original": "e99_1203",
   "page_count": 4,
   "order": 283,
   "p1": "1203",
   "pn": "1206",
   "abstract": [
    "We present a combination of an extended vector quantization (VQ) algorithm for training a speaker model and a gaussian interpretation of the VQ speaker model in the verification phase. This leads to a large decrease of the error rates compared to normal vector quantization and only a slight deterioration compared to full Gaussian mixture model (GMM) training. The training costs of the new method are only slightly higher than for pure vector quantization.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-281"
  },
  "li99_eurospeech": {
   "authors": [
    [
     "Qi",
     "Li"
    ],
    [
     "Augustine",
     "Tsai"
    ],
    [
     "Weon-Goo",
     "Kim"
    ]
   ],
   "title": "A language-independent personal voice controller with embedded speaker verification",
   "original": "e99_1207",
   "page_count": 4,
   "order": 284,
   "p1": "1207",
   "pn": "1210",
   "abstract": [
    "In this paper, we introduce a personal voice controller for name dialing. As a personal system, all the control commands are trained based on owner-selected phrases in the owner's language. Since the owner's voice characteristics is modeled together with the command voice, the system has embedded speaker verification capability without additional processing. In other words, once the system is trained, the system can only accept the voice commands from the owner. Impostor's command can be rejected although the command may be in the owner's list. The controller can recognize the speaker's command and dial the destinate phone number automatically. Such a system is useful for wireless handsets and portable communication devices. Its implementation requires much less memory space and computation resource compared to a speaker-independent system. Preliminary experiments showed that the proposed system has the state-of-the-art performances in both speech recognition and speaker verification.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-282"
  },
  "lindberg99_eurospeech": {
   "authors": [
    [
     "Johan",
     "Lindberg"
    ],
    [
     "Mats",
     "Blomberg"
    ]
   ],
   "title": "Vulnerability in speaker verification - a study of technical impostor techniques",
   "original": "e99_1211",
   "page_count": 4,
   "order": 285,
   "p1": "1211",
   "pn": "1214",
   "abstract": [
    "This paper reports on some ways to try to deceive a state-of-the-art speaker verification (SV) system. In order to evaluate the risk in SV systems one has to take into account the possible intentional impostors who know whom they are attacking. We defined a worst case scenario where the impostor has extensive knowledge about the person to deceive as well as the system to attack. In this framework we tested our SV system against concatenated client speech, re-synthesis of the client speech and diphone synthesis of the client.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-283"
  },
  "mclaughlin99_eurospeech": {
   "authors": [
    [
     "Jack",
     "McLaughlin"
    ],
    [
     "Douglas A.",
     "Reynolds"
    ],
    [
     "Terry",
     "Gleason"
    ]
   ],
   "title": "A study of computation speed-UPS of the GMM-UBM speaker recognition system",
   "original": "e99_1215",
   "page_count": 4,
   "order": 286,
   "p1": "1215",
   "pn": "1218",
   "abstract": [
    "The Gaussian Mixture Model Universal Background Model (GMM-UBM) speaker recognition system has demonstrated very high performance in several NIST evaluations. Such evaluations, however, are concerned only with classification accuracy. In many applications, system effectiveness must be evaluated in light of both accuracy and execution speed. We present here a number of techniques for decreasing computation. Using data from the Switchboard telephone speech corpus, we show that significant speed-ups can be obtained while sacrificing surprisingly little accuracy. We expect that these techniques, involving lowering model order as well as processing fewer speech frames, will apply equally well to other recognition systems.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-284"
  },
  "maes99_eurospeech": {
   "authors": [
    [
     "Stéphane H.",
     "Maes"
    ]
   ],
   "title": "Conversational biometrics",
   "original": "e99_1219",
   "page_count": 4,
   "order": 287,
   "p1": "1219",
   "pn": "1222",
   "abstract": [
    "In this paper, we present a new modality for speaker recognition: conversational biometrics. By combining diverse simultaneous conversational technologies, high accuracy transparent speaker recognition becomes possible even in channel or environment mismatches. For speaker identification over very large populations, we combine dialogs to reduce the set of confusable speakers and textindependent speaker identification to pin-point the actual speaker. Similarly, dialogs with personal random or predefined questions are used to perform simultaneously knowledge-based and acousticbased verifications of the user. Adequate design of the dialog allows to tailor the ROC curves to the needs of most applications.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-285"
  },
  "masuko99_eurospeech": {
   "authors": [
    [
     "Takashi",
     "Masuko"
    ],
    [
     "Takafumi",
     "Hitotsumatsu"
    ],
    [
     "Keiichi",
     "Tokuda"
    ],
    [
     "Takao",
     "Kobayashi"
    ]
   ],
   "title": "On the security of HMM-based speaker verification systems against imposture using synthetic speech",
   "original": "e99_1223",
   "page_count": 4,
   "order": 288,
   "p1": "1223",
   "pn": "1226",
   "abstract": [
    "For speaker verification systems, security against imposture is one of the most important problems, and many approaches to reducing false acceptance of impostors as well as false rejection of clients have been investigated. On the other hand, imposture using synthetic speech has not been considered. In this paper, we investigate imposture against speaker verification systems using synthetic speech. We use an HMM-based text-prompted speaker verification system with a false acceptance rate of 0% for human impostors as a reference system, and adopt a trainable HMM-based speech synthesis system for imposture. Experimental results show that false acceptance rates for synthetic speech reached over 70% by training the synthesis system using only 1 sentence from each customer, and current security of HMM-based speaker verification systems against synthetic speech is inadequate.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-286"
  },
  "majewski99_eurospeech": {
   "authors": [
    [
     "Wojciech",
     "Majewski"
    ],
    [
     "Grazyna",
     "Mazur-Majewska"
    ]
   ],
   "title": "Speech signal parametrization for speaker recognition under voice disguise conditions",
   "original": "e99_1227",
   "page_count": 4,
   "order": 289,
   "p1": "1227",
   "pn": "1230",
   "abstract": [
    "An experiment was performed to find out, if any of commonly applied techniques of speech signal parametrization is particularly resistant to voice disguise. As experimental material three vowels extracted from the word logarytm\" spoken 10 times by each of 10 speakers under seven different speaking conditions were used. Three methods of parametrization were tested: FFT, LPC and ZCR. The results of the experiments indicated that the smallest intraspeaker variations were obtained for ZCR parameters, LPC provided reasonably good results, while FFT parameters were very seensitive to voice disguise and provided the worst results. Generally, however, it has to be stated that the experiments performed did not indicate explicitly which method of parametrization is particularly resistant to voice disguise.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-287"
  },
  "satuevillar99_eurospeech": {
   "authors": [
    [
     "Antonio",
     "Satué-Villar"
    ],
    [
     "Marcos",
     "Faúndez-Zanuy"
    ]
   ],
   "title": "On the relevance of language in speaker recognition",
   "original": "e99_1231",
   "page_count": 4,
   "order": 290,
   "p1": "1231",
   "pn": "1234",
   "abstract": [
    "This paper presents a new database collected from a bilingual speakers set (49), in two different languages: Spanish and Catalan. Phonetically there are significative differences between both languages. These differences have let us to establish several conclusions on the relevance of language in speaker recognition, using two methods: vector quantization and covariance matrices.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-288"
  },
  "yamashita99_eurospeech": {
   "authors": [
    [
     "Yoichi",
     "Yamashita"
    ]
   ],
   "title": "Prediction of keyword spotting accuracy based on simulation",
   "original": "e99_1235",
   "page_count": 4,
   "order": 291,
   "p1": "1235",
   "pn": "1238",
   "abstract": [
    "This paper proposes a method of predicting accuracy of keyword spotting in terms of FA count and spotting score of correct detections. A new measure F for predicting the FA count is calculated by simulation of the keyword spotting for phoneme sequences that phoneme-based language model generates. Another measure C for predicting the spotting score of correct detections is obtained from a product of correct recognition probabilities of phonemes. Both correlation coeficients and prediction errors are used to evaluate these measures in comparison with a simple measure of the keyword phoneme length, L. The prediction errors of FA count based on L was 7.71. The measure F reduced the prediction errors by 16%, and it had stronger correlation with the FA count. Furthermore a combined measure of F and L reduced the errors by 23%. On the other hand, L was more effective to predict the spotting score of correct detections than the measure C.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-289"
  },
  "castro99_eurospeech": {
   "authors": [
    [
     "M. J.",
     "Castro"
    ],
    [
     "D.",
     "Llorens"
    ],
    [
     "Joan-Andreu",
     "Sánchez"
    ],
    [
     "F.",
     "Casacuberta"
    ],
    [
     "P.",
     "Aibar"
    ],
    [
     "E.",
     "Segarra"
    ]
   ],
   "title": "A fast version of the atros system",
   "original": "e99_1239",
   "page_count": 4,
   "order": 292,
   "p1": "1239",
   "pn": "1242",
   "abstract": [
    "Atros is an automatic speech recognition/understanding/translation system whose knowledge sources (acoustic models, lexical models, syntactic language models, semantic models and translation models) can be learnt automatically from training data by using similar techniques. The search process in Atros is performed through a Synchronous Beam Search technique. In this paper, a faster version of Atros is presented and evaluated. This version supports improved acoustic and syntactical models. It also incorporates improved search algorithms to reduce and the computational requirements for decoding: Fast Phoneme Look-Ahead and Histogram Pruning. The system has been tested on a Spanish task of queries to a geographical database (with a vocabulary of 1,264 words). The best result achieved (in real time) was 7.10% of word error rate.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-290"
  },
  "goel99_eurospeech": {
   "authors": [
    [
     "Vaibhava",
     "Goel"
    ],
    [
     "William",
     "Byrne"
    ]
   ],
   "title": "Task dependent loss functions in speech recognition: a* search over recognition lattices",
   "original": "e99_1243",
   "page_count": 4,
   "order": 293,
   "p1": "1243",
   "pn": "1246",
   "abstract": [
    "A recognition strategy that can be matched to specific system performance criteria such as word error rate or F-measure has recently been found to yield improvements over the usual maximum aposteriori probability strategy [1] [2] [3]. In this matched-to-the-task strategy a hypothesis is chosen to minimize the expected loss or the Bayes Risk under a loss function defined by a performance measure of interest. Due to the prohibitive of exact implementation of this strategy, only an approximate implementation as an N-best list rescoring scheme been used [1] [2]. Our goal is to improve the performance of such risk-based decoders by developing search strategies that can consider more hypotheses and incorporate more acoustic evidence. In this paper we present search algorithms to implement the risk-based recognition strategy over word lattices that contain acoustic and language model scores. These algorithms are extensions of the N-best list rescoring approximation and are formulated as A* algorithms. Results are reported on the Switch-board conversational telephone speech corpus. We find that lattice based rescoring yields modest but significant improvements in word error rate relative to N-best list rescoring at comparable computational cost.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-291"
  },
  "hanzl99_eurospeech": {
   "authors": [
    [
     "Václav",
     "Hanzl"
    ]
   ],
   "title": "Theory of structured cogitation in speech recognition",
   "original": "e99_1247",
   "page_count": 4,
   "order": 294,
   "p1": "1247",
   "pn": "1250",
   "abstract": [
    "We propose general representation of knowledge and hypotheses used in speech recognition systems. General abstract entity is described and further specialised to represent various types of information - time intervals, speech signal, sequences of Markov model states, pronunciation, written text etc. These types of data represent tiers of an overall knowledge about the utterance and during the recognition process not all the tiers are fully known. Furthermore, mutual alignment of individual tiers is not fully known. Operations with these multitier partially known structures have common definition on the abstract level and individual datatypes are created by further restrictions of the basic abstraction. Common properties are however strong enough to enable design of algorithms optimizing the whole recognition system.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-292"
  },
  "ljolje99b_eurospeech": {
   "authors": [
    [
     "Andrej",
     "Ljolje"
    ],
    [
     "Fernando",
     "Pereira"
    ],
    [
     "Michael",
     "Riley"
    ]
   ],
   "title": "Efficient general lattice generation and rescoring",
   "original": "e99_1251",
   "page_count": 4,
   "order": 295,
   "p1": "1251",
   "pn": "1254",
   "abstract": [
    "We describe a lattice generation method that produces high-quality lattices with less than 10% increased computation over standard Viterbi decoding. Using the North American Business News (NAB) task, we show our method is within 0.2% in lattice word-error rate of full lattices, which are those that contain all the recognition hypotheses within the search beam. Our method is closely related to previous lattice generation methods, but applies to more general network topologies. We also give real-time results on the NAB task, in which we generate lattices in a first pass and then rescore them with stronger acoustic and language models in a second pass. We are able to achieve at 3x real-time a word error rate of 11.2% on the Eval 95 test set, which is only 1.7% worse than AT&Ts official bench-mark result that year using what was then a 1000x real-time system.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-293"
  },
  "xu99_eurospeech": {
   "authors": [
    [
     "Mingxing",
     "Xu"
    ],
    [
     "Fang",
     "Zheng"
    ],
    [
     "Wenhu",
     "Wu"
    ]
   ],
   "title": "A fast and effective state decoding algorithm",
   "original": "e99_1255",
   "page_count": 4,
   "order": 296,
   "p1": "1255",
   "pn": "1258",
   "abstract": [
    "In this paper a fast and effective algorithm named equal feature variance sum (EFVS) frame-synchronous searching is presented for state decoding. EFVS controls the state transition by using only the feature variance of the speech, instead of by using the state dwell distribution. The basic hypothesis of this new algorithm is the equality of feature variance sum in each state of the speech. Given the boundaries of the speech recognition unit (SRU), EFVS can generate the state sequence without dynamic searching. In a continuous speech word recognition system, this novel algorithm reduces the error rate by 36.8% and speed up the system by 65.6% compared with the traditional state decoding methods.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-294"
  },
  "jeanrenaud99_eurospeech": {
   "authors": [
    [
     "Philippe",
     "Jeanrenaud"
    ],
    [
     "Greg",
     "Cockroft"
    ],
    [
     "Allard",
     "VanderHeidjen"
    ]
   ],
   "title": "A multimodal, multilingual telephone application: the wildfire electronic assistant",
   "original": "e99_1259",
   "page_count": 4,
   "order": 297,
   "p1": "1259",
   "pn": "1262",
   "abstract": [
    "This paper describes how a telephone-based application can perform a variety of tasks in a completely hands-free mode. The overall architecture of the speech component is multi-modal in that each mode is tailored to a specific need of the interface. The various modes are described as well as the underlying core technology. To illustrate the effectiveness of the implementation, we present experimental results in American English, UK English and French on a variety of benchmarks, including live data collected during actual use of the system.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-295"
  },
  "os99_eurospeech": {
   "authors": [
    [
     "Els den",
     "Os"
    ],
    [
     "Hans",
     "Jongebloed"
    ],
    [
     "Alice",
     "Stijsiger"
    ],
    [
     "Lou",
     "Boves"
    ]
   ],
   "title": "Speaker verification as a user-friendly access for the visually impaired",
   "original": "e99_1263",
   "page_count": 4,
   "order": 298,
   "p1": "1263",
   "pn": "1266",
   "abstract": [
    "There are few operational services that use speaker verification (SV) as a means to provide secure, yet easy to use access. In this paper we describe the first semi-operational service that is offered by KPN Telecom that has been developed within the framework of the LE-4 project PICASSO. We report on the objective and subjective evaluation of a service that offers free access to directory information in the Netherlands. We concentrate on the enrolment phase, since this is the most critical phase for operational services. Hundred seventy-four persons used this service since the end of 1998; all interaction data are logged for analysis. We have interviewed 26 subjects. We have learned that naive subjects have difficulty in understanding what is expected from them; this holds both for the enrolment and for the access phase. Only half of the subjects finished the enrolment in the minimum number of two calls. The other subjects needed more calls, or did not succeed at all. During access, we observed that more than half of the calls which were refused by SV were calls in which the caller did not behave as (s)he should (screaming, whispering, not entering speech at all, or incomplete utterances). Real false rejects amount to 4 % which makes clear that from the technology point of view improvements are needed. Still, even persons who have occasional trouble using the service have a quite positive opinion about the speech-secured service.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-296"
  },
  "robinson99_eurospeech": {
   "authors": [
    [
     "Tony",
     "Robinson"
    ],
    [
     "Dave",
     "Abberley"
    ],
    [
     "David",
     "Kirby"
    ],
    [
     "Steve",
     "Renals"
    ]
   ],
   "title": "Recognition, indexing and retrieval of british broadcast news with the THISL system",
   "original": "e99_1267",
   "page_count": 4,
   "order": 299,
   "p1": "1267",
   "pn": "1270",
   "abstract": [
    "This paper describes the THISL spoken document retrieval system for British and North American Broadcast News. The system is based on the Abbot large vocabulary speech recognizer and a probabilistic text retrieval system. We discuss the development of a realtime British English Broadcast News system, and its integration into a spoken document retrieval system. Detailed evaluation is performed using a similar North American Broadcast News system, to take advantage of the TREC SDR evaluation methodology. We report results on this evaluation, with particular reference to the effect of query expansion and of automatic segmentation algorithms.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-297"
  },
  "seneff99_eurospeech": {
   "authors": [
    [
     "Stephanie",
     "Seneff"
    ],
    [
     "Raymond",
     "Lau"
    ],
    [
     "Joseph",
     "Polifroni"
    ]
   ],
   "title": "Organization, communication, and control in the GALAXY-II conversational system",
   "original": "e99_1271",
   "page_count": 4,
   "order": 300,
   "p1": "1271",
   "pn": "1274",
   "abstract": [
    "Galaxy-II is the designated initial common architecture for the DARPA Communicator project in the U.S. Its key feature is the ability to control system integration via a run-time executable scripting language. This paper describes our experience in developing complex systems based on the Galaxy-II framework. Our current system consists of four domains and two languages (English and Mandarin). Users can interface with the system in both displayful and displayless modes. Users can switch freely among the various domains in a single conversation, and multiple users can access the system in simultaneous conversations. In addition to the hub script that controls live interaction with users, we have also configured many other hub scripts that permit various batchmode runs, including the capability to reprocess log files through improved versions of the system to measure progress. The hub scripting capability has greatly accelerated our pace of system development, and has allowed us to configure considerably more complex systems than we would have previously envisioned.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-298"
  },
  "pateras99_eurospeech": {
   "authors": [
    [
     "Claudia",
     "Pateras"
    ],
    [
     "Nicolas",
     "Chapados"
    ],
    [
     "Remi",
     "Kwan"
    ],
    [
     "Dominic",
     "Lavoie"
    ],
    [
     "Réal",
     "Tremblay"
    ]
   ],
   "title": "A mixed-initiative natural dialogue system for conference room reservation",
   "original": "e99_1275",
   "page_count": 4,
   "order": 301,
   "p1": "1275",
   "pn": "1278",
   "abstract": [
    "Telephone based spoken dialogue systems have the potential to automate many routine tasks. Yet, to become widely accepted they must be natural, easy to use, efficient, and robust. We introduce the Conference Room Reservation System (CRRS), a mixed-initiative natural dialogue system used routinely by the employees of our lab. The CRRS allows callers to reserve or cancel rooms by simply stating their constraints in a natural way. The system prompts for missing information and offers alternative solutions if the original constraints cannot be satisfied. Our system uses Nortel Networks OpenSpeech continuous speech recognizer with barge-in, a robust natural language parser, and a mixed-initiative discourse manager. We describe the overall system with an emphasis on the discourse manager. We present an analysis of real data collected over several months. We then discuss the issue of defining dialogue performance metrics that are independent of the room occupancy rate.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-299"
  },
  "kuratate99_eurospeech": {
   "authors": [
    [
     "Takaaki",
     "Kuratate"
    ],
    [
     "Kevin G.",
     "Munhall"
    ],
    [
     "Philip E.",
     "Rubin"
    ],
    [
     "Eric",
     "Vatikiotis-Bateson"
    ],
    [
     "Hani",
     "Yehia"
    ]
   ],
   "title": "Audio-visual synthesis of talking faces from speech production correlates",
   "original": "e99_1279",
   "page_count": 4,
   "order": 302,
   "p1": "1279",
   "pn": "1282",
   "abstract": [
    "This paper presents technical refinements and extensions of our system for correlating audible and visible components of speech behavior and subsequently using those correlates to generate realistic talking faces. Introduction of nonlinear estimation techniques has improved our ability to generate facial motion either from the speech acoustics or from orofacial muscle EMG. Also, preliminary evidence is given for the strong correlation found 3D head motion and fundamental frequency (F0). Coupled with improved methods for deriving facial d e-formation parameters from static 3D face scans, more realistic talking faces are now being synth e-sized.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-300"
  },
  "macdonald99_eurospeech": {
   "authors": [
    [
     "John",
     "MacDonald"
    ],
    [
     "Soren",
     "Andersen"
    ],
    [
     "Talis",
     "Bachmann"
    ]
   ],
   "title": "Hearing by eye: visual spatial degradation and the mcgurk effect",
   "original": "e99_1283",
   "page_count": 4,
   "order": 303,
   "p1": "1283",
   "pn": "1286",
   "abstract": [
    "McGurk and MacDonald (Nature 264, 746-748, 1976) discovered that when a discrepancy is created between visual information from lip movements and speech information from the auditory channel then perceivers often report a percept that is neither the auditory or visual stimulus, an illusory response. This McGurk effect is strong evidence that perceivers extract key information about a speech sound from concomitant visual articulation. This study investigates the effects of spatial quantisation on the McGurk effect. Participants (N=20) were presented with incongruous auditory-visual combinations of simple consonant vowel tokens. The visual stimulus was intact or had undergone various degrees of degradation through spatial quantisation. McGurk type responses were significantly influenced by levels of quantisation with more veridical auditory responses at the coarser levels of quantisation. However, even at the coarsest level of quantisation some McGurk type responses were reported.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-301"
  },
  "nankaku99_eurospeech": {
   "authors": [
    [
     "Yoshihiko",
     "Nankaku"
    ],
    [
     "Keiichi",
     "Tokuda"
    ],
    [
     "Tadashi",
     "Kitamura"
    ]
   ],
   "title": "Intensity- and location-normalized training for HMM-based visual speech recognition",
   "original": "e99_1287",
   "page_count": 4,
   "order": 304,
   "p1": "1287",
   "pn": "1290",
   "abstract": [
    "This paper describes an approach to estimating the pa-rameters of continuous density HMMs for visual speech recognition. One of the key issues of image-based vi-sual speech recognition is normalization of lip location and lighting condition prior to estimating the parameters of HMMs. We present an average-intensity and loca-tion normalized training method, in which the normalization process is integrated in the model training. The proposed method provides a theoretically-well-defined algorithm based on a maximum likelihood formulation, hence the likelihood for the training data is guaranteed to increase at each iteration of the normalized training. Experimental results show that the recognition performance can be significantly improved by the normalized training.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-302"
  },
  "potamianos99_eurospeech": {
   "authors": [
    [
     "Gerasimos",
     "Potamianos"
    ],
    [
     "Alexandros",
     "Potamianos"
    ]
   ],
   "title": "Speaker adaptation for audio-visual speech recognition",
   "original": "e99_1291",
   "page_count": 4,
   "order": 305,
   "p1": "1291",
   "pn": "1294",
   "abstract": [
    "In this paper, speaker adaptation is investigated for audiovisual automatic speech recognition (ASR) using the multistream hidden Markov model (HMM). First, audio-only and visual-only HMM parameters are adapted by combining maximum a posteriori and maximum likelihood linear regression adaptation. Subsequently, the audio-visual HMM stream exponents are adapted to better capture the reliability of each modality for the specific speaker, by means of discriminative training. Various visual feature sets are compared, and features based on linear discriminant analysis are demonstrated to result in superior multispeaker and speaker-adapted recognition performance. In addition, visual feature mean normalization is shown to significantly improve visual-only and audio-visual ASR performance. Adaptation experiments on a 49-subject database are reported. On average, a 28% relative word error reduction is achieved by adapting the multi-speaker audiovisual HMM to each subject in the database.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-303"
  },
  "radeau99_eurospeech": {
   "authors": [
    [
     "M.",
     "Radeau"
    ],
    [
     "C.",
     "Colin"
    ]
   ],
   "title": "The role of spatial separation on ventriloquism and mcgurk illusions",
   "original": "e99_1295",
   "page_count": 4,
   "order": 306,
   "p1": "1295",
   "pn": "1298",
   "abstract": [
    "This study aimed at assessing whether two audiovisual interactions (the McGurk and the ventriloquism effects) are affected by different degrees of spatial separation between the auditory and the visual signals. The materials consisted in trains of three audiovisual monosyllables. They were visually displayed in front of the participants head on a TV screen and auditorily played through one of nine hidden loudspeakers (placed every 20°, from straight ahead to 80° to the left or to the right). There were two different conditions in which the speakers face was presented either upright or inverted. Each condition included an identification task (to measure the McGurk effect) and a localization task (to estimate the ventriloquism effect). The ventriloquism effect was maximal at 20° and decreased as the loudspeaker location moved away to the left or to the right, but it was not affected by inverted presentation of the speakers face. The McGurk effect exhibited the reverse pattern.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-304"
  },
  "souza99b_eurospeech": {
   "authors": [
    [
     "Peter de",
     "Souza"
    ],
    [
     "Bhuvana",
     "Ramabhadran"
    ],
    [
     "Yuqing",
     "Gao"
    ],
    [
     "Michael",
     "Picheny"
    ]
   ],
   "title": "Enhanced likelihood computation using regression",
   "original": "e99_1699",
   "page_count": 4,
   "order": 307,
   "p1": "1699",
   "pn": "1702",
   "abstract": [
    "In a rank based large vocabulary continuous speech recognition system [1], the correct leaf is expectedto occupy the top rank positions. An increase in thenumber of times the correct leaf occurs in the top rankpositions translates to an increase in word accuracy.In order to achieve low error rates, we need to discriminate the most confusable incorrect leaves fromthe correct leaf by lowering their ranks. Therefore, the goal here is to increase the likelihood of the cor-rect leaf of a frame, while decreasing the likelihoodsof the confusable leaves. In order to do this, we usethe auxiliary information from the prediction of theneighboring frames to augment the likelihood computation of the current frame. We then use the residual errors in the predictions of neighboring frames todiscriminate between the correct (best) and incorrectleaves of a given frame. In this paper, we presenta new algorithm that incorporates prediction errorlikelihoods into the overall likelihood computation toimprove the rank position of the correct leaf. Experimental results on the Wall Street Journal task and anin-house large vocabulary continuous speech recogni-tion task show a relative accuracy improvements inspeaker-independent performance of 10%.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-305"
  },
  "liu99d_eurospeech": {
   "authors": [
    [
     "Chaojun",
     "Liu"
    ],
    [
     "Xintian",
     "Wu"
    ],
    [
     "Yonghong",
     "Yan"
    ]
   ],
   "title": "High accuracy acoustic modeling using two-level decision-tree based state-tying",
   "original": "e99_1703",
   "page_count": 4,
   "order": 308,
   "p1": "1703",
   "pn": "1706",
   "abstract": [
    "Phonetic decision-tree based acoustic modeling has been widely used in speech recognition systems. However, the assumption that all states clustered in the same leaf node share both their Gaussians and mixture weights restricts the improvement of the acoustic models. In this paper, we propose a new structure called a two-level decision-tree. With this structure we can make better use of training data and improve the model accuracy and robustness. Two-level decision trees provide more flexibility to control the number of parameters. By tuning the balance of the first and second level tree nodes, we can get better performance with even fewer parameters than the traditional decision-tree based approach. Experiments on the Wall Street Journal tasks show that our approach can achieve about a 10% word error rate reduction over the conventional approach.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-306"
  },
  "singh99_eurospeech": {
   "authors": [
    [
     "R.",
     "Singh"
    ],
    [
     "B.",
     "Raj"
    ],
    [
     "Richard M.",
     "Stern"
    ]
   ],
   "title": "Domain adduced state tying for cross-domain acoustic modelling",
   "original": "e99_1707",
   "page_count": 4,
   "order": 309,
   "p1": "1707",
   "pn": "1710",
   "abstract": [
    "In situations when automatic speech recognition (ASR) sys-tems are rapidly deployed for a new task, the availability of within-domain training data may be limited. In such cases one needs to build the ASR systemfrom other, possibly out-of-domain databases. We refer to the process of building ASR systems for one task domain using data from other domains as cross-domain modelling or CDM. Conventional CDM-based systems perform poorly because the disparity between the triphonetic distributions of the training and test domains is not well accounted for. In this paper we describe two techniques to impose the acoustic-phonetic structure of the task domain on acoustic models built from out-of-domain data. The first technique, called Extrinsic CDM, combines decision tree structures obtained froma database close in domain to the task domain with acoustic models that are trained from a third less domain-relevant database. In the second technique, called Intrinsic CDM, the task domain data is used to impose the triphonetic distribution of the task domain on the decision trees built from an out-of-domain large database. Both these techniques result in acoustic models which perform better than conventional CDMmodels.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-307"
  },
  "sankar99_eurospeech": {
   "authors": [
    [
     "Ananth",
     "Sankar"
    ],
    [
     "Venkata Ramana",
     "Rao Gadde"
    ]
   ],
   "title": "Parameter tying and gaussian clustering for faster, better, and smaller speech recognition",
   "original": "e99_1711",
   "page_count": 4,
   "order": 310,
   "p1": "1711",
   "pn": "1714",
   "abstract": [
    "We present a new view of hidden Markov model (HMM) state ty­ing, showing that the accuracy of phonetically tied mixture (PTM) models is similar to, or better than, that of the more typical state­clustered HMM systems. The PTM models require fewer Gaussian distance computations during recognition, and can lead to recog­nition speedups. We describe a per­phone Gaussian clustering algorithm that automatically determines the number of Gaussians for each phone in the PTM model. Experimental results show that this method gives a substantial decrease in the number of Gaussians and a corresponding speedup with little degradation in accuracy. Finally, we study mixture weight thresholding algorithms to drastically decrease the number of mixture weights in the PTM model without degrading accuracy. More than a factor of 10 reduction in mixture weights is achieved with no degradation in performance.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-308"
  },
  "schluter99_eurospeech": {
   "authors": [
    [
     "Ralf",
     "Schlüter"
    ],
    [
     "Wolfgang",
     "Macherey"
    ],
    [
     "Boris",
     "Müller"
    ],
    [
     "Hermann",
     "Ney"
    ]
   ],
   "title": "A combined maximum mutual information and maximum likelihood approach for mixture density splitting",
   "original": "e99_1715",
   "page_count": 4,
   "order": 311,
   "p1": "1715",
   "pn": "1718",
   "abstract": [
    "In this work a method for splitting continuous mixture density hidden Markov models (HMM) is presented. The approach com-bines a model evaluation measure based on the Maximum Mutual Information (MMI) criterion with subsequent standard Max-imum Likelihood (ML) training of the HMMparameters. Experiments were performed on the SieTill corpus for telephone line recorded German continuous digit strings. The proposed split-ting approach performed better than discriminative training with conventional splitting and as good as discriminative training after the new splitting approach.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-309"
  },
  "ammicht99_eurospeech": {
   "authors": [
    [
     "Egbert",
     "Ammicht"
    ],
    [
     "Allen",
     "Gorin"
    ],
    [
     "Tirso",
     "Alonso"
    ]
   ],
   "title": "Knowledge collection for natural language spoken dialog systems",
   "original": "e99_1375",
   "page_count": 4,
   "order": 312,
   "p1": "1375",
   "pn": "1378",
   "abstract": [
    "The development of natural language spoken dialog systems requires collection and labeling of a large set of user-system interactions (knowledge). Ideally, it should be collected from live traffic in the field, since scripted scenarios in a lab typically result in unnatural phraseology. To achieve this, we introduce an extension of the \"Wizard of Oz\", a hidden human agent who oversees the machine side of the interaction in collaboration with the automated dialog manager, unbeknownst to the user. The Wizard can provide a continuum of supervision ranging from explicitly controlling every step to a Wizard-override mode where the machine operates semi-autonomously and the human overrides only when necessary. All interactions are instrumented and entered directly into a database using a standard interface and SQL. The collected data will typically be analyzed in the laboratory to quantify system performance, and to introduce algorithmic improvements prior to a new set of experiments. Over the course of system development iterations, the nature of the Wizard evolves over a continuum of functions from actual control (in the absence of any system knowledge) to passive observer. The user cannot tell the difference.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-310"
  },
  "byron99_eurospeech": {
   "authors": [
    [
     "Donna K.",
     "Byron"
    ]
   ],
   "title": "Improving discourse management in TRIPS-98",
   "original": "e99_1379",
   "page_count": 4,
   "order": 313,
   "p1": "1379",
   "pn": "1382",
   "abstract": [
    "The discourse manager is the component of the TRIPS-98 system that maintains a list of candidate antecedents to be used for re-solving anaphoric referring expressions. The model currently implemented in the system is a monologue-based model of context that has a variety of limitations. This paper discusses changes to the underlying model to make it handle a much broader range of referring expressions. Some of these changes have already been implemented in the system and some are the subject of a long-term project.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-311"
  },
  "wu99_eurospeech": {
   "authors": [
    [
     "Chung-Hsien",
     "Wu"
    ],
    [
     "Gwo-Lang",
     "Yan"
    ],
    [
     "Chien-Liang",
     "Lin"
    ]
   ],
   "title": "Speech act modeling in a spoken dialogue system using fuzzy hidden Markov model and bayes' decision criterion",
   "original": "e99_1383",
   "page_count": 4,
   "order": 314,
   "p1": "1383",
   "pn": "1386",
   "abstract": [
    "In this paper, a corpus-based fuzzy hidden Markov model (FHMM) is proposed to model the speech act in a spoken dialogue system. In the training procedure, 29 FHMMs are defined and trained, each representing one speech act in our approach. In the identification process, the Viterbi algorithm is used to find the top M candidate speech acts. Then Bayes decision criterion, which stores the relationship between the phrase and the speech act, is employed to choose the most probable speech act from the top M speech acts. In order to evaluate the proposed method, a spoken dialogue system for air travel information service is investigated. The experiments were carried out using a test database from 25 speakers (15 male and 10 female). There are 120 dialogues, which contains 725 sentences in the test database. The experimental results show that the correct response rate can achieve about 82.7% using the FHMM and the Bayes decision criterion.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-312"
  },
  "ehrlich99_eurospeech": {
   "authors": [
    [
     "Ute",
     "Ehrlich"
    ]
   ],
   "title": "Task hierarchies representing sub-dialogs in speech dialog systems",
   "original": "e99_1387",
   "page_count": 4,
   "order": 315,
   "p1": "1387",
   "pn": "1390",
   "abstract": [
    "The application of a speech dialog system is modeled as a hierarchy of sub-tasks in order to - have the possibility of structuring complex dialogs into sub-dialogs and thus restricting the lexicon to be recognized at each dialog state, - to model possible sequences of sub-dialogs, - to enable several instances of the same sub-tasks, - to make references and/or switches to prior discussed sub-tasks possible. Thus, also more complex applications can be modeled and handled without losing speech recognition accuracy or getting lost in different dialog themes. As these restrictions are quite natural users will not perceive any limitation.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-313"
  },
  "hirasawa99_eurospeech": {
   "authors": [
    [
     "Jun-Ichi",
     "Hirasawa"
    ],
    [
     "Mikio",
     "Nakano"
    ],
    [
     "Takeshi",
     "Kawabata"
    ],
    [
     "Kiyoaki",
     "Aikawa"
    ]
   ],
   "title": "Effects of system barge-in responses on user impressions",
   "original": "e99_1391",
   "page_count": 4,
   "order": 316,
   "p1": "1391",
   "pn": "1394",
   "abstract": [
    "When designing a spoken dialogue system, in particular a real-time one, not only what the system responds but also whenit responds need to be considered. This paper focuses on when the system should appropriately respond with backchannels, and reports an experiment that compared two response-time conditions: the immediate response and the orderly response. The results of the experiment show that the immediate barge-in backchannels can cause some subjects to have negative impressions of the system. This implies that the orderly response strategy is better and less risky than the simple immediate strategy and that some considerations might be required in designing a better barge-in response.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-314"
  },
  "lopezcozar99_eurospeech": {
   "authors": [
    [
     "R.",
     "López-Cózar"
    ],
    [
     "Antonio J.",
     "Rubio"
    ],
    [
     "P.",
     "García"
    ],
    [
     "J. C.",
     "Segura"
    ]
   ],
   "title": "A new word-confidence threshold technique to enhance the performance of spoken dialogue systems",
   "original": "e99_1395",
   "page_count": 4,
   "order": 317,
   "p1": "1395",
   "pn": "1398",
   "abstract": [
    "Spoken dialogue systems generally use one or two confidence thresholds during speech recognition. A confidence value assigned to a word represents the recognizers confidence in the correct recognition of the word. If the confidence value is under a threshold then the word is considered a recognition error and the system must ask the user to re-enter it. Alternatively, the system can ask for a confirmation from the user. Environmental conditions and peculiarities of the speakers voice can change from one dialogue to another, so that it is necessary to decide the most appropriate value for the confidence threshold. If the selected value is too low, the words that are wrongly inserted by the recognizer may be considered correctly recognized. On the other hand, if the selected value is too high, even the words actually uttered by the user can be considered recognition errors, or words that must be confirmed. In this paper we present an experimental strategy to automatically select the most appropriate value for the confidence threshold. This strategy has been applied to the dialogue system we have developed, which aims to deal with telephone-based fast food queries and orders. We present the results obtained and indicate possibilities for future work .\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-315"
  },
  "lavelle99_eurospeech": {
   "authors": [
    [
     "C. Alexia",
     "Lavelle"
    ],
    [
     "Martine de",
     "Calmés"
    ],
    [
     "Guy",
     "Pérennou"
    ]
   ],
   "title": "Confirmation strategies to improve correction rates in a telephonic inquiry dialogue system",
   "original": "e99_1399",
   "page_count": 4,
   "order": 318,
   "p1": "1399",
   "pn": "1402",
   "abstract": [
    "In the framework of ARISE project (Automatic Railways Inquiry Systems for Europe) we developed DEMON, a real time spontaneous speech dialogue system allowing mixed initiative. This system gives information for train connection. On this system three different confirmation strategies were developed and compared with regard to erroneous confirmation rates. The first strategy was implicit confirmation question oriented. Those questions were used as widely as possible. Refutation for those questions appeared to be quite low. The reordering of parameters in implicit questions to match the order of mental\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-316"
  },
  "niimi99_eurospeech": {
   "authors": [
    [
     "Yasuhisa",
     "Niimi"
    ],
    [
     "Takuya",
     "Nishimoto"
    ]
   ],
   "title": "Mathematical analysis of dialogue control strategies",
   "original": "e99_1403",
   "page_count": 4,
   "order": 319,
   "p1": "1403",
   "pn": "1406",
   "abstract": [
    "In this paper we describe a quantitative relation between the efficiency of dialogue control strategies, we measure by the average number of exchanges taken during a dialogue, and the performance of a speech recognition system used in a spoken dialogue system. We consider four dialogue control strategies: (1) direct confirmation, (2) indirect confirmation, (3) multiple item answer followed by direct confirmations, and (4) multiple item answer followed by multiple item confirmations. If n information items are necessary to execute a task, a status of a dialogue is represented by a triplet (u; k; c) (u+k+c=n), where u, k and c are the numbers of unknown, known not yet confirmed, and confirmed items respectively. The mathematical analyses of the strategies are based on the fact that the state transition of a dialogue can be represented by a Markov process in which a space of states are a set of triplets (u; k; c), and state transition probabilities are decided by a speech recognition rate and user's behaviors.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-317"
  },
  "ocelikova99_eurospeech": {
   "authors": [
    [
     "Jana",
     "Ocelikova"
    ],
    [
     "Vaclav",
     "Matousek"
    ]
   ],
   "title": "Processing of anaphoric and elliptic sentences in a spoken dialog system",
   "original": "e99_1407",
   "page_count": 4,
   "order": 320,
   "p1": "1407",
   "pn": "1410",
   "abstract": [
    "This paper presents an approach to processing of ambiguous requests in spoken dialogs in information train timetable service systems. The main problem of human-machine interaction is in the fact, that speaking human does not express some essential information, because it results from the receivers knowledge and the dialog context. Therefore the real understanding system have to combine the incomplete semantic information extracted from the analyzed utterance with the previous data stored in the dialog history path. This is provided in the developed dialog system by the dialog module. Its is based on the representation and storage of facts extracted from the previous steps of dialog (dialog path), and on the creation of simple knowledge bases containing the reasoning rules linking the meaning detected in the analyzed users utterance with the facts of the dialog path.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-318"
  },
  "papineni99_eurospeech": {
   "authors": [
    [
     "K. A.",
     "Papineni"
    ],
    [
     "Salim",
     "Roukos"
    ],
    [
     "T.",
     "Ward"
    ]
   ],
   "title": "Free-flow dialog management using forms",
   "original": "e99_1411",
   "page_count": 4,
   "order": 321,
   "p1": "1411",
   "pn": "1414",
   "abstract": [
    "This paper describes a task oriented, mixed initiative dialog manager. To perform a task according to a user's request, the dialog manager needs many pieces of information. It may have to query the user for missing information, clarify ambiguous information, inherit information from context, and confirm information before performing critical tasks. We describe a form-based framework for freeow dialog management that deals with these issues. Here, a form corresponds to a specific task in the domain. To allow users to address any task any time, the dialog manager determines the best form that corresponds to the user's intention on every turn. Our dialog manager associates a score to each form as a measure of matching between the form and the user's input. Then the best scoring form corresponds to user's intention.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-319"
  },
  "ries99_eurospeech": {
   "authors": [
    [
     "Klaus",
     "Ries"
    ]
   ],
   "title": "Towards the detection and description of textual meaning indicators in spontaneous conversations",
   "original": "e99_1415",
   "page_count": 4,
   "order": 322,
   "p1": "1415",
   "pn": "1418",
   "abstract": [
    "The description of textual and stylistic features has so far been largely neglected in the empirical study of conversational speech. In this paper we want to make a couple of strong initial points towards the use textual meaning and stylistic features in language engineering: First of all we want to show that there are other besides the traditional features in spontaneous speech that are worth studying and that might reveal good information: These are related to the interactive nature of the language and to the distribution of the most frequent (non-topical) words. Secondly we want to present two tasks that we have chosen as our benchmark and present detection results. Finally we want to motivate how this can be used in information access applications.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-320"
  },
  "sturm99_eurospeech": {
   "authors": [
    [
     "Janienke",
     "Sturm"
    ],
    [
     "Els den",
     "Os"
    ],
    [
     "Lou",
     "Boves"
    ]
   ],
   "title": "Dialogue management in the dutch ARISE train timetable information system",
   "original": "e99_1419",
   "page_count": 4,
   "order": 323,
   "p1": "1419",
   "pn": "1422",
   "abstract": [
    "We present the evaluation of the most recent version of the Dutch ARISE train timetable information system. The original version of this spoken dialogue system has been adjusted according to the findings of two user tests [1,2]. The new version applies a mixture of implicit and explicit confirmation of information items, based on confidence measures. In addition, the negotiation part of the dialogue tells the user explicitly what he can ask. Furthermore, the exceptions handling was made very explicit. The new dialogue has been evaluated by 25 experts and by 200+ anonymous calls to the system. To be able to compare the two versions of the system, the same scenarios as in [1] were used. It was shown that the mixture of implicit and explicit confirmation results in shorter dialogues and in slightly higher dialogue success rates. Also, we observed a better performance in the negotiation part of the dialogue. However, the shortcomings of working with complicated scenarios are once again made clear.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-321"
  },
  "krahmer99_eurospeech": {
   "authors": [
    [
     "Emiel",
     "Krahmer"
    ],
    [
     "Marc",
     "Swerts"
    ],
    [
     "Mariet",
     "Theune"
    ],
    [
     "Mieke",
     "Weegels"
    ]
   ],
   "title": "Problem spotting in human-machine interaction",
   "original": "e99_1423",
   "page_count": 4,
   "order": 324,
   "p1": "1423",
   "pn": "1426",
   "abstract": [
    "In human-human communication, dialogue participants are con-tinuously sending and receiving signals on the status of the inform-ation being exchanged. We claim that if spoken dialogue systems were able to detect such cues and change their strategy accordingly, the interaction between user and systemwould improve. Therefore, the goals of the present study are as follows: (i) to find out which positive and negative cues people actually use in human-machine interaction in response to explicit and implicit verification questions and (ii) to see which (combinations of) cues have the best predictive potential for spotting the presence or absence of problems. It was found that subjects systematically use negative/marked cues (more words, marked word order, more repetitions and corrections, less new information etc.) when there are communication problems. Using precision and recall matrices it was found that various combinations of cues are accurate problem spotters. This kind of information may turn out to be highly relevant for spoken dia-logue systems, e.g., by providing quantitative criteria for changing the dialogue strategy or speech recognition engine.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-322"
  },
  "lin99b_eurospeech": {
   "authors": [
    [
     "Bor-shen",
     "Lin"
    ],
    [
     "Hsin-min",
     "Wang"
    ],
    [
     "Lin-shan",
     "Lee"
    ]
   ],
   "title": "Consistent dialogue across concurrent topics based on an expert system model",
   "original": "e99_1427",
   "page_count": 4,
   "order": 325,
   "p1": "1427",
   "pn": "1430",
   "abstract": [
    "There have been many working spoken dialogue systems, but very few of them are able to handle concurrent topics consistently if the user freely surfs among different topics at any time, because it is really difficult to resolve semantic ambiguities and check the knowledge consistencies among correlated topics. The issues include not only how to handle the information across multiple topics and domains with shared slots, but how to infer a reasonable dialogue state considering the new information or inconsistencies among knowledge structures after some correlated topics are activated. In this paper, a plan-based dialogue control mechanism that is capable of handling the very complicated dialogue problem is proposed. This mechanism can keep the knowledge consistent among multiple topics and domains when the topic is switched. Also, by representing the problem-solving procedures of different goals as loadable trees, the dialogue strategies are easy to maintain and modify, and can be dynamically adapted to the users during the dialogue.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-323"
  },
  "chapman99_eurospeech": {
   "authors": [
    [
     "Thomas M.",
     "Chapman"
    ],
    [
     "C. S.",
     "Xydeas"
    ]
   ],
   "title": "Secondary codebook storage quantisation",
   "original": "e99_1431",
   "page_count": 4,
   "order": 326,
   "p1": "1431",
   "pn": "1434",
   "abstract": [
    "Efficient reduction of storage and complexity demands in VQ and MQ systems is a key issue when developing new, more powerful compression algorithms. Secondary Storage Quantisation (SSQ) is capable of drastically reducing VQ storage through an efficient representation of codebook elements. Rather than the conventional fixed or floating point representation, codebook elements are quantised using a set of secondary codebooks\" and represented as a set of quantisation indices. The number of bits required for these indices is relatively small and hence the amount of storage required for codebook representation is reduced. The potential of SSQ for codebook compression is demonstrated in a Split Matrix Quantisation (SMQ) application. A reduction of 65 75 % in the amount of memory required for SMQ codebooks is achieved.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-324"
  },
  "edmondson99_eurospeech": {
   "authors": [
    [
     "W. H.",
     "Edmondson"
    ],
    [
     "D. J.",
     "Iskra"
    ],
    [
     "P.",
     "Kienzle"
    ]
   ],
   "title": "Pseudo-articulatory representations: promise, progress and problems",
   "original": "e99_1435",
   "page_count": 4,
   "order": 327,
   "p1": "1435",
   "pn": "1438",
   "abstract": [
    "Pseudo-Articulatory Representations (PARs) have been proposed and discussed in relation to speech processing with results reported for both synthesis and recognition. PARs are derived from linguistic specifications of articulatory activity which are both abstract and idealized. The abstractions and idealizations permit the linguistic generality to be distinguished from the articulatory reality; this is what we need in speech processing. PARs attempt to retain the linguistic generality whilst also gaining some realism through adoption of continuous articulatory feature values; the latter permits mapping to acoustic values. PARs promise a way of mapping acoustic (and other) data onto linguistic specification of speech (and vice versa), and although enough progress has been made to demonstrate the concept various problems remain for further study.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-325"
  },
  "gao99_eurospeech": {
   "authors": [
    [
     "Ge",
     "Gao"
    ],
    [
     "P. C.",
     "Ching"
    ]
   ],
   "title": "A 1.7KBPS waveform interpolation speech coder using decomposition of pitch cycle waveform",
   "original": "e99_1439",
   "page_count": 4,
   "order": 328,
   "p1": "1439",
   "pn": "1442",
   "abstract": [
    "In this paper, we propose a low bit rate waveform interpolation speech coder where the novelty lies with an effective decomposition method of pitch cycle waveform(PCW). PCWs exhibit very different perceptual characteristics in different frequency bands. For frequency components below 1kHz, they are quantized using Variable Dimensional Vector Quantization(VDVQ) scheme. Hereby retaining the fine harmonic structure of the speech signal. For the upper frequency band ranging from 1 to 4kHz, the formant structure is perceptually more dominant. It is therefor desirable to capture the formant peaks in order to maintain high speech quality. In this case, we employ a sparse frequency representation(SFR) method. A 1.7kbps speech coder has been developed by this technique and the quality of the output speech is perceptually good.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-326"
  },
  "gottesman99_eurospeech": {
   "authors": [
    [
     "Oded",
     "Gottesman"
    ],
    [
     "Allen",
     "Gersho"
    ]
   ],
   "title": "Enhanced analysis-by-synthesis waveform interpolative coding at 4 KBPS",
   "original": "e99_1443",
   "page_count": 4,
   "order": 329,
   "p1": "1443",
   "pn": "1446",
   "abstract": [
    "This paper presents an Enhanced analysis-by-synthesis (AbS) Waveform Interpolative (EWI) speech coder at 4 kbps. The system incorporates novel features such as: AbS quantization of the slowly evolving waveform (SEW), AbS vector quantization (VQ) of the dispersion phase, a special pitch search for transitions, and switched-predictive analysis-by-synthesis gain VQ. Subjective quality tests indicate that it exceeds MPEG-4 at 4 kbps and of G.723.1 at 5.3 kbps, and it is slightly better than G.723.1 at 6.3 kbps.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-327"
  },
  "gortz99_eurospeech": {
   "authors": [
    [
     "Norbert",
     "Görtz"
    ]
   ],
   "title": "Joint source-channel decoding by channel-coded optimal estimation (CCOE) for a CELP speech codec",
   "original": "e99_1447",
   "page_count": 4,
   "order": 330,
   "p1": "1447",
   "pn": "1450",
   "abstract": [
    "A new algorithm called Channel-Coded Optimal Estimation (CCOE) is adapted to a CELP speech codec. The CCOE-algorithm performs joint source-channel decodingby optimal estimation using bit-reliability informations(soft-bits) and source statistics. The basic algorithm fora simple transmission scenario with a single scalar quantizer has been recently stated by the author. In this paper the basic idea is extended for the use in a CELPspeech-codec that was developed for enhanced speechtransmission in the GSM mobile radio channel. Simulation results based on informal listening tests are givenwhich show that the new algorithm achieves substantialgains over separate error correction with hard decisionsprior to source decoding and classical techniques for error detection and bad frame handling as widely used in the present mobile-radio systems.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-328"
  },
  "li99b_eurospeech": {
   "authors": [
    [
     "Chunyan",
     "Li"
    ],
    [
     "Allen",
     "Gersho"
    ],
    [
     "Vladimir",
     "Cuperman"
    ]
   ],
   "title": "Analysis-by-synthesis low-rate multimode harmonic speech coding",
   "original": "e99_1451",
   "page_count": 4,
   "order": 331,
   "p1": "1451",
   "pn": "1454",
   "abstract": [
    "This paper presents an analysis-by-synthesis multimode harmonic coder(AbS-MHC) that employs new techniques to improve both the speech model accuracy and the parameter estimation robustness in the low rate harmonic coding framework. To improve the speech model accuracy, an enhanced frequency domain transition model is used in conjunction with the sinusoidal model based harmonic coding of voiced/unvoiced speech signals. To achieve robust parameter estimation, a generalized analysis-by-synthesis parameter estimation scheme in the harmonic coding framework is proposed. This scheme uses a time scale signal modification technique to allow for waveform matching in harmonic coding. This concept is demonstrated in our AbS-MHC coder with a specific method for efficient closed-loop pitch estimation and speech classification. The speech quality of the unquantized AbS-MHC coder is better than the 6.3 kbps G.723 quality.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-329"
  },
  "lois99_eurospeech": {
   "authors": [
    [
     "László",
     "Lois"
    ]
   ],
   "title": "Variable length coding of transformed LSF coefficients",
   "original": "e99_1455",
   "page_count": 4,
   "order": 332,
   "p1": "1455",
   "pn": "1458",
   "abstract": [
    "In this paper, the use of Karhunen-Loeve transform (KLT) and discrete cosine transform (DCT) is studied for encoding of the line spectrum frequency (LSF) parameters at variable bit rate (VBR). For VBR coding, scalar quantization (SQ) is used with Huffman coding. The basic idea in developing these schemes is using linear transform to exploit the strong intraframe and interframe correlation of LSF parameters to increase the performance of the SQ. First, several adaptive KLT schemes are developed to increase the efficiency of the KLT. To reduce the interframe correlation efficiently, two-dimensional transforms and moving average prediction of the transformed coefficients are also investigated. It is shown that these schemes introduce as good as or better performance in the examined bit rates compared to other methods in the field of LSF coding.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-330"
  },
  "mayrench99_eurospeech": {
   "authors": [
    [
     "R.",
     "Mayrench"
    ],
    [
     "D.",
     "Malah"
    ]
   ],
   "title": "Low bit-rate speech coding using quantization of variable length segments",
   "original": "e99_1459",
   "page_count": 4,
   "order": 333,
   "p1": "1459",
   "pn": "1462",
   "abstract": [
    "This paper describes a new segmentation and quantization technique for low bit-rate speech coders. Bit-rate reduction is achieved by combining segmentation and quantization, where a segment consists of one or more adjacent frames. The algorithm for selecting and quantizing segments from a pre-determined number of frames extends the frame-based trellis techniques. It models the input speech as a sequence of variable length segments with the option to interpolate frames in skipped segments. The new algorithm, denoted as Trellis Segmentation-Quantization (TSQ), reduces the bit-rate needed for spectral envelope representation, with only a small degradation in log-spectral distance values. Experimental results show that TSQ achieves a lower spectral distance than alternate frame transmission, matrix quantization and trellis based frame selection and interpolation.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-331"
  },
  "martin99_eurospeech": {
   "authors": [
    [
     "Rainer",
     "Martin"
    ],
    [
     "Hong-Goo",
     "Kang"
    ],
    [
     "Richard V.",
     "Cox"
    ]
   ],
   "title": "Low delay analysis/synthesis schemes for joint speech enhancement and low bit rate speech coding",
   "original": "e99_1463",
   "page_count": 4,
   "order": 334,
   "p1": "1463",
   "pn": "1466",
   "abstract": [
    "In this contribution we discuss methods to reduce the algorithmic delay of joint speech enhancement and low bit rate speech coding algorithms. We introduce a novel overlap/add scheme which can reduce the overall delay of the joint system for some speech coders considerably. The new scheme takes advantage of the fact that some low bit rate coders store a significant amount of look-ahead samples in their input buffer. The look-ahead samples often have less in uence on the parameter estimation than the samples in the current frame. Therefore, these samples might be used by the coder without being fully overlapped or reconstructed and with only little effect on the quality of the coded speech. The concept has been successfully used for joint enhancement and coding using the 2.4 kbps versions of the MELP and the WI speech coders.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-332"
  },
  "oliva99_eurospeech": {
   "authors": [
    [
     "Oscar",
     "Oliva"
    ],
    [
     "Marcos",
     "Faúndez-Zanuy"
    ]
   ],
   "title": "A comparative study of several ADPCM schemes with linear and nonlinear prediction",
   "original": "e99_1467",
   "page_count": 4,
   "order": 335,
   "p1": "1467",
   "pn": "1470",
   "abstract": [
    "In this paper we compare several ADPCM schemes with nonlinear prediction based on neural nets with the classical ADPCM schemes based on several linear prediction schemes. Main studied variations of the ADPCM scheme with adaptive quantization (2 to 5 bits) are: -forward vs backward -sample adaptive vs block adaptive.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-333"
  },
  "ohmura99_eurospeech": {
   "authors": [
    [
     "H.",
     "Ohmura"
    ],
    [
     "K.",
     "Tanaka"
    ]
   ],
   "title": "Segmental feature extraction and coding for speech synthesis",
   "original": "e99_1471",
   "page_count": 4,
   "order": 336,
   "p1": "1471",
   "pn": "1474",
   "abstract": [
    "This paper describes a segmental feature extraction and speech coding method in an acoustic-articulatory domain using nomograms that represent a mapping between formant frequencies and articulatory parameters. The vocal tract model is a modified Fant model, in which we newly introduced a parameter for successively adjusting vocal tract lengths. We investigated first the relationship between formant contours and those of articulatory parameters and found the effectiveness of the articulatory domain for organizing acoustic-phonetic features with little dependency upon languages. Next, we applied the method to the low bit rate coder and confirmed that good quality speech synthesis was achieved in the condition of 18 bit used for articulatory code words.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-334"
  },
  "pelaezmoreno99_eurospeech": {
   "authors": [
    [
     "C.",
     "Peláez-Moreno"
    ],
    [
     "F.",
     "Díaz-de-María"
    ]
   ],
   "title": "Backward adaptive RBF-based hybrid predictors for CELP-type coders at medium bit-rates",
   "original": "e99_1475",
   "page_count": 4,
   "order": 337,
   "p1": "1475",
   "pn": "1478",
   "abstract": [
    "Nonlinear prediction is a natural way to increase the quality of speech coders. Several approaches have been recently proposed in this direction ([1,2,3,4] are some examples) and most of them use neural networks as predictors. Nevertheless, the computational cost due to the network training is very high, since it usally involves a gradient descent-based nonlinear optimization process. In this paper we propose some improvements of our previous work reported in [3], all of them aiming at reducing the computational cost. Our predictor can be used in CELP-type coders and provides a 0.6 dB increase of the SEGSNR with respect to conventional CELP coders.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-335"
  },
  "sercov99_eurospeech": {
   "authors": [
    [
     "Valentin V.",
     "Sercov"
    ],
    [
     "Alexander A.",
     "Petrovsky"
    ]
   ],
   "title": "An improved speech model with allowance for time-varying pitch harmonic amplitudes and frequencies in low bit-rate MBE coders",
   "original": "e99_1479",
   "page_count": 4,
   "order": 338,
   "p1": "1479",
   "pn": "1482",
   "abstract": [
    "The paper presents a more accurate model of speech synthesis which can be applied to the low bit-rate vocoders producing synthetic speech of high quality. Speech is described as a combination of the periodic and the noise components within given time frame. The periodic spectrum that appears in the voiced bands is a set of pitch harmonics, the amplitude and the frequency of which linearly changes from one window of analysis to another. The noise component is described as a difference between the input speech signal and the synthesized periodic spectrum. The system of functions employed in the analysis procedure is determined taking into account the changes in frequencies of pitch harmonics. This allows to reproduce the periodic spectrum with minimal distortion and separate it from the noise component. The paper includes results of experiments showing advantages of the suggested model in comparison with the existing ones.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-336"
  },
  "petrinovic99_eurospeech": {
   "authors": [
    [
     "Davor",
     "Petrinovic"
    ],
    [
     "Davorka",
     "Petrinovic"
    ]
   ],
   "title": "Sparse vector linear prediction matrices with multidiagonal structure",
   "original": "e99_1483",
   "page_count": 4,
   "order": 339,
   "p1": "1483",
   "pn": "1486",
   "abstract": [
    "A modification of the classical vector linear prediction (VLP) problem is presented. The introduced technique called the sparse VLP (sVLP) is based on the assumption that each component of a single LSF vector is highly correlated only to a few neighboring vector components of consecutive vectors, while the correlation between distant vector components can be ignored. This leads to simplification of predictor matrices in a way that for a chosen number of neighboring components, predictor matrices obtain multidiagonal form. It is shown that prediction gain resulting from sVLP is only slightly lower than for the case of full matrix predictors but with significant reduction of computation, both for coding and predictor design.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-337"
  },
  "stefanovic99_eurospeech": {
   "authors": [
    [
     "M.",
     "Stefanovic"
    ],
    [
     "A.",
     "Kondoz"
    ]
   ],
   "title": "Source-dependent variable rate speech coding below 3 KBPS",
   "original": "e99_1487",
   "page_count": 4,
   "order": 340,
   "p1": "1487",
   "pn": "1490",
   "abstract": [
    "This work addresses the need for high quality, very low bit rate speech compression algorithms that can be utilised in many forthcoming multimedia applications. The speech coding algorithm proposed in this paper is a variable rate system based on the adaptive source-driven frame length scheme. Maximum speech compression is achieved for long-term steady-state speech and non-speech (silence and unvoiced) conditions. In addition, shorter frame sizes are used to code those difficult-to-model speech transitions, thus improving the overall perceptual quality when compared with traditional fixed rate schemes. This codec may be used for implementing various voice communication systems, such as Voice Store and Forward systems and digital answering machines, or to augment low bit rate integrated digital packet networks.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-338"
  },
  "chen99c_eurospeech": {
   "authors": [
    [
     "Xiaoping",
     "Chen"
    ],
    [
     "Yantao",
     "Song"
    ],
    [
     "Tiecheng",
     "Yu"
    ]
   ],
   "title": "A novel speech coding approach based on half-wave vector quantization *",
   "original": "e99_1491",
   "page_count": 4,
   "order": 341,
   "p1": "1491",
   "pn": "1494",
   "abstract": [
    "In this paper, a novel waveform coding approach based on half-wave vector quantization is presented. The input speech signal is divided into frames, each of which is classified as silence, unvoiced and voiced. The voiced category is subdivided into 64 sub-categories according to the length of each half-wave vector. The unvoiced category is subdivided into 16 sub-categories according to the average zero-crossing rate of each frame. For each type of vectors, a set of codebook is generated by using training data. Then each kind of sub-category is encoded, transmitted, and decoded with its own particular bit allocation configuration. This variable bit coding approach can operate at medium rate and provide acceptable-to-good speech quality.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-339"
  },
  "zolfaghari99_eurospeech": {
   "authors": [
    [
     "Parham",
     "Zolfaghari"
    ],
    [
     "Tony",
     "Robinson"
    ]
   ],
   "title": "Speech coding using mixture of gaussians polynomial model",
   "original": "e99_1495",
   "page_count": 4,
   "order": 342,
   "p1": "1495",
   "pn": "1498",
   "abstract": [
    "We have investigated a novel method of spectral estimation based on mixture of Gaussians in a sinusoidal analysis and synthesis framework. After quantisation of this parametric scheme a fixed frame-rate coder operating at a bit-rate of around 2.4 kbits/s has been developed. This paper describes an extension to this spectral model based on constraining the parameters of the mixture of Gaussians to be on a polynomial trajectory over a segment of speech data. This is referred to as the mixture of Gaussians polynomial model (MGPM). In order to realise a segmental coder, dynamic programming over the utterance is performed. The segmental representation of the spectra results in a log-likelihood score over a segment which is used as the cost function in the dynamic programming algorithm. Speech coding components such aspitch, voicing and gain are described segmentally. A number of segmental coders are presented with bit-rates in the range of 350 to 650 bits/s. These coders offer good and intelligible coded speech evaluated using DRT scoring at these bit-rates.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-340"
  },
  "chucarroll99_eurospeech": {
   "authors": [
    [
     "Jennifer",
     "Chu-Carroll"
    ]
   ],
   "title": "Form-based reasoning for mixed-initiative dialogue management in information-query systems",
   "original": "e99_1519",
   "page_count": 4,
   "order": 343,
   "p1": "1519",
   "pn": "1522",
   "abstract": [
    "Previous work on practical dialogue systems focused mainly on developing robust systems that operate in specific domains. However, these systems are often heavily handcrafted and are thus difficult to port to new domains. This paper describes general mixed-initiative dialogue management strategies that are decoupled from domain-dependent task specifications. These strategies are used to select appropriate system actions in response to user utterances, focusing on situations in which user queries are inconsistent, ambiguous, or underspecified. If the dialogue manager fails to resolve the problem using its own knowledge or information from the dialogue history, it adopts one of three query refinement strategies to engage the user in subdialogues to solicit the required information. An evaluation of the dialogue management strategies on a corpus of 263 unseen queries shows that the system is able to correctly answer 90.2\\% of the queries after redirecting 3\\% of the calls to a human operator.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-341"
  },
  "dahlback99_eurospeech": {
   "authors": [
    [
     "Nils",
     "Dahlbäck"
    ],
    [
     "Arne",
     "Jönsson"
    ]
   ],
   "title": "Knowledge sources in spoken dialogue systems",
   "original": "e99_1523",
   "page_count": 4,
   "order": 344,
   "p1": "1523",
   "pn": "1526",
   "abstract": [
    "We describe how the architecture of the modularized lin-lin dialogue manager can be augmented to handle thecases where the user's initial information request is underspecified, and where therefore the system needs to askthe user about the missing pieces of information. In manyexisting spoken dialogue systems this has been handledby a so-called task model, which has been embedded inthe dialogue move management. The present paper makestwo points. First, that the term 'task' is ambiguous, sinceit can refer to the user's underlying reasons for engagingin the dialogue, to the user's information seeking activity,and to the system's actions when preparing for providingthe user with an answer to the posed question. Second, weargue that this aspect of dialogue management should bemanaged by a separate module of the system, both becauseour previous work has shown that it is not necessary in allcases, and because it is conceptually different from general interaction management knowledge. For these reasonswe advocate using a separate knowledge module called anInformation Specification Form for managing these cases.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-342"
  },
  "os99b_eurospeech": {
   "authors": [
    [
     "Els den",
     "Os"
    ],
    [
     "Lou",
     "Boves"
    ],
    [
     "Lori",
     "Lamel"
    ],
    [
     "Paolo",
     "Baggia"
    ]
   ],
   "title": "Overview of the ARISE project",
   "original": "e99_1527",
   "page_count": 4,
   "order": 345,
   "p1": "1527",
   "pn": "1530",
   "abstract": [
    "The LE-3 project ARISE (Automatic Railway Information Sys-tems for Europe) ran from October 1996 through December 1998. Four prototypes of train timetable information systems for three different languages were developed, tested, and validated: Italian (based on technology developed by CSELT), French (two systems, one based on Philips technology, the other on technology by LIMSI), and Dutch (based on Philips technol-ogy). The goal of ARISE was to improve the basic technology and to enhance our general understanding of the issues involved in actual deployment of spoken language dialogue systems for restricted domain information. This paper summarises the main findings of the project in terms of speech recognition, dialogue state dependent language modelling, dialogue control, information presentation, system output, evaluation of spoken dialogue systems, and operational issues.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-343"
  },
  "rudnicky99_eurospeech": {
   "authors": [
    [
     "Alexander I.",
     "Rudnicky"
    ],
    [
     "E.",
     "Thayer"
    ],
    [
     "Paul",
     "Constantinides"
    ],
    [
     "C.",
     "Tchou"
    ],
    [
     "R.",
     "Shern"
    ],
    [
     "Kevin",
     "Lenzo"
    ],
    [
     "W.",
     "Xu"
    ],
    [
     "A.",
     "Oh"
    ]
   ],
   "title": "Creating natural dialogs in the carnegie mellon communicator system",
   "original": "e99_1531",
   "page_count": 4,
   "order": 346,
   "p1": "1531",
   "pn": "1534",
   "abstract": [
    "The Carnegie Mellon Communicator system helps users create complex travel itineraries through a conversational interface. Itineraries consist of (multi-leg) flights, hotel and car reser-vations and are built from actual travel informa-tion for North America, obtained from the Web. The system manages dialog using a schema-based approach. Schemas correspond to major units of task information (such as a flight leg) and define conversational topics, or foci of interaction, meaningful to the user.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-344"
  },
  "rosset99_eurospeech": {
   "authors": [
    [
     "Sophie",
     "Rosset"
    ],
    [
     "Samir",
     "Bennacef"
    ],
    [
     "Lori",
     "Lamel"
    ]
   ],
   "title": "Design strategies for spoken language dialog systems",
   "original": "e99_1535",
   "page_count": 4,
   "order": 347,
   "p1": "1535",
   "pn": "1538",
   "abstract": [
    "The development of task-oriented spoken language dialog system requires expertise in multiple domains including speech recognition, natural spoken language understanding and generation, dialog management and speech synthesis. The dialog manager is the core of a spoken language dialog system, and makes use of multiple knowledge sources. In this contribution we report on our methodology for developing and testing different strategies for dialog management, drawing upon our experience with several travel information tasks. In the LIMSI ARISE system for train travel information we have implemented a 2-level mixed-initiative dialog strategy, where the user has maximum freedom when all is going well, and the system takes the initiative if problems are detected. The revised dialog strategy and error recovery mechanisms have resulted in a 5-10% increase in dialog success depending upon the word error rate.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-345"
  },
  "amodio99_eurospeech": {
   "authors": [
    [
     "A.",
     "Amodio"
    ],
    [
     "Gang",
     "Feng"
    ]
   ],
   "title": "A wideband speech coder based on harmonic coding at 16KBS",
   "original": "e99_1539",
   "page_count": 4,
   "order": 348,
   "p1": "1539",
   "pn": "1542",
   "abstract": [
    "In this paper, we present an improved harmonic coder for the wideband. In an effort to improve spectrum modelling with harmonic representation, we include in the structure, a post error spectrum modelling that is essentially useful for noisy signals or mixed spectra of high pitch voices. The bit constrained error spectrum modelling procedure coupled with an efficient quantization of the harmonic model parameters allow us to develop a low fixed bit rate coder. The detailed structure of all the analysis, synthesis and quantization steps are outlined for the developed 16kbs coder on the [50-7000 Hz] bandwidth. Informal listening tests show that the quality obtained is comparable to the ITU_T Recommendation G722 at 48 kbs\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-346"
  },
  "bernard99_eurospeech": {
   "authors": [
    [
     "Alexis",
     "Bernard"
    ],
    [
     "Abeer",
     "Alwan"
    ]
   ],
   "title": "Perceptually based and embedded wideband CELP coding of speech",
   "original": "e99_1543",
   "page_count": 4,
   "order": 349,
   "p1": "1543",
   "pn": "1546",
   "abstract": [
    "This paper presents a novel multi-band CELP coder with the following characteristics: wideband coding (6.5 kHz), variable bit rate (VBR) coding (10-24 kbps), low-delay (10 ms), embeddibility, and perceptually based dynamic bit allocation. The excitation signal of the linear prediction filter is the vector sum of eight off-line pre-filtered bandpass excitation vectors. The eight excitation codebooks are tree structured, providing embeddibility and variable bit rate. The dynamic allocation of the bitstream among the different bands is based on the perceptual importance of each band. The multi-band and perceptual structure of the coding scheme results in graceful degradation with decreasing bit rates both in quiet and in the presence of background noise.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-347"
  },
  "foldvari99_eurospeech": {
   "authors": [
    [
     "Rudolf",
     "Földvári"
    ],
    [
     "László",
     "Gyimesi"
    ]
   ],
   "title": "Very low bit rate voice coder based on a nonlinear hearing model",
   "original": "e99_1547",
   "page_count": 4,
   "order": 350,
   "p1": "1547",
   "pn": "1550",
   "abstract": [
    "A new hearing model based on the knowledge of anatomy and phychoacoustical phenomena was presented by R. Földvári and Gy. ács in 1996. After a short survey of older hearing models, they proved that their hearing model is able to describe true hearing experience (critical bandwidth, phase limit frequency). We suppose that the outputs of Zwicker's filters and the nonlinear transformation secured by the nonlinear hearing model is suitable for brain processing. Furthermore, we wish to point out that by realizing this method a very low bit rate vocoder and in addition an efficient speech enhancement can be achieved in cases when signal-to-noise ratio is about 0 dB.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-348"
  },
  "perreauguimaraes99_eurospeech": {
   "authors": [
    [
     "Marcos",
     "Perreau-Guimaraes"
    ],
    [
     "Madeleine",
     "Bonnet"
    ],
    [
     "Nicolas",
     "Moreau"
    ]
   ],
   "title": "Low complexity bit allocation algorithm with psychoacoustical optimisation",
   "original": "e99_1551",
   "page_count": 4,
   "order": 351,
   "p1": "1551",
   "pn": "1554",
   "abstract": [
    "High qualitymusic coders use an auditory masked threshold to account for the characteristics of the human ear. The masked thresholds calculated by these coders do not correspond to the theoretical threshold, solution of a non linear constrained deconvolution problem because of the huge complexity required. We present a binary allocation algorithm solving at the same time the deconvolution problem, while maintaining a tolerable complexity.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-349"
  },
  "wan99_eurospeech": {
   "authors": [
    [
     "Wanggen",
     "Wan"
    ],
    [
     "Oscar C.",
     "Au"
    ],
    [
     "Cyan L.",
     "Keung"
    ],
    [
     "Chi H.",
     "Yim"
    ]
   ],
   "title": "A novel approach of low bit-rate speech coding based on sinusoidal representation and auditory model",
   "original": "e99_1555",
   "page_count": 4,
   "order": 352,
   "p1": "1555",
   "pn": "1558",
   "abstract": [
    "In this paper, a new auditory spectrum based speech feature is proposed using sinusoidal representation and auditory model. The feature is optimized using the properties of auditory perception and masking. After quantizing and encoding the optimized feature parameters, a new speech-coding algorithm with average bit-rate of 3.25kbps is developed. The experimental results show that the synthetic speech retains most of the intelligibility and clearness of articulation of the original speech. Compared with the conventional algorithms, no voiced/unvoiced decision and pitch estimation are needed, complexity of the algorithm is much reduced, robustness and adaptation are both raised. The algorithm makes it possible to be realized with single DSP chip.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-350"
  },
  "adda99_eurospeech": {
   "authors": [
    [
     "Gilles",
     "Adda"
    ],
    [
     "Michéle",
     "Jardino"
    ],
    [
     "Jean-Luc",
     "Gauvain"
    ]
   ],
   "title": "Language modeling for broadcast news transcription",
   "original": "e99_1759",
   "page_count": 4,
   "order": 353,
   "p1": "1759",
   "pn": "1762",
   "abstract": [
    "This paper addresses the problem of language modeling for the transcription of broadcast news data. Different approaches for language model training were explored and tested in the context of a complete transcription system. Language model efficiency was investigated for the following aspects: mixing of different training material (sources and epoch); approach for mixing (interpolation vs count merging); and using class-based language models. The experimental results indicate that judicious selection of the training source and epoch is important, and that given sufficient broadcast new transcriptions, newspaper and newswire texts are not necessary. Results are given in terms of perplexity and word error rates. The combined improvements in text selection, interpolation, 4-gram and class-based LMs led to a 20% reduction in the perplexity of the LM of the final pass (3-gram class interpolated with a word 4-gram) compared with the 3-gram LM used in the the LIMSI Nov'97 BN system.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-351"
  },
  "bechet99_eurospeech": {
   "authors": [
    [
     "Frédéric",
     "Béchet"
    ],
    [
     "Alexis",
     "Nasr"
    ],
    [
     "Thierry",
     "Spriet"
    ],
    [
     "Renato de",
     "Mori"
    ]
   ],
   "title": "Large Span statistical language models: application to homophone disambiguation for large vocabulary speech recognition in French",
   "original": "e99_1763",
   "page_count": 4,
   "order": 354,
   "p1": "1763",
   "pn": "1766",
   "abstract": [
    "Homophone words is one of the specific problems of Automatic Speech Recognition (ASR) in French. Moreover, this phenomenon is particularly high for some inflections like the singular/plural inflection (72% of the 40.7K lemma of our 240K word dictionary have inflected forms which are homophonic). In order to take into account word-dependencies spanning over a variable number of words, it is interesting to merge local language models, like 3-gram or 3-class models, with large-span models. We present in this paper two kinds of models : a phrase-based model, using phrases obtained from a training corpus by means of a finitestate parser; a homophone cache-based model, using derivation of constraints from word histories stored in a cache memory.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-352"
  },
  "baggia99_eurospeech": {
   "authors": [
    [
     "P.",
     "Baggia"
    ],
    [
     "A.",
     "Kellner"
    ],
    [
     "Guy",
     "Pérennou"
    ],
    [
     "C.",
     "Popovici"
    ],
    [
     "Janienke",
     "Sturm"
    ],
    [
     "Frank",
     "Wessel"
    ]
   ],
   "title": "Language modelling and spoken dialogue systems - the ARISE experience",
   "original": "e99_1767",
   "page_count": 4,
   "order": 355,
   "p1": "1767",
   "pn": "1770",
   "abstract": [
    "The aim of this paper is to describe the experiences gained in the field of language modelling during the LE-3 ARISE (Automatic Railway Information Systems for Europe) project. All of the different techniques presented in this paper are related to the field of Spoken Dialogue Systems, and they cope with the issues of limited amount of training material and the exploitation of the constraints available in a dialogue system. The results obtained may be useful for the future development of similar applications.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-353"
  },
  "brieusselpousse99_eurospeech": {
   "authors": [
    [
     "Laure",
     "Brieussel-Pousse"
    ],
    [
     "Guy",
     "Perennou"
    ]
   ],
   "title": "Language model level vs. lexical level for modeling pronunciation variation in a French CSR",
   "original": "e99_1771",
   "page_count": 4,
   "order": 356,
   "p1": "1771",
   "pn": "1774",
   "abstract": [
    "In this paper, we present a markovian system, which takes into account pronunciation variation of French. After doing a brief overview of different methods allowing to deal with pronunciation variation in ASR, we describe our approach (which is based on the MHAT (Markovian Harmonic Adaptation and Transduction) model), as well as the lexical and phonological materials defined in order to implement MHAT into a classic ASR system based on HMM models. We finally compare two approaches (both issue from the MHAT model) that differ each other by the level of pronunciation modeling : at the lexicon level and at the language model level (by introducing an intermediate level of words representations depending on the context of words in the sentence). Results show an improvement of French continuous speech recognition when taking into account the context of words in the sentence within the language model.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-354"
  },
  "leung99_eurospeech": {
   "authors": [
    [
     "Roger H.Y.",
     "Leung"
    ],
    [
     "Chi-Yan",
     "Choy"
    ],
    [
     "Hong C.",
     "Leung"
    ]
   ],
   "title": "Characteristics of Chinese language models for large vocabulary telephone speech",
   "original": "e99_1775",
   "page_count": 4,
   "order": 357,
   "p1": "1775",
   "pn": "1778",
   "abstract": [
    "This paper is concerned with language modeling (LM) for large vocabulary speech recognition in Mandarin Chinese. As the language characteristics of Chinese are quite unique, we investigate some novel techniques in language modeling. We also borrow some of techniques that have been applied to other languages. Experiments have been conducted on the Call Home Mandarin, HUB4, and HUB5 corpora obtained from the Linguistic Data Consortium (LDC). The training set consists of 9.8 hours of spontaneous speech and 100K words in text. The test set consists of 1.6 hours of spontaneous speech and 20K words in text. We have found that our results compare favorably to the results reported in the literature.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-355"
  },
  "langlois99_eurospeech": {
   "authors": [
    [
     "D.",
     "Langlois"
    ],
    [
     "K.",
     "Smadli"
    ]
   ],
   "title": "A new based distance language model for a dictation machine: application to MAUD",
   "original": "e99_1779",
   "page_count": 4,
   "order": 358,
   "p1": "1779",
   "pn": "1782",
   "abstract": [
    "This paper deals with the use of a stochastic language model based on the split of the words history into d words where d is the length of the history. One of our aims is to modelise the semantic and syntactic relationships between words. This model can be considered as a first step for this goal. We experimented our model through the Shannon game (on 10 000 truncated sentences) and implemented it in MAUD, our dictation machine. Tests on MAUD have been done on 300 sentences pronounced by several women and men. This model predicts more words (in the Shannon game) than any other methods we developed before in our team. However, these models are sophisticated in contrast to the one we describe. Moreover, when including unknown words, the results are better than the model ones we presented in a recent work in terms of mean rank, ranks from 1 to 5 and perplexity. This work has needed to use two interpolation methods inspired from Markov model. Also, we discuss the problem of the unknown word modelling.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-356"
  },
  "muller99b_eurospeech": {
   "authors": [
    [
     "Ludek",
     "Müller"
    ],
    [
     "Josef",
     "Psutka"
    ]
   ],
   "title": "Using various language model smoothing techniques for the transcription of a weather forecast broadcasted by the czech radio",
   "original": "e99_1783",
   "page_count": 4,
   "order": 359,
   "p1": "1783",
   "pn": "1786",
   "abstract": [
    "This paper presents an experimental speech recognition system used to transcribe a weather forecast broadcasted by the Czech radio. The system is based on the HMM with mixture Gaussian continuous densities and is designed as a speaker independent. To overcome very sparse training data various language models supported by smoothing of model parameters based on the leaving-one-out technique, discounting and backing-off approach were tested. The results of recognition experiments are discussed in the paper.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-357"
  },
  "mcallaster99_eurospeech": {
   "authors": [
    [
     "Don",
     "McAllaster"
    ],
    [
     "Larry",
     "Gillick"
    ]
   ],
   "title": "Studies in acoustic training and language modeling using simulated speech data",
   "original": "e99_1787",
   "page_count": 4,
   "order": 360,
   "p1": "1787",
   "pn": "1790",
   "abstract": [
    "We continue our study of the use of fabricated data in the investigation of speech recognition algorithms. After reviewing the basic data generation algorithm and some earlier results involving the recognition of fabricated conversational speech data, we go on to describe some new and intriguing experiments concerning on the one hand, training acoustic models from simulated speech data, and on the other, recognizing fabricated data with different amounts of context in the language model. Among other things, we conclude that standard training algorithms are remarkably good at recovering the underlying structure in acoustic models when they are given 30 hours of data generated from those models. We also propose an alternative to erplexity for measuring the quality of a language model - the word error rate on fabricated data - that takes into account the inherent acoustic confusability of words.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-358"
  },
  "reichl99_eurospeech": {
   "authors": [
    [
     "Wolfgang",
     "Reichl"
    ]
   ],
   "title": "Language model adaptation using minimum discrimination information",
   "original": "e99_1791",
   "page_count": 4,
   "order": 361,
   "p1": "1791",
   "pn": "1794",
   "abstract": [
    "In this paper, adaptation of language models using the minimum discrimination information criteria is presented. Language model probabilities are adapted based on unigram, bigram and trigram features using a modified version of the generalized iterative scaling algorithm. Furthermore, a lan-guage model compression algorithm, based on conditional relative entropy is discussed. It removes probability terms from the language model, which can be closely approximated by back-off distributions. The proposed algorithms are used to adapt a mismatched, newspaper style language model to a natural language call routing task. The experiments show a significant reduction in perplexity and word error rate for small amounts of adaptation data.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-359"
  },
  "smadli99_eurospeech": {
   "authors": [
    [
     "K.",
     "Smadli"
    ],
    [
     "A.",
     "Brun"
    ],
    [
     "I.",
     "Zitouni"
    ],
    [
     "Jean-Paul",
     "Haton"
    ]
   ],
   "title": "Automatic and manual clustering for large vocabulary speech recognition: a comparative study",
   "original": "e99_1795",
   "page_count": 4,
   "order": 362,
   "p1": "1795",
   "pn": "1798",
   "abstract": [
    "This article describes a comparative study of language models in which the evaluation protocol has been set by AUPELF-UREF 1 . We especially pay attention on the comparison between two methods of clustering words which are necessary in the design of the corresponding language models. The first classification is done by following a linguistic and theoretical method and the second one is based on an optimization method. Both methods are evaluated through the Shannon game. The vocabulary used is 20 000 words, the training corpus is made of two years of Le Monde newspaper (42M of words) and the test corpus (400 000 words) is extracted from 6 years of Le Monde Diplomatique. First evaluations show an improvement of 13% of recognized words in the first five ranks and a decrease of 25% in perplexity.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-360"
  },
  "sanchez99_eurospeech": {
   "authors": [
    [
     "Joan-Andreu",
     "Sánchez"
    ],
    [
     "José-Miguel",
     "Benedí"
    ]
   ],
   "title": "Learning of stochastic context-free grammars by means of estimation algorithms",
   "original": "e99_1799",
   "page_count": 4,
   "order": 363,
   "p1": "1799",
   "pn": "1802",
   "abstract": [
    "The use of the Inside-Outside (IO) algorithm for the estimation of the probability distributions of Stochastic Context-Free Grammars is characterized by the use of all the derivations in the learning process. However, its application in real tasks for Language Modeling is restricted due to the time complexity per iteration and the large number of iterations that it needs to converge. Alternatively, several estimations algorithms which consider a certain subset of derivations in the estimation process have been proposed elsewhere. This set of derivations can be chosen according to structural criteria, or by selecting the k-best derivations. These alternatives are studied in this paper, and they are tested on the corpus of the Wall Street Journal processed in the Penn Treebank project.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-361"
  },
  "yamamoto99_eurospeech": {
   "authors": [
    [
     "Hirofumi",
     "Yamamoto"
    ],
    [
     "Yoshinori",
     "Sagisaka"
    ]
   ],
   "title": "Part-of-speech n-gram and word n-gram fused language model",
   "original": "e99_1803",
   "page_count": 4,
   "order": 364,
   "p1": "1803",
   "pn": "1806",
   "abstract": [
    "In th is paper, an accurate and com pact language model is proposed to cope robustly with data sparseness and task dependencies. This language model adopts new categories which are generated by continuously interpolating POS word-class categories and word categories using M A P estimation. The new categories can reflect word statistics efficiently without loosing accuracy and task-independent general word-characteristics (i.e. grammatical constraints captured by POS statistics) are embedded to prevent task-overtuning. This modeling reduces the model size to 50% of the conventional models. T he bi-directional word-cluster N-grams generated by this modeling have 3% lower perplexity measured on a matched domain and 15% lower on a mismatched domain compared to a conventi onal word 2-gram.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-362"
  },
  "zhu99b_eurospeech": {
   "authors": [
    [
     "Xiaojin",
     "Zhu"
    ],
    [
     "Stanley F.",
     "Chen"
    ],
    [
     "Ronald",
     "Rosenfeld"
    ]
   ],
   "title": "Linguistic features for whole sentence maximum entropy language models",
   "original": "e99_1807",
   "page_count": 4,
   "order": 365,
   "p1": "1807",
   "pn": "1810",
   "abstract": [
    "Whole sentence maximum entropy models directly model the probability of a sentence using features  arbitrary computable properties of the sentence. We investigate whether linguistic features that capture the underlying linguistic structure of a sentence can improve modeling. We use a shallow parser to parse sentences into linguistic constituents in two corpora; one is the original training corpus, and the other is an artificial corpus generated from an initial trigram model. We define three sets of candidate linguistic features based on these constituents, and compute the prevalence of each feature in the two data sets. We select features with significantly different frequencies. These correspond to phenomena poorly modeled by traditional trigrams, and reveal interesting linguistic deficiencies of the initial model. We found 6798 linguistic features in the Switchboard domain and achieved small improvements in perplexity and speech recognition accuracy with these features.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-363"
  },
  "zitouni99_eurospeech": {
   "authors": [
    [
     "I.",
     "Zitouni"
    ],
    [
     "J. F.",
     "Mari"
    ],
    [
     "K.",
     "Smadli"
    ],
    [
     "Jean-Paul",
     "Haton"
    ]
   ],
   "title": "Variable-length sequence language model for large vocabulary continuous dictation machine",
   "original": "e99_1811",
   "page_count": 4,
   "order": 366,
   "p1": "1811",
   "pn": "1814",
   "abstract": [
    "In natural language, some sequences of words are very frequent. A classical language model, like n-gram, does not adequately take into account such sequences, because it underestimates their probabilities. A better approach consists in modeling word sequences as if they were individual dictionary elements. Sequences are considered as additional entries of the word lexicon, on which language models are computed. In this paper, we present two methods for automatically determining frequent phrases in unlabeled corpora of written sentences. These methods are based on information theoretic criteria which insure a high statistical consistency. Our models reach their local optimum since they minimize the perplexity. One procedure is based only on the n-gram language model to extract word sequences. The second one is based on a class n-gram model trained on 233 classes extracted from the eight grammatical classes of French. Experimental tests, in terms of perplexity and recognition rate, are carried out on a vocabulary of 20000 words and a corpus of 43 million words extracted from the Le Monde\" newspaper. Our models reduce perplexity by more than 20% compared with n-gram (nR3) and multigram models. In terms of recognition rate, our models outperform n-gram and multigram models.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-364"
  },
  "zhang99c_eurospeech": {
   "authors": [
    [
     "Ruiqiang",
     "Zhang"
    ],
    [
     "Ezra",
     "Black"
    ],
    [
     "Andrew",
     "Finch"
    ]
   ],
   "title": "Using detailed linguistic structure in language modelling",
   "original": "e99_1815",
   "page_count": 4,
   "order": 367,
   "p1": "1815",
   "pn": "1818",
   "abstract": [
    "Recently, considerable attention has been accorded to attempts to apply natural language processing techniques to language modelling for speech recognition. Another extension to the standard n-gram technique has been the use of trigger-pair predictors. In the present experiments, we incorporate into language models, information derived from detailed syntactic and semantic parses and taggings. We use a human expert to define the interesting features of the history, and these are formalized as triggers and integrated with a trigram language model using the maximum entropy framework. We select maximum entropy because it provides a convenient method of combining multiple information sources. We employ two different kinds of triggering events: those based on a knowledge of the full parse of the previous sentences in the document, and those based on knowledge of the syntactic/semantic tags to the left of and in the same sentence as the word being predicted. We contrast results obtained using these events plus a baseline n{gram language model, both with the baseline model itself, and with the baseline model plus triggers based on word triggers chosen automatically. Mutual information selects the best trigger pairs from all candidates generated by combining each of these triggering events with every word in the vocabulary. The grammar and tagset used to express linguistic information about English are unusually detailed. The tagset contains some 3,000 syntactic/semantic tags. Using a 200-million-word training set composed of Wall Street Journal and Associated Press newswire text we reduced test-set perplexity by 11.3% as against the baseline model. Further, our method when combined with long{distance word triggers reduced test-set perplexity by 21.7%.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-365"
  },
  "chen99d_eurospeech": {
   "authors": [
    [
     "Aimin",
     "Chen"
    ],
    [
     "Shu Lian",
     "Wong"
    ],
    [
     "Saeed",
     "Vaseghi"
    ],
    [
     "Charles",
     "Ho"
    ]
   ],
   "title": "Decision tree micro-prosody structures for text to speech synthesis",
   "original": "e99_1615",
   "page_count": 4,
   "order": 368,
   "p1": "1615",
   "pn": "1618",
   "abstract": [
    "This paper explores the use of micro-prosody in improving the quality of synthesised speech in concatenated text to speech synthesis (TTS) systems. Micro-prosody are defined as prosodic signals within context-dependent triphone units and across neighbouring triphones. Micro-prosody parameters are modelled using a Markovian model whose state distributions depend on the current linguistic-prosodic state as well as the current and the neighbouring phones. The use of various speech unit selection criteria in the design of the TTS sound inventory and their effects in reducing the variance of micro-prosodic parameters in concatenated speech and on the TTS output speech are explored. The effect of the variability of the prosodic parameters of speech in the recorded samples from a given speaker, and the influence of accents, such as the US and the UK accented English, on speech prosody variability and on the design of TTS are considered.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-366"
  },
  "cordoba99_eurospeech": {
   "authors": [
    [
     "R.",
     "Córdoba"
    ],
    [
     "J. A.",
     "Vallejo"
    ],
    [
     "J. M.",
     "Montero"
    ],
    [
     "J.",
     "Gutierrez-Arriola"
    ],
    [
     "M. A.",
     "López"
    ],
    [
     "Juan Manuel",
     "Pardo"
    ]
   ],
   "title": "Automatic modeling of duration in a Spanish text-to-speech system using neural networks",
   "original": "e99_1619",
   "page_count": 4,
   "order": 369,
   "p1": "1619",
   "pn": "1622",
   "abstract": [
    "Accurate prediction of segmental duration from text in a text-tospeech system is difficult for several reasons. One specially relevant is the great quantity of contextual factors that affect timing and how to model them. There are many parameters that affect duration, but not all of them are always relevant. We present a complete environment in which to decide which parameters are more relevant in different situations and the best way to code them. The system is based in a neural network absolutely configurable, and the main effort is made in the parameters to be used, including the contextual effects using windows of variable length.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-367"
  },
  "clark99_eurospeech": {
   "authors": [
    [
     "Robert A.J.",
     "Clark"
    ],
    [
     "Kurt E.",
     "Dusterhoff"
    ]
   ],
   "title": "Objective methods for evaluating synthetic intonation",
   "original": "e99_1623",
   "page_count": 4,
   "order": 370,
   "p1": "1623",
   "pn": "1626",
   "abstract": [
    "This paper describes the development and evaluation of objective methods for testing synthetic intonation. While subjective methods are available for assessing the quality of synthetic intonation, such tests consume time and resources, and are not useful for day-to-day model development. Therefore, objective measures of F0 modelling are necessary. Currently, objective evaluation of synthetic intonation involves the use of Root Mean Squared Error and Correlation. However, it is unclear how large an improvement in either score must be before it is reflected perceptually. It is also unclear how detailed an analysis these metrics provide. Therefore, two other metrics are to be tested, both of which are similar to a basic RMSE measurement. All of the evaluation results are compared to a perceptual study in order to determine how the objective measures relate to perceived differences in the contours.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-368"
  },
  "dusterhoff99_eurospeech": {
   "authors": [
    [
     "Kurt E.",
     "Dusterhoff"
    ],
    [
     "Alan W.",
     "Black"
    ],
    [
     "Paul",
     "Taylor"
    ]
   ],
   "title": "Using decision trees within the tilt intonation model to predict F0 contours",
   "original": "e99_1627",
   "page_count": 4,
   "order": 371,
   "p1": "1627",
   "pn": "1630",
   "abstract": [
    "This paper presents an intonation generation system for use in a text-to-speech synthesis system. The intonation generation system uses classification trees to predict intonation event location and regression trees to predict parameters relating to the F0 shape for the predicted events. The decision trees model intonation within the Tilt intonation model, which provides a parameterized description of fundmaental frequency and an intuitive labelling scheme. The event location trees predict an event class (e.g. accent, boundary, none) for each syllable in an utterance based on local and global context (e.g. stress, phrasing, part of speech). The parameter prediction trees then provide the parameterized description of each intonation event based on similar context features. Informal results of the full system are presented together with results for the individual components.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-369"
  },
  "esposito99_eurospeech": {
   "authors": [
    [
     "Richard",
     "Esposito"
    ],
    [
     "Li-chiung",
     "Yang"
    ]
   ],
   "title": "Levels of prosodic representation in spoken discourse: an empirical approach",
   "original": "e99_1631",
   "page_count": 4,
   "order": 372,
   "p1": "1631",
   "pn": "1634",
   "abstract": [
    "In this paper we propose that prosodic structures in spontancous discourse exhibit both linear and superpositional characterictics, and that these reflect the different scopes of multi-tiered emotional and cognitive processes. The multi-tiered structure encompasses the syllable and word level, interphrase movement, and extended pitch level baseline rise and fall. Analysis of the data suggests that integration of the 3 different prosodic levels within an overall prosodic model provides a critical link for the generation of natural-sounding interactive speech systems.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-370"
  },
  "fernandezsalgado99_eurospeech": {
   "authors": [
    [
     "Xavier",
     "Fernández-Salgado"
    ],
    [
     "Eduardo R.",
     "Banga"
    ]
   ],
   "title": "Segmental duration modelling in a text-to-speech system for the galician language",
   "original": "e99_1635",
   "page_count": 4,
   "order": 373,
   "p1": "1635",
   "pn": "1638",
   "abstract": [
    "In this contribution we propose a segmental duration model for the Galician language. We have focused our work on the study of allophonic durations in their syllabic environment. Firstly, a study of the speech rate over a recorded corpus led us to consider different behaviours in certain types of sentences. Secondly, the corpus was analyzed in order to determine the main factors affecting duration (phonetic class, context, ...). Prosodic factors (stress and final lengthening) were found to be the most determinant, in quantitative terms, to predict timing. Finally, a model for assigning segmental durations is proposed.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-371"
  },
  "hirst99_eurospeech": {
   "authors": [
    [
     "Daniel",
     "Hirst"
    ]
   ],
   "title": "The symbolic coding of segmental duration and tonal alignment: an extension to the INTSINT system.",
   "original": "e99_1639",
   "page_count": 4,
   "order": 374,
   "p1": "1639",
   "pn": "1642",
   "abstract": [
    "This paper presents work based on an analysis-by-synthesis approach which aims to develop a reversible coding system for prosody, capable of deriving a linguistic-like surface phonological representation directly from acoustic data that is sufficient to reproduce a synthetic version of the original utterance without significant loss of linguistic information. With such a coding system, capable of representing any significant prosodic distinctions, the task of predicting such representations would be greatly simplified, becoming one of mapping between sets of symbolic representations. This approach has already been applied to the stylisation and symbolic coding of fundamental frequency curves by means of the INTSINT transcription system. An automatic version has also been proposed. This paper presents a preliminary proposal for an extension to the INTSINT system to cover segmental duration and the relative alignment of phonematic and tonal segments.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-372"
  },
  "morlec99_eurospeech": {
   "authors": [
    [
     "Yann",
     "Morlec"
    ],
    [
     "Gérard",
     "Bailly"
    ],
    [
     "Véronique",
     "Aubergé"
    ]
   ],
   "title": "Training an application-dependent prosodic model corpus, model and evaluation",
   "original": "e99_1643",
   "page_count": 4,
   "order": 375,
   "p1": "1643",
   "pn": "1646",
   "abstract": [
    "A model of intonation is trained here in order to capture stylistic factors for an application: reading of telephone directory listings. The system was designed to carry out one of the evaluation tasks of the 3rd International Workshop on Speech Synthesis in Jenolan-Australia and the input to the system conforms to the format of the listings defined there. The resulting synthetic prosody is fed into the ICP concatenative synthesis system and compared to natural prosody and prosody obtained from text reading material.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-373"
  },
  "sheikhzadeh99_eurospeech": {
   "authors": [
    [
     "H.",
     "Sheikhzadeh"
    ],
    [
     "A.",
     "Eshkevari"
    ],
    [
     "M.",
     "Khayatian"
    ],
    [
     "R.",
     "Sadigh"
    ],
    [
     "S. M.",
     "Ahadi"
    ]
   ],
   "title": "Farsi language prosodic structure, research and implementation using a speech synthesizer",
   "original": "e99_1647",
   "page_count": 4,
   "order": 376,
   "p1": "1647",
   "pn": "1650",
   "abstract": [
    "In this research, we have investigated about prosodic features of Farsi (Persian) language and quantified major stress rules and some intonation rules for speech synthesis purpose. The research is mostly concentrated on pitch variations and then on durational changes. We have implemented the proposed simplified prosodic rules using a Klatt formant synthesizer, specially modified for Farsi phonemes. In order to achieve to a better speech quality, we have exploited different allophonic forms for some consonants, leading to a total of 207 Farsi diphones synthesized by the speech synthesizer. Subjective listening tests show that the addition of the prosodic features drastically increases both the intelligibility and naturalness of the synthesized speech. The synthesizer is software-implemented on a Pentium PC and operates in real-time.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-374"
  },
  "teixeira99b_eurospeech": {
   "authors": [
    [
     "Joao Paulo",
     "Teixeira"
    ],
    [
     "Elisabete Rosa",
     "Paulo"
    ],
    [
     "Diamantino",
     "Freitas"
    ],
    [
     "Maria da Graca",
     "Pinto"
    ]
   ],
   "title": "Acoustical characterisation of the accented syllable in portuguese, a contribution to the naturalness of speech synthesis",
   "original": "e99_1651",
   "page_count": 4,
   "order": 377,
   "p1": "1651",
   "pn": "1654",
   "abstract": [
    "Text-to-Speech systems require control of the prosodic parameters of the produced speech waveform in order to achieve a higher naturalness and degree of perception. Amongst the several dimensions into which prosody can be unfolded, the accented syllable realisation brings the basic problem of producing a set of comprehensive rules for accurate control of the acoustic realisation of the syllable parameters, which remains to be solved. In particular, for the Portuguese language, in Europe, a set of comprehensive quantitative characterisation data and rules is absolutely lacking. The present paper is intended, as a quantitative contribution, as far as we know the first, to the solution of this problem. The duration Intensity, and variation of F0 were modelled in the tonic syllable according to its position in the word and the position of the word in the sentence.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-375"
  },
  "wang99b_eurospeech": {
   "authors": [
    [
     "Changfu",
     "Wang"
    ],
    [
     "Hiroya",
     "Fujisaki"
    ],
    [
     "Sumio",
     "Ohno"
    ],
    [
     "Tomohiro",
     "Kodama"
    ]
   ],
   "title": "Analysis and synthesis of the four tones in connected speech of the standard Chinese based on a command-response model",
   "original": "e99_1655",
   "page_count": 4,
   "order": 378,
   "p1": "1655",
   "pn": "1658",
   "abstract": [
    "The Chinese language is a typical tone language in which a syllable possesses several tone types and thus can represent different morphemes. While these tone types have rather clear manifestations in the fundamental frequency contour in isolated syllables, they vary considerably in connected speech due to the in uences of such factors as tones of adjacent syllables, syntactic and pragmatic information of the whole utterance. This paper describes the results of analysis of F0 contours of the Standard Chinese using a command-response model, and shows that systematic relationships exit between the timing of the tone commands and the vowel plus coda\" part of a syllable. The results are then used to derive rules for tone command generation in speech synthesis. The validity of the rules has been confirmed by the naturalness of prosody of synthetic speech.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-376"
  },
  "williams99_eurospeech": {
   "authors": [
    [
     "Sandra",
     "Williams"
    ],
    [
     "Catherine I.",
     "Watson"
    ]
   ],
   "title": "A profile of the discourse and intonational structures of route descriptions",
   "original": "e99_1659",
   "page_count": 4,
   "order": 379,
   "p1": "1659",
   "pn": "1662",
   "abstract": [
    "A corpus of spontaneous route descriptions was collected from 8 speakers (5 males and 3 females). The corpus was labelled according to the ToBI standard and a discourse analysis was completed. Four discourse tour acts were identified and these were found to occur mainly in non-embedded linear sequences. In general, the intonation of each route description was characterised by a single intonational phrase containing many intermediate phrases. There was a tendency for boundaries between tour acts and intermediate phrases to coincide, but there is usually more than one intermediate phrase to one tour act.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-377"
  },
  "amano99_eurospeech": {
   "authors": [
    [
     "Shigeaki",
     "Amano"
    ],
    [
     "Tadahisa",
     "Kondo"
    ]
   ],
   "title": "Neighborhood effects on spoken word recognition in Japanese",
   "original": "e99_1663",
   "page_count": 4,
   "order": 380,
   "p1": "1663",
   "pn": "1666",
   "abstract": [
    "Reaction time of lexical decision and word recognition score were measured to investigate whether the neighborhood, a word candidate set with single mora substitution with a target word, has inhibitory effects on spoken word recognition in Japanese. Partial correlation analyses were conducted between subjects performance of spoken word recognition (reaction time and recognition score) and neighborhood properties (density, mean familiarity, maximum familiarity, and sum of familiarity). The results showed significant inhibitory effects of neighborhood on the recognition score but not on the reaction time. It is suggested that some amount of time is necessary for the neighborhood to be activated and to compete with a target word in spoken word recognition in Japanese.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-378"
  },
  "chereau99_eurospeech": {
   "authors": [
    [
     "C.",
     "Chéreau"
    ],
    [
     "P. A.",
     "Hallé"
    ],
    [
     "J.",
     "Segui"
    ]
   ],
   "title": "Interference between surface form and abstract representation in spoken word perception",
   "original": "e99_1667",
   "page_count": 4,
   "order": 381,
   "p1": "1667",
   "pn": "1670",
   "abstract": [
    "The aim of this study was to examine the influence of an abstract code (orthographic and/or morphophonemic) on the perception of the sounds in French spoken words. For this purpose, we used words whose surface phonology was not congruent either with both orthographic and morphophonemic codes or with orthographic code alone. Experiment 1 showed that the processing of spoken word is affected by an underlying abstract code, which is presumably orthographic and/or morphophonemic. Experiment 2 examined the role of orthography alone, and showed that the sole orthographic code influenced spoken word processing. However, the effect obtained in Experiment 2 was smaller than that obtained Experiment 1. These data are interpreted in the framework of an interactive model of speech perception, where the written/abstract code and the phonetic code interact in both directions.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-379"
  },
  "colin99_eurospeech": {
   "authors": [
    [
     "C.",
     "Colin"
    ],
    [
     "M.",
     "Radeau"
    ]
   ],
   "title": "Are the mcgurk illusions affected by left or right presentation of the speaker face?",
   "original": "e99_1671",
   "page_count": 4,
   "order": 382,
   "p1": "1671",
   "pn": "1674",
   "abstract": [
    "When conflicting syllables are presented in the auditory and visual modalities, two kinds of illusions have been reported by McGurk and MacDonald (1976): combinations (cluster responses) and fusions (fused responses). In the present experiment, we examined the lateralization issue of these illusions. In one half of the Session: the center of the TV screen was displaced 5° to the right of straight ahead and in the other half, it was displaced 5° to the left. The sound, played at an average level of 40dB, always came from straight ahead. Neither for fusions nor for combinations did we found any difference in the percentage of illusion as a function of the hemifield in which the visual stimuli were presented. This result does not corroborate Dieschs (1995) finding. Using a somewhat different method, this author found a left hemifield advantage for fusions and a right hemifield advantage for combinations.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-380"
  },
  "dupoux99_eurospeech": {
   "authors": [
    [
     "Emmanuel",
     "Dupoux"
    ],
    [
     "Takao",
     "Fushimi"
    ],
    [
     "Kazuhiko",
     "Kakehi"
    ],
    [
     "Jacques",
     "Mehler"
    ]
   ],
   "title": "Prelexical locus of an illusory vowel effect in Japanese",
   "original": "e99_1675",
   "page_count": 4,
   "order": 383,
   "p1": "1675",
   "pn": "1678",
   "abstract": [
    "Studies in vision have demonstrated that the visual system can induce the perception of illusory contours. In this study we document a similar phenomenon in the auditory mode: Japanese speakers report perceiving vowels that are absent in the acoustic signal. Such an illusion is due to the fact that in Japanese, succession of consonants are not allowed. Hence the linguistic system inserts an illusory vowel between adjacent vowels in order to conform to the expected pattern in this language. Here, we manipulate the lexical neighborhood of non-words that contain illegal consonant clusters and show that this illusion is not due to lexical influence. Rather, it arises before lexical knowledge is activated, suggesting that phonotactics impact perception routines at a very early processing stage.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-381"
  },
  "feijoo99_eurospeech": {
   "authors": [
    [
     "Sergio",
     "Feijóo"
    ],
    [
     "Santiago",
     "Fernández"
    ],
    [
     "Nieves",
     "Barros"
    ],
    [
     "Ramón",
     "Balsa"
    ]
   ],
   "title": "Acoustic and perceptual characteristics of the Spanish fricatives",
   "original": "e99_1679",
   "page_count": 4,
   "order": 384,
   "p1": "1679",
   "pn": "1686",
   "abstract": [
    "This study deals with the relation between the spectral representation and the perceptual identification of the Spanish fricatives and affricates. Several spectral representations have been analyzed: FFT-derived linear cepstrum, mel cepstrum coefficients, LPC cepstral coefficients and the first four statistical moments. Quadratic discriminant analysis including the leave-oneout method have been carried out on a large database. For this particular classification procedure, both the order of every spectral parametrization and the order of the temporal trajectory of those parameters have been optimized. The results indicate that a low order representation performs satisfactorily and that a three order temporal trajectory is adequate to encode the dynamics of the fricatives. The best classification rates were obtained by the cepstral (79.5%) and linear cepstrum coefficients (75.2%). They attained a correlation coefficient with respect to the perceptual identification of 0.78 and 0.75, respectively.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-382"
  },
  "gelin99_eurospeech": {
   "authors": [
    [
     "Philippe",
     "Gelin"
    ],
    [
     "Jean-Claude",
     "Junqua"
    ]
   ],
   "title": "Techniques for robust speech recognition in the car environment",
   "original": "e99_3003",
   "page_count": 0,
   "order": 385,
   "p1": "Abstract",
   "pn": "",
   "abstract": [
    "The use of voice commands or navigation features in the car is becoming a necessity. As keyboard and display interfaces cannot be used safely while driving, much effort has been done to make automatic speech recognition (ASR) and Text-to-Speech synthesis (TTS) ubiquitous features in the car. From voice dialing to car navigation, the requirements for voice technology vary greatly. While the use of a hands-free microphone and noise robust algorithms is a must, a wide range of technology spanning from small vocabulary isolated word/continuous speech to phonetic-based flexible vocabulary ASR has to be developed. Except for voice dialing, speaker-independent technology eventually combined with fast adaptation is mandatory. In this paper, we present our efforts in these directions. After focusing on two novel techniques for robust speech recognition in the car, we focus on fast speaker adaptation and report on experiments for a small set of 10 keywords, continuous digit/letter recognition along with phonetic-based recognition for 1800 words.\n",
    ""
   ]
  },
  "karlsson99_eurospeech": {
   "authors": [
    [
     "Fredrik",
     "Karlsson"
    ],
    [
     "Anders",
     "Eriksson"
    ]
   ],
   "title": "Difference limen for formant frequency discrimination at high fundamental frequencies",
   "original": "e99_1687",
   "page_count": 4,
   "order": 386,
   "p1": "1687",
   "pn": "1690",
   "abstract": [
    "In previous studies of formant frequency discrimination, variation in the stimuli has mainly concerned the formant frequencies while other factors which may affect formant frequency discrimination have largely been ignored. In most studies, fundamental frequencies typical of adult male speakers have been used. In the study presented here, formant frequency discrimination as a function of fundamental frequency level was studied. Contrary to expectations, fundamental frequency level did not seem to affect formant frequency discrimination. This was true even when the fundamental frequency was higher than the tested formant frequency. The implication of these findings is that formant frequency discrimination must, at least partly, rely on some other mechanism than reconstruction of the vocal tract transfer function. It is suggested that listeners use differences in the amplitude of partials to discriminate between complex vowel like sounds. Asymmetries in the distribution of these amplitudes may also explain the corresponding asymmetries found in perception.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-383"
  },
  "samarta99_eurospeech": {
   "authors": [
    [
     "Eduardo",
     "Sá Marta"
    ],
    [
     "Luis",
     "Vieira de Sá"
    ]
   ],
   "title": "Auditory features for human communication of stop consonants under full-band and low-pass conditions",
   "original": "e99_1691",
   "page_count": 4,
   "order": 387,
   "p1": "1691",
   "pn": "1694",
   "abstract": [
    "A set of auditorily-formulated features for PLACE discrimination in stop consonants, uncovered in extensive experiments with natural and edited sounds, are now being modeled using fuzzy logic and being applied to large databases of monosyllabic and spelled letters speech sounds, in various languages, in full-band and low-pass conditions. The rationale is that any valid model of human communication should replicate the human listener \"feats\" of very good (albeit not perfect) discrimination of stop PLACE even from speakers of different languages, and of \"graceful degradation\" when faced with markedly low-pass filtered sounds (e.g., telephone-like). This paper reports mainly about fuzzy-logical models, expressing known auditory phenomena, that evaluate the high-frequency content of the burst+aspiration segment in stop consonants, and provides a powerful cue for discrimination of DENTAL consonants. This evaluation is robust to mild variations of the frequency response curve such as those caused by different recording microphones. These cues are absent from markedly low-pass filtered sounds, however. Other cues, which survive in low-pass sounds (reported in previous papers), are also discussed.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-384"
  },
  "widera99_eurospeech": {
   "authors": [
    [
     "Christina",
     "Widera"
    ],
    [
     "Thomas",
     "Portele"
    ]
   ],
   "title": "Levels of reduction for German tense vowels",
   "original": "e99_1695",
   "page_count": 4,
   "order": 388,
   "p1": "1695",
   "pn": "1698",
   "abstract": [
    "In natural speech there are differences in the realisation of vowels. Numerous factors such as speaking style, prosody, or word class can cause vowel reductions. It is investigated whether vowel reductions can be described using discrete levels and, if yes, how many levels can be reliably perceived. The reduction of a vowel was judged by matching stimuli to representatives of reduction levels (prototypes). The results were investigated on the basis of inter-subject agreement. The resulting prototypes were evaluated by further perception experiments as well as artificial neural networks. The transferability of the reduction levels to other speakers was also investigated. The experiments show that listeners can reliably discriminate 3 to 5 reduction levels depending on the vowel. They use the prototypes speaker-independently, while neural networks trained with the material from one speaker are not applicable to other speakers. Lastly, the relationships between reduction levels and prosodic factors (lexical word stress, pitch accent, prominence) as well as word class (content words vs. function words) were investigated.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-385"
  },
  "julia99_eurospeech": {
   "authors": [
    [
     "Luc",
     "Julia"
    ],
    [
     "Adam",
     "Cheyer"
    ]
   ],
   "title": "Is talking to virtual more realistic?",
   "original": "e99_1719",
   "page_count": 4,
   "order": 389,
   "p1": "1719",
   "pn": "1722",
   "abstract": [
    "Virtual worlds and animated computer avatars are becoming more realistic, more natural, and more widespread. Accordingly, we are looking at new ways of interacting with machines based on \"old\" methods for interacting with humans, such as talking, writing and gesturing. By applying a synergistic, multimodal approach to several application domains that incorporate avatars, augmented reality or virtual reality, we investigate whether this interface style is more suitable for realistic environments. Concrete examples and studies are used to discuss this point, raising other key questions in the design of this new generation of interfaces.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-386"
  },
  "matsusaka99_eurospeech": {
   "authors": [
    [
     "Yosuke",
     "Matsusaka"
    ],
    [
     "Tsuyoshi",
     "Tojo"
    ],
    [
     "Sentaro",
     "Kubota"
    ],
    [
     "Kenji",
     "Furukawa"
    ],
    [
     "Daisuke",
     "Tamiya"
    ],
    [
     "Keisuke",
     "Hayata"
    ],
    [
     "Yuichiro",
     "Nakano"
    ],
    [
     "Tetsunori",
     "Kobayashi"
    ]
   ],
   "title": "Multi-person conversation via multi-modal interface - a robot who communicate with multi-user -",
   "original": "e99_1723",
   "page_count": 4,
   "order": 390,
   "p1": "1723",
   "pn": "1726",
   "abstract": [
    "This paper describes a robot who converses with multi-person using his multi-modal interface. The multi-person conversation includes many new problems, which are not cared in the conventional one-to-one conversation: such as information ow problems (recognizing who is speaking and to whom he is speaking / appealing to whom the system is speaking), space information sharing problem and turn holder estimation problem (estimating who is the next speaker). We solved these problems by utilizing multi-modal interface: face direction recognition, gesture recognition, sound direction recognition, speech recogni tion and gestural expression. The systematic combination of these functions realized human friendly multi-person conversation system.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-387"
  },
  "narayanan99_eurospeech": {
   "authors": [
    [
     "Shrikanth",
     "Narayanan"
    ],
    [
     "Alexandros",
     "Potamianos"
    ],
    [
     "Haohong",
     "Wang"
    ]
   ],
   "title": "Multimodal systems for children: building a prototype",
   "original": "e99_1727",
   "page_count": 4,
   "order": 391,
   "p1": "1727",
   "pn": "1730",
   "abstract": [
    "In this paper, we describe our efforts in designing and building a prototype multimodal system for children users. Data collection efforts and user experience results from a WoZ study using a popular computer game are reviewed first. Automatic speech recognition and spoken language understanding technology for children speakers are discussed next. A multimodal prototype is designed for a personal agent and a gaming application. Emphasis is placed on a modular architecture, handling of multimodal input and multimedia output, and providing an engaging user interface. Informal evaluation by children users was very positive especially for the animated agent and the speech interface.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-388"
  },
  "okada99_eurospeech": {
   "authors": [
    [
     "Michio",
     "Okada"
    ],
    [
     "Noriko",
     "Suzuki"
    ],
    [
     "Masaaki",
     "Date"
    ]
   ],
   "title": "Social bonding in talking with social autonomous creatures",
   "original": "e99_1731",
   "page_count": 5,
   "order": 392,
   "p1": "1731",
   "pn": "1734",
   "abstract": [
    "Cyberpets and artificial autonomous creatures become popular applications of spoken dialogue systems as well as of artificial life and artificial intelligence research. One of next targets in the fields is to give them social skills so as to be social entities in human-inhabited environments. To behave as social entities, they first have to establish the social bonding with social others, and to coordinate social distance with them. The abilities are also what conventional spoken dialogue systems lack. This paper discusses the social aspects in interacting between human and autonomous creatures, such as dialogue coordination, social bonding and social understanding. One of the points is how to establish and maintain a dynamic, structural coupling with social others. We are focusing the relationship between entrusting behavior and its grounding in our everyday activities. And then, we discuss on a model of the structural coupling with dialogue participants based on the notion of the entrusting behavior and its grounding. Finally, we show an example of artificial autonomous creatures that is implemented in multiple situated agents for the model.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-389"
  },
  "purson99_eurospeech": {
   "authors": [
    [
     "A.",
     "Purson"
    ],
    [
     "S.",
     "Santi"
    ],
    [
     "R.",
     "Bertrand"
    ],
    [
     "Isabelle",
     "Guaitella"
    ],
    [
     "J.",
     "Boyer"
    ],
    [
     "C.",
     "Cavé"
    ]
   ],
   "title": "The relationships between voice and gesture: eyebrow movements and questioning.",
   "original": "e99_1735",
   "page_count": 4,
   "order": 393,
   "p1": "1735",
   "pn": "1738",
   "abstract": [
    "Our research deals with the relationship between voice and gesture during speech in an interview situation. In this study, we focus on rapid eyebrow movements during questioning. Eyebrow movements were recorded using a motion analysis system (ELITE) enabling computer reconstruction of the trajectories of small infrared retroflexive markers attached to the subject's skin. This kinematic data and the speech signal were analyzed on a Sparc SUN station. A classification based on all 150 question-answer pairs revealed recurrent rapid eyebrow movements. This kinesic marker was mainly located on answers, and appears to function as a speaking-turn onset signal which probably helps the speaker plan his/her verbal message. We also found this marker on questions (particularly during requests for confirmation), where eyebrow movements appear to emphasize the utterance's melodic curves and thus have a specific pragmatic function.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-390"
  },
  "chang99b_eurospeech": {
   "authors": [
    [
     "Wen-Whei",
     "Chang"
    ],
    [
     "Heng-Iang",
     "Hsu"
    ],
    [
     "De-Yu",
     "Wang"
    ]
   ],
   "title": "Robust vector quantization for channels with memory",
   "original": "e99_1739",
   "page_count": 4,
   "order": 394,
   "p1": "1739",
   "pn": "1742",
   "abstract": [
    "This study focuses on two issues: parametric modeling of the channel and index assignment of codevectors, to design a vector quantizer that achieves high robustness against channel errors. We first formulated the design of a robust zero-redundancy vector quantizer as a combi-natorial optimization problem leading to a genetic search for the minimum distortion index assignment. This study also presents an index assignment algorithm based on Gilbert's model with parameter values estimated using a real-coded genetic algorithm. Simulation results indicate that the global explorative properties of genetic algorithms make them very effective at estimating Gilbert's model parameters and by using this model the index assignment can be developed to respond to channel conditions.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-391"
  },
  "kovesi99_eurospeech": {
   "authors": [
    [
     "Balázs",
     "Kövesi"
    ],
    [
     "Claude",
     "Lamblin"
    ],
    [
     "Catherine",
     "Quinquis"
    ],
    [
     "Philippe",
     "Thiérion"
    ],
    [
     "William",
     "Navarro"
    ]
   ],
   "title": "A multi-rate codec family based on GSM EFR and ITU-t g.729",
   "original": "e99_1743",
   "page_count": 4,
   "order": 395,
   "p1": "1743",
   "pn": "1746",
   "abstract": [
    "This paper gives the description of the multi-rate speech codec family which was the basis of the candidate developed by France Telecom and Nortel Networks for the ETSI GSM AMR normalization. This codec family is built on the ACELP technology. For each type of parameters derived from CELP analysis like LP filter coefficients, adaptive and fixed codebooks and gains, a set of vector quantizers has been designed. This allows to select the bit-rate for any parameter and consequently to vary the global speech coder bit-rate gradually in the range of 4.7  12.2 kbit/s. Formal subjective listening test results of the GSM AMR candidate codec performed by two world wide recognized listening laboratories in two languages (English and French) are also given.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-392"
  },
  "pan99_eurospeech": {
   "authors": [
    [
     "J. S.",
     "Pan"
    ],
    [
     "C. S.",
     "Shieh"
    ],
    [
     "T. F.",
     "Chiang"
    ]
   ],
   "title": "A novel channel distortion measure for vector quantization and a fuzzy model for codebook index assignment",
   "original": "e99_1747",
   "page_count": 4,
   "order": 396,
   "p1": "1747",
   "pn": "1750",
   "abstract": [
    "Vector quantization is very efficient for data compression of speech and image. The channel distortions are introduced due to channel noise. Assigning suitable indices to codevectors can reduce distortion due to an imperfect channel. Several codebook index assignment algorithms were proposed. Unfortunately, no algorithm is always better than the others for any bit error rate due to these algorithms are operated under the assumption of some fixed channel bit error rate which is not realistic. In this paper, a novel channel distortion measure is proposed by computing the expected chanel distortion using Belta distribution function. All codebook index assignment algorithms can be optimized based on this distortion measure. Besides, a fuzzy channel optimized vector quantization for codebook design and index assignment is also derived in this paper.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-393"
  },
  "sriratanaban99_eurospeech": {
   "authors": [
    [
     "C.",
     "Sriratanaban"
    ],
    [
     "A.",
     "Kondoz"
    ]
   ],
   "title": "A full-rate GSM-AMR candidate",
   "original": "e99_1751",
   "page_count": 4,
   "order": 397,
   "p1": "1751",
   "pn": "1754",
   "abstract": [
    "This paper describes a full-rate (FR) candidate speech codec for a new Adaptive Multi-Rate (AMR) codec standard. The proposed codec is a multi-rate codec, operating in various modes that have different bit-rate partitionings between source and channel coding. The source coding is based on Algebraic Code Excited Linear Predictive Coding (ACELP). Convolutional coding is used for the channel coding. Subjective tests show that at poor channel error conditions with low C/I-ratios of 7 dB, the lowest mode of the proposed codec produces speech quality comparable to G.728 in clean channel condition. The codec also provides substantial improvement on error robustness over the Enhanced Full Rate (EFR) GSM codec, by selecting an optimal coding rate to suit the channel condition. The codec performance passes twenty-six out of the twenty-nine test conditions, conducted according to [2]. Therefore, the proposed codec has passed the ETSI qualification test.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-394"
  },
  "villette99_eurospeech": {
   "authors": [
    [
     "S.",
     "Villette"
    ],
    [
     "M.",
     "Stefanovic"
    ],
    [
     "A.",
     "Kondoz"
    ]
   ],
   "title": "A multi-rate speech and channel codec: a GSM AMR half-rate candidate",
   "original": "e99_1755",
   "page_count": 5,
   "order": 398,
   "p1": "1755",
   "pn": "1758",
   "abstract": [
    "The current GSM standard is a fixed rate system which has been optimised to give good performance in all channel conditions. However since the channel conditions in a terrestrial mobile communication network such as GSM vary significantly between the best and the worst case, the existing GSM speech and channel coding performance can be improved by incorporating the dynamic channel conditions into the codec design. Under the initiative of the European Telecommunications Standards Institute (ETSI) such a system has been launched. This new standard is called AMR for Adaptive Multi-Rate: the source and channel coding rates can be adapted depending on the state of the channel, thus providing optimal balance between them at any time. The University of Surrey has submitted a candidate for this competition through the Mobile VCE. This candidate was the only one amongst eleven to use a vocoder in the half-rate GSM channel instead of a CELP based coder and tests ranked it among the best. This paper presents the system submitted for the half-rate channel as well as the results of the testing.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-395"
  },
  "bulyko99_eurospeech": {
   "authors": [
    [
     "Ivan",
     "Bulyko"
    ],
    [
     "Mari",
     "Ostendorf"
    ]
   ],
   "title": "Predicting gradient F0 variation: pitch range and accent prominence",
   "original": "e99_1819",
   "page_count": 4,
   "order": 399,
   "p1": "1819",
   "pn": "1822",
   "abstract": [
    "Many aspects of prosody prediction in speech synthesis could be improved, from placement of symbolic accent and phrase boundary markers to control of continuously varying parameters (e.g., duration, fundamental frequency). The goal of this work is to develop algorithms for predicting aspects of fundamental frequency typically said to have gradient variation: pitch range and prominence. In addition, the results of the automatic training methodology are used to investigate differences in prominence patterns associated with different genres of speech.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-396"
  },
  "deans99_eurospeech": {
   "authors": [
    [
     "Paul",
     "Deans"
    ],
    [
     "Andrew",
     "Breen"
    ],
    [
     "Peter",
     "Jackson"
    ]
   ],
   "title": "CART-based duration modeling using a novel method of extracting prosodic features",
   "original": "e99_1823",
   "page_count": 4,
   "order": 400,
   "p1": "1823",
   "pn": "1826",
   "abstract": [
    "The prediction of accurate segmental durations remains a difficult problem when synthesising speech from text. Inaccurate durations are often perceptually prominent and detract from the naturalness of the quality of speech. For a concatenative system, a statistical approach is an excellent way of predicting segmental durations. More specifically a CART (Classification And Regression Tree) method is appropriate [1], but only if it has been correctly trained with data that reflects a phonemes characteristics. A feature-set is used to describe the flavour of a phoneme in the process of building of CART trees. We describe a novel method where BTs Laureate Text-to-Speech system (TTS) is used to automatically donate the prosodic information required to make up the feature-set, ultimately being used as training data for building a CART tree. This tree, in turn, is used to predict segmental durations. The extraction of salience (derived from a metrical analysis of the text) and the other prosodic and segmental features in this way, is a novel concept. CART trees consistently show that this salience feature, in particular, has a large effect on the duration of a phoneme. The paper describes in detail this concept and shows the importance of salience. An evaluation of the effectiveness of CART-based duration modelling against the rule-based Laureate TTS method is given in the results.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-397"
  },
  "eom99_eurospeech": {
   "authors": [
    [
     "Ki-Wan",
     "Eom"
    ],
    [
     "Jin-Young",
     "Kim"
    ],
    [
     "Sun-Mi",
     "Kim"
    ]
   ],
   "title": "A primary study on the randomness control of the prosodic boundary index for natural synthetic speech",
   "original": "e99_1827",
   "page_count": 4,
   "order": 401,
   "p1": "1827",
   "pn": "1830",
   "abstract": [
    "In a text-to-speech (TTS) system a proper prosody control is necessary for natural synthetic speech. But synthetic speech generated by regular rules can make a listener irritated and bored, because the implemented prosody is always same for the same sentence. In this paper, the randomness of the prosodic boundary index (PBI), as a primary study on irregularity of prosody is discussed. We examined the PBI data of 1,800 spoken sentences and concluded that there were irregularities. We applied the conventional stochastic method (CSM) to the PBI prediction. However, CSM could not implement PBI irregularity. So we proposed local constraint Viterbi search algorithm (LCVS) as an alternative method. LCVS was evaluated by computer simulation.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-398"
  },
  "ferencz99_eurospeech": {
   "authors": [
    [
     "Attila",
     "Ferencz"
    ],
    [
     "István",
     "Nagy"
    ],
    [
     "Tünde-Csilla",
     "Kovács"
    ],
    [
     "Teodora",
     "Ratiu"
    ],
    [
     "Maria",
     "Ferencz"
    ]
   ],
   "title": "On a hybrid time domain-LPC technique for prosody superimposing used for speech synthesis",
   "original": "e99_1831",
   "page_count": 4,
   "order": 402,
   "p1": "1831",
   "pn": "1834",
   "abstract": [
    "Wishing to obtain a more natural quality of the synthesized speech and to eliminate the disadvantages of the previous text-to-speech (TTS) systems evolved in our institute (Software ITC Cluj-Napoca, Romania), we experimented and developed a new synthesis method that combines the advantages of time domain signal processing with the requirement of pitch and duration modification (required by intonation). This paper presents some theoretical considerations, signal processing and implementation aspects of this pitch alteration technique that was adapted for the new version of the ROMVOX TTS system.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-399"
  },
  "fackrell99_eurospeech": {
   "authors": [
    [
     "J. W. A.",
     "Fackrell"
    ],
    [
     "H.",
     "Vereecken"
    ],
    [
     "J.-P.",
     "Martens"
    ],
    [
     "Bert Van",
     "Coile"
    ]
   ],
   "title": "Multilingual prosody modelling using cascades of regression trees and neural networks",
   "original": "e99_1835",
   "page_count": 4,
   "order": 403,
   "p1": "1835",
   "pn": "1838",
   "abstract": [
    "This paper describes the use of automatically-trained models (Regression Trees and Multilayer Perceptrons) to predict three prosodic variables  phrase-boundary strength, word prominence and phoneme duration. The models are arranged in a cascade so that the predictions of phrase-boundaries are used as input features to the prominence model, and so on. Cascade models of this type have been constructed for 6 languages, using specially constructed databases, and objective performance statistics are described. For two languages (American English and Dutch) the results of a subjective evaluation experiment suggest that these prosodic models are at least as good as hand-crafted models, and sometimes better. Furthermore, preparing the training data automatically, rather than by manual labelling, seems to have no negative impact on the model performance.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-400"
  },
  "gu99_eurospeech": {
   "authors": [
    [
     "Wentao",
     "Gu"
    ],
    [
     "Chilin",
     "Shih"
    ],
    [
     "Jan P.H. van",
     "Santen"
    ]
   ],
   "title": "An efficient speaker adaptation method for TTS duration model",
   "original": "e99_1839",
   "page_count": 4,
   "order": 404,
   "p1": "1839",
   "pn": "1842",
   "abstract": [
    "This paper is a continuation of our previous study [1], where an efficient speaker adaptation method was proposed for TTS duration model. The goal was achieved by text selection and weight estimation. The result there was preliminary because its only derived from one sentence set. After the analysis on multiple sentence sets, we can now evaluate the robustness of the method better and hence a more confident conclusion is given. Based on the observation that some language-specific information is well preserved across speakers, the proposed method is supported. By a further comparison between various adaptation models, the linear weighted model shows the best performance, and therefore presents an efficient way to adapt the duration model from the source speaker to target speakers with a very small training corpus.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-401"
  },
  "house99_eurospeech": {
   "authors": [
    [
     "David",
     "House"
    ],
    [
     "Linda",
     "Bell"
    ],
    [
     "Kjell",
     "Gustafson"
    ],
    [
     "Linn",
     "Johansson"
    ]
   ],
   "title": "Child-directed speech synthesis: evaluation of prosodic variation for an educational computer program",
   "original": "e99_1843",
   "page_count": 4,
   "order": 405,
   "p1": "1843",
   "pn": "1846",
   "abstract": [
    "This paper addresses the question of how children between the ages of nine and eleven perceive and respond to prosodic variation in speech synthesis. Prosodic features were varied in samples of both concatenative and formant synthesis. Children and an adult control group were asked to compare these samples and to evaluate which were the most fun and which were the most natural. Results indicate that children perceive prosodic differences in the synthesis examples and prefer large manipulations in F0 and duration when a fun voice is intended. Even for naturalness, the children often prefer larger manipulations in F0 than are present in the default versions of the synthesis. These results can have implications for the implementation of synthesis in the context of a commercial computer program for children and more widely for child-directed speech synthesis.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-402"
  },
  "huckvale99_eurospeech": {
   "authors": [
    [
     "Mark",
     "Huckvale"
    ]
   ],
   "title": "Representation and processing of linguistic structures for an all-prosodic synthesis system using XML",
   "original": "e99_1847",
   "page_count": 4,
   "order": 406,
   "p1": "1847",
   "pn": "1850",
   "abstract": [
    "The ProSynth speech synthesis project aims to re-implement and extend the YorkTalk all-prosodic synthesis system in an open manner preserving its most ap-pealing theoretical aspects. A significant novel aspect of the architecture of ProSynth is the use of the extensible mark-up language (XML) as a computational formalism for the representation of hierarchical linguistic structures. The facilities provided by XML match closely the requirements to represent the phonological features of an utterance in a metrical prosodic structure, namely: nodes described by attribute-value pairs forming strict hierarchies. The XML formalism also leads to an elegant and efficient method for representing declarative phonological contexts under which phonetic interpretation is performed.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-403"
  },
  "park99_eurospeech": {
   "authors": [
    [
     "Won",
     "Park"
    ],
    [
     "Hyung-Bin",
     "Park"
    ],
    [
     "Myung-Jin",
     "Bae"
    ]
   ],
   "title": "A study on a pitch alteration by using the formant and phase compensation technique",
   "original": "e99_1851",
   "page_count": 4,
   "order": 407,
   "p1": "1851",
   "pn": "1854",
   "abstract": [
    "In the area of the speech synthesis techniques, the waveform coding methods maintain the intelligibility and naturalness of synthetic speech. In order to apply the waveform coding techniques to synthesis by rule, we must be able to alter the pitch of synthetic speech. In this paper, we propose a new pitch alteration method that compensates the formant and phase information by using the inverse filter of LP analysis and the pitch alteration method of the time domain. For performance test we used as the spectrum distortion rate as objective criterion and the MOS(Mean Opinion Score) was used as subjective criterion. As a result, the spectrum distortion and MOS are obtained by 0.6% and 4.0, respectively.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-404"
  },
  "lee99_eurospeech": {
   "authors": [
    [
     "Tan",
     "Lee"
    ],
    [
     "Helen M.",
     "Meng"
    ],
    [
     "Wai H.",
     "Lau"
    ],
    [
     "W. K.",
     "Lo"
    ],
    [
     "P. C.",
     "Ching"
    ]
   ],
   "title": "Micro-prosodic control in cantonese text-to-speech synthesis",
   "original": "e99_1855",
   "page_count": 5,
   "order": 408,
   "p1": "1855",
   "pn": "1858",
   "abstract": [
    "This paper describes a pioneer study on prosodic control for Cantonese text-to-speech synthesis. We attempt to establish a set of segment-level duration rules and context-dependent F0 profiles and apply them to a syllable-based concatenative speech synthesizer which uses TD-PSOLA as prosodic modification technique. The prosodic features are extracted by statistical characterization of a large amount of speech data. Subjective listening test shows that the micro-prosodic control results in a marginal but consistent improvement in perceptual naturalness.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-405"
  },
  "mixdorff99_eurospeech": {
   "authors": [
    [
     "Hansjörg",
     "Mixdorff"
    ],
    [
     "Dieter",
     "Mehnert"
    ]
   ],
   "title": "Exploring the naturalness of several German high-quality-text-to-speech systems",
   "original": "e99_1859",
   "page_count": 4,
   "order": 409,
   "p1": "1859",
   "pn": "1862",
   "abstract": [
    "The synthesis of near-to-natural F0 contours is an important issue in text-to-speech and crucial to the naturalness and intelligibility of synthetic speech. In earlier studies of the first author a model of German intonation was developed that is based on the quantitative Fujisaki-model. The current paper addresses a perception experiment comparing a TTS-system incorporating this new approach with several German TTS-systems with high segmental quality. Natural speech samples and a synthesis version with natural segment durations were used as references. Results show, that the natural speech samples unanimously received 10 points on a 0 to 10 point scale. The best TTS-systems cluster around a mean value of 5.0, whereas the variant with natural durations reached a mean score of 6.6 points, indicating the importance of closely modeling natural segment durations.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-406"
  },
  "sakurai99_eurospeech": {
   "authors": [
    [
     "A.",
     "Sakurai"
    ],
    [
     "Hiromichi",
     "Kawanami"
    ],
    [
     "Keikichi",
     "Hirose"
    ]
   ],
   "title": "Detecting accent sandhi in Japanese using a superpositional F0 model",
   "original": "e99_1863",
   "page_count": 4,
   "order": 410,
   "p1": "1863",
   "pn": "1866",
   "abstract": [
    "In this report, we propose a method for automatic prosodic structure recognition of Japanese utterances based on a superpositional F0 model, focusing particularly on the accent sandhi phonemenon in compound nouns. The method enables automatic labeling of F0 contours using the model, which can be useful for creating prosodic databases containing F0 contours in a parametric form. The prosodic structure is identified by comparing the distances between F0 contours generated by hypothetical model configurations and the extracted F0 contour, and choosing the configuration that yields the smallest distance. In this paper, we apply the method to detect the accent sandhi pattern of compound nouns made up of 2 or more words, and show that the method can correctly identify their prosodic structure, except for 1-mora deviations in the position of the accent nucleus.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-407"
  },
  "kitagawa99_eurospeech": {
   "authors": [
    [
     "Satoshi",
     "Kitagawa"
    ],
    [
     "Nick",
     "Campbell"
    ]
   ],
   "title": "Focus detection by comparison of speech waveforms",
   "original": "e99_1867",
   "page_count": 4,
   "order": 411,
   "p1": "1867",
   "pn": "1870",
   "abstract": [
    "For the eficient translation of speech by machine, the word sequence alone is not always sufficient to convey the intended meaning. Prosodic information can be lost in the speech recognition process. This paper presents methods by which focus can be detected in the input speech using timing and pitch information. By comparing the prosodic characteristics of an input utterance against profiles generated by components of a speech synthesiser for a default rendition of the same sequence of words, we are able to detect areas in the signal where prominence has been added.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-408"
  },
  "tatham99_eurospeech": {
   "authors": [
    [
     "Mark",
     "Tatham"
    ],
    [
     "Eric",
     "Lewis"
    ],
    [
     "Katherine",
     "Morton"
    ]
   ],
   "title": "An advanced intonation model for synthesis",
   "original": "e99_1871",
   "page_count": 4,
   "order": 412,
   "p1": "1871",
   "pn": "1874",
   "abstract": [
    "The advanced intonation model for speech synthesis described here has a three level architecture. An initial abstract characterisation designed to represent intonation at the level of cognitive percept is rewritten to an intermediate representation which is speaker independent, yet which accurately reflects physical pitch contours. At this stage the contours lack the variability associated with natural speech. This representation is then further rewritten to provide an actual physical contour (now including variability and other natural phenomena such as micro-intonation). One or two examples are given for stages one and two, and some indication of how we tackle stage three.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-409"
  },
  "takano99_eurospeech": {
   "authors": [
    [
     "Satoshi",
     "Takano"
    ],
    [
     "Masanobu",
     "Abe"
    ]
   ],
   "title": "A new F0 modification algorithm by manipulating harmonics of magnitude spectrum",
   "original": "e99_1875",
   "page_count": 4,
   "order": 413,
   "p1": "1875",
   "pn": "1878",
   "abstract": [
    "This paper proposes a new speech modification algorithm based on a vocoder framework to synthesize high quality speech. Its innovation is in preserving the fine structure of the magnitude spectrum. A key point is the use of a compensatory gaussian window\" to extract moderate F0 harmonics structures in the magnitude spectrum. The other key point is, starting from the magnitude spectrum, generating the F0 harmonics structures that match the target's fundamental frequency. Preference tests show that the proposed algorithm synthesizes higher quality speech than TD-PSOLA if large prosody modification is needed, and that the spectral envelope produced by the proposed algorithm is superior to any other conventional vocoders, especially when modifying the frequency upward.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-410"
  },
  "villarnavarro99_eurospeech": {
   "authors": [
    [
     "Juan Manuel",
     "Villar Navarro"
    ],
    [
     "Eduardo",
     "López Gonzalo"
    ],
    [
     "José",
     "Relaño Gil"
    ]
   ],
   "title": "A mixed strategy approach to Spanish prosody",
   "original": "e99_1879",
   "page_count": 4,
   "order": 414,
   "p1": "1879",
   "pn": "1882",
   "abstract": [
    "In this paper we introduce a new method for the synthesis of Spanish prosody suitable for the automatic generation of prosody in a Text-to-Speech system. As some methods we have already proposed our approach is data-based, it models joint F0 contour and segmental durations and its linguistic analysis is rule-based. Unlike previous works it uses a mixture of a priori breath group classification (linguistically based) and data-based phonological mapping. This new approach together with the previous ones form a quite open framework for analysis and synthesis of Spanish prosody. This approach leaves room for growing from a general prosodic model towards application specific prosody. The new method is successfully tested in a particular style of telephone number reading, quite dificult to pick up by previous methods.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-411"
  },
  "ainsworth99_eurospeech": {
   "authors": [
    [
     "William A.",
     "Ainsworth"
    ]
   ],
   "title": "Perception of overlapping syllables",
   "original": "e99_1883",
   "page_count": 4,
   "order": 415,
   "p1": "1883",
   "pn": "1886",
   "abstract": [
    "Some experiments have been performed in which the perception of two consonant-vowel syllables, the starting time of one was delayed relative to the other, have been studied. The consonant was /w/ or /j/, the vowel /i/ or /a/, and the fundamental frequency of the syllable 100 or 150 Hz. The delay was varied from 0 ms (simultaneous presentation) to 200 ms (consecutive presentation). As expected, the frequency of hearing both consonants correctly increased as the delay in starting times increased and the amount of overlap decreased. Unexpectantly, however, differences in fundamental frequency appeared to have no effect. With certain degrees of overlap, consonants were perceived which were not present in the physical stimulus.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-412"
  },
  "cerrato99_eurospeech": {
   "authors": [
    [
     "Loredana",
     "Cerrato"
    ],
    [
     "Andrea",
     "Paoloni"
    ]
   ],
   "title": "Are transcriptions of speech material recorded by means of bugs reliable?",
   "original": "e99_1887",
   "page_count": 4,
   "order": 416,
   "p1": "1887",
   "pn": "1890",
   "abstract": [
    "In this paper we will present the results of a per-ceptual test run to evaluate the effects of some postperceptual analysis of speech recorded by means of concealed microphones (i.e.bugs). Our main aim was to verify the hypothesis that in Italian post-perceptually at least, the grammaticality of a perceived utterance overrides other factors, in particular semantics, when the speech signal is weak or noisy. The results of our experiment might give some in-sight to the problems related to the task of transcription of speech recorded by means of bugs for forensic purposes.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-413"
  },
  "erdeljac99_eurospeech": {
   "authors": [
    [
     "Vlasta",
     "Erdeljac"
    ],
    [
     "Damir",
     "Horga"
    ]
   ],
   "title": "Influence of morphology on phoneme identification in spoken croatian",
   "original": "e99_1891",
   "page_count": 4,
   "order": 417,
   "p1": "1891",
   "pn": "1894",
   "abstract": [
    "The representation of morphological structures in the mental lexicon constitutes the inevitable topic in the investigation of cognitive aspects of the language data processing. In the present paper the influence of the noun case functions of the phoneme identification in Croatian language is investigated within the framework of the spoken word recognition. By means of the gating paradigm 36 subjects listened to and identified 24 words. The results showed strong influence of the morphological component, especially of the nominative case, on the word recognizability, regardless of the suffix phoneme (/a/ for feminine and /e/ for neuter gender). The results also showed the dynamics of the word identification process because it was possible to determine the decision-bias point position. Finally, the results could be interpreted in terms of the decompositional storage of morphological information of the regular complex Croatian nouns in the mental lexicon.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-414"
  },
  "hant99_eurospeech": {
   "authors": [
    [
     "James J.",
     "Hant"
    ],
    [
     "Abeer",
     "Alwan"
    ]
   ],
   "title": "Modeling the masking of formant transitions in noise",
   "original": "e99_1895",
   "page_count": 4,
   "order": 418,
   "p1": "1895",
   "pn": "1898",
   "abstract": [
    "Formant transitions are critical for identifying the place ofarticulation for consonants. If these transitions are masked bybackground noise, perceptual confusions can occur. To betterunderstand the masking of formant transitions, maskingthresholds were measured for tone glides and single-formanttrajectories of varying frequency extent (0 3 ERBs), duration(10, 30 and 100 ms), and center frequency (.5, 1.5, 3.5 kHz).Results show that thresholds are independent of frequencyextent and only depend on the duration and center-frequency ofthe transition. A novel, time-frequency detection model, fit toprevious noise-in-noise masking experiments (JASA 101, 2789-802 (1997)), is proposed which can predict these data.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-415"
  },
  "irino99_eurospeech": {
   "authors": [
    [
     "Toshio",
     "Irino"
    ],
    [
     "Roy D.",
     "Patterson"
    ]
   ],
   "title": "Stabilised wavelet mellin transform: an auditory strategy for normalising sound-source size",
   "original": "e99_1899",
   "page_count": 4,
   "order": 419,
   "p1": "1899",
   "pn": "1902",
   "abstract": [
    "We hear phonemes pronounced by men, women and children as approximately the same although the length of the vocal tract varies considerably from group to group. At the same time, we can identify the speaker group. This suggests that we extract and separate the size and shape information of sound sources. The impulse response of the vocal tract is compressed or expanded in time when the length of the vocal tract is compressed or expanded proportionally with the same cross-area function. The compressed and dilated versions of the impulse response can be converted into the same distribution using the Mellin transform. In this paper we show that the Mellin transform can be applied to the stabilised wavelet transform that forms the basis of the Auditory Image Model (AIM) of processing in the auditory pathway. The combined processing normalises source size information and produces a new, fruitful representation of source shape information, referred to as the \"Mellin Image\". This \"Stabilised Wavelet-Mellin Transform\" (SWMT) also provides the mathematical framework for the derivation of the gammachirp auditory filterbank and the signal synchronous analysis in AIM.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-416"
  },
  "palkova99_eurospeech": {
   "authors": [
    [
     "Zdena",
     "Palková"
    ],
    [
     "Jitka",
     "Janíková"
    ]
   ],
   "title": "Unintended preferences in the perceptive evaluation of rhythmical units in czech",
   "original": "e99_1903",
   "page_count": 6,
   "order": 420,
   "p1": "1903",
   "pn": "1906",
   "abstract": [
    "The experience shows the evaluative judgements obtained through perceptive testing may contain unintended preferences made by listeners with no apparent motivation.An advantage for additional interpretation adjustments is if a general trend in such a distortion and its main direction can be detected. In experiments devoted to finding out sound qualities that motivate the Czech listener to dividing a syllable chain into stress units (phonetic words) in a given manner, some listeners tend to choose one of .the two possible variants with higher probability with no apparent motivation. Results from experiment presented in the contribution allow to assume that the tendency to prefer, in the 5-syllable sequence segmentation, a 3:2 chain of syllables over a 2:3 chain is possibly a prevalent listening preference in Czech.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-417"
  },
  "pallier99_eurospeech": {
   "authors": [
    [
     "Christophe",
     "Pallier"
    ],
    [
     "Nuria",
     "Sebastián Gallés"
    ],
    [
     "Angels",
     "Colomé"
    ]
   ],
   "title": "Phonological representations and repetition priming",
   "original": "e99_1907",
   "page_count": 4,
   "order": 421,
   "p1": "1907",
   "pn": "1910",
   "abstract": [
    "An ubiquitous phenomenon in psychology is the repetition effect: a repeated stimulus is perceived better on the second occurrence than on the first. Yet, what counts as a repetition? When a spoken word is repeated, is it the acoustic shape or the linguistic type that matters? In the present study, we contrasted the contribution of acoustic and phonological features by using participants with different linguistic backgrounds: they came from two pop-ulations sharing a common vocabulary (Catalan) yet possessing different phonemic systems [1]. They performed a lexical decision task with lists containing words that were repeated verbatim, as well as words that were repeated with one phonetic feature changed. The feature changes were phonemic, i.e. linguistically relevant, for one population, but not for the other. The results revealed that the repetition effect was modulated by linguistic, not acoustic, similarity: it depended on the subjects phonemic system.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-418"
  },
  "vicsi99b_eurospeech": {
   "authors": [
    [
     "Klára",
     "Vicsi"
    ],
    [
     "F.",
     "Csatari"
    ],
    [
     "Zs.",
     "Bakcsi"
    ],
    [
     "A.",
     "Tantos"
    ]
   ],
   "title": "Distance score evaluation of the visualised speech spectra at audio-visual articulation training",
   "original": "e99_1911",
   "page_count": 4,
   "order": 422,
   "p1": "1911",
   "pn": "1914",
   "abstract": [
    "In the frame of the Inco-Copernicus program of the European Commission titled \"A Multimedia Multilingual Teaching and Training System for Speech Handicapped children\" an audio-visual pronunciation teaching and training method and software system has been developed for hearing and speech-handicapped persons to help them to control their speech production. During a part of the training the interpretation of the signal is based on the comparisons of the signal with the stored references. The aim of the present study is to find a distance measure that can help these comparisons and mirror the judgement of the listeners. Three spectral distance calculations have been compared. The good and unacceptable examples were separated well on the base of the Average Spectrum Distance calculation. This calculation can be the base of an automatic feedback of the actual pronunciation that could approach the decision of the listeners well.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-419"
  },
  "leeuwen99_eurospeech": {
   "authors": [
    [
     "David A. van",
     "Leeuwen"
    ],
    [
     "Michael de",
     "Louwere"
    ]
   ],
   "title": "Objective and subjective evaluation of the acoustic models of a continuous speech recognition system",
   "original": "e99_1915",
   "page_count": 4,
   "order": 423,
   "p1": "1915",
   "pn": "1918",
   "abstract": [
    "Two methods of assessing the acoustic modelling of an automatic speech recognition system are presented. The first is objective, and is based on determination of the phone substitution matrix. For this, a generalized alignment procedure is introduced, which leads to better results. The second method is subjective. Hypothesis words, as segmented by the recognizer, are evaluated by test subjects on their validity. Without additional context, humans accept about half of the false alarms found by a word spotter.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-420"
  },
  "whiteside99_eurospeech": {
   "authors": [
    [
     "S. P.",
     "Whiteside"
    ],
    [
     "R. A.",
     "Varley"
    ]
   ],
   "title": "Verbo-motor priming in the phonetic encoding of real and non-words",
   "original": "e99_1919",
   "page_count": 4,
   "order": 424,
   "p1": "1919",
   "pn": "1922",
   "abstract": [
    "We report an investigation of the production of real and non-words in two normal speaker groups. Group 1 consists of 6 young females (mean age 26 years) and Group 2 consists of 5 older females (mean age 54 years). The speech material used in the study consisted of two repetitions of 10 real, 10 pseudo-real and 10 non-words. The results from both repetitions for measures of response latency, utterance duration and word duration of both groups are presented and discussed with reference to observed patterns of verbo-motor priming. These patterns are discussed together with implications for phonetic encoding and the motor execution of speech.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-421"
  },
  "chen99e_eurospeech": {
   "authors": [
    [
     "Langzhou",
     "Chen"
    ],
    [
     "Taiyi",
     "Huang"
    ]
   ],
   "title": "An improved MAP method for language model adaptation",
   "original": "e99_1923",
   "page_count": 4,
   "order": 425,
   "p1": "1923",
   "pn": "1926",
   "abstract": [
    "This paper presents an improved MAP method for language model adaptation. The traditional MAP method mixes the task independent corpus and task dependent corpus using a fixed weight. In the method presented in this paper, we replaced the fixed weight with a function of history word. Another work in this paper is that a fuzzy controller was introduced in adaptation process, and three factors were used to be the input of the controller, they are: 1) the confidence of the estimation value, 2) the importance of the word, 3) the difference between the estimation value from the general corpus and from the adaptive corpus. The experiments showed that the improved method has the better performance than traditional model.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-422"
  },
  "clarkson99_eurospeech": {
   "authors": [
    [
     "Philip",
     "Clarkson"
    ],
    [
     "Tony",
     "Robinson"
    ]
   ],
   "title": "Towards improved language model evaluation measures",
   "original": "e99_1927",
   "page_count": 4,
   "order": 426,
   "p1": "1927",
   "pn": "1930",
   "abstract": [
    "Much recent research has demonstrated that the correlation between a language models perplexity and its effect on the word error rate of a speech recognition system is not as strong as was once thought. This represents a major problem for those in-volved in developing language models. This paper describes the development of new measures of language model quality. These measures retain the ease of computation and task inde-pendence that are perplexitys strengths, yet are considerably better correlated with word error rate. This paper also shows that mixture-based language models are improved by applying interpolation weights which are optimised with respect to these new measures, rather than a maximum likelihood criterion.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-423"
  },
  "huang99c_eurospeech": {
   "authors": [
    [
     "Taiyi",
     "Huang"
    ],
    [
     "Langzhou",
     "Chen"
    ]
   ],
   "title": "A novel language model based on self-organized learning",
   "original": "e99_1931",
   "page_count": 4,
   "order": 427,
   "p1": "1931",
   "pn": "1934",
   "abstract": [
    "Statistical language model is very important to speech recognition. To a system of special topic, domain dependent language model is much better than general model. There are two problems in traditional method to train topic dependent model: 1. The corpus of special topic is not as enough as general corpus. 2. An individual article always relates to more than one topics, traditional method has not considered this phenomena. This paper try to solve these two problems. We have present a new method to organize the corpus--the method based on fuzzy training subset. And the training of domain dependent models are based on these fuzzy subsets. At the same time, a self organized learning approach is introduced in training process to improve the models predicting ability. The self organized learning can improve the performance of models evidently.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-424"
  },
  "kilian99_eurospeech": {
   "authors": [
    [
     "Ute",
     "Kilian"
    ],
    [
     "Fritz",
     "Class"
    ]
   ],
   "title": "Combining syntactical and statistical language constraints in context-dependent language models for interactive speech applications",
   "original": "e99_1935",
   "page_count": 4,
   "order": 428,
   "p1": "1935",
   "pn": "1938",
   "abstract": [
    "In interactive speech applications the expected vocabulary and the expected user utterances change from one dialogue step to the next one. The use of several context dependent language models results in a better system performance than the use of a single model. In this paper we present a new approach combining syntactical and statistical language constraints to a single language model. Recognition results on a database of spelled city names are presented. Furthermore a match against the list of all possible city names is performed.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-425"
  },
  "martin99b_eurospeech": {
   "authors": [
    [
     "Sven",
     "Martin"
    ],
    [
     "Christoph",
     "Hamacher"
    ],
    [
     "Jorg",
     "Liermann"
    ],
    [
     "Frank",
     "Wessel"
    ],
    [
     "Hermann",
     "Ney"
    ]
   ],
   "title": "Assessment of smoothing methods and complex stochastic language modeling",
   "original": "e99_1939",
   "page_count": 4,
   "order": 429,
   "p1": "1939",
   "pn": "1942",
   "abstract": [
    "This paper studies the overall effect of language modeling on perplexity and word error rate, starting from a trigram model with a standard smoothing method up to complex state-of-the-art language models: (1) We compare different smoothing methods, namely linear vs. absolute discounting, interpolation vs. backing-off, and back-off functions based on relative frequencies vs. singleton events. (2) We show the effect of complex language model techniques by using distant-trigrams and automatically selected word classes and word phrases using a maximum likelihood criterion (i.e. minimum perplexity). (3) We show the overall gain of the combined application of the above techniques, as opposed to their separate assessment in past publications. (4) We give perplexity and word error rate results on the North American Business corpus (NAB) with a training text of about 240 million words and on the German Verbmobil corpus.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-426"
  },
  "bippus99_eurospeech": {
   "authors": [
    [
     "Rolf",
     "Bippus"
    ],
    [
     "Alexander",
     "Fischer"
    ],
    [
     "Volker",
     "Stahl"
    ]
   ],
   "title": "Domain adaptation for robust automatic speech recognition in car environments",
   "original": "e99_1943",
   "page_count": 4,
   "order": 430,
   "p1": "1943",
   "pn": "1946",
   "abstract": [
    "A major obstacle for the migration of automatic speech recog-nition into every-day life products is environmental robustness. Automatic speech recognition systems work reasonably well un-der clean (laboratory) conditions but degrade seriously under real world conditions (e.g. out-door, car). A lot of research work is devoted to increase the environmental robustness of automatic speech recognition systems. A common method is to use clean (office) data as a starting point and simulate the degraded environ-mental situation by additive artificial (e.g. Gaussian) or recorded noise from the real environment [1]. We study the validity of such additive noise experiments with regard to a real noisy environment. With regard to a previously published work on database adaptation we also examine the possible benefit when using models trained in the simulated environment as a starting point for adap-tation ([2]). We present experimental results on data recorded for task-dependent whole word and phoneme modeling in the car envi-ronment on data from the the MoTiV Car Speech Data Collection (CSDC) [3].\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-427"
  },
  "huang99d_eurospeech": {
   "authors": [
    [
     "Jun",
     "Huang"
    ],
    [
     "Yunxin",
     "Zhao"
    ],
    [
     "Stephen",
     "Levinson"
    ]
   ],
   "title": "A DCT-based fast enhancement technique for robust speech recognition in automobile usage",
   "original": "e99_1947",
   "page_count": 4,
   "order": 431,
   "p1": "1947",
   "pn": "1950",
   "abstract": [
    "In this paper, a fast computational method is proposed to approximate the Karhunen-Lo_eve transform (KLT) in signal-subspace-based speech enhancement algorithm. The discrete cosine transform (DCT) is shown to be a good approximation of KLT for the covariance matrix of the autoregressive process of order p (AR(p)). A fast algorithm which reduces the computation of eigenvalues of an N _ N symmetric Toeplitz matrix from O(N 3) inKLT to O(N 2 ) is developed. Experiment results demonstrate that the performance of the fast algorithm is very close to that of the KLT-based method in robust speech recognition in car environment while significantly reduces the computation time. An acoustic normalization scheme is also found to be usful to compensate the mismatch between the training and test conditions and thus further improves the recognition performance.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-428"
  },
  "hermus99_eurospeech": {
   "authors": [
    [
     "Kris",
     "Hermus"
    ],
    [
     "Ioannis",
     "Dologlou"
    ],
    [
     "Patrick",
     "Wambacq"
    ],
    [
     "Dirk Van",
     "Compernolle"
    ]
   ],
   "title": "Fully adaptive SVD-based noise removal for robust speech recognition",
   "original": "e99_1951",
   "page_count": 4,
   "order": 432,
   "p1": "1951",
   "pn": "1954",
   "abstract": [
    "This paper presents a new approach to improve the ro-bustness of large vocabulary continuous speech recogni-tion. The proposed technique { based on Singular ValueDecomposition (SVD) { originates from classical signalenhancement, but it is adapted to the specific require-ments imposed by the speech recognition process.Additive noise reduction is obtained by altering the sin-gular value spectrum of the signal observation matrix,thereby preserving speech signal components and sup-pressing noise-related components.The basic algorithms are developed for white noise butthey can easily be extended to the general coloured noisecase. With the aid of a noise reference, non-stationarynoise can be handled as well. All schemes are adaptive,and work in real-time.Recognition experiments on a noise-corrupted databasewith large vocabulary, continuous speech (Resource Man-agement) reveal that relative reductions of the WER1ofmore than 60 % are obtained.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-429"
  },
  "westphal99_eurospeech": {
   "authors": [
    [
     "Martin",
     "Westphal"
    ],
    [
     "Alex",
     "Waibel"
    ]
   ],
   "title": "Towards spontaneous speech recognition for on-board car navigation and information systems",
   "original": "e99_1955",
   "page_count": 4,
   "order": 433,
   "p1": "1955",
   "pn": "1958",
   "abstract": [
    "Speech recognition is seen to be of great benefit in on-board car navigation systems and assistance. The command word approach will be used for applications in the near future since the small active vocabulary and the hierarchical structure is much easier to cope with, from the developers side. An alternative approach, using spontaneous speech input, is far more complex but provides the user with an interface that is very intuitive and has fewer restrictions. The user can rely upon his or her experience in inter-human communication and utter spontaneous queries. In this paper, we describe the requirements and the collection of a continuous car speech data base and show first recognition results obtained under different environmental conditions in the car.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-430"
  },
  "das99_eurospeech": {
   "authors": [
    [
     "Subrata",
     "Das"
    ],
    [
     "David",
     "Lubensky"
    ],
    [
     "Cheng",
     "Wu"
    ]
   ],
   "title": "Towards robust speech recognition in the telephony network environment - cellular and landline conditions",
   "original": "e99_1959",
   "page_count": 4,
   "order": 434,
   "p1": "1959",
   "pn": "1962",
   "abstract": [
    "We describe several speaker-independent speech recognition studies conducted with both landline and cellular network telephone data. The cellular environment included the three dominant standards found in the United States: CDMA, TDMA and GSM. Our goal was to design a system that operated over all these four channels, handling their innate variations, such as those of background and line characteristics. Our baseline system was trained on 200 hours of landline telephone speech from about 25,000 speakers. We experimented with MAP adaptation utilizing some training sentences from our cellular and landline databases. We applied an LDA procedure to improve our performance. We compared the performances of these systems on several independent test databases and demonstrated the effectiveness of a hybrid system built with data taken from all four networks.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-431"
  },
  "bimbot99_eurospeech": {
   "authors": [
    [
     "Frédéric",
     "Bimbot"
    ],
    [
     "Mats",
     "Blomberg"
    ],
    [
     "Louis",
     "Boves"
    ],
    [
     "Gérard",
     "Chollet"
    ],
    [
     "Cédric",
     "Jaboulet"
    ],
    [
     "Bruno",
     "Jacob"
    ],
    [
     "Jamal",
     "Kharroubi"
    ],
    [
     "Johan",
     "Koolwaaij"
    ],
    [
     "Johan",
     "Lindberg"
    ],
    [
     "Johnny",
     "Mariethoz"
    ],
    [
     "Chafic",
     "Mokbel"
    ],
    [
     "Houda",
     "Mokbel"
    ]
   ],
   "title": "An overview of the PICASSO project research activities in speaker verification for telephone applications",
   "original": "e99_1963",
   "page_count": 4,
   "order": 435,
   "p1": "1963",
   "pn": "1966",
   "abstract": [
    "This paper presents a general overview of the current research activities in the European PICASSO project on speaker verification for telephone applications. First, the general formalism used by the project is described. Then the scientific issues under focus are discussed in detail. Finally, the paper briefly describes the Picassoft research platform. Along the article, entry points to more specific work also published in the Eurospeech99 proceedings are given.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-432"
  },
  "charlet99_eurospeech": {
   "authors": [
    [
     "D.",
     "Charlet"
    ]
   ],
   "title": "Integrating time-alignment information into the decision making for text-dependent HMM-based speaker verification",
   "original": "e99_1967",
   "page_count": 4,
   "order": 436,
   "p1": "1967",
   "pn": "1970",
   "abstract": [
    "This paper proposes an integration of the time-alignment information in the decision making for HMM-based text-dependent speaker verification. The principle is to consider acoustical score and time-alignment as joint observations for which a log-likelihoodratio is computed and compared to a threshold. It is shown that such integration has two distincts aspects, one being a kind of adaptation of the acoustical score threshold to the observed alignment, the other being the integrationof the speaker-specificity information of the alignment in the decision making. Exper-iments on a large-scale and realistic database are reported. They showthe interest of the proposed method and encour-age further investigation in such an approach.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-433"
  },
  "genoud99_eurospeech": {
   "authors": [
    [
     "Dominique",
     "Genoud"
    ],
    [
     "Gérard",
     "Chollet"
    ]
   ],
   "title": "Deliberate imposture: a challenge for automatic speaker verification systems.",
   "original": "e99_1971",
   "page_count": 4,
   "order": 437,
   "p1": "1971",
   "pn": "1974",
   "abstract": [
    "Recently, some large-scale text dependent speaker verification systems have been tested. They show that around 1% False Acceptance rate can be obtained on a test set when using an a priori threshold. So far, the majority of impostor tests are performed using speakers who dont really try to fool the system. This paper explores some ways to generate more realistic impersonations. The first system uses recorded sentences of the speaker which has to be impersonated, then, using an automatic speech recognition system as to align temporally the input speech, phonemes segments are concatenated to generate the password of a registered speaker. The second system describes a simple prototype of a voice transformation vocoder used as an impostor generator. The goal of these impersonations is not only to assess speaker verification systems but also to understand the parameters that define the intra- and inter-speaker variabilities using parametric models of speech.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-434"
  },
  "melin99_eurospeech": {
   "authors": [
    [
     "H.",
     "Melin"
    ],
    [
     "Johan",
     "Lindberg"
    ]
   ],
   "title": "Variance flooring, scaling and tying for text-dependent speaker verification",
   "original": "e99_1975",
   "page_count": 4,
   "order": 438,
   "p1": "1975",
   "pn": "1978",
   "abstract": [
    "The problem of how to estimate variance parameters in client models from scarce data is addressed in the context of text-dependent, HMM-based, automatic speaker verification. Variance flooring and variance scaling are investigated as two alternative estimation techniques and are used with or without variance tying on the state level to reduce the number of parameters to estimate. The best results are achieved with no tying and a variance flooring method where the floor to a variance vector in a client model is proportional to the corresponding variance vector in a gender-dependent, multi-speaker, non-client model. Further, variance tying reduces storage requirements considerably without much loss in recognition accuracy. It is also confirmed from a previous study that re-using non-client variances has comparable performance to variance flooring and is much simpler. Comparisons are made on three large telephone quality speech corpora.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-435"
  },
  "mariethoz99_eurospeech": {
   "authors": [
    [
     "Johnny",
     "Mariethoz"
    ],
    [
     "Dominique",
     "Genoud"
    ],
    [
     "Frédéric",
     "Bimbot"
    ],
    [
     "Chafic",
     "Mokbel"
    ]
   ],
   "title": "Client / world model synchronous alignement for speaker verification",
   "original": "e99_1979",
   "page_count": 4,
   "order": 439,
   "p1": "1979",
   "pn": "1982",
   "abstract": [
    "In speaker verification, two independent stochastic models, i.e. a client model and a non-client (world) model, are generally used to verify the claimed identity using a likelihood ratio score. This paper investigates a variant of this approach based on a common hidden process for both models. In this framework, both models share the same topology, which is conditioned by the underlying phonetic structure of the utterance. Then, two different output distributions are defined corresponding to the client vs. world hypotheses. Based on this idea, a synchronous decoding algorithm and the corresponding training algorithm are derived. Our first experiments on the SESP telephone database indicate a slight improvement with respect to a baseline system using independent alignments. Moreover, synchronousalignment offers a reduced complexity during the decoding process. Interesting perspectives can be expected.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-436"
  },
  "boros99_eurospeech": {
   "authors": [
    [
     "Manuela",
     "Boros"
    ],
    [
     "Paul",
     "Heisterkamp"
    ]
   ],
   "title": "Linguistic phrase spotting in a simple application spoken dialogue system",
   "original": "e99_1983",
   "page_count": 4,
   "order": 440,
   "p1": "1983",
   "pn": "1986",
   "abstract": [
    "Spoken dialogue systems have to cope with well known problems of spontaneous speech such as ungrammaticalities, hesitations or corrections. Besides, unrestricted speech poses the problem of paraphrasing, rising the number of utterances exceeding the linguistic coverage of the system. Concerning the linguistic processing of spontaneous speech, partial parsing represents a good method to achieve suficient robustness against these phenomena. In simple application domains however, mostly systems are implemented that do completely without speech understanding. In our paper we present an approach for robust linguistic speech processing and understanding for simple application domains. Our approach provides a complete analysis of user utterances based on partial analysis of semantically relevant sub-parts of the utterance. The semantic representations may be directly mapped onto data base indices thus bypassing the interpretation by the dialogue manager. On the other hand, the integration into complex dialogue systems allows for free dialogues and recovery strategies in case of system faults and misunderstandings.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-437"
  },
  "deinzer99_eurospeech": {
   "authors": [
    [
     "F.",
     "Deinzer"
    ],
    [
     "J.",
     "Fischer"
    ],
    [
     "U.",
     "Ahlrichs"
    ],
    [
     "Elmar",
     "Nöth"
    ]
   ],
   "title": "Learning of domain dependent knowledge in semantic networks",
   "original": "e99_1987",
   "page_count": 4,
   "order": 441,
   "p1": "1987",
   "pn": "1990",
   "abstract": [
    "For an eficient linguistic analysis of spoken queries alot of domain specific knowledge is needed and usuallyhas to be entered manually into the knowledge baseof each domain. This makes the adaption of dialoguesystems which base on explicit knowledge representation to new domains a very costly procedure. We usea frequency based statistical method combined withgeneral hidden markov models in order to learn do-main specific knowledge within a semantic networkformalism. As a framework we use a dialogue systemfor German train timetable information. By means ofexperiments we show that our statistical approach isnot only able to reach, but even outperforms previousresults with manually entered restrictions.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-438"
  },
  "hakkanitur99_eurospeech": {
   "authors": [
    [
     "Dilek",
     "Hakkani-Tür"
    ],
    [
     "Gökhan",
     "Tür"
    ],
    [
     "Andreas",
     "Stolcke"
    ],
    [
     "Elizabeth",
     "Shriberg"
    ]
   ],
   "title": "Combining words and prosody for information extraction from speech",
   "original": "e99_1991",
   "page_count": 4,
   "order": 442,
   "p1": "1991",
   "pn": "1994",
   "abstract": [
    "Information extraction from speech is a crucial step on the way from speech recognition to speech understanding. A preliminary step toward speech understanding is the detection of topic boundaries, sentence boundaries, and proper names in speech recognizer output. This is important since speech recognizer output lacks the usual textual cues to these entities (such as headers, paragraphs, sentence punctuation, and capitalization). Nu-merous word-based approaches to these tasks have been developed in the past; in this work we demonstrate the use of prosodic cues, alone and in combination with words, for segmentation and name finding. In experiments on the Broadcast News corpus, we find that prosodic cues alone allow sentence and topic segmentation that is at least as good as word-based methods alone, and that combining both types of cues gives significant wins. Named entity recognition, on the other hand, currently does not seem to benefit from prosodic cues, for several interesting reasons.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-439"
  },
  "ishikawa99_eurospeech": {
   "authors": [
    [
     "Kai",
     "Ishikawa"
    ],
    [
     "Eiichiro",
     "Sumita"
    ]
   ],
   "title": "Error correction translation using text corpora",
   "original": "e99_1995",
   "page_count": 4,
   "order": 443,
   "p1": "1995",
   "pn": "1998",
   "abstract": [
    "In this paper, we propose an error correction method using text corpora. In this method, recognition errors are corrected using phonetically similar examples in the text corpora. The reliability of the correction hypotheses are judged according to their semantic consistency and their phonetic similarity to the original input. We previously proposed an error correction method that uses a treebank [1]. However, the previous method was not flexible in its use of examples, because structural mismatches occurred between the input and examples due to recognition errors. In our new proposal, examples are treated as morpheme sequences. This enables us to use examples partially when there are no useful full-sentence-examples. We built our proposed method into a speech translation system and compared the translation quality for simple translation and translation with error correction. The rate of acceptable translation increased about 10% with our proposed method compared to simple translation.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-440"
  },
  "kronenberg99_eurospeech": {
   "authors": [
    [
     "S.",
     "Kronenberg"
    ],
    [
     "K.",
     "Skuplik"
    ]
   ],
   "title": "Efficient sentence disambiguation by preferred constituent order",
   "original": "e99_1999",
   "page_count": 4,
   "order": 444,
   "p1": "1999",
   "pn": "2002",
   "abstract": [
    "A major problem with (partially) free constituent order is to manifest preferences among structurally distinct parses of ambiguous sentences. In order to obtain scoring criteria a preferred constituent order can considerably support a best-first strategy. This work presents an experimentally evaluated model of preferred German constituent order in the middle field and its application for the implementation of a robust and eficient parsing strategy for spontaneous speech. This constituent order is used to guide the parallel LR(1)-parser to derive the preferred interpretation of ambiguous sentences.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-441"
  },
  "lee99b_eurospeech": {
   "authors": [
    [
     "Yue-Shi",
     "Lee"
    ],
    [
     "Hsin-Hsi",
     "Chen"
    ]
   ],
   "title": "Identifying linguistic segmentations in Chinese spoken dialogue",
   "original": "e99_2003",
   "page_count": 4,
   "order": 445,
   "p1": "2003",
   "pn": "2006",
   "abstract": [
    "In a continuous speech recognition system, a longer waveform is usually segmented into some shorter pieces based on simple acoustic criteria, such as unfilled pauses (i.e., silences). We call such a kind of segmentation as an acoustic segmentation. In general, the acoustic segmentations do not reflect the linguistic structure. They may fragment sentences or semantic units. Besides, they may also group together some unrelated units. Therefore, we need to resegment acoustic segmentations in order to output linguistically meaningful units such as clauses. We call such a kind of segmentation as a linguistic segmentation. This paper employs several acoustic and prosodic clues to resegment acoustic segmentations for identifying linguistic segmentations. Based on these clues, the experimental results show that a precision rate of 94.46% and a recall rate of 87.38% can be achieved.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-442"
  },
  "chiang99_eurospeech": {
   "authors": [
    [
     "Tung-Hui",
     "Chiang"
    ],
    [
     "Yi-Chung",
     "Lin"
    ]
   ],
   "title": "Error recovery for robust language understanding in spoken dialogue systems",
   "original": "e99_2007",
   "page_count": 4,
   "order": 446,
   "p1": "2007",
   "pn": "2010",
   "abstract": [
    "In this paper, we proposed an example-based approach aiming at recovering ill-formed inputs to improve robustness of spoken dialogue systems. In this approach, a treebank, which contains example sentences and their correct parse trees, is used to provide clues for fixing the errors of ill-formed inputs. Particularly, the proposed error recovery method is suitable for spoken dialogue application because of computationally efficiency. In addition, when evaluated in a Mandarine spoken dialogue system, the proposed method has shown to improve the system's understaning rate very significantly.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-443"
  },
  "liu99e_eurospeech": {
   "authors": [
    [
     "Xiaohu",
     "Liu"
    ],
    [
     "Pascale",
     "Fung"
    ],
    [
     "Chi Shun",
     "Cheung"
    ]
   ],
   "title": "A monolingual semantic decoder based on word sense disambiguation for mixed language understanding",
   "original": "e99_2011",
   "page_count": 5,
   "order": 447,
   "p1": "2011",
   "pn": "2014",
   "abstract": [
    "In this paper, a new method for spoken mixed language understanding is presented. By mixed language, we mean that the words included in one sentence may come from different languages, a primary language and a secondary language. In conventional statistical semantic decoders, the conceptual structure is represented as a hidden Markov model, the decoding of the conceptual content of a sentence is carried out with the Viterbi algorithm. To handle mixed language, an unsupervised word sense disambiguation module is proposed to convert the secondary language words into the primary language. The approach is evaluated in the ATIS domain, where the primary language is English and we assume the secondary language is Chinese. The average accuracy of our extended semantic decoder is 26% higher than the accuracy of the baseline semantic decoder. The advantages of the extended semantic decoder are (1) it can handle mixed language input, and (2) it needs neither secondary language training data nor mixed language training data. The approach can be used for any main-secondary language pairs.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-444"
  },
  "meng99_eurospeech": {
   "authors": [
    [
     "Helen M.",
     "Meng"
    ],
    [
     "Wai",
     "Lam"
    ],
    [
     "Carmen",
     "Wai"
    ]
   ],
   "title": "To believe is to understand",
   "original": "e99_2015",
   "page_count": 4,
   "order": 448,
   "p1": "2015",
   "pn": "2018",
   "abstract": [
    "This paper is about language understanding using Belief Networks. Language understanding is a key technology in human-computer conversational systems. These systems often need to handle information-seeking queries from the user regarding a restricted domain. We devised a method for identifying the users communicative goal(s) out of a finite set of within-domain goals. The problem is formulated as N binary decisions, each performed by a Belief Network. This formulation allows for the identification of queries with multiple goals, as well as queries with out-of-domain goals. Experiments with the ATIS corpus shows that around 90% of the user queries are correctly handled via goal classification, rejection or multiple goal identification.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-445"
  },
  "noth99_eurospeech": {
   "authors": [
    [
     "Elmar",
     "Nöth"
    ],
    [
     "Jürgen",
     "Haas"
    ],
    [
     "Volker",
     "Warnke"
    ],
    [
     "Florian",
     "Gallwitz"
    ],
    [
     "Manuela",
     "Boros"
    ]
   ],
   "title": "A hybrid approach to spoken dialogue understanding: prosody, statistics and partial parsing",
   "original": "e99_2019",
   "page_count": 4,
   "order": 449,
   "p1": "2019",
   "pn": "2022",
   "abstract": [
    "Linguistic processing in spoken dialogue systems has to be robust against a large number of phenomena such as recognizer errors, spontaneous speech phenomena and out-of-vocabulary (OOV) words. A commonly used solution to this problem is partial parsing, that aims at detecting only parts of sentences/utterances that are vital for the respective task of the parser. In our paper we present a framework for robust linguistic processing in our spoken dialogue system EVAR for train timetable information. The linguistic processor combines partial parsing with prosody and statistical concept prediction. Parsing is restricted to the detection and analysis of those parts of an utterance that are crucial for its understanding by the system. In order to accomplish this task most efficiently, the parser operates not only on word lattices as delivered by the recognizer, but also on prosodic information and statistical concept prediction.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-446"
  },
  "obuchi99_eurospeech": {
   "authors": [
    [
     "Yasunari",
     "Obuchi"
    ],
    [
     "Atsuko",
     "Koizumi"
    ],
    [
     "Yoshinori",
     "Kitahara"
    ],
    [
     "Jun'ichi",
     "Matsuda"
    ],
    [
     "Toshihisa",
     "Tsukada"
    ]
   ],
   "title": "Portable speech interpreter which has voice input and sophisticated correction functions",
   "original": "e99_2023",
   "page_count": 4,
   "order": 450,
   "p1": "2023",
   "pn": "2026",
   "abstract": [
    "This paper describes the development of a portable speech interpreter by which the user can look for words and relating sentences easily using his/her voice. The prototype uses a RISC microprocessor for the embedded use, resulting in a compact body of 15cm(W) and 6cm(H). The voice user interface is helpful, especially in small devices, but it has a serious problem related to the recognition errors. The total user interface must be designed carefully to minimize the inconvenience caused by the recognition errors. We have developed an interface in which the confinement of the dictionary realizes the quick and accurate recognition, and the syllable correction procedure provides an easy way to correct recognition errors. These features make it possible to input any words, which are found in a standard Japanese dictionary. Experimental evaluations show that the N-best recognition rate for words and syllables are acceptable.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-447"
  },
  "potamianos99b_eurospeech": {
   "authors": [
    [
     "Alexandros",
     "Potamianos"
    ],
    [
     "Giuseppe",
     "Riccardi"
    ],
    [
     "Shrikanth",
     "Narayanan"
    ]
   ],
   "title": "Categorical understanding using statistical ngram models",
   "original": "e99_2027",
   "page_count": 4,
   "order": 451,
   "p1": "2027",
   "pn": "2030",
   "abstract": [
    "In this paper, the speech understanding problem in the context of a spoken dialog system is formalized in a maximum likelihood framework. Word and dialog-state n-grams are used for building categorical understanding and dialog models, respectively. Acoustic confidence scores are incorporated in the understanding formulation. Problems due to data sparseness and out-of-vocabulary words are discussed. Incorporating dialog models reduces relative understanding error rate by 1525%, while acoustic confidence scores achieve a further 10% error reduction for a computer gaming application.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-448"
  },
  "spilker99_eurospeech": {
   "authors": [
    [
     "Jörg",
     "Spilker"
    ],
    [
     "Hans",
     "Weber"
    ],
    [
     "Günther",
     "Görz"
    ]
   ],
   "title": "Detection and correction of speech repairs in word lattices",
   "original": "e99_2031",
   "page_count": 4,
   "order": 452,
   "p1": "2031",
   "pn": "2034",
   "abstract": [
    "Speech repairs occur often in spontaneous spoken dialogues. The ability to detect and correct those repairs is necessary for every spoken language system. We present a framework to detect and correct speech repairs where all relevant levels of information, i.e., acoustics, lexic, syntax and semantics could be integrated. The basic idea is to reduce the search space for repairs as soon as possible by cascading filters that involve more and more features. At first an acoustic module generates hypotheses about the existence of a repair. Second a stochastic model suggests a correction for every hypothesis. Well scored corrections are inserted as new paths in the word lattice. A lattice parser then makes the final decision about accepting the repair.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-449"
  },
  "schadle99_eurospeech": {
   "authors": [
    [
     "Igor",
     "Schadle"
    ],
    [
     "Jean-Yves",
     "Antoine"
    ],
    [
     "Daniel",
     "Memmi"
    ]
   ],
   "title": "Connectionist language models for speech understanding: the problem of word order variation",
   "original": "e99_2035",
   "page_count": 4,
   "order": 453,
   "p1": "2035",
   "pn": "2038",
   "abstract": [
    "This paper emphasizes the importance of the question of word-order variation in connectionist language modeling. More precisely, it investigates whether recurrent networks can integrate various linguistic constraints to process variable word-order languages. This paper reports three experiments on the understanding of spoken French that suggest that recurrent architectures could apply to the understanding of variable spoken languages.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-450"
  },
  "siu99_eurospeech": {
   "authors": [
    [
     "Kai-Chung",
     "Siu"
    ],
    [
     "Helen M.",
     "Meng"
    ]
   ],
   "title": "Semi-automatic acquisition of domain-specific semantic structures",
   "original": "e99_2039",
   "page_count": 4,
   "order": 454,
   "p1": "2039",
   "pn": "2042",
   "abstract": [
    "This paper describes a methodology for semi-automatic grammar induction from unannotated corpora belonging to a restricted domain. The grammar contains both semantic and syntactic structures, which are conducive towards language understanding. Our work aims to ameliorate the reliance of grammar development on expert handcrafting or the availability of annotated corpora. To strive for a reasonable model for real data, as well as portability across domain and languages, we adopt a statistical approach. Our approach is also amenable to the optional injection of prior knowledge to aid grammar induction, and subsequent hand editing for grammar refinement. This constitutes the semi-automatic nature of the approach. Experiments with the ATIS corpus showed positive results in semantic parsing, when compared to an entirely handcrafted grammar.\n",
    "Keywords: grammar induction, semantic processing.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-451"
  },
  "takezawa99_eurospeech": {
   "authors": [
    [
     "Toshiyuki",
     "Takezawa"
    ]
   ],
   "title": "Transformation into language processing units by dividing and connecting utterance units",
   "original": "e99_2043",
   "page_count": 4,
   "order": 455,
   "p1": "2043",
   "pn": "2046",
   "abstract": [
    "The utterance units that serve as input to speech translation and/or spoken dialogue systems that handle spontaneous speech are not always sentences. However, the processing units of language translation are sentences. Since we do not have enough knowledge about the sentences of spoken languages, we use the term \\language processing units\" instead of \\sentences.\" First, using conventionally interpreted dialogue data, we show that utterance units sometimes need to be divided into several language processing units, and sometimes need to be connected to make up a single language processing unit. Next, we propose a method of transforming utterance units into language processing units based on pause information and the N -gram of fine-grained part-of-speech subcategories. We confirm in experiments that our method yields good results. The technique reported in this paper has been introduced into the ATR-MATRIX speech-to-speech translation system from Japanese to English.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-452"
  },
  "wong99_eurospeech": {
   "authors": [
    [
     "Aboy",
     "Wong"
    ],
    [
     "Dekai",
     "Wu"
    ]
   ],
   "title": "Learning a lightweight robust deterministic parser",
   "original": "e99_2047",
   "page_count": 4,
   "order": 456,
   "p1": "2047",
   "pn": "2050",
   "abstract": [
    "We describe a method for automatically learning a parser from labeled, bracketed corpora that results in a fast, robust, lightweight parser that is suitable for real-time dialog systems and similar applications. Unlike ordinary parsers, all grammatical knowledge is captured in the learned decision trees, so no explicit phrase-structure grammar is needed. Another characteristic of the architecture is robustness, since the input need not fit pre-specified productions. Even without using specific lexical features, we have achieved respectable labeled bracket accuracies of about 81% precision and 82% recall. Processing speed is more than 500 words per CPU second. We keep the parameter space small (in comparison to other statistically learned parsers) by using only part-of-speech tags and constituent labels as features. Without any optimization, the decision trees consume only 6M of memory, making it possible to run on platforms with limited memory. The learning method is readily applicable to other languages. Preliminary experiments on a Chinese corpus (which contains about 3000 sentences from Chinese primary school text) have yielded results comparable to that for English.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-453"
  },
  "wu99b_eurospeech": {
   "authors": [
    [
     "Dekai",
     "Wu"
    ],
    [
     "Zhifang",
     "Sui"
    ],
    [
     "Jun",
     "Zhao"
    ]
   ],
   "title": "An information-based method for selecting feature types for word prediction",
   "original": "e99_2051",
   "page_count": 4,
   "order": 457,
   "p1": "2051",
   "pn": "2054",
   "abstract": [
    "This paper uses an information-based approach to conduct feature types selection for language modeling in a systematic manner. We describe a quantitative analysis of the information gain and the information redundancy for various combinations of feature types inspired by both dependency structure and bigram structure through analyzing an English treebank corpus and taking word prediction as the object. The experiments yield several conclusions on the predictive value of several feature types and feature types combinations for word prediction, which are expected to provide reliable reference for feature type selection in language modeling.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-454"
  },
  "wang99c_eurospeech": {
   "authors": [
    [
     "Ye-Yi",
     "Wang"
    ]
   ],
   "title": "A robust parser for spoken language understanding",
   "original": "e99_2055",
   "page_count": 4,
   "order": 458,
   "p1": "2055",
   "pn": "2058",
   "abstract": [
    "This paper describes a robust parsing algorithm for spoken language understanding. Comparing with the other work in robust parsing, we focus on building a parser that is robust to not only ill-formed spontaneous spoken language inputs but also under-specified grammars. Preliminary experiment results show that the parsing performance deteriorates more gracefully than another parser we have used when the grammar is more under-specified.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-455"
  },
  "barbosa99_eurospeech": {
   "authors": [
    [
     "Plínio A.",
     "Barbosa"
    ],
    [
     "Fábio",
     "Violaro"
    ],
    [
     "Eleonora C.",
     "Albano"
    ],
    [
     "Flávio",
     "Simoes"
    ],
    [
     "Patrícia",
     "Aquino"
    ],
    [
     "Sandra",
     "Madureira"
    ],
    [
     "Edson",
     "Francozo"
    ]
   ],
   "title": "Aiuruete: a high-quality concatenative text-to-speech system for brazilian portuguese with demisyllabic analysis-based units and a hierarchical model of rhythm production",
   "original": "e99_2059",
   "page_count": 4,
   "order": 459,
   "p1": "2059",
   "pn": "2062",
   "abstract": [
    "Aiuruete is a high-quality concatenative TTS system for Brazilian Portuguese. Its name (pronounced [aju,rue'te]) illustrates the challenges we have fixed as a research paradigm: to feed the system with the specificities of our language, highlighted by an up-to-date discussion of the Phonology/Phonetics and prosody/segments interfaces, without a huge computational cost. The choice for the concatenative method of synthesis was determined by a trade-off between scientific (the desired human-like naturalness of the acoustic output) and practical (mainly reduced staff and tight schedule) constraints. Procedural and declarative modules are described here: the ortofon, the unit inventory, the rhythm model and the synthesis techniques. Aiuruete is still being evaluated, but when compared to the previous system, adopted by the national telephony company, its superior quality is apparent.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-456"
  },
  "burileanu99_eurospeech": {
   "authors": [
    [
     "Dragos",
     "Burileanu"
    ],
    [
     "Claudius",
     "Dan"
    ],
    [
     "Mihai",
     "Sima"
    ],
    [
     "Corneliu",
     "Burileanu"
    ]
   ],
   "title": "A parser-based text preprocessor for romanian language TTS synthesis",
   "original": "e99_2063",
   "page_count": 4,
   "order": 460,
   "p1": "2063",
   "pn": "2066",
   "abstract": [
    "Text preprocessing plays an important role in a text-tospeech (TTS) synthesis system. The correct detection and interpretation of input strings influence the overall system accuracy and contribute to the conversion of an unrestricted text into synthetic speech. This paper describes the design philosophy of a preprocessing module for a TTS system in Romanian language. The preprocessor is implemented using the standard flex/bison lexer and parser generators. The paper discusses the text preprocessing task and the major difficulties connected with Romanian language, proposes a set of definitions and rules, gives some implementation details and concludes with a few considerations about the TTS system and performances of the preprocessing module.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-457"
  },
  "carlberger99_eurospeech": {
   "authors": [
    [
     "Alice",
     "Carlberger"
    ]
   ],
   "title": "Nparse - a shallow n-gram-based grammatical-phrase parser",
   "original": "e99_2067",
   "page_count": 4,
   "order": 461,
   "p1": "2067",
   "pn": "2070",
   "abstract": [
    "Nparse is a shallow probabilistic unification-based parser for N-best list resorting and the finding of simple grammatical phrases. It is data-driven and robust, allowing both domain-specific and unrestricted-language training. We believe it can be an interesting alternative for use in a synthesis or recogni-tion front end. This parser has been trained for Swedish on a fine-grained set of grammatical-phrase nodes and grammatical features and evaluated on three language domains. A tree bank database has been built and a detailed linguistic assessment performed. Later, these results will be compared with evalua-tion on a simplified node-and-feature system. Our aim is to find the optimal system complexity for accurately establishing phrase boundaries and phrase types in newspaper text and, ul-timately, unrestricted language. For this, a combination of it-erative manual training and unsupervised training will be used.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-458"
  },
  "dermatas99_eurospeech": {
   "authors": [
    [
     "Evangelos",
     "Dermatas"
    ],
    [
     "George",
     "Kokkinakis"
    ]
   ],
   "title": "A language-independent probabilistic model for automatic conversion between graphemic and phonemic transcription of words",
   "original": "e99_2071",
   "page_count": 4,
   "order": 462,
   "p1": "2071",
   "pn": "2074",
   "abstract": [
    "In this paper we present a novel language-independent probabilistic model for automatic grapheme-to-phoneme and phoneme-to-grapheme conversion of words. In a fully unsupervised training procedure, two processes are applied; the transformation rules, which usually fail to provide the correct symbols, are eliminated, and new variable-length string transformation rules are defined improving the string transformation accuracy in the training data. In an iterative process the probabilistic transformation rules are updated in the direction of reducing the error rate of the transformed symbols. Long-term dependencies are defined automatically. Training and testing of the model was carried out on lexicon and natural language corpora of six European Languages. Accurate generalisations have been achieved in all experiments for both transformation directions using a relative small number of defined rules in the training procedure. It is demonstrated that the variable-length probabilistic rules are sufficiently effective for describing bi-directional transcription.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-459"
  },
  "gros99_eurospeech": {
   "authors": [
    [
     "Jerneja",
     "Gros"
    ],
    [
     "F.",
     "Mihelic"
    ]
   ],
   "title": "Acquisition of an extensive rule set for slovene grapheme-to-allophone transcription",
   "original": "e99_2075",
   "page_count": 4,
   "order": 463,
   "p1": "2075",
   "pn": "2078",
   "abstract": [
    "An extensive rule set for grapheme-to-allophone conversion of Slovene texts has been defined and evaluated. Another rule set has been developed for pronunciation of names. The efficiency of both S5 rule sets was compared to the one of the Onomastica rule set on two manually transcribed test data sets. A performance test applied on the S5 pronunciation dictionary showed error rates of about 30% in the stress assignment and consequently in the phonetic transcription. In case stress assignment and the transcriptions of graphemic /e/ and /o/ in stressed syllables had been marked in advance a transcription success rate of nearly 100% was achieved both on names and on standard words with the S5 names rule sets and the S5 standard words rule set respectively.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-460"
  },
  "ho99b_eurospeech": {
   "authors": [
    [
     "Ching-Hsiang",
     "Ho"
    ],
    [
     "Saeed",
     "Vaseghi"
    ],
    [
     "Aimin",
     "Chen"
    ]
   ],
   "title": "Voice conversion between UK and US accented English",
   "original": "e99_2079",
   "page_count": 4,
   "order": 464,
   "p1": "2079",
   "pn": "2082",
   "abstract": [
    "This paper presents an HMM-based method and ex-perimental results for voice conversion between UK and US accented English. Phonetic-tree based tied-state triphone HMMs are used to map equivalent states of the source and target spectra. Then a linear transformation method is incorporated to estimate the most likely target spectra for a given input. The map-ping is between two different sets of phoneme i.e. the 44-phoneme UK English BEEP phone set and 39-phoneme US CMU phone set. Finally, a prosody ad-aptation is applied to tune the prosodic parameters. The experiments are based on voice conversion be-tween speakers speaking different unrestricted texts. Acoustic-phonetic mapping between two different ac-cents database enables us to attempt to deconstruct accents to investigate how they are distributed among different parameters such as spectra, energy contour, pitch, and duration.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-461"
  },
  "mizuno99_eurospeech": {
   "authors": [
    [
     "Hideyuki",
     "Mizuno"
    ],
    [
     "Masanobu",
     "ABE"
    ],
    [
     "Shin'ya",
     "Nakajima"
    ]
   ],
   "title": "Development of speech design tool \"SESIGN99\" to enhance synthesized speech",
   "original": "e99_2083",
   "page_count": 4,
   "order": 465,
   "p1": "2083",
   "pn": "2086",
   "abstract": [
    "This paper introduces a new speech design tool (Sesign99) that can convert monotonous synthesized speech into a variety of speech styles. As multimedia services such as games, interactive movies and WWW home pages become more popular, more attention is being focusedon the creation, management, and transmission of speech messages. Although speech synthesis-by-rule has improved with recent advances in TTS, the monotonous features of speech produced by synthesis-by-rule hamper the introduction of TTS to the application areas listed above. There is demand for software that allows even the novice to produce engaging and natural-sounding speech messages.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-462"
  },
  "hain99_eurospeech": {
   "authors": [
    [
     "Horst-Udo",
     "Hain"
    ]
   ],
   "title": "Automation of the training procedures for neural networks performing multi-lingual grapheme to phoneme conversion",
   "original": "e99_2087",
   "page_count": 4,
   "order": 466,
   "p1": "2087",
   "pn": "2090",
   "abstract": [
    "Any TTS system requires accurate grapheme tophoneme conversion routines due to the limited pro-nunciation dictionaries. Data driven approaches of-fer the possibility to train new languages withoutthe knowledge of rules. A big problem is to preparehuge databases for the training. In this paper theautomation of the data preparation and the patterngeneration for the training of a neural network is ad-dressed. This automation is language independent,and no native expert is required. The data prepara-tion, the generation of the training patterns and thetraining itself are done completely automatically.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-463"
  },
  "koutny99_eurospeech": {
   "authors": [
    [
     "Ilona",
     "Koutny"
    ]
   ],
   "title": "Parsing hungarian sentences in order to determine their prosodic structures in a multilingual TTS system",
   "original": "e99_2091",
   "page_count": 4,
   "order": 467,
   "p1": "2091",
   "pn": "2094",
   "abstract": [
    "Naturally sounding synthesized speech requires proper prosodic structure. The unequivocal relation between syntax and prosody is contestable, but for lack of other information on discourse structure, we have to rely on syntactic structure in order to determine some prosodic features. This work - based on basic research results in Hungarian linguistics - started with a preliminary parser for simple Hungarian sentences. It tries to establish phrase boundaries with possible breaks, and to determine the focus as well as other stressed and unstressed elements The current goal is to automatically predict the above prosodic features for complex sentences. The steps of the prosody module are: pre-processing, morphological analysis, phrase parsing, sentence parsing, search for the focus, stress and break distribution. The prosodic predictor is implemented in the MULTIVOX multilingual text-to-speech system.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-464"
  },
  "mihkla99_eurospeech": {
   "authors": [
    [
     "Meelis",
     "Mihkla"
    ],
    [
     "Arvo",
     "Eek"
    ],
    [
     "Einar",
     "Meister"
    ]
   ],
   "title": "Text-to-speech synthesis of estonian",
   "original": "e99_2095",
   "page_count": 5,
   "order": 468,
   "p1": "2095",
   "pn": "2098",
   "abstract": [
    "The aim of this text-to-speech synthesis system is to convert the Estonian orthographic text to an orthoepically correct and natural-sounding spoken text for a wide range of practical application (especially for visual and speech disabled persons).\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-465"
  },
  "montero99_eurospeech": {
   "authors": [
    [
     "J. M.",
     "Montero"
    ],
    [
     "J.",
     "Gutiérrez-Arriola"
    ],
    [
     "J.",
     "Colás"
    ],
    [
     "J.",
     "Macías-Guarasa"
    ],
    [
     "E.",
     "Enríquez"
    ],
    [
     "Juan Manuel",
     "Pardo"
    ]
   ],
   "title": "Development of an emotional speech synthesiser in Spanish",
   "original": "e99_2099",
   "page_count": 4,
   "order": 469,
   "p1": "2099",
   "pn": "2102",
   "abstract": [
    "Currently, an essential point in speech synthesis is the addressing of the variability of human speech. One of the main sources of this diversity is the emotional state of the speaker. Most of the recent work in this area has been focused on the prosodic aspects of speech and on rule-based formant-synthesis experiments. Even when adopting an improved voice source, we cannot achieve a smiling happy voice or the menacing quality of cold anger. For this reason, we have performed two experiments aimed at developing a concatenative emotional synthesiser, a synthesiser that can copy the quality of an emotional voice without an explicit mathematical model.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-466"
  },
  "pavesic99_eurospeech": {
   "authors": [
    [
     "N.",
     "Pavesic"
    ],
    [
     "Jerneja",
     "Gros"
    ]
   ],
   "title": "S5: the SQEL slovene speech synthesis system",
   "original": "e99_2103",
   "page_count": 4,
   "order": 470,
   "p1": "2103",
   "pn": "2106",
   "abstract": [
    "An improved version of the Slovene text-to-speech system S5 is described. S5 can be used either as a stand-alone reading system or it can be integrated into other applications. S5 is based on concatenation of basic speech units, diphones, using the TD-PSOLA technique. The input text is transformed into its spoken equivalent by a series of modules. F0 modeling is based primarily on predicting the appropriate tonemic accent. Phone duration is predicted by a two level approach, taking into account how acceleration or slowing down applies to the duration of individual phones. The adequacy of the spoken output was evaluated by several subjective tests as they are recommended by the International Telecommunication Union (ITU).\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-467"
  },
  "rojc99_eurospeech": {
   "authors": [
    [
     "Matej",
     "Rojc"
    ],
    [
     "Janez",
     "Stergar"
    ],
    [
     "Ralph",
     "Wilhelm"
    ],
    [
     "Horst-Udo",
     "Hain"
    ],
    [
     "Martin",
     "Holzapfel"
    ],
    [
     "Bogomir",
     "Horvat"
    ]
   ],
   "title": "A multilingual text processing engine for the PAPAGENO text-to-speech synthesis system",
   "original": "e99_2107",
   "page_count": 4,
   "order": 471,
   "p1": "2107",
   "pn": "2110",
   "abstract": [
    "Automatic synthesis of speech from arbitrary text requires two basic operations: linguistic analysis of input text and speech waveform generation. The achieved quality of the second stage very much de-pends on the reliability and richness of information generated in the first stage. In this paper we discuss possibilities and problems of text analysis for multilingual speech synthesis. The language independent approach requires the separation of all the language specific information into the language specific inventory, which is com-posed of different lexica, various dictionaries and lists. The remaining core represents the language independent text-processing engine.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-468"
  },
  "suh99_eurospeech": {
   "authors": [
    [
     "Chang K.",
     "Suh"
    ],
    [
     "Takehiko",
     "Kagoshima"
    ],
    [
     "Masahiro",
     "Morita"
    ],
    [
     "Shigenobu",
     "Seto"
    ],
    [
     "Masami",
     "Akamine"
    ]
   ],
   "title": "Toshiba English text-to-speech synthesizer (TESS)",
   "original": "e99_2111",
   "page_count": 4,
   "order": 472,
   "p1": "2111",
   "pn": "2114",
   "abstract": [
    "Toshiba English Text-to-Speech Synthesizer utilizes several new techniques to produce synthesized speech that is more natural-sounding and intelligible than that created by conventional synthesizers. The closed-loop training method creates synthesis units that most closely resemble the training data and are the least susceptible to prosodic distortion noise by analytically solving an equation that minimizes distortion between target units and training data. The pitch contour model creates a codebook of representative word-based F0 contours by first clustering the training data using word stress and syllable numbers. Within each cluster, the training data is divided into different groups using lexical and phonological attributes of each word. In each group, a representative contour is created using approximate error estimation. The resulting approximate errors are used in offset level prediction for each contour. These techniques have significantly improved the prosodic quality, naturalness and intelligibility of the resulting synthesized speech.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-469"
  },
  "sannier99_eurospeech": {
   "authors": [
    [
     "Frédérique",
     "Sannier"
    ],
    [
     "Véronique",
     "Aubergé"
    ]
   ],
   "title": "Towards the generation of French phonetic inflected forms",
   "original": "e99_2115",
   "page_count": 4,
   "order": 473,
   "p1": "2115",
   "pn": "2118",
   "abstract": [
    "After having given the reasons why it is legitimate to dedicate a special attention to the oral code with regard to the written code, we propose an approach to the automatic generation of French phonetic inflected forms. We shall examine it first inside the general context of automatic phoneticization, with a special attention to the phoneticization of loanwords. We shall then give the methodology we adopted to build up our generation system.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-470"
  },
  "tzoukermann99_eurospeech": {
   "authors": [
    [
     "Evelyne",
     "Tzoukermann"
    ],
    [
     "Lucie",
     "Ménard"
    ],
    [
     "Marise",
     "Ouellet"
    ]
   ],
   "title": "Canadian French text-to-speech synthesis: modeling an optimal set of realizations for dialect markers",
   "original": "e99_2119",
   "page_count": 4,
   "order": 474,
   "p1": "2119",
   "pn": "2122",
   "abstract": [
    "We report on the development of a text-to-speech system for Canadian French which builds on both our theoretical and practical experience from the European French system [8, 10]. In order to extend to an additional dialect, we took an approach which would permit us to rapidly build the new TTS system. Specifically, we (1) identified speech markers for the new dialect, (2) determined optimal implementation points for these markers, at the phone inventory, diphone recording, text analysis, and duration levels. Potential units were filtered based on generalizable spectral properties and on frequency of occurrence.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-471"
  },
  "busser99_eurospeech": {
   "authors": [
    [
     "Bertjan",
     "Busser"
    ],
    [
     "Walter",
     "Daelemans"
    ],
    [
     "Antal van den",
     "Bosch"
    ]
   ],
   "title": "Machine learning of word pronunciation: the case against abstraction",
   "original": "e99_2123",
   "page_count": 4,
   "order": 475,
   "p1": "2123",
   "pn": "2126",
   "abstract": [
    "Word pronunciation can be learned by inductive machine learning algorithms when it is represented as a classification task: classify a letter within its local word context as mapping to its pronunciation. On the basis of generalization accuracy results from empirical studies, we argue that word pronunciation, particularly in complex spelling systems such as that of English, should not be modelled in a way that abstracts from exceptions. Learning methods such as decision tree and backpropagation learning, while trying to abstract from noise, also throw away alarge number of useful exceptional cases. Our empirical results suggest that a memory-based approach which stores all available word-pronunciation knowledge as cases in memory, and generalises from this lexicon via analogical reasoning, is at all times the optimal modelling method.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-472"
  },
  "bowerman99_eurospeech": {
   "authors": [
    [
     "Chris",
     "Bowerman"
    ],
    [
     "Anders",
     "Eriksson"
    ],
    [
     "Mark",
     "Huckvale"
    ],
    [
     "Mike",
     "Rosner"
    ],
    [
     "Mark",
     "Tatham"
    ],
    [
     "Maria",
     "Wolters"
    ]
   ],
   "title": "Criteria for evaluating internet tutorials in speech communication sciences",
   "original": "e99_2455",
   "page_count": 4,
   "order": 476,
   "p1": "2455",
   "pn": "2458",
   "abstract": [
    "The Computer Aided Learning (CAL) working group of the SOCRATES thematic network in Speech Communication Science have studied how the Internet is being used and could be used for the provision of self-study materials for education. In this paper we follow up previous recommendations for the design of Internet tutorials with recommendations for their evaluation. The paper proposes that evaluation should be seen as a necessary quality assurance mechanism operating within the life-cycle of CAL materials development. We propose a structured set of criteria for evaluation, based on the features of good tutorials, against which a tutorial might be judged. Since evaluation against fixed criteria is only one possible approach, we outline how evaluation could also be performed by student users and in controlled trials.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-473"
  },
  "drygajlo99_eurospeech": {
   "authors": [
    [
     "Andrzej",
     "Drygajlo"
    ],
    [
     "Guy",
     "Delafontaine"
    ]
   ],
   "title": "Javaspeechlab - interactive speech analysis laboratory on the world-wide web",
   "original": "e99_2459",
   "page_count": 4,
   "order": 477,
   "p1": "2459",
   "pn": "2462",
   "abstract": [
    "The aim of the paper is to introduce a versatile and user-friendly computer assisted learning (CAL) system in order to support traditional teaching in the Speech Science domain. This system is based on Java as a powerful programming language for developing platform-independent, interactive and computational-based software that can be used on the World-Wide Web through a Java-enabled Web browser. The interactive and computational capabilities of Java are demonstrated through modular speech analysis laboratory (JavaSpeechLab) based on Java application and applet. It is designed to transform the WWWPage into an easy-to-use speech analysis workstation for distance learning applications.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-474"
  },
  "digalakis99_eurospeech": {
   "authors": [
    [
     "Vassilis",
     "Digalakis"
    ],
    [
     "Stavros",
     "Tsakalidis"
    ],
    [
     "Leonardo",
     "Neumeyer"
    ]
   ],
   "title": "Reviving discrete HMMs: the myth about the superiority of continuous HMMs",
   "original": "e99_2463",
   "page_count": 4,
   "order": 478,
   "p1": "2463",
   "pn": "2466",
   "abstract": [
    "Despite what is generally believed, we have recently shown that discrete-distribution HMMs can outperform continuous-density HMMs at significantly faster decoding speeds. Recognition performance and decoding speed of the discrete HMMs are improved by using product-code Vector Quantization (VQ) and mixtures of discrete distributions. In this paper, we present efficient training and decoding algorithms for the discrete-mixture HMMs (DMHMMs). Our experimental results show that the high-level of recognition accuracy of continuous mixture-density HMMs (CDHMMs) can be maintained at significantly faster decoding speeds.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-475"
  },
  "fujisaki99_eurospeech": {
   "authors": [
    [
     "Hiroya",
     "Fujisaki"
    ],
    [
     "Hiroyuki",
     "Kameda"
    ],
    [
     "Sumio",
     "Ohno"
    ],
    [
     "Kenji",
     "Abe"
    ],
    [
     "Michio",
     "Iijima"
    ],
    [
     "Masayoshi",
     "Suzuki"
    ],
    [
     "Kazunari",
     "Taketa"
    ]
   ],
   "title": "Principles and design of an intelligent system for information retrieval over the internet with a multimodal dialogue interface",
   "original": "e99_2467",
   "page_count": 4,
   "order": 479,
   "p1": "2467",
   "pn": "2470",
   "abstract": [
    "In the information society of the next millenium, information retrieval over the Internet will be indispensable for everyday life, and spoken language will be an essential medium for human-machine dialogue. This paper presents an overview of an intelligent system for information retrieval based on spoken dialogue, use of key concepts, processing of unknown words, knowledge acquisition, and agent technology. Three agents are introduced to be respectively responsible for user interface, information retrieval, and information acquisition. Dialogue management through user and system modeling, implementation of information retrieval through key concepts, and inference on concepts of unknown words based on syntactic and semantic analyses of their structures, are then brie y described as the innovative features of the system.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-476"
  },
  "nishimoto99_eurospeech": {
   "authors": [
    [
     "Takuya",
     "Nishimoto"
    ],
    [
     "Hidehiro",
     "Yuki"
    ],
    [
     "Takehiko",
     "Kawahara"
    ],
    [
     "Yasuhisa",
     "Niimi"
    ]
   ],
   "title": "An asynchronous virtual meeting system for bi-directional speech dialog",
   "original": "e99_2471",
   "page_count": 4,
   "order": 480,
   "p1": "2471",
   "pn": "2474",
   "abstract": [
    "Voice-mail and video-mail systems for the Internet are becoming very popular, because the speech and video compression technology is greatly improved in these days. Asynchronous voice-mail system, however, does not seem attractive because it's not designed for discussions. We propose a network-based voice conference system which enables asynchronous and bi-directional discussions. The system displays the voice messages as the threaded written words. The user can manipulate voice messages just as if they are text messages. If the participant wants to quote or annotate to a message, the user has only to play the sound and barge into the message while it is playing. Such user interface increases the usefulness of the voice-mail system. We performed a preliminary experiment to evaluate the proposed conversation method. The voice messages are transcribed manually, because our system is not integrated with the speech recognition at present. Using the system, asynchronous voice conversations were realized eficiently. AVM is an application platform for the many kinds of speech processing, such as spontaneous speech recognition and automatic dialog tagging. The applications of the system include the entertainment systems, such as interactive radio dramas, or asynchronous multi-user game using speech dialog.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-477"
  },
  "bellegarda99_eurospeech": {
   "authors": [
    [
     "Jerome R.",
     "Bellegarda"
    ]
   ],
   "title": "Context scope selection in multi-Span statistical language modeling",
   "original": "e99_2163",
   "page_count": 4,
   "order": 481,
   "p1": "2163",
   "pn": "2166",
   "abstract": [
    "A multi-span framework was recently proposed to integrate the various constraints, both local and global, that are present in the language. In this approach, local constraints are captured via n-gram language modeling, while global constraints are taken into account through the use of latent semantic analysis. The complementarity between these two paradigms translates into improved modeling performance, as measured by both perplexity and word error rate reduction. This performance improvement is sensitive to the context scope, i.e., the e ective length of the document history used in latent semantic analysis during recognition. Context scope selection via exponential forgetting is proposed to discount older utterances as necessary. Experiments on a subset of the Wall Street Journal task led to a reduction in average word error rate of up to 22.5%.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-478"
  },
  "gildea99_eurospeech": {
   "authors": [
    [
     "Daniel",
     "Gildea"
    ],
    [
     "Thomas",
     "Hofmann"
    ]
   ],
   "title": "Topic-based language models using EM",
   "original": "e99_2167",
   "page_count": 4,
   "order": 482,
   "p1": "2167",
   "pn": "2170",
   "abstract": [
    "In this paper, we propose a novel statistical language model to capture topic-related long-range dependencies. Topics are modeled in a latent variable framework in which we also derive an EM algorithm to perform a topic factor decomposition based on a segmented training corpus. The topic model is combined with a standard language model to be used for on-line word prediction. Perplexity results indicate an improvement over previously proposed topic models, which unfortunately has not translated into lower word error.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-479"
  },
  "galescu99_eurospeech": {
   "authors": [
    [
     "Lucian",
     "Galescu"
    ],
    [
     "Eric K.",
     "Ringger"
    ]
   ],
   "title": "Augmenting words with linguistic information for n-gram language models",
   "original": "e99_2171",
   "page_count": 4,
   "order": 483,
   "p1": "2171",
   "pn": "2174",
   "abstract": [
    "The main goal of the present work is to explore the use of rich lexical information in language modelling. We reformulated the task of a language model from predicting the next word given its history to predicting simultaneously both the word and a tag encoding various types of lexical information. Using part-of-speech tags and syntactic/semantic feature tags obtained with a set of NLP tools developed at Microsoft Research, we obtained a reduction in perplexity compared to the baseline phrase trigram model in a set of preliminary tests performed on part of the WSJ corpus.\n",
    "Keywords: speech recognition, statistical language modelling, n-gram models, phrase models, augmented-word models, POS tags, semantic/syntactic tags, NLPWin, WSJ corpus.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-480"
  },
  "nasr99_eurospeech": {
   "authors": [
    [
     "Alexis",
     "Nasr"
    ],
    [
     "Yannick",
     "Estéve"
    ],
    [
     "Frédéric",
     "Béchet"
    ],
    [
     "Thierry",
     "Spriet"
    ],
    [
     "Renato de",
     "Mori"
    ]
   ],
   "title": "A language model combining n-grams and stochastic finite state automata",
   "original": "e99_2175",
   "page_count": 4,
   "order": 484,
   "p1": "2175",
   "pn": "2178",
   "abstract": [
    "This paper describes a new kind of language models composed of several local models and a general model linking the local models together. Local models describe more finely subparts of the textual data than a conventional n-gram trained on the complete corpus. They are built on lexical and syntactic criteria. Both local and global models are integrated in a single hidden Markov model. Experiments showed a 14% decrease in perplexity compared to a bigram model on a small corpus of telephonic communications.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-481"
  },
  "wu99c_eurospeech": {
   "authors": [
    [
     "Jun",
     "Wu"
    ],
    [
     "Sanjeev",
     "Khudanpur"
    ]
   ],
   "title": "Combining nonlocal, syntactic and n-gram dependencies in language modeling",
   "original": "e99_2179",
   "page_count": 4,
   "order": 485,
   "p1": "2179",
   "pn": "2182",
   "abstract": [
    "A new language model is presented which incorporates local N-gram dependencies with two important sources of long-range dependencies: the syntactic structure and the topic of a sentence. These dependencies or constraints are integrated using the maximum entropy method. Substantial improvements are demonstrated over a trigram model in both perplexity and speech recognition accuracy on the Switchboard task. It is shown that topic dependencies are most useful in predicting words which are semantically related by the subject matter of the conversation. Syntactic dependencies on the other hand are found to be most helpful in positions where the best predictors of the following word are not within N-gram range due to an intervening phrase or clause. It is also shown that these two methods individually enhance an N-gram model in complementary ways and the overall improvement from their combination is nearly additive.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-482"
  },
  "kiss99_eurospeech": {
   "authors": [
    [
     "Imre",
     "Kiss"
    ],
    [
     "Pekka",
     "Kapanen"
    ]
   ],
   "title": "Robust feature vector compression algorithm for distributed speech recognition",
   "original": "e99_2183",
   "page_count": 5,
   "order": 486,
   "p1": "2183",
   "pn": "2186",
   "abstract": [
    "In this paper we propose an algorithm for efficient compression of feature extracted parameters used in speech recognition. The algorithm provides a compression ratio of roughly 1:10 and causes negligible or no loss in recognition performance. It is also shown to be robust against enviromental noise. Combined with an appropriate framing structure, a complete system is obtained, which can be used for implementing speech recognition applications e.g. in a cellular mobile environment. The system achieves a gross bitrate as low as 4200 bps.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-483"
  },
  "karjalainen99_eurospeech": {
   "authors": [
    [
     "Matti",
     "Karjalainen"
    ],
    [
     "Tero",
     "Tolonen"
    ]
   ],
   "title": "Separation of speech signals using iterative multi-pitch analysis and prediction",
   "original": "e99_2187",
   "page_count": 4,
   "order": 487,
   "p1": "2187",
   "pn": "2190",
   "abstract": [
    "A model for multi-pitch analysis is extended into an iterative multi-pitch analysis and prediction (IMPAP) scheme. The method is efficient in finding harmonic complex tones, such as voiced speech signals, in a mixture of such signals and possible noise background. It can also be used to separate the signal into perceptually relevant speech components. The method may be used in applications ranging from speech transmission (enhancement) to recognition (noise and extra sound rejection).\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-484"
  },
  "parris99_eurospeech": {
   "authors": [
    [
     "Eluned S.",
     "Parris"
    ],
    [
     "Michael J.",
     "Carey"
    ],
    [
     "Harvey",
     "Lloyd-Thomas"
    ]
   ],
   "title": "Feature fusion for music detection",
   "original": "e99_2191",
   "page_count": 4,
   "order": 488,
   "p1": "2191",
   "pn": "2194",
   "abstract": [
    "Automatic discrimination between music, speech and noise has grown in importance as a research topic over recent years. The need to classify audio into categories such as music or speech is an important part of the multimedia document retrieval problem. This paper extends work previously carried out by the authors which compared performance of static and transitional features based on cepstra, amplitude, zero-crossings and pitch for music and speech discrimination. Two approaches are described to combine the features to improve overall performance. The first approach uses separate GMM classifiers for each feature type and fuses the outputs of the classifiers. The second approach combines different features into a single vector prior to modelling the data with a GMM. Significant improvements in performance have been observed using both approaches over the results achieved by a single type of feature. An equal error rate of 0.3% is achieved for the best system on ten second tests using seventeen hours of test material. The performance is maintained as the length of test file is reduced with an equal error rate of less than 1% being achieved with only two seconds of data.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-485"
  },
  "vuuren99_eurospeech": {
   "authors": [
    [
     "Sarel van",
     "Vuuren"
    ],
    [
     "Hynek",
     "Hermansky"
    ]
   ],
   "title": "Speech variability in the modulation spectral domain - SANOVA technique -",
   "original": "e99_2195",
   "page_count": 4,
   "order": 489,
   "p1": "2195",
   "pn": "2198",
   "abstract": [
    "This paper examines sources of variability in the speech signal using a new technique that is based on a nested spectral analysis of variance (SANOVA). By constructing an ANOVA in the modulation spectral domain, the technique allows a characterization of unwanted variability in the time sequences of logarithmic energy caused by extraneuous sources of variability such as additive noise, convolutional noise, and telephone handset transducer. Very low and moderate to high modulation frequencies are shown to be particularly affected by these sources. Verification results for 500 speakers on Switchboard data from the 1998 NIST speaker recognition evaluation confirms the conclusions. It is shown that a bandpass filtering and down sampling of the time sequences of logarithmic energy, compared to a conventional highpass filtering, leads to a 13% relative reduction of the EER in mismatched conditions.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-486"
  },
  "yang99b_eurospeech": {
   "authors": [
    [
     "Dekun",
     "Yang"
    ],
    [
     "Georg F.",
     "Meyer"
    ],
    [
     "William A.",
     "Ainsworth"
    ]
   ],
   "title": "Improving harmonic selection for speech intelligibility enhancement by the reassignment method",
   "original": "e99_2199",
   "page_count": 4,
   "order": 490,
   "p1": "2199",
   "pn": "2202",
   "abstract": [
    "Harmonic selection is a useful mechanism for speech enhancement. However, there is difficulty in detecting harmonic structure in the spectrum in the presence of noise or interfering speech. In this paper we address the problem of applying the reassignment method to produce a higher-resolution spectrum for improving the harmonic selection. The reassignment method is to assign the value of the spectrogram computed by the short-time Fourier transformto the gravity center of the region rather than the geometric center of the region. We analyze the resolution capability of the reassigned Fourier spectrum and show that it has better frequency separation than Fourier spectrum. We incorporate the reassignment method with the amplitude-modulation based harmonic selection to segregate a target vowel from interfering vowels. Experimental results showthat the target vowel recognition performance is improved by using the reassignment method for increasing the readability of the harmonic structure in speech signals.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-487"
  },
  "beigi99_eurospeech": {
   "authors": [
    [
     "Homayoon S. M.",
     "Beigi"
    ],
    [
     "Stéphane H.",
     "Maes"
    ],
    [
     "Upendra V.",
     "Chaudhari"
    ],
    [
     "Jeffrey S.",
     "Sorensen"
    ]
   ],
   "title": "A hierarchical approach to large-scale speaker recognition",
   "original": "e99_2203",
   "page_count": 4,
   "order": 491,
   "p1": "2203",
   "pn": "2206",
   "abstract": [
    "This paper presents a hierarchical approach to the Large-Scale Speaker Recognition problem. In here the authors present a binary tree data-base approach for arranging the trained speaker models based on a distance measure designed for comparing two sets of distributions. The combination of this hierarchical structure and the distance measure [1] provide the means for conducting a large-scale verification task. In addition, two techniques are presented for creating a model of the complement-space to the cohort which is used for rejection purposes. Results are presented for the drastic improvements achieved mainly in reducing the false-acceptance of the speaker verification system without any significant false-rejection degradation.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-488"
  },
  "cernocky99_eurospeech": {
   "authors": [
    [
     "J.",
     "Cernocky"
    ],
    [
     "D.",
     "Petrovska-Delacrélaz"
    ],
    [
     "S.",
     "Pigeon"
    ],
    [
     "P.",
     "Verlinde"
    ],
    [
     "Gérard",
     "Chollet"
    ]
   ],
   "title": "A segmental approach to text-independent speaker verification",
   "original": "e99_2207",
   "page_count": 4,
   "order": 492,
   "p1": "2207",
   "pn": "2210",
   "abstract": [
    "Current text-independent speaker verification systems are usually based on modeling globally the probability density function (PDF) of the speaker feature vectors. In this paper, segmental approaches to text-independent speaker verification are discussed. Unlike the schemes based on Large Vocabulary Continuous Speech Recognition (LVCSR) with previously trained phone models, our systems are based on units derived in unsupervised manner using the ALISP (Automatic Language Independent Processing) tools. Speaker modeling is then done independently for each class of speech sounds. Among the techniques to merge the classdependent scores, linear combination was tested and logistic regression and a method based on the Mixture of Experts technique are under investigation. The experimental results were obtained on the data from the NIST-NSA'98 campaign.\n",
    "Keywords: text-independent speaker verification, segmental approach, data fusion.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-489"
  },
  "johnson99_eurospeech": {
   "authors": [
    [
     "S. E.",
     "Johnson"
    ]
   ],
   "title": "Who spoke when? - automatic segmentation and clustering for determining speaker turns",
   "original": "e99_2211",
   "page_count": 4,
   "order": 493,
   "p1": "2211",
   "pn": "2214",
   "abstract": [
    "The problem of labelling speaker turns by automaticallysegmenting and clustering a continuous audio streamis addressed. A new clustering scheme is presentedand evaluated using a clustering efficiency score whichtreats both agglomerative and divisive clustering strategies equally. Results show an efficiency of 70% can beobtained on both manually and automatically derivedsegments on the 1996 Hub4 development data.For the task of identifying potentially unknown anchorspeakers within broadcast news shows, the frame classification error rate is very important. To re ect this, aframe-based cluster efficiency is defined and the resultsshow a 90% frame-based efficiency can be achieved. Finally a frame-based comparison between the manuallyand automatically derived segment/cluster sets showsthat approximately one third of the errors are introducedduring segmentation and two-thirds during clustering.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-490"
  },
  "przybocki99_eurospeech": {
   "authors": [
    [
     "Mark A.",
     "Przybocki"
    ],
    [
     "Alvin F.",
     "Martin"
    ]
   ],
   "title": "The 1999 NIST speaker recognition evaluation, using summed two-channel telephone data for speaker detection and speaker tracking",
   "original": "e99_2215",
   "page_count": 4,
   "order": 494,
   "p1": "2215",
   "pn": "2218",
   "abstract": [
    "The 1999 NIST Speaker Recognition Evaluation encompassed three tasks: one-speaker detection, two-speaker detection, and speaker tracking. All tasks were performed in the context of conversational telephone speech. The one-speaker task used single channel mu-law data; the other tasks used summed two-channel data. Twelve sites from the United States, Europe, and India participated in the evaluation. Performance was measured by a decision cost function and compared among systems and test conditions via DET Curves. Performance factors examined include segment duration, degradation resulting from the presence of a second speaker, sex mix of the two-speaker segments, matched or mismatched between training and test handsets, and the variation in handset type.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-491"
  },
  "sonmez99_eurospeech": {
   "authors": [
    [
     "Kemal",
     "Sönmez"
    ],
    [
     "Larry",
     "Heck"
    ],
    [
     "Mitchel",
     "Weintraub"
    ]
   ],
   "title": "Speaker tracking and detection with multiple speakers",
   "original": "e99_2219",
   "page_count": 4,
   "order": 495,
   "p1": "2219",
   "pn": "2222",
   "abstract": [
    "We describe a speaker tracking and detection system, for Switchboard conversations, that uses a two­speaker and silence hidden Markov model (HMM)with a minimumstate duration constraint and Gaussian mixture model (GMM) state distributionsadapted from a single gender- and hand­set­independent imposter model distribution. Speaker tracking is used to segment speakers for detection, which is carried out by averaging frame scores of the Viterbi path and HNORMing via a novel parameter interpolation extension of HNORM for use with files of arbitrary lengths. Use of duration statistics augmenting the acoustic scores is also introduced via a nonlinear combination function. Results are reported on the NIST 1998 Multispeaker development evaluation dataset.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-492"
  },
  "choukri99_eurospeech": {
   "authors": [
    [
     "Khalid",
     "Choukri"
    ],
    [
     "ValéRie",
     "Mapelli"
    ],
    [
     "Jeff",
     "Allen"
    ]
   ],
   "title": "New developments within the european language resources association (ELRA)",
   "original": "e99_2691",
   "page_count": 4,
   "order": 496,
   "p1": "2691",
   "pn": "2694",
   "abstract": [
    "The ELRA catalogue as of April 1999 lists 95 speech resources, 47 monolingual lexica, 105 multilingual lexica, 19 written corpora and more than 275 terminological databases. In the speech area, many databases are available for machine dictation, telephone-based applications, speaker identification and verification applications, as well as several phonetic lexica, for languages such as English, German, Swiss-French, Danish, French, Italian, Spanish, Portuguese, Russian, Mandarin Chinese, etc. However, many Language Resources (LRs) need to be identified and/or produced. To this effect, ELRA is active in promoting and funding the co-production of new LRs through several calls for proposals. As for the validity of the existence of ELRA for the distribution of language resources, the statistics from the past two years speak for themselves. The 1998 fiscal report showed a significant rise with the sale of 179 LRs (86 for research and 93 for commercial purposes; with speech databases representing nearly 70%), compared to 31 sold in 1997.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-493"
  },
  "eskenazi99b_eurospeech": {
   "authors": [
    [
     "Maxine",
     "Eskenazi"
    ],
    [
     "Alexander I.",
     "Rudnicky"
    ],
    [
     "Karin",
     "Gregory"
    ],
    [
     "Paul",
     "Constantinides"
    ],
    [
     "Robert",
     "Brennan"
    ],
    [
     "Christina",
     "Bennett"
    ],
    [
     "Jwan",
     "Allen"
    ]
   ],
   "title": "Data collection and processing in the carnegie mellon communicator",
   "original": "e99_2695",
   "page_count": 4,
   "order": 497,
   "p1": "2695",
   "pn": "2698",
   "abstract": [
    "In order to create a useful, gracefully functioning system for travel arrangements, we have first observed the task as it is accomplished by a human. We then imitated the human while making the user believe he was dialoguing with an automatic system. As we gradually built our system, we devised ways to assess progress and to detect errors. The following described the manner in which the Carnegie Mellon Communicator was built, data collected, and assessment begun using these criteria.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-494"
  },
  "hoge99_eurospeech": {
   "authors": [
    [
     "Harald",
     "Höge"
    ],
    [
     "Christoph",
     "Draxler"
    ],
    [
     "Henk van den",
     "Heuvel"
    ],
    [
     "Finn Tore",
     "Johansen"
    ],
    [
     "Eric",
     "Sanders"
    ],
    [
     "Herbert S.",
     "Tropf"
    ]
   ],
   "title": "Speechdat multilingual speech databases for teleservices: across the finish line",
   "original": "e99_2699",
   "page_count": 4,
   "order": 498,
   "p1": "2699",
   "pn": "2702",
   "abstract": [
    "The goal of the SpeechDat project is to develop spoken language resources for speech recognisers suited to realise voice driven teleservices. SpeechDat created speech databases for all official languages of the European Union and some major dialectal varieties and minority languages. The size of the databases ranges between 500 and 5000 speakers. In total 20 databases are recorded over the fixed telephone network, 5 databases over the cellular network, and 3 databases are designed for speaker verification. To date the project has successfully reached its end. This paper briefly describes the project, addresses the validation of the databases, their availability to consortium members and third parties, publicity and awareness, and the spin-off of the project in speech recognition research.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-495"
  },
  "mengel99_eurospeech": {
   "authors": [
    [
     "Andreas",
     "Mengel"
    ],
    [
     "Ulrich",
     "Heid"
    ]
   ],
   "title": "Enhancing reusability of speech corpora by hyperlinked query output",
   "original": "e99_2703",
   "page_count": 4,
   "order": 499,
   "p1": "2703",
   "pn": "2706",
   "abstract": [
    "In speech technology more and more databases of spoken language are becoming available. For research the availability of these data offers the possibility to study huge corpora. Apart from the fact that these corpora may be represented in different formats, it is sometimes difficult to relate annotations of one corpus to those of another corpus. This contribution argues for a representation of information in speech corpora that allows for the integrated representation of information on various levels of description in XML. Secondly, the study of huge amounts of speech data requires adequate retrieval mechanisms. A query architecture is described that allows for the retrieval of encoded entities by specifying their properties or various relations to other entities. The output of the query processor is represented in XML and thus can be used for further queries or a new level of description. The work presented here is part of the results of the MATE project (http://mate.mip.ou.dk).\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-496"
  },
  "silverman99_eurospeech": {
   "authors": [
    [
     "Kim",
     "Silverman"
    ],
    [
     "Victoria",
     "Anderson"
    ],
    [
     "Jerome",
     "Bellegarda"
    ],
    [
     "Kevin",
     "Lenzo"
    ],
    [
     "Devang",
     "Naik"
    ]
   ],
   "title": "Design and ccollection of a corpus of polyphones and prosodic contexts for speech synthesis research and development",
   "original": "e99_2707",
   "page_count": 4,
   "order": 500,
   "p1": "2707",
   "pn": "2708",
   "abstract": [
    "The design principles and collection procedures behind a speech synthesis corpus directly impact the performance of the resulting text-to-speech system. This paper describes the design and collection of the Victoria corpus, created to support speech synthesis research and development at Apple Computer. This corpus is composed of five constituent parts, each designed to cover a specific aspect of speech synthesis: polyphones, prosodic contexts, reiterant speech, function word sequences, and continuous speech. It was spoken in general U.S. English by one linguistically-trained adult female. Portions of the corpus are being used in the statistical estimation of duration and pitch models for Apple's next-generation text-to-speech system, MacinTalk 4.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-497"
  },
  "au99_eurospeech": {
   "authors": [
    [
     "Oscar C.",
     "Au"
    ],
    [
     "Wanggen",
     "Wan"
    ],
    [
     "Cyan L.",
     "Keung"
    ],
    [
     "Chi H.",
     "Yim"
    ]
   ],
   "title": "Sinusoidal representation and auditory model-based parametric matching and smoothing and its application in speech analysis/synthesis",
   "original": "e99_2287",
   "page_count": 4,
   "order": 501,
   "p1": "2287",
   "pn": "2290",
   "abstract": [
    "This paper presents a parametric matching and smoothing method that is applied to a sinusoidal representation and auditory model-based speech analysis/synthesis system. A 2.6kbps speech-coding algorithm is finally derived based on the speech analysis/synthesis system. The synthetic speech is almost same as that of 3.25kbps speech coding algorithm with overlapping and adding method. A linear interpolation method is utilized to smooth the amplitude parameters, and a nonlinear polynomial interpolation method is used to smooth the frequency and phase parameters. The experimental results demonstrate that the parametric matching and smoothing method can reduce the bit-rate with the speech quality unchanged when it is applied to the sinusoidal representation and auditory model-based speech-coding algorithm.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-498"
  },
  "balestri99_eurospeech": {
   "authors": [
    [
     "Marcello",
     "Balestri"
    ],
    [
     "Alberto",
     "Pacchiotti"
    ],
    [
     "Silvia",
     "Quazza"
    ],
    [
     "Pier Luigi",
     "Salza"
    ],
    [
     "Stefano",
     "Sandri"
    ]
   ],
   "title": "Choose the best to modify the least: a new generation concatenative synthesis system",
   "original": "e99_2291",
   "page_count": 4,
   "order": 502,
   "p1": "2291",
   "pn": "2294",
   "abstract": [
    "The paper describes a corpus-based approach applied in the evolution of ELOQUENS ® , the CSELT text-to-speech synthesis system for Italian, towards multi-voice, multi-language, high-naturalness concatenative synthesis. The acoustic modules have been redesigned, according to the idea of reducing the number of junctions and the need of prosodic modification. Appropriate phonetic coverage methods were applied in the acoustic database design. Automatic processing tools performed phone and diphone segmentation, pitch marking, prosodic feature detection. The synthesis algorithm exploits the speech material at its best, searching for the longest suitable sequences in the database, according to weighted distance measures on phonetic/prosodic parameters. Signal modification techniques are applied only if necessary, to smooth residual prosodic jumps at unit boundaries. The resulting voice is quite human-sounding.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-499"
  },
  "chou99b_eurospeech": {
   "authors": [
    [
     "Fu-chiang",
     "Chou"
    ],
    [
     "Chiu-yu",
     "Tseng"
    ],
    [
     "Lin-shan",
     "Lee"
    ]
   ],
   "title": "Selection of waveform units for corpus-based Mandarin speech synthesis based on decision trees and prosodic modification costs",
   "original": "e99_2295",
   "page_count": 4,
   "order": 503,
   "p1": "2295",
   "pn": "2298",
   "abstract": [
    "A lazy decision tree approach is described in this paper for the selection of concatenative units for Mandarin speech synthesis. The concept is not to induce a concise hypothesis from a given training data; the selection is delayed until a test instance is given. Thus we can construct the \"best\" decision tree for each selection. The selection of waveform units is guided with other simultaneously selected prosodic parameters. The selected waveform units can be directly concatenated into output speech or modified with the selected prosodic parameters. This method can produce synthetic speech that sounds very natural and resembles the acoustic and prosodic characteristics of the original speaker. A Mandarin speech synthesizer is described in this paper. However, most of the technologies are language independent and can be extended to a multi-lingual system.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-500"
  },
  "etxebarria99_eurospeech": {
   "authors": [
    [
     "B.",
     "Etxebarria"
    ],
    [
     "I.",
     "Hernáez"
    ],
    [
     "I.",
     "Madariaga"
    ],
    [
     "E.",
     "Navas"
    ],
    [
     "J. C.",
     "Rodríguez"
    ],
    [
     "R.",
     "Gándara"
    ]
   ],
   "title": "Improving quality in a speech synthesizer based on the MBROLA algorithm",
   "original": "e99_2299",
   "page_count": 4,
   "order": 504,
   "p1": "2299",
   "pn": "2302",
   "abstract": [
    "Speech synthesis based on the Multiband Resynthesis OverLap-Add (MBROLA) algorithm produces high quality speech without requiring too much effort to design the diphone database, and using a low computational power. The main drawback of this algorithm is the slightly metallic sound or buzziness that can be perceived on voiced segments. We are working on a speech synthesizer based on the MBROLA algorithm, and trying to improve its speech quality by means of on an enhanced phase control strategy.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-501"
  },
  "huang99e_eurospeech": {
   "authors": [
    [
     "Yan",
     "Huang"
    ],
    [
     "Bo",
     "Xu"
    ]
   ],
   "title": "A novel model TD-PSPTP for speech synthesis",
   "original": "e99_2303",
   "page_count": 4,
   "order": 505,
   "p1": "2303",
   "pn": "2306",
   "abstract": [
    "In this paper, a novel approach based on time-domain pitch-synchronous point-to-point (TD-PSPTP) model for speech synthesis is presented. Compared to TD-PSOLA, which is currently one of the most popular concatenation methods, TD-PSPTP model provides a wider range of pitch and time modification. The quality of synthesized speech by TD-PSPTP shows to be high, especially its capability of overcoming reverberation, existing in TD-PSOLA when there is a drastic prosodic modification. The computational expense of TD-PSPTP model is no higher than that of TD-PSOLA. It provides an efficient way for the real time implementation of synthesis system.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-502"
  },
  "kapilow99_eurospeech": {
   "authors": [
    [
     "David",
     "Kapilow"
    ],
    [
     "Yannis",
     "Stylianou"
    ],
    [
     "Juergen",
     "Schroeter"
    ]
   ],
   "title": "Detection of non-stationarity in speech signals and its application to time-scaling",
   "original": "e99_2307",
   "page_count": 4,
   "order": 506,
   "p1": "2307",
   "pn": "2310",
   "abstract": [
    "This paper describes an automatic method for the detec-tion of non-stationarity in speech signals. It is based onthree measures of non-stationarity using Line Spectrum Frequencies (LSFs), the derivative of RMS values, and acombination of these two features. The application of the proposed method to time-scaling of speech signals is also presented. Results from an informal listening test sup-port its usefulness. Following these results, the method seems to be a powerful tool for the automatic control oftime-scale factors based on the characteristics of the in-put speech signal. Listeners preferred our new method over applying a constant time-scale factor in 90% of allcases. Other possible applications of the proposed toolare also discussed.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-503"
  },
  "koyama99_eurospeech": {
   "authors": [
    [
     "Takao",
     "Koyama"
    ],
    [
     "Jun-ichi",
     "Takahashi"
    ]
   ],
   "title": "A v-CV waveform based speech synthesis using global minimization of pitch conversion and concatenation distortion in v-CV unit sequence",
   "original": "e99_2311",
   "page_count": 4,
   "order": 507,
   "p1": "2311",
   "pn": "2314",
   "abstract": [
    "This paper proposes a new speech synthesis method for high-quality Japanese TTS (Text-to-speech) based on the waveform synthesis. The method uses V-CV as a basic synthesis unit to preserve the intelligibility of consonant. An efficient unit reconstruction method is newly adopted both to minimize pitch conversion and concatenation distortion when selecting waveforms. The minimization can provide fluency for synthesized speech. Furthermore, the proposed method enables to make a compact waveform dictionary keeping with high quality of synthesized speech. Using the waveform generation function of the method, the size of waveform dictionary can be drastically reduced by 1/40. Experimental evaluation using 32 ordinary peoples showed that high intelligibility of 97% was attained by the proposed V-CV speech synthesis method.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-504"
  },
  "mann99_eurospeech": {
   "authors": [
    [
     "Iain",
     "Mann"
    ],
    [
     "Steve",
     "McLaughlin"
    ]
   ],
   "title": "Stable speech synthesis using recurrent radial basis functions",
   "original": "e99_2315",
   "page_count": 4,
   "order": 508,
   "p1": "2315",
   "pn": "2318",
   "abstract": [
    "The problem of stable vowel sound synthesis using a non-linear free-running recurrent radial basis function (RBF) neural network is addressed. Voiced speech production is modelled as the output of a nonlinear dynamical system, rather than the conventional linear source-filter approach, which, given the nonlinear nature of speech, is expected to produce more natural-sounding synthetic speech. Our RBF network has the centre positions fixed on a hyperlattice, so only the linear-in-the-parameters weights need to be learnt for each vowel realisation. This leads to greatly reduced complexity without degrading performance. The proposed structure, in which regularisation theory is used in learning the weights, is demonstrated to be stable when functioning in recurrent mode with no external input, correctly producing the desired vowel sound.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-505"
  },
  "meron99_eurospeech": {
   "authors": [
    [
     "Yoram",
     "Meron"
    ],
    [
     "Keikichi",
     "Hirose"
    ]
   ],
   "title": "Efficient weight training for selection based synthesis",
   "original": "e99_2319",
   "page_count": 4,
   "order": 509,
   "p1": "2319",
   "pn": "2322",
   "abstract": [
    "Synthesis systems, based on unit selection from databases with a large number of unit examples from different phonetic and prosodic contexts, have been shown to allow a high quality speech synthesis [1]. One of the dificult problems associated with this kind of synthesis is determining the strategy for unit selection, and tuning its parameters. In this work we suggest a way to extend the usefulness of two existing training methods, by using phoneme pairs as the basic comparison unit. Using unit pairs is shown to significantly increase the efifciency of the exhaustive weight search training method on one hand, and refining the regression weight training method on the other.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-506"
  },
  "matousek99_eurospeech": {
   "authors": [
    [
     "Jindrich",
     "Matousek"
    ]
   ],
   "title": "Speech synthesis using HMM-based acoustic unit inventory",
   "original": "e99_2323",
   "page_count": 5,
   "order": 510,
   "p1": "2323",
   "pn": "2326",
   "abstract": [
    "The usage of multiple Hidden Markov Models (HMMs) to prepare a Czech acoustic unit inventory and speech synthesis based on this inventory are presented in this paper. Triphone HMMs are trained on the basis of the speech corpus spoken by a single speaker. The states of triphone HMMs are automatically clustered down using binary decision trees. The clustered states are then used to automatically segment the speech corpus and to create a speech segment database. The acoustic unit inventory constructed in this way is assumed to enable more precise context modeling than was previously possible. Concatenation-based speech synthesizer can be designed on the basis of the speech segment database. Several speech synthesis techniques are discussed for this purpose. In the end, a Czech text-to-speech (TTS) system is presented.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-507"
  },
  "macon99_eurospeech": {
   "authors": [
    [
     "Michael W.",
     "Macon"
    ],
    [
     "Mark A.",
     "Clements"
    ]
   ],
   "title": "An enhanced ABS/OLA sinusoidal model for waveform synthesis in TTS",
   "original": "e99_2327",
   "page_count": 4,
   "order": 511,
   "p1": "2327",
   "pn": "2330",
   "abstract": [
    "This paper describes a method for text-to-speech waveform synthesis based on the Analysis-by-Synthesis/Overlap-Add (ABS/OLA) sinusoidal model. This model has been shown in previous work to be a useful framework for pitch and time-scale modification of both speech and music signals. This paper explores some extensions of the original ABS/OLA formulation that attempt to overcome specific artifacts, including a phase dithering approach for unvoiced speech synthesis and an improved pitch modification method that compensates for undesirable energy modulation effects. The implementation of the model within a text-to-speech synthesis (TTS) system is described, and the results of a listener evaluation of the method are discussed.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-508"
  },
  "ouellet99_eurospeech": {
   "authors": [
    [
     "Marise",
     "Ouellet"
    ],
    [
     "Evelyne",
     "Tzoukermann"
    ],
    [
     "Lucie",
     "Ménard"
    ]
   ],
   "title": "High vowel /i y u/ in canadian and continental French: an analysis for a TTS system",
   "original": "e99_2331",
   "page_count": 4,
   "order": 512,
   "p1": "2331",
   "pn": "2334",
   "abstract": [
    "This paper deals with the treatment of high vowels /i y u/ in Canadian French Text-to-Speech system. Among significant differences in the structure of the vowel system, we focused on two aspects in which high vowels behave quite differently in the two dialects of French: 1) the tenseness rule according to which the three vowels are systematically contrasted by the tense/lax feature in Canadian French; 2) the reduction-deletion rule. Reduction and deletion are frequently observed at the phonetic level in Canadian dialect. Moreover, both variety of French exhibit some effects of devoicing and vowel shortening besides rule 2. In order to determine the effect of geographic origin and linguistic style on the phonetic features of /i y u/ we performed analysis on corpora recorded by two professional speakers in the two dialects of French. The outcome of the comparison allowed us to create a set of new rules for the treatment of high vowels in the Canadian version of the Bell Labs TTS system.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-509"
  },
  "tychtl99_eurospeech": {
   "authors": [
    [
     "Zbynìk",
     "Tychtl"
    ],
    [
     "Josef",
     "Psutka"
    ]
   ],
   "title": "Speech production based on the mel-frequency cepstral coefficients",
   "original": "e99_2335",
   "page_count": 4,
   "order": 513,
   "p1": "2335",
   "pn": "2338",
   "abstract": [
    "The mel-frequency cepstral coefficients (MFCCs) are frequently used as a speech parameterization in speech recognizers. Practical applications of speech recognition and dialogue systems bring sometimes a requirement to synthesize or reconstruct the speech from the saved or transmitted MFCCs. Presented paper describes an approach to the construction of a MFCC-based speech production system and discusses various possibilities of its excitation.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-510"
  },
  "rank99_eurospeech": {
   "authors": [
    [
     "Erhard",
     "Rank"
    ]
   ],
   "title": "Exploiting improved parameter smoothing within a hybrid concatenative/LPC speech synthesizer",
   "original": "e99_2339",
   "page_count": 4,
   "order": 514,
   "p1": "2339",
   "pn": "2342",
   "abstract": [
    "We depict the interpolation strategies for the concatenation of inventory demisyllables in our hybrid concatena-tive/ LPC speech synthesizer. Inventory elements for vowels and nasals are cut in the steady state of the phoneme. Concatenating elements in the synthesis stage requires smoothing of spectral content and energy to avoid annoying discontinuities in these parameters, which is of vital importance for the quality of synthesized speech. The hy-brid synthesizer concept allows the application of smoothing algorithms uncommon to pure time domain synthesis. Using the smoothing algorithmalso the occurrence of musical tones due to the LPC filter resonances at the begin of pauses can be suppressed.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-511"
  },
  "stylianou99_eurospeech": {
   "authors": [
    [
     "Yannis",
     "Stylianou"
    ]
   ],
   "title": "Synchronization of speech frames based on phase data with application to concatenative speech synthesis",
   "original": "e99_2343",
   "page_count": 4,
   "order": 515,
   "p1": "2343",
   "pn": "2346",
   "abstract": [
    "Synchronization of speech frames is an important issue in a concatenative speech synthesis system. In terms of signal processing this is translated in removing linear phase mismatches between concatenated speech frames. This paper presents two novel approaches to the problem of synchronization of speech frames with an application to concatenative speech synthesis. Both methods are based on a processing of phase spectra without decreasing the quality of the input speech, in contrast to previously proposed methods. The first method is based on the notion of center of gravity and the second on differentiated phase data. The proposed methods have been tested with the Harmonic plus Noise Model, HNM, in the context of Textto-Speech synthesis. The resulting synthetic speech is free of linear phase mismatches.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-512"
  },
  "yoshimura99_eurospeech": {
   "authors": [
    [
     "Takayoshi",
     "Yoshimura"
    ],
    [
     "Keiichi",
     "Tokuda"
    ],
    [
     "Takashi",
     "Masuko"
    ],
    [
     "Takao",
     "Kobayashi"
    ],
    [
     "Tadashi",
     "Kitamura"
    ]
   ],
   "title": "Simultaneous modeling of spectrum, pitch and duration in HMM-based speech synthesis",
   "original": "e99_2347",
   "page_count": 4,
   "order": 516,
   "p1": "2347",
   "pn": "2350",
   "abstract": [
    "In this paper, we describe an HMM­based speech synthesis system in which spectrum, pitch and state duration are modeled simultaneously in a unified framework of H­MM. In the system, pitch and state duration are modeled by multispace probability distribution HMMs and multidimensional Gaussian distributions, respectively. The distributions for spectral parameter, pitch parameter and the state duration are clustered independently by using a decision­tree based context clustering technique. Synthetic speech is generated by using an speech parameter generation algorithm from HMMand a melcepstrum based vocod­ing technique. Through informal listening tests, we have confirmed that the proposed system successfully synthesizes natural­sounding speech which resembles the speaker the training database.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-513"
  },
  "glotin99_eurospeech": {
   "authors": [
    [
     "Hervé",
     "Glotin"
    ],
    [
     "Frédéric",
     "Berthommier"
    ],
    [
     "Emmanuel",
     "Tessier"
    ]
   ],
   "title": "A CASA-labelling model using the localisation cue for robust cocktail-party speech recognition",
   "original": "e99_2351",
   "page_count": 4,
   "order": 517,
   "p1": "2351",
   "pn": "2354",
   "abstract": [
    "We propose a new cocktail-party recognition technique based on the coupling of a CASA-labelling method using the TDOA (Time Delay Of Arrival) with multistream recognition. This is an alternative to the classical \"segregate and recognise\" architecture. First, we have recorded a stereo database ST-NB95 from the mono Numbers95. This is composed of binary mixtures of sentences at 0dB, placed left and right. The probability to get the labels \"left\" and \"right\" is assigned to the subband time frames thanks to a mapping function. This depends on the relative level. It is established a priori, using a reference database composed of isolated words recorded in the same condition. We adapt the recognition paradigm to this particular situation. The model WER of binary mixtures is about 50%. This is a great improvement relatively to the WER (73%) of the fullband PLP. We conclude the model is able to recognise the dominant words of a binary mixture.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-514"
  },
  "bayya99_eurospeech": {
   "authors": [
    [
     "Aruna",
     "Bayya"
    ],
    [
     "B.",
     "Yegnanarayana"
    ]
   ],
   "title": "Noise-invariant representation for speech signals",
   "original": "e99_2355",
   "page_count": 4,
   "order": 518,
   "p1": "2355",
   "pn": "2358",
   "abstract": [
    "A new group-delay based spectral domain is explored for representation of speech signals and for extraction of robust features. The spectrum is computed using the group-delay functions defined on the autocorrelation of a short segment of speech. The features derived from this spectrum are easy to compute and are robust to the background noise. The invariance of the spectral shape to noise in this domain is demonstrated by comparing the group-delay spectrum to the Discrete Fourier transform (DFT) based spectrum and the LPC-derived spectrum. The new domain representation can be applied for parameter estimation as well as speech recognition. In this paper we present preliminary results of using such features in Speaker-Dependent (SD) as well Speaker-Independent (SI) recognition systems.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-515"
  },
  "elmaleh99_eurospeech": {
   "authors": [
    [
     "Khaled",
     "El-Maleh"
    ],
    [
     "Peter",
     "Kabal"
    ]
   ],
   "title": "Natural-quality background noise coding using residual substitution",
   "original": "e99_2359",
   "page_count": 4,
   "order": 519,
   "p1": "2359",
   "pn": "2362",
   "abstract": [
    "Existing approaches to background noise coding at very low bit rates (i.e., below 1 kbps) fail to reproduce the noise with natural quality, resulting in a degradation of the overall perceived quality. In this paper, we propose a novel scheme for natural-quality reduced-rate coding of background acoustic noise in voice communication systems. A better representation of the excitation signal in a noise-synthesis model is achieved by classifying the type of acoustic environment noise. Class-dependent residual substitution is used at the receive side to synthesize background noise that sounds similar to the background noise at the transmit side. The improvement in the quality of synthesized noise during speech gaps helps in preserving noise continuity between talk spurts and speech pauses, and enhances the perceived quality of a conversation.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-516"
  },
  "fernandez99_eurospeech": {
   "authors": [
    [
     "Julian",
     "Fernández"
    ],
    [
     "Eduardo",
     "Lleida"
    ],
    [
     "Enrique",
     "Masgrau"
    ]
   ],
   "title": "Microphone array design for robust speech acquisition and recognition",
   "original": "e99_2363",
   "page_count": 4,
   "order": 520,
   "p1": "2363",
   "pn": "2366",
   "abstract": [
    "The aim of this paper is to study the use of a robust acquisition system based on a microphone array for speech related applications in real situations. A comparison is performed between two beamforming methods: the Delay and Sum beamforming (DS) and the Spatial Reference Optimal beamforming (SRO). Both of them are frequency domain designed, using harmonic spatial distributed microphones. The quality of the microphone array output signal for the different beamforming approaches is measured by using a continuous speech recognition system based on discrete HMMs. Results of the speech recognition system show the advantage of using a microphone array with the SRO beamforming as front-end.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-517"
  },
  "guilmin99_eurospeech": {
   "authors": [
    [
     "Gwénaél",
     "Guilmin"
    ],
    [
     "Régine Le",
     "Bouquin-Jeannès"
    ],
    [
     "Philippe",
     "Gournay"
    ]
   ],
   "title": "Study of the influence of noise pre-processing on the performance of a low bit rate parametric speech coder",
   "original": "e99_2367",
   "page_count": 4,
   "order": 521,
   "p1": "2367",
   "pn": "2370",
   "abstract": [
    "This paper describes a prospective study of the contribution of a single-sensor noise pre-processing method, prior to coding, to the performance of a parametric low bit rate speech coder in adverse conditions. The 2.4kbits/s vocoder we use estimates four parameters: fundamental frequency, voicing, linear prediction coefficients and energy. Firstly, we study the influence of different noise levels on the estimated parameters with and without noise reduction system. Secondly, we measure the contribution of (i) each speech coder parameter and (ii) the speech enhancement system to the global output intelligibility. Finally, results show the interest of such a speech enhancement system for low bit rate parameter estimation and underline the interest to adapt different pre-processing techniques for each parameter estimation.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-518"
  },
  "haverinen99_eurospeech": {
   "authors": [
    [
     "Hemmo",
     "Haverinen"
    ],
    [
     "Petri",
     "Salmela"
    ],
    [
     "Juha",
     "Häkkinen"
    ],
    [
     "Mikko",
     "Lehtokangas"
    ],
    [
     "Jukka",
     "Saarinen"
    ]
   ],
   "title": "MLP network for enhancement of noisy MFCC vectors",
   "original": "e99_2371",
   "page_count": 4,
   "order": 522,
   "p1": "2371",
   "pn": "2374",
   "abstract": [
    "The performance of voice dialling systems often degrades rapidly as the intensity of the background noise increases. In this paper, we describe a neural network based speech enhancement technique for improving the speech recognition performance of a voice dialling sys-tem in very noisy real world type conditions. The speech samples were recorded in laboratory conditions and after-wards corrupted by adding car noise or babble noise recorded in a cafe. These noise corrupted speech samples were enhanced in cepstral domain by a context dependent multilayer perceptron (MLP) network before performing the recognition using a hidden Markov model (HMM) based speech recognition system. The accuracy of the test set increased 58%, 55% and 46% in the car noise envi-ronments having -5 dB, 0 dB and 5 dB SNRs, respec-tively. The accuracy of the test set increased 44%, 48% and 39% in the babble noise environments having SNR 5 dB, 10 dB and 15 dB, respectively. The accuracy remained approximately same for both car and babble noise environments when having SNR of 20 dB.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-519"
  },
  "isosipila99_eurospeech": {
   "authors": [
    [
     "J.",
     "Iso-Sipilä"
    ],
    [
     "K.",
     "Laurila"
    ],
    [
     "Ramalingam",
     "Hariharan"
    ],
    [
     "Olli",
     "Viikki"
    ]
   ],
   "title": "Hands-free voice activation in noisy car environment",
   "original": "e99_2375",
   "page_count": 4,
   "order": 523,
   "p1": "2375",
   "pn": "2378",
   "abstract": [
    "In this paper, we propose an algorithm that enables hands-free and eyes-free voice activated dialing in noisy car environment. Noise robustness is achieved by duration modeling and feature vector normalization techniques. High acceptance of valid command words and high rejection of out-ofvocabulary input is obtained by combination of command word partitioning into proper subword models and a user interface related multi-level approach which utilizes users' custom to repeat unrecognized commands. Experimental simulations show that both in, speaker-independent and speaker-dependent case, we are capable of achieving 99% acceptance of valid command words with less than one false alarm in 25 hours of conversational speech, music and car noise. In addition, real-time car environment tests with novice users show the validity of the simulation results.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-520"
  },
  "karray99_eurospeech": {
   "authors": [
    [
     "Lamia",
     "Karray"
    ],
    [
     "Emmanuel",
     "Polard"
    ]
   ],
   "title": "A wavelet denoising technique to improve endpoint detection in adverse conditions",
   "original": "e99_2379",
   "page_count": 4,
   "order": 524,
   "p1": "2379",
   "pn": "2382",
   "abstract": [
    "Recognition performance decreases when automatic recognition systems are used over the telephone network, especially wireless network and noisy environments. Previous studies have shown that non efficient speech/non-speech detection is a very important source of this degradation. Hence, speech detector robustness to noise is highly required, in order to improve recognition performance for the very noisy communications. Several studies were conducted aiming at increasing the robustness of speech/non-speech detection used for speech recognition in adverse conditions. However, despite the improvements, many segments of noise may be wrongly detected by the robust speech/non-speech detector, which increases the false acceptance errors. Therefore, this paper introduces an efficient method to reject such false detections in order to provide a robust word boundary detection algorithm reliable in the very noisy cellular network environment. The algorithm is based on a denoising technique using a discrete wavelet transform of the detector's output segments.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-521"
  },
  "kuropatwinski99_eurospeech": {
   "authors": [
    [
     "Marcin",
     "Kuropatwinski"
    ],
    [
     "Dieter",
     "Leckschat"
    ],
    [
     "Kristian",
     "Kroschel"
    ],
    [
     "Andrzej",
     "Czyzewski"
    ],
    [
     "Chaz",
     "Hales"
    ]
   ],
   "title": "Speech enhancement for linear-predictive-analysis-by-synthesis coders",
   "original": "e99_2383",
   "page_count": 4,
   "order": 525,
   "p1": "2383",
   "pn": "2386",
   "abstract": [
    "Speech coding techniques commonly used in low bit rate analysis-by-synthesis linear predictive coders (LPAS coders) create a model that emphasizes the important features of a speech signal. The utilization of these coding methods for speech enhancement is shown. Specifically, the speech signal will be modeled as the output of a cascade of an adaptive formant filter and an adaptive pitch filter, driven by a white Gaussian process with a time changing variance. The parameter and signal estimation method, which is based on the Expectation Maximization (EM) algorithm and implements this speech signal model, is investigated. The proposed approach yields better performance both in SNR and subjective impression than do speech enhancement methods which use only AR speech signal parameters.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-522"
  },
  "matsumoto99_eurospeech": {
   "authors": [
    [
     "Hiroshi",
     "Matsumoto"
    ],
    [
     "Hiroaki",
     "Ubukata"
    ]
   ],
   "title": "Robust HMM to variation of noisy environments based on variance extension of noise models",
   "original": "e99_2387",
   "page_count": 4,
   "order": 526,
   "p1": "2387",
   "pn": "2390",
   "abstract": [
    "This paper addresses the problem of making the PMC-based HMM robust to variation of SNR. The method proposed consists of simply expanding the variace of cepstral coefficients for the noise model in the parallel model combination (PMC). The effect of this technique is examined through speaker independent isolated digit recognition tests using NOISEX-92 noise data. The results show that the variance expansion of several lower order cepstra extremely improves robustness to a wide range of SNR mismatch over the standard PMC. The appropriate expansion factor is such that the expanded variance of the zeroth cepstrum is around 20dB with respect to its geometric mean level.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-523"
  },
  "nemer99_eurospeech": {
   "authors": [
    [
     "Elias",
     "Nemer"
    ],
    [
     "Rafik",
     "Goubran"
    ],
    [
     "Samy",
     "Mahmoud"
    ]
   ],
   "title": "The fourth-order cumulant of speech signals with application to voice activity detection",
   "original": "e99_2391",
   "page_count": 4,
   "order": 527,
   "p1": "2391",
   "pn": "2394",
   "abstract": [
    "This paper explores the fourth order cumulants (FOC) of the LPC residual of speech signals and presents a new algorithm for Voice Activity detection (VAD) based on the newly established FOC properties. Analytical expressions for the horizontal slice of the 4th cumulant as well as the kurtosis of voiced speech are derived based on a reported sinusoidal model [4]. The derivations demonstrate that the kurtosis of voiced speech is distinct from that of Gaussian noise and can be used to aid in detecting voicing. The proposed VAD combines FOC metrics with SNR measures to classify speech and noise frames. Its performance is compared to the ITU-T G.729B VAD [1] in various noise conditions, and quantified using the probability of correct and false classifications. The results show the proposed VAD has overall comparable performance to the G.729B: Its probability of false classification is lower in low SNR and Gaussian-like noise, but higher in speech-like noises.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-524"
  },
  "shieh99_eurospeech": {
   "authors": [
    [
     "Woei-Chyang",
     "Shieh"
    ],
    [
     "Sen-Chia",
     "Chang"
    ]
   ],
   "title": "The dependence of feature vectors under adverse noise",
   "original": "e99_2395",
   "page_count": 4,
   "order": 528,
   "p1": "2395",
   "pn": "2398",
   "abstract": [
    "The performance degradation of automatic speech recognition system due to acoustic mismatch in training and testing environment is a severe problem for practical use of speech recognizer [1]. In this paper, we explore the effects of noise on individual speech feature vector statistics, and several feature normalization methods are used to compensate environment influence on feature vectors. We try to find out what kind of normalization is most effective and which feature vectors should be normalized in order to achieve robust features under adverse noise.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-525"
  },
  "tchorz99_eurospeech": {
   "authors": [
    [
     "Jürgen",
     "Tchorz"
    ],
    [
     "Birger",
     "Kollmeier"
    ]
   ],
   "title": "Speech detection and SNR prediction basing on amplitude modulation pattern recognition",
   "original": "e99_2399",
   "page_count": 5,
   "order": 529,
   "p1": "2399",
   "pn": "2402",
   "abstract": [
    "A sound classification algorithm is presented which estimates the signal-to-noise ratio between speech and noise in 15 different frequency channels. The algorithm bases on the extraction of spectro-temporal features from the acoustical waveform. The approach is motivated by neurophysiological findings on periodicity coding in the auditory system of mammals. The extracted feature patterns are called Amplitude Modulation Spectrograms (AMS), as each AMS pattern contains information on both center frequencies and amplitude modulations in a short segment (32ms) of the input signal. An artificial neural network is trained on a large set of AMS patterns from mixtures of speech and noise and is then used to predict the narrow-band signal-to-noise ratio of unknown sounds.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-526"
  },
  "vicente99_eurospeech": {
   "authors": [
    [
     "Luis",
     "Vicente"
    ],
    [
     "Stephen J.",
     "Elliott"
    ],
    [
     "Enrique",
     "Masgrau"
    ]
   ],
   "title": "Fast active noise control for robust speech acquisition",
   "original": "e99_2403",
   "page_count": 4,
   "order": 530,
   "p1": "2403",
   "pn": "2406",
   "abstract": [
    "Active noise control (ANC) can be a valuable resort for robust speech acquisition. Furthermore, this technique can have the added benefit of minimising the Lombard effect. In the context of broadband active noise cancellation, the adaptive algorithm most widely used is the well-known Filtered-x Least Mean Squares (FxLMS). In the present work we study an alternative to FxLMS algorithm that tries to overcome its sometimes slow convergence without loss of cancellation capability. The alternative presented here is the ALE + FxLMS system, where an Adaptive Line Enhancer (ALE) is used as decorrelating stage for the FxLMS algorithm. The single-channel case (one reference signal, one actuator and one error sensor) and three different extensions of the system to the multiple channel case are presented and evaluated. The proposed system has proven to be able to achieve faster convergence with reference to the single FxLMS, without decorrelating pre-processing.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-527"
  },
  "vizinho99_eurospeech": {
   "authors": [
    [
     "Ascension",
     "Vizinho"
    ],
    [
     "Phil",
     "Green"
    ],
    [
     "M.",
     "Cooke"
    ],
    [
     "Ljubomir",
     "Josifovski"
    ]
   ],
   "title": "Missing data theory, spectral subtraction and signal-to-noise estimation for robust ASR: an integrated study",
   "original": "e99_2407",
   "page_count": 4,
   "order": 531,
   "p1": "2407",
   "pn": "2410",
   "abstract": [
    "In the missing data approach to robust Automatic Speech Recognition (ASR), time-frequency regions which carry reliable speech information are identified. Recognition is then based on these regions alone. In this paper, we address the problem of identifying reliable regions and propose two criteria to solve this based on negative energy and SNR. These criteria are evaluated on the TIDigits corpus for several noise sources and compared with spectral subtraction. We show that in this task the missing data method performs considerably better than spectral subtraction and the combination of the two techniques outperforms either technique used alone. We report robust performance at 0dB SNR for car noise and 10dB SNR for factory noise.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-528"
  },
  "vetter99_eurospeech": {
   "authors": [
    [
     "Rolf",
     "Vetter"
    ],
    [
     "Nathalie",
     "Virag"
    ],
    [
     "Philippe",
     "Renevey"
    ],
    [
     "Jean-Marc",
     "Vesin"
    ]
   ],
   "title": "Single channel speech enhancement using principal component analysis and MDL subspace selection",
   "original": "e99_2411",
   "page_count": 4,
   "order": 532,
   "p1": "2411",
   "pn": "2414",
   "abstract": [
    "We present in this paper a novel subspace approach for single channel speech enhancement and speech recognition in highly noisy environments. Our algorithm is based on principal component analysis and the optimal subspace selection is provided by a minimum description length criterion. This choice overcomes the limitations encountered with other selection criteria, like the overestimation of the signal-plus-noise subspace or the need for empirical parameters. We have also extended our subspace algorithm to take into account the case of colored noise. The performance evaluation shows that our method provides a higher noise reduction and a lower signal distortion than existing enhancement methods and that speech recognition in noise is improved. Our algorithm succeeds in extracting the relevant features of speech even in highly noisy conditions without introducing artefacts such as \\musical noise\".\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-529"
  },
  "barrachina99_eurospeech": {
   "authors": [
    [
     "Sergio",
     "Barrachina"
    ],
    [
     "Juan Miguel",
     "Vilar"
    ]
   ],
   "title": "Automatically deriving categories for translation",
   "original": "e99_2415",
   "page_count": 4,
   "order": 533,
   "p1": "2415",
   "pn": "2418",
   "abstract": [
    "An adequate approach to speech translation for small to medium sized tasks is the use of subsequential trans-ducers - a finite state model - as language model for a speech recognizer. These transducers can be automatically trained from sample corpora. The use of manually defined categories improves the training of the subsequential transducers when the available data are scarce. These categories depend on the source and target languages we want to translate. We introduce an automatic approach to derive categories that can be used in training subsequential transducers. This approach extends monolingual word clustering methods to the bilingual case using alignments obtained from statistical models. Experimental results indicate that the models trained with these categories have lower translation errors.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-530"
  },
  "corazza99_eurospeech": {
   "authors": [
    [
     "A.",
     "Corazza"
    ]
   ],
   "title": "An inter-domain portable approach to interchange format construction",
   "original": "e99_2419",
   "page_count": 4,
   "order": 534,
   "p1": "2419",
   "pn": "2422",
   "abstract": [
    "The work presented in this paper regards the construction of the formal representation of the content ofthe input sentence. It is part of a speech-to-speech translation system, where this language independent format is used as an intermediate representation between source and target languages. The general application domain is travel planning, but it is divided in more specific domains, such as hotel and transport reservation. Starting from the word string output by the recognizer, the construction of the content representation requires the recognition of: 1) the speech act and concept list, and 2) a list of arguments in the form of argument name/value pairs. For the former, semantic classification trees [5] are used and domain portability depends on the availability of labeled data for the new domain. For argument extraction a pipeline of filters is used, where each filter is based on a semantic grammar, which in most cases is regular. Then, these filters can be used all together for the most general analysis, or separately when only one or a few domains are considered. Argument extraction is described and evaluated in terms of the changes in precision and recall with respect to the number of filters and of rules.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-531"
  },
  "casan99_eurospeech": {
   "authors": [
    [
     "G. A.",
     "Casañ"
    ],
    [
     "M. A.",
     "Castaño"
    ]
   ],
   "title": "Distributed representation of vocabularies in the RECONTRA neural translator",
   "original": "e99_2423",
   "page_count": 5,
   "order": 535,
   "p1": "2423",
   "pn": "2426",
   "abstract": [
    "A simple neural translator called RECONTRA (REcurrent CONnectionist TRAnslator) has recently shown to successfully approach simple text-to-text limited-domain Machine Translation tasks. In this approach the vocabularies involved in the translations were represented according to (simple and clear) local codifications. However, in order to deal with large vocabularies local representations would led to networks with an excessive number of connections to be trained. Consequently, distributed representations of both source and target vocabularies are required. This paper studies appropriate types of distributed codifications to represent large vocabularies in the RECONTRA translator.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-532"
  },
  "reithinger99_eurospeech": {
   "authors": [
    [
     "Norbert",
     "Reithinger"
    ]
   ],
   "title": "Robust information extraction in a speech translation system",
   "original": "e99_2427",
   "page_count": 4,
   "order": 536,
   "p1": "2427",
   "pn": "2430",
   "abstract": [
    "Speech processing systems must be able to cope with recognition errors or non grammatical input from the users. We present an approach used in Verbmobil to robustly process domain relevant information using cascaded automata. One set of automata is used to extract expressions representing relevant data for the travel planning and hotel reservation domains. This information is e.g. used by the dialogue processing module. Another set of automata is used to generate natural language expressions from these representations. This robust reductionistic translation is one of four translation tracks within Verbmobil .\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-533"
  },
  "sugaya99_eurospeech": {
   "authors": [
    [
     "Fumiaki",
     "Sugaya"
    ],
    [
     "Toshiyuki",
     "Takezawa"
    ],
    [
     "Akio",
     "Yokoo"
    ],
    [
     "Seiichi",
     "Yamamoto"
    ]
   ],
   "title": "End-to-end evaluation in ATR-MATRIX: speech translation system between English and Japanese",
   "original": "e99_2431",
   "page_count": 4,
   "order": 537,
   "p1": "2431",
   "pn": "2434",
   "abstract": [
    "ATR Interpreting Telecommunications Research Laboratories developed ATR-MATRIX speech translation system, which translates both ways between English and Japanese, enough to hold natural on-line real-time conversations. Using this system we started an end-to-end evaluation of a speech translation system through a dialog test with naive speakers who are not involved in system development and not familiar with a speech translation technology. This paper explains the speech translation design concept, evaluation system overview, evaluation procedure and some interesting results observed in the test. Finally after concluding we will mention our future plan.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-534"
  },
  "dharanipragada99_eurospeech": {
   "authors": [
    [
     "S.",
     "Dharanipragada"
    ],
    [
     "Martin",
     "Franz"
    ],
    [
     "J. S.",
     "McCarley"
    ],
    [
     "Salim",
     "Roukos"
    ],
    [
     "T.",
     "Ward"
    ]
   ],
   "title": "Story segmentation and topic detection for recognized speech",
   "original": "e99_2435",
   "page_count": 4,
   "order": 538,
   "p1": "2435",
   "pn": "2438",
   "abstract": [
    "In this paper we present algorithms for story segmentation, topic detection, and topic tracking. The algorithmsuse a combination of machine learning, statistical naturallanguage processing and information retrieval techniques.The story segmentation algorithm is a two stage algorithm that uses a decision tree based probabilistic modelin the first stage and incorporates aspects of our topicdetection system via an information-retrieval based refinement scheme in the second stage. The topic detectionand tracking algorithm is an incremental clustering algorithm that employs a novel dynamic cluster-dependentsimilarity measure between documents and clusters. Per-formance of these algorithms are measured on the 1998DARPA sponsored Topic Detection and Tracking Phase2 (TDT2) evaluation task.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-535"
  },
  "jin99_eurospeech": {
   "authors": [
    [
     "Hubert",
     "Jin"
    ],
    [
     "Richard",
     "Schwartz"
    ],
    [
     "Sreenivasa",
     "Sista"
    ],
    [
     "Frederick",
     "Walls"
    ]
   ],
   "title": "Topic tracking for radio, TV broadcast, and newswire",
   "original": "e99_2439",
   "page_count": 4,
   "order": 539,
   "p1": "2439",
   "pn": "2442",
   "abstract": [
    "We present our tracking system for the 1998 Topic Detection and Tracking project (TDT-2). This project addresses multiple sources of information in the form ofboth text and speech from newswire, radio and television news broadcast programs. Our tracking system isprobability based and we successfully solve the problemof score normalization across topics with a simple buteffective solution. Tested on the 20K TDT-2 stories collected between March and April 1998, our tracking systemachieves a performance of 1.5% miss error (on closed cap-tion and newswire) and 3.0% miss error (on automaticspeech recognition output and newswire) at the cost of0.1% false alarm error. In the 1998 TDT-2 evaluation,our tracking system was ranked the best with the officialtopic-weighted Ctrack measure of 0.0056. We observedthat there was no degradation in tracking performancedue to the speech recognition errors.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-536"
  },
  "lowe99_eurospeech": {
   "authors": [
    [
     "Stephen A.",
     "Lowe"
    ]
   ],
   "title": "The beta-binomial mixture model for word frequencies in documents with applications to information retrieval",
   "original": "e99_2443",
   "page_count": 4,
   "order": 540,
   "p1": "2443",
   "pn": "2446",
   "abstract": [
    "This paper describes a continuous-mixture statistical model for word occurrence frequencies in documents, and the application of that model to the DARPA-sponsored TDT topic identification tasks [1]. This model was originally proposed in 1990 by L. Gillick [2] as a means to account for variation in word frequencies across documents more accurately than the binomial model. The present paper presents further mathematical development of the model, leading to the implementation of a topic-tracking system. Performance results for this system on the Tracking Task in the December 1998 DARPA TDT Evaluation will be shown and compared with Dragons existing, more complex multinomial-model-based system. (Results from other systems applied to this task are available in [3].) We will conclude with plans for further development.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-537"
  },
  "nakazawa99_eurospeech": {
   "authors": [
    [
     "Masayuki",
     "Nakazawa"
    ],
    [
     "Jianxin",
     "Zhang"
    ],
    [
     "Ryuichi",
     "Oka"
    ]
   ],
   "title": "Topic spotting and its description of summary from spontaneous speech",
   "original": "e99_2447",
   "page_count": 4,
   "order": 541,
   "p1": "2447",
   "pn": "2450",
   "abstract": [
    "This paper proposes two new methods of how to carry out automatic topic spotting in continuous speech and how to describe its summary in the form of slot expression. The topic spotting works independently of both speakers and topics. The method of topic spotting is based on a dialogue model to segment a topic using only a surface feature of continuous speech. We have evaluated our methods using the speech of the broadcasting news and the spoken dialogue speech. The result was that detection quality was over 80\\% on the broadcasting news. This paper shows some experimental results for Japanese spontaneous speech.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-538"
  },
  "walls99_eurospeech": {
   "authors": [
    [
     "Frederick",
     "Walls"
    ],
    [
     "Hubert",
     "Jin"
    ],
    [
     "Sreenivasa",
     "Sista"
    ],
    [
     "Richard",
     "Schwartz"
    ]
   ],
   "title": "Topic detection in broadcast news",
   "original": "e99_2451",
   "page_count": 4,
   "order": 542,
   "p1": "2451",
   "pn": "2454",
   "abstract": [
    "We propose a system for the Topic Detection and Tracking (TDT) detection task concerned with the unsupervised grouping of news stories according to topic. We use an incremental k -means algorithm for clustering stories. For comparing stories, we utilize a probabilistic document similarity metric and a traditional vector-space metric. We note that that the clustering algorithm requires two different types of metrics and adapt similarity metrics for each purpose. The system achieves a topic-weighted miss rate of 12% at a false accept rate of 0.22%.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-539"
  },
  "botinis99_eurospeech": {
   "authors": [
    [
     "Antonis",
     "Botinis"
    ],
    [
     "Marios",
     "Fourakis"
    ],
    [
     "Irini",
     "Prinou"
    ]
   ],
   "title": "Prosodic effects on segmental durations in greek",
   "original": "e99_2475",
   "page_count": 4,
   "order": 543,
   "p1": "2475",
   "pn": "2478",
   "abstract": [
    "This is an acoustic study of prosodic effects on segmental durations with reference to syllable structure, stress, focus and tempo in Greek. Disyllabic nonsense words with one, or two, or three consonants in the initial syllable were examined in stressed/unstressed, focused/unfocused and normal/fast tempo productions in a carrier sentence contexts. The results indicate that: (1) syllabic onset branching has a bigger effect on the consonant than the vowel; (2) stress has a bigger effect on the vowel than the consonant; (3) focus effects are not substantially different from stress effects; and (4) tempo has a rather even effect on consonant and vowel durations.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-540"
  },
  "blomberg99_eurospeech": {
   "authors": [
    [
     "Mats",
     "Blomberg"
    ]
   ],
   "title": "Within-utterance correlation for speech recognition",
   "original": "e99_2479",
   "page_count": 4,
   "order": 544,
   "p1": "2479",
   "pn": "2482",
   "abstract": [
    "Relations between non-adjacent parts of an utterance are commonly regarded as an important source of information for speech recognition. However, they have not been very much used in speech recognition systems. In this paper, we include this information by joint distributions of pairs of phones occurring in the same utterance. In addition to relations between acoustic events, we also have incorporated relations between spectral and prosodically oriented information, such as phone duration, position in utterance and funda-mental frequency. Preliminary recognition results on N-best rescoring show 10% word error reduction compared to a baseline Viterbi decoder.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-541"
  },
  "gelin99b_eurospeech": {
   "authors": [
    [
     "Philippe",
     "Gelin"
    ],
    [
     "Jean-Claude",
     "Junqua"
    ]
   ],
   "title": "Techniques for robust speech recognition in the car environment",
   "original": "e99_2483",
   "page_count": 4,
   "order": 545,
   "p1": "2483",
   "pn": "2486",
   "abstract": [
    "The use of voice commands or navigation features in the car is becoming a necessity. As keyboard and display interfaces cannot be used safely while driving, much effort has been done to make automatic speech recognition (ASR) and Text-to-Speech synthesis (TTS) ubiquitous features in the car. From voice dialing to car navigation, the requirements for voice technology vary greatly. While the use of a hands-free microphone and noise robust algorithms is a must, a wide range of technology spanning from small vocabulary isolated word/continuous speech to phonetic-based flexible vocabulary ASR has to be developed. Except for voice dialing, speaker-independent technology eventually combined with fast adaptation is mandatory. In this paper, we present our efforts in these directions. After focusing on two novel techniques for robust speech recognition in the car, we focus on fast speaker adaptation and report on experiments for a small set of 10 keywords, continuous digit/letter recognition along with phonetic-based recognition for 1800 words.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-542"
  },
  "giuliani99_eurospeech": {
   "authors": [
    [
     "Diego",
     "Giuliani"
    ]
   ],
   "title": "An on-line acoustic compensation technique for robust speech recognition",
   "original": "e99_2487",
   "page_count": 4,
   "order": 546,
   "p1": "2487",
   "pn": "2490",
   "abstract": [
    "In this work we report on the use of an on-line acoustic compensation technique for robust speech recognition.With this technique acoustic mismatch between training and actual conditions is reduced through acoustic mapping. At recognition stage, observation vectors delivered by the acoustic front-end are mapped into a reference acoustic space, while input data are exploited to update the statistical parameters of the mapping. Experimental results, obtained for matched and unmatched training and testing environment conditions, show that the investigated technique tangibly improves the performance of a speaker independent speech recognizer based on hidden Markov models. Furthermore, recognition results are close to those obtained with unsupervised incremental model adaptation based on maximum likelihood linear regression.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-543"
  },
  "hung99_eurospeech": {
   "authors": [
    [
     "Wei-Wen",
     "Hung"
    ],
    [
     "Hsiao-Chuan",
     "Wang"
    ]
   ],
   "title": "Using adaptive signal limiter together with noise-robust techniques for noisy speech recognition",
   "original": "e99_2491",
   "page_count": 4,
   "order": 547,
   "p1": "2491",
   "pn": "2494",
   "abstract": [
    "In a speech recognition system, environmental mismatch between speech models and test speech causes serious performance degradation. To solve this environmental mismatch problem, smoothing process is one of the most widely used techniques. In this paper, an adaptive signal limiter (ASL) is developed to smooth speech features so that the undesired spectral variations could be effectively reduced. In addition, we propose the method for combining ASL with other noise-robust techniques in the recognition of noisy speech.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-544"
  },
  "hong99_eurospeech": {
   "authors": [
    [
     "Wei-Tyng",
     "Hong"
    ],
    [
     "Sin-Horng",
     "Chen"
    ]
   ],
   "title": "A robust environment-effects suppression training algorithm for adverse Mandarin speech recognition",
   "original": "e99_2495",
   "page_count": 5,
   "order": 548,
   "p1": "2495",
   "pn": "2498",
   "abstract": [
    "In this paper, a new robust training algorithm for the generation of a set of bias-removed, noise-suppressed reference speech HMM models directly from a training database collected in adverse environment suffering with both convolutional channel bias and additive noise is proposed. Its main idea is to incorporate a signal bias-compensation operation and a PMC noise-compensation operation into its iterative training process in order to make the resulting speech HMM models more suitable to the given robust speech recognition method using the same signal bias-compensation and PMC noise-compensation operations in the recognition process. Experimental results showed that the speech HMM models it generated outperformed both the clean-speech HMM models and those generated by the conventional k-means algorithm for two adverse Mandarin speech recognition tasks. So it is a promising robust training algorithm.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-545"
  },
  "harju99_eurospeech": {
   "authors": [
    [
     "Mikko",
     "Harju"
    ],
    [
     "Petri",
     "Salmela"
    ],
    [
     "Olli",
     "Viikki"
    ],
    [
     "Mikko",
     "Lehtokangas"
    ],
    [
     "Jukka",
     "Saarinen"
    ]
   ],
   "title": "Robust speaker adaptation of continuous density HMMS using multilayer perceptron network",
   "original": "e99_2499",
   "page_count": 4,
   "order": 549,
   "p1": "2499",
   "pn": "2502",
   "abstract": [
    "The performance of global affine and nonlinear trans-formations for speaker adaptation in a hidden Markov model (HMM) speech recognition system are compared in this paper. The nonlinear transformation was obtained with a multilayer perceptron network (MLP) which was trained during the adaptation process to transform the mean vectors of the HMMs such that the output proba-bilities of the HMMs for the adaptation utterances were maximized. The performance of the MLP adaptation method was compared to the maximum likelihood linear regression (MLLR) adaptation procedure. Both of these methods were tested in a connected digit speech recogni-tion system using multi-environment models. The results show that the nonlinear MLP transformation clearly out-performs MLLR in terms of adaptation speed. Moreover, the performance of MLP adaptation with larger amounts of data was comparable to the MLLR performance.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-546"
  },
  "li99c_eurospeech": {
   "authors": [
    [
     "Chengrong",
     "Li"
    ],
    [
     "Jingdong",
     "Chen"
    ],
    [
     "Bo",
     "Xu"
    ]
   ],
   "title": "Regression class selection and speaker adaptation with MLLR in Mandarin continuous speech recognition",
   "original": "e99_2503",
   "page_count": 4,
   "order": 550,
   "p1": "2503",
   "pn": "2506",
   "abstract": [
    "Currently, CDHMM based continuous speech recognition has been widely extended to speaker-independent (SI) system. However, the performance of the SI system is highly dependent on the speakers, especially for Mandarin speech with accent, speaker adaptation becomes crucial important for real application. In this paper, MLLR approach is studied for speaker adaptation in mandarin continuous speech recognition and three approaches for defining regression classes are investigated: the first is based on Chinese phonetic classification, the second is based on statistical information of mixture distribution parameters and the third is based on state duration using segmental information. Other experiments like the effect of adaptation data and mixtures are presented also in the paper. The new variance-based regression class selecting scheme is proposed and has been proved to be effective.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-547"
  },
  "li99d_eurospeech": {
   "authors": [
    [
     "Guoqiang",
     "Li"
    ],
    [
     "Limin",
     "Du"
    ],
    [
     "Ziqiang",
     "Hou"
    ]
   ],
   "title": "Regression transformation of prior means for speaker adaptation",
   "original": "e99_2507",
   "page_count": 4,
   "order": 551,
   "p1": "2507",
   "pn": "2510",
   "abstract": [
    "Maximum a posteriori adaptation method combines the prior knowledge with adaptation data from a new speaker, which has a nice asymptotical property, but has a slow adaptation rate for not modifying unseen models. In a strictly Bayesian approach, prior parameters are assumed known, based on common or subjective knowledge. But a practical solution is to adopt an empirical Bayesian approach, where the prior parameters are estimated directly from training speech data itself. So there is a problem of mismatches between training and testing conditions. In this paper we propose a prior parameter transformation (PPT) adaptation approach that transforms the prior parameters to be more representative of the new speaker. It can influence unseen models by tying prior parameter transformations across different models according to amount of adaptation data available. Based on the improved prior information better model parameters can be obtained even with small amount of adaptation data.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-548"
  },
  "feng99_eurospeech": {
   "authors": [
    [
     "Liu",
     "Feng"
    ],
    [
     "Chi-wei",
     "Che"
    ],
    [
     "Peng",
     "Yu"
    ],
    [
     "Zuoying",
     "Wang"
    ]
   ],
   "title": "Linguistic tree based maximum likelihood model interpolation",
   "original": "e99_2511",
   "page_count": 4,
   "order": 552,
   "p1": "2511",
   "pn": "2514",
   "abstract": [
    "In this paper, a speaker adaptation method is presented which computes the speaker adapted model by a weighted sum of a set of speaker dependent models. The set of weights are estimated to maximize the likelihood of the adaptation data. Then a linguistic tree is constructed to cluster the mean vectors. The means in the same linguistic class share the same weight set, while the means in different classes use different weight set to compute the adapted model. Experiments show that with as little as 1-3 sentences a significant performance improvement is obtained. As more adaptation data is available, further improvement can be obtained.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-549"
  },
  "naito99_eurospeech": {
   "authors": [
    [
     "Masaki",
     "Naito"
    ],
    [
     "Li",
     "Deng"
    ],
    [
     "Yoshinori",
     "Sagisaka"
    ]
   ],
   "title": "Model-based speaker normalization methods for speech recognition",
   "original": "e99_2515",
   "page_count": 4,
   "order": 553,
   "p1": "2515",
   "pn": "2518",
   "abstract": [
    "We address the problem of how vocal-tract (VT) parameters and and the related VT geometric model can be used effectively to normalize the speech acoustic properties of the speakers. The problem is important since speaker variability is one major obstacle to high-accuracy speech recognition and use of VT parameters offers a natural way to account for such a variability. The data-driven methods used in the conventional technology for speaker adaptation requires a large amount of adaptation data, but our experimental results show the new model-based speaker normalization method described in this paper is superior in performance while drastically reducing the amount of adaptation data needed to normalize speakers.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-550"
  },
  "nguyen99_eurospeech": {
   "authors": [
    [
     "Patrick",
     "Nguyen"
    ],
    [
     "Christian",
     "Wellekens"
    ],
    [
     "Jean-Claude",
     "Junqua"
    ]
   ],
   "title": "Maximum likelihood eigenspace and MLLR for speech recognition in noisy environments",
   "original": "e99_2519",
   "page_count": 4,
   "order": 554,
   "p1": "2519",
   "pn": "2522",
   "abstract": [
    "A technique for rapid speaker adaptation, called eigenvoices, was introduced recently. The key idea is to confine models in a very low-dimensional lin-ear vector space. This space summarizes a prioriknowl-edge that we have about speaker models. In many practical systems, however, there is a mismatch between the conditions in which the training data were collected and test conditions: prior knowledge becomes improper. Furthermore, prior statistics or models of this mismatch may not be available. We expose two key results: first, we use a maximum-likelihood estimator of prior infor-mation in matched conditions, called MLES, leading to an improvement of adaptation by a relative 14%, and second, we show how we can apply a blind scheme for learning noise, MLLR, achieving an additional 7:7% relative improvement in noisy conditions.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-551"
  },
  "ono99_eurospeech": {
   "authors": [
    [
     "Yoshio",
     "Ono"
    ],
    [
     "Maki",
     "Yamada"
    ],
    [
     "Masakatsu",
     "Hoshimi"
    ]
   ],
   "title": "A study of speaker adaptation for speaker independent speech recognition method using phoneme similarity vector",
   "original": "e99_2523",
   "page_count": 4,
   "order": 555,
   "p1": "2523",
   "pn": "2526",
   "abstract": [
    "In this paper we introduce an effective speaker adaptation technique to our unique compact speech recognizer especially designed for consumer electronics products. The compact ASR method we have developed in our previous work employs phoneme similarities as feature parameters, which are extracted temporally successive matching between speech sample and 24 context-independent phoneme standard templates in forms of time-spectral pattern. With these unique feature parameters, the recognizer has been successfully embedded in a single DSP without any external memory chips. Speaker adaptation method we propose in this paper focuses on adaptation of the phoneme standard templates in the ASR method. When evaluated with 20 or less training words in 100 isolated words recognition test, the proposed method achieved reduction of error rate by 20 % approximately. The test results also show its effect for child subjects and validity of applying this technology in the recognizer which is used by various users in consumer electronics products.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-552"
  },
  "uebel99_eurospeech": {
   "authors": [
    [
     "L. F.",
     "Uebel"
    ],
    [
     "P. C.",
     "Woodland"
    ]
   ],
   "title": "An investigation into vocal tract length normalisation",
   "original": "e99_2527",
   "page_count": 4,
   "order": 556,
   "p1": "2527",
   "pn": "2530",
   "abstract": [
    "This paper investigates several different methods for performing vocal tract length normalisation (VTLN) which are either completely linear or piece-wise linear. Furthermore the combination of VTLN with either standard unconstrained maximum likelihood linear regression (MLLR) or constrained MLLR is considered. Results on the Switchboard corpus show that there is little difference in performance between the different forms of VTLN, and that as previously reported that the effects of VTLN and unconstrained MLLR are largely additive. However it was found that if multiple iterations of constrained MLLR is used there is no additional advantage to also using VTLN.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-553"
  },
  "yuk99_eurospeech": {
   "authors": [
    [
     "Zong Suk",
     "Yuk"
    ],
    [
     "James",
     "Flanagan"
    ],
    [
     "Mahesh",
     "Krishnamoorthy"
    ],
    [
     "Krishna",
     "Dayanidhi"
    ]
   ],
   "title": "Adaptation to environment and speaker using maximum likelihood neural networks",
   "original": "e99_2531",
   "page_count": 4,
   "order": 557,
   "p1": "2531",
   "pn": "2534",
   "abstract": [
    "When there is a mismatch between training and testing conditions, statistical speech recognition algorithms suffer from severe degra-dation in recognition accuracy. The mismatch could be due to the interference from acoustical environments where systems are actually used or from speakers themselves. In this paper, a neural network based transformation approach is studied to handle the data distribution mismatches between training and testing conditions. The conditional probability that comes from hidden Markov model (HMM) based recognizers is used for the objective function of a neural network. It maximizes the likelihood of the data from a test-ing environment, and allows global optimization of the network when used with HMM-based recognizers. The new objective function can be used to transform speech feature vectors, or the mean vectors and covariance matrices of a recognizer. The proposed algorithm is evaluated on a noisy distant-talking version of the Resource Management database.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-554"
  },
  "yu99_eurospeech": {
   "authors": [
    [
     "Xiuyang",
     "Yu"
    ],
    [
     "Wayne",
     "Ward"
    ]
   ],
   "title": "Corrective training for speaker adaptation",
   "original": "e99_2535",
   "page_count": 4,
   "order": 558,
   "p1": "2535",
   "pn": "2538",
   "abstract": [
    "This paper reports results on an experiment to use corrective training techniques for rapid acoustic speaker adaptation in a semi-continuous speech recognition system. Decoder output is used to adjust HMM acoustic models to improve discrimination between correct words and near misses. Twenty sentences are used as an adaptation set. A speech recognizer is run on each utterance to generate a word lattice. The lattice is pruned relative to the correct path. The forward-backward algorithm is used to align each path in the lattice against the speech input and compute observation counts. For each input frame, counts in correct models are adjusted upward, and counts in incorrect models are adjusted downward. The adjusted counts are normalized to generate new observation probabilities for the models. The parameters being adjusted are the mixture weights for the semi-continuous HMMs. The technique reduced word error for a test subject by 37% relative.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-555"
  },
  "obradovic99_eurospeech": {
   "authors": [
    [
     "R.",
     "Obradovic"
    ],
    [
     "D.",
     "Pekar"
    ],
    [
     "S.",
     "Krco"
    ],
    [
     "V.",
     "Delic"
    ],
    [
     "V.",
     "Senk"
    ]
   ],
   "title": "A robust speaker-independent CPU-based ASR system",
   "original": "e99_2881",
   "page_count": 4,
   "order": 559,
   "p1": "2881",
   "pn": "2884",
   "abstract": [
    "In this paper a new automatic speech recognition (ASR) CPU-based software, called AlfaNum, with the chosen few heuristics optimized for applications in heterogeneous conditions is described. AlfaNum is a discrete speaker-independent ASR product intended for application in the largest bank-by-phone interactive voice response (IVR) system in Yugoslavia, with a lot of customers all over Serbia. That means a large variety of dialects, telephone line quality, and microphones used. This system has been tested on 500 speakers and it achieved an average accuracy of 98,2% in real life conditions. The whole software is developed in C++ programming language. Object oriented programming gave the software an elegant look, and minimized all possible errors. On the other hand, the power of C++ language and its tight interaction with machine made the software fast and efficient.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-556"
  },
  "abouchakra99_eurospeech": {
   "authors": [
    [
     "Rabih",
     "Abouchakra"
    ],
    [
     "Peter",
     "Kabal"
    ]
   ],
   "title": "Delay estimation for transform domain acoustical echo cancellation",
   "original": "e99_2539",
   "page_count": 4,
   "order": 560,
   "p1": "2539",
   "pn": "2542",
   "abstract": [
    "Acoustic echo cancellation can be used to remove talker feedback in hands-free systems. Fast convergence and good tracking capabilities cannot be achieved by classical transform domain adaptive filtering algorithms when the reference signal has a variable rank autocorrelation matrix. During the low rank phases of the speech signal, some of the transform-domain tap coeficients become irrelevant to the adaptation process and stop adapting. When the autocorrelation matrix gains full rank, there will be no longer any \\frozen\" weights. In this paper, we focus on the DCTLMS algorithm and present a new method using a DCT based delay estimate from other coeficients to move the frozen weights closer to the optimal point and, consequently, reduce the overall re-convergence time.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-557"
  },
  "beaugeant99_eurospeech": {
   "authors": [
    [
     "C.",
     "Beaugeant"
    ],
    [
     "Pascal",
     "Scalart"
    ]
   ],
   "title": "Noise reduction using perceptual spectral change",
   "original": "e99_2543",
   "page_count": 4,
   "order": 561,
   "p1": "2543",
   "pn": "2546",
   "abstract": [
    "This article deals with the problem of noise reduction for hands-free sound pick-up. Our context is more precisely the sound pick-up using single microphone in cars. In such environments, the speech is highly corrupted by engine and aerodynamics noises. The methods proposed here try to enhance noise reduction systems based on Wiener filtering in the frequency domain by taking into account the short time variations of speech. It aims in fact at adapting the estimation of the signal spectrum and the estimation of the noise spectrum to the spectrum variations of the noisy signal. By preserving voice attacks, distortion on the useful signal is reduced as shown in this article. Background residual noise and musical tone phenomenon are also discussed about.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-558"
  },
  "hussain99_eurospeech": {
   "authors": [
    [
     "Amir",
     "Hussain"
    ],
    [
     "Douglas R.",
     "Campbell"
    ]
   ],
   "title": "Intelligibility improvements using diverse sub-band processing applied to noisy speech",
   "original": "e99_2547",
   "page_count": 4,
   "order": 562,
   "p1": "2547",
   "pn": "2550",
   "abstract": [
    "The paper presents the results of a series of experiments to assess the capability of a multi-microphone sub-band adaptive (MMSBA) signal processing scheme for improving the intelligibility of speech corrupted with noise recorded in an automobile. The noise corrupted speech signals were presented to 15 normal hearing volunteer subjects at various SNRs, and numbers and distributions of sub-bands. The results of listening tests were analysed to determine: (i) The direction and statistical significance (DSS) of any effect of processing treatment on intelligibility. (ii) The DSS of any effect of processing treatment on perceived quality. (iii) The DSS of any effect of the numbers of sub-bands used in sub-band processing (SBP). (iv) The DSS of any effect of the spacing of sub-bands used in SBP. In the experimental cases considered, the MMSBA scheme employing diverse sub-band processing is shown to deliver a statistically significant improvement in terms of both speech intelligibility and perceived quality when compared with both the wide-band processed and the noisy unprocessed case.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-559"
  },
  "koutras99_eurospeech": {
   "authors": [
    [
     "Athanasios",
     "Koutras"
    ],
    [
     "Evangelos",
     "Dermatas"
    ],
    [
     "George",
     "Kokkinakis"
    ]
   ],
   "title": "Recognizing simultaneous speech: a genetic algorithm approach",
   "original": "e99_2551",
   "page_count": 4,
   "order": 563,
   "p1": "2551",
   "pn": "2554",
   "abstract": [
    "In this paper it is shown experimentally that a new blind signal separation method in the frequency domain improves significantly the speaker signal to interference ratio (SIR) and the phoneme recognition score of a continuous speech, speaker-independent acoustic decoder in a two-simultaneous-speaker environment. The implemented two-sensor separation method is based on evolutionary minimization of the cross-correlation of the separated speech signals. Extensive experiments have been conducted in three types of artificially created mixture scenarios: instantaneous, time delayed and convolutive, using real room impulse responses. The experiments showed that in the worst case (convolutive mixture scenario) a mean improvement of 11dB SIR is achieved by the proposed GaBSS method for both output channels. Furthermore, the phoneme recognition rate of the separated signals was found to approach the rate measured with the clean signals in all experiments. The recognition rate improvement is maximised in the case of convoluted mixing of equal energy speech signals.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-560"
  },
  "bielawski99_eurospeech": {
   "authors": [
    [
     "Krzysztof",
     "Bielawski"
    ],
    [
     "Alexander A.",
     "Petrovsky"
    ]
   ],
   "title": "Speech enhancement system for hands-free telephone based on the psychoacoustically motivated filter bank with allpass frequency transformation #",
   "original": "e99_2555",
   "page_count": 4,
   "order": 564,
   "p1": "2555",
   "pn": "2558",
   "abstract": [
    "In this paper the application of multirate acoustic echo canceller derived from cochlea model and psychoacustically motivated noise and residual echo attenuation system operating on the signal decomposed in cochlear spaced subbands are described. Polyphase realisation of the FIR filter bank with non-uniform frequency resolution achieved by the frequency transformation of filter characteristic using recursive allpasses is presented as a starting point to construct of the analysis and synthesis filter bank used in proposed speech enhancement system. Human auditory perception model delivered by the psychoacoustic as well cochlear model with discrete time and discrete space are used to create this allpass transformation filter bank. Performing the acoustic echo cancellation by adaptive filtering in subbands we get echo compensated signal, which is forwarded to the noise and residual echo suppression unit. Presented idea of the system combines frequency warping and acoustic echo and noise control techniques to serve in application of communication hands-free device.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-561"
  },
  "shields99_eurospeech": {
   "authors": [
    [
     "P. W.",
     "Shields"
    ],
    [
     "Douglas R.",
     "Campbell"
    ]
   ],
   "title": "Speech enhancement using a multi-microphone sub-band adaptive griffiths-jim noise canceller",
   "original": "e99_2559",
   "page_count": 4,
   "order": 565,
   "p1": "2559",
   "pn": "2562",
   "abstract": [
    "The goal for most speech enhancement techniques is to improve speech quality by increasing intelligibility and/or reducing listener fatigue. The Multi-microphone Sub-band Adaptive (MMSBA) processing scheme has been shown to improve both the SNR and speech intelligibility of a wanted speech source in the presence of an unwanted noise source, in a moderately reverberant real-room environment using two microphones [1]. The proposed Sub-band Adaptive Griffiths-Jim (SBAGJ) processing scheme uses features from the Griffiths-Jim [2] adaptive noise canceller, in conjunction with the MMSBA scheme. The combination of the Griffiths-Jim front end, and the MMSBA noise cancelling section results in a system with the improved filter performance of a sub-band processing scheme, but bypasses the need to explicitly deal with the causality issues associated with many adaptive noise cancellation algorithms.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-562"
  },
  "szarvas99_eurospeech": {
   "authors": [
    [
     "M.",
     "Szarvas"
    ],
    [
     "T.",
     "Fegyó"
    ],
    [
     "P.",
     "Tatai"
    ],
    [
     "Géza",
     "Gordos"
    ]
   ],
   "title": "Qualiphone-a: a perceptual speech quality evaluation system for analog mobile networks",
   "original": "e99_2563",
   "page_count": 4,
   "order": 566,
   "p1": "2563",
   "pn": "2566",
   "abstract": [
    "This paper describes an objective speech quality assessment method developed for the Hungarian NMT-450 mobile telephone system. The method is based on a psychoacoustic front end followed by a cognitive modeling component. Special problems of the NMT system, such as hand-overs, the effects of automatic gain control (AGC) and intrusion of signaling noise are addressed in the cognitive module. Correlation of the subjective and objective quality measures is maximized by finding a transformation that linearizes their relationship. A correlation of 0.94 is achieved on an independent test set between the subjective speech quality and the proposed objective quality measure.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-563"
  },
  "saruwatari99_eurospeech": {
   "authors": [
    [
     "Hiroshi",
     "Saruwatari"
    ],
    [
     "Shoji",
     "Kajita"
    ],
    [
     "Kazuya",
     "Takeda"
    ],
    [
     "Fumitada",
     "Itakura"
    ]
   ],
   "title": "Speech enhancement using nonlinear microphone array under nonstationary noise conditions",
   "original": "e99_2567",
   "page_count": 4,
   "order": 567,
   "p1": "2567",
   "pn": "2570",
   "abstract": [
    "This paper describes a spatial spectral subtraction method by using the complementary beamforming microphone array to enhance noisy speech signals for speech recognition. The complementary beamforming is based on two types of beamformers de-signed to obtain complementary directivity patterns with respect to each other. In this paper, it is shown that the nonlinear subtraction processing with complementary beamforming can result in a kind the spectral subtraction without the need for speech pause detection. To evaluate the effectiveness, speech enhancement exper-iments and speech recognition experiments are performed based on computer simulations under both stationary and nonstationary noise conditions. In comparison with the optimized conventional delay-and-sum array, it is shown that: (1) the proposed array performs more than 20% better in word recognition rates under the conditions that the white Gaussian noise is used, (2) the proposed array improves the word recognition rate by about 5% when the interfering noise is a single speaker or the overlap of some speakers, (3) the proposed array improves the word recognition rate by more than 10% when the noise is a nonstationary bubble noise.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-564"
  },
  "sarikaya99_eurospeech": {
   "authors": [
    [
     "Ruhi",
     "Sarikaya"
    ],
    [
     "John H. L.",
     "Hansen"
    ]
   ],
   "title": "Auditory masking threshold estimation for broadband noise sources with application to speech enhancement",
   "original": "e99_2571",
   "page_count": 4,
   "order": 568,
   "p1": "2571",
   "pn": "2574",
   "abstract": [
    "This paper addresses issues encountered in the use of an Auditory Masking Threshold (AMT) for speech enhancement and proposes an algorithm to improve AMT estimation for broadband noise sources. We determined that while AMT estimation is fairly accurate, and hence an enhancement scheme based on AMT can suppress audible noise to a greater extent for low frequency colored noise sources, the algorithm fails to converge to the clean speech AMT for broadband communication channel noise. We propose a new AMT estimation scheme and incorporate the proposed algorithm into a previously developed enhancement framework [2].We evaluate our algorithm on a set of sentences obtained from the standard TIMIT database for at communications channel noise (FLN), and automobile highway noise (HWY) at 5 dB and 0 dB SNR levels, respectively. Evaluations were performed for 8 kHz and 16 kHz sampled speech and performance is measured with both objective and subjective assessment methods. The results show that the new AMT codebook based enhancement method is more effective than traditional AMT methods. Also, that traditional AMT methods may not be as effective for reduced bandwidth speech (4 kHz), or broadband interference, but that alternative AMT estimation methods can help improve convergence properties.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-565"
  },
  "unoki99_eurospeech": {
   "authors": [
    [
     "Masashi",
     "Unoki"
    ],
    [
     "Masato",
     "Akagi"
    ]
   ],
   "title": "Segregation of vowel in background noise using the model of segregating two acoustic sources based on auditory scene analysis",
   "original": "e99_2575",
   "page_count": 4,
   "order": 569,
   "p1": "2575",
   "pn": "2578",
   "abstract": [
    "This paper proposes an auditory sound segregation model based on auditory scene analysis. It solves the problem of segregating two acoustic sources by using constraints related to the heuristic regularities proposed by Bregman and by making an improvement to our previously proposed model. The improvement is to reconsider constraints on the continuity of instantaneous phases as well as constraints on the continuity of instantaneous amplitudes and fundamental frequencies in order to segregate the desired signal from a noisy signal precisely even in waveforms. Simulations performed to segregate a real vowel from a noisy vowel and to compare the results of using all or only some constraints showed that our improved model can segregate real speech precisely even in waveforms using all the constraints related to the four regularities, and that the absence of some constraints reduces the segregation accuracy.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-566"
  },
  "veaux99_eurospeech": {
   "authors": [
    [
     "Christophe",
     "Veaux"
    ],
    [
     "Pascal",
     "Scalart"
    ],
    [
     "André",
     "Gilloire"
    ]
   ],
   "title": "Analysis and on-line detection of audible distortions in GSM telephony",
   "original": "e99_2579",
   "page_count": 4,
   "order": 570,
   "p1": "2579",
   "pn": "2582",
   "abstract": [
    "Channel errors significantly impair the quality of GSM transmitted speech. In this contribution, we first analyze the speech distortions caused by error control failures or due to the basic frame substitution technique used by the GSM Full Rate. In particular, we show how this frame substitution procedure may introduce frame rate harmonics in the speech spectrum. Then we present algorithms that perform on the decoded speech in order to detect and conceal these distortions. The frame rate harmonics are detected by comparing a 50 Hz periodicity measure to a pitch estimate. The distortions due to Cyclic Redundancy Check (CRC) failures are detected by exploiting time and mutual correlation of speech parameters. These algorithms are optimized in order to reduce false alarms.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-567"
  },
  "ru99_eurospeech": {
   "authors": [
    [
     "Wen Rong",
     "Ru"
    ],
    [
     "Shih-Chen",
     "Lin"
    ],
    [
     "Po-Cheng",
     "Chen"
    ],
    [
     "Chun-Hung",
     "Kuo"
    ]
   ],
   "title": "A parameter-based 2-talker detection apparatus for echo cancellation",
   "original": "e99_2583",
   "page_count": 4,
   "order": 571,
   "p1": "2583",
   "pn": "2586",
   "abstract": [
    "A new robust double talk detector (DTD) for acoustic echo cancellation is proposed. This detector uses the square norm of the adaptive filter weight vector as the detection criterion. A fast algorithm for the norm computation is also developed. Simulations showthat the proposed DTD method can significanlty outperform conventional algorithms in noisy environments.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-568"
  },
  "yen99_eurospeech": {
   "authors": [
    [
     "Kuan-Chieh",
     "Yen"
    ],
    [
     "Jun",
     "Huang"
    ],
    [
     "Yunxin",
     "Zhao"
    ]
   ],
   "title": "Co-channel speech separation in the presence of correlated and uncorrelated noises",
   "original": "e99_2587",
   "page_count": 4,
   "order": 572,
   "p1": "2587",
   "pn": "2590",
   "abstract": [
    "The noise robustness of an adaptive decorrelation filtering (ADF)-based co-channel speech separation system is addressed. While ADF algorithm has been shown as an effective method in speech-alone scenarios, its performance deteriorates in the presence of background noise. In this work, it is shown that the performance of the ADF-based system deteriorates when the speech-to-noise ratio (SNR) worsens, especially when the the noise components in the acquired signals are highly correlated. To reduce the negative impact of background noise, an energy-constrained signal subspace (ECSS) speech enhancement method was integrated as a processing front-end. Experimental results show that with the integration of ECSS method, the separation performance of the ADF-based system against background noise of various spectral and spatial characteristics was improved in most SNR conditions. The improvement was especially significant when the SNR was low and the noises were highly correlated.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-569"
  },
  "burshtein99_eurospeech": {
   "authors": [
    [
     "David",
     "Burshtein"
    ],
    [
     "Sharon",
     "Gannot"
    ]
   ],
   "title": "Speech enhancement using a mixture-maximum model",
   "original": "e99_2591",
   "page_count": 4,
   "order": 573,
   "p1": "2591",
   "pn": "2594",
   "abstract": [
    "We present a new spectral domain, speech enhancement algorithm. The new algorithm is based on a mixture model for the short time spectrum of the clean speech signal, and on a maximum assumption in the production of the noisy speech spectrum. The new algorithm is shown to be effective for improving the quality of speech signals corrupted by additive noise. The computational requirements of the algorithm can be significantly reduced, essentially without paying performance penalties, by incorporating a dual codebook scheme with tied variances. Experiments, using recorded speech signals and actual noise sources, show that in spite of its low computational requirements, the algorithm shows improved performance compared to alternative speech enhancement algorithms.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-570"
  },
  "gonzalezrodriguez99_eurospeech": {
   "authors": [
    [
     "Joaquin",
     "Gonzalez-Rodriguez"
    ],
    [
     "Santiago",
     "Cruz-Llanas"
    ],
    [
     "Javier",
     "Ortega-Garcia"
    ]
   ],
   "title": "Concurrent speakers separation through binaural processing of stereo recordings",
   "original": "e99_2595",
   "page_count": 4,
   "order": 574,
   "p1": "2595",
   "pn": "2598",
   "abstract": [
    "In this contribution, we address the problem of speech separation from concurrent speakers through the use of a binaural processor. In this system, we take advantage of both ITDs (Inter-aural Time Differences) and ILDs (Inter-aural Level Difs.) present in the binaural input signals to perform spatial localisation and discrimination. However, in applications as Forensic Acoustics where hidden recordings are desired we cannot make use of head simulators, but we can also obtain ITDs and ILDs through stereo recordings with a cardioid microphone pair. In this way, after a new training of the processor, we are able to extract the information arriving from the desired direction from a simple stereo recording. Several experiments with static and moving speakers have been performed, and some audio results on concurrent speakers separation are also included.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-571"
  },
  "gustafsson99_eurospeech": {
   "authors": [
    [
     "Harald",
     "Gustafsson"
    ],
    [
     "Sven",
     "Nordholm"
    ],
    [
     "Ingvar",
     "Claesson"
    ]
   ],
   "title": "Spectral subtraction with adaptive averaging of the gain function",
   "original": "e99_2599",
   "page_count": 4,
   "order": 575,
   "p1": "2599",
   "pn": "2602",
   "abstract": [
    "The handsfree mode is convenient when using a mobile phone in a vehicle. This mode brings the microphone away from the talkers mouth. As a consequence the speech signal picked up has low Signal to Noise Ratio. Thus, to improve the SNR a noise reduction method is employed. The proposed method is an improvement of the well-known spectral subtraction method which uses one SNR-depen-dent gain function to suppress the noise. The proposed method reduces the residual noise artifacts by using an adaptive averaging of the gain function. The adaptation is controlled by a spectral discrepancy measure, which is obtained by comparing the averaged noise spectrum and the current input signal spectrum. Experiments show a noise reduction of 10 dB with good noise quality and sig-nificantly low residual noise distortion. Sound results are presented athttp://www.its.hk-r.se/research.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-572"
  },
  "gaillard99_eurospeech": {
   "authors": [
    [
     "François",
     "Gaillard"
    ],
    [
     "Frédéric",
     "Berthommier"
    ],
    [
     "Gang",
     "Feng"
    ],
    [
     "Jean-Luc",
     "Schwartz"
    ]
   ],
   "title": "A reliability criterion for time-frequency labeling based on periodicity in an auditory scene",
   "original": "e99_2603",
   "page_count": 4,
   "order": 576,
   "p1": "2603",
   "pn": "2606",
   "abstract": [
    "Computational Auditory Scene Analysis (CASA) aims to model our ability to structure our acoustical environment. In a CASA context, this paper deals with a method for time-frequency labeling based on harmonic properties. The method is based on a classical pitch extraction algorithm, termed the « zero-crossing method », which is known to be particularly sensitive to any kind of interference. This work shows that its sensitivity can be turned into an advantage for harmonicity detection in interfering conditions, and provides, according to two estimators which are precisely characterized, a time-frequency representation labeled according to a reliability criterion. A model for speech segregation is subsequently designed and evaluated in different interference paradigms.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-573"
  },
  "koval99_eurospeech": {
   "authors": [
    [
     "Serguei",
     "Koval"
    ],
    [
     "Mikhail",
     "Stolbov"
    ],
    [
     "Mikhail",
     "Khitrov"
    ]
   ],
   "title": "Broadband noise cancellation systems: new approach to working performance optimization",
   "original": "e99_2607",
   "page_count": 4,
   "order": 577,
   "p1": "2607",
   "pn": "2610",
   "abstract": [
    "The report deals with speech enhancement in a noisy environment. A new, direct shaping parametric formulation of generalised adaptive spectral subtraction algorithm for non-stationary interference is considered. The idea of the authors is not to select but to form desirable envelope of spectral estimator function in SNR*Gain-function area. Some principles of parameters choice are proposed, based not only on aural estimations of signal quality but also on graphical analysis of target test signals. The field examinations showed good working performance of the proposed approach for noisy speech signal enhancement. The accuracy of various speech recognisers with the described pre-processing was significantly improved for noisy input speech. The paper develops approach originally reported in [5].\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-574"
  },
  "linhard99_eurospeech": {
   "authors": [
    [
     "Klaus",
     "Linhard"
    ],
    [
     "Tim",
     "Haulick"
    ]
   ],
   "title": "Noise subtraction with parametric recursive gain curves",
   "original": "e99_2611",
   "page_count": 4,
   "order": 578,
   "p1": "2611",
   "pn": "2614",
   "abstract": [
    "A well known technique for speech enhancementis spectral subtraction. This technique is attractivedue to its simplicity and its low computational cost.We have proposed a simple modification of the filtercoeficient calculation in form of a recursion of theprevious filter coeficient. Because of this recursionthe subtraction rule (gain curve) consists of twodifferent paths, one for increasing signal-to-noise-ratio and another for the decreasing ratio. In thispaper we will introduce a parametric version of thisrecursive approach with two parameters controllingthe crossover behaviour of the two paths. Althoughthis recursive version is a very simple algorithm itis easily adjusted to produce no musical noise andadditionally to get low speech distortion and/or lowresidual noise.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-575"
  },
  "masgrau99_eurospeech": {
   "authors": [
    [
     "Enrique",
     "Masgrau"
    ],
    [
     "Luis",
     "Aguilar"
    ],
    [
     "Eduardo",
     "Lleida"
    ]
   ],
   "title": "Performance comparison of several adaptive schemes for microphone array beamforming",
   "original": "e99_2615",
   "page_count": 4,
   "order": 579,
   "p1": "2615",
   "pn": "2618",
   "abstract": [
    "The use of microphone array can be a valuable resort for robust speech acquisition. This paper describes a comparative study of the performance achieved by three different schemes of adaptive beamforming with microphone arrays: linear constrained beamforming (Frost [1]), AMNOR system [2] and Affes system [3]. In general, these adaptive schemes are suitable for speech enhancement systems in noisy and reverberant environments, and in the presence of other interfering speech sources (multi-talkers environments). This analysis also includes the classical delay and sum (DS) scheme as a reference for comparative purposes. In order to achieve a homogeneous comparison of the performance, several experimental environments was simulated to evaluate the different schemes, consisting of a desired speech source, an interfering speech or white noise source and the presence of reverberation and omnidirectional noise. This analysis includes simulations of uniform linear arrays, harmonically nested arrays, as well as planar configurations (circular and L-shaped arrays). The Affes system shows be the most suitable system in presence of directional interferences, reverberation and omni-directional noise.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-576"
  },
  "mizumachi99_eurospeech": {
   "authors": [
    [
     "Mitsunori",
     "Mizumachi"
    ],
    [
     "Masato",
     "Akagi"
    ]
   ],
   "title": "An objective distortion estimator for hearing aids and its application to noise reduction",
   "original": "e99_2619",
   "page_count": 4,
   "order": 580,
   "p1": "2619",
   "pn": "2622",
   "abstract": [
    "In this paper, an objective distortion estimator called auditory-oriented spectral distortion(ASD) is proposed. It is confirmed that the ASD can accurately predict the auditory perceptual distortions represented by the mean opinion score(MOS). The ASD is used as a criterion to optimize the noise reduction algorithm for instruments that need to reduce noises appealing to the ear, for example hearing aids. Experimental results say that a suitable value of the parameter should be different for the purpose to use as between improving the auditory impressions and a front-end of speech recognition systems.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-577"
  },
  "nemer99b_eurospeech": {
   "authors": [
    [
     "Elias",
     "Nemer"
    ],
    [
     "Rafik",
     "Goubran"
    ],
    [
     "Samy",
     "Mahmoud"
    ]
   ],
   "title": "Speech enhancement using fourth-order cumulants and time-domain optimal filters",
   "original": "e99_2623",
   "page_count": 4,
   "order": 581,
   "p1": "2623",
   "pn": "2626",
   "abstract": [
    "A new method for speech enhancement based on optimal filtering, subbands, and higher-order cumulants is proposed in this paper. The key idea is to use the 4th cumulant to estimate the parameters required for the enhancement fil-ters: It is shown that the kurtosis of noisy speech may be used to estimate the SNR and the probability of speech presence when speech is divided in narrow bands and mod-eled as a sinusoidal signal. The resulting algorithm is tested in typical mobile noise conditions and proves effective under such types as street, office and fan noises. Compared to the TIA IS-127 standard for noise reduction, the proposed algorithm is better at preserving the harmonic structure of the speech and results in overall more noise reduction in Gaussian-like conditions. However, this comes at the cost of slightly more noise artifact, mostly at very low SNR and non-Gaussian conditions.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-578"
  },
  "renevey99_eurospeech": {
   "authors": [
    [
     "Philippe",
     "Renevey"
    ],
    [
     "Andrzej",
     "Drygajlo"
    ]
   ],
   "title": "Missing feature theory and probabilistic estimation of clean speech components for robust speech recognition",
   "original": "e99_2627",
   "page_count": 4,
   "order": 582,
   "p1": "2627",
   "pn": "2630",
   "abstract": [
    "In the framework of Hidden Markov Models (HMMs), this paper presents a new approach towards robust speech recognition in adverse conditions. The approach is based on statistical modeling of noise by Gaussian distributions and an estimation of idealized clean speech directly in the probabilistic domain using a statistical spectral subtraction method and missing feature compensation. The missing feature approach in the probabilistic domain allows the speech features masked by noise to be dynamically detected and estimated in probability calculations performed in HMM based recognizers. The method com-bines the advantages of two techniques: the first based on the statistical compensation similar to the parallel model combination and the second one issued from the missing feature theory.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-579"
  },
  "salavedra99_eurospeech": {
   "authors": [
    [
     "Josep M.",
     "Salavedra"
    ],
    [
     "Xavier",
     "Bou"
    ]
   ],
   "title": "Distortion effects of several cumulant-based wiener filtering algorithms",
   "original": "e99_2631",
   "page_count": 5,
   "order": 583,
   "p1": "2631",
   "pn": "2634",
   "abstract": [
    "Some Single-Microphone Speech Enhancement algorithms based on the iterative Wiener filtering Method due to Lim-Oppenheim [2] are evaluated. In the original Lim-Oppenheim algorithm, AR spectral estimation of speech is carried out using a second-order analysis, but our algorithms consider an AR estimation from a cumulant analysis. This work extends some preceding papers due to the authors [4], [5]. Third- and fourth-order cumulant-based algorithms are compared to classical second-order one. This comparison is evaluated by considering three different noisy environments. A detailed study based on a frame-by-frame analysis leads to an optimum iteration of each algorithm as a trade-off between noise reduction and distortion effects. Voiced and unvoiced sounds are separately discussed. We conclude that third-order cumulant-based algorithm offers a more valuable performance than the others.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-580"
  },
  "svoboda99_eurospeech": {
   "authors": [
    [
     "Milan",
     "Svoboda"
    ],
    [
     "Pavel",
     "Sovka"
    ],
    [
     "Petr",
     "Pollák"
    ]
   ],
   "title": "Combined noise suppression system for monaural cochlear implants",
   "original": "e99_2635",
   "page_count": 4,
   "order": 584,
   "p1": "2635",
   "pn": "2638",
   "abstract": [
    "This contribution is devoted to the noise suppression system for monoaural SPEAK-type cochlear implants. The system is based on the combination of the superdirective two-microphone array and the noise suppression post-processing, both implemented in the frequency domain. The relative insensitivity of the two-microphone superdirective system to the spectral magnitude differences in both channels enables the two-channel noise suppression post-processing. The new criterion originating from the SPEAK coding strategy is used for the evaluation of the whole system.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-581"
  },
  "wijngaarden99_eurospeech": {
   "authors": [
    [
     "Sander J. van",
     "Wijngaarden"
    ],
    [
     "Herman J. M.",
     "Steeneken"
    ]
   ],
   "title": "Objective prediction of speech intelligibility at high ambient noise levels using the speech transmission index",
   "original": "e99_2639",
   "page_count": 4,
   "order": 585,
   "p1": "2639",
   "pn": "2642",
   "abstract": [
    "In many cases the intelligibility of speech in noise may be assumed independent of the absolute sound level; the speech-to-noise ratio (SNR) primarily determines intelligibility. However, at high sound levels, speech intelligibility is found to decrease. Subjective Speech Reception Threshold (SRT) measurements were performed at various speech and noise levels, and with various noise spectra. Decreases in intelligibility between noise levels of 75 and 105 dB(A) were found that correspond to 1 to 3 dB difference in SNR, depending on the noise spectrum. This decrease is not predicted by the standardized Speech Transmission Index (STI), which may be calculated from speech and noise spectra. By introducing level-dependent auditory masking in the STI-calculations, a decrease in intelligibility is predicted that corresponds well to the SRT results.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-582"
  },
  "wan99b_eurospeech": {
   "authors": [
    [
     "Eric A.",
     "Wan"
    ],
    [
     "Rudolph van der",
     "Merwe"
    ]
   ],
   "title": "Noise-regularized adaptive filtering for speech enhancement",
   "original": "e99_2643",
   "page_count": 4,
   "order": 586,
   "p1": "2643",
   "pn": "2646",
   "abstract": [
    "The removal of noise from speech signals has applications ranging from speech enhancement for cellular communications to front ends for speech recognition systems. In this paper, we present a new nonlinear time-domain method called NoiseRegularized Adaptive Filtering.The approach is based on minimum mean-squared estimation using a modified cost function and allows designing both linear and nonlinear filters using only the observed noisy speech.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-583"
  },
  "zarubin99_eurospeech": {
   "authors": [
    [
     "F.",
     "Zarubin"
    ],
    [
     "A.",
     "Kovtonyuk"
    ],
    [
     "K.",
     "Zadiraka"
    ]
   ],
   "title": "Speech enhancement using karhunen-loeve transformation and wiener filtering in critical bands",
   "original": "e99_2647",
   "page_count": 4,
   "order": 587,
   "p1": "2647",
   "pn": "2650",
   "abstract": [
    "This paper deals with method of speech enhancement based on Karhunen-Loeve (K-L) and Wiener critical band spectral subtraction algorithm. The K-L algorithm is used twice to, first, track a spectral envelope of the contaminated signal and subtract sub-words - spectrally- homogeneous segments, and ,secondly, to construct an estimate of the spectral envelope of extracted sub-words using Wieners filter.\n",
    "The paper is organized as follows: In section 2, we introduce the sub-word segmentation algorithm based on decomposition of covariance matrix in K-L transformation. Then in section 3, we describe our approach to speech parametr estimation in noise based on K-L transformation critical band optimal Wiener filter. In section 4, we compare our technique with the existing optimal K-L technique in adverse conditions, using both subjective and objective speech assessment measures. Result indicate that the proposed method provides better speech enhancement than the previous proposed algorithm.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-584"
  },
  "brondsted99_eurospeech": {
   "authors": [
    [
     "Tom",
     "Brondsted"
    ]
   ],
   "title": "The CPK NLP suite for spoken language understanding",
   "original": "e99_2651",
   "page_count": 4,
   "order": 588,
   "p1": "2651",
   "pn": "2654",
   "abstract": [
    "This paper describes a number of freely available tools for implementing and running spoken language understanding systems. Unlike other free tools (e.g. the CSLU toolkit), the main emphasis is on spoken language understanding (syntactic/semantic parsing, generation of language models for recognition etc.). The suite supports (reads and/or writes) a number of grammar formats defined for speech recognisers like Entropics GrapHvite (HTK standard lattice) as well as common unification grammar formats used in NLP: The American PATRII and the European EUROTRA (Augmented Phrase Structure Grammar) format. The open architecture is completed by a general API allowing simple interfacing to dialogue management and speech recognition. The suite, implemented in C++ (partly C, Lex, Yacc), can compile and run on any machine under any OS having a 32 bit C++ compiler, Flex/Bison or Lex/Yacc. The suite can be downloaded athttp://www.cpk.auc.dk/~tb/nlp suite.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-585"
  },
  "chung99_eurospeech": {
   "authors": [
    [
     "Grace",
     "Chung"
    ],
    [
     "Stephanie",
     "Seneff"
    ],
    [
     "Lee",
     "Hetherington"
    ]
   ],
   "title": "Towards multi-domain speech understanding using a two-stage recognizer",
   "original": "e99_2655",
   "page_count": 4,
   "order": 589,
   "p1": "2655",
   "pn": "2658",
   "abstract": [
    "This paper describes our efforts in designing a twostage recognizer with the objective of developing a multidomain speech understanding system. We envisage one first-stage recognition engine that is domain-independent, and multiple second-stage systems specializing in individual domains. A major novelty in our initial two-stage design is a front-end that incorporates angie -based hierarchical sublexical probability models encapsulated within affinite-state transducer (FST) paradigm. This first stage is a context-dependent syllable-level recognizer which outputs acoustic-phonetic networks to be processed in a second pass. The second stage incorporates higher order linguistic knowledge, from phonological to syntactic and semantic, in a tightly coupled search. This system has yielded up to a 28.5% reduction in understanding error, compared with a single stage context-dependent recognizer which does not use angie -based probabilities.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-586"
  },
  "ipsic99_eurospeech": {
   "authors": [
    [
     "I.",
     "Ipsic"
    ],
    [
     "F.",
     "Mihelic"
    ],
    [
     "S.",
     "Dobrisek"
    ],
    [
     "Jerneja",
     "Gros"
    ],
    [
     "N.",
     "Pavesic"
    ]
   ],
   "title": "A slovenian spoken dialog system for air flight inquiries",
   "original": "e99_2659",
   "page_count": 4,
   "order": 590,
   "p1": "2659",
   "pn": "2662",
   "abstract": [
    "In this paper we give an overview of a Slovenian spoken dialog system, developed within a joint project in multilingual speech recognition and understanding. The aim of the project is the development of an information retrieval system, capable of having a dialog with a user. The paper presents the work on the development of the Slovenian spoken dialog system. Such a system has to be able to handle spontaneous speech, and to provide the user with correct infor-mation. The information system being developed for Slovenian speech is used for air ight information retrieval. The system has to answer questions about air flight connections and their time and date.In the paper we present the developed modules of theSlovenian system and show some results with respectto word accuracy, semantic accuracy and dialog success rate.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-587"
  },
  "ramaswamy99_eurospeech": {
   "authors": [
    [
     "Ganesh",
     "Ramaswamy"
    ],
    [
     "Jan",
     "Kleindienst"
    ],
    [
     "Daniel",
     "Coffman"
    ],
    [
     "Ponani",
     "Gopalakrishnan"
    ],
    [
     "Chalapathy",
     "Neti"
    ]
   ],
   "title": "A pervasive conversational interface for information interaction",
   "original": "e99_2663",
   "page_count": 4,
   "order": 591,
   "p1": "2663",
   "pn": "2666",
   "abstract": [
    "In this paper, we describe a new pervasive conversational sys-tem that provides access to multiple desktop applications, from multiple client devices, using multiple input modalities. Client devices currently supported include desktop and telephone, and the applications incorporated include email, calendarand address book. When the access is from a desktop, both conversational natural language and graphical inputs are supported. The paper describes the overall architecture to support such a pervasive conversational system, along with innovations in con-tinuous speech recognition, statistical natural language under-standing, and dialog management that were developed to build the system. The paper also describes the partially unsupervised Wizard-of-Oz style setup to collect real-use data, and the performance of the statistical models constructed using this data for speech recognition and natural language understanding.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-588"
  },
  "vromans99_eurospeech": {
   "authors": [
    [
     "B.",
     "Vromans"
    ],
    [
     "R.J. van",
     "Vark"
    ],
    [
     "B.",
     "Rueber"
    ],
    [
     "A.",
     "Kellner"
    ]
   ],
   "title": "Extending the SUSI system with negative knowledge",
   "original": "e99_2667",
   "page_count": 4,
   "order": 592,
   "p1": "2667",
   "pn": "2670",
   "abstract": [
    "The Philips SUSI system is a set of modules for the construction of automatic inquiry systems such as the train timetable information system TABA and the Philips Automatic Directory Information System (PADIS). These systems conduct a dialogue in continuous spontaneous speech with the user, giving access to information in a database. To improve the system behaviour we have extended the knowledge representation and update functions of the SUSI system so that they collect and use the negative knowledge during the dialogue. Negative knowledge in this context is the information that results from denials by the user in the dialogue, such as \"No\" and \"Not Mr. Smiths\". The negative knowledge handling has been implemented and tested for the SUSI based application PADIS. For PADIS, we found that certain dialogues improve significantly because of the use of negative knowledge.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-589"
  },
  "crouzet99_eurospeech": {
   "authors": [
    [
     "Olivier",
     "Crouzet"
    ],
    [
     "Nicole",
     "Bacri"
    ]
   ],
   "title": "Phonological constraints in speech segmentation processes: investigating levels of implementation.",
   "original": "e99_2671",
   "page_count": 4,
   "order": 593,
   "p1": "2671",
   "pn": "2674",
   "abstract": [
    "Recent data have been considered as evidence for a role of phonological constraints in speech segmentation processes. A distributional analysis of consonant sequences in the French lexicon shows that these results may be accounted for by lexical competition or by transitional probability models in which no claim is made about the usefulness of phonology in speech perception. Two word-spotting experiments were devised in order to investigate this issue. In the first experiment, only half of the participants showed the previous effect. In a modified replication of this study, the expected effect emerged clearly with the same target stimuli. While the role of phonotactics in speech segmentation seems to be independent from lexical competition and transitional probabilities, the likely role of strategical effects is discussed.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-590"
  },
  "damper99_eurospeech": {
   "authors": [
    [
     "Robert I.",
     "Damper"
    ],
    [
     "Steve R.",
     "Gunn"
    ]
   ],
   "title": "Learning phonetic distinctions from speech signals",
   "original": "e99_2675",
   "page_count": 4,
   "order": 594,
   "p1": "2675",
   "pn": "2678",
   "abstract": [
    "Previous work has shown that connectionist learning systems can simulate important aspects of the categorization of speech sounds by human and animal listeners. Training is on repre-sentations of synthetic, exemplar voiced and unvoiced stop consonants passed through a computational model of the auditory periphery. In this work, we use the modern inductive inference technique of support vector machines (SVMs) as the learning system. Visualization of the SVMs weight vector reveals what has been learned about the voiced/unvoiced distinction.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-591"
  },
  "moreton99_eurospeech": {
   "authors": [
    [
     "Elliott",
     "Moreton"
    ],
    [
     "Shigeaki",
     "Amano"
    ]
   ],
   "title": "Phonotactics in the perception of Japanese vowel length: evidence for long-distance dependencies",
   "original": "e99_2679",
   "page_count": 4,
   "order": 595,
   "p1": "2679",
   "pn": "2682",
   "abstract": [
    "These experiments investigate whether the perceptual boundary between [a] and [a:] is affected by the differing phonotactics of the Sino-Japanese and Foreign strata of the Japanese lexicon. We presented a range of edited natural vowels from [a] to [a:] at the end of carrier nonwords of the form [CoC'__] and asked subjects to judge whether the final vowel was long or short, while C and C' were varied to make 9 carriers ranging from very Sino-Japanese-like to very Foreign-like. The perceptual boundary between [a] and [a:] was affected by both C and C', shifting through about 20 mseca larger and more robust effect than was obtained in a word-superiority experiment with the same paradigm and subjects. These results cast doubt on explanations of phonotactic effects based on lexical-activation spreading. The influence of C at a distance of three phonemes from the ambiguous segment cannot be explained by simple segment-to-segment transitional probabilities.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-592"
  },
  "peperkamp99_eurospeech": {
   "authors": [
    [
     "Sharon",
     "Peperkamp"
    ],
    [
     "Emmanuel",
     "Dupoux"
    ],
    [
     "Núria",
     "Sebastián-Gallés"
    ]
   ],
   "title": "Perception of stress by French, Spanish, and bilingual subjects",
   "original": "e99_2683",
   "page_count": 4,
   "order": 596,
   "p1": "2683",
   "pn": "2686",
   "abstract": [
    "Previous research has shown that French subjects, as opposed to Spanish subjects, have difficulties in distinguishing two words that differ only as far as the location of stress is concerned. In French, stress is not contrastive, and French subjects are deaf to stress contrasts. In Experiment 1, we replicate this finding with a new and more powerful paradigm for assessing the perception of stress. With this new method, we obtain a complete separation of the two subject populations. In Experiment 2, we test highly proficient French-Spanish bilinguals with the same paradigm. Our findings are that the performance of individual bilinguals is either French-like or Spanish-like. The factor that best predicts the bilinguals performance is the country in which the subject is born. Consequences for models of bilingualism are discussed.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-593"
  },
  "silipo99_eurospeech": {
   "authors": [
    [
     "Rosaria",
     "Silipo"
    ],
    [
     "Steven",
     "Greenberg"
    ],
    [
     "Takayuki",
     "Arai"
    ]
   ],
   "title": "Temporal constraints on speech intelligibility as deduced from exceedingly sparse spectral representations",
   "original": "e99_2687",
   "page_count": 4,
   "order": 597,
   "p1": "2687",
   "pn": "2690",
   "abstract": [
    "A novel means of quantifying the contribution of specific spectral bands for intelligibility is described. The spectrum of spoken English sentences is partitioned into one-third octave bands (\"slits\") and the contribution of each of four slits ascertained independently and in combination with other slits distributed across the spectrum. The intelligibility baseline (four concurrent slits) yields ca. 85% intelligibility. The current study demonstrates that intelligibility progressively declines as the two central slits (2+3) are desynchronized between 25 and 250 ms. Beyond 250 ms, intelligibility often declines even further but then begins to increase for greater degrees of asynchrony, suggesting the presence of a perceptual processing buffer of ca. 200-300 ms in duration. The utility of the spectral slit technique is also demonstrated for estimating the contribution towards intelligibility of different regions of the modulation spectrum. The mid-frequency (10-25 Hz) modulations are shown to be of particular significance for encoding speech information above 1.5 kHz. These two experiments demonstrate the power and utility of using circumscribed portions of the spectrum for quantitative evaluation of the contribution made by specific spectro-temporal properties of the speech signal.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-594"
  },
  "chang99c_eurospeech": {
   "authors": [
    [
     "Sen-Chia",
     "Chang"
    ],
    [
     "Shih-Chieh",
     "Chien"
    ],
    [
     "Woei-Chyang",
     "Shieh"
    ]
   ],
   "title": "Mandarin telephone speech recognition using MCE/GPD-based speaker cluster HMM",
   "original": "e99_2709",
   "page_count": 4,
   "order": 598,
   "p1": "2709",
   "pn": "2712",
   "abstract": [
    "In this paper we successfully apply the MCE/GPD method to train speaker cluster HMM. The essential concept of our approach is to adjust all the parameters of the speaker cluster HMM simultaneously using each utterance of the whole training set. In other words, the parameters of each cluster-dependent HMM are no longer independently estimated by using only the training data of the speakers who belong to its corresponding cluster. To achieve this purpose, the discriminant function used in the MCE/GPD method need to be defined by the parameter set of the entire speaker cluster HMM. In our implementation, we define it as a function of the log-likelihood scores given the cluster-dependent HMMs. The proposed discriminative training procedure would increase the cluster separability and then improve the recognition rate.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-595"
  },
  "fonollosa99_eurospeech": {
   "authors": [
    [
     "A. R.",
     "Fonollosa"
    ],
    [
     "Eloi",
     "Batlle"
    ]
   ],
   "title": "Combining length restrictions and n-best techniques in multiple-pass search strategies",
   "original": "e99_2713",
   "page_count": 4,
   "order": 599,
   "p1": "2713",
   "pn": "2716",
   "abstract": [
    "Multiple-pass search strategies are a necessary choice to reduce the computational cost and memory requirements of large vocabulary speech recognition systems uisng different kinds or complex modelling of speech and language. In the N-best approach the most efficient knowledge sources (usually acoustic models and a bi/trigram language model) are used first to select a short list (N-best) of alternative hypotheses. Then, the remaining more complex knowledge sources are used to rescore the sequences. In this paper we propose a new algorithm to overcome the problems associated with the simple ''Traceback-Based'' N-best algorithm while conserving its simplicity and low computational cost.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-596"
  },
  "gelinhuet99_eurospeech": {
   "authors": [
    [
     "Cecile",
     "Gelin-Huet"
    ],
    [
     "Kenneth",
     "Rose"
    ],
    [
     "Ajit",
     "Rao"
    ]
   ],
   "title": "The deterministic annealing approach for discriminative continuous HMM design",
   "original": "e99_2717",
   "page_count": 4,
   "order": 600,
   "p1": "2717",
   "pn": "2720",
   "abstract": [
    "We propose a deterministic annealing (DA) algorithm to design classifiers based on continuous observation hidden Markov models. The algorithm belongs to the class of minimum classification error (MCE) techniques that are known to outperform maximum likelihood (ML) design. Most MCE methods smooth the piecewise constant classification error cost to facilitate the use of local descent optimization methods, but are susceptible to the numerous shallow local minimum traps that riddle the cost surface. The DA approach employs randomization of the classification rule followed by minimization of the corresponding expected misclassification rate, while controlling the level of randomness via a constraint on the Shannon entropy. The effective cost function is smooth and converges to the MCE cost at the limit of zero entropy. The proposed algorithm significantly outperforms both standard ML and standard MCE design methods on the E-set database.\n",
    "Keywords: Speech recognition, Discriminative training, Deterministic annealing, continuous HMM.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-597"
  },
  "huo99_eurospeech": {
   "authors": [
    [
     "Qiang",
     "Huo"
    ],
    [
     "Bin",
     "Ma"
    ]
   ],
   "title": "On-line adaptive learning of CDHMM parameters based on multiple-stream prior evolution and posterior pooling",
   "original": "e99_2721",
   "page_count": 4,
   "order": 601,
   "p1": "2721",
   "pn": "2724",
   "abstract": [
    "Based on the concept of multiple-stream prior evolution andposterior pooling, we propose a new incremental adaptiveBayesian learning framework for eficient on-line adaptationof the continuous density hidden Markov model (CDHMM)parameters. As a first step, we apply the affine transformations to the mean vectors of CDHMMs to control the evolution of their prior distribution. This new stream of priordistribution can be combined with another stream of priordistribution evolved without any constraints applied. In aseries of comparative experiments on the task of continuousMandarin speech recognition, we show that the new adaptation algorithm achieves a similar fast-adaptation performance as that of incremental MLLR (maximum likelihoodlinear regression) in the case of small amount of adaptation data, while maintains the good asymptotic convergenceproperty as that of our previously proposed quasi-Bayesadaptation algorithms.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-598"
  },
  "kemp99_eurospeech": {
   "authors": [
    [
     "Thomas",
     "Kemp"
    ],
    [
     "Alex",
     "Waibel"
    ]
   ],
   "title": "Unsupervised training of a speech recognizer: recent experiments",
   "original": "e99_2725",
   "page_count": 4,
   "order": 602,
   "p1": "2725",
   "pn": "2728",
   "abstract": [
    "Current speech recognition systems require large amountsof transcribed data for parameter estimation. The transcription, however, is tedious and expensive. In this workwe describe our experiments which are aimed at traininga speech recognizer with only a minimal amount (30 minutes) of transcriptions and a large portion (50 hours) of untranscribed data. A recognizer is bootstrapped on the tran-scribed part of the data and initial transcripts are generatedwith it for the remainder (the untranscribed part). Usinga lattice-based confidence measure, the recognition errorsare (partially) detected and the remainder of the hypotheses is used for training. Using this scheme, the word errorrate on a broadcast news speech recognition task droppedfrom more than 32.0% to 21.4%. In a cheating experimentwe show, that this performance cannot be significantly improved by improving the measure of confidence. By com-bining the unsupervisedly trained system with our currentlybest recognizer which is trained on 15.5 hours of transcribeddata, an additional error reduction of 5% relative (as compared to the system trained in a standard fashion) is possible.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-599"
  },
  "chesta99b_eurospeech": {
   "authors": [
    [
     "C.",
     "Chesta"
    ],
    [
     "Pietro",
     "Laface"
    ],
    [
     "M.",
     "Nigra"
    ]
   ],
   "title": "Piecewise HMM discriminative training",
   "original": "e99_2729",
   "page_count": 4,
   "order": 603,
   "p1": "2729",
   "pn": "2732",
   "abstract": [
    "This paper address the problem of training HMMs using long files of uninterrupted speech with limited and constant memory requirements. The classical training algorithms usually require limited duration training utterances due to memory constraints for storing the generated trellis. Our solution allows to exploits databases that are transcribed, but not partitioned into sentences, using a sliding window Forward-Backward algorithm. This approach has been tested on the connected digits TI/NIST database and on long sequences of Italian digits. Our experimental results show that for a lookahead value L of about 1-2 sec it is possible to achieve reestimation counts that are affected by errors less than 1.e-7, producing similar reestimated models. Another application of our sliding window Forward-Backward algorithm is MMIE training, that we have tested on the TI/NIST database connected digits using as a general model the recognition tree rather than the N-best hypotheses, or the word lattices.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-600"
  },
  "lefevre99_eurospeech": {
   "authors": [
    [
     "Fabrice",
     "Lefévre"
    ],
    [
     "Claude",
     "Montacié"
    ],
    [
     "Marie-José",
     "Caraty"
    ]
   ],
   "title": "A MLE algorithm for the k-NN HMM system",
   "original": "e99_2733",
   "page_count": 4,
   "order": 604,
   "p1": "2733",
   "pn": "2736",
   "abstract": [
    "In this paper, a theoretical framework is proposed for the introduction of the K-NN pdf estimator in an HMM-based speech recognition system. The estimation of the state output probabilities with the K-NN pdf estimator is shown to imply the introduction of a new parameter : the membership coefficient. To learn this coefficient with the Baum-Welch algorithm, a maximum likelihood (ML) reestimation formula is derived. This new formula is tested and compared with the formula we had introduced previously [1]. Then, the edition/condensation techniques are introduced in the context of Markov models in an attempt to improve the appropriateness of the reference data set to the K-NN HMM system. Two new algorithms are proposed for editing and condensing the reference set which present the advantage of being compatible with the K-NN rule.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-601"
  },
  "mcdonough99_eurospeech": {
   "authors": [
    [
     "John",
     "McDonough"
    ],
    [
     "William",
     "Byrne"
    ]
   ],
   "title": "Single-pass adapted training with all-pass transforms",
   "original": "e99_2737",
   "page_count": 4,
   "order": 605,
   "p1": "2737",
   "pn": "2740",
   "abstract": [
    "In recent work, the all-pass transform (APT) was proposed as the basis of a speaker adaptation scheme intended for use with a large vocabulary speech recognition system. It was shown that APT-based adaptation reduces to a linear transformation of cepstral means, much like the better known maximum likelihood linear regression (MLLR), but is specified by far fewer free parameters. Due to its linearity, APT-based adaptation can be used in conjunction with speaker-adapted training (SAT), an algorithm for performing maximum likelihood estimation of the parameters of an HMM when speaker adaptation is to be employed during both training and test. In this work, we propose a refinement of SAT called single-pass adapted training (SPAT) which achieves the same improvement in system performance as SAT but requires much less computation for HMM training. In a set of speech recognition experiments conducted on the Switchboard Corpus, we report a word error rate reduction of 5.3% absolute using a single, global APT.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-602"
  },
  "nogueirasrodriguez99_eurospeech": {
   "authors": [
    [
     "Albino",
     "Nogueiras-Rodríguez"
    ],
    [
     "José B.",
     "Marino"
    ]
   ],
   "title": "Minimum confusibility training of context dependent demiphones",
   "original": "e99_2741",
   "page_count": 4,
   "order": 606,
   "p1": "2741",
   "pn": "2744",
   "abstract": [
    "During the last years two different approaches have been widely used in order to improve the acoustic modeling in continuous speech recognition systems: discriminative training algorithms and context dependent subword units. However, while the use of each of these techniques leads to much better results than standard maximum likelihood trained phone models, their combination, i.e. discriminative training of context dependent units, has revealed to be a much more dificult task. In this paper we deal with minimum confusibility training of demiphones using TIMIT database. By applying this approach recently introduced by the authors, the string error rate in the recognition of TIDIGITS using demiphones is reduced some 24% with respect to maximum likelihood training. This improvement is added to the 8% reduction already provided by demiphones with respect to minimum confusibility trained phones.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-603"
  },
  "rudzionis99_eurospeech": {
   "authors": [
    [
     "A.",
     "Rudzionis"
    ],
    [
     "V.",
     "Rudzionis"
    ]
   ],
   "title": "Phoneme recognition in fixed context using regularized discriminant analysis",
   "original": "e99_2745",
   "page_count": 4,
   "order": 607,
   "p1": "2745",
   "pn": "2748",
   "abstract": [
    "Speaker independent discrimination of four confusable consonants in the strictly fixed context of six vowels is considered. The consonants are depicted by features of consonants stationary part and changing rate of features (delta features) in transition from consonant to the following vowel. The mel frequency cepstrum (MFCC), linear prediction cepstrum (LPCC), recursive filter (F12) features and set of discriminants were evaluated seeking for better phoneme discrimination. It is postulated that Gaussian mixture capabilities are similar to k-means (kMN) capabilities and several discriminants including regularized discriminant analysis (RDA) were analyzed too. The experiments showed that the discrimination error averaged per environments of six vowels decreases from 23.3% using kMN to 7.0% using RDA for the best F12 features. Consonant discrimination error rate decreases from 21.6% to 3.6% in the open vowel context and from 27.9% to 11.4% in closed vowel context.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-604"
  },
  "tran99_eurospeech": {
   "authors": [
    [
     "Dat",
     "Tran"
    ],
    [
     "Michael",
     "Wagner"
    ]
   ],
   "title": "Hidden Markov models using fuzzy estimation",
   "original": "e99_2749",
   "page_count": 4,
   "order": 608,
   "p1": "2749",
   "pn": "2752",
   "abstract": [
    "In the conventional hidden Markov model, the model parameters are reestimated by an iterative procedure known as the Baum-Welch method. This paper proposes an alternative procedure using fuzzy estimation, which is generalised from the fuzzy c-means and the Baum-Welch methods. An extension of this approach, which uses a garbage state to deal with outlier data is also proposed. Experiments using the TI46 speech data corpus show this approach can be applicable to speech and speaker recognition.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-605"
  },
  "vair99_eurospeech": {
   "authors": [
    [
     "Claudio",
     "Vair"
    ],
    [
     "Massimiliano",
     "Mercogliano"
    ],
    [
     "Luciano",
     "Fissore"
    ]
   ],
   "title": "Incremental training of CDHMMs using bayesian learning",
   "original": "e99_2753",
   "page_count": 4,
   "order": 609,
   "p1": "2753",
   "pn": "2756",
   "abstract": [
    "The Bayesian Learning approach (MAP Maximum A Posteriori) can be used for the incremental training of Continuous Density Hidden Markov Models (CDHMM), performed through speech data collected in real applications. The effectiveness of MAP is heavily conditioned by the correct balance between the apriori knowledge and the field training data. In this paper we propose and evaluate several optimization methods of the MAP combination function, based either on maximum likelihood (ML) and heuristics criteria. To adjust the relevance of the apriori knowledge we use the exponential forgetting technique into the MAP framework. We present several tests that compare the error rate reduction as a function of the selected optimization method and of the size of adaptation data.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-606"
  },
  "willett99_eurospeech": {
   "authors": [
    [
     "Daniel",
     "Willett"
    ],
    [
     "Stefan",
     "Müller"
    ],
    [
     "Gerhard",
     "Rigoll"
    ]
   ],
   "title": "A discriminative training procedure based on language model and dictionary for LVCSR",
   "original": "e99_2757",
   "page_count": 4,
   "order": 610,
   "p1": "2757",
   "pn": "2760",
   "abstract": [
    "In today's HMM-based speech recognition systems, the parameters are most commonly estimated according to the Maximum Likelihood criterion. Because of limited training data, however, discriminative objectives provide better parameter estimates with respect to the Maximum A-Posteriori decision used for decoding. The question of which distribution functions to discriminate from which and to what degree is the most crucial when performing discriminative parameter estimation. This is particularly dificult because beside the distribution functions, the recognition procedure is restricted and guided by several other sources of information, such as language model and transition matrices. This paper extends the approach presented in [10] to the case of triphones, refines the theory and estimation of the state-to-state confusion metric and proposes an approximation that allows the application of the approach on context-dependent systems with reasonable computational cost. The evaluation is performed on continuous HMM speech recognition systems for the WSJ0 5k-task. The results prove the practicability of the approach and its extensions.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-607"
  },
  "wu99d_eurospeech": {
   "authors": [
    [
     "Jian",
     "Wu"
    ],
    [
     "Qing",
     "Guo"
    ]
   ],
   "title": "A novel discriminative method for HMM in automatic speech recognition",
   "original": "e99_2761",
   "page_count": 4,
   "order": 611,
   "p1": "2761",
   "pn": "2764",
   "abstract": [
    "A novel discriminative method for estimating the parameters of Hidden Markov Models (HMMs) is described. In this method, the parameter values are chosen to ensure that the characteristics of each sound class can be maximally separated. Compared with the significant method known as the Maximum Mutual Information (MMI) estimation, the novel method represented in this paper adopts a new kind of criteria called MSDI (Maximum Samples Distinction Information). It parries many computational problems in estimating iteration. The experimental results show that the hit rate can be raised by about 6 percent compared with the MLE (Maximum Likelihood Estimation), which is similar to the other experimental results based on MMI training.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-608"
  },
  "avadhanulu99_eurospeech": {
   "authors": [
    [
     "J. V.",
     "Avadhanulu"
    ],
    [
     "M.",
     "Mathew"
    ],
    [
     "T. V.",
     "Sreenivas"
    ]
   ],
   "title": "EARLYZER: perceptualy motivated robust TFR of speech",
   "original": "e99_2765",
   "page_count": 5,
   "order": 612,
   "p1": "2765",
   "pn": "2768",
   "abstract": [
    "Development of robust and efficient front-end is crucial for robust ASR. Proper time and frequency resolution of the TFR of speech, motivated by the auditory models is considered an important factor for robustness. An efficient method of realizing a variable resolution TFR using DTFT/Goertzel algorithm is proposed instead of the standard FFT based approach. It is shown that the new representation, called EarLyzer, is more robust than the FFT based Mel frequency cepstral coefficient representation for an automobile noisy speech recognition task.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-609"
  },
  "aguilera99_eurospeech": {
   "authors": [
    [
     "C. M.",
     "Aguilera"
    ],
    [
     "A.",
     "Navas"
    ],
    [
     "R.",
     "Urquiza"
    ],
    [
     "A.",
     "Gago"
    ]
   ],
   "title": "Frequency lowering using a discrete exponential transform",
   "original": "e99_2769",
   "page_count": 4,
   "order": 613,
   "p1": "2769",
   "pn": "2772",
   "abstract": [
    "Many listeners with medium-severe hearing loss present audiograms with high frequency loss descending profiles. These patients can resolve spectral cues normally for lower frequency signals but they often show less ability to use high frequency information. We have presented in this paper a new Discrete Exponential Transform (DET), its fundamentals, and the correspondingly algorithm designed to compensate this particular feature of the impairment. The preliminary results of the processed speech material in two impaired listeners suggests that DET could be implemented in digital hearing aids to be used in patients with severe and moderate-severe hearing loses.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-610"
  },
  "martino99_eurospeech": {
   "authors": [
    [
     "Joseph Di",
     "Martino"
    ],
    [
     "Yves",
     "Laprie"
    ]
   ],
   "title": "An efficient F0 determination algorithm based on the implicit calculation of the autocorrelation of the temporal excitation signal",
   "original": "e99_2773",
   "page_count": 4,
   "order": 614,
   "p1": "2773",
   "pn": "2776",
   "abstract": [
    "In this paper we are presenting a new algorithm for determining the fundamental frequency. The evaluation of pitch is a very difficult problem mainly because of the great variability and irregularity of the speech signals. The algorithm we are presenting is original so far as it relies on the implicit calculation of the autocorrelation of the temporal excitation signal. We have tested our algorithm on the Bagshaw database, created at the Center for Speech Technology Research at Edinburgh, which is primarily dedicated to the evaluation of algorithms estimating the fundamental frequency of speech. The results of our experiments show that our approach is very reliable.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-611"
  },
  "howitt99_eurospeech": {
   "authors": [
    [
     "Andrew Wilson",
     "Howitt"
    ]
   ],
   "title": "Vowel landmark detection",
   "original": "e99_2777",
   "page_count": 4,
   "order": 615,
   "p1": "2777",
   "pn": "2780",
   "abstract": [
    "Landmark based speech processing is a component of Lexical Access From Features (LAFF), a novel paradigm for feature based speech recognition. Detection and classification of landmarks is a crucial first step in a LAFF system. This work implements a Vowel Landmark Detector using a syllabic segmentation algorithm [Mermelstein 75] and examines the relative utility of its several constraints. The detector is scored against the TIMIT database, using a novel algorithm to convert the segmental transcriptions to a landmark representation for scoring. The results show that substantial improvement in performance can be gained by modifying the frequency range for peak detection. An additional advantage of this modification is that post processing to remove fricative peaks is no longer necessary, which substantially simplifies the algorithm.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-612"
  },
  "kawahara99_eurospeech": {
   "authors": [
    [
     "Hideki",
     "Kawahara"
    ],
    [
     "Haruhiro",
     "Katayose"
    ],
    [
     "Alain de",
     "Cheveigné"
    ],
    [
     "Roy D.",
     "Patterson"
    ]
   ],
   "title": "Fixed point analysis of frequency to instantaneous frequency mapping for accurate estimation of F0 and periodicity",
   "original": "e99_2781",
   "page_count": 4,
   "order": 616,
   "p1": "2781",
   "pn": "2784",
   "abstract": [
    "An accurate fundamental frequency (F0) estimation method for non-stationary, speech-like sounds is proposed based on the differential properties of the instantaneous frequencies of two sets of filter outputs. A specific type of fixed points of mapping from the filter center frequency to the output instantaneous frequency provides frequencies of the constituent sinusoidal components of the input signal. When the filter is made from an isometric Gabor function convoluted with a cardinal B-spline basis function, the differential properties at the fixed points provide practical estimates of the carrier-to-noise ratio of the corresponding components. These estimates are used to select the fundamental component and to integrate the F0 information distributed among the other harmonic components.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-613"
  },
  "lawlor99_eurospeech": {
   "authors": [
    [
     "B.",
     "Lawlor"
    ],
    [
     "A. D.",
     "Fagan"
    ]
   ],
   "title": "A novel high quality efficient algorithm for time-scale modification of speech",
   "original": "e99_2785",
   "page_count": 4,
   "order": 617,
   "p1": "2785",
   "pn": "2788",
   "abstract": [
    "We present a novel efficient algorithm for time-scale modification (TSM) of speech which gives output quality equal to that of a conventional TSM algorithm, but having computational load an order of magnitude less. The algorithm presented uses a fixed length rectangular stepping window and a simple peak alignment criterion to track the local natural scaling factor and adapt the window step size. The desired TSM factor is realised by the appropriate number of applications of the constantly varying local natural scaling factor. The local natural scaling factor estimate is updated at sub-pitch period intervals giving accurate pitch tracking and high quality in the output scaled signal.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-614"
  },
  "lee99c_eurospeech": {
   "authors": [
    [
     "Minkyu",
     "Lee"
    ],
    [
     "Jan van",
     "Santen"
    ],
    [
     "Bernd",
     "Möbius"
    ],
    [
     "Joseph",
     "Olive"
    ]
   ],
   "title": "Formant tracking using segmental phonemic information",
   "original": "e99_2789",
   "page_count": 4,
   "order": 618,
   "p1": "2789",
   "pn": "2792",
   "abstract": [
    "A new formant tracking algorithm using phoneme depen-dent nominal formant values is tested. The algorithmcon-sists of three phases: (1) analysis, (2) segmentation, and (3) formant tracking. In the analysis phase, formant can-didates are obtained by solving for the roots of the lin-ear prediction polynomial. In the segmentation phase, the input text is converted into a sequence of phonemic symbols. Then the sequence is time aligned with the speech utterance. Finally, a set of formant candidates that are close to the nominal formant estimates while satisfy-ing the continuity constraints are chosen. The new algo-rithm significantly reduces the formant tracking error rate (3.62%) over a formant tracking algorithmusing only con-tinuity constraints (13.04%). We will also discuss how to further reduce the tracking error rate.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-615"
  },
  "mckenna99_eurospeech": {
   "authors": [
    [
     "John",
     "McKenna"
    ],
    [
     "Stephen",
     "Isard"
    ]
   ],
   "title": "Tailoring kalman filtering towards speaker characterisation",
   "original": "e99_2793",
   "page_count": 4,
   "order": 619,
   "p1": "2793",
   "pn": "2796",
   "abstract": [
    "This paper describes a method for obtaining smoothed vocal tract parameters from analysis during the closed phase of the glottis. The method is based upon Expectation Maximisation (EM) and uses Kalman-Rauch forward-backward iterations through a voiced segment, in which the speech data during excitation and open phases are excluded by treating them as missing data. This approach exploits the non-independence of neighbouring spectra and compensates for small numbers of available points, while preserving speaker-characteristic information and tracking variations in it. The vocal tract filter parameters are then used for inverse filtering the speech, thus obtaining estimates of the source excitation. The extracted excitation signal can be used to excite other sets of parameters to produce natural sounding speech.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-616"
  },
  "salomon99_eurospeech": {
   "authors": [
    [
     "Ariel",
     "Salomon"
    ],
    [
     "Carol",
     "Espy-Wilson"
    ]
   ],
   "title": "Automatic detection of manner events based on temporal parameters",
   "original": "e99_2797",
   "page_count": 4,
   "order": 620,
   "p1": "2797",
   "pn": "2800",
   "abstract": [
    "In this study, we investigated how well acoustic events extracted from a cross-spectral temporal measure could be used to classify the manner and voicing of consonants. In particular, we developed seven measures that look at the strength and time difference between various onsets and offsets of acoustic energy. Consistent with findings by Shannon et al. (1995), our classification results show that manner and voicing information can be determined from dynamic temporal cues.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-617"
  },
  "seck99_eurospeech": {
   "authors": [
    [
     "Mouhamadou",
     "Seck"
    ],
    [
     "Frédéric",
     "Bimbot"
    ],
    [
     "Didier",
     "Zugaj"
    ],
    [
     "Bernard",
     "Delyon"
    ]
   ],
   "title": "Two-class signal segmentation for speech/music detection in audio tracks",
   "original": "e99_2801",
   "page_count": 4,
   "order": 621,
   "p1": "2801",
   "pn": "2804",
   "abstract": [
    "We present a technique for the segmention of a sound track into two classes of segments. Each frame of signal is preprocessed by extracting cepstral coefficients and their first order derivatives. For each class, the distri-bution of the frame parameter vectors is modeled by a Gaussian Mixture Model (GMM). GMM order is se-lected using two criteria : the Minimum Description Length (MDL) criterion and the Aka¨ike Information Cri-terion (AIC). Frame score is based on a weighted log-likelihood ratio in a window around the frame. De-cision for each frame is taken by comparing its score to a threshold. Experiments are presented on speech / music segmentation in audio tracks. In these experi-ments, the MDL criterion leads to a reasonable GMMor-der. Using the MDL criterion for GMM order selection, frame classification error rate is around 20%. However, using GMMs with much lower orders, only decreases marginally performances.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-618"
  },
  "tuan99_eurospeech": {
   "authors": [
    [
     "Vu Ngoc",
     "Tuan"
    ],
    [
     "Christophe",
     "d'Alessandro"
    ]
   ],
   "title": "Robust glottal closure detection using the wavelet transform",
   "original": "e99_2805",
   "page_count": 4,
   "order": 622,
   "p1": "2805",
   "pn": "2808",
   "abstract": [
    "In this work, a time-scale framework for analysis of glottal closure instants is proposed. As glottal closure can be soft or sharp, depending on the type of vocal activity, the analysis method should be able to deal with both wide-band and low-pass signals. Thus, a multi-scale analysis seems well-suited. The analysis is based on a dyadic wavelet filterbank. Then, the amplitude maxima of the wavelet transform are computed, at each scale. These maxima are organized into lines of maximal amplitude (LOMA) using a dynamic programming algorithm. These lines are forming ``trees'' in the time-scale domain. Glottal closure instants are then interpreted as the top of the strongest branch, or trunk, of these trees. Interesting features of the LOMA are their amplitudes. The LOMA are strong and well organized for voiced speech, and rather weak and widespread for unvoiced speech. The accumulated amplitude along the LOMA gives a very good measure of the degree of voicing.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-619"
  },
  "santen99_eurospeech": {
   "authors": [
    [
     "Jan P. H. van",
     "Santen"
    ],
    [
     "Richard W.",
     "Sproat"
    ]
   ],
   "title": "High-accuracy automatic segmentation",
   "original": "e99_2809",
   "page_count": 4,
   "order": 623,
   "p1": "2809",
   "pn": "2812",
   "abstract": [
    "We propose a system for automatically determining boundaries between phonetic segments in a speech wave given a phonetic transcription: automatic segmentation. The system uses edge detectors that are applied to various speech representations; both are optimized for each diphone or diphone class. Output from these detectors, which contains spuriously detected edges, is then combined with alternative pronunciations generated via rules from the canonical pronunciation. The final output is generated with lowest-cost path algorithms applied to finite state transducers.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-620"
  },
  "zeljkovic99_eurospeech": {
   "authors": [
    [
     "Ilija",
     "Zeljkovic"
    ],
    [
     "Yannis",
     "Stylianou"
    ]
   ],
   "title": "Single complex sinusoid and ARHE model based pitch extractors",
   "original": "e99_2813",
   "page_count": 4,
   "order": 624,
   "p1": "2813",
   "pn": "2816",
   "abstract": [
    "In this paper we propose two techniques for the estimation of the fundamental frequency of speech signals. The first technique is based on the Autoregressive Harmonic Excitation (ARHE) speech model. ARHE model consists of an autoregressive process driven simultaneously by white noise and a periodic excitation. The second technique is based on the estimation of a complex sinusoid in white Gaussian noise. It uses the Hilbert transform of the speech signal and the derivative of its phase function over the time. The derivative of the phase information is seen as a simple model of a moving average process driven by noise. The fundamental frequency is obtained by the minimum variance estimator of the model. The proposed methods have comparable performance to previous reported pitch detectors while they maintain their performance under noisy conditions.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-621"
  },
  "alvarez99_eurospeech": {
   "authors": [
    [
     "A.",
     "Álvarez"
    ],
    [
     "R.",
     "Martínez"
    ],
    [
     "P.",
     "Gómez"
    ],
    [
     "V.",
     "Nieto"
    ],
    [
     "M. M.",
     "Pérez"
    ]
   ],
   "title": "A robust isolated word recognizer for highly non-stationary environments. recognition results",
   "original": "e99_2817",
   "page_count": 4,
   "order": 625,
   "p1": "2817",
   "pn": "2820",
   "abstract": [
    "Through the present paper, the evaluation results of a Speaker Independent Robust-to-Noise Isolated Word Recognizer are presented. The system, which is in part the results achieved by the project IVORY (ESPRIT project No. 20277) [6], is intended for working in highly non-stationary environments. The system comprises two main modules: the Noise Cancellator and the Speech Recognizer itself. System robustness is achieved in the noise cancellation module. This module incorporates an Adaptive Filter [7] [13], operating in the time domain, and a subsequent Spectral Subtraction step [2] operating in the frequency domain with the enhanced signal provided by the previous stage. Recognition results for different noise cancellation configurations and for several Parameter Extraction Front-Ends, including LPC [7], FFT Cepstrum [3] and PLP based methods [8] are presented.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-622"
  },
  "afify99_eurospeech": {
   "authors": [
    [
     "Mohamed",
     "Afify"
    ]
   ],
   "title": "Sequential bias compensation for robust speech recognition",
   "original": "e99_2821",
   "page_count": 4,
   "order": 626,
   "p1": "2821",
   "pn": "2824",
   "abstract": [
    "Additive bias compensation is a simple and effective technique to overcome the performance degradation caused by acoustic mismatch in speech recognition systems. Bias is usually estimated in a batch mode, assuming that its parameters are constant for the whole utterance. This paper develops a new sequential algorithm for additive bias estimation, which can potentially track time varying mismatch effects within a test utterance. Relation to recursive Kullback-Leibler technique is pointed out, and the method is tested using computer simulations and speech recognition experiments. Significant performance improvements in the recognition rate are obtained for supervised adaptation.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-623"
  },
  "tarcisio99_eurospeech": {
   "authors": [
    [
     "Coianiz",
     "Tarcisio"
    ],
    [
     "Falavigna",
     "Daniele"
    ],
    [
     "Gretter",
     "Roberto"
    ],
    [
     "Orlandi",
     "Marco"
    ]
   ],
   "title": "Use of simulated data for robust telephone speech recognition",
   "original": "e99_2825",
   "page_count": 4,
   "order": 627,
   "p1": "2825",
   "pn": "2828",
   "abstract": [
    "The collection of telephone databases, for training speech recognisers, is a time consuming and costly work. In the paper we propose a method for producing simulated telephone data starting from clean wide band databases. The result of the simulation is the generation of a noisy database that can be used, in addition to other techniques, for compensating or adapting speech recogniser parameters with respect to different test environments. For the first of the two adopted test sets, performance improvements ranging from about 30% to about 9% have been measured, as a function of the quantity of real telephone data used, in addition to the simulated ones, for system training. For the second test set no significant improvements were obtained.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-624"
  },
  "hauptman99_eurospeech": {
   "authors": [
    [
     "Y.",
     "Hauptman"
    ],
    [
     "Y.",
     "Bistritz"
    ]
   ],
   "title": "On the use of time alignments for noisy speech recognition",
   "original": "e99_2829",
   "page_count": 4,
   "order": 628,
   "p1": "2829",
   "pn": "2832",
   "abstract": [
    "One of the basic aspects of modern pattern matching algorithms used in speech recognition is time-alignment. The use of time-alignment is essential for offsetting speaking rate variations, which is an inherent property of the speech signal. It is known that time-alignment contributes to increased accuracy in speech recognition. However, a key question is whether time-alignment information still contributes to recognition accuracy in highly degraded speech. In this paper we examine the robustness of time-alignment information by introducing a robustness indicator. Isolated words recognition experiments with and without time alignment (using DTW and VQ respectively) are used and to illustrate the issue.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-625"
  },
  "hakkinen99_eurospeech": {
   "authors": [
    [
     "Juha",
     "Häkkinen"
    ],
    [
     "J.",
     "Suontausta"
    ],
    [
     "Ramalingam",
     "Hariharan"
    ],
    [
     "M.",
     "Vasilache"
    ],
    [
     "K.",
     "Laurila"
    ]
   ],
   "title": "Improved feature vector normalization for noise robust connected speech recognition",
   "original": "e99_2833",
   "page_count": 4,
   "order": 629,
   "p1": "2833",
   "pn": "2836",
   "abstract": [
    "Feature vector normalization has been successfully usedto improve the noise robustness of speech recognizers.Unfortunately, it may cause additional insertion errors inconnected digit recognition in clean environments. Wepropose two methods to reduce the number of insertions.Based on estimated instantaneous signal-to-noise ratiowe form a reliability measure for the recognized digits.We discard unreliable digits from the beginning and theend of the recognized digit sequence. Since the proposedreliability hypotheses are independent of the likelihoodsproduced by an HMM classifier, we are capable ofbringing new useful information into the classificationprocess. In addition, we constrain the normalizationprocess on the basis of statistics obtained from thetraining data. Experimental results show that we arecapable of achieving an average 32% string level errorrate reduction in simulations of a noisy car environment.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-626"
  },
  "josifovski99_eurospeech": {
   "authors": [
    [
     "Ljubomir",
     "Josifovski"
    ],
    [
     "Martin",
     "Cooke"
    ],
    [
     "Phil",
     "Green"
    ],
    [
     "Ascension",
     "Vizinho"
    ]
   ],
   "title": "State based imputation of missing data for robust speech recognition and speech enhancement",
   "original": "e99_2837",
   "page_count": 4,
   "order": 630,
   "p1": "2837",
   "pn": "2840",
   "abstract": [
    "Within the context of continuous-density HMM speech recognition in noise, we report on imputation of missing time-frequency regions using emission state probability distributions. Spectral subtraction and local signalto noise estimation based criteria are used to separate the present from the missing components. We consider two approaches to the problem of classification with missing data: marginalization and data imputation. A formal-ism for data imputation based on the probability distributions of individual Hidden Markov model states is presented. We report on recognition experiments comparing state based data imputation to marginalization in the context of connected digit recognition of speech mixed with factory noise at various global signal-to-noise ratios, and wideband restoration of speech. Potential advantages of the approach are that it can be followed by conventional techniques like cepstral features or artificial neural net-works for speech recognition.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-627"
  },
  "kermorvant99_eurospeech": {
   "authors": [
    [
     "Christopher",
     "Kermorvant"
    ],
    [
     "Andrew",
     "Morris"
    ]
   ],
   "title": "A comparison of two strategies for ASR in additive noise: missing data and spectral subtraction",
   "original": "e99_2841",
   "page_count": 4,
   "order": 631,
   "p1": "2841",
   "pn": "2844",
   "abstract": [
    "This paper addresses the problem of speech recognition in the presence of additive noise. To deal with this problem, it is possible to estimate the noise characteristics using methods which have previously been developed for speech enhancement techniques. Spectral subtraction can then be used to reduce the effect of additive noise on speech in the spectral domain. Some techniques have also recently been proposed for recognition with missing data. These approaches require an estimation of the local SNR to detect the speech spectral features which are relatively free from noise so as to perform recognition on these parts only. In this article, we compare these two different strategies, spectral subtraction and \"missing data\", on continuous speech additively disturbed with real noise. It is shown that missing data methods can improve recognition performance under certain noise conditions but still need to be improved in order to to reach the performance of the spectral subtraction.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-628"
  },
  "milner99_eurospeech": {
   "authors": [
    [
     "Ben",
     "Milner"
    ],
    [
     "Mark",
     "Farrell"
    ]
   ],
   "title": "A comparison of techniques for tone compensation in payphone-based speech recognition",
   "original": "e99_2845",
   "page_count": 4,
   "order": 632,
   "p1": "2845",
   "pn": "2848",
   "abstract": [
    "This paper compares two noise compensation techniques for increasing recognition performance on tone corrupted speech from payphones. An analysis of payphone speech is made which identifies two harmful processes - signalling tones and increased background noise. The techniques of spectral subtraction and parallel model combination are compared on this task and are shown to give some improvement. Further robustness is demonstrated using RASTA-CTM speech features.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-629"
  },
  "menendezpidal99_eurospeech": {
   "authors": [
    [
     "Xavier",
     "Menéndez-Pidal"
    ],
    [
     "Ruxin",
     "Chen"
    ],
    [
     "Duanpei",
     "Wu"
    ],
    [
     "Mick",
     "Tanaka"
    ]
   ],
   "title": "Front-end improvements to reduce stationary & variable channel and noise distortions in continuous speech recognition tasks",
   "original": "e99_2849",
   "page_count": 4,
   "order": 633,
   "p1": "2849",
   "pn": "2852",
   "abstract": [
    "This paper introduces our actual work in front-end techniques to obtain robust speech recognition devices in mismatch conditions (additive noise mismatch and channel mismatch). Two algorithms have been combined to compensate the distortions due to different channel characteristics and additive noise: 1) A Cepstral Mean Normalization and Variance Scaling technique (MNVS) and 2) An Adaptive Gaussian Attenuation algorithm (AGA). Combining both techniques the channel distortion effects were reduced to 90% on the HTIMIT task and the additive noise effects were reduced to 80% on the TIMIT task corrupted with additive car noise.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-630"
  },
  "nokas99_eurospeech": {
   "authors": [
    [
     "G.",
     "Nokas"
    ],
    [
     "E.",
     "Dermatas"
    ]
   ],
   "title": "Speech recognition in noisy reverberant rooms using a frequency domain blind deconvolution method",
   "original": "e99_2853",
   "page_count": 4,
   "order": 634,
   "p1": "2853",
   "pn": "2856",
   "abstract": [
    "The aim of this paper is to present and evaluate two adaptive speech enhancement methods in the frequency domain by measuring the recognition rate of a speaker-independent word recognition system of isolated words. In a hands-free speech recognition experiment, a factory noise source and a speaker are the acoustic sources in a real room environment. A close-talking microphone is positioned near the noise source while the primary omni-directional microphone captures the convoluted speech and noise signals. Adaptive noise cancellation reduces the presence of noise in the primary microphone followed by a blind deconvolution algorithm used to minimize reverberations. Experimental results showed that the proposed speech enhancement methods increase 2.4 times the recognition rate for a vocabulary of 21 words of the Greek language, but the achieved recognition rate of 16% is inapplicable for commercial applications.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-631"
  },
  "schless99_eurospeech": {
   "authors": [
    [
     "Volker",
     "Schless"
    ],
    [
     "Fritz",
     "Class"
    ],
    [
     "Peter",
     "Sandl"
    ]
   ],
   "title": "Optimization of a speech recognizer for aircraft environments",
   "original": "e99_2857",
   "page_count": 4,
   "order": 635,
   "p1": "2857",
   "pn": "2860",
   "abstract": [
    "Speech recognition in aircrafts can greatly simplify operation of equipment in both military and civil environments. This paper describes the development of specialized recognizers for two military applications: One for assisting a jet pilot wearing a breathing mask and another for a radar evaluator within an aircraft (similarly to AWACS). In both cases it is not practical to collect sufficient speech data under real conditions for a robust specialized recognizer. This paper describes two methods for overcoming this data problem and building a recognizer with minimal effort: retraining the baseline system with real application data and combining the baseline system with a new trained system. These methods greatly improved the performance of the baseline system (US English recognizer adapted to car environment).\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-632"
  },
  "yoma99_eurospeech": {
   "authors": [
    [
     "Nestor Becerra",
     "Yoma"
    ],
    [
     "Lee Luan",
     "Ling"
    ],
    [
     "Sandra Dotto",
     "Stump"
    ]
   ],
   "title": "Temporal constraints in viterbi alignment for speech recognition in noise",
   "original": "e99_2861",
   "page_count": 4,
   "order": 636,
   "p1": "2861",
   "pn": "2864",
   "abstract": [
    "This paper addresses the problem of temporal constraints in the Viterbi algorithm using conditional transition probabilities. The results here presented suggest that in a speaker dependent small vocabulary task the statistical modelling of state durations is not relevant if the max and min state duration restrictions are imposed, and that truncated probability densities give better results than a metric previously proposed [1]. Finally, context dependent and context independent temporal restrictions are compared in a connected word speech recognition task and it is shown that the former leads to better results with the same computational load.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-633"
  },
  "yamamoto99b_eurospeech": {
   "authors": [
    [
     "Kazumasa",
     "Yamamoto"
    ],
    [
     "Seiichi",
     "Nakagawa"
    ]
   ],
   "title": "HMM composition of segmental unit input HMM for noisy speech recognition",
   "original": "e99_2865",
   "page_count": 4,
   "order": 637,
   "p1": "2865",
   "pn": "2868",
   "abstract": [
    "For robust speech recognition in noisy environments, various methods have been studied. In this paper, we apply parallel model combination (PMC) for segmental unit input HMM to recognize corrupted speech in additive noise. Since several successive frames are combined and treated as an input vector in segmental unit input modeling, the increased dimension of vector degrades the precision in estimating covariance matrices. Therefore Karhunen-Loeve expansion or LDA is used to reduce the dimension. Thus the inverse transformation of segmental statistics to cepstral domain is needed and correlations between frames have to be taken into account. We expanded the original PMC to segmental unit input HMM. Experimental results showed PMC for segmental unit input HMM proposed here gives better recognition performance than the original PMC.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-634"
  },
  "yoma99b_eurospeech": {
   "authors": [
    [
     "Nestor Becerra",
     "Yoma"
    ],
    [
     "Lee Luan",
     "Ling"
    ],
    [
     "Sandra Dotto",
     "Stump"
    ]
   ],
   "title": "Robust connected word speech recognition using weighted viterbi algorithm and context-dependent temporal constraints",
   "original": "e99_2869",
   "page_count": 4,
   "order": 638,
   "p1": "2869",
   "pn": "2872",
   "abstract": [
    "This paper addresses the problem of connected word speech recognition with signals corrupted by additive and convolutional noise. Context-dependent temporal constraints are proposed and compared with the ordinary temporal restrictions, and used in combination with the weighted Viterbi algorithm which had been tested with isolated word recognition experiments in previous papers. Connected-word recognition tests show that the weighted Viterbi algorithm depends on the accuracy of the state duration modelling and the approach here covered can lead to reductions as high as 90 or 95% in the error rate at moderate SNR using spectral subtraction, an easily implemented technique, even with a poor estimation for noise and without using any information about the speaker. It is also shown that the weighting procedure can reduce the error rate when cepstral mean normalization is also used to cancel both additive and convolutional noise.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-635"
  },
  "yao99_eurospeech": {
   "authors": [
    [
     "Kaisheng",
     "Yao"
    ],
    [
     "Bertram",
     "Shi"
    ],
    [
     "Pascale",
     "Fung"
    ],
    [
     "Zhigang",
     "Cao"
    ]
   ],
   "title": "Liftered forward masking procedure for robust digits recognition",
   "original": "e99_2873",
   "page_count": 4,
   "order": 639,
   "p1": "2873",
   "pn": "2876",
   "abstract": [
    "Using TI digits recognition experiments, we show that a combination of two dynamic speech features, Liftered Forward Masked (LFM) MFCC and 2-D cepstrum, can improve system robustness to additive Volvo noise while maintaining system per-formance comparable to standard MFCC features in clean conditions. Through experiments, we show that the information extracted by forward masking and by the 2D cepstrum are in some sense orthogonal. By combining the LFM MFCC and the 2-D cepstrum plus > 2-D cepstrum, we achieve a recognition rate above 90% on the TI connected digits task, even in additive Volvo noise condition with SNR as low as 0dB. This corresponds to a SNR gain over 30dB compared with standard MFCC plus dynamic and acceleration coefficients.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.1999-636"
  },
  "zhao99b_eurospeech": {
   "authors": [
    [
     "Yunxin",
     "Zhao"
    ]
   ],
   "title": "Channel identification and spectrum estimation for robust automatic speech recognition",
   "original": "e99_2877",
   "page_count": 1,
   "order": 640,
   "p1": "2877",
   "pn": "2880",
   "abstract": [
    "A feature estimation technique is proposed for speech signals that are corrupted by both additive and convolutive noises via com-bining channel identification with power spectrum estimation. A correlation-matching algorithm is developed for channel identification, and a Gaussian mixture density model of speech DFT spectra is formulated for estimation of speech power spectra. Cepstral features of speech are calculated from the estimated power spec-tra. Using the proposed method, significantly improved accuracy was achieved on speaker-independent continuous speech recognition where the speech data were corrupted by a simulated linear distortion channel and additive white noise.\n",
    ""
   ]
  }
 },
 "sessions": [
  {
   "title": "Keynotes",
   "papers": [
    "jelinek99_eurospeech",
    "gosy99_eurospeech",
    "maybury99_eurospeech",
    "lindblom99_eurospeech"
   ]
  },
  {
   "title": "Speech Recognition, Adaptation 1",
   "papers": [
    "chou99_eurospeech",
    "goronzy99_eurospeech",
    "hirsch99_eurospeech",
    "huang99_eurospeech",
    "logan99_eurospeech"
   ]
  },
  {
   "title": "Prosody - Prosodic Features in Dialogues",
   "papers": [
    "brinckmann99_eurospeech",
    "grigorova99_eurospeech",
    "horne99_eurospeech",
    "kitazawa99_eurospeech",
    "tamoto99_eurospeech"
   ]
  },
  {
   "title": "Speech Recognition - Confidence Measures",
   "papers": [
    "cox99_eurospeech",
    "jia99_eurospeech",
    "klakow99_eurospeech",
    "lin99_eurospeech",
    "rahim99_eurospeech"
   ]
  },
  {
   "title": "Speech Recognition - Acoustic Processing",
   "papers": [
    "chengalvarayan99_eurospeech",
    "veth99_eurospeech",
    "heracleous99_eurospeech",
    "hermansky99_eurospeech",
    "macho99_eurospeech",
    "meinedo99_eurospeech",
    "paliwal99_eurospeech",
    "pachesleal99_eurospeech",
    "sansegundo99_eurospeech",
    "warakagoda99_eurospeech",
    "yato99_eurospeech",
    "zhu99_eurospeech"
   ]
  },
  {
   "title": "Articulatory Measurements and Modelling",
   "papers": [
    "beautemps99_eurospeech",
    "engwall99_eurospeech",
    "kienast99_eurospeech",
    "kaburagi99_eurospeech",
    "krstulovic99_eurospeech",
    "miki99_eurospeech",
    "matsuda99_eurospeech",
    "okadome99_eurospeech",
    "laprie99_eurospeech",
    "owens99_eurospeech",
    "richmond99_eurospeech",
    "silva99_eurospeech",
    "thimm99_eurospeech",
    "teixeira99_eurospeech",
    "vaxelaire99_eurospeech",
    "vilain99_eurospeech"
   ]
  },
  {
   "title": "First and Second Language Learning",
   "papers": [
    "wet99_eurospeech",
    "kawai99_eurospeech",
    "nouza99_eurospeech",
    "song99_eurospeech",
    "santiagooriola99_eurospeech",
    "trancoso99_eurospeech",
    "yvon99_eurospeech",
    "laczko99_eurospeech"
   ]
  },
  {
   "title": "Speech Recognition - Adaptation 2",
   "papers": [
    "byrne99_eurospeech",
    "chien99_eurospeech",
    "chesta99_eurospeech",
    "hariharan99_eurospeech",
    "rottland99_eurospeech"
   ]
  },
  {
   "title": "Prosody - Prosodic Phrasing and Interruptions",
   "papers": [
    "alter99_eurospeech",
    "goto99_eurospeech",
    "iwano99_eurospeech",
    "warnke99_eurospeech",
    "yang99_eurospeech"
   ]
  },
  {
   "title": "Assessment",
   "papers": [
    "constantinides99_eurospeech",
    "fiscus99_eurospeech",
    "sonntag99_eurospeech",
    "steeneken99_eurospeech",
    "suzuki99_eurospeech"
   ]
  },
  {
   "title": "Speech Recognition - Confidence Measures 2",
   "papers": [
    "bauer99_eurospeech",
    "bartkova99_eurospeech",
    "chang99_eurospeech",
    "davies99_eurospeech",
    "elmeliani99_eurospeech",
    "jouvet99_eurospeech",
    "koo99_eurospeech",
    "moreau99_eurospeech",
    "maciasguarasa99_eurospeech",
    "pfau99_eurospeech",
    "rose99_eurospeech",
    "strom99_eurospeech",
    "stolcke99_eurospeech",
    "wessel99_eurospeech"
   ]
  },
  {
   "title": "Speech Analysis and Tools",
   "papers": [
    "pratzas99_eurospeech",
    "ma99_eurospeech",
    "ittycheriah99_eurospeech",
    "lobanov99_eurospeech",
    "liew99_eurospeech",
    "mctear99_eurospeech",
    "kajarekar99_eurospeech",
    "shimamura99_eurospeech",
    "souza99_eurospeech",
    "soraghan99_eurospeech",
    "tokuhira99_eurospeech"
   ]
  },
  {
   "title": "Language Identification",
   "papers": [
    "berkling99_eurospeech",
    "tsai99_eurospeech",
    "cummins99_eurospeech",
    "harbeck99_eurospeech",
    "hombert99_eurospeech",
    "itahashi99_eurospeech",
    "matrouf99_eurospeech",
    "mori99_eurospeech",
    "barkat99_eurospeech",
    "pellegrino99_eurospeech"
   ]
  },
  {
   "title": "Speech Recognition - Speaking Rate",
   "papers": [
    "anderson99_eurospeech",
    "faltlhauser99_eurospeech",
    "richardson99_eurospeech",
    "saul99_eurospeech",
    "tuerk99_eurospeech"
   ]
  },
  {
   "title": "Speech Acoustics",
   "papers": [
    "bloothooft99_eurospeech",
    "bouteille99_eurospeech",
    "jesus99_eurospeech",
    "petlyuchenko99_eurospeech",
    "son99_eurospeech"
   ]
  },
  {
   "title": "Speech Recognition - Search and Pronunciation Modelling",
   "papers": [
    "abe99_eurospeech",
    "chen99_eurospeech",
    "eide99_eurospeech",
    "franz99_eurospeech",
    "coletti99_eurospeech",
    "foslerlussier99_eurospeech",
    "finke99_eurospeech",
    "husson99_eurospeech",
    "hanna99_eurospeech",
    "iyer99_eurospeech",
    "kwon99_eurospeech",
    "kabre99_eurospeech",
    "korkmazskiy99_eurospeech",
    "mangu99_eurospeech",
    "ortmanns99_eurospeech",
    "padmanabhan99_eurospeech",
    "ramabhadran99_eurospeech",
    "shirosaki99_eurospeech",
    "saraclar99_eurospeech",
    "aubert99_eurospeech"
   ]
  },
  {
   "title": "Prosody - Stress, Accent and Prominence Phrasing",
   "papers": [
    "batliner99_eurospeech",
    "conkie99_eurospeech",
    "fach99_eurospeech",
    "fivela99_eurospeech",
    "haas99_eurospeech",
    "kim99_eurospeech",
    "mersdorf99_eurospeech",
    "nakatani99_eurospeech",
    "streefkerk99_eurospeech",
    "theune99_eurospeech"
   ]
  },
  {
   "title": "Speech Disorders &amp; Speech for Disabled",
   "papers": [
    "bonneau99_eurospeech",
    "godinollorente99_eurospeech",
    "harborg99_eurospeech",
    "jo99_eurospeech",
    "maruyama99_eurospeech",
    "ogner99_eurospeech",
    "prizljakovac99_eurospeech"
   ]
  },
  {
   "title": "Speech Recognition - Multi-stream ASR",
   "papers": [
    "cerisara99_eurospeech",
    "janin99_eurospeech",
    "mirghafori99_eurospeech",
    "morris99_eurospeech",
    "okawa99_eurospeech"
   ]
  },
  {
   "title": "Speech Generation and Synthesis - Concatenation",
   "papers": [
    "beutnagel99_eurospeech",
    "chen99b_eurospeech",
    "lewis99_eurospeech",
    "stober99_eurospeech",
    "taylor99_eurospeech"
   ]
  },
  {
   "title": "Speech Communication Education",
   "papers": [
    "bloothooft99b_eurospeech",
    "cooke99_eurospeech",
    "mctear99b_eurospeech",
    "hoffmann99_eurospeech",
    "qvarfordt99_eurospeech"
   ]
  },
  {
   "title": "Speech Recognition - Broadcast News",
   "papers": [
    "harris99_eurospeech",
    "liu99_eurospeech",
    "palmery99_eurospeech",
    "renals99_eurospeech",
    "woodland99_eurospeech"
   ]
  },
  {
   "title": "Prosody - Temporal and/or Intonational Features",
   "papers": [
    "brindopke99_eurospeech",
    "demenko99_eurospeech",
    "duez99_eurospeech",
    "gurlekian99_eurospeech",
    "tolba99_eurospeech",
    "isogai99_eurospeech",
    "kleckova99_eurospeech",
    "mohler99_eurospeech",
    "smirnova99_eurospeech",
    "wang99_eurospeech",
    "zhang99_eurospeech"
   ]
  },
  {
   "title": "Speaker Recognition - Acoustic Features and Robustness",
   "papers": [
    "besacier99_eurospeech",
    "balchandran99_eurospeech",
    "magrinchagnolleau99_eurospeech",
    "faundezzanuy99_eurospeech",
    "jang99_eurospeech",
    "lavner99_eurospeech",
    "lo99_eurospeech",
    "miyajima99_eurospeech",
    "ortegagarcia99_eurospeech",
    "quatieri99_eurospeech",
    "ramanujam99_eurospeech",
    "vergin99_eurospeech",
    "zilca99_eurospeech"
   ]
  },
  {
   "title": "Speech Recognition - Large Vocabulary Continuous Speech Recognition (LVCSR)",
   "papers": [
    "che99_eurospeech",
    "ljolje99_eurospeech",
    "mohri99_eurospeech",
    "reichert99_eurospeech",
    "zheng99_eurospeech"
   ]
  },
  {
   "title": "Speech Generation and Synthesis - Systems and Evaluation",
   "papers": [
    "fitt99_eurospeech",
    "maeda99_eurospeech",
    "mcinnes99_eurospeech",
    "traber99_eurospeech",
    "tanaka99_eurospeech"
   ]
  },
  {
   "title": "Speech Technology for Language Learning",
   "papers": [
    "deville99_eurospeech",
    "eskenazi99_eurospeech",
    "franco99_eurospeech",
    "herron99_eurospeech",
    "vicsi99_eurospeech"
   ]
  },
  {
   "title": "Speech Recognition - Multilinguality",
   "papers": [
    "ahadi99_eurospeech",
    "fegyo99_eurospeech",
    "fung99_eurospeech",
    "grocholewski99_eurospeech",
    "hirose99_eurospeech",
    "ho99_eurospeech",
    "imperl99_eurospeech",
    "liu99b_eurospeech",
    "liu99c_eurospeech",
    "lopezdeipina99_eurospeech",
    "nassar99_eurospeech",
    "nieuwoudt99_eurospeech",
    "uebler99_eurospeech"
   ]
  },
  {
   "title": "Systems, Architectures, Interfaces",
   "papers": [
    "altosaar99_eurospeech",
    "draxler99_eurospeech",
    "edgington99_eurospeech",
    "gegenmantel99_eurospeech",
    "muller99_eurospeech",
    "matsunaga99_eurospeech",
    "nemeth99_eurospeech",
    "olaszy99_eurospeech",
    "penn99_eurospeech",
    "rodriguez99_eurospeech",
    "sanderman99_eurospeech",
    "tamura99_eurospeech",
    "wouters99_eurospeech"
   ]
  },
  {
   "title": "Speaker Recognition - Scoring and Decision",
   "papers": [
    "ariyaeeinia99_eurospeech",
    "altincay99_eurospeech",
    "elmaliki99_eurospeech",
    "fakotakis99_eurospeech",
    "fredouille99_eurospeech",
    "isobe99_eurospeech",
    "luettin99_eurospeech",
    "mathew99_eurospeech",
    "pelecanos99_eurospeech",
    "rodriguezlinares99_eurospeech",
    "ruizmezcua99_eurospeech",
    "sabac99_eurospeech",
    "tadj99_eurospeech",
    "yoshida99_eurospeech",
    "zhang99b_eurospeech"
   ]
  },
  {
   "title": "Speech Generation and Synthesis - Acoustic Synthesis",
   "papers": [
    "acero99_eurospeech",
    "bailly99_eurospeech",
    "laine99_eurospeech",
    "obrien99_eurospeech",
    "beutnagel99b_eurospeech"
   ]
  },
  {
   "title": "Disorders in Speech Production and/or Speech Perception",
   "papers": [
    "garciagomez99_eurospeech",
    "hoshino99_eurospeech",
    "imatomi99_eurospeech",
    "rezaeiaghbash99_eurospeech"
   ]
  },
  {
   "title": "Speech Recognition - Acoustic Modelling 1",
   "papers": [
    "deng99_eurospeech",
    "albesano99_eurospeech",
    "gales99_eurospeech",
    "zhao99_eurospeech",
    "vaich99_eurospeech"
   ]
  },
  {
   "title": "Dialogue 1",
   "papers": [
    "asoh99_eurospeech",
    "bell99_eurospeech",
    "bernsen99_eurospeech",
    "gustafson99_eurospeech",
    "grisvard99_eurospeech",
    "huang99b_eurospeech",
    "sasajima99_eurospeech",
    "nakano99_eurospeech",
    "pirker99_eurospeech",
    "pargellis99_eurospeech",
    "relanogil99_eurospeech",
    "veldhuijzenvanzanten99_eurospeech"
   ]
  },
  {
   "title": "Speaker Recognition and Topic Detection",
   "papers": [
    "ashour99_eurospeech",
    "bovbel99_eurospeech",
    "delacourt99_eurospeech",
    "glaeser99_eurospeech",
    "kolano99_eurospeech",
    "li99_eurospeech",
    "lindberg99_eurospeech",
    "mclaughlin99_eurospeech",
    "maes99_eurospeech",
    "masuko99_eurospeech",
    "majewski99_eurospeech",
    "satuevillar99_eurospeech",
    "yamashita99_eurospeech"
   ]
  },
  {
   "title": "Speech Recognition - Search",
   "papers": [
    "castro99_eurospeech",
    "goel99_eurospeech",
    "hanzl99_eurospeech",
    "ljolje99b_eurospeech",
    "xu99_eurospeech"
   ]
  },
  {
   "title": "Systems, Architectures",
   "papers": [
    "jeanrenaud99_eurospeech",
    "os99_eurospeech",
    "robinson99_eurospeech",
    "seneff99_eurospeech",
    "pateras99_eurospeech"
   ]
  },
  {
   "title": "Audio-Visual Speech",
   "papers": [
    "kuratate99_eurospeech",
    "macdonald99_eurospeech",
    "nankaku99_eurospeech",
    "potamianos99_eurospeech",
    "radeau99_eurospeech"
   ]
  },
  {
   "title": "Speech Recognition - Acoustic Modelling 2",
   "papers": [
    "souza99b_eurospeech",
    "liu99d_eurospeech",
    "singh99_eurospeech",
    "sankar99_eurospeech",
    "schluter99_eurospeech"
   ]
  },
  {
   "title": "Dialogue 2",
   "papers": [
    "ammicht99_eurospeech",
    "byron99_eurospeech",
    "wu99_eurospeech",
    "ehrlich99_eurospeech",
    "hirasawa99_eurospeech",
    "lopezcozar99_eurospeech",
    "lavelle99_eurospeech",
    "niimi99_eurospeech",
    "ocelikova99_eurospeech",
    "papineni99_eurospeech",
    "ries99_eurospeech",
    "sturm99_eurospeech",
    "krahmer99_eurospeech",
    "lin99b_eurospeech"
   ]
  },
  {
   "title": "Speech Coding",
   "papers": [
    "chapman99_eurospeech",
    "edmondson99_eurospeech",
    "gao99_eurospeech",
    "gottesman99_eurospeech",
    "gortz99_eurospeech",
    "li99b_eurospeech",
    "lois99_eurospeech",
    "mayrench99_eurospeech",
    "martin99_eurospeech",
    "oliva99_eurospeech",
    "ohmura99_eurospeech",
    "pelaezmoreno99_eurospeech",
    "sercov99_eurospeech",
    "petrinovic99_eurospeech",
    "stefanovic99_eurospeech",
    "chen99c_eurospeech",
    "zolfaghari99_eurospeech"
   ]
  },
  {
   "title": "Dialogue",
   "papers": [
    "chucarroll99_eurospeech",
    "dahlback99_eurospeech",
    "os99b_eurospeech",
    "rudnicky99_eurospeech",
    "rosset99_eurospeech"
   ]
  },
  {
   "title": "Wideband and Perceptually Based Coding",
   "papers": [
    "amodio99_eurospeech",
    "bernard99_eurospeech",
    "foldvari99_eurospeech",
    "perreauguimaraes99_eurospeech",
    "wan99_eurospeech"
   ]
  },
  {
   "title": "Speech Recognition - Language Modelling",
   "papers": [
    "adda99_eurospeech",
    "bechet99_eurospeech",
    "baggia99_eurospeech",
    "brieusselpousse99_eurospeech",
    "leung99_eurospeech",
    "langlois99_eurospeech",
    "muller99b_eurospeech",
    "mcallaster99_eurospeech",
    "reichl99_eurospeech",
    "smadli99_eurospeech",
    "sanchez99_eurospeech",
    "yamamoto99_eurospeech",
    "zhu99b_eurospeech",
    "zitouni99_eurospeech",
    "zhang99c_eurospeech"
   ]
  },
  {
   "title": "Prosody - Study of Prosody for Speech Synthesis",
   "papers": [
    "chen99d_eurospeech",
    "cordoba99_eurospeech",
    "clark99_eurospeech",
    "dusterhoff99_eurospeech",
    "esposito99_eurospeech",
    "fernandezsalgado99_eurospeech",
    "hirst99_eurospeech",
    "morlec99_eurospeech",
    "sheikhzadeh99_eurospeech",
    "teixeira99b_eurospeech",
    "wang99b_eurospeech",
    "williams99_eurospeech"
   ]
  },
  {
   "title": "Speech Perception 1",
   "papers": [
    "amano99_eurospeech",
    "chereau99_eurospeech",
    "colin99_eurospeech",
    "dupoux99_eurospeech",
    "feijoo99_eurospeech",
    "gelin99_eurospeech",
    "karlsson99_eurospeech",
    "samarta99_eurospeech",
    "widera99_eurospeech"
   ]
  },
  {
   "title": "Multimodal Interaction",
   "papers": [
    "julia99_eurospeech",
    "matsusaka99_eurospeech",
    "narayanan99_eurospeech",
    "okada99_eurospeech",
    "purson99_eurospeech"
   ]
  },
  {
   "title": "Joint Source-Channel Coding",
   "papers": [
    "chang99b_eurospeech",
    "kovesi99_eurospeech",
    "pan99_eurospeech",
    "sriratanaban99_eurospeech",
    "villette99_eurospeech"
   ]
  },
  {
   "title": "Speech Generation and Synthesis - Prosody",
   "papers": [
    "bulyko99_eurospeech",
    "deans99_eurospeech",
    "eom99_eurospeech",
    "ferencz99_eurospeech",
    "fackrell99_eurospeech",
    "gu99_eurospeech",
    "house99_eurospeech",
    "huckvale99_eurospeech",
    "park99_eurospeech",
    "lee99_eurospeech",
    "mixdorff99_eurospeech",
    "sakurai99_eurospeech",
    "kitagawa99_eurospeech",
    "tatham99_eurospeech",
    "takano99_eurospeech",
    "villarnavarro99_eurospeech"
   ]
  },
  {
   "title": "Speech Perception 2",
   "papers": [
    "ainsworth99_eurospeech",
    "cerrato99_eurospeech",
    "erdeljac99_eurospeech",
    "hant99_eurospeech",
    "irino99_eurospeech",
    "palkova99_eurospeech",
    "pallier99_eurospeech",
    "vicsi99b_eurospeech",
    "leeuwen99_eurospeech",
    "whiteside99_eurospeech"
   ]
  },
  {
   "title": "Speech Recognition - Language Modelling 1",
   "papers": [
    "chen99e_eurospeech",
    "clarkson99_eurospeech",
    "huang99c_eurospeech",
    "kilian99_eurospeech",
    "martin99b_eurospeech"
   ]
  },
  {
   "title": "Speech and Noise",
   "papers": [
    "bippus99_eurospeech",
    "huang99d_eurospeech",
    "hermus99_eurospeech",
    "westphal99_eurospeech",
    "das99_eurospeech"
   ]
  },
  {
   "title": "Text-Dependent Speaker Verification",
   "papers": [
    "bimbot99_eurospeech",
    "charlet99_eurospeech",
    "genoud99_eurospeech",
    "melin99_eurospeech",
    "mariethoz99_eurospeech"
   ]
  },
  {
   "title": "Speech Understanding - Miscellaneous Topics",
   "papers": [
    "boros99_eurospeech",
    "deinzer99_eurospeech",
    "hakkanitur99_eurospeech",
    "ishikawa99_eurospeech",
    "kronenberg99_eurospeech",
    "lee99b_eurospeech",
    "chiang99_eurospeech",
    "liu99e_eurospeech",
    "meng99_eurospeech",
    "noth99_eurospeech",
    "obuchi99_eurospeech",
    "potamianos99b_eurospeech",
    "spilker99_eurospeech",
    "schadle99_eurospeech",
    "siu99_eurospeech",
    "takezawa99_eurospeech",
    "wong99_eurospeech",
    "wu99b_eurospeech",
    "wang99c_eurospeech"
   ]
  },
  {
   "title": "Speech Generation and Synthesis - Systems, Linguistic Processing",
   "papers": [
    "barbosa99_eurospeech",
    "burileanu99_eurospeech",
    "carlberger99_eurospeech",
    "dermatas99_eurospeech",
    "gros99_eurospeech",
    "ho99b_eurospeech",
    "mizuno99_eurospeech",
    "hain99_eurospeech",
    "koutny99_eurospeech",
    "mihkla99_eurospeech",
    "montero99_eurospeech",
    "pavesic99_eurospeech",
    "rojc99_eurospeech",
    "suh99_eurospeech",
    "sannier99_eurospeech",
    "tzoukermann99_eurospeech",
    "busser99_eurospeech"
   ]
  },
  {
   "title": "Speech &amp; the Internet",
   "papers": [
    "bowerman99_eurospeech",
    "drygajlo99_eurospeech",
    "digalakis99_eurospeech",
    "fujisaki99_eurospeech",
    "nishimoto99_eurospeech"
   ]
  },
  {
   "title": "Speech Recognition - Language Modelling 2",
   "papers": [
    "bellegarda99_eurospeech",
    "gildea99_eurospeech",
    "galescu99_eurospeech",
    "nasr99_eurospeech",
    "wu99c_eurospeech"
   ]
  },
  {
   "title": "Speech Signal Processing",
   "papers": [
    "kiss99_eurospeech",
    "karjalainen99_eurospeech",
    "parris99_eurospeech",
    "vuuren99_eurospeech",
    "yang99b_eurospeech"
   ]
  },
  {
   "title": "Text-Independent Speaker Verification and Tracking",
   "papers": [
    "beigi99_eurospeech",
    "cernocky99_eurospeech",
    "johnson99_eurospeech",
    "przybocki99_eurospeech",
    "sonmez99_eurospeech"
   ]
  },
  {
   "title": "Corpora",
   "papers": [
    "choukri99_eurospeech",
    "eskenazi99b_eurospeech",
    "hoge99_eurospeech",
    "mengel99_eurospeech",
    "silverman99_eurospeech"
   ]
  },
  {
   "title": "Speech Generation and Synthesis - Acoustic Synthesis and Units",
   "papers": [
    "au99_eurospeech",
    "balestri99_eurospeech",
    "chou99b_eurospeech",
    "etxebarria99_eurospeech",
    "huang99e_eurospeech",
    "kapilow99_eurospeech",
    "koyama99_eurospeech",
    "mann99_eurospeech",
    "meron99_eurospeech",
    "matousek99_eurospeech",
    "macon99_eurospeech",
    "ouellet99_eurospeech",
    "tychtl99_eurospeech",
    "rank99_eurospeech",
    "stylianou99_eurospeech",
    "yoshimura99_eurospeech"
   ]
  },
  {
   "title": "Speech and Noise 1",
   "papers": [
    "glotin99_eurospeech",
    "bayya99_eurospeech",
    "elmaleh99_eurospeech",
    "fernandez99_eurospeech",
    "guilmin99_eurospeech",
    "haverinen99_eurospeech",
    "isosipila99_eurospeech",
    "karray99_eurospeech",
    "kuropatwinski99_eurospeech",
    "matsumoto99_eurospeech",
    "nemer99_eurospeech",
    "shieh99_eurospeech",
    "tchorz99_eurospeech",
    "vicente99_eurospeech",
    "vizinho99_eurospeech",
    "vetter99_eurospeech"
   ]
  },
  {
   "title": "Speech Translation",
   "papers": [
    "barrachina99_eurospeech",
    "corazza99_eurospeech",
    "casan99_eurospeech",
    "reithinger99_eurospeech",
    "sugaya99_eurospeech"
   ]
  },
  {
   "title": "Topic Detection and Tracking",
   "papers": [
    "dharanipragada99_eurospeech",
    "jin99_eurospeech",
    "lowe99_eurospeech",
    "nakazawa99_eurospeech",
    "walls99_eurospeech"
   ]
  },
  {
   "title": "Speech Recognition - Adaptation",
   "papers": [
    "botinis99_eurospeech",
    "blomberg99_eurospeech",
    "gelin99b_eurospeech",
    "giuliani99_eurospeech",
    "hung99_eurospeech",
    "hong99_eurospeech",
    "harju99_eurospeech",
    "li99c_eurospeech",
    "li99d_eurospeech",
    "feng99_eurospeech",
    "naito99_eurospeech",
    "nguyen99_eurospeech",
    "ono99_eurospeech",
    "uebel99_eurospeech",
    "yuk99_eurospeech",
    "yu99_eurospeech",
    "obradovic99_eurospeech"
   ]
  },
  {
   "title": "Enhancements, Echo Cancellation, and Quality Measures",
   "papers": [
    "abouchakra99_eurospeech",
    "beaugeant99_eurospeech",
    "hussain99_eurospeech",
    "koutras99_eurospeech",
    "bielawski99_eurospeech",
    "shields99_eurospeech",
    "szarvas99_eurospeech",
    "saruwatari99_eurospeech",
    "sarikaya99_eurospeech",
    "unoki99_eurospeech",
    "veaux99_eurospeech",
    "ru99_eurospeech",
    "yen99_eurospeech"
   ]
  },
  {
   "title": "Speech and Noise 2",
   "papers": [
    "burshtein99_eurospeech",
    "gonzalezrodriguez99_eurospeech",
    "gustafsson99_eurospeech",
    "gaillard99_eurospeech",
    "koval99_eurospeech",
    "linhard99_eurospeech",
    "masgrau99_eurospeech",
    "mizumachi99_eurospeech",
    "nemer99b_eurospeech",
    "renevey99_eurospeech",
    "salavedra99_eurospeech",
    "svoboda99_eurospeech",
    "wijngaarden99_eurospeech",
    "wan99b_eurospeech",
    "zarubin99_eurospeech"
   ]
  },
  {
   "title": "Spoken Dialogue Systems",
   "papers": [
    "brondsted99_eurospeech",
    "chung99_eurospeech",
    "ipsic99_eurospeech",
    "ramaswamy99_eurospeech",
    "vromans99_eurospeech"
   ]
  },
  {
   "title": "Speech Perception",
   "papers": [
    "crouzet99_eurospeech",
    "damper99_eurospeech",
    "moreton99_eurospeech",
    "peperkamp99_eurospeech",
    "silipo99_eurospeech"
   ]
  },
  {
   "title": "Speech Recognition - Training",
   "papers": [
    "chang99c_eurospeech",
    "fonollosa99_eurospeech",
    "gelinhuet99_eurospeech",
    "huo99_eurospeech",
    "kemp99_eurospeech",
    "chesta99b_eurospeech",
    "lefevre99_eurospeech",
    "mcdonough99_eurospeech",
    "nogueirasrodriguez99_eurospeech",
    "rudzionis99_eurospeech",
    "tran99_eurospeech",
    "vair99_eurospeech",
    "willett99_eurospeech",
    "wu99d_eurospeech"
   ]
  },
  {
   "title": "Speech Analysis and Segmentation",
   "papers": [
    "avadhanulu99_eurospeech",
    "aguilera99_eurospeech",
    "martino99_eurospeech",
    "howitt99_eurospeech",
    "kawahara99_eurospeech",
    "lawlor99_eurospeech",
    "lee99c_eurospeech",
    "mckenna99_eurospeech",
    "salomon99_eurospeech",
    "seck99_eurospeech",
    "tuan99_eurospeech",
    "santen99_eurospeech",
    "zeljkovic99_eurospeech"
   ]
  },
  {
   "title": "Speech and Noise 3",
   "papers": [
    "alvarez99_eurospeech",
    "afify99_eurospeech",
    "tarcisio99_eurospeech",
    "hauptman99_eurospeech",
    "hakkinen99_eurospeech",
    "josifovski99_eurospeech",
    "kermorvant99_eurospeech",
    "milner99_eurospeech",
    "menendezpidal99_eurospeech",
    "nokas99_eurospeech",
    "schless99_eurospeech",
    "yoma99_eurospeech",
    "yamamoto99b_eurospeech",
    "yoma99b_eurospeech",
    "yao99_eurospeech",
    "zhao99b_eurospeech"
   ]
  }
 ],
 "doi": "10.21437/Eurospeech.1999"
}