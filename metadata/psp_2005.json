{
 "title": "ISCA Workshop on Plasticity in Speech Perception (PSP 2005)",
 "location": "Senate House, London, UK",
 "startDate": "15/6/2005",
 "endDate": "17/6/2005",
 "conf": "PSP",
 "year": "2005",
 "name": "psp_2005",
 "series": "",
 "SIG": "",
 "title1": "ISCA Workshop on Plasticity in Speech Perception",
 "title2": "(PSP 2005)",
 "date": "15-17 June 2005",
 "papers": {
  "flege05_psp": {
   "authors": [
    [
     "James E.",
     "Flege"
    ]
   ],
   "title": "Evidence for plasticity in second language (L2) speech acquisition",
   "original": "psp5_001",
   "page_count": 20,
   "order": 1,
   "p1": "1",
   "pn": "20",
   "abstract": [
    "This talk will summarize research examining the production and perception of phonetic segments in a second language (L2). It is by now well established that children (early learners) eventually show more native-like performance than adolescents and adults (late learners). However, the early-late differences observed in L2 speech research cannot be adequately explained as arising from maturational constraints (the critical period hypothesis). Close examination often reveals differences between early learners and L2 native speakers; conversely, some late learners show relatively native-like performance in the L2. Also, it is now clear that language use patterns exert a strong effect on L2 performance, both for early and late learners. Thus biology (in the form of maturational state at the time of first exposure to the L2) is not destiny! The talk will focus on the issue of inter-subject variability in L2 research. It will discuss the implications of this variability for models that attempt to account for age effects on L2 speech learning.\n",
    ""
   ]
  },
  "pallier05_psp": {
   "authors": [
    [
     "Christophe",
     "Pallier"
    ]
   ],
   "title": "Linguistic plasticity in adopted subjects",
   "original": "psp5_021",
   "page_count": 0,
   "order": 2,
   "p1": "21 (abstract)",
   "pn": "",
   "abstract": [
    "Does intensive exposure to a language early in life leave everlasting traces in the brain? In a previous study, we tested participants of Korean origin who had been adopted by French families while children, and stopped using their native language for 15 to 20 years. Behavioral tests showed that they had become unable to recognize Korean sentences among other sentences from several languages, and that they could not identify common Korean words. Moreover, they had similar brain activation patterns as native French speakers, when they listened to Korean, French, Japanese or Polish sentences.\n",
    "We will present data from follow-up experiments designed to assess their linguistics skills in Korean and French. First, we tested Korean adoptees in discrimination and identification experiments involving Korean consonants. Second, we evaluated their knowledge of French on two dimensions : phonotactics and grammatical gender. We found that their performance differed markedly from that of Korean speakers and was quite similar to that of native French listeners.\n",
    ""
   ]
  },
  "ramus05_psp": {
   "authors": [
    [
     "Franck",
     "Ramus"
    ],
    [
     "Gayaneh",
     "Szenkovits"
    ]
   ],
   "title": "Disruption and plasticity of the phonological system in developmental dyslexia",
   "original": "psp5_065",
   "page_count": 0,
   "order": 3,
   "p1": "65 (abstract)",
   "pn": "",
   "abstract": [
    "Many dyslexics manage to partly compensate for their difficulties to reach the lower normal range of reading ability. At the same time, psycholinguistic experiments show that their phonological deficit perdures in a qualitatively similar form throughout their life, evidencing very limited plasticity. I will present new experiments exploring the nature of the phonological deficit and further confirming its persistence in dyslexic adults. I will then discuss neurobiological data in search of insights into the reasons for such a remarkable absence of plasticity.\n",
    ""
   ]
  },
  "dorman05_psp": {
   "authors": [
    [
     "Michael",
     "Dorman"
    ],
    [
     "Anu",
     "Sharma"
    ]
   ],
   "title": "A critical period for the development of central auditory pathways",
   "original": "psp5_098",
   "page_count": 4,
   "order": 4,
   "p1": "98",
   "pn": "101",
   "abstract": [
    "A common finding in developmental neurobiology is that patterned stimulation must be delivered to a sensory system within a narrow window of time (a critical period) during development in order for that sensory system to develop normally. Experiments with congenitally deaf children fit with cochlear implants at different times during childhood have allowed us to establish the existence and time limits of a critical period for the development of central auditory pathways in humans. Using the latency of cortical auditory evoked potentials (CAEPs) as a measure we have found that central auditory pathways are maximally plastic for a period of about 3.5 years. If stimulation is delivered within that period CAEP latencies reach age-normal values within 3- 6 months following the onset of stimulation. However, if stimulation is withheld for more than 7 years, we find that plasticity in central auditory pathways is greatly reduced. In late-implanted children, CAEP latencies decrease significantly over a period of approximately one month following the onset of stimulation then remain constant or change very slowly over months or years.\n",
    "The loss of central auditory plasticity in congenitally deaf children implanted after age 7 is correlated with relatively poor development of oral speech and language skills. We suppose the link is, in fact, causal. Animal models suggest that primary auditory cortex may be functionally disconnected from higher-order auditory cortex, due to restricted development of inter- and intra-cortical connections, in late-implanted children (Kral et al., in press). This would account for late-implanted children who ëhearí by means of the implant but who experience very slow development of speech and language skills. Another aspect of plasticity that works against late-implanted children is the take over of higher-order auditory cortex by other function, e.g. vision. Lee et al. (2001) have shown via PET scans that after the age of 4-5 higher-order auditory cortex in congenitally deaf children is not ëquietí as would be expected in the absence of auditory stimulation, but rather is active, suggesting take over by other functions. The hypothesis of a decoupling of primary cortex from higher-order auditory and language cortex in children deprived of sound for a long period provides an account for the oral language-learning difficulties of children who receive an implant after the end of the critical period.\n",
    ""
   ]
  },
  "kuhl05_psp": {
   "authors": [
    [
     "Patricia K.",
     "Kuhl"
    ],
    [
     "Barbara",
     "Conboy"
    ]
   ],
   "title": "Infants' brain and behavioral responses to speech: implications for the critical period",
   "original": "psp5_102",
   "page_count": 0,
   "order": 5,
   "p1": "102 (abstract)",
   "pn": "",
   "abstract": [
    "This presentation focuses on recent research linking speech perception in infancy to later language development, as well as on new empirical data examining that linkage. Infant phonetic discrimination is initially language universal, but a decline in phonetic discrimination occurs for nonnative phonemes by the end of the first year. Exploiting this transition in phonetic perception between 6 and 12 months of age, we tested the hypothesis that the decline in nonnative phonetic discrimination is associated with native-language phonetic learning. We measured speech discrimination in 7.5 month-old infants using both event-related potentials (ERPs) as well as a standard behavioral measure of speech discrimination, and followed these infants to measure language abilities at 14-, 18-, 24-, and 30-months. Our results show: (a) a negative correlation between infants' early native versus nonnative phonetic discrimination skills, and (b) that native- and nonnative-phonetic discrimination skills at 7.5 months differentially predict future language ability. Better native-language discrimination at 7.5 months predicts accelerated later language abilities, whereas better nonnative-language discrimination at 7.5 months predicts reduced later language abilities. I will discuss the: (i) theoretical connection between speech perception and language development, and (ii) implications of these findings for the putative 'critical period' for phonetic learning.\n",
    ""
   ]
  },
  "cutler05_psp": {
   "authors": [
    [
     "Anne",
     "Cutler"
    ],
    [
     "James",
     "McQueen"
    ],
    [
     "Dennis",
     "Norris"
    ]
   ],
   "title": "The lexical utility of phoneme-category plasticity",
   "original": "psp5_103",
   "page_count": 5,
   "order": 6,
   "p1": "103",
   "pn": "107",
   "abstract": [
    "Exposure to an accented production of a particular phoneme in word contexts induces a shift in listeners' representations of the inclusiveness of that phoneme category. In a lexical decision experiment, the same ambiguous phoneme (between /f/ and /s/) replaced /f/ in 20 words ending with /f/ (e.g. carafe) for some listeners, while for others it replaced /s/ in 20 words ending with /s/. A subsequent phonetic categorisation experiment showed that the /f/ category had become more inclusive for the former group, while the /s/ category became more inclusive for the latter group (Norris, McQueen & Cutler, 2003). Importantly, exposure to the same ambiguous sound in a nonword context had no effect on category boundaries. The observed plasticity could not be accounted for by adaptation or contrast effects; Norris et al. argued that the plasticity occurred in the service of word recognition. Adjusting category boundaries allowed more rapid recognition of an unusual speaker's speech.\n",
    "Adjustment would be useless if it did not generalise to other words. In further research the same exposure conditions were used, but the phoneme categorisation test phase was replaced by a test phase involving another lexical task, capitalising on cross-modal identity priming effects: recognition of a written word is faster if the same word has just been heard. The critical words in this case were /f/-/s/ minimal pairs (e.g. knifenice). The spoken form ended with the ambiguous sound, and at issue was how much priming this form produced for recognition of the two written words. For listeners with initial-phase exposure to the ambiguous sound replacing /f/, more priming resulted for words ending with /f/ (knife), while for listeners with /s/- exposure more priming resulted for /s/-words. Thus the learning generalised from the 20 words used in the first phase to the rest of the lexicon, as predicted.\n",
    "This suggests that the utility of phoneme-category plasticity is indeed facilitation of word recognition. An open question is how stable listeners' lexical representations need to be for such rapid adjustment of category boundaries to occur. For instance, does less lexical stability imply less plasticity in phonemic representations? Less lexical stability might be expected, for example, in young children's lexical representations, in L2 learners' lexical representations, or in the representations for low-frequency words in the adult lexicon. In each case we might predict less induced adjustment of category boundaries.\n",
    ""
   ]
  },
  "gaskell05_psp": {
   "authors": [
    [
     "Gareth",
     "Gaskell"
    ],
    [
     "Nicolas",
     "Dumay"
    ]
   ],
   "title": "Plasticity in lexical competition: the impact of vocabulary acquisition",
   "original": "psp5_108",
   "page_count": 0,
   "order": 7,
   "p1": "108",
   "pn": "",
   "abstract": [
    "Learning a new word involves the acquisition of form and meaning, and at some point the integration of this information with existing knowledge in the learner's mental lexicon. In human speech perception, the latter \"lexicalisation\" stage is characterized by the engagement of the novel word in a competition process, in which it is able to inhibit identification of existing words (Gaskell & Dumay, 2003). Here we show that although the simple acquisition of a spoken form is swift, the impact of the novel item on lexical competition is more delayed and associated with sleep.\n",
    "Two participant groups were familiarised with a set of nonwords either in the morning or the evening. The effects of familiarisation were then tested immediately afterwards, and retested 12 and 24 hours later in order to examine the effects of delays with and without sleep. Our tests were designed to assess knowledge of the form of the novel items (2AFC judgements and free recall measures), and also the engagement of the items in lexical competition (a pause detection task, in which delayed responses indicate increased lexical competition; Mattys & Clark, 2002; Gaskell & Dumay, 2003).\n",
    "Tests of direct recognition immediately after exposure showed that both groups had learnt the novel items well, with further sleep-related improvements in recall at later time points. However, for the lexical competition test, there were strong delayed effects, correlated with the presence of sleep. Words learnt in the evening did not induce inhibitory effects in pause detection immediately after exposure, but did so after a 12-hour interval including a night's sleep, and this effect remained after 24 hours. Conversely, words learnt in the morning did not show such effects immediately or after 12 hours of wakefulness, but these effects emerged after 24 hours, following a night's sleep. We interpret these results in terms of the differing plasticity of two types of storage for novel items: a highly flexible episodic representation, and a more stable representation in which lexical competition occurs. This dissociation fits well with neural and connectionist models of learning in which sleep provides an opportunity for hippocampal information to be fed into the long-term neocortical store.\n",
    "s Gaskell, M. G., & Dumay, N. (2003). Lexical competition and the acquisition of novel words. Cognition, 89, 105-132. Mattys, S. L., & Clark, J. H. (2002). Lexical activity in speech processing: evidence from pause detection. Journal of Memory and Language, 47, 343-359.\n",
    ""
   ]
  },
  "moore05_psp": {
   "authors": [
    [
     "Roger",
     "Moore"
    ],
    [
     "Stuart",
     "Cunningham"
    ]
   ],
   "title": "Plasticity in systems for automatic speech recognition: a review",
   "original": "psp5_109",
   "page_count": 4,
   "order": 8,
   "p1": "109",
   "pn": "112",
   "abstract": [
    "Although this workshop is primarily aimed at examining changes in human speech perception, the authors submit that it may be illuminating to consider in parallel the degree to which current state-of-the-art automatic speech recognition (ASR) systems also change their behaviour over time. Therefore, for the benefit of those who are not familiar with ASR, this paper will provide a review of the computational mechanisms underlying contemporary ASR systems with a particular focus on their adaptive and learning behaviour. The paper will describe how ASR systems change dynamically in order (i) to accommodate new speakers, (ii) to handle unexpected user behaviour and (iii) to track and compensate for a constantly varying acoustic environment. A distinction will be made between 'supervised' and 'unsupervised' learning and attention will be paid to changes that occur in the acoustic model and the language model components of a typical ASR system. While many of these techniques are highly mathematical in nature, the paper will attempt to describe the underlying principles in behavioural terms in order to maximise the opportunity for inter-disciplinary exchange. Finally, it will be shown how such plastic behaviour, although valuable, is somewhat limited in its ability to allow an ASR system to operate robustly in a wide range of real-world situations. The paper will conclude by identifying potential areas where knowledge about plasticity in human speech perception might be important in the design of next-generation ASR systems and applications.\n",
    ""
   ]
  },
  "scott05_psp": {
   "authors": [
    [
     "Sophie",
     "Scott"
    ]
   ],
   "title": "The neural bases of plasticity in speech perception",
   "original": "psp5_156",
   "page_count": 0,
   "order": 9,
   "p1": "156 (abstract)",
   "pn": "",
   "abstract": [
    "Functional imaging studies, both fMRI and PET, have allowed us to elaborate the neural systems important in speech perception. In particular, these techniques have allowed us to integrate models of human speech perception in the framework of primate auditory cortex, where different functional and anatomical streams of processing can be identified. This neuroanatomical approach also has the potential of relating plasticity in speech processing to the known plasticity of auditory cortex. In this talk I will outline candidate neural systems involved in speech perception, and use this as a framework to address three kinds of plasticity in speech perception: after perceptual learning, after cochlear implantation, and after aphasic stroke. These different aspects of plasticity are associated with different aspects of neural change. Specifically, I will address the roles of streams of processing in speech plasticity, top down prefrontal modulation following cochlear implantation and right hemisphere changes after aphasic stroke.\n",
    ""
   ]
  },
  "iverson05_psp": {
   "authors": [
    [
     "Paul",
     "Iverson"
    ]
   ],
   "title": "Perceptual interference and learning in speech perception",
   "original": "psp5_195",
   "page_count": 0,
   "order": 10,
   "p1": "195 (abstract)",
   "pn": "",
   "abstract": [
    "Acquiring one's native language phoneme categories alters perception so that individuals become more sensitive to between- than within-category acoustic variation. Although this pattern of perceptual warping is adaptive for understanding one's native language, perceptual warping can also emerge from hearing and language disorders. This talk will discuss how these kinds of perceptual warpings can interfere with learning a new language as an adult, and interfere with the ability to make other adjustments to phonetic perception (e.g., when acclimitizing to a cochlear implant). I will give examples from recent work on the acquisition of English /r/-/l/ by Japanese adults, the perception of English /w/-/v/ by native Sinhalese speakers, and nativelanguage speech perception by adult cochlear implant patients. The results suggest that at least some aspects of the speech recognition difficulties of second-language learners and adult cochlear implant patients can be explained within the same theoretical model.\n",
    ""
   ]
  },
  "moore05b_psp": {
   "authors": [
    [
     "David R.",
     "Moore"
    ]
   ],
   "title": "Auditory learning: implications for speech perception",
   "original": "psp5_196",
   "page_count": 5,
   "order": 11,
   "p1": "196",
   "pn": "200",
   "abstract": [
    "Auditory learning resulting from repeated discrimination and other forms of auditory training has been associated with improved, broad-based outcome measures of speech perception. However, both the existence and nature of this association remain contentious. There seem to be several more or less distinct issues in the debate. The first is to what extent basic auditory processing skills contribute directly to speech perception and, in particular, whether poor auditory processing is causally connected with various language disabilities. A second issue is whether, by whatever mechanism, auditory training can improve speech perception. If it does, a third issue is the nature of that mechanism - sensory learning or a 'higher level' cognitive enhancement? A fourth issue is how we can optimise auditory learning for speech perception and how optimization protocols transfer within and between tasks.\n",
    "In this presentation I will report the results of a study that used phoneme discrimination training to achieve significantly enhanced phonological awareness (PhAB receptive indices) in mainstream school children aged 8- 9 years. I will then review a series of experiments in which we have used simple acoustic stimuli to examine how auditory training may be structured to achieve optimal learning. Several surprising results have emerged, including demonstrations of very rapid auditory learning, and efficient and effective learning without a physical difference between the training stimuli. Overall, the results suggest a positive effect of auditory training on speech perception, but also suggest that the training is acting at a level of processing I tentatively characterize as task-specific auditory attention. These findings meld well with those of others suggesting that speech, language and reading disabilities may relate more to poor auditory attention than to impaired sensory processing per se.\n",
    ""
   ]
  },
  "rosen05_psp": {
   "authors": [
    [
     "Stuart",
     "Rosen"
    ]
   ],
   "title": "Plasticity in speech perception: lessons from cochlear implants",
   "original": "psp5_201",
   "page_count": 0,
   "order": 12,
   "p1": "201 (abstract)",
   "pn": "",
   "abstract": [
    "Cochlear implants, which replace damaged inner ear transduction mechanisms with direct electrical stimulation of residual auditory nerve fibres, have progressed over the last 30 years from laboratory curiosities to standard clinical application in many thousands of users. They also provide an interesting testing ground, with important practical implications, for exploring the nature and limits of plasticity in the human speech perceptual system.\n",
    "For post-lingually deafened adults, implanted late in life, the primary focus of plasticity-related questions relates to adaptation to unusual representations of speech signals already well known to users. Of particular interest is the typical upward shift in spectral representation caused by incomplete electrode insertions into the higher turns of the cochlea where low frequencies are represented. Although a serious impediment to accurate perception initially, it appears that such shifts can be adapted to quite quickly. Studies concerning the speed and extent of adaptation, and ways to optimise the process, will be reviewed.\n",
    "Quite different questions arise in connection with implanted children with little or no auditory experience of speech. One striking finding is that earlier implantation appears to lead to better outcomes for speech and language, even for children under 3 years old. Results from relevant studies will be discussed in the light of the notion of a critical period for language development, and what kinds of biological and environmental factors limit eventual performance.\n",
    ""
   ]
  },
  "barry05_psp": {
   "authors": [
    [
     "Johanna",
     "Barry"
    ],
    [
     "Kathy",
     "Lee"
    ]
   ],
   "title": "Perceiving isn't only hearing: the role of speech production in the development of lexical tone perception",
   "original": "psp5_202",
   "page_count": 4,
   "order": 13,
   "p1": "202",
   "pn": "205",
   "abstract": [
    "It is generally accepted that speech production is guided by speech perception. However among profoundly hearing-impaired children, improvements in speech perception scores over time are overwhelmingly attributable to improvements in language and speech production (Blamey et al., 2001). These improvements mask the negative effects on perception due to deteriorating hearing.\n",
    "Lexical tone has an important contrastive role to play in Cantonese and to achieve full communicative competence it is important to be able to perceive and produce the six tones in the language. Pre-linguistically deafened implant users perform poorly at tone perception tasks suggesting the implant is minimally successful in aiding tone perception. By contrast, post-linguistically deafened adult users perform better at these tasks and it may be that linguistic experience plays an important role in aiding in perception.\n",
    "In this paper, we report on a study investigating the relationship between tone production and tone perception in young pre-linguistically deafened Cantonese-speaking children. The study comprised were two groups of children aged 4-11years, one group of implant users and one group of normally-hearing children. The tone production task involved naming 90 black-and-white line drawings of items which are familiar to children aged 4 years. Responses were recorded and analysed acoustically and results were summarized as 'size of tonal space'. For the tone identification task, children were shown two pictures presented side-by-side from the production task. These represented minimal pairs of different tone contrasts. One of the words illustrated on the picture card was presented through a speaker and the child was required to point to the corresponding picture.\n",
    "A positive correlation (p<0.05) was found between tonal space area and mean number of tones correctly identified. Most implant users had reduced tonal spaces relative to the normally-hearing children and performed poorly at the tone identification task. A few implant users however identified tones as successfully as their normally-hearing peers. They also had tonal areas that matched those used by the normally-hearing children. Since the implant does not provide much useable information about tone to users, it suggests that acquisition of tone production skills has helped these children develop reasonable tone perception abilities.\n",
    "These results suggest that to compensate for shortcomings in current cochlear implant technology, tone production training should be emphasised in speech habilitation programs.\n",
    "",
    "",
    "Blamey, P. J., et al. (2001). Relationships among speech perception, production, language, hearing loss, and age in children with impaired hearing. JSLHR, 44, 264-285.\n",
    ""
   ]
  },
  "schellenberg05_psp": {
   "authors": [
    [
     "Glenn",
     "Schellenberg"
    ]
   ],
   "title": "Sensitive periods for absolute pitch",
   "original": "psp5_206",
   "page_count": 1,
   "order": 14,
   "p1": "206",
   "pn": "",
   "abstract": [
    "I review research that attempts to demystify Absolute Pitch (AP), the rare ability to identify or produce a musical tone (e.g., middle C, concert A) in isolation. The traditional view is that listeners switch from absolute to relative pitch processing early in life: Those who continue to process absolute pitch have exposure to music lessons within a sensitive period (usually before age 7) as well as the appropriate genetic makeup. Genetics is also used to explain the higher prevalence of AP in Asian populations. As traditionally defined, AP cannot be found in listeners without musical training because they have no names for tones. We developed a new paradigm that allowed us to test untrained listeners with ecologically valid musical materials (excerpts from familiar instrumental recordings) and no naming requirements. On each trial, listeners heard two versions of the same excerpt: one at the original pitch level and one shifted upward or downward in pitch. Their task was to identify the original. Listeners included children and adults of European and Asian ancestry. The results indicate that: (1) untrained listeners have accurate memory for pitch, (2) accurate pitch memory is evident across development, (3) pitch memory is superior in Asian samples, and (4) the Asian advantage is due to non-linguistic environmental factors rather than genetics. In short, the so-called sensitive period for acquiring AP is not a sensitive period for pitch memory, but, rather, a sensitive period for attaching arbitrary labels to isolated auditory events.\n",
    ""
   ]
  },
  "saffran05_psp": {
   "authors": [
    [
     "Jenny",
     "Saffran"
    ],
    [
     "Erik",
     "Thiessen"
    ]
   ],
   "title": "Learning how to learn: the acquisition of stress-based word segmentation strategies by infants",
   "original": "psp5_248",
   "page_count": 4,
   "order": 15,
   "p1": "248",
   "pn": "251",
   "abstract": [
    "In order to successfully acquire language, infants must flexibly adapt their learning strategies to best fit the structure of the linguistic input they receive. While some learning strategies are likely applicable widely across languages, such as detecting the sequential probabilities of sounds, other learning strategies need to be tailored to particular native language structures, such as discovering word boundaries using lexical stress cues. We will review a series of studies examining how infants learn how to learn word boundary cues like lexical stress as a function of linguistic experience.\n",
    ""
   ]
  },
  "maye05_psp": {
   "authors": [
    [
     "Jessica",
     "Maye"
    ]
   ],
   "title": "The development of positional effects on phonetic discrimination",
   "original": "psp5_252",
   "page_count": 0,
   "order": 16,
   "p1": "252 (abstract)",
   "pn": "",
   "abstract": [
    "Infants' discrimination of speech sounds begins to show native language effects by 12 months of age. For example, Werker and Tees (1984) found that contrasts that are poorly discriminated by adult English speakers but well discriminated by English-learning 6-month-olds become difficult by 12 months. Infants also begin to show awareness of native language phonotactic patterns by 9 months (Jusczyk et al., 1993; Friedierici & Wessels, 1994). The present study examines the development of the combination of these effects: namely, phonotactically induced discrimination difficulty. Some discrimination difficulties arise on the basis of nativelanguage phonotactic constraints. For example, Dupoux and colleagues (1999) found that Japanese listeners have trouble discriminating *ebzo, which is phonotactically illegal in Japanese, from its legal counterpart, ebuzo (Dupoux et al., 1999). Maye (in prep) demonstrated that this difficulty is only true in a particular context: when the difficult part of the contrast is moved to a different context ([zo] and [uzo] were moved to the beginning of the sequence) Japanese speakers had no trouble discriminating. Likewise, a Japanese contrast that is difficult for American English speakers in word-medial position - gudo vs. guro - was discriminated well when the difficult element was moved to a different context. We presented 7- and 9-month-old infants from English-speaking and Japanese-speaking homes with the Japanese gudo~guro contrast. Infants were tested in a habituation paradigm, in which the dependent measure was looking time (LT). Trials were initiated when infants looked at a checkerboard stimulus presented on a computer monitor. On each habituation trial infants heard multiple tokens of gudo, produced by two female native Japanese speakers. Trials ended when the infant looked away from the visual stimulus for more than 2 seconds. When LT decreased to 50% of the initial LT, infants were presented with two test trials on which they heard multiple tokens of guro, produced by the same two speakers. At 7 months both language groups showed significant dishabituation (English, p < .01; Japanese, p < .01), but at 9 months only the Japanese infants discriminated the contrast (English, p = .13; Japanese, p < .05). These results demonstrate that the development of phonotactically based discrimination effects follows a similar time course to the development of both phonetic discrimination and phonotactic patterns. Future research will investigate the possibility that phonetic and phonotactic development arise from a single mechanism that tracks conditional probabilities of phonetic distributions.\n",
    ""
   ]
  },
  "bosch05_psp": {
   "authors": [
    [
     "Laura",
     "Bosch"
    ],
    [
     "Marta",
     "Ramon-Casas"
    ],
    [
     "Daniel",
     "Swingley"
    ],
    [
     "Núria",
     "Sebastián-Gallés"
    ]
   ],
   "title": "Bilingual input and vowel categorization processes: infant and young children data",
   "original": "psp5_253",
   "page_count": 4,
   "order": 17,
   "p1": "253",
   "pn": "256",
   "abstract": [
    "Previous research with pre-linguistic infants has revealed differences in the time course of the perceptual reorganization and categorization processes between infants growing up in monolingual and bilingual environments. For bilinguals, discrimination of target vowel contrasts, which reflect different amount of overlapping and acoustic distance between the two languages of exposure, suggested a U-shaped developmental pattern. A similar trend was observed in infants' ability to discriminate a voicing fricative contrast, present in only one of the languages in their environment. This temporary decline in sensitivity found at 8 months for vowel targets and at 12 months for the voicing contrast suggests the specific perceptual processes that bilingual infants develop in order to deal with their complex linguistic input. Once in the lexical stage, young children growing up in monolingual families have been shown to have wellspecified lexical representations, broadly suggesting a developmental continuity from pre-linguistic sound discrimination capacities and the phonetic detail used in the encoding of early words. However, similar studies concerning simultaneous bilingual acquisition have just begun to be undertaken. Recent research in our laboratory has analyzed young children's sensitivity to different vowel contrasts present in the representation of their first productive lexicon, comparing monolingual and bilingual populations. Children were tested using a visual fixation task: they were presented with sentences that contained nouns for known objects that could be correctly pronounced or mispronounced, while they were shown two pictures simultaneously, but only one was the referent of the utterance. Sensitivity to vowel mispronunciations (lower fixation time and higher orientation latencies to target pictures), would reflect the degree of phonetic detail present in the lexical representation. The mispronunciation effect can also be indicative of the contrastive categories that are functional in the children's phonology. Results from a first series of experiments show interesting differences between monolinguals and bilinguals (18- to 24-month-olds) for vowel contrasts that belong just to one of the languages of exposure. The role of age, number of words in the expressive vocabulary and amount of exposure to each of the languages in the environment, will be considered in the discussion.\n",
    ""
   ]
  },
  "behne05_psp": {
   "authors": [
    [
     "Dawn",
     "Behne"
    ],
    [
     "Yue",
     "Wang"
    ]
   ],
   "title": "Effects of musical and linguistic experience on hemispheric processing of prosody",
   "original": "psp5_022",
   "page_count": 0,
   "order": 18,
   "p1": "22 (abstract)",
   "pn": "",
   "abstract": [
    "Brain functions associated with pitch processing are generally believed to be distributed. Broadly speaking, whereas lexical tone is processed in the left hemisphere (Van Lancker & Fromkin, 1973), findings on intonation remain mixed, indicating either right hemisphere dominance (e.g., Blumstein & Cooper, 1974), or bilateral processing (e.g., Stiller et al., 1997). Cross-linguistic research shows that native tone language speakers do not process tones of another tone language in the left hemisphere, indicating that tones are processed in the left hemisphere as language only when they are linguistically meaningful (e.g., Gandour et al., 2003; Wang et al, 2004). Research has also addressed whether musical experience affects linguistic pitch perception. Although musicians and nonmusicians may not differ in hemispheric processing of pitch in their native language (e.g., van Lancker and Fromkin, 1978), behavioral measures of linguistic pitch perception may improve with musical experience (e.g., Burnham and Brooker, 2002). Does a listener´s combined previous linguistic and musical experience affect hemispheric processing of linguistic pitch? Is there a shift from more acoustic to more linguistic processing with experience? That Mandarin Chinese and Norwegian differ in their gradation of tone use provides a good measure of functional reliance on prosody and in this pilot study are used to address effects of musical and linguistic experience on hemispheric processing of linguistic pitch. The influence of nonlinguistic pitch experience on prosody perception was tested using a dichotic listening paradigm with two groups of native Norwegian listeners: musical trained and without musical training. To examine the effect of linguistic experience on prosodic perception, both groups of Norwegian listeners were tested with intonation patterns (statement or question) and lexical tones in both Norwegian and Mandarin. Results are discussed in terms of the functional and acoustic hypotheses (Gandour et al., 2003) which propose broad accounts for the mechanisms underlying hemispheric specialization of prosody.\n",
    ""
   ]
  },
  "clarke05_psp": {
   "authors": [
    [
     "Constance",
     "Clarke"
    ],
    [
     "Paul",
     "Luce"
    ]
   ],
   "title": "Perceptual adaptation to speaker characteristics: VOT boundaries in stop voicing categorization",
   "original": "psp5_023",
   "page_count": 4,
   "order": 19,
   "p1": "23",
   "pn": "26",
   "abstract": [
    "Recent research suggests that speech perception processes are flexible. For example, Norris et al. [Norris D, McQueen JM, & Cutler A (2003). Cognit. Psychol., 47, 204-238] demonstrated that listeners trained on stimuli containing ambiguous /s/-/f/ tokens subsequently showed an appropriate shift in their /s/-/f/ categorization boundaries based on the lexicality of the training stimuli. Moreover, recent research has shown that experience with a speaker's voice improves processing of that voice [Nygaard LC & Pisoni DB (1998). Percept. Psychophys., 60, 355- 376] and that native listeners appear to perceptually adapt to foreign-accented speech after only brief exposure [Clarke CM & Garrett MF (2004). J. Acoust. Soc. Amer., 116, 3647-3658]. These findings point to a perceptual system that is capable of learning about the variable features of speech. The present study extended this work by exploring changes in acoustic-phonetic criteria for stop category perception (voiced vs. voiceless) following brief exposure to a speaker. In a word-monitoring task, native English listeners heard sentences produced by a native English speaker in which all syllable-initial /t/ and /d/ segments were digitally modified to be atypical of native English pronunciation. Specifically, the /t/ voice onset times (VOTs) were reduced to a mean of 30 ms, and prevoicing was added to the /d/s. (Typical native English /t/s and /d/s are produced with long-lag and short-lag VOTs, respectively.) There were no other stop consonants in the sentences. Categorization of /t/ and /d/ was tested using a 5-token VOT continuum prior to exposure and again following 20, 40, and 60 sentences. As predicted, listeners' mean categorization boundary shifted to a lower VOT after exposure to the modified speech. Further, the boundary shift was evident after the first exposure block, containing less than two minutes of speech. No shift was found for a control group exposed to the unmodified sentences. These results suggest listeners' perceptual criteria for stop consonants can be adjusted to better match speakers' productions. They also indicate that this learning can occur when the key segment productions are embedded in full sentences, in addition to isolated words. Generalization of learning to the voicing distinction in other stops was also tested. These results will be discussed in terms of whether (a) perceptual learning of an abstract phonetic feature (i.e., voicing) is possible, or (b) each phonetic contrast must be learned on its own. [Work supported by NIDCD.]\n",
    ""
   ]
  },
  "cohen05_psp": {
   "authors": [
    [
     "Annabel",
     "Cohen"
    ],
    [
     "Elizabeth",
     "McFadden"
    ],
    [
     "Betty",
     "Bailey"
    ]
   ],
   "title": "Acquisition of musical grammar: is adolescence a sensitive period?",
   "original": "psp5_027",
   "page_count": 4,
   "order": 20,
   "p1": "27",
   "pn": "30",
   "abstract": [
    "Adolescents seem to soak up popular music with the ease of children acquiring language. The linguistic ability has long been associated with an early sensitive period of plasticity for language acquisition. Here we raise the possibility of a sensitive period for music acquisition, focusing in particular on adolescence. The notion of adolescence as a sensitive period for music is consistent with evidence for adolescent brain plasticity observed in longitudinal fMRI studies of Giedd, et al.(1999) and with the view of an adolescent critical period during which certain exposure has lifelong impact, such as in formation of addictions (Dahl & Spear, 2004). A Plasticity Framework for Music Grammar Acquisition (Cohen, 2000) was proposed to direct and accommodate our program of cross-sectional lifespan research on music style. The Framework assumes that exposure to music during a sensitive period readies the brain for particular musical structures. This ìsetting of parametersî of the grammar then has lifelong influence. Only musical styles that match the grammar can be readily encoded. Encoding enables recognition memory. Most of our empirical research entails two successive tasks: first a familiarity or preference rating of excerpts of popular music spanning 10 decades, and. second, a surprise immediate recognition test of the excerpts (with stylistically comparable foils). Differences in recognition performance arise as an interaction of the decade of popularity of the music and age cohort. Young children seem open to all styles whereas older age groups show priority for the music of their adolescence and early adulthood. These results are consistent with the notion of two sensitive periods for music acquisition: the first in early childhood for a basic musical grammar and the second in adolescence for vernacular stylistic and emotional aspects of the music are represented. We have recently taken a second empirical approach that compares different age groups on recognition of native music and non-native music and native and non-native language. Preliminary data which will be reported is consistent with the hypothesis of advantages for music during childhood and adolescence.\n",
    "s Cohen, A. J. (2000). Development of tonality induction: Plasticity, exposure, and training. Music Perception, 17, 437-459. Dahl, R.E. & Spear, L. P. (2004). Adolescent brain development: Vulnerabilities and opportunities. N.Y.: New York Academy of Sciences. Giedd, J. N. et al. (1999). Brain development during childhood and adolescence: A longitudial MRI study. Nature Neuroscience, 2, 861-863.\n",
    ""
   ]
  },
  "hawkins05_psp": {
   "authors": [
    [
     "Sarah",
     "Hawkins"
    ],
    [
     "Rachel",
     "Smith"
    ]
   ],
   "title": "Coding speech into useful structures: a neural basis for Polysp",
   "original": "psp5_031",
   "page_count": 0,
   "order": 21,
   "p1": "31",
   "pn": "",
   "abstract": [
    "This paper presents thinking concerning a neurological and psychoacoustic basis to a polysystemic approach to speech perception, Polysp, with particular reference to behavioural, physiological, and biochemical research on hippocampal function and plasticity in sound classification. Polysp proposes that the incoming signal is mapped probabilistically onto polysystemic prosodic-linguistic structures. Attention to fine detail is crucial for accessing the right structure and optimally adapting to new speakers; it sometimes allows auditory patterns to be understood without intermediate analytical stages. The model includes exemplar and abstract representation. Exemplar representation is plausible logically (speech is but one aspect of communicative actions) and psychoacoustically (signals seem to be analysed in the auditory pathway in terms of spectro-temporal excitation patterns (STEPs), which are assumed to be held in short-term memory; some may be passed to long-term memory). STEPs excite distinctive patterns of spectro-temporal receptive fields (STRFs) in the primary auditory cortex. STRFs reflect particular attributes of auditory stimuli, but, crucially, their response-specificity varies with task demands. Shifts occur within minutes, and can last hours. Such neocortical plasticity in responding to auditory stimuli might underpin the sensitivity of the perceptual system to changes in stimulus or task demands, including those which are typically 'controlled for' in categorical perception experiments (e.g. stimulus range, response set). This type of representation is proposed as the basis for storage, in associative networks, of all units derivable from speech, and for adaptation in ongoing speech perception.\n",
    "Abstraction (including classification) and long-term storage are hypothesized to include hippocampal processing together with neocortical plasticity. The process involves linking disparate cortical excitation patterns by building hierarchical indices to them in the hippocampus. The links made depend on how attention is directed. Links might be thought of as creating 'road maps', each map representing a distinct polysystemic structure, linguistic, socialaffective, cognitive, as attention dictates. For example, connectivity with the limbic system allows social-affective attributes of heard speech to be understood, as well as (or sometimes instead of) the so-called purely linguistic message. Critical for Polysp, novel stimuli have a disproportionate influence on hippocampal coding, and an entire structure can be excited from activation of just part of the hippocampal hierarchy. This model is compatible with others that are based on emergence of auditory objects e.g. Adaptive Resonance Theory and auditory scene analysis, but includes a fundamental role for fine phonetic detail. The paper includes predictions derived from these principles.\n",
    ""
   ]
  },
  "heeren05_psp": {
   "authors": [
    [
     "Willemijn",
     "Heeren"
    ]
   ],
   "title": "Perceptual development of a nonnative length contrast: Dutch adults learning Finnish /t-t:/",
   "original": "psp5_032",
   "page_count": 4,
   "order": 22,
   "p1": "32",
   "pn": "35",
   "abstract": [
    "How does the perception of a nonnative phoneme contrast develop? In answering this question we test two hypotheses: (i) Development through Acquired Distinctiveness: before learning a new phoneme contrast, differences between and within phoneme categories are hard to discriminate. Through training, the phoneme boundary is learned. (ii) Development through Acquired Similarity: before learning a new phoneme contrast, differences between and within phoneme categories are well discriminated. Due to training, only the phoneme boundary remains discriminable. The specific question that the present study dealt with is: how do Dutch listeners learn to perceive a length contrast from Finnish, although their native language does not use phonological length contrastively? On the basis of results from the literature and earlier findings within this project, we expect that the new contrast will be learned through Acquired Distinctiveness.\n",
    "A phoneme training study with 28 native Dutch listeners (mean age = 23 years) was run. Half of them completed the full procedure of pretesttraining- posttest, whereas the other half formed the control group and only participated in pretest and posttest. As materials, the Finnish pseudowords 'ata' (/AtA/) and 'atta' (/At:A/) were manipulated into phoneme continua by varying stop closure duration from /t/ to /t:/ in productions from six speakers (3 males, 3 females). In pretest and posttest, classification and 2IFC discrimination of one speaker's continuum were administered. Training consisted of classification (with feedback) of the other five speakers' continua until participants reached a score of 90% correct responses in two subsequent 350-trial training sessions. Control data from six Finnish listeners were also collected.\n",
    "Trained listeners learned to classify the new length contrast in a more native-like manner, whereas control listeners showed no such progress. With respect to 2IFC discrimination, neither trained listeners nor controls clearly showed the same perceptual sensitivity at the phoneme boundary as had been found for the Finnish controls. Trained listeners only tended to be more sensitive to the relevant region of the continuum. Thus, short-term training on a nonnative phoneme contrast may not successfully alter perceptual sensitivity. This means that nonnative listeners possibly need additional experience to become as sensitive to the phoneme boundary as native listeners are. However, comparison of Dutch and Finnish discrimination results leads us to the conclusion that learning most likely follows the first hypothesis: Acquired Distinctiveness.\n",
    ""
   ]
  },
  "johnson05_psp": {
   "authors": [
    [
     "Elizabeth",
     "Johnson"
    ],
    [
     "Amanda",
     "Seidl"
    ]
   ],
   "title": "Cross-linguistic differences in 6-month-olds' clause segmentation strategies",
   "original": "psp5_036",
   "page_count": 0,
   "order": 23,
   "p1": "36 (abstract)",
   "pn": "",
   "abstract": [
    "Past research has demonstrated that English-learning infants begin segmenting clauses from speech by six months of age (Nazzi et al., 2000). Clause boundaries are marked by many different acoustic cues, such as, pause, pitch reset, and final vowel lengthening. In the current study, we take a first step towards determining what acoustic information English-learners rely on most heavily to extract clauses from speech. More specifically, we ask whether 6-month-olds can extract clauses from speech when pausal cues to clause boundaries are removed. In addition, we investigate this same issue in Dutch-learners. By testing two different language-learning populations with similar stimuli, we are able to examine the possibility that language experience impacts clause segmentation strategies.\n",
    "In Experiment 1, the Headturn Preference Procedure was used to familiarize infants to the same string of words produced in two ways: as a prosodically well-formed clause (e.g. Rabbits eat leafy vegetables) and as a prosodically ill-formed clause (... rabbits eat. Leafy vegetables....). In Experiment 2, Dutch learning infants were familiarized to similar word strings produced in two ways: a prosodically well-formed clause (Koude pizza smaakt niet zo goed 'Cold pizza doesn't taste very nice') and a prosodically ill-formed clause (...Koude pizza. Smaakt niet zo goed... 'Cold pizza. Doesn't taste very nice'). In Experiment 1 English-learners preferred to listen to a passage containing this string of words produced as a prosodically well-formed clause over a passage containing this string of words produced as a prosodically ill-formed clause. In Experiment 2, similar results were obtained with Dutch-learners. Thus, we replicated findings showing that English-learning 6- month-olds segment clauses from speech, and we also extended these findings to Dutch-learners.\n",
    "In Experiments 3 and 4, we used the exact same stimuli and procedure used in Experiments 1 and 2, however the pauses associated with clause boundaries were removed. In this case, English- but not Dutch-learners continued to segment and remember clausal units in their native language better than non-clausal units. This suggests that Dutch-learners may be more reliant on pausal cues than English-learners. In a follow-up study, we are testing 6 month-old English-learners on the Dutch stimuli used in Experiments 2 and 4. If language experience shapes segmentation strategies, then English-learners may impose English-like segmentation strategies on the Dutch stimuli. In other words, in contrast to the Dutch-learning infants, the English-learners may extract the Dutch clauses even when pause cues to clause boundaries are absent.\n",
    ""
   ]
  },
  "kitamura05_psp": {
   "authors": [
    [
     "Christine",
     "Kitamura"
    ],
    [
     "Janice",
     "Gregory"
    ],
    [
     "Stacey",
     "Kuan"
    ]
   ],
   "title": "Infant discrimination of spectrally weighted speech",
   "original": "psp5_037",
   "page_count": 4,
   "order": 24,
   "p1": "37",
   "pn": "40",
   "abstract": [
    "In this study infant discrimination of spectrally weighted speech was examined using frequency responses typically found in adult hearing aids. We know infants perceive speech differently to adults and children, but we do not know much about how infants process speech with differing degrees and directions of spectral tilt. To date, the only research on spectral tilt discrimination comes from studies using complex tones found in music. Here we tested infants' ability to discriminate normal speech from speech with a positive or negative tilt (6dB and 9dB). In each of the eight groups of infants (N=190) half were aged 24 weeks and half aged 36 weeks. Each group of infants was tested with normal speech and either -6dB,+6dB,-9dB or +9dB. An habituation procedure was used in which the same stimulus was presented until there was an average 50% decrement in looking times over two trials (compared to the average of the first two trials). Once this criterion was met, two no-change control trials followed and then infants were presented with two test trials of the novel stimulus. The order of the habituation and novel stimulus was counterbalanced so that half the infants were habituated with speech with no spectral tilt (hereinafter called 0dB) and tested with one of the 4 spectral tilts (-6dB, +6dB, - 9dB, or +9dB), and for the other half of the infants this was reversed. The results showed no order effects. For the main analyses two (2)x2x2 (trialtype x age x tilt direction) ANOVAs were conducted one for 6dB and one for 9dB experiments. The dependent variable was the average looking time (in the two control versus the two novel trials). For the four 9dB experiments, there was a significant main effect for trial type (p=.000), and no other main effects or interactions. Paired t-tests showed both age groups could discriminate 0dB from positive and negative 9dB (all ps .8) while 24-week-olds did not discriminate 0dB and +9dB (p>.8) but showed a trend to discriminating 0dB and -9dB (p=.09). The emerging sensitivity to spectrally tilted speech will be discussed in relation to the relevant literature.\n",
    ""
   ]
  },
  "kushnerenko05_psp": {
   "authors": [
    [
     "Elena",
     "Kushnerenko"
    ],
    [
     "Eira",
     "Jansson-Verkasalo"
    ]
   ],
   "title": "Speech processing in prematurely born infants as indexed by event-related potentials",
   "original": "psp5_041",
   "page_count": 0,
   "order": 25,
   "p1": "41 (abstract)",
   "pn": "",
   "abstract": [
    "Deficient language development encountered in prematurely born children (Grunau et al., 2002) may be linked to abnormal auditory processing, as indexed by behavioural tests (Jennische & Sedin, 1999) and auditory event-related potentials (Jansson-Verkasalo et al., 2003). So far it is not well-known whether auditory processing differs already at an early age between children born preterm and their controls and whether these differences are acoustic or language based in nature. In the present study auditory event-related potentials (ERPs) were used for investigating auditory processing in infants born preterm and their controls to find possible differences between the two groups. The subjects were 15 children born very premature (GA < 32 at birth, age 12 months CA) and 13 age-matched controls. Hearing level of all the children was normal as indexed by brainstem auditory evoked potentials (BAEP) and otoacoustic emissions. Auditory ERPs were recorded in response to vowels (/e/ as a frequent, Finnish /S/ and Estonian /o:/ as infrequent). The acoustic change was smaller between the /e/ and Finnish /S/ than e and the Estonian /o:/. ERPs in response to deviant and standard stimuli were evaluated separately. The mismatch negativity (MMN, the response elicited by the frequent stimulus subtracted from that elicited by the infrequent stimulus) was also analyzed. Preliminary results showed that the N450 in response to frequent stimuli was smaller in prematurely born infants than that in the controls. Two types of mismatch response were observed - negative and positive, both, however, assumed to reflect automatic auditory discrimination. Preterm infants had an enhanced MMN in response to Estonian /o/ whereas controls had earlier, but not larger MMN in response to Finnish /S/. In addition, the following positive mismatch response was observed only in the control group. Our results showed that auditory processing of speech stimuli in children born preterm was deficient when compared to the controls as suggested by the diminished N450 and the MMN. Our study showed that speech stimuli elicited larger-amplitude N450 than the harmonic tones as found earlier in school-age children (Èeponiene et al., 2001). The positive mismatch response, has been previously suggested to reflect an automatic categorization of the stimulus (Friedrich et al, 2004). Therefore, the results suggest that preterm children do not development language-specific memory traces as precisely as the controls and continue to process stimuli more acoustically.\n",
    ""
   ]
  },
  "meunier05_psp": {
   "authors": [
    [
     "Christine",
     "Meunier"
    ],
    [
     "Robert",
     "Espesser"
    ],
    [
     "Cheryl",
     "Frenck-Mestre"
    ]
   ],
   "title": "The role of temporal information in vowel perceptual areas: a cross-linguistic study",
   "original": "psp5_042",
   "page_count": 0,
   "order": 26,
   "p1": "42 (abstract)",
   "pn": "",
   "abstract": [
    "Each language has its own more or less fixed inventory of phonemic units. Given this, each language differs as concerns the number of phonemes in its inventory. The study performed by Manuel & Krakow (1984) showed that the tolerance for variation in the production of a vowel is lesser in a language with a filled vocalic system. On the other hand, Maddieson & Wright (1991) observed very few variation in a low density system (three vowels). As concerns perception, the majority of studies have dealt with the more global problem of perceptual assimilation rather than with the specific issue of density. Flege and Munro (1994) argue in favor of a universal perceptual process, based upon a purely auditory component which is independent of any given phonological system. In recent studies (Meunier et al., 2003 & 2004), we observed that American English listeners had great difficulty in a vowel identification task, while French and Spanish had not. It has been suggested that English listeners could not find the cues they need to take their decision: French and Spanish system are mainly characterized by F1 and F2 values while, in English system, stress and duration are also relevant. In the present study, we compared the categorisation ability of English and French listeners. Three continua of F1 and F2 values have been synthesized (/i/-/a/, /a/-/u/, and /u/-/i/). Each stimulus was presented with two durations: short (200ms) and long (400ms). Stimuli were presented to French and English subjects which made and identification task (Reaction Times were measured). We observed that the perception areas of French and English are similar: a very large category for /a/ and smaller for /i/ and /u/. /i/ and /u/ showed abrupt boundaries while /a/ one were larger. For both languages, RT increased at the boundary of the categories, and were longer for /a/. This reflect that /a/ seem to be a vowel with fuzzy boundaries for both languages. For English subjects (but not for French ones) /a/ responses were different according as the vowel is short or long. To summarize, the size of English and French perceptual areas seem to be similar. We have to confirm, with a low density system, that this similarity can be due to density. The experiment have also pointed out that, for English listeners, duration change the boundary, and so the size, of vowel categories.\n",
    ""
   ]
  },
  "mora05_psp": {
   "authors": [
    [
     "Joan C.",
     "Mora"
    ]
   ],
   "title": "Lexical knowledge effects on the discrimination of non-native phonemic contrasts in words and nonwords by Spanish/catalan bilingual learners of English",
   "original": "psp5_043",
   "page_count": 4,
   "order": 27,
   "p1": "43",
   "pn": "46",
   "abstract": [
    "In current models of second language (L2) phonological acquisition such as Flege's Speech Learning Model (SLM) or Best's Perceptual Assimilation Model (PAM), correct category formation for L2 sounds is largely conditioned by the ability of L2 learners to perceive non-native phonological contrasts, which is viewed as a necessary attainment in the development of phonological competence. This paper reports on the results of a study designed to investigate the effect of L2 lexical knowledge on the perception of non-native phonemic contrasts. An AX minimal-pair auditory discrimination task was used to test the ability of a group of bilingual Spanish/Catalan advanced learners of English (N=74) to perceive 9 phonemic contrasts in English words and nonwords (a control group of 10 English native speakers provided base-line data). The test contained 144 aural stimuli consisting of 108 minimal pairs and 36 distractors, i.e. two realizations (non-identical tokens) of the same word, 50% of which were English nonwords. The non-native phonemic contrasts were chosen according to their relative difficulty for Spanish/Catalan learners of English, as predicted by Best's perceptual assimilation model (PAM). Real-word pairs and nonword pairs were matched with respect to phonological contrast and phonetic environment, randomized and presented to listeners with an interstimulus interval of 1 second between members of a word pair and 3 seconds between word pairs. A lexical knowledge post-test was used to assess the subjects' knowledge of the English words and nonwords in the auditory discrimination task. The subjects were asked to listen to a token of a word or nonword and decide whether they knew the item by ticking one of three options: 'yes', 'no', 'not sure'. Their knowledge of all items was further tested by means of a multiple-choice vocabulary translation test. An analysis of the scores on the perception and lexical knowledge tests revealed that advanced learners of English were more successful in perceiving non-native phonemic contrasts in known words than in nonwords. It was only with those phonemic contrasts that were harder to perceive, however, that the subjects' scores on the auditory discrimination task produced statistically significant differences between known words and nonwords. These results suggest that lexical knowledge is a significant factor conditioning the perception of nonnative phonemic contrasts, a factor to be taken into account when assessing learners' perceptual ability in the L2.\n",
    ""
   ]
  },
  "nielsen05_psp": {
   "authors": [
    [
     "Kuniko",
     "Nielsen"
    ]
   ],
   "title": "Generalization of phonetic imitation across place of articulation",
   "original": "psp5_047",
   "page_count": 4,
   "order": 28,
   "p1": "47",
   "pn": "50",
   "abstract": [
    "Previous studies have shown listeners' ability to remember fine phonetic details (e.g., Mullennix et al., 1989), providing support for the episodic view of speech perception. The imitation paradigm (Goldinger, 1998, Shockley et al., 2004), in which subjects' speech is compared before and after they are exposed to target speech (= study phase), has shown that subjects shift their production in the direction of the target, indicating not only episodic memory in speech perception but also the close tie between speech perception and production. The purpose of the current study is to determine whether and how this imitation effect is generalized to new stimuli, as well as to investigate the locus of the effect. In the study phase, subjects listened to a word containing items with initial /p/ and /t/ that had extended VOT. In the pre- and post-study phases, subjects produced words from lists including 1) the words in the study list, 2) the segments /p/ and /t/ in new words, and 3) the segment /k/, which did not occur during study. Initial results replicated Shockley et al. (2004), showing the imitation effect of extended VOT: subjects produced longer VOT for post-study than pre-study. Furthermore, the results indicated the trend that the modeled feature for one segment (/p/) was generalized to a new segment (/k/). The specific source of this imitation effect is still unknown. That is, when a subject shifts production of a particular sound in a particular word, it is uncertain whether the shift has been caused by change in the speaker's register (e.g., 'speak more carefully'), or by change in lexical/phonetic representations. If the shift in production is indeed due to some change in lexical/phonetic representation, what would be the size of linguistic unit influenced by the effect? If the imitation effect is truly due to episodic memory, only the manipulated variable (in this case, VOT) should be affected. On the other hand, if we observe changes in other variables (e.g., the following vowel, entire word), the imitation effect is more likely to be due to some more global aspect of speech production. The implications of the data for exemplar versus abstract views of speech perception will be discussed.\n",
    ""
   ]
  },
  "nygaard05_psp": {
   "authors": [
    [
     "Lynne",
     "Nygaard"
    ],
    [
     "Sabrina",
     "Sidaras"
    ],
    [
     "Jessica",
     "Duke"
    ]
   ],
   "title": "Perceptual learning of accented speech",
   "original": "psp5_051",
   "page_count": 0,
   "order": 29,
   "p1": "51 (abstract)",
   "pn": "",
   "abstract": [
    "The present paper discusses research investigating adult listeners' perceptual learning of talker- and accent-specific properties of spoken language. Traditionally, neural and behavioral plasticity in speech perception was assumed to end with childhood and/or adolescence. However, recent research suggests that adult listeners are sensitive to a variety of talker-specific properties of speech and that perceptual processing of speech changes as a function of exposure to and familiarity with these properties. Mechanisms involved in perceptual learning and change in speech processing were examined by evaluating the effects of exposure to foreign accented speech. Adult native speakers of American English were asked to transcribe English words produced by six native Spanish-speaking adults (three male and three female). Prior to this transcription task, listeners were exposed to a onehour training session with items produced by either the same Spanish-accented talkers as heard during the transcription task, a different set of six Spanish-accented talkers, or a different set of native American English talkers. A control condition with no training was also included. The results showed that listeners were most accurate when transcribing Spanish-accented speech at test if they had been exposed to Spanish-accented speech during training. In addition, listeners were more accurate when transcribing items produced by the specific Spanish-accented talkers they had heard during training than when transcribing items produced by different Spanish-accented talkers. Additional experiments examining the role of training and stimulus form on perceptual learning and generalization will also be reported. The results suggest that even brief exposure to both talker-specific and accent-specific properties of speech significantly changes the way in which adult listeners perceive speech. Adult listeners perceptually adapt to idiosyncratic characteristics of individual talkers as well as to systematic properties of non-native speech patterns suggesting that listeners are sensitive to both talker-specific and accent-general regularities in spoken language. These findings provide support for behavioral and representational plasticity in speech perception in adult listeners.\n",
    ""
   ]
  },
  "panneton05_psp": {
   "authors": [
    [
     "Robin",
     "Panneton"
    ],
    [
     "Megan",
     "McIlreavy"
    ]
   ],
   "title": "Developmental differences in infants' attention to utterance duration",
   "original": "psp5_052",
   "page_count": 4,
   "order": 30,
   "p1": "52",
   "pn": "55",
   "abstract": [
    "A considerable amount of research has focused on infants' attention to speech typically directed to them by adult caretakers (infant directed speech or IDS). Although IDS differs from adult-directed speech (ADS) in many ways, primary focus has been on pitch characteristics; IDS is typically higher in absolute pitch, and more variable in pitch (exaggerated pitch excursions; wider pitch ranges). However, IDS is also typically slower in temporal form, largely because many of its voiced segments are stretched over time. Little attention has been paid to this characteristic of IDS and its affect on infant attention.\n",
    "In these experiments, we took IDS-Normal utterances and either attenuated segmental durations (i.e., decreased voiced segments, producing IDS-Fast) or augmented segmental durations (i.e., increased voiced segments, producing IDS-Slow); in both cases, the relative pitch characteristics remained unchanged. We then tested groups of 4- and 8-month-old infants in a serial attention protocol with either IDS-Normal vs. IDS-Fast, or IDS-Normal vs. IDS-Slow recordings. Two measures of attention were recorded: fixation of a visual target and heart rate activity. Visual fixation was recorded because looking at the target (a colorful bullseye pattern) was contingently yoked to hearing the recordings; that is, infants heard one of two speech patterns as long as they looked at the target. Heart rate activity was simultaneously recorded because increases in heart period reflect heart rate (HR) deceleration, an indicator of sustained attention. We expected no differences in average looking times or HR as a function of speech type if segmental duration does not influence infants' attention to IDS.\n",
    "In the first experiment, both 4-month-olds (n = 19) and 8-month-olds (n = 18) heard ID-Normal and IDFast speech. The results showed that the younger infants looked significantly longer to ID-Normal; older infants showed no difference in visual attention. Interestingly, both ages showed greater degrees of HR deceleration (sustained attention) to ID-Normal. In the second experiment, both 4- month-olds (n = 16) and 8-month-olds (n = 20) heard ID-Normal and ID-Slow speech; the results showed equal visual attention across groups, but increased sustained attention (HR deceleration) to ID-Slow in the 4-month-olds.\n",
    "These data suggest that the temporal features of IDS affect infants' attention, with longer segmental durations (slower speech) increasing attention more in younger than older infants. Attending more to slower speech may reflect younger infants' increased interest in highly emotive speech, but may also act to increase certain aspects of language learning such as native phonemes.\n",
    ""
   ]
  },
  "scobbie05_psp": {
   "authors": [
    [
     "James",
     "Scobbie"
    ]
   ],
   "title": "Interspeaker variation as the long term outcome of dialectally varied input: speech production evidence for fine-grained plasticity",
   "original": "psp5_056",
   "page_count": 4,
   "order": 31,
   "p1": "56",
   "pn": "59",
   "abstract": [
    "The linguistic systems used by native speakers are influenced by and reflect the input received; in particular during childhood from caregivers and peers. Different children in the same speech community (even siblings) can be expected to display individual flexibility reflecting input differences, cognitive differences and the demands of different interlocutor networks. Speakers can, however, also be expected to make recurrent functionally-oriented decisions, both in novel situations of contact and through mastering local systems (linguistic and sociolinguistic) which provide normative solutions to interacting with different groups.\n",
    "Phonetic and phonological production data from 12 mutually-acquainted native young adult Shetlanders with differing input due to different parental backgrounds have been analysed Their parents were either Shetlandic, from elsewhere in Scotland, or from England. The fine phonetic detail of these speakers indicated plasticity in the formation of systems of contrast.\n",
    "For example (Scobbie, in press), the VOT cue to the stop voicing contrast was found to demonstrate a high degree of interspeaker variation. The speakers' 'voiceless' stops were spread across the VOT continuum, but the distribution was non-arbitrary. First, it reflected experience: vernacular Shetlandic contrast systems (prevoiced /bdg/ vs. short lag /ptk/) were generally found in speakers whose parents were Shetlanders. Second, there was a functionally stable encoding of the contrast: a gradient inverse relation between the relative number of prevoiced tokens produced by an individual and their mean aspiration duration. However, two speakers used high numbers of prevoiced stops yet also long lag aspiration, a linguistically highly marked system albeit one which could be expected to function successfully both in general perceptual terms and specifically in the context of Shetlandic inter-speaker variation. Further ongoing analysis of vowel duration (a cue to post-vocalic consonant voicing) appears similarly plastic, showing similar patterns of fine-grained interspeaker variation. This new data will be presented.\n",
    "Individuals were not limited to the use of unmarked oppositions, and the fine-detail specification of individuals' systems (as opposed to population-wide tendencies) do not provide evidence for a small inventory of discrete universal features/categories. Indexical and phonological cues were simultaneously present. The relevance of the results to exemplar models of the mental lexicon and the implications for the phonetics / phonology interface are discussed.\n",
    ""
   ]
  },
  "surowiecki05_psp": {
   "authors": [
    [
     "Vanessa",
     "Surowiecki"
    ],
    [
     "David",
     "Grayden"
    ],
    [
     "Richard",
     "Dowell"
    ],
    [
     "Graeme",
     "Clark"
    ],
    [
     "Paul",
     "Maruff"
    ]
   ],
   "title": "Changing perceptions: the influence of visual speech information to auditory perceptions and working memory in children using a cochlear implant and children with normal hearing",
   "original": "psp5_060",
   "page_count": 0,
   "order": 32,
   "p1": "60",
   "pn": "",
   "abstract": [
    "The changing perceptual skills of paediatric cochlear implant (CI) users provide a unique opportunity to examine the process and development of speech perception in an altered auditory system. Habilitation specialists disagree as to whether children using a CI should develop their speech perception skills under auditory-alone (AA) or audiovisual (AV) conditions. Audiovisual information aids auditory perception by reducing some of the ambiguity in the speech signal, and in doing so reduces the processing load on working memory. Children using a CI continually process a degraded auditory signal. The influence of visual speech information to auditory perceptions and working memory was investigated in children using a CI and children with normal hearing. Across two test periods, comparisons were made between the children's perception of synthetic speech stimuli under AA and AV listening conditions. The speech stimuli varied along a voiced plosive continuum, /badaga/, and were presented AA or paired with visual /ba/ and /ga/ recordings in congruent and incongruent AV test conditions. Under each condition, children were asked to indicate what sound they heard by selecting a response button. Results were compared to performance on nonverbal measures of working memory. The first test period included 16 children using a Nucleus cochlear implant with an average 4.75 years device experience. Children with normal hearing were matched to the CI group on age, gender and Performance IQ. Children using a CI were less categorical in their perception of the AA stimuli and produced more variable responses. All children perceived the auditory stimuli more consistently when paired with visual speech cues. Results indicated that children using a CI benefited more from the visual speech information than did the comparison group. Children with better working memory skills also generally perceived the exemplar auditory stimuli at higher response rates, were more likely to select the response that matched congruent AV information and were less influenced by incongruent visual speech cues. Preliminary results are now available for a follow-up assessment conducted 24 months later, including 9 children from the CI group and 4 children from the comparison group. Few AA or AV responses differed between the first and second assessment, although at the second assessment the children's AA responses were more categorical for some stimuli. With further auditory experience, children using a CI appeared to respond more like their hearing-peers to AV presentations. The implications of these findings to habilitation practices will be discussed.\n",
    ""
   ]
  },
  "taitelbaumswead05_psp": {
   "authors": [
    [
     "Riki",
     "Taitelbaum-Swead"
    ],
    [
     "Minka",
     "Hildesheimer"
    ],
    [
     "Liat",
     "Kishon-Rabin"
    ]
   ],
   "title": "Acoustic cue weighting in the perception of initial voicing in Hebrew speaking children and adults",
   "original": "psp5_061",
   "page_count": 0,
   "order": 33,
   "p1": "61 (abstract)",
   "pn": "",
   "abstract": [
    "Studies have demonstrated that while the primary cue for perceiving initial voicing is Voice Onset Time (VOT), listeners may also use secondary cues such as F1 transition and the initial burst. Furthermore studies suggested that the weight assigned to each of these cues for auditory perception may depend on many variables. Therefore it is not possible to directly apply the results stated above to other languages, such as Hebrew, for which different acoustics was demonstrated (even for phonemes common with other languages e.g., ba/pa). In Hebrew, for example, the voiced plosives have a substantial voicing lead and its vowel system includes only five short vowels which may influence the transitions, such as, F1 cutback. This suggests that the cues for voicing perception and the relative weighting may be different in Hebrew from those reported in English. The purpose of the present study was twofold: (a) to examine the relative weighting of some acoustic cues to the perception of initial voicing in Hebrew in different age groups and (b) to compare the influence of vowel context on the perception in children and adults. Three groups of children aged 4-5, 6-7 and 9-10 years and one group of adults, all with normal hearing, participated in this study. Stimuli consisted of naturally produced pairs of meaningful words (/bar/-/par/, /bil/-/pil/). The relative weighting of three cues to the perception of voicing were evaluated: VOT, initial burst, and the transitions. From each pair, the burst and the transitions parts (vowel included ) were extracted, and reconcatenated to produce four new stimuli combinations on a VOT continuum (from -40 to +40 msec). Results show that: (1) phoneme boundaries (PB) of children were least affected by the different stimuli compared to adults, suggesting that children rely more on the VOT cue. (2) the steepness of the slope of the identification curves increased as a function of age, indicating a progressive development in categorization of voicing during childhood, and, (3) children showed different identification functions for the different vowels whereas adults showed similar functions for /a/ and /i/. Thus, the relative weighting of the cues for initial voicing appears to be different in Hebrew than for English. Furthermore, children seem to rely more on temporal than on spectral cues. Finally it is hypothesized that children do not complete the normalization process for speech perception and therefore require separate auditory processing for each consonant- vowel combination.\n",
    ""
   ]
  },
  "wayland05_psp": {
   "authors": [
    [
     "Ratree",
     "Wayland"
    ],
    [
     "Bin",
     "Li"
    ]
   ],
   "title": "Training native Chinese and native English listeners to perceive Thai tones",
   "original": "psp5_062",
   "page_count": 4,
   "order": 34,
   "p1": "62",
   "pn": "65",
   "abstract": [
    "This study evaluated two training procedures that might be used to increase native English (NE) and native Chinese (NC) listeners' ability to discriminate the mid vs. low tone contrast in Thai under two different inter-stimulus-interval (ISI) conditions (i.e., 500-ms and 1500-ms). Participants were assigned to receive training using either a two-alternative forced-choice identification (ID) procedure or a categorial same/different discrimination (SD) procedure. Preliminary results from 14 NE and 14 NC listeners showed significant increases in percent correct identification in both ISI conditions for NE listeners who received a two-alternative forced choice training. However, performance of the NE listeners who were administered a same/different categorial discrimination training procedure did not improve significantly after training in either ISI condition. On the contrary, NC listeners seemed to have benefited equally from the two training procedures. That is, a comparable and significant improvement in Thai tone discrimination were observed among NC participants in both the ID and the SD groups. Additionally, no effect of ISI was observed in either group of listeners either before or after training. These results suggest that the identification training procedure is superior to the same/different discrimination method in promoting phonological representation of Thai tones among English listeners, but not among Chinese listeners. However, more data, which is being collected, is needed to confirm these results. If upheld, these results suggest that prior exposure to tones may facilitate the formation of foreign tones, thus causing the differential benefit of the two training procedures to be minimized.\n",
    ""
   ]
  },
  "yasin05_psp": {
   "authors": [
    [
     "Ifat",
     "Yasin"
    ],
    [
     "Dorothy",
     "Bishop"
    ]
   ],
   "title": "An objective measure of hemispheric differences in processing dichotic meaningful words and meaningless pseudowords as indexed by mismatch negativity",
   "original": "psp5_063",
   "page_count": 0,
   "order": 35,
   "p1": "63 (abstract)",
   "pn": "",
   "abstract": [
    "One of the main advantages of using a dichotic listening task to investigate hemispheric lateralisation to auditory stimuli is that since there is ipsilateral suppression of the auditory pathways (Bryden, 1988), any lateralised response is more pronounced. Dichotic listening tasks using speech sounds have often shown a right-ear advantage, indicating left-hemisphere dominance (Schwartz and Tallal, 1980). An objective measure of brain laterality can be obtained by measuring auditory evoked potentials and obtaining a measure of the mismatch negativity (MMN); a cortical brain response elicited by a deviant sound presented occasionally in a sequence of standard sounds. The aim of this study was to investigate the extent of left and right hemisphere contribution to MMN strength, for both meaningful words and meaningless pseudowords, using a dichotic presentation. The task was a dichotic version of a paradigm devised by Shtyrov and Pulvermuller (2002). The meaningful word and meaningless pseudoword stimuli were bisyllables with identical first syllables and different second syllables. 10 normal-hearing, right-handed subjects were presented with a repeating standard (meaningless pseudoword) to both right and left ears at a probability of 0.8, with a deviant (a meaningful word, OR meaningless pseudoword) presented at a probability of 0.1 in the left OR right ear. The MMN was calculated by subtracting the response of the standard stimulus from that of the deviant stimulus after the onset of the second syllable. Results indicate that left hemispheric activity is greater for a meaningful word (baby or lady) than for a meaningless pseudoword (bady, bagy or laby, lagy). Furthermore, a left-right hemispheric difference was seen only when the second syllable created a meaningful word.\n",
    ""
   ]
  },
  "yuen05_psp": {
   "authors": [
    [
     "Ivan",
     "Yuen"
    ]
   ],
   "title": "Local context effect on downtrend normalisation in Cantonese",
   "original": "psp5_064",
   "page_count": 1,
   "order": 36,
   "p1": "64",
   "pn": "",
   "abstract": [
    "Downtrend results in different f0 values for a phonologically equivalent accent/tone (Pierrehumbert 1979, Prieto 1996, 1998, Shih 2000). In a late-occurring position-in-utterance, the phonetic f0 value of an accent is realised lower than in an early-occurring position-in-utterance. In equating different f0 values to the same perceived prominence of an accent, it has been shown that listeners compensate for downtrend with regard to a global reference line (Pierrehumbert 1979, Gussenhoven and Rietveld 1988, Terken 1991, Gussenhoven et al 1997). The current study investigated normalisation of downtrend in a tone language, which allows us to examine which f0 targets in the f0 contour can be employed as a reference frame in normalising for downtrend.\n",
    "A perception experiment was conducted on Cantonese tones to test whether the listeners compensate for downtrend in tone perception and how different f0 values in different positions-inutterance can be equated to the same tone. Cantonese has two tones - High Mid and Mid Low, which are contrastive in terms of f0 height. With downtrend, a late-occurring High Mid tone is realised with a lower f0 level than an early-occurring High Mid tone. The same is also found for a Mid Low tone. On account of downtrend, it is likely that a lateoccurring High Mid tone will exhibit an f0 value similar to an early-occurring Mid Low tone. Under such scenario, how can the listener decide whether an f0 value belongs to a High Mid tone or a Mid Low tone?\n",
    "In the first part of the experiment, subjects were instructed to identify a lexical word occurring in different positions-in-utterance. Results showed that a given f0 value in a late-occurring position-inutterance led to more High Level tone responses than in an early-occurring position-in-utterance.\n",
    "The second part of the experiment was concerned with whether local f0 context was used as a frame of reference in normalising for downtrend. Results showed that more Mid Low tones were perceived in a late-occurring utterance position when the f0 value of an adjacent tone was raised than in the original late-occurring utterance position without any modification of the local f0 context.\n",
    "The results of the experiment will be discussed in terms of time-dependent downtrend and the primacy of local f0 context as a frame of reference in equating different phonetic f0 values to the same phonological tone in Cantonese.\n",
    ""
   ]
  },
  "vierudimulescu05_psp": {
   "authors": [
    [
     "Bianca",
     "Vieru-Dimulescu"
    ],
    [
     "Philippe",
     "Boula de Mareüil"
    ]
   ],
   "title": "Contribution of prosody to the perception of a foreign accent: a study based on Spanish/Italian modified speech",
   "original": "psp5_066",
   "page_count": 4,
   "order": 37,
   "p1": "66",
   "pn": "68",
   "abstract": [
    "In this article, we try to quantify the contribution of prosody (timing and melody) to the perception of a foreign accent, by using a prosodic transplantation paradigm. For the reported experiment, a dozen sentences which are spoken in almost the same way in Italian and in Spanish were recorded: e.g. ha visto la casa del presidente americano ('(s)he/you saw the American president's house'). They were read by 3 Spanish monolinguals, 3 Italian monolinguals and 3 Spanish/Italian bilinguals (once in the Spanish way, once in the Italian way). Prosody crossings were performed after an automatic segmentation of the sentences into phonemes: in order to copy the prosodic parameters of an utterance onto another one, a script was written for the PRAAT software, which enables speech handling and re-synthesis (extraction and then transplantation, phoneme by phoneme, of duration and then pitch). In addition to crosslanguage combinations, we crossed each original voice with the prosody of the same language of another speaker, to make sure that the impression of foreign accent comes from signal manipulation. Perceptual tests were run on 100 resulting stimuli, with three samples of individuals: for native listeners (20 Spanish and 20 Italian), the task consisted of judging whether the utterance was 'Spanish', 'Spanish with an Italian accent' (i.e. Spanish spoken by an Italian), 'Italian with a Spanish accent' (i.e. Italian spoken by a Spaniard) or 'Italian'; for 20 French listeners, the task consisted of telling if the mother tongue of the speakers they heard was 'very probably Spanish', 'probably Spanish', 'probably Italian' or 'very probably Italian'. It turned out that the original stimuli were well identified by the three groups of listeners, even though Italian subjects were inclined to answer 'Italian with a Spanish accent' as soon as the voice or prosody was Spanish Ñ and symmetrically for Spanish subjects. The artefacts introduced by the acoustic manipulation of the stimuli did not affect the speakers' origin identification. For cross-language crossings, results suggest that prosody outweighs segmental cues in the perception of Spanish/Italian foreign accent. They were analysed statistically, and the corpus was investigated acoustically. In particular, the duration ratio between stressed and unstressed syllables is 1.4 for the Italian language, and only 1.1 for the Spanish language. In conclusion, all other things being equal and at least for bilinguals, prosody and particularly rhythm plays a major role in the perception of Spanish/Italian foreign accent.\n",
    ""
   ]
  },
  "clayards05_psp": {
   "authors": [
    [
     "Meghan",
     "Clayards"
    ],
    [
     "Richard",
     "Aslin"
    ],
    [
     "Michael",
     "Tanenhaus"
    ]
   ],
   "title": "Weighting and waiting: experience mediated cue integration in spoken word recognition",
   "original": "psp5_069",
   "page_count": 0,
   "order": 38,
   "p1": "69 (abstract)",
   "pn": "",
   "abstract": [
    "Phonemic contrasts are signalled by multiple acoustic cues, with any one cue often being ambiguous. Cues must be evaluated with respect to each other due to trading relations and may also vary in reliability across different contexts, making it necessary for cues to be integrated and weighted. In vision, cues are weighted according to their reliability (Jacobs, 2002), with cue weights continually updated by recent experience. We evaluated the effects of experience in cue weighting of speech stimuli by varying the reliability of two cues to word medial voicing (preceding vowel length and closure duration). During a short training phase (15-20 min), subjects heard a word as they viewed a four picture display and clicked on the named picture. Training words had voiced or unvoiced medial consonants (e.g., bubble and rocket). The minimal pairs were not words (e.g., bupple and rogget) allowing the lexical bias to provide implicit training. For one group, preceding vowel length was a reliable cue (predicted voicing) and closure duration was not. For the second group, the reverse was true. During testing, we monitored subject's eye movements as they performed the same 4AFC task. They now heard pairs which were temporarily ambiguous between the voiced and unvoiced alternatives(e.g., baker and bagel). The display contained pictures consistent with both alternatives, and the stimuli used a range of vowel and closure durations. Because lexical candidates are evaluated continuously as the acoustic signal unfolds, and acoustic cues arrive asynchronously, relative proportion of looks to the alternative pictures should reflect the strength of the subject's commitment to each cue over time. Initial biases should be towards the candidate most consistent with the vowel duration cue. Later looks should be to the candidate consistent with the closure duration cue. As predicted, early looks were consistent with biases generated by the vowel duration cue and later looks were consistent with biases generated by the closure duration cue. Crucially, subjects who heard the useful vowel duration cue in training maintained the vowel duration bias for longer than subjects who heard the useful closure duration cue in training. Our results demonstrate that listeners weight probabilistic acoustic cues according to their reliability, updating the weighting of those cues to reflect recent experience.\n",
    "",
    "",
    "R. A. Jacobs (2002). What determines visual cue reliability? TRENDS in Cognitive Sciences, 6:8, 345-350.\n",
    ""
   ]
  },
  "davis05_psp": {
   "authors": [
    [
     "Matt",
     "Davis"
    ],
    [
     "Alexis",
     "Hervais-Adelman"
    ],
    [
     "Karen",
     "Taylor"
    ],
    [
     "Robert P.",
     "Carlyon"
    ],
    [
     "Ingrid",
     "Johnsrude"
    ]
   ],
   "title": "Transfer of perceptual learning of vocoded speech: evidence for abstract pre-lexical representations",
   "original": "psp5_070",
   "page_count": 0,
   "order": 39,
   "p1": "70 (abstract)",
   "pn": "",
   "abstract": [
    "Recent work has shown that understanding noisevocoded (NV) speech requires a top-down, lexically driven perceptual learning process. Higher-level influences are revealed by enhanced learning with spoken or written feedback and by the absence of learning from nonword NV sentences (Davis, Johnsrude, Hervais-Adelman, Taylor, McGettigan, in press). However, learning must modify pre-lexical processes, since report scores improve for words that have not been heard in NV speech. Here, we ask whether perceptual learning alters acoustic processing or more abstract, non-acoustic representations of speech. We address this question by changing the carrier signal (Experiments 1/2) and frequency bands (Experiment 3) used in vocoding and assessing whether training on one form of vocoded speech improves performance on an acoustically different form.\n",
    "Experiment 1 tested transfer between vocoded stimuli created using noise (NV) or pulse-train carriers (PTV). Four groups of volunteers reported 40 vocoded sentences with feedback to promote perceptual learning (Davis et al, in press). For two groups, all sentences were vocoded using the same carrier signal. For two transfer groups the carrier signal changed after 20 sentences, allowing testing of generalisation from one carrier to the other. Results showed some significant transfer: listeners trained on NV speech outperformed nave listeners when tested on PTV speech. However, the reverse transfer effect was non-significant, perhaps because report of NV sentences was near ceiling. In Experiment 2 care was taken to ensure that wordreport scores were approximately equal for both carriers. Nonetheless, results still showed asymmetric transfer: training on NV speech improved report scores for PTV speech, but not vice versa. Asymmetric transfer suggests that some but not all perceptual learning of vocoded speech is independent of the fine-structure used to encode speech information.\n",
    "Experiment 3 tested whether perceptual learning of NV speech generalises between non-overlapping frequency ranges. Two groups reported 40 NV sentences filtered into either low (50 to 1406Hz) or high (1593-8000Hz) frequency regions with feedback to assist learning as before. In two transfer groups, the frequency region changed after 20 sentences. Both transfer groups showed complete generalisation: report was not different from listeners exposed to a single frequency range throughout. Thus, perceptual learning of NV speech alters representations that are not frequencyspecific.\n",
    "In summary, perceptual learning of vocoded speech generalises over changes to fine-structure (Experiments 1/2) and frequency range (Experiment 3), demonstrating that perceptual learning modifies pre-lexical representations that are abstracted from the acoustic properties of vocoded speech.\n",
    ""
   ]
  },
  "evans05_psp": {
   "authors": [
    [
     "Bronwen",
     "Evans"
    ],
    [
     "Paul",
     "Iverson"
    ]
   ],
   "title": "Plasticity in speech production and perception: a study of accent change in young adults",
   "original": "psp5_071",
   "page_count": 0,
   "order": 40,
   "p1": "71",
   "pn": "",
   "abstract": [
    "This study investigated changes in speech production and perception among university students in England, as individuals adapt their accent from regional to 'educated' norms. Crosslanguage research has demonstrated that plasticity in language learning decreases with age although a certain amount of plasticity is retained in adulthood. The present study investigated plasticity for accents within the same language, as a consequence of both exposure and sociolinguistic factors. Subjects were tested longitudinally: before beginning university, 3 months later and on completion of their first year of study. Subjects were recorded reading a set of experimental words and a short passage. This data was analyzed for changes in production. Subjects also completed two perceptual tasks; they chose best exemplar locations for test words in carrier sentences and identified words in noise. The results demonstrated that subjects changed their spoken accent after experience of attending university. These changes appeared to be linked to sociolinguistic factors; subjects who were highly motivated to fit in with their university community changed their accent more. There was some evidence for a link between production and perception; subjects chose similar vowels to the ones they produced. The results suggested that while subjects were able to adjust their perceptual representations at a late stage in their linguistic development, they were unable to fully acquire the phonological rules that govern how those categories are used.\n",
    ""
   ]
  },
  "gauthier05_psp": {
   "authors": [
    [
     "Bruno",
     "Gauthier"
    ],
    [
     "Rushen",
     "Shi"
    ],
    [
     "Yi",
     "Xu"
    ]
   ],
   "title": "Recognising tones by tracking movements - how infants may develop tonal categories from adult speech input",
   "original": "psp5_072",
   "page_count": 4,
   "order": 41,
   "p1": "72",
   "pn": "75",
   "abstract": [
    "Previous research has demonstrated that the perception of speech in infants moves gradually from being language-general to being language-specific during the first year of life. Recent research found that infants learning a tone language begin to show particular response patterns to tones in their native language by the age of six months (Mattock, 2004). The present study uses connectionist modelling to explore how infants might develop tones in Mandarin. F0 is generally considered the main cue for tone perception. However, F0 patterns in connected speech yield considerable betweencategory overlap and large within-category variability. Since speech input to infants consists mainly of multi-word utterances by multiple speakers, tone learning must involve processes that can effectively resolve both types of variability. In this study we explore the Target Approximation model (Xu & Wang, 2001), which characterises surface F0 as asymptotic movements toward underlying pitch targets defined as simple linear functions. The model predicts that it is possible to infer underlying pitch targets from the manners of F0 movements, for they may more directly reflect the characteristics of intended goals. Using the production data of multiple speakers in connected speech from Xu (1997), we trained a self-organising neural network with both F0 profiles and F0 velocity profiles as input, with no initial stipulation about the number of tonal categories to be discovered. F0 velocity profiles (i.e., first derivatives of F0s, hereafter referred to as D1) represent the vocal fold changing fundamental frequency. The testing phase showed that D1 yielded almost perfect categorisation of the four tones, far superior than F0. Visualisation techniques showed that D1 distribution in the network formed distinct regions of clustering neighbourhoods representing each tone, an organisation pattern resembling frequency topographic maps observed in the primary auditory cortex. The results indicate that D1 can effectively abstract away from surface variability and directly reflect underlying articulatory goals. The finding thus points to one way through which infants can successfully derive at phonetic categories from adult speech, namely, by extracting underlying phonetic targets based on information directly reflecting production. The implications of our finding for understanding the link between speech articulation and motor movements in general will be discussed. Research supported by FCAR Scholarship, NSERC and FQRSC.\n",
    ""
   ]
  },
  "hervaisadelman05_psp": {
   "authors": [
    [
     "Alexis",
     "Hervais-Adelman"
    ],
    [
     "Matt",
     "Davis"
    ],
    [
     "Robert P.",
     "Carlyon"
    ]
   ],
   "title": "Perceptual learning of noise vocoded words",
   "original": "psp5_076",
   "page_count": 0,
   "order": 42,
   "p1": "76",
   "pn": "",
   "abstract": [
    "Noise-vocoded (NV) speech is a spectrally reduced form of speech which can simulate perception by cochlear implant users. Although initially unintelligible, listeners can correctly report 70% of words in NV sentences after 30 minutes of training with English sentences, but not after training with nonword sentences. The current work explores the processes involved in learning to understand single NV words: (i) does perceptual learning generalise to untrained words, (ii)and is learning affected by providing feedback, (iii) can participants learn from training with nonwords and (iv) does training improve discrimination, or simply modify subjects' report strategies? Using NV words rather than sentences reduces the role of context and shortPSP2005 term memory (STM) in the learning and report process, permitting a more detailed assessment of factors affecting the perception of NV speech. Experiment 1: 20 naïve volunteers were asked to repeat two groups of 60 NV words. Following each response, listeners received feedback: they heard either the NV ('distorted') word repeated and then the same word clearly (DC), or the word clearly then distorted (CD). Recognition was more accurate on the second group of NV words, showing that perceptual-learning generalises to untrained lexical items. Consistent with sentence studies, subjects who received CD feedback performed significantly better than those receiving DC feedback. Knowledge of the identity of NV words speeds perceptual learning, consistent with involvement of top-down processes. Experiment 2: 24 naïve listeners took part in a crossover study; training materials were blocks of 60 distorted words or non-words with DC feedback. Subjects were tested for recognition on different blocks of 40 NV words before training, after one type of training and after both. Training with words was significantly more effective than training with non-words, suggesting that the lack of learning with nonword sentences does not only reflect limited STM capacity. Experiment 3: 32 naïve participants were tested on reporting 20 NV words, and on a 2AFC phoneme-discrimination test for 40 NV words, both at baseline (before training), and after training with varying numbers of NV words with CD feedback. Increased amounts of training improved report scores, but reliable changes in discrimination performance were unrelated to the amount of training exposure provided, suggesting that improved report scores do not only reflect improved discrimination. In conclusion, learning to report NV words depends a top-down learning mechanism which operates more effectively when the identity (Experiment 1) and lexicality (Experiment 2) of the training stimuli is known.\n",
    ""
   ]
  },
  "honbolygo05_psp": {
   "authors": [
    [
     "Ferenc",
     "Honbolygo"
    ],
    [
     "Valéria",
     "Csépe"
    ],
    [
     "Gergely",
     "Sárközy"
    ],
    [
     "Rozália",
     "Kálmánchey"
    ]
   ],
   "title": "Segmental and suprasegmental speech processing in a child with Landau-kleffner syndrome",
   "original": "psp5_077",
   "page_count": 4,
   "order": 43,
   "p1": "77",
   "pn": "80",
   "abstract": [
    "In the present study we examined the speech processing skills of a child having Landau-Kleffer syndrome (LKS). LKS is a rare childhood epileptic disorder, characterized by an abnormal EEG pattern and the loss of language skill leading to aphasia. The mechanisms behind LKS are not yet fully understood, and questions concern the origin of epileptic activity, the link between epileptic activity and language deficit, as well as the developmental issue, notably the fact that LKS appears exclusively in the developing brain. In our study we assessed two aspects of language processing skills using event related brain potentials (ERPs): discrimination of segmental and that of suprasegmental cues. The segmental task consisted of the discrimination of word initial phonemes, where a two syllable meaningful word was contrasted with its meaningless pair differing only in the initial phoneme ('b' vs. 'p'). The suprasegmental task used differing stress patterns: the obligatory Hungarian trochaic stress pattern (stressed followed by an unstressed syllable) was contrasted with a non-existing iambic pattern (unstressed followed by a stressed syllable). Results indicated that the two cues of spoken language were processed asymmetrically: the mismatch negativity component (MMN) was obtained for the phoneme difference, while the stress pattern difference failed to elicit any MMN. At the same time, a study with normal adults show that both contrasts are processed reliably, resulting in a MMN. Therefore result of the study confirm, that in LKS processing of segmental information is intact, that means phonemes are automatically discriminated, and that of the suprasegmental (stress) information is not. Beside the ERP study several other assessments were carried out so as to obtain a more complex picture on the language deficit, including examination of the source of epileptic activity by using electronic source localization and PET, as well as neuropsychological assessment of cognitive and behavioral skills. As a conclusion we believe that LKS is a childhood language disorder that can be used to study what happens to the language system if in the course of development the essential neural circuits are severely disturbed. We also believe that the tools we use, that is the electrophysiological measurement of pre-attentive language processing is a fruitful approach since it makes possible the assessment of fundamental sub-processes during language comprehension while it does not require the patient / child to be responsive.\n",
    ""
   ]
  },
  "lacerda05_psp": {
   "authors": [
    [
     "Francisco",
     "Lacerda"
    ],
    [
     "Anna",
     "Svensk"
    ],
    [
     "Charlotte",
     "Molde"
    ],
    [
     "Jenny",
     "Olofsson"
    ],
    [
     "Linda",
     "Ristolainen"
    ],
    [
     "Maria",
     "Carlson"
    ],
    [
     "Maria",
     "Sjöberg"
    ],
    [
     "Marianne",
     "Eklund"
    ],
    [
     "Sara",
     "Ekman-Brandt"
    ],
    [
     "Eeva",
     "Klintfors"
    ]
   ],
   "title": "An ecological view on general mechanisms of early language acquisition",
   "original": "psp5_081",
   "page_count": 0,
   "order": 44,
   "p1": "81 (abstract)",
   "pn": "",
   "abstract": [
    "This paper will present the results of a study currently in progress in which four related experiments are being used to investigate the mechanisms of early audio-visual integration. The experiments attempt to replicate the findings from Kuhl and Meltzoff's experiment (Kuhl & Meltzoff, 1982) and investigate how far general perceptual mechanisms may account for the observed audiovisual integration in speech. It is hypothesized that the audio-visual integration observed for speech is a particular manifestation of a general propensity to integrate synchronic audio-visual events (Bahrick, 2004) that is available in ecologically relevant speech communication settings. The subjects are 5 to 6 month-old infants, who are assigned to different sessions. In one session, the subjects are presented with split-screen displaying four synchronized filmed variants of a speaker uttering different CV-syllables while the original sound track from one of the films was played. In another session, the subjects are presented with four alternative asynchronous faces of the same speaker along with a sound track that presents a non-speech sound synchronized with one of the alternatives. Looking times and detailed visual scanning of the speakers' faces are obtained using an eye-tracking system. No training or habituation to the audio-visual materials is provided. In yet another session, the infants are tested in their sensitivity to the 'McGurk effect' (McGurk & MacDonald, 1976). In this case the infants are previously familiarized with the words 'boll' and 'doll' that are heard while the corresponding objects are displayed. In a test phase immediately after the familiarization the subjects watch four films showing the speaker's face in between the two objects. The four film sequences are produced by combining each sound track with each of the video sequences. The looking times and the detailed eye-tracking are used to infer about the infant's integration of audio-visual speech information. Finally, a variant of this session was created by partially masking the audio signals (Fernald, Swingley, & Pinto, 2001) used during the test phase in order to assess the infants' 'phoneme restoration' ability in the presence of visual information. Research carried out within the frame of MILLE project, supported by The Bank of Sweden Tercentenary Foundation, by students from the Dpt of Clinical Science, Logopedics and Phoniatrics at Karolinska University Hospital, working at Dpt of Linguistics, Stockholm University, Sweden\n",
    ""
   ]
  },
  "mclennan05_psp": {
   "authors": [
    [
     "Conor",
     "McLennan"
    ],
    [
     "Paul",
     "Luce"
    ],
    [
     "Robert La",
     "Vigne"
    ],
    [
     "Jan",
     "Charles-Luce"
    ]
   ],
   "title": "Changes in adult speech perception and implicit learning of non-adjacent phonotactic dependencies",
   "original": "psp5_082",
   "page_count": 0,
   "order": 45,
   "p1": "82",
   "pn": "",
   "abstract": [
    "We investigated the learning of non-adjacent phonotactic dependencies in adult speech perception. Following previous research examining learning of dependencies at a grammatical level (Gomez, 2002), we manipulated the co-occurrence of non-adjacent phonological segments within a spoken syllable. Each listener was exposed to consonant-vowel-consonant (CVC) nonword stimuli produced by one of two phonological grammars. Both languages contained the same adjacent dependencies between the initial CV and final VC sequences but differed on the co-occurrences of initial and final consonants. Two blocks of spoken stimuli were presented to listeners, a training block and a test block. Participants' repeated (or shadowed) the stimuli in both blocks and their learning of non-adjacent segmental dependencies was evaluated by examining their reaction times to repeat the stimuli in the test block as a function of what was presented in the training block. During the test block, participants heard (1) old nonwords on which they had been trained (complete CVC overlap), (2) new nonwords sharing adjacent dependencies with the nonwords on which they had been trained (either CV or VC overlap), (3) new nonwords sharing non-adjacent dependencies with the nonwords on which they had been trained (initial and final C overlap), and (4) new nonwords sharing neither adjacent nor non-adjacent dependencies with the nonwords on which they had been trained (either initial C or final C overlap only). The results provide evidence that listeners are sensitive to nonadjacent phonotactic dependencies in the perception of spoken language. In particular, participants were faster to repeat nonwords with the non-adjacent dependencies from the phonological grammar on which they had been trained than those from the grammar on which they had not been trained. A second experiment demonstrated that this facilitative effect was not due to the explicit recognition of the non-adjacent dependencies. In a third experiment, we extended this line of research by evaluating whether more extensive training with the nonword stimuli would lead these nonwords to become lexicalized and, if so, what role non-adjacent phonotactic dependencies would play in lexicalized stimuli. In particular, we asked whether the previous effect of facilitation would now manifest itself as inhibition, presumably due to the competition between lexicalized items sharing non-adjacent phonotactic dependencies. The results add to our understanding of the role that non-adjacent phonotactic dependencies play in the perception of spoken language and to the types of changes that are possible in adult speech perception.\n",
    ""
   ]
  },
  "mitterer05_psp": {
   "authors": [
    [
     "Holger",
     "Mitterer"
    ]
   ],
   "title": "Short- and medium-term plasticity for speaker adaptation seem to be independent",
   "original": "psp5_083",
   "page_count": 4,
   "order": 46,
   "p1": "83",
   "pn": "86",
   "abstract": [
    "At least two forms of plasticity in speech perception serve speaker adaptation. Norris, McQueen, and Cutler (2003) showed that exposure to ambiguous phonemes in words leads to a recalibration of phonetic categories: If a listener encounters an ambiguous fricative, and the lexical context indicates the fricative's identity, as in legiXlature, listeners are more likely to perceive the sound 'X' as an instance of the 's'- category in other, later, words too. This adaptation is speaker-specific and does not necessarily generalize to fricatives produced by other speakers. While this form of plasticity occurs on a medium time frame over a couple of minutes, another instance of speaker adaptation can occur even faster: Ladefoged and Broadbent (1957) showed that listeners adapt their phoneme boundaries in a compensatory way on the basis of the formant range encountered in a carrier phrase: Listeners are more likely to perceive a low vowel (i.e., higher F1) if the carrier sentence contained comparably a low F1. Most vowel normalization studies using the Ladefoged and Broadbent paradigm have used meaningful carrier sentences. The question arises whether the short-term plasticity observed in vowel-normalization may profit from lexical information, which provides the phonetic category of the vowels encountered in the carrier phrase. Accordingly, the lexical status of the constituents of the carrier phrase was manipulated in two vowel-normalization experiments. Simultaneously, F2 range in the carrier phrase was manipulated and the effects on the perception of a F2-vowel continuum was assessed. In Experiment 1, the carrier phrase contained high, low, front, and back vowels. Listeners adapted their vowel categories in a compensatory way for the F2 range in the carrier, independent of the lexical status of its constituents. In Experiment 2, the carrier phrase contained only high-front vowels, in order to minimize the information about the formant range of the speaker. Nevertheless, lexical status of the constituents of the carrier phrase again failed to influence vowel normalization. The perception of the test continuum still indicated compensation for the F2 range in the carrier phrase, but only for the category boundary between the high-front and highmid vowel, which is close to the high-front vowels presented in the carrier. The boundary between a high-mid vowel and a high-back vowel was, in contrast, unaffected. This indicates, first, that vowel normalization is vowel-specific and, second, that short- and medium-term plasticity for speaker adaptation seem to be independent.\n",
    ""
   ]
  },
  "pan05_psp": {
   "authors": [
    [
     "Ho-hsien",
     "Pan"
    ],
    [
     "Wan-Ting",
     "Huang"
    ],
    [
     "Ying-Hui",
     "Huang"
    ]
   ],
   "title": "Perception of new, given and contrastive information in Taiwanese",
   "original": "psp5_087",
   "page_count": 4,
   "order": 47,
   "p1": "87",
   "pn": "90",
   "abstract": [
    "In plastic language such as Dutch, information status could modify accent distribution, thus perception of focus in Dutch depends on accent distribution within sentences (Cutler 1984; Krahmer and Swerts 2001; Krahmer, Ruttkay et al. 2002; Swerts, Krahmer et al. 2002). However, in Italian, a non-plastic language, information status does not influence accent distribution, thus perception of focus relies on gradient expansion of f0 range on focused syllable without chaning the location of pitch accent (D'Imperio 2001; Swerts, Krahmer et al. 2002). The perception of focus in a tone language, such as Taiwan Mandarin has been underinvestigated. Using spontaneous Taiwan Mandarin data, this study investigated the acoustic parameters signaling given, new, and contrastive information. Sentences produced with variation in focus were acoustically compared. Sentences were collected through games in which speakers must identify the location of objects within a 4X4 matrix. In the first game, speakers responded to questions, such as \"What is next to the black cat?\" and \"The black cat is next to what?\" and placed new information on the NP1 or NP2. In the second game, speakers responded to sentences, such as \"The black cat is next to the brown fruit.\" by placing contrastive information on either the adjective or the noun of the NP1 or NP2. In the third game, speakers named the objects with respect to other objects in the matrix and placed broad focus on the entire utterance. These productions were used in two perceptual experiments, (1) a dialogue history experiment, of which listeners identified the questions preceding the sentences, and (2) a synthetic speech experiment, of which f0 range and duration for syllable carrying given information was gradually increased to investigate the threshold required to perceive the same syllable as carrying new information. Preliminary results showed that although both F0 range expansion and duration elongation were observed in the NP carrying new and contrastive information, listeners paid more attention to duration elongation than f0 range expansion during the dalogue hitory experiment. In the synthetic speech experiment, duration elongation cued contrastive focus more than f0. These results suggest that focus in Taiwan Mandarin is more similar to focus in non-plastic languages where the tone on each syllable is not changed, but f0 range expansion and duration signal narrow focus.\n",
    ""
   ]
  },
  "pattamadilok05_psp": {
   "authors": [
    [
     "Chotiga",
     "Pattamadilok"
    ],
    [
     "Paulo",
     "Ventura"
    ],
    [
     "Jose",
     "Junca de Morais"
    ],
    [
     "Regine",
     "Kolinsky"
    ]
   ],
   "title": "The all-or-none or graded nature of the orthographic consistency effect in speech recognition",
   "original": "psp5_091",
   "page_count": 4,
   "order": 48,
   "p1": "91",
   "pn": "94",
   "abstract": [
    "During the last 20 years, several studies have demonstrated the influence of orthographic knowledge on speech processing. Nevertheless, we recently collected data suggesting that not all the phonological representations activated during recognition processes are sensitive to orthography. While an influence of orthographic representations is systematically observed in metaphonological tasks, in speech recognition this influence seems to depend on the presence of lexical process as well as on the time required by the recognition process. Using both French and Portuguese materials, we manipulated the orthographic consistency of words and pseudowords presented either in an auditory lexical decision task or in a shadowing task. The comparison of these two languages was of interest since they differ in their degree of orthographic (in)consistency, French orthography being much more inconsistent than Portuguese orthography. While inconsistent words produced longer auditory lexical decision latencies than consistent words, no word consistency effect was found in shadowing. Similar results were observed in French and Portuguese. As concerns pseudowords, an unreliable effect was found with the French material in both lexical decision and shadowing, but only in participants presenting the longer average response latencies. Further studies conducted in Portuguese also demonstrated that the consistency effect observed in lexical decision was due to the lexical rather than to the decisional component of the task. Indeed, the comparison of two situations in which the shadowing response was made contingent upon either a lexical or a phonemic criterion showed a significant effect of orthographic consistency only in lexically-contingent shadowing. Taken together, our results suggest that lexical access is a necessary condition to the full emergence of the orthographic consistency effect. In addition, the assumption proposed by Ziegler and collaborators (2004) that the size of the consistency effect would depend on the degree of consistency of the stimuli was not confirmed in our study. Indeed, once differences in overall response latencies were partialled out, similar effect sizes were obtained in French and Portuguese, despite the difference in the degree of orthographic (in)consistency between these two languages.\n",
    ""
   ]
  },
  "sedin05_psp": {
   "authors": [
    [
     "Leanne",
     "Sedin"
    ],
    [
     "Gareth",
     "Gaskell"
    ]
   ],
   "title": "Acquiring novel words and their past tense forms: evidence from lexical effects on phonetic categorisation",
   "original": "psp5_095",
   "page_count": 0,
   "order": 49,
   "p1": "95 (abstract)",
   "pn": "",
   "abstract": [
    "Here we examined plasticity in speech perception by testing whether familiarisation with a novel word can lead to a 'lexical' bias on phonetic categorisation (cf. Ganong, 1980). We also investigated whether this lexicalisation bias would generalise to an inflected form of the novel item. Three sets of matched /d/- final nonwords were used in the experiment, as follows:\n",
    "(1) Familiarisation: Participants were exposed to novel items repeatedly in three conditions: (a) with the final /d/ (e.g., WECTADE), (b) without the final /d/ (e.g., WECTAY), or (c) not at all. Exposure tasks involved past tense formation (writing down the past tense of the novel item) and phoneme monitoring. Three item sets were used to ensure that each item was encountered in just one condition by each participant, but that all items were represented in all conditions across participants.\n",
    "(2) Phonetic Categorisation: Participants made twoalternative forced-choice decisions to /d/-/t/ continua embedded word-finally in the nonwords. The three conditions determined by the item status at familiarisation were: (a) uninflected (e.g., WECTA[d/t] having learnt WECTADE); (b) inflected (WECTA[d/t] having learnt WECTAY); (c) nonword (WECTA[d/t] having never heard it before). Phonetic categorisation took place either immediately (40 participants) or one week following familiarisation (41 participants).\n",
    "Lexicalisation biases were indicated by significantly more /d/-responses in both the uninflected and inflected conditions than in the nonword condition: participants familiarised with WECTADE or WECTAY both judged the ambiguous phoneme in WECTA[d/t] to be more /d/-like than those not familiarised with either. This effect was more pronounced in the uninflected condition, with significantly more /d/- responses given to WECTA[d/t] by participants exposed to WECTADE than those exposed to WECTAY. These effects were evident both immediately following familiarisation and a week later despite no further exposure to the novel items, and were taken as evidence for the immediate use of episodic representations of the novel words (cf. Gaskell & Dumay, 2003) in categorisation, and their subsequent lexicalisation. The lexicalisation bias in the inflected condition suggested that the past tense forms of newly acquired words were recognised as lexical items despite having never previously been encountered. These results are discussed with reference to current theories of lexicalisation, phonetic categorisation, and the lexical representation of inflectional morphology.\n",
    "s Ganong, W.F. (1980). Phonetic categorization in auditory word perception. JEP: HPP, 6, 110-125. Gaskell, M.G., & Dumay, N. (2003). Lexical competition and the acquisition of novel words. Cognition, 89, 105-122.\n",
    ""
   ]
  },
  "feest05_psp": {
   "authors": [
    [
     "Suzanne van der",
     "Feest"
    ],
    [
     "Paula",
     "Fikkert"
    ]
   ],
   "title": "Segmental details in children's early lexical representations",
   "original": "psp5_096",
   "page_count": 0,
   "order": 50,
   "p1": "96",
   "pn": "",
   "abstract": [
    "Studies on Dutch child language production have claimed that children's phonological representations are underspecified: Fikkert & Levelt (2004) provided evidence for the underspecified nature of coronal place of articulation and Kager et al. (2004) provided evidence for the underspecification of voiceless stops. In both studies the evidence came from asymmetries in production. If phonological representations are underspecified, we expect to find evidence for this in perception as well. Evidence from previous word perception studies suggests that children store word with phonetic detail. Fennell & Werker (2003) used the switch procedure to investigate whether Canadian children perceive the difference between well-known minimal pairs like ball-doll. They showed that while 14- month-old children did not perceive the difference between the nonce minimal pairs bin-din in a word learning task, they did perceive the contrast between the known words ball-doll. Swingley & Aslin (2000) and Swingley (2003) have shown that 18- to 24-month-old American children are able to detect mispronunciations in well-known words. Using a preferential looking paradigm, they established that children look slower or shorter to the target ball when hearing dall than when hearing ball. A variety of mispronunciations were used in these experiments, including changes in vowel height, consonant place, manner and voice, but it was not systematically tested whether asymmetries in perception exist.\n",
    "Using the same split-screen preferential looking paradigm as Swingley (2003), we tested the perception of mispronunciations of well-known Dutch words by forty-eight 24-month-old Dutch children. Targets consisted of CVC words starting with one of the following stops: /p, b, t, d/. Each target word was used in three conditions: correctly pronounced (ball), with a voice-mispronunciation (pall) and with a place-mispronunciation (dall).\n",
    "Mispronounced words resulted in non-words or low frequency words unknown to the children. Children were sensitive to both place and voice mispronunciations. Furthermore, the results clearly show perceptual asymmetries in the predicted directions. Perceiving a voice-mispronunciation resulted in longer looking latencies for voiceless targets than for voiced targets. However, when a voiceless stop was perceived, there were no differences in looking latencies for voiced and voiceless targets. Place-mispronunciations led to longer looking latencies for labial-initial than for coronal-initial targets.\n",
    "These results indicate that children have stored underspecified phonological representations of words. The results cannot be accounted for by assuming that children merely perceive changes in the phonetic realization of targets. Moreover, the data suggest that there is a tight link between perception and production.\n",
    ""
   ]
  },
  "wagner05_psp": {
   "authors": [
    [
     "Anita",
     "Wagner"
    ],
    [
     "Mirjam",
     "Ernestus"
    ]
   ],
   "title": "Listeners' sensitivity to formant transition is adapted to the requirements of their native language",
   "original": "psp5_097",
   "page_count": 0,
   "order": 51,
   "p1": "97 (abstract)",
   "pn": "",
   "abstract": [
    "Previous research presents conflicting evidence about the relevance of formant transitions for fricative identification. We investigated whether listeners' attention to formant transitions is language-dependent, and is determined by the presence of spectrally similar fricatives in the language's phoneme repertoire. We run a series of phoneme-monitoring experiments in which native Dutch, German, English, Spanish, Polish, and Italian listeners detected a target fricative, /s/ or /f/, in nonwords. Misleading formant transitions were introduced to half of the words by cross-splicing. The listeners showed language-specific patterns of fricative perception: Dutch and German listeners were not affected by the misleading formant transitions, whereas Spanish, English, Polish, and Italian listeners missed more fricatives surrounded by misleading than by correct formant transitions. Both Dutch and German contain no spectrally similar fricatives. There is thus no need for Dutch and German listeners to pay attention to the formant transitions, since all fricatives can be distinguished on the basis of their spectral characteristics only. Spanish and English, both languages with the spectrally similar labiodental and dental fricatives in their phoneme inventory, paid attention to the formant transitions especially when detecting the labiodental /f/. Polish listeners paid attention to formant transitions, especially for /s/. The alveolar sibilant has been shown to be a very robust fricative and less confusable in other languages. As Polish contains postdental, alveolar and alveolopalatal fricatives, the perceptual distinctiveness of these sibilants might be reduced. We conclude that listeners' sensitivity to formant transitions for fricatives is determined by the presence of spectrally confusable fricatives in their native languages. This listening strategy is restricted to the confusable fricatives, and is not generalized to spectrally more distinct fricatives. Also the Italians show sensitivity to formant transitions, even though Italian has few distinctive fricatives. However, there is regional variation in the production of fricatives and there is lenitization of stops. This results in spectrally similar fricatives of allophonic status carrying extralinguistic information. Apparently, as a consequence Italian listeners show sensitivity to formant transitions. An additional experiment with only Dutch, German, and Spanish listeners shows that the results are independent of the exact realization of the phonemes or the native language of the speaker. The sensitivity to acoustic cues does not adapt to the requirements of different phoneme realizations, but is determined by the native language, and this seems to limit plasticity in speech perception.\n",
    ""
   ]
  },
  "abelin05_psp": {
   "authors": [
    [
     "Åsa",
     "Abelin"
    ]
   ],
   "title": "Change of perception of emotions in multimodal and crosslinguistic settings",
   "original": "psp5_113",
   "page_count": 4,
   "order": 52,
   "p1": "113",
   "pn": "116",
   "abstract": [
    "This paper adresses two questions. The first question is if perception of emotional prosody changes when visual stimuli are present. It has already been shown by others (e.g. Massaro) that the perception of vowels and consonants is augmented or changed when visual information is present.The question is then if perception of such categories as emotions, mainly expressed by prosody, are also affected by multimodal stimuli. The second question concerns the cross-linguistic perception of multimodal stimuli. Earlier crosscultural, studies indicate that facial expression of emotions is more universal than prosody is. Crosslinguistic interpretation of emotions could then be more successful multimodally than only vocally. The specific questions asked in the present study are to what degree Spanish and Swedish listeners can interpret Spanish emotional prosody, and whether simultaneously presented faces, expressing the same emotions, change or improve the interpretation. Audio recordings of Spanish emotional expressions were presented to Spanish and Swedish listeners, in two experimental settings. In the first setting the listeners only attended to prosody, in the second one they also saw a face, expressing different emotions. The results indicate that intra-linguistic as well as cross-linguistic perception of emotional prosody is improved by visual stimuli. Cross-linguistic interpretation of prosody is more poorly accomplished than interlinguistic, but seems to be greatly augmented by multimodal stimuli. Acoustic analysis of the stimuli are made and discussed in relation to the results of he perceptions of the stimuli of the different language groups. The results are also discussed in terms of age and gender of listeners.\n",
    ""
   ]
  },
  "brasileiro05_psp": {
   "authors": [
    [
     "Ivana",
     "Brasileiro"
    ],
    [
     "Paola",
     "Escudero"
    ]
   ],
   "title": "The perceptual development of a Dutch vowel contrast by Brazilian learners",
   "original": "psp5_117",
   "page_count": 0,
   "order": 53,
   "p1": "117 (abstract)",
   "pn": "",
   "abstract": [
    "In a series of experiments we investigated whether L2 learners could learn to perceive non-native phonological contrasts that are based on acoustic cues not used in the native language. We examined the perception of the Dutch contrast (/a:/-/A/) by native-speakers of Brazilian Portuguese (BP). This Dutch contrast involves spectral and durational differences and is not found (phonemically) in Portuguese. Forty-one subjects took part in the experiments: 10 Dutch listeners, and 31 BP learners of Dutch who had lived in The Netherlands for 6 to 216 months. Subjects perfomed three experiments:\n",
    "Experiment 1 was an AXB experiment with natural Dutch words (/tAk/ 'branch' and /ta:k/ 'task'). Subjects heard three words in a row (/ta:k/ or /tAk/) and had to decide whether the middle word (X) was equal to the first (A) or to the third (B);\n",
    "Experiment 2 was similar to Experiment 1, but with hybrid stimuli with synthetic vowels inserted between natural consonants. The vowel was any of the 49 tokens resulting from a 7-step spectrum and 7-step duration manipulation;\n",
    "Experiment 3 consisted of a read-aloud task, and was used as an independent measure of learner's L2-experience/proficiency.\n",
    "Our results show that several learners performed in a native-like manner, which suggests that their L2 perception has developed to match that of Dutch listeners. However, results from Experiment 1 also reveal that not all L2 learners were native-like. We examined two possible explanations for this divergence: (1) The learners have no categories for the L2 contrast; (2) Their L2 categories deviate from the native ones. Results of Experiment 2 show that all L2 learners were able to identify the vowels in a native-like manner when spectral and durational cues were manipulated at the same time. However, results of the same experiment reveal that, unlike Dutch listeners who always integrate both temporal and spectral cues when perceiving the contrast, several L2 learners typically integrate these cues in a non-native manner. This leads us to conclude that hypothesis (2) is borne. As for the differences in degree of native-like perception, we found no correlation between Experiment 1 or 2 and Experiment 3 (or any other independent measure of language experience/proficiency). Nevertheless, we can explain such differences by suggesting distinct categorization patterns at the onset of the learning process, which lead to the different developmental paths attested in the experiments.\n",
    ""
   ]
  },
  "dellwo05_psp": {
   "authors": [
    [
     "Volker",
     "Dellwo"
    ],
    [
     "Petra",
     "Wagner"
    ]
   ],
   "title": "English and French native language influence on the perception of syllable prominence in German",
   "original": "psp5_118",
   "page_count": 0,
   "order": 54,
   "p1": "118 (abstract)",
   "pn": "",
   "abstract": [
    "Syllable prominence perception is highly influenced by the native language background of the listener. When listeners perceive speech in their native language, the acoustic information of a syllable competes with expectations about its perceptual prominence, triggered by language specific higher level knowledge (e.g. sentence or word stress, part of speech, syntactic postition, etc.). Current models assume that listeners with no previous experience in a particular language (henceforth: non-speakers) are not distracted by any higher level information; their perception is based merely on the acoustic signal. For this reason non-speakers are often regarded as being able to judge the acoustic information in the speech signal on a neutral basis. This study argues that prominence perception in non-speakers is greatly influenced by the listeners' native language background and the degree of competence in the target language. For example, it is expected that listeners of French with a so called Ôstress deafness' will perceive syllable prominence in German speech differently than listeners of English, a language in which stress is of major prosodic importance. It is unclear, whether and to what degree competence in the target language (German) has an effect on syllable prominence perception of listeners with, in this respect, such a diverse language background like French and English. In this study the syllable prominence perception of French and English listeners with different degrees of competence in German (no knowledge, basic knowledge, and fluency) was investigated in a syllable prominence rating experiment. Results thus far indicate that a) listeners' native language background has significant influence on the perception of syllable prominence in German and b) knowledge of the target language shows very diverse effects according to the different native language background (French and English).\n",
    ""
   ]
  },
  "girard05_psp": {
   "authors": [
    [
     "Frédérique",
     "Girard"
    ],
    [
     "Caroline",
     "Floccia"
    ],
    [
     "Jeremy",
     "Goslin"
    ],
    [
     "Gabrielle",
     "Konopczynski"
    ]
   ],
   "title": "Coping with an unfamiliar regional accent: a lexical decision study in French listeners",
   "original": "psp5_119",
   "page_count": 1,
   "order": 55,
   "p1": "119",
   "pn": "",
   "abstract": [
    "In this study on regional accent perception we conducted two experiments to examine a hypothesis based on previous work on talker variability (Pisoni et al., 1998), and on compressed speech (Dupoux et al., 1997), which predicts that accent-related variations would elicit a perceptual cost in the course of word recognition. In both experiments participants were asked for a lexical decision on the last item of sentences uttered in a familiar or an unfamiliar regional accent. The first experiment examined whether the effect of regional accent was sensitive to sentence length. Two accents were used, Besan?on (familiar) and Toulouse (unfamiliar), with ten disyllabic test items and six legal nonwords. These words were appended onto a set of 192 sentences which were split equally between the two accents and produced by two female speakers per accent. Three blocks of different sentence length were used. After testing thirty-five monolingual French-speaking participants from the Franche- Comté region we found that familiar accents were processed faster than unfamiliar accents (559 ms versus 574 ms). Moreover a significant interaction between accent and sentence length revealed that the effect of the accent increased in relation with sentence length. These findings suggest that unfamiliar regional accents elicit a cost in word recognition, possibly reflecting a normalisation process, emerging mainly after long utterances. A second experiment was conducted to determine whether this processing delay was due to the normalisation process per se, or to the disruption of this mechanism caused by the random presentation of accents. The procedure of the second experiment was identical to the first, departing only in the presentation order of stimuli. Instead of randomly alternating the accents from one sentence to the other participants were presented with blocks of sentences uttered in each of the accents. Twentyfour monolingual French-speaking subjects from the Franche-Comté region were tested in this experiment which showed no significant effect of accent familiarity (Franche-Comté: 593 ms; Toulouse: 595 ms). These results indicate that when participants are presented with the same unfamiliar regional accent for a certain period of time the processing cost observed in the previous experiment habituates. The implications of these findings and their impact on the understanding of accent variability processing are discussed, particularly in light of previous research on compressed speech.\n",
    ""
   ]
  },
  "hjen05_psp": {
   "authors": [
    [
     "Anders",
     "Højen"
    ]
   ],
   "title": "Vowel discrimination in early bilinguals: how plastic?",
   "original": "psp5_120",
   "page_count": 4,
   "order": 56,
   "p1": "120",
   "pn": "123",
   "abstract": [
    "Recent neuroscience research has revealed far more plasticity (e.g., in motor, sensory, and auditory functions) than previously thought possible. This has spurred renewed interest in whether the systems underlying human speech perception remain plastic. Recent L2 speech perception research has shown that even early bilinguals may differ from native speakers. This might suggest a lack of plasticity in the perceptual system following L1 acquisition. However, the finding that some early bilinguals show native-like performance raises the question of 'to what extent' L2 perceptual learning is constrained by the L1 perceptual system. To evaluate this question, two reference points are needed. Bilinguals' perception of the L2 should be compared to that of monolingual speakers of the target L2, to reveal how much the bilinguals have yet to learn, and they should be compared to monolingual speakers of the L1, to determine how much they have already learned. This study compared the discrimination of English vowels by English monolinguals, Spanish monolinguals, and early Spanish-English bilinguals (n=20 each). The vowels in one 'easy' contrast were heard as two distinct Spanish vowels by Spanish monolinguals whereas vowels in the three 'difficult' contrasts were heard as a single Spanish vowel. To avoid the ceiling effects often seen in cross-language vowel discrimination research, within-trial F0 variation was introduced into the categorial AXB test used here, and the 64 trials testing all four contrasts were presented in two randomized blocks that differed in inter-stimulus interval (0 vs. 1000 ms). This design was intended to yield low scores on difficult contrasts for Spanish monolinguals, but high scores for English monolinguals. As expected, all three groups obtained high percent correct scores for the easy contrast (means=88-98%). As intended by the design, the Spanish monolinguals obtained near-chance scores for the three difficult contrasts (mean=58%) whereas the English monolinguals obtained very high scores (mean=96%). An ANOVA revealed that the bilinguals obtained significantly higher scores (mean=89%) than the Spanish monolinguals did for all three difficult contrasts in both ISI conditions. They obtained significantly lower scores than the English monolinguals for two difficult contrasts in the 0-ms condition, but no contrast in the 1000-ms condition. Sixty-five percent of individual bilinguals obtained scores falling within 2 SDs of the English monolinguals' mean scores, as against just 2% of individual Spanish monolinguals. The results will be discussed with respect to theories of plasticity.\n",
    ""
   ]
  },
  "johnsrude05_psp": {
   "authors": [
    [
     "Ingrid",
     "Johnsrude"
    ],
    [
     "Matt",
     "Davis"
    ],
    [
     "Barry",
     "Horwitz"
    ]
   ],
   "title": "Temporofrontal functional connectivity is modulated by sentence intelligibility",
   "original": "psp5_124",
   "page_count": 0,
   "order": 57,
   "p1": "124 (abstract)",
   "pn": "",
   "abstract": [
    "The multiple cognitive processes required for speech comprehension rely on multiple cortical networks that operate in parallel. This functional organization resembles the anatomical separation of the primate auditory system into dorsal and ventral processing streams. In this work, we test whether coupling within these networks depends on the nature of the auditory input. If observed, modulations of functional connectivity would suggest rapid, shortterm changes in functional organization for speech comprehension: a form of neural plasticity.\n",
    "The present study uses fMRI to identify brain regions involved in sentence comprehension. Davis & Johnsrude (2003) distinguished two response profiles in lateral temporal regions that show activity correlated with the intelligibility of acoustically distorted sentences. BOLD responses were either: (1) form-dependent: responding differentially to three acoustically different forms of distorted speech, or (2) form-independent: insensitive to acoustic differences in the form of speech. Formdependent regions border primary auditory cortex, suggesting early-stage acoustic processing of speech, whereas more distant form-independent regions may be primarily involved in higher-order, linguistic processes.\n",
    "We scanned a further 15 neurologically normal, right-handed English volunteers on a passivelistening version of the previous study. Participants listened to and tried to understand sentences (1.7 to 4.3 sec long) presented with three types of distortion in three different amounts. fMRI data were acquired with a 3T Bruker MRI system using sparse imaging (TR 9s, TA 3s; sentences presented in silence between scans). As before, a significant correlation between sentence intelligibility and BOLD signal was observed in the left inferior frontal gyrus (LIFG) and along the superior and middle temporal gyri bilaterally. This correlation was form dependent around auditory cortex and form-independent in four left-hemisphere regions, which we chose as seed voxels for connectivity analysis: (1) anterior STS (- 56, 0, -18), (2) posterior STS (-60, -54, 20), (3) anteroventral LIFG (-50, 28, -16) and posterodorsal LIFG (-52, 16, 18).\n",
    "Functional connectivity analysis of all distortedspeech conditions revealed significant functional coupling between anterior temporal and ventral frontal regions, and between posterior temporal and dorsal LIFG regions. This pattern of connectivity conforms to the anatomical organization observed in nonhuman primates, suggesting that speech perception recruits multiple, temporofrontal networks. Furthermore, coupling between the posterodorsal LIFG and posterior temporal regions increased in strength as sentence intelligibility increased. This finding indicates that functional connectivity among regions is plastic, changing with the nature of the current input and with task demands. Acknowledgements: Research supported by Medical Research Council, UK; National Institutes of Health, USA.\n",
    ""
   ]
  },
  "krebslazendic05_psp": {
   "authors": [
    [
     "Lidija",
     "Krebs-Lazendic"
    ],
    [
     "Heather",
     "Winskel"
    ],
    [
     "Catherine",
     "Best"
    ],
    [
     "Denis",
     "Burnham"
    ]
   ],
   "title": "Pronunciation accuracy of four English vowels by Serbian learners of English",
   "original": "psp5_125",
   "page_count": 0,
   "order": 58,
   "p1": "125 (abstract)",
   "pn": "",
   "abstract": [
    "Various theories have been offered to explain why children achieve native-like accuracy in second language (L2) pronunciation and adults do not. Both age of learning and interaction between first language (L1) and second language (L2) phonetic inventories influences production accuracy by late L2 learners. The Perceptual Assimilation Model (PAM) and Speech Learning Model (SLM) have been devised to explain the influence of sounds from the native language in the perception of phones that are not contrastive in a learner's native language. The PAM focuses on the way non-native categories are filtered through the phonetic properties of the native language, while the SLM is concerned primarily with L2 acquisition and within cross-language speech perception and production. This study was conducted in order to test the SLM's predictions and extend the PAM's predictions in relation to L2 sound acquisition. Three Experiments were conducted. In Experiment 1 production accuracy of four English vowels /., i, ., . / by native speakers of Serbian was tested. The early learners were expected to produce all tested vowels accurately, whereas late learners were expected to produce /., ./, and fail to produce /., i / accurately. In Experiment 2 identification and goodness-to-fit ratings of targeted English vowels by the same subjects were tested. The aim was to assess the perceptual relationship between targeted English vowels and the closest Serbian vowels and perceived similarity between them. It was expected that production accuracy is a result of English vowels being identified as exemplars of closest native vowels. In Experiment 3 two trained phoneticians identified vowels produced by the subjects in Experiment 1. The aim was to see whether subjects who failed to produce targeted vowels accurately substituted the non-native categories with perceived categories. Results from these experiments will be reported in the presentation.\n",
    ""
   ]
  },
  "mayo05_psp": {
   "authors": [
    [
     "Catherine",
     "Mayo"
    ],
    [
     "Alice",
     "Turk"
    ]
   ],
   "title": "No available theories currently explain all adult-child cue weighting differences",
   "original": "psp5_126",
   "page_count": 4,
   "order": 59,
   "p1": "126",
   "pn": "129",
   "abstract": [
    "Children and adults appear to weight some acoustic cues differently in perceiving certain speech contrasts. There are currently two main theories to explain this difference. One of these is Nittrouer and colleagues' Developmental Weighting Shift (DWS) theory, which proposes that children process speech in terms of large units (the size of a syllable or monosyllabic word) while adults process in terms of smaller units [Nittrouer, S. (1992). J. Phon. 20, 351- 382]. This processing difference then impacts on speech perception in terms of the attention that listeners give to acoustic cues. On this view, children are expected to attend more than adults to syllableinternal cues such as vowel-onset formant transitions. An alternative explanation is that adultchild cue weighting differences are due to more general sensory (auditory) processing differences between the two groups. It has been proposed that children may be less able to deal with incomplete or insufficient acoustic information than are adults, and thus may require cues that are longer, louder or more spectrally distinct to identify or discriminate between auditory stimuli [Sussman, J. (2001). J. Acoust. Soc. Am. 109, 1173-1180].\n",
    "The current study tested these hypotheses in two ways. First, we examined adults' and three- to seven-year-old children's weighting of vowel-onset formant transitions in contrasts in which we systematically varied (i) the consonantal context of the transition: /s/-/sh/, /d/-/b/, /t/-/d/, and /-/m/; and (ii) the spectral distinctiveness of the transition (in terms of the onset frequency, extent, direction and duration of the vowel-onset formant transitions), varying from spectrally distinct (o/-/mo/, /do/-/bo/, and /ta/-/da/) to spectrally similar (i/-/mi/, /de/-/be/, and /ti/-/di/).\n",
    "Second, we examined adults's and five-year-old children's weighting of vowel-formant offset transitions in phonetically identical withinmonosyllabic- word and across-monosyllabic-word contexts. The transition in question was from the vowel /e/ into a following /b/ or /d/ closure, either within-word: \"Abe E\" versus \"Ade E\", or acrossword: \"A bee\" versus \"A dee\".\n",
    "The results of the study showed that adult-child differences in cue weighting are affected by the segmental context of the cues, the salience of the cues, and the position of the cue in the word. However, neither of the above two theories, either on their own or in combination, can account for all of the observed cue weighting behaviour. Possible alternative explanations are discussed.\n",
    ""
   ]
  },
  "mclennan05b_psp": {
   "authors": [
    [
     "Sean",
     "McLennan"
    ]
   ],
   "title": "A dynamic adaptive model of linguistic rhythm",
   "original": "psp5_130",
   "page_count": 0,
   "order": 60,
   "p1": "130 (abstract)",
   "pn": "",
   "abstract": [
    "It is well established that although adult speakers perceive spoken words as having clear boundaries, in reality the signal rarely shows correlates of those boundaries. While adult speakers might exploit a well developed lexicon to parse the speech stream, this is not a viable mechanism for infants who have yet to learn a single word.\n",
    "Recent focus has been directed at 'rhythm' as the relevant mechanism for accessing speech. There is a correlation between languages of different rhythmic types and strategies their speakers use to segment words and syllables (cf. Cutler, 1997). However, just as the human perception of discreteness in speech is deceiving, so too is our perception of rhythm. Rhythm implies an underlying isochrony which, empirically, we have failed to reliably find in natural speech.\n",
    "Evidence seems to be leaning towards the conclusion that the traditional categories are relevant but that the underlying reality behind our perception of rhythm is something more complex than simply isochrony. Rhythm remains implicated in a wide variety of cognitive functions, and is a compelling candidate for a linguistic bootstrap into speech segmentation.\n",
    "My current research attempts to draw a bridge between two areas of rhythm research via a computational model: Ramus et al. (1999)'s views of what signal correlates underlie rhythm, and the impact of rhythm-class on the segmentation of words from the speech stream (Cutler, 1997). The primary question that is asked is 'Can a simple learning mechanism (an adaptive oscillator model) that responds Ramus et al.'s factorsÑthe percent of the signal that is vocalic, the variance in the duration of consonantal intervals, and the variance in the duration of consonantal intervals produce behaviour that is consistent with observed differences in segmentation behaviour?' Preliminary results mimic the patterns Ramus and colleagues have observed and ongoing research is promising. If ultimately successful, this model would provide support for the hypothesis that these factors underlie human perception of rhythm and would provide a plausible explanation for why these factors impact on how humans parse the speech signal, a heretofore unaddressed question. It could also give insight into an open question about the nature of linguistic rhythm: is it categorical or continuous?\n",
    "s Cutler, A. (1997). The syllable's role in the segmentation of stress languages. Language and Cognitive Processes, 12(5/6):839-845. Ramus, F., Nespor, M., and Mehler, J. (1999). Correlates of linguistic rhythm in the speech signal. Cognition, 73:265-292.\n",
    ""
   ]
  },
  "messaoudgalusi05_psp": {
   "authors": [
    [
     "Souhila",
     "Messaoud-Galusi"
    ],
    [
     "René",
     "Carré"
    ],
    [
     "Liliane",
     "Sprenger-Charolles"
    ],
    [
     "Willy",
     "Serniclaes"
    ]
   ],
   "title": "Effect of age and reading level on the perceptual weight assigned to acoustic cues",
   "original": "psp5_131",
   "page_count": 0,
   "order": 61,
   "p1": "131 (abstract)",
   "pn": "",
   "abstract": [
    "The identification of speech contrasts is known to depend on the integration of multiple acoustic cues. The DWS model (Nittrouer & Miller, 1997), based on previous results from Nittrouer et al (Nittrouer, 1992, 1996; Nittrouer & Studdert-Kennedy, 1987), predicts that children will weight dynamic acoustic information more while adults will weight static acoustic information more. The first goal of the present study was to evaluate the development of cue weighting strategies in French monolingual adults and children from 7 to 12 years of age. Listeners had to identify a [s-S] continuum followed by the vowel [a] or [u] whose formant transition configurations were appropriate for a preceding [s] or a preceding [S]. We observed that: 1) Adults weighted all the acoustic cues signaling the contrast more than children did, contrary to DWS predictions. 2) The weights of the transition and the vowel context increased at a younger age than did the weight of the frication cue, indicating that the weighting of different cues may not develop simultaneously. The same experiment was conducted with dyslexics, known to experience categorical perception deficit (Godfrey, Syrdal-Lasky, Millay, & Knox, 1981; Serniclaes, Sprenger- Charolles, Carré, & Demonet, 2001), and with the same chronological age (CAC) and reading level controls (RLC). 3) Dyslexics weighted frication similarly to the CAC but weighted transition and vowel less than did the CAC, indicating only some cues might be processed differently by dyslexic children. 4) Dyslexics weighted transitions and vowels more than the RLC did, indicating a developmental delay rather than a deviant developmental trajectory.\n",
    ""
   ]
  },
  "mueller05_psp": {
   "authors": [
    [
     "Jutta",
     "Mueller"
    ],
    [
     "Angela D.",
     "Friederici"
    ]
   ],
   "title": "Generalization of language rules: syntactic and thematic processing of unfamiliar words",
   "original": "psp5_132",
   "page_count": 0,
   "order": 62,
   "p1": "132",
   "pn": "",
   "abstract": [
    "Event-related potentials (ERPs) provide reliable measures of specific processes involved in native language comprehension, such as syntactic and semantic analysis of sentences. ERP responses of late second language (L2) learners were shown to differ from those of natives specifically in the syntactic domain (Weber-Fox & Neville, 1996). Recently, some ERP experiments simulated L2 processing in the model of miniature languages. The observed ERPs were remarkably similar to those observed in native speakers (Friederici, Steinhauer & Pfeifer, 2002; Mueller, Hahne, Fujii & Friederici, in press). The studies suggest that some native-like ERP responses emerge already after a short period of training if high behavioural skills are assured. The present experiment tested if learners of a miniature language can generalize the acquired sentence processing mechanisms to new lexical items.\n",
    "We conducted an ERP experiment with German participants who had previously learned the miniature language Mini-Nihongo, which is a subset of natural Japanese. Participants were presented correct sentences of Mini-Nihongo and sentences which contained either word category or case violations. Half of the sentences contained only familiar words, the other half included an unfamiliar word. Unfamiliar words occurred at the position which was critical for the violation detection.\n",
    "Case violations, which were indicated by incorrect morphological case markers, elicited an N400-P600 pattern in familiar-word sentences. The N400 was interpreted as reflecting difficulties in the thematic ordering of the arguments while the P600 was seen as indication of controlled syntactic processes of repair. In unfamiliar-word sentences only the P600 was observed. Thus, the lexical difficulties affected thematic processes while controlled syntactic repair processes were unimpaired.\n",
    "Word category violations, which were indicated by a verb (morphologically unmarked) in a position in which only nouns (morphologically marked) were licensed, elicited an early negativity and a P600 in familiar-word sentences. The early negativity was related to the unexpected prosody of incorrect sentences and the P600 to processes of syntactic repair. In unfamiliar-word sentences only the early negativity was present, suggesting that prosody is processed irrespective of the presence of lexical information. The absence of the P600 effect could result from an enhanced P600 for the correct condition.\n",
    "The differences between familiar and unfamiliar-word sentences suggest differential sensitivities of the different ERP components with respect to the accessability of lexical information. The P600 difference between the case and the word category violation condition may result from differences in the morphological markedness of the critical words.\n",
    ""
   ]
  },
  "nenonen05_psp": {
   "authors": [
    [
     "Sari",
     "Nenonen"
    ],
    [
     "Anna",
     "Shestakova"
    ],
    [
     "Minna",
     "Huotilainen"
    ],
    [
     "Risto",
     "Näätänen"
    ]
   ],
   "title": "Speech-sound duration processing in a second language is specific to phonetic categories",
   "original": "psp5_133",
   "page_count": 0,
   "order": 63,
   "p1": "133 (abstract)",
   "pn": "",
   "abstract": [
    "Mismatch negativity (MMN) is a component of the auditory event-related potential (ERP) that reflects pre-attentive and automatic detection of acoustic changes in repetitive sound features (see Näätänen, 2001, for a review). In several studies, the MMN has been used as an index of duration-discrimination sensitivity in the brain. In the present crosslinguistic study, we used the MMN to determine the effect of native language, Russian, on the processing of speech-sound duration in a second language (L2), Finnish, that uses duration as a cue for phonological distinction. Based on the hypotheses of Flegeís (1995) Speech Learning Model (SLM), the nativelanguage effect was compared with Finnish vowels that either can or cannot be categorized using the Russian phonological system (ìsimilarî and ìnewî sounds in the SLM, respectively). The results showed that in Russian L2 users of Finnish, the durationchange MMN for the Finnish sounds that could be categorized through Russian was reduced in comparison with that for the Finnish sounds having no Russian equivalent. For the Finnish sounds that have no Russian equivalent, new vowel categories independent of the native Russian language have apparently been established, enabling a native-like attuning of duration processing. However, in the sounds that can be mapped through the Russian phonological system, plastic changes facilitating the duration processing may be inhibited by the native Russian language.\n",
    "s Näätänen, R. (2001). The perception of speech sounds by the human brain as reflected by the mismatch negativity (MMN) and its magnetic equivalent (MMNm). Psychophysiology, 8, 1-21. Flege, J.E. (1995). Second language speech learning. Theory, Findings, and problems. In W. Strange (Ed.), Speech perception and linguistic experience. Issues in cross-language research (pp. 233-277). Timonium, MD: York press.\n",
    ""
   ]
  },
  "pape05_psp": {
   "authors": [
    [
     "Daniel",
     "Pape"
    ],
    [
     "Christine",
     "Mooshammer"
    ],
    [
     "Susanne",
     "Fuchs"
    ],
    [
     "Phil",
     "Hoole"
    ]
   ],
   "title": "Intrinsic pitch differences between German vowels /i:/, /i/ and /y:/ in a crosslinguistic perception experiment",
   "original": "psp5_134",
   "page_count": 4,
   "order": 64,
   "p1": "134",
   "pn": "137",
   "abstract": [
    "Perceptual pitch differences between high vs. low vowels with identical F0 have been reported in the literature (Fowler 1997, Stoll 1984). However, the reasons for this perceptual pitch shifts are controversial: Fowler (1997) claimed a compensatory mechanism for intrinsic F0 differences to avoid disturbance of prosodic parsing. Stoll (1984) argued that a psychoacoustic pitch shift accounts for the pitch differences. In German a specific phenomenon is found: Tense and lax vowels differ in their tongue height. Interestingly, tense vowels tend to have the same fundamental frequency as their lax counterparts, which is in contradiction to common explanation theories for intrinsic F0. An explanation could be provided by Hoole (2004) who found higher CT activity for lax vowels in comparison to tense vowels, speaking for an active compensation mechanism for F0.\n",
    "To test whether German listeners judge an laryngeal activity or the F0 a vowel pitch comparison experiment was conducted for three different German vowels showing a similar phonological vowel height, but differing in their tense/lax distinction and roundedness. To test whether pitch perception was influenced by language inventory the same perception experiment was conducted crosslinguistically with Catalan listeners in Spain (who do not have a tense/lax distinction nor front rounded vowels).\n",
    "The differences in tenseness were not significant in either language family, therefore pitch does not seem to be perceptually exploited by listeners to enhance vowel differentiation for the tense-lax distinction, at least for the conditions in the given experiment. The difference in roundedness showed significant pitch differences in both languages. This can be explained by an influence of 'sibilant pitch' (Traunmüller 1987) on the general pitch judgements of the listeners. Musical education significantly influenced sensitivity to pitch differences for German listeners, the values are in accordance to results found in literature (Rauscher and Hinton 2003). Surprisingly, Catalan listeners show an insensitivity to even large F0 differences and judge the pitch differences in vowels mainly based on the categorical vowel identity. More surprisingly, this insensitivity in pitch discrimination for Catalan listeners was not dependent on musical education.\n",
    "It is not clear what mechanism causes the existing different pitch sensitivity in the two languages. The design of a new pitch discrimination experiment will be presented which will examine the underlying mechanisms and clarify the causes for the differences in pitch perception, i.e. testing the possible existence of different pitch perception mechanism for speech and complex musical tones.\n",
    ""
   ]
  },
  "fuchs05_psp": {
   "authors": [
    [
     "Susanne",
     "Fuchs"
    ],
    [
     "Jana",
     "Brunner"
    ],
    [
     "Pascal",
     "Perrier"
    ],
    [
     "Christian",
     "Geng"
    ],
    [
     "Bernd",
     "Pompino-Marschall"
    ]
   ],
   "title": "The potential role of speaker's vocal tract morphology onto speech perception: preliminary results",
   "original": "psp5_138",
   "page_count": 4,
   "order": 65,
   "p1": "138",
   "pn": "141",
   "abstract": [
    "Perkell et al. (2003) have experimentally shown for American English native speakers relations between perceptual discrimination capacity and articulatory gestures accuracy: the best the discrimination, the more canonical and the less variable the articulation. This exemplifies how speech production and speech perception interact in their developments and how they contribute to shape each other. In the same vein, our work addresses the issue whether morphological characteristics of the vocal tract can influence perceptual patterns and discrimination. In a previous work on token-to-token variability during vowel production in German (Mooshammer et al. 2004), it was shown that for high/front vowels speaker dependent morphological differences of the palate shape influenced the articulatory variability. Indeed, for the subject having a flat coronal palate shape, tongue sensors located in the mid-sagittal plane exhibited very small, nearly circular dispersion ellipses, whereas two speakers with dome shaped palates showed distinctively larger variability patterns. In accordance with Perkell et al. (1997) we interpreted these results with respect to perception: for a flat palate small articulatory changes have a large impact onto the acoustics, in comparison with a dome shaped palate. Thus, in order to avoid perceptual confusion between neighboring vowels, speakers with a flat palate shape could control their tongue positioning more precisely. The current study aims at assessing the relation between vocal tract morphology and the structure of the perception space of the speakers. First, we investigate the relation between formants variability and palatal shape by means of simulations with a 2D biomechanical tongue model (see e.g. Perrier et al. 2003). Simulations were run for sequences /kak/, /kik/ and /kuk/ for a set of different muscle activations for each vowel (resulting in realistic variability of tongue positioning for each vowel) and for different palate shapes (the default palate of the model, a flat and a dome shaped palate). Area functions of the vocal tract (Perrier et al. JSLR, 1992). The five first formants were, then, computed, and finally the sound was synthesized with a Klatt Synthesizer. Results provide evidence in the predicted direction, i.e. changes in articulatory movements have a larger auditory impact when the flat palate shape, and it is especially strong for high/front vowels. In second phase we have run perceptual tests of these artificial stimuli with the three subjects analyzed in Mooshammer et al. (2004), and the link between perceptual discrimination and the shape of the palate was assessed.\n",
    ""
   ]
  },
  "sereno05_psp": {
   "authors": [
    [
     "Joan",
     "Sereno"
    ],
    [
     "Allard",
     "Jongman"
    ]
   ],
   "title": "Language learning in adults and children",
   "original": "psp5_142",
   "page_count": 0,
   "order": 66,
   "p1": "142 (abstract)",
   "pn": "",
   "abstract": [
    "The present study investigates how linguistic categories develop in adults and children. Challenging the classical critical period hypothesis, recent research has shown that the adult perceptual system may be more plastic than previously thought. These studies indicate that cortical representations may be continuously shaped throughout life, with accumulating evidence documenting second language learners' ability to perceive and produce non-native contrasts at any age. The present experiments extend this research on the learning of language contrasts by investigating not only what is learned but also how it is learned. Little is known about the acquisition pattern itself, particularly about the shape of the learning curve for new linguistic contrasts and how this affects ultimate attainment.\n",
    "The present study therefore focused on the learning that occurs during the process of acquisition by assessing learning at different stages in the training regime, examining the step-by-step acquisition of a non-native language contrast. We focused on the suprasegmental tonal contrast (4 Mandarin tones) that has been used productively in our past research (Wang, Spence, Jongman, and Sereno, 1999) and determined the daily gains in accuracy throughout the training procedure for both adults and children.\n",
    "Six adult native speakers and four elementary school children (ages 6-11) participated. The stimuli were monosyllabic Mandarin words presented in isolation. Five different talkers of Mandarin recorded the stimuli, one talker for the test stimuli (Pretest and Posttests) and 4 talkers for the Training stimuli. The procedure consisted of a Pretest, 6 Training sessions with feedback, and 6 Posttests. Listeners were instructed to indicate which of the four Mandarin tones they heard by pressing one of 4 buttons in front of them.\n",
    "Overall, adult trainees improved by 19% from Pretest to the final Posttest, very similar to our earlier results showing an overall 21% improvement. Children also did very well, showing a similar improvement (19%) over the 6 training sessions. However, overall accuracy rates were well below those of the adults, with the children only achieving a final 40% correct identification rate while the adults averaged 69% after all training sessions. Differences in the nature of the learning across adults and children were also observed. The present study will evaluate adult-child differences in the acquisition of a novel language contrast and consider the implications for mechanisms involved in learning.\n",
    ""
   ]
  },
  "stacey05_psp": {
   "authors": [
    [
     "Paula",
     "Stacey"
    ],
    [
     "Quentin",
     "Summerfield"
    ]
   ],
   "title": "Auditory-perceptual training using a simulation of a cochlear-implant system: a controlled study",
   "original": "psp5_143",
   "page_count": 4,
   "order": 67,
   "p1": "143",
   "pn": "146",
   "abstract": [
    "Speech recognition with a cochlear implant is impaired by tonotopic misalignment between the frequency band transmitted by an electrode and the characteristic frequency of the location of that electrode (Skinner et al., 2002: JARO 3 332-350). Experiments with normally-hearing subjects who listen to simulations of the information provided by cochlear implants have also shown that speech recognition is poor when signals are spectrally shifted to simulate tonotopic misalignment (Rosen et al., 1999: JASA 106, 3629-3636). Although subjects can partially adapt to such signals (e.g. Rosen et al., 1999), previous studies have not distinguished between improvements that result from training and improvements that result from repeated testing.\n",
    "The present experiment investigated the effectiveness of an unsupervised high-variability lexical training regime in 32 normally-hearing subjects who listened through an 8-channel vocoder simulation of the information provided by a cochlear implant. A 3-mm basalward tonotopic misalignment was simulated. The training task was a 2-alternative forced-choice procedure. On each trial, subjects chose which of two words displayed orthographically on a computer screen matched a target word. Feedback on accuracy was given. In the auditory version of the training task, targets were presented acoustically. In the control version, targets were presented orthographically masked by visual noise. Following baseline testing, all subjects received a session of 1200 trials of auditory training and a session of 1200 trials of control training in a counter-balanced order. Half the subjects received High-Variability auditory training, in which targets were recorded by 10 talkers (4 males, 4 females, 2 female children). The other half received Single- Talker auditory training, in which targets were recorded by a single male. Effects of training were assessed by measuring the percentage of key words correctly identified in IEEE sentences recorded by 10 talkers.\n",
    "Auditory training was more effective than control training: control training produced an improvement of 4.1% (95% c.i. 2.0-6.2%), while auditory training produced an improvement of 9.4% (95% c.i. 7.4- 11.4%). There was a trend for High-Variability auditory training to be associated with greater improvements (mean improvement 11.5%, 95% c.i. 8.7-14.2%) than Single-Talker auditory training (mean improvement 7.4%, 95% c.i. 4.6-10.1%).\n",
    "These results show that auditory training is effective, even when exposure to the training vocabulary and the test materials is controlled. Although the benefits of (a single session of) training were small, the results are encouraging as they indicate that an unsupervised computer-based lexical training regime can improve the ability to recognise spectrally distorted speech.\n",
    ""
   ]
  },
  "uther05_psp": {
   "authors": [
    [
     "Maria",
     "Uther"
    ],
    [
     "Anu",
     "Kujala"
    ],
    [
     "Minna",
     "Huotilainen"
    ],
    [
     "Yury",
     "Shtyrov"
    ],
    [
     "Risto",
     "Näätänen"
    ]
   ],
   "title": "Learning morse code results in cortical plastic changes: evidence from ERPs.",
   "original": "psp5_147",
   "page_count": 0,
   "order": 68,
   "p1": "147",
   "pn": "",
   "abstract": [
    "This study examined whether learning Morse code could result in cortical plasticity in processing of acoustic features, as indexed by the mismatch negativity (MMN) and P3a components of the auditory event-related potential (ERP). ERPs were recorded in 9 subjects who were learning Morse code. The subjects were presented with auditory stimuli at 3 different times relative to their training (before, during and after). These stimuli were presented within an auditory 'oddball' paradigm, with repetitive standard stimuli interspersed by one of 3 infrequent deviant stimuli (duration, frequency or ISI). The data showed that there was a significant increase in the P3a only for frequency deviants as a function of training. There were no differences in MMN amplitude as a function of training. These data are interpreted in terms of an attentional switching to unfamiliar changes that would not be expected whilst receiving Morse code.\n",
    ""
   ]
  },
  "winters05_psp": {
   "authors": [
    [
     "Stephen",
     "Winters"
    ],
    [
     "David",
     "Pisoni"
    ]
   ],
   "title": "When and why feedback matters in the perceptual learning of visual properties of speech",
   "original": "psp5_148",
   "page_count": 4,
   "order": 69,
   "p1": "148",
   "pn": "151",
   "abstract": [
    "This study investigated the effects of feedback on the perception of words in point-light and fullyilluminated displays of speech. Participants attempted to identify individual words that they could see (but not hear) being spoken. After attempting to identify the word in each visual-only stimulus, the participants were informed of the identity of the word by receiving feedback in one of three forms: seeing the visual stimulus again, with sound (AV feedback); hearing only the audio track from the original visual stimulus (audio-only feedback); reading the word that was spoken (orthographic feedback). Another group of participants, in a control condition, did not receive any feedback after each trial (no feedback).\n",
    "It was expected that correct identification scores would improve more over the duration of the experiment for the groups of participants that received feedback than for the group of participants that did not receive feedback. It was also expected that correct identification scores would improve more for the groups that received feedback which provided event-related information about the original stimulus (AV feedback, audio-only feedback) than for the group which received orthographic feedback, which is abstract in nature and not directly related to the event(s) that the participants are being asked to perceive.\n",
    "It was found that effects of feedback on correct identification scores only emerged when the participants saw stimuli again, after they had already received feedback on them. Under these conditions, the groups of participants who received feedback correctly identified the visual-only stimuli significantly better than did the group of participants who received no feedback. Furthermore, the type of feedback also had an effect on the amount of perceptual improvement made by the participants who saw the point-light stimuli. With each successive repetition of a stimulus, the group of participants who received audio-visual feedback had increasingly higher correct identification scores than did the participants who received either audio-only or orthographic feedback. This pattern of results suggests that providing feedback to participants in the form of the original stimulus facilitates the perceptual learning of repeated stimuli more than does feedback which only provides derived or abstract information about the original stimulus.\n",
    ""
   ]
  },
  "yoo05_psp": {
   "authors": [
    [
     "Julie",
     "Yoo"
    ],
    [
     "Frank",
     "Guenther"
    ],
    [
     "Joseph",
     "Perkell"
    ]
   ],
   "title": "Cortical networks underlying audio-visual speech perception in normally hearing and hearing impaired",
   "original": "psp5_152",
   "page_count": 4,
   "order": 70,
   "p1": "152",
   "pn": "155",
   "abstract": [
    "Functional magnetic resonance imaging (fMRI) was used to investigate the brain activity underlying audio-visual speech perception in normally hearing and congenitally deaf individuals. Data were collected while subjects experienced three different types of speech stimuli: audio stimuli without visual input, video of a speaking face without audio input, and video of a speaking face with audio input. A control condition consisted of viewing a blank screen. The stimuli were vowels or CVCV syllables, presented in different blocks.\n",
    "Brain regions that were involved in the lipreading process were identified for both normally hearing and congenitally deaf subject groups. Differences in brain activation patterns across these two subject groups were studied. Active brain regions for normal hearing subjects during the visual-only condition included visual cortex, angular gyrus, fusiform gyrus, and auditory cortex, as well as premotor areas in the frontal cortex. The pattern of activation found for deaf subjects while viewing visual-only stimuli was similar to that of normal hearing subjects, but showed distinctly more activity in the right hemisphere (for both vowels and CVCVs), and far less activity in premotor and parietal cortex. Interestingly, the pattern of activity for deaf subjects in the visual-only case was found to be similar to normal-hearing subject activation in the audio-visual case. Additionally, for deaf subjects, inferior cerebellum was active, particularly in right hemisphere, but not in normal hearing subjects.\n",
    "Finally, effective connectivity analyses (structural equation modeling and dynamic causal modeling) were performed to investigate neural connectivity between brain regions in the different conditions for both groups of subjects. Preliminary results from both structural equation modeling and dynamic causal modeling analyses suggest that a visual cortex => fusiform gyrus => auditory cortex pathway may be the primary projection in which visual speech is processed to create an auditory percept. [Research Supported by NIDCD.]\n",
    ""
   ]
  },
  "roth05_psp": {
   "authors": [
    [
     "Daphne Ari-Even",
     "Roth"
    ],
    [
     "Liat",
     "Kishon-Rabin"
    ],
    [
     "Minka",
     "Hildesheimer"
    ],
    [
     "Avi",
     "Karni"
    ]
   ],
   "title": "Asymmetrical generalization of learning in a speech-in-noise identification task to the untrained ear",
   "original": "psp5_157",
   "page_count": 0,
   "order": 71,
   "p1": "157 (abstract)",
   "pn": "",
   "abstract": [
    "One important aspect in adult human perceptual learning is studying the generalization of learning to untrained conditions (e.g. untrained stimuli, untrained paired organ etc.). The limits of generalization have been used as a behavioral tool assumed to reflect the level at which the neural changes underlying the learning process occur. Studies in the visual and motor modalities have reported inconclusive findings regarding generalization of learning to the untrained paired organ. In the auditory modality, there are only limited reports on the generalization of learning a non-verbal auditory task to the untrained ear. The aim of the present study was to investigate whether improved performance following training on a verbal auditory task generalizes to the untrained ear. A total of 45 right-handed, normal-hearing young adults, participated in a single-session intensive training experiment. Subjects were trained monaurally to identify the consonant-vowel stimuli /da/-/ga/ embedded in increasing levels of white noise. A group of 23 subjects were trained on the left ear and a second group of 22 additional subjects were trained on the right ear. Twenty-four hours post-training, gains in performance were first assessed in the trained ear. Generalization of learning to the untrained ear was assessed at the same speech-to-noise ratio as for the trained ear. Subjects were exposed to the stimuli in the untrained ear only at the 24 hours post-training session. Results show that following training, both groups showed similar significant delayed gains in performance for the trained ear. Improvement, however, did not generalize to the untrained ear similarly in both groups. Specifically, subjects trained in the right ear demonstrated generalization of learning to the untrained left ear, while those trained on the left ear showed generalization to the right untrained ear but to a lesser extent. To our knowledge, these findings provide first evidence suggesting an asymmetrical generalization of auditory learning of verbal stimuli to the untrained ear following single-session training. These findings may contribute to further understanding of auditory learning and brain lateralization as well as to the design of effective training protocols for enhanced auditory perception.\n",
    ""
   ]
  },
  "chait05_psp": {
   "authors": [
    [
     "Maria",
     "Chait"
    ],
    [
     "Steven",
     "Greenberg"
    ],
    [
     "Takayuki",
     "Arai"
    ],
    [
     "Jonathan",
     "Simon"
    ],
    [
     "David",
     "Poeppel"
    ]
   ],
   "title": "Two time scales in speech segmentation",
   "original": "psp5_158",
   "page_count": 0,
   "order": 72,
   "p1": "158 (abstract)",
   "pn": "",
   "abstract": [
    "The basic units into which the speech-signal is segmented is an issue of central importance for speech research. Evidence points to the perceptual reality of both the phonetic segment (~30ms) and the syllable (~300ms) during the course of speech processing. We propose a new method of systematically examining the extraction and combination of these informational constituents of speech. We build on accumulating evidence regarding the importance of temporal envelope for speech processing to create a specially crafted stimulus. The original signal is split into 1/3 octavewide bands, the amplitude envelope from each band is extracted and low or high-passed before being combined again with the carrier signal. The result for each signal (S) is S_low and S_high, containing only low or high modulation information. We demonstrate that although each of these, when presented separately in intelligibility judgment tasks, has low (c.a. 40% and 15%) intelligibility, the dichotic presentation of S_low with S_high results in significantly better (c.a. 70%) performance, suggesting a binding mechanism between the syllabic and segmental information. To investigate the properties of this mechanism, we introduce a time-shift in the onset of S_low relative to S_high. Asynchronies <45ms have no effect on intelligibility, performance declines sharply between 45-150 ms, remaining constant thereafter. This evidence suggests a process of multi-resolution analysis (MRA) of speech: segmental and supra-segmental information are extracted simultaneously but separately from the input stream from 'short' (~30ms) and 'long' (~300ms) windows of integration. These streams are then bound together to create a stable representation which is the perceptual unit that is used for subsequent higher order perceptual computations. Crucially, according to this model, supra-segmental units as well as phoneme-sized units are equally fundamental. We discuss these findings in light of other experimental evidence and suggest that the envelope carries information that is critical for our ability to segment speech and that the precise information extracted from these temporal-integration windows depends on phonological and prosodic constraints related to the listeners' native language. These segmentation mechanisms, or 'temporal windows' are plastic and 'stretch'/'shrink' up to a certain degree, but once the 'segmentation cues' begin to come too fast (timecompressed speech experiments) or too slow (temporally-segmented speech experiments), they fail. The MRA model is an attempt to bridge the gap between speech psychophysics and auditory neuroscience, by providing common terminology to describe, understand and integrate the experimental results in these often independent fields.\n",
    ""
   ]
  },
  "creel05_psp": {
   "authors": [
    [
     "Sarah",
     "Creel"
    ],
    [
     "Richard",
     "Aslin"
    ],
    [
     "Michael",
     "Tanenhaus"
    ]
   ],
   "title": "The salience of consonants and vowels in learning an artificial lexicon: the effects of noise",
   "original": "psp5_159",
   "page_count": 4,
   "order": 73,
   "p1": "159",
   "pn": "162",
   "abstract": [
    "We examined the effects of acoustic noise on the relative weightings of different segment types during lexical learning and lexical retrieval. In previous research, we showed that learners more often confused newly learned words disambiguated only by their vowels (\"consonant-matched\") than words disambiguated only by consonants (\"vowelmatched\"). This suggests that, at least within the English phonological system, consonants are more integral to lexical identity than vowels.\n",
    "However, in natural listening situations segmental information is often obscured by various types of noise. In such situations, the learner/perceiver might benefit from the use of different, more reliable cues to word identity, rather than a fixed set of weights assigned to cues in low-noise conditions.\n",
    "We designed an artificial lexicon consisting of 16 CVCV words, using 4 consonants and 4 vowels. Each C and V occurred equally often in each word position across the set. For each word (dabo), another word shared the same consonants (dubeI) and a third had the same vowels (gapo). Listeners learned these words as names for 16 black-and-white shapes. There were 384 exposure trials (24 per word); in each, a picture was presented, its name was spoken, and the participant clicked on the picture to proceed to the next trial. We then tested listeners in a 4AFC task, where 4 pictures were presented and the name of one of them was spoken. The participant was asked to click on that picture. On each trial, one picture had a name with the same vowels or same consonants as the target picture; the other two pictures had phonologically unrelated names. Learning and test stimuli were presented either without noise or embedded in white noise (3 dB SNR).\n",
    "When words were learned and tested without noise, consonant-matched items were more often confused with one another than vowel-matched items, replicating our earlier findings. When learning and test items were embedded in noise, overall error rates did not increase compared to the no noise condition. However, the pattern of errors did change: vowel-matched items were much more often mistakenly selected, indicating that vowels were more robustly used to identify words in the noise condition than in the no noise condition. Thus, consonant and vowel information was weighted differently in the presence of noise. Our results support a mechanism of adaptive plasticity whereby acoustic/phonetic cues are re-weighted to maximize the intelligibility of words during lexical learning and lexical retrieval.\n",
    ""
   ]
  },
  "fonteneau05_psp": {
   "authors": [
    [
     "Elisabeth",
     "Fonteneau"
    ],
    [
     "Heather van der",
     "Lely"
    ]
   ],
   "title": "Auditory neurophysiological signatures of tone discrimination in children with SLI",
   "original": "psp5_163",
   "page_count": 0,
   "order": 74,
   "p1": "163 (abstract)",
   "pn": "",
   "abstract": [
    "In this study we investigate an influential theory which claims that developmental disorders of higherlevel cognitive processing, in particular Specific Language Impairment (SLI), is caused by a deficit in the speed of auditory processing (a domain-general account, Tallal, et al.,1985). Previous research using behavioural techniques reveals contradictory results and indicates that at least some SLI children do not have auditory deficits. Instead their deficits appear to be restricted to the grammatical system (a domain-specific account, van der Lely, 2005). We use a neuroimaging technique that has the required excellent temporal resolution, event-related potentials (ERP) to illuminate this controversy. We aim, first to define the neural correlates (temporal, amplitude and topography) of auditory evoked potentials (AEPs: N1, P2, N2 and P3) in children with SLI. Second, we aim to define age-related changes of these neurophysiological markers in typically and atypically developing individuals. We utilized highresolution electroencephalography (Electrical Geodesic Inc, 128 electrodes) to investigate auditory ERPs from different groups: adults (18-38 years old), G(rammatical) SLI children (10-22 years old, for selection criteria see van der Lely, 2005), age match controls (matched with G-SLI children on sex, age, laterality and non-verbal IQ), and 7-8 years old language match controls (matched with the G-SLI group on receptive vocabulary). A classical auditory oddball paradigm was used with two pure tones: standard tones (1000 Hz, 80 %) and target tones (2000 Hz, 20%) with a stimulus onset asynchrony of 800 ms (SOA). Participants were asked to detect the target tones and to press a button as quick as possible. During the seven-minute period of stimulation, participants were asked to look at pictures of animals presented visually. The resulting auditory evoked potentials from all groups showed an N1 after about 100 ms followed by a P2 around 180 ms for both tones, and a P300 component for the target tones. Decrease in latencies and increase in amplitudes revealed a maturation of the auditory system. Furthermore, qualitative differences in the auditory evoked potentials were recorded from the younger children (7-8 years old) with a prominent N2 component for the standard tones (Johnstone, et al., 1996). More importantly, children with G-SLI revealed age-appropriate waveforms for the N1/P2 complex and the P300 component. Our results indicate that G-SLI children do not show a deficit in the discrimination of pure tones. We discuss the implications of our results for disentangling the domain-general vs. domain-specific hypotheses of SLI.\n",
    ""
   ]
  },
  "hamann05_psp": {
   "authors": [
    [
     "Silke",
     "Hamann"
    ],
    [
     "Anke",
     "Sennema"
    ]
   ],
   "title": "Voiced labio-dental fricatives or glides - all the same to Germans",
   "original": "psp5_164",
   "page_count": 4,
   "order": 75,
   "p1": "164",
   "pn": "167",
   "abstract": [
    "Dutch has a cross-linguistically very unusual contrast in labio-dental fricatives, namely one between a fully voiced [V], a voiced and simultaneously fricated [v], and a voiceless [f]. Phonologically, these sounds are categorised as voiced glide and voiced and voiceless fricatives, respectively (Booij 1995, Gussenhoven 1999). Minimal triplets illustrating the contrast are given in (1).\n",
    "(1) wee vee fee 'ache, cattle, fairy'; wijl vijl feil 'while, rasp, error'\n",
    "German learners of Dutch have major problems acquiring this contrast since their native language differentiates only a voiced and a voiceless labiodental fricative ([v] and [f], respectively). German speakers usually do not distinguish the Dutch glide and voiced fricative but collapse both into one category corresponding to their native voiced fricative. This is due to the fact that both Dutch categories show vocal fold vibration, which is the only cue to distinguish German [v] from the voiceless [f] (Jessen 1998). The present study investigates the acquisition of the Dutch labio-dental fricative system by two groups of German learners of Dutch, ten in an early stage of their acquisition, and ten Germans proficient in Dutch (living in the Netherlands for more than 4 years). For this purpose, a Dutch native speaker with a three-way contrast was recorded reading five times a list that contained ten minimal triplets as in (1) and 40 filler triplets, embedded in a carrier sentence. The resulting items were randomised and auditorily presented to the subjects, who had to click on one of the orthographic representations of the three possible answer words on a computer screen. After the perception experiment, the subjects had to read the same list as the Dutch native speaker. Preliminary results indicate that proficient speakers of Dutch can distinguish better between voiced fricative and glide than beginners, but only a very small number of proficient speakers can actually produce the Dutch three-way contrast consistently.\n",
    ""
   ]
  },
  "hanpejaudier05_psp": {
   "authors": [
    [
     "Yumi",
     "Han-Pejaudier"
    ]
   ],
   "title": "Korean children living in france and becoming bilingual: prosodic, pragmatic, and lexical aspects",
   "original": "psp5_168",
   "page_count": 4,
   "order": 76,
   "p1": "168",
   "pn": "171",
   "abstract": [
    "This article is devoted to the study of two children aged from 2 years and 6 months old to 3 years and 10 months old, who are confronted with the parallel acquisition of the Korean language and the French second language after the installation of their families in France. The corpus analysis obtained throughout this study permit us to clarify the specific role of the prosodic realisation during this period between 3 ;1 and 3 ;2 years old. A period when two distincts types of two elements utterances (each from his own. prosodic system, either Koean the other French) are established. In fact, this capacity of prosodic differenciation is based on the knowledge of pragmatic conditions of language interaction. It is a matter of taking the interlocutors and the situation into account. This language choice according to interlocutors must also allow children to differenciate two lexicons according to their needs. We can see how a child, at the time of language choice and being submit to context needs, gives importance to the French language at the dominant language. This adjustement of pragmatic differenciation in favour of one language takes place between 3 ;7 and 3 ;9 years old. The same tendancy is found during the lexical choice, when the presence is more and more marked by French words in the Korean utterances which happens between 3 ;8 and 3 ;10 years old respectively. Owing to the study of the installation of French dominant language, we can see that pragmatic diffrenciation comes before lexical differenciation with one month interval. This work would be permit to observe how in the early bilingualism Korean-French the capacity to diffrenciate languages appears first from prosodicpragmatic mastery and then lexical mastery.\n",
    ""
   ]
  },
  "hazan05_psp": {
   "authors": [
    [
     "Valerie",
     "Hazan"
    ],
    [
     "Paul",
     "Iverson"
    ],
    [
     "Kerry",
     "Bannister"
    ]
   ],
   "title": "The effect of acoustic enhancement and variability on phonetic category learning by L2 learners",
   "original": "psp5_172",
   "page_count": 0,
   "order": 77,
   "p1": "172 (abstract)",
   "pn": "",
   "abstract": [
    "The success of high-variability phonetic training techniques (HVPT) for second-language learners (e.g., Japanese adults learning English /r/-/l/) demonstrates that plasticity in speech perception is retained through adulthood. HVPT has been thought to be effective because it exposes listeners to naturalistic phonetic variability, from many talkers and phonetic contexts [e.g., Logan et al., J. Acoust. Soc. Am. 89, 874-886 (1991)]. The present investigation examined: (a) whether the learning of new phonetic categories is best promoted by exposing the learner to natural token variability or by enhancing key acoustic cues and (b) whether exposure to natural tokens at some stage of the auditory training is essential for category learning.\n",
    "Groups of Japanese listeners were trained on the /r/- /l/ distinction in initial position using one of four training methods. In all training methods, listeners were exposed to tokens produced by 10 talkers during ten training sessions. In the HVPT training method, they were trained using natural stimuli only. In the All Enhanced training method, F3 was set to extreme values and listeners were never exposed to natural stimuli. In the Perceptual Fading training method, the amount of F3 enhancement progressively decreased during training; in the Secondary-Cue Variability training method, variability in cues that are not informative for the /r/-/l/ distinction (F2 and closure duration) was progressively increased during training.\n",
    "All four training methods were similarly effective in improving /r/-/l/ identification by an average of 15- 20 percentage points for Japanese adults. The presence of naturalistic phonetic variability may not therefore be critical to the effectiveness of HVPT. The implications of these findings for exemplar models of speech perception will be discussed. There appeared to be no advantage in enhancing acoustic cues that are critical to the phonetic contrast. [Work supported by ESRC grant RES-000-22-0445]\n",
    ""
   ]
  },
  "hirata05_psp": {
   "authors": [
    [
     "Yukari",
     "Hirata"
    ]
   ],
   "title": "Effects of speaking rate on native English speakers' identification of Japanese vowel length",
   "original": "psp5_173",
   "page_count": 0,
   "order": 78,
   "p1": "173 (abstract)",
   "pn": "",
   "abstract": [
    "Japanese vowel length distinction is phonemic and is difficult for adult native English (NE) speakers to acquire. A Japanese long vowel is longer in duration than a short vowel by definition (with little difference in formant frequencies), but a long vowel spoken quickly can be shorter than a short vowel spoken slowly. Native Japanese speakers accurately identify length of vowels at varied rates by using the rate of the sentence as a perceptual cue, i.e., by way of rate normalization (Hirata and Lambacher, 2004). However, it is not known to what extent NE speakers can normalize rate and accurately identify Japanese vowel length at varied rates. If rate normalization is part of a general auditory process (Pisoni et al., 1983), and therefore, to occur even for beginning second language (L2) learners, their identification accuracy would not differ between sentences spoken quickly and slowly. However, if rate normalization is not an automatic but a learned process, beginning L2 learners' accuracy might differ between different speech rates, perhaps biased by absolute vowel duration.\n",
    "Participants were sixteen NE speakers who had studied Japanese for fifteen weeks in the U.S. Stimuli were three types of Japanese disyllabic nonwords, /mVmV/ (two short vowels), /mV:mV/ (long and short vowels), and /mVmV:/ (short and long vowels) spoken in a carrier sentence at fast and slow rates by a native Japanese speaker. All the three word types had a pitch accent on the first syllable. The stimuli with two rates and three word types were all mixed and randomized. Participants choose one of the four alternatives: short+short, long+short, short+long, and long+long. Results showed no main effect of rate: the overall identification accuracy did not significantly differ between the fast (69.8%) and slow rates (74.2%). Thus, participants were overall able to adjust their identification according to the rate of stimuli. The effect of word type was significant: long+short type (86.9%) > short+short type (72.9%) > short+long type (56.1%). The lowest accuracy for the short+long type was probably because NE speakers were biased to perceive unaccented vowels as short (Minagawa et al., 2002). A significant interaction of rate and word type was found: only for the short+long type, accuracy was higher for the slow (65.8%) than the fast rate (46.5%). This implies that rate normalization is not an automatic process for beginning L2 learners when dealing with a very difficult word type, and that slower rates help for accurate identification.\n",
    ""
   ]
  },
  "howell05_psp": {
   "authors": [
    [
     "Peter",
     "Howell"
    ],
    [
     "Stephen",
     "Davis"
    ],
    [
     "Kate",
     "Watkins"
    ],
    [
     "Katharina",
     "Dworzynski"
    ],
    [
     "Ceri",
     "Savage"
    ]
   ],
   "title": "Plasticity in recovery and persistence of stuttering",
   "original": "psp5_174",
   "page_count": 4,
   "order": 79,
   "p1": "174",
   "pn": "177",
   "abstract": [
    "Recovery from a speech disorder may indicate that correction can be made to the neural control systems that subserve speech production. Conversely, people who persist in this same speech disorder may lack such plasticity. In this submission we report on longitudinal work with a group of individuals all of whom stuttered in childhood. It is known which of these individuals, now aged 18 and over, have persisted in the disorder and which have recovered. This has allowed us to investigate what factors led speakers who persist in their stutter to be less 'plastic' than those who recover. We report results addressing whether speakers who persist in their stutter (SPS) show different types of disfluency in their speech to speakers who recover from their stutter (SRS) at the onset of the disorder. There appear to be no difference between SPS and SRS near onset of the disorder. We also report results addressing whether SPS differ in type of disfluency form SRS in adulthood. Here there are differences between the speaker groups (the SPS show a disproportionate number of part word disfluencies on content words). Data on the family background of stuttering in the two groups showed similar patterns, suggesting that persistent stuttering is a functional rather than a biological disorder. At the time of writing this abstract, we have completed structural and functional scanning studies on these two groups of speakers. The results are not available at present but these were also designed to test whether there is a biological problem that leads to stuttering persisting in some speakers. The earlier family history data suggest that there will be no biostructural differences between the two speaker groups. Thus, it would appear that one of the groups of speakers who stutter in early life have behavioral plasticity. Results are discussed that examine whether it is the SRS who change (a plastic process of recovery) or the SPS (a maladaptive plastic process these speakers adopt).\n",
    ""
   ]
  },
  "jones05_psp": {
   "authors": [
    [
     "Mark",
     "Jones"
    ],
    [
     "Rachael-Anne",
     "Knight"
    ]
   ],
   "title": "Non-plasticity and the perceptual equivalence of 'defective' and non-defective /r/ realisations",
   "original": "psp5_178",
   "page_count": 0,
   "order": 80,
   "p1": "178 (abstract)",
   "pn": "",
   "abstract": [
    "Children typically acquire adult realisations of /r/ after most other adult-like targets (Vihman 1996: 218-219). Impressionistic transcriptions in the literature often equate defective realisations of approximant /r/ with [w], or are labelled as labiodental approximants. For most children, a perceptual miscategorisation of adult /r/ realisations as /w/ does not explain these patterns (Slawinski and Fitzgerald 1998). Instrumental production data are lacking. However, in many cases of apparent [w] realisations, 'covert contrasts' too fine-grained for adult transcribers to detect and represent in transcription may occur (see Scobbie et al. 2000).\n",
    "The status of labiodental /r/ as a defect has been called into question in recent acoustic work on adult speech (Jones 2004a, b). Principled similarities between the acoustic structure of labiodental /r/ (possibly involving lingual constriction) and the acoustic structure of apical approximant /r/ suggest that labiodental /r/ users may aim to produce a midfrequency resonance at around 1400-1600 Hz. For labiodental /r/ users, this resonance is F2, for apical approximant /r/ users, it is F2+F3 (or F2+FR, Stevens 1998). Labiodental /r/ may persist into adulthood and be hard to correct in clinic because it fulfils a specific intended function, rather than being a 'best attempt' due to immature control of the articulators.\n",
    "Regardless of its origins, in continuing realisations associated with child-speech into adulthood, the use of labiodental /r/ is evidence for non-plasticity. Those speakers who employ it have 'fossilised' this aspect of their speech, irrespective of the overwhelming majority of non-labiodental tokens of /r/ in the linguistic environment. This latter point raises problems for phonetically-detailed exemplarbased models of speech perception which view frequency of occurrence as an important determinant of target forms (e.g. Pierrehumbert 2001).\n",
    "The research presented here will begin to address the question of why labiodental /r/ users maintain a realisation of /r/ which differs from the majority of /r/ tokens around them. Following on from the hypothesis that labiodental /r/ users view labiodental /r/ as perceptually equivalent to apical approximant /r/, we test this empirically in perceptual experiments of labiodental /r/ users and nondefective /r/ users. The implications of the results for clinical treatment of labiodental /r/ and phonetically-detailed exemplar-based models of speech perception are discussed.\n",
    ""
   ]
  },
  "kalliorinne05_psp": {
   "authors": [
    [
     "Virpi",
     "Kalliorinne"
    ],
    [
     "Maija S.",
     "Peltola"
    ],
    [
     "Olli",
     "Aaltonen"
    ]
   ],
   "title": "Perception of non-native vowels by Finnish learners of French",
   "original": "psp5_179",
   "page_count": 4,
   "order": 81,
   "p1": "179",
   "pn": "182",
   "abstract": [
    "Non-native speech sound perception is based on the mother tongue. In this study we tested the identification, goodness rating and discrimination performances of Finnish learners of French to see whether they have acquired the French perceptual patterns. We examined a back vowel continuum with only two categories in Finnish (/o/ and /a/), but three in French (/o/, open /o/ and /a/). According to Flege's Speech Learning Model (1987), learners may have difficulties with perceiving the French open /o/ because of its similarity with the Finnish /o/. Three groups were included: 1) advanced Finnish students of French (n=9), 2) native French speakers (n=10) and 3) monolingual Finns (n=12). Groups 1 and 2 listened to the French vowels and group 3 to the Finnish ones. The stimulus continuum consisted of 12 synthesized vowels. The F1 values varied with steps of 20.4 mels (480-740 Hz), and F2 values with steps of 20.9 mels (850-1200 Hz).\n",
    "The results showed that students and French speakers identified the stimulus continuum similarly. The locations of category boundaries did not differ significantly, but - at the shared boundary between Finnish /o/-/a/ and French open /o/-/a/ - the students' identification consistency was low in comparison with the monolingual Finns (F(1,19)=10,879, p=0,004). In goodness rating, students and French speakers evaluated the quality of the shared boundary differently from the monolingual Finns (F(2,28)=9,929, p=0,001). The discrimination task indicated that monolingual Finns showed an increased discrimination function at the boundary in comparison with the central category members. In contrast, this pattern was not found in students, or in French speakers. Consequently, it seems that Finnish learners of French perceived the stimuli in accordance with the French system. However, the inconsistency in identification task may imply that the students continued to hesitate in their responses, which may reflect the ongoing nature of the learning process. In short, the present results supported the findings by Flege et al. (1999) where they showed that perception can become native-like in advanced students of language.\n",
    "s Flege, J. 1987. The production of \"new\" and \"similar\" phones in a foreign language: evidence for the effect of equivalence classification. Journal of Phonetics 15, 47-65. Flege, J., MacKay, I. & Meador, D. 1999. Native Italian speakers' perception and production of English vowels. Journal of the Acoustical Society of America 106 (5), 2973-2987.\n",
    ""
   ]
  },
  "kotz05_psp": {
   "authors": [
    [
     "Sonja",
     "Kotz"
    ],
    [
     "Johannes",
     "Schwarz"
    ],
    [
     "Dirk",
     "Winkler"
    ],
    [
     "Christoph",
     "Preul"
    ],
    [
     "D. Yves von",
     "Cramon"
    ],
    [
     "Angela D.",
     "Friederici"
    ]
   ],
   "title": "Recovery of syntactic function during auditory language processing following subthalamic nucleus stimulation",
   "original": "psp5_183",
   "page_count": 0,
   "order": 82,
   "p1": "183 (abstract)",
   "pn": "",
   "abstract": [
    "The sequencing of auditory information during language perception seems to be modulated by subcortical structures such as the basal ganglia (BG). In particular, syntactic reanalysis processes as reflected in the P600 component of the event-related brain potential (ERP) are affected in patients with lesions [1] or neurodegenerative change [2] of the BG during auditory language processing. One critical question is whether this deficit is function specific or not. The role of the BG in language perception has been discussed controversial [3]. However, their involvement in language production [4], attention and memory [5], and timing [6,7] find support. Recent work with subcortical patients has demonstrated that external rhythmic auditory stimulation re-elicits the P600 during the presentation of syntactically erroneous sentences [8]. This result is in line with a compensation hypothesis put forward in the motor domain. If a sensory stimulus is predictive (serially or isochronously) the impairment of the BG can be compensated by overactivation in pre-motor areas [9,10]. The current auditory experiment followed up the compensation hypothesis by investigating patients with deep brain stimulation (DBS) of the subthalamic nucleus. If DBS correlates with compensation in pre-motor areas, auditory sequencing should be facilitated and the P600 reelicited. Patients were tested in an on/off paradigm before and after DBS placement, respectively. While the P600 was not elicited by syntactically erroneous sentences before DBS placement, the P600 resurfaced after DBS placement during both the on and off testing. It is important to note that the patients always showed a P300, a component correlated with non-linguistic attentional processes, ensuring their capacity to perceive and understand both task and stimulus material. We conclude that the current data support a compensation hypothesis in the auditory language domain comparable to the motor domain.\n",
    "s\n",
    "Kotz S.A. et al., JINS, 9, 1053-1060 (2003) Friederici A.D. et al., Neuropsychology, 17, 133-142 (2003) Nadeau, S.E. & Crosson, B., Brain and Language, 58, 355-402, 1997) Alexander, M.P. et al., Brain, 110, 961-991, (1987) Grossman, M. et al., Brain and Language, 42, 347-384, (1992) Gibbon, J. et al., Curr Opin Neurobio, 7, 170- 184, (1997) Meck, W.H. & Benson, A.M., Brain Cogn, 48, 195-211, (2002) Kotz S.A. & Gunter, T.C., (subm) McIntosh, G.C. et al., J Neurl Neurosurg Psychiatry, 62, 22-26, (1997) Samuel, M. et al., Brain, 120, 963-976, (1997)\n",
    ""
   ]
  },
  "nazzi05_psp": {
   "authors": [
    [
     "Thierry",
     "Nazzi"
    ]
   ],
   "title": "Perceived phonetic details are not necessarily used in lexical acquisition",
   "original": "psp5_184",
   "page_count": 4,
   "order": 83,
   "p1": "184",
   "pn": "187",
   "abstract": [
    "Previous research by Nazzi and Gopnik (2001) used an object manipulation task to evaluate the ability of English-learning infants to form new object categories based on naming information alone. Results showed that naming information was used at 20 months, but not at 16 months. Given results on the lexical development of the infants tested, these results further suggest a link between the emergence of name-based categorization and changes in lexical development. After having replicated this effect in a population of Frenchlearning infants, we used this procedure to evaluate whether French-learning 20-month-olds use detailed phonetic information when simultaneously learning two new word/category pairings. We explored minimal contrasts on both consonants and vowels. Infants performed above chance level on minimal consonantal contrasts, whether the contrast was in word-initial position (e.g., /pize/ vs./tize/) or not (e.g., /pide/ vs. /pigue/). On the other hand, infants failed to learn when they were presented with words differing only by one of their vowels, independently of the position of the contrasting vowels within the word or of their acoustic distance (e.g., /pize/- /puze/, or /pize/-/paze/, or /pize/-/pizou/). This last result suggests that contrasts that have been shown to be discriminated in early infancy are not necessarily used to contrast words at the onset of lexical acquisition. Moreover, the comparison of the results for consonants and vowels suggests that these infants make different use of consonants and vowels in early lexical acquisition, a result compatible with the proposal that consonants and vowels play different roles in languages (Nespor, Pea & Mehler, 2003). Ongoing research is exploring whether this differential effect is better interpreted in terms of a consonant/vowel difference, or in terms of continuous on-continuous phonemes, by testing infants with pairs of words contrasting by continuous consonants (e.g., /lize/-/rize/).\n",
    ""
   ]
  },
  "nguyen05_psp": {
   "authors": [
    [
     "Noel",
     "Nguyen"
    ],
    [
     "Leonardo",
     "Lancia"
    ],
    [
     "Maïtine",
     "Bergounioux"
    ],
    [
     "Sophie",
     "Wauquier-Gravelines"
    ],
    [
     "Betty",
     "Tuller"
    ]
   ],
   "title": "Role of training and short-term context effects in the perception of /s/ and /st/ in French",
   "original": "psp5_188",
   "page_count": 0,
   "order": 84,
   "p1": "188 (abstract)",
   "pn": "",
   "abstract": [
    "Recent work has shown that the perceptual categories built up in the identification of speech sounds remain highly plastic in adults and continuously evolve along a wide range of time scales, that extend from the short (local contextual effects) to the very long (across the listener's lifetime). Tuller and colleagues (Tuller et al., 1994; Case et al., 1995) examined how English-speaking listeners responded to manipulations in the order of presentation of a set of stimuli ranging on a \"say\"- \"stay\" continuum. The response patterns showed a number of dynamical characteristics which included hysteresis and enhanced contrast. These results were accounted for by a non-linear dynamical model in which perceptual categories are associated with attractors of a potential function. In this model, the listener's response is governed by the acoustic properties of the stimulus, the previous percept, and the combined effects of learning, linguistic experience and attentional factors.\n",
    "In the present work, Tuller et al.'s experimental paradigm is extended to French, with a view to compare dynamical perceptual patterns in both languages. We also seek to examine the influence of previous experience on the listener's behavior, by manipulating the listener's level of phonetic training. A third objective is to define a quantitative method for evaluating the model's goodness of fit with the perceptual data.\n",
    "The material was composed of fifteen stimuli on a continuum between \"c?pe\" /sEp/ and \"steppe\" /stEp/. Each stimulus contained a silent interval ranging from 0 (Stimulus 1) to 56 ms (Stimulus 15) in 4-ms steps between the /s/ and the vowel. Twelve native speakers of French, divided in two groups matched in age and educational level, took part in the experiment. Group 1 was composed of trained phoneticians, while Group 2 had little or no background in phonetics. The fifteen stimuli were presented to each listener both in random and sequential (1-15-1 or 15-1-15) order. The presentation order changed between consecutive runs.\n",
    "Preliminary results suggest that hysteresis (a tendency for the listener's initial response to persist across the continuum when the stimuli are presented sequentially) was the most common response pattern. Responses proved much less stable for Group 2 than for Group 1, as shown by the higher number of flip-flops (switches between the two possible responses in sequential presentations) for Group 2. These results are consistent with the hypothesis that training enhances the stability of perceptual attractors in the categorization of speech sounds.\n",
    ""
   ]
  },
  "shestakova05_psp": {
   "authors": [
    [
     "Anna",
     "Shestakova"
    ],
    [
     "Elvira",
     "Brattico"
    ],
    [
     "Minna",
     "Huotilainen"
    ]
   ],
   "title": "Abstract phoneme representations in children: a magnetic mismatch negativity study",
   "original": "psp5_189",
   "page_count": 0,
   "order": 85,
   "p1": "189 (abstract)",
   "pn": "",
   "abstract": [
    "Our study aimed at seeking for a neural correlate of automatic phoneme-category discrimination in children in the condition of vast natural acoustical variation of the male voice. Mismatch negativity, MMN, an auditory event-related response to any discriminable change in an ongoing sound stream, as well as its magnetic counterpart MMNm, and the MMN paradigm as such, have been shown to be extremely useful in studies of speech perception in humans. In practice, the MMN or MMNm can be seen in a difference waveform elicited by subtracting the response to a standard stimulus from the response to a deviant stimulus. (Näätänen and Winkler, 1999). Importantly, MMN is more resistant to the attentional manipulations than other endogenous component, e.g. N2b or P3a, which makes it a very suitable tool for studying auditory discrimination in children (Cheour et al., 2000). A number of earlier studies using MMN and MMNm provided evidence for the existence of language-specific memory traces which guide the categorical perception of speech sounds in the auditory cortex (Näätänen et al., 1997; Dehaene-Lambertz et al., 2000). In adults, a robust MMNm can be recorded in the left hemisphere in response to the change across the phoneme categories despite the vast acoustical variation of stimuli (Shestakova et al., 2002). The present study aimed at probing whether such long-term memory traces could be found in pre-school and early-schoolage children. The magnetoencephalogram (MEG) was recorded in Russian 6-year-old children with a whole-head magnetometer. Stimuli were produced by 450 speakers and randomly distributed across the three phonetic categories of Russian. The MEG data were analyzed using Equivalent Current Dipoles (ECD) and Minimum Current Estimate (MCE) (Hämäläinen et al., 1993). The preliminary results showed that despite the correct behavioral vowel identification no difference between the standard and deviant response could be found in the magnetic responses of the children given that theirs obligatory responses were robust. From our findings we can conlude the following. First, in children neural populations which differ from those in adults might be involved in the preattentive category-change processing. Second, it might be that in 6-ear-old children the vowel-category discrimination is not fully automatic. Therefore it would be absolutely necessary to add a forced-choice task in addition to the main experiment and replicate the magnetoencephalographic experiment using electroencephalogram.\n",
    ""
   ]
  },
  "thomson05_psp": {
   "authors": [
    [
     "Jenny",
     "Thomson"
    ],
    [
     "Torsten",
     "Baldeweg"
    ],
    [
     "Usha",
     "Goswami"
    ]
   ],
   "title": "Amplitude envelope onsets and dyslexia: a behavioural and electrophysiological study",
   "original": "psp5_190",
   "page_count": 0,
   "order": 86,
   "p1": "190 (abstract)",
   "pn": "",
   "abstract": [
    "With a core deficit in phonological representation widely accepted as a key feature of developmental dyslexia, the research field has moved to try and understand the underlying mechanisms of this phonological deficit. Many theories posit auditory perceptual causes, yet in the findings reported poor auditory skills only tend characterise a subset of the dyslexic group assessed. One reason for this may be that studies have tended to restrict their focus to acoustic cues acting within the transient time window of phonemic perception. If we take a developmental perspective, it may be more logical to examine acoustic cues within longer temporal time frames and working at a more basic level of speech processing, such as those associated with the amplitude envelope of the speech stream. Such cues are known to be important for very young infants when initially attempting to segment the stream of continuous speech around them. In previous behavioural studies we have found that amplitude envelope onset detection (AEOD) can be highly predictive of concurrent reading performance in school age dyslexic children and their controls across languages (Goswami et al., 2002; Richardson et al., 2004; Muneaux et al., 2004). This study aimed to characterise the neural correlate of the behavioural insensitivity to AEO rise time observed in dyslexic children. 8 dyslexic children (mean age 10 years) and 9 age-matched controls have participated to date. N1 potentials have been evoked to non-speech tones with 15ms and 90ms AEO rise times. The preliminary results show that whilst the amplitude of the N1 waveforms for the typicallydeveloping children varies as a function of AEO rise time, the dyslexic children's N1s exhibit no such differentiation. This finding supports the hypothesis that children with dyslexia have a developmental delay, manifest at a neural level, in their perception of basic auditory suprasegmental cues.\n",
    ""
   ]
  },
  "wang05_psp": {
   "authors": [
    [
     "Hongyan",
     "Wang"
    ],
    [
     "Vincent van",
     "Heuven"
    ]
   ],
   "title": "Plasticity in vowel and consonant perception by Chinese learners of Dutch-accented English",
   "original": "psp5_191",
   "page_count": 4,
   "order": 87,
   "p1": "191",
   "pn": "194",
   "abstract": [
    "Native language acts as a filter in speech perception for L2 learners, so that the sounds of L2 are typically perceived in terms of the categories of the L1. However, learners may adapt their perceptual system so as to be better tuned to the contrasts that are functional in the new language. The aim of our research is to better understand (the extent of) the ability for adult L2 learners to adapt their speech perception to a new language. The present research focuses on adaptation not to the native norm for the target language but to a non-native type of pronunciation which is neither the L1 of the learner nor that of the target language. In the experiment we followed ten adult Chinese learners of English (aged 24-31 at arrival) who studied in the Netherlands for more than two years. The language of instruction, and in much of everyday communication, was English. The Dutch pride themselves of being excellent speakers of English and do not encourage foreigners to speak Dutch to them. Inevitably, therefore, foreigners communicate with Dutch nationals in non-native English (disparagingly called Dunglish by native speakers of English). We wished to determine to what extent the Chinese learners of English would become better tuned to Dutch-accented English, and to what extent their perception of (American) native English might still benefit from exposure to Dutch-accented English. The first part of our experiment was done shortly after the students' arrival in the Netherlands; the second part was done after two years' exposure to Dutch accented-English. In both sections we ran phoneme identification tests using the same materials (19 vowels in /hVd/ contexts and 24 consonants in /aCa/ contexts) produced by Chinese, Dutch and American speakers of English. We test the hypothesis that our Chinese learners will identify the English vowels and consonants more successfully in the re-run of the experiment, but only for the Dutchaccented variety of English. The perception of the Chinese-accented variety of English does not benefit from prolonged exposure to Dutch English, nor do the results generalize to (American) native English. Specifically, our results show that the confusion structure in the vowel and consonant identification tests converges toward the pattern we found for Dutch listeners having to identify vowels and consonants in Dutch-accented English.\n",
    ""
   ]
  },
  "agrawal05_psp": {
   "authors": [
    [
     "Deepashri",
     "Agrawal"
    ],
    [
     "S. R.",
     "Savithri"
    ]
   ],
   "title": "Perception of non-native speech sounds",
   "original": "psp5_207",
   "page_count": 4,
   "order": 88,
   "p1": "207",
   "pn": "210",
   "abstract": [
    "Speech perception is a specialised aspect of general human ability,the ability to seek and recognize patterns.The present study investigates the auditory discrimination of the three form of laterals /l/ that are dental,retroflex,palatal in native and non native children and adults. Malyalam has all three forms of laterals on the other hand Hindi does not have the palatal and retroflex form.Malyalam was the native language and Hindi was the non native language.Four groups of normal hearing subjects participated in the study.Group I consisted of 10 native Malyalam speakers in age group of 4-6 years, Group II had 10 native Malyalam speakers in age group of 19-21 years,Group III consisted of 10 Hindi speakers in the age group of 4-6 years and Group IV consisted of 10 Hindi speakers in the age group of 19-21 years. The material consisted of 42 bisyllabic meaningful Malyalam words with all three types of laterals in word medial and word final positions. Two sets of words were made.There were 16 words in Set I (Palatal-Retroflex) and 26 words in Set II (Retroflex-Dental form).Each word was paired with the same word also, each word with Palatal lateral was paired with the retroflex form.In Set II each word with retroflex form was paired with same word and also, with similar word with dental form.A total of 8 word pairs were made. These word pairs were iterated thrice and randomised to make 24 and 39 word pairs,respectively.A 22 years old female Malyalam speaker uttered these word pairs with a interpair interval of 5 seconds. All the word pairs were audiorecorded and these formed the stimuli. Subjects were tested individually.They were audiopresented with the word pairs and were asked to identify the two words in a pair as 'Same' or 'Different' and to respond on a binary forced choice format provided. The responses of the subjects were tabulated and percent 'same' - 'Different' was calculated and Native Malyalam adults had 97% and 89.22% in Set I and Set II,respectively.Native Malyalam speaking children had 95.41% and 91.27% in Set I and Set II, respectively. Non native adult speakers had 83% and 79.77% in Set I and Set II,respectively. Non Native children had 71% and 55% on Set I and Set II,respectively. The results indicated that the discrimination was poor in non native speakers compared to native speakers. Also, non native children had poor discrimination compared to non native adults. The results did not support the Universal Theory.\n",
    ""
   ]
  },
  "biersack05_psp": {
   "authors": [
    [
     "Sonja",
     "Biersack"
    ],
    [
     "Vera",
     "Kempe"
    ]
   ],
   "title": "Tracing vocal expression of emotion along the speech chain: do listeners perceive what speakers feel?",
   "original": "psp5_211",
   "page_count": 4,
   "order": 89,
   "p1": "211",
   "pn": "214",
   "abstract": [
    "To date, research on the vocal expression of emotion has mainly focused on the relationship between acted emotions and vocal cues on one hand, and vocal cues and perceived emotions on the other hand (Scherer, 2003), implying a close agreement between what is produced and what is perceived. This study examines whether emotional valence reported by speakers and emotional valence perceived by listeners are linked to the same vocal cues. We reassess the congruity between production and perception in the domain of emotion expression by tracing the functionality of vocal cues along the speech chain. Eighty-eight men and 112 women rated their current emotional state using the Brief Mood Introspection Scale (Meyer & Gaschke, 1988), and produced a target sentence which was embedded in a referential communication task. Twenty other participants rated the target sentences for perceived happiness on a scale from 1 to 7. The first interesting result was that reported positive emotion and perceived happiness were higher in women, but were not correlated within the genders. Furthermore, from the target sentences, we obtained measures for pitch, pitch range, speech rate, intensity, shimmer and jitter, as well as the first and second formants. All acoustic measures except speech rate were converted into z-scores for men and women separately to normalise for gender. Correlations between the acoustic measures, reported emotional valence, and perceived happiness showed that positive emotion reported by the speaker was positively correlated with pitch range. Stepwise regression analyses confirmed pitch range as the best predictor of reported positive emotion. Perceived happiness was correlated with higher pitch, faster speech rate, wider pitch range, steeper declination, and lower perturbations. These findings suggest that most vocal cues used by the listener in emotion perception may not necessarily be indicative of the emotional state of the speaker, and support the view that vocal cues, especially cues related to timing and speech rate, are not just epiphenomena of the emotional state of the speaker, but may serve as signals to affect the listener (Owren & Bachorowski, 2003). Still, pitch range seems to be the most reliable indicator of the valence of the speaker's emotion suggesting that it can mediate between experienced and perceived emotions, and may be the vocal cue most closely associated with genuine emotion expression. More generally, our results cast doubt on a direct mapping of vocal cues between perception and production in the domain of emotion expression.\n",
    ""
   ]
  },
  "denham05_psp": {
   "authors": [
    [
     "Susan",
     "Denham"
    ],
    [
     "Martin",
     "Coath"
    ]
   ],
   "title": "A model base upon response fields derived during early experience can account for the interference effects of synthetically degraded speech signals",
   "original": "psp5_215",
   "page_count": 4,
   "order": 90,
   "p1": "215",
   "pn": "218",
   "abstract": [
    "In animals adult-like response properties in cortex develop through exposure to sounds during an early critical period [1]. The structure of spectrotemporal response fields (STRFs) in human auditory cortex is not known, but if they too develop through early acoustic experience then it seems likely that speech might play a large part in their formation. We investigated this hypothesis by developing a model of auditory processing in which STRFs were derived from fragments of a limited set of utterances [2]. We found that the responses of an ensemble of STRFs supported the classification of novel words and was robust to variability introduced by different speakers, sex and accents. Furthermore, the ensemble response could be interpreted in qualitatively different ways; eg. to classify the sex and identity of the speaker, or the prosody of the word [3]. The summed response of the ensemble of STRFs clearly indicates the presence of discrete events in an ongoing stream of sounds, and provides a way of quantifying the responsiveness of the model to arbitrary sounds. This suggests that the strength of the ensemble response to a sound could be used to predict its effectiveness as an interferer; the stronger the response the more interfering the sound. The entropy of the ensemble response was quantified for the range of degradations used in a recent study [4], and mirrored the interference effects of time-reversed speech, sine-wave speech and modulated sine-band speech. However, the basic linear model failed completely to account for the reduction of interference with increasing number of noise bands in modulated noise band speech. The introduction of an output non-linearity in the form of divisive inhibition [5] between the STRFs rectified this problem. We conclude that the model may provide a useful way to predict the degree to which any sounds will interfere with speech perception and could be used to investigate the influence of different acoustic environments on the formation of STRFs in early development and on subsequent perceptual abilities.\n",
    "s Zhang, L.I., S. Bao, and M.M. Merzenich, Nat Neurosci, 2001. 4(11): p. 1123-30. Coath, M. and S.L. Denham, Biological Cybernetics, 2004. submitted. Coath, M., et al., Network: Computation in Neural Systems special issue on Sensory Coding And The Natural Environment, 2004. submitted. Brungart, D.S., et al., J Acoust Soc Am, 2005. 117(1): p. 292-304. Schwartz, O. and E.P. Simoncelli. in NIPS-00. 2000. Denver: MIT Press.\n",
    ""
   ]
  },
  "geng05_psp": {
   "authors": [
    [
     "Christian",
     "Geng"
    ],
    [
     "Katalin",
     "Mady"
    ],
    [
     "Caroline",
     "Bogliotti"
    ],
    [
     "Souhila",
     "Messaoud-Galusi"
    ],
    [
     "Vicky",
     "Medina"
    ],
    [
     "Willy",
     "Serniclaes"
    ]
   ],
   "title": "Do palatal consonants correspond to the fourth category in the perceptual F2-F3 space?",
   "original": "psp5_219",
   "page_count": 4,
   "order": 91,
   "p1": "219",
   "pn": "222",
   "abstract": [
    "In recent years, the cross-linguistic plasticity of perceptual boundaries has drawn a lot of attention. It is often assumed that natural phonetic boundaries in the pre-linguistic child are replaced by languagespecific ones. For consonantal perception, the most prominent empirical domains encompass research on voicing and stop place of articulation perception. Specifically, for place perception, a vast amount of studies on the transitional cues has evolved in the tradition of Delattre et al. (1955, for a review see Sussman et al., 1998). In our view, it is noteworthy that this research has almost exclusively focused on the three oral stop place categories velar, alveolar and bilabial, leaving the palatal place of articulation aside.\n",
    "The focus on this particular issue emerged as the result of our previous research (Carré et al., 2002, Serniclaes et al., 2003): According to these studies, perception of consonant place of articulation is organized around a central reference given by the neutral vowel. In the neutral vocalic context, place boundaries correspond to flat F2-F3 transitions in stimuli where the latter covary. When separately modified, the perceptual F2-F3 space is divided into four distinct regions. In Serniclaes et al. (2003), we have investigated the partitioning of this space into the three phonologically relevant stop place categories present in the French language. Results indicated that even when natural boundaries are located inside phonological categories, they can still affect consonant discrimination. Furthermore, nonphonological categories remained perceptible for some adult subjects: A subset of the participants exhibited above threshold (50 %) identification responses for a category absent in their language. The purpose of the present work is to see whether it is possible to relate this additional category to the palatal stop place of articulation of a four categorylanguage. To achieve this, we will conduct a replication of the identification and discrimination experiments reported in Serniclaes et al.(2003). Positive results would substantiate the palatal place of articulation to be grounded in a natural, phonetically motivated category. Negative results, on the contrary, would indicate that the palatal stop does not rely on the information present in the transitions of the second and third formants: The perception of the palatal place of articulation would then not mainly rely on F2-F3 formant transitions. Rather, the burst characteristics, or still other acoustic cues, would play a major role in the identification of palatal consonants.\n",
    ""
   ]
  },
  "hay05_psp": {
   "authors": [
    [
     "Jessica",
     "Hay"
    ],
    [
     "Adrian",
     "Garcia-Sierra"
    ]
   ],
   "title": "Effects of linguistic experience on perception and learnability of non-speech categories",
   "original": "psp5_223",
   "page_count": 4,
   "order": 92,
   "p1": "223",
   "pn": "226",
   "abstract": [
    "Experience with a native language causes a drastic re-organization of speech sounds into language-specific categories; such that listeners from two languages may hear the same acoustic sound, yet perceive it as belonging to different categories. The present study addresses the question of whether linguistic experience can affect the perception of non-speech sounds in a similar language-specific manner. Although some speech sounds have nonspeech analogues that are processed by similar mechanisms, it has generally been assumed that experience in one domain does not affect processing in the other. Specifically, researchers who use nonspeech stimuli in their studies, wishing to make conclusions about the manner in which speech is processed, have assumed that perception of nonspeech sounds is unaffected by linguistic experience. As a direct test of this assumption, participants from two languages, English and Spanish, which have different mapping between voice-onset-time (VOT) and their voicing categories, are trained to categorize a series of analogous non-speech (toneonset- time) sounds into two groups. The boundaries between the distributions of tone-onset-time (TOT) stimuli are either consistent or inconsistent with known auditory discontinuities. The number of blocks required to reach criteria performance is assessed, and systematic differences between languages are interpreted in terms of the effects of linguistic experience on the learnability of nonspeech categories. This study addresses the additional question of whether learning non-speech categories affects speech perception. Prior to and following training, participants are asked to label a series of VOT stimuli, providing a measure of the effect of non-speech category training on VOT labeling boundary location. Results have implications for the effects of linguistic experience on auditory processing and learnability, and also address whether short-term experience with non-speech stimuli can affect speech perception.\n",
    ""
   ]
  },
  "johnson05b_psp": {
   "authors": [
    [
     "Elizabeth",
     "Johnson"
    ],
    [
     "Ellen",
     "Westrek"
    ],
    [
     "Thierry",
     "Nazzi"
    ]
   ],
   "title": "The role of language familiarity on early voice discrimination",
   "original": "psp5_227",
   "page_count": 4,
   "order": 93,
   "p1": "227",
   "pn": "230",
   "abstract": [
    "Past research has demonstrated that infants recognize familiar voices from a very early age (DeCasper & Fifer, 1980). However, very little is known about infants' ability to discriminate unfamiliar voices. In the current study, we test infants' ability to discriminate voices in their native language as well as a non-native language. Three experiments using the Visual Fixation Procedure were carried out with Dutch-learning 7- month-olds. In the first two experiments, infants were habituated to three female voices in one language and then tested on a new female voice in the habituated language (Voice Change) and a new female voice in a new language (Language Change). In Experiment 1 (Dutch versus Japanese) and Experiment 2 (Japanese versus Italian), infants dishabituated to Language Change trials. But only Dutch-habituated infants in Experiment 1 dishabituated to Voice Change trials. These results suggest that infants may be more sensitive to voice changes in a familiar language than voice changes in an unfamiliar language. This language familiarity effect could be driven by attention and/or perceptual learning factors. However, an alternative explanation for these results is that the Dutch voices used in Experiment 1 were acoustically more distinct than the Italian and Japanese voices used in Experiments 1 and 2. In order to test this possibility, a third experiment was carried out using backwards speech. Reversing speech retains many acoustic factors that are important for voice identification (fundamental frequency, speech rate, breathiness, etc), but destroys the language-specific prosody that is so important for infants' ability to recognize a language as familiar (Van Lancker, Kreiman, & Emmorey, 1985). Thus, by repeating Experiment 1 with backwards speech, we were able to test the possibility that the infants in Experiment 1 discriminated the Dutch voices more readily than the Japanese voices simply because the Dutch voices were more distinct than the Japanese voices. If this were the case, then we expected the infants in Experiment 3 to dishabituate to Voice Change trials just as the infants in Experiment 1 did. However, the infants tested with backwards speech failed to dishabituate to Voice Change trials or Language Change trials. In combination, these results constitute the first evidence that voice discrimination in infants is affected by language familiarity, a finding with important implications for the study of voice encoding and identification.\n",
    ""
   ]
  },
  "kishonrabin05_psp": {
   "authors": [
    [
     "Liat",
     "Kishon-Rabin"
    ],
    [
     "Liat",
     "Noy"
    ],
    [
     "Noa",
     "Gubi"
    ]
   ],
   "title": "Identification functions of /ba-pa/ continua in noise and their relation to open-set word recognition in noise",
   "original": "psp5_231",
   "page_count": 0,
   "order": 94,
   "p1": "231 (abstract)",
   "pn": "",
   "abstract": [
    "Minka Hildesheimer University of Tel-Aviv Daphne Ari-Even Roth University of Tel-Aviv Difficulty of understanding speech in noise characterizes different populations, such as, children, hearing impaired, learning disabled and the elderly. In order to gain better insight to this problem, we investigated the effect of noise on what is considered the first stage of linguistic processing, that is, the stage at which the hearing system extracts the acoustic information from the physical signal and transforms it to mental representations of phonetic units. These units, also known as speech contrasts, are thought to be the basic building blocks of speech and are assumed to be minimally constrained by linguistic redundancy. The goals of the present study were, therefore: (1) to investigate the effect of background noise on the identification functions of Hebrew voicing in initial position along a continuum of voice-onset-time (VOT), and to compare these functions to those obtained in quiet, and (2) to examine the relationship between identification functions of voicing and word identification in noise. Fourteen normal-hearing, Hebrew-speaking, young adults participated in the study. Naturally produced stimuli consisted of: (1) a /ba-pa/ continuum which varied in VOT values from a lead of -100 ms to a lag of +50 ms in 10 ms steps, and were presented in quiet and in white noise at signal-to-noise ratios (SNR) of 0 and -3; and (2) one-syllable Hebrew AB word lists presented in SNR of 0 and +3. Subjects were asked to label the syllables as /ba/ or /pa/ and repeat the words they heard, respectively. Depended variables included category boundary (CB) at the 50% correct crossover point, the integral (areas) of the functions, and % word correct. Results show that with increasing level of background noise: (1) CB became more negative, i.e., syllables sounded more voiceless than voiced; (2) variability in performance increased among the subjects; (3) the areas under the identification functions decreased, more so for the voiced /ba/; and (4) 43% of the variance in open-set word performance in noise can be explained by the integral of the functions. These results suggest that the degrading effect of background noise is already evident at the very basic level of speech perception processing. It may contribute to our understanding of the difficulties that young children may encounter while forming mental representations of phonetic categories via a \"noisy\" system (internal noise produced by their impaired hearing system and/or external noise from hearing assistive devices and/or the environment).\n",
    ""
   ]
  },
  "larsson05_psp": {
   "authors": [
    [
     "Johan",
     "Larsson"
    ],
    [
     "Fatima",
     "Vera"
    ],
    [
     "Nuria",
     "Sebastian-Galles"
    ],
    [
     "Gustavo",
     "Deco"
    ]
   ],
   "title": "Neurodynamical modelling of long-term plasticity in speech perception",
   "original": "psp5_232",
   "page_count": 0,
   "order": 95,
   "p1": "232 (abstract)",
   "pn": "",
   "abstract": [
    "In an auditory lexical decision task experiment, the performances of highly proficient early Spanish- Catalan and Catalan-Spanish bilinguals were contrasted using Catalan materials (Sebastian-Galles et al. 2005, JML). In this task, two types of experimental non-words were specifically constructed by substituting the Catalan phoneme /E/ for a Catalan /e/, or vice versa. As expected from previous research, Spanish-Catalan bilinguals showed great difficulties in discriminating experimental words from non-words. Indeed, these mistakes were manifestations of a perceptual assimilation in the context of lexical decision; both Catalan phonemes being 'mapped' to a Spanish /E/. Especially interesting was the asymmetry across both experimental conditions for Catalan-dominant bilinguals. They made more mistakes for /E/->/e/ changes, than for /e/->/E/ ones. This pattern was taken as evidence of a developed acceptance for mispronounced Catalan /E/-words, i.e. a sort of long term plasticity, the origin of this phenomenon being exposure to a bilingual environment, where such mispronunciations by Spanish-dominant bilinguals abound. These results lend themselves well to biophysically realistic neurodynamical modelling employing descriptions at the level of spiking neurons and the synaptic activity which evokes the spikes in the neuron membrane potential. Using such a model, we construct a network of pools of neurons, which consequently engage in competitive and cooperative interactions with each other, trying to represent their input in a context-dependent way. Thus we can carefully study the underlying nonlinear dynamics of the context-dependent assimilation effect as manifested in the behavioural results, by using mean-field techniques as well as by running full spiking simulations, looking for stable areas in parameter space exhibiting such phenomena. In particular, the asymmetry phenomenon observed for Catalan-dominant bilinguals, i.e. the 'environmental assimilation effect', is shown by varying network connection strengths, thus representing effects of long-term plasticity. This model makes specific predictions about the consequences at the phoneme level of different patterns of asymmetries (at the lexical-contextual level). Additional data comparing the (behavioural) performance of Catalan-Spanish bilinguals in a discrimination task and a lexical decision task will be presented. The use of a biophysically realistic model further gives us possibilities of making predictions at all neuroscientific levels, namely microscopic (single cells), mesoscopic (ERPs, fMRI) and macroscopic (behaviour), thereby opening for an extension of our study to other types of data when further results are obtained in planned and current relevant experiments.\n",
    ""
   ]
  },
  "mcqueen05_psp": {
   "authors": [
    [
     "James",
     "McQueen"
    ],
    [
     "Holger",
     "Mitterer"
    ]
   ],
   "title": "Lexically-driven perceptual adjustments of vowel categories",
   "original": "psp5_233",
   "page_count": 4,
   "order": 96,
   "p1": "233",
   "pn": "236",
   "abstract": [
    "Previous research has examined lexically-driven perceptual learning about consonants. Norris, McQueen & Cutler (2003) used an exposure-test paradigm. Listeners exposed to an ambiguous fricative in lexical contexts which biased its interpretation towards [f] identified more sounds on an [Ef]-Es] test continuum as [f] than listeners exposed to the same ambiguous sound in [s]-biased lexical contexts. Listeners can thus use lexical knowledge to adjust consonant categories while listening to a speaker who -- perhaps because of dialect differences -- produces unusual tokens of those sounds.\n",
    "Dialect differences are often carried by vowels, however. We therefore tested here whether this type of lexically-driven perceptual learning occurs for vowels. We also tested whether learning generalises to the perception of other vowels. We used the same exposure-test paradigm, with critical stimuli based on the [i]-[e] height distinction. In the lexical decision exposure phase in two experiments, one group of listeners heard ambiguous vowels, midway between [i] and [e], in sequences which were Dutch words if the final syllable contained [i] but not if it contained [e] (e.g., \"satell?t\", from \"satelliet\", satellite), plus unambiguous words with [e] in the final syllable (e.g., \"atleet\", athlete). A second group heard the reverse (e.g., \"atl?t\" and \"satelliet\").\n",
    "In Experiment 1, listeners were tested on three continua, in this order: 1. Exposed: [ift]-[eft]; 2. (or 3.) Near: [Ift]-[Eft] (also a front height contrast); 3. (or 2.) Far: [oft]-[aft] (a low back height contrast); and 4. Exposed. There was a vowel identification shift only in Block 4: Listeners exposed to [?] in [i]- biased contexts identified more sounds on the [ift]- [eft] continuum as [i] than those who heard [?] in [e]-biased contexts. In Experiment 2, these test continua were presented in a different order: 1. Near (or Far); 2. Far (or Near); 3. Exposed; 4. Exposed; 5. Near. There was a reliable lexically-driven shift on the Exposed continuum only in Block 3. Although there was again no effect for the Near continuum, there was for the Far continuum, consistent with the direction of the vowel height adjustment encouraged by the exposure conditions (i.e., more [o] responses from the listeners exposed to [?] in [i]-biased contexts than from those who heard [?] in [e]- biased contexts). These results show that lexicallydriven perceptual adjustments can be made to vowel as well as consonant categories, and can sometimes generalise from the vowel contrast heard during exposure to perception of another vowel contrast.\n",
    ""
   ]
  },
  "miller05_psp": {
   "authors": [
    [
     "James",
     "Miller"
    ],
    [
     "Charles",
     "Watson"
    ],
    [
     "Jonathan",
     "Dalby"
    ],
    [
     "Deborah",
     "Burleson"
    ]
   ],
   "title": "Training experienced hearing-aid users to identify syllable constituents in quiet and noise",
   "original": "psp5_237",
   "page_count": 0,
   "order": 97,
   "p1": "237 (abstract)",
   "pn": "",
   "abstract": [
    "Preliminary results and a proposed Speech Perception Assessment and Training System (SPATS) for the aural rehabilitation of adult hearingaid and cochlear-implant users are described. The proposed system targets bottom-up, or analytic, speech-perception abilities, and has two parts: an assessment component and a training component. The assessment component provides an inventory of a client's abilities to identify the constituents of English syllables, that is, syllable onsets, nuclei, and codas in quiet and in background noise. The training algorithm adaptively selects items and noise levels so that practice is concentrated on items of moderate difficulty weighted by their importance for speech understanding. Item importance is estimated from frequency of occurrence data. Further, learning is enhanced by providing many practice trials, immediate feedback, and, following errors, rapid comparisons of intended sounds with their confusors. To encourage transfer to real-world speech, brief periods of training on words and phrases are interspersed with the training on syllable constituents. Validation studies will measure how well the assessment and training system predicts and improves general success in speech communication. In a preliminary study, five experienced hearing-aid users with sensorineural hearing loss were given 14 hours of intensive training identifying syllable-initial consonants in quiet and noise. Their performance was compared to that of five similar hearing-aid users with no special training. All listeners had moderate to severe hearing losses and had worn hearing aids for at least one year. All were pre-tested with a set of 20 consonants combined with three vowels /i,a,u/ as spoken by six different talkers. Pretests were conducted in quiet and in noise (multi-talker babble) at moderate signal-to-noise ratios (SNR's). Training was conducted with eight target consonants (TCs). The TCs were in each listener's middle range of difficulty and the three most common confusors for each target were individually selected forming target sets of four consonants. Training was conducted in quiet and noise. During training, trial-by-trial feedback was given and, following an error, the listener could rapidly compare the intended syllable with its confusor. In noise, the SNR adapted to a criterion of 80% correct. There were no differences between Training and Control listeners on the Pretests. After training, there was a significant fivepercent advantage for the trained listeners. Training generalized to talkers never heard during training. Graphs depicting the progress of adaptive training in noise are presented. [Supported by NIDCD]\n",
    ""
   ]
  },
  "paulmann05_psp": {
   "authors": [
    [
     "Silke",
     "Paulmann"
    ],
    [
     "Sonja",
     "Kotz"
    ]
   ],
   "title": "ERP evidence on the processing of emotional prosody",
   "original": "psp5_238",
   "page_count": 0,
   "order": 98,
   "p1": "238",
   "pn": "",
   "abstract": [
    "Emotional encoding is central to human communication. Among others, emotional states are communicated by evaluating affective prosody and the accompanying semantics (i.e. the verbal content of the message). Obviously, the two channels do have to interact in order to be able to evaluate the emotional state of a message (or the messenger). However, so far it is yet not fully understood how and at which point in time emotional prosody and (emotional) semantics interact at the sentence level. Previous evidence (Kotz et. al, 2001 & Kotz & Paulmann, in prep.) has shown that the time course of the emotional prosodic processing and semantics differ. In order to further investigate the two channels we carried out two ERP experiments in which we tried to isolate the emotional prosody channel from the semantic content channel using a cross-splicing method. The ERP evidence shows that violations of the emotional prosodic contour (with neutral semantic content in Experiment 1 and pseudo-sentences used in Experiment 2) mainly elicit a positivity whereas violations of the semantic content mainly elicit an N400-like negativity. This pattern seems to evolve independent of speaker voice (male or female). Furthermore, it appears that this pattern accounts for all basic emotions (fear, sad, anger, disgust, happpiness, pleasant surprise and neutral serving as a baseline) investigated in these experiments. Only the results for happiness show deviant ERP results. We conclude that prosodic information is processed differently from semantic information, and that each channel (prosody and semantic) contributes individually to the perception of emotional speech. Furthermore, the data suggest that semantic information can override prosody when the two channels interact in time, that is when the emotional prosodic contour agrees with the semantic content of a sentence.\n",
    ""
   ]
  },
  "peelle05_psp": {
   "authors": [
    [
     "Jonathan",
     "Peelle"
    ],
    [
     "Arthur",
     "Wingfield"
    ]
   ],
   "title": "Effects of adult ageing oo adaptation to time- and frequency-compressed speech",
   "original": "psp5_239",
   "page_count": 0,
   "order": 99,
   "p1": "239",
   "pn": "",
   "abstract": [
    "Healthy aging is typically accompanied by declines in efficacy of both peripheral and central auditory processing. Age-related declines are also present in several domains of cognitive ability. Such changes result in decreased comprehension accuracy for older adults relative to their younger counterparts when listening to altered speech, including timecompressed speech, frequency-compressed speech, and speech in broadband noise. These difficulties raise the question of whether older adults may also be differentially impaired in their ability to adapt to these altered stimuli over time. In the current set of experiments we investigated the degree to which older adult listeners were able to adapt to altered speech compared to young adults. Time compression was performed using a variation of the sampling technique as implemented in SoundEdit software (Macromedia, Inc., San Francisco, CA). Small portions of the speech signal were removed at regular intervals, with the remaining segments abutted in time. The resulting signal retains pitch, amplitude, and relative timing cues, but is reproduced in less time. Frequency compression was accomplished by noise-vocoding the speech signal using 16 analysis bands, and shifting the output of each band downwards after vocoding. The large number of bands ensured that the vocoding process itself only minimally reduced intelligibility, and that most of the perceptual difficult was due to the spectral shifting of the speech information. In Experiment 1, we replicated previous findings with young adults and showed that older adults, when equated for starting accuracy, show similar patterns of perceptual adaptation to time-compressed speech, but fail to transfer this learning to a different speech rate. In Experiment 2, participants received a greater amount of practice than previous studies, which benefited only young adults, and then only at later trials. In Experiment 3 participants adapted to spectrally-shifted noise-vocoded speech, demonstrating that age similarities in perceptual learning are not specific to one type of stimulus. Control experiments demonstrated that there were no age differences in interference effects, and that changes in improvement in either group were not a result of simple strategy change or practice with the recall task. We conclude that fast perceptual learning is comparable in young and older adults, but maintenance and transfer of this learning decline with age.\n",
    ""
   ]
  },
  "pichorafuller05_psp": {
   "authors": [
    [
     "Kathleen",
     "Pichora-Fuller"
    ],
    [
     "Bruce",
     "Schneider"
    ],
    [
     "Nancy",
     "Benson"
    ],
    [
     "Stan",
     "Hamstra"
    ],
    [
     "Edward",
     "Storzer"
    ]
   ],
   "title": "Age-related changes in temporal processing by adults: periodicity and gap coding in speech and non-speech signals",
   "original": "psp5_240",
   "page_count": 0,
   "order": 100,
   "p1": "240 (abstract)",
   "pn": "",
   "abstract": [
    "Auditory temporal processing likely contributes to the difficulty of older adults in understanding speech in noise. Gap detection is one measure of auditory temporal processing. In the present study, we relate this psychoacoustic measure directly to the perception of stop consonant type gaps in speech markers. Ability to detect gaps in speech and nonspeech stimuli was measured in young and old adults with good audiograms and also in young adults in conditions of simulated auditory aging. Auditory aging was simulated using temporal jittering to disrupt the periodicity of the signal. The markers surrounding the gap varied in duration (40 vs 250 msec) and in spectral symmetry. In spectrally symmetrical conditions, the leading and lagging markers were the same: the vowel [u] in speech conditions and a 500-Hz tone in non-speech conditions. In asymmetrical speech conditions, the lagging marker was the same as in the symmetrical conditions, but the leading marker was the consonant [s] in the speech conditions and a broadband noise (1 to 6 kHz) in the non-speech conditions. For the intact stimuli, the gap detection thresholds for both age groups in spectrally symmetrical markers were far smaller than in spectrally asymmetrical markers. In all conditions, gap thresholds were significantly smaller in young adults than in older adults. For both age groups, gaps between spectrally asymmetrical speech markers were detected better than gaps between analogous non-speech stimuli. It is argued that phonological knowledge compensates for auditory processing difficulties in both age groups. For the young adults tested under conditions of simulated auditory aging, gap detection thresholds in the symmetrical conditions were significantly larger than for either age group when intact stimuli were used; however, performance in the asymmetrical conditions was not worse. Both markers used in the symmetrical conditions were periodic (tones or vowels) whereas the leading markers in the asymmetrical conditions were aperiodic (consonants or noise bands); therefore, it is not surprising that the simulation of auditory aging has a pronounced effect on gap detection in the former but not in the latter conditions. The findings are discussed in terms of the different aspects of temporal processing involved in the detection of gaps in different types of stimuli and the age-related changes in different aspects of temporal processing as they may relate to speech perception. Research funded by the International Dyslexia Association, the Natural Sciences and Engineering Research Council of Canada and the Canadian Institutes of Health Research.\n",
    ""
   ]
  },
  "purcell05_psp": {
   "authors": [
    [
     "David",
     "Purcell"
    ],
    [
     "Ingrid",
     "Johnsrude"
    ],
    [
     "Kevin",
     "Munhall"
    ]
   ],
   "title": "Perception of altered formant feedback influences speech production",
   "original": "psp5_241",
   "page_count": 0,
   "order": 101,
   "p1": "241 (abstract)",
   "pn": "",
   "abstract": [
    "Auditory feedback from a speaker's voice is an important sensorimotor control signal. Hearing your own speech is essential for vocal learning in infancy, and is also important for the maintenance of fluent speech articulation as an adult. It is well known that hearing impairment can produce changes in pitch and loudness control as well as changes in the precision and reliability of consonant and vowel production. Recently, changes to this speech feedback have been studied in the laboratory using perturbations of the pitch, amplitude and spectral distribution of speech sounds. While subjects produce speech samples wearing headphones, realtime modifications to the auditory feedback can be carried out through signal processing.\n",
    "In this paper, we present data showing that subjects respond rapidly to formant modification. We also present data on sensitivity to change, both in feedback perception during production, and in perception of recorded samples. In the work reported here, the first formant of steady-state isolated vowels was altered within trials. Since previous studies have employed whispered speech, it was necessary to develop a new system to manipulate the formants of voiced speech using real-time formant tracking and filtering. Formants (such as F1 and/or F2) can be shifted by filtering out the signal at the frequency where the given formant number was estimated, and emphasizing the signal at the new desired formant frequency. This is accomplished using a filter transfer function with a pair of spectral zeroes in the numerator to attenuate the energy (harmonics of the glottal fundamental frequency F0 for vowels) near the existing formant, and a pair of spectral poles in the denominator to amplify energy near the new formant. The first formant of vowel /{/ was manipulated within trials 100% towards either /a/ or /I/ for the individual. Participants responded by altering their production with average F1 compensation as large as 14.4% and 11.3% of the applied formant shift, respectively. Small concomitant changes were also observed in F2. In a separate study, vowel formant discrimination was tested to examine the similarity between speech perception of recorded samples, and the perception of speech feedback during production. The results will be discussed in terms of the relationship between speech perception and production.\n",
    ""
   ]
  },
  "smith05_psp": {
   "authors": [
    [
     "Matthew",
     "Smith"
    ],
    [
     "Andrew",
     "Faulkner"
    ]
   ],
   "title": "Perceptual adaptation by normally-hearing listeners to a simulated 'hole' in hearing",
   "original": "psp5_242",
   "page_count": 0,
   "order": 102,
   "p1": "242",
   "pn": "",
   "abstract": [
    "Distorting the spectral representation of speech signals detrimentally affects speech perception, acutely at least. Studies have examined different distortions including upward shifts in the frequency spectrum, linear and non-linear warping, and spectral compression/expansion. Often, subjects have been given little opportunity to adapt to the distorted speech signals. However, some studies have demonstrated that the detrimental effect of upward shifts in the frequency spectrum can be significantly reduced over three hours of training.\n",
    "Shannon et al. (2002) examined a spectral distortion in connection with 'holes' or dead regions in hearing. Here, the spectral information that would normally be represented in the hole region was shifted to channels either side of the hole. Reassignment partially preserves the information that would be lost but also entails warping the place-code of the spectrum which may in itself lead to poorer performance. Results suggested rerouting information around a hole was no better than simply dropping it. This has implications for cochlear implant patients for whom the pathology of hearing loss may involve regions of dead/dysfunctional neurons (leading to 'holes' in the tonotopic representation of spectral information). However, in this study too, subjects received no training.\n",
    "Here we examine the impact of a hole in the midfrequency region of the spectrum and the ability of listeners to adapt to spectral reassignments similar to those investigated by Shannon and colleagues. Noise-band speech processors were created with three output channels either side of the hole. Spectral information that would normally have been presented to the hole was either dropped or reassigned. Subjects trained in two different reassignment conditions. Performance in speech recognition improved considerably, in one condition sentence scores rising from 32% to 70% after three hours of training. Another group of subjects was trained in a condition where information from the hole region was simply dropped. Though some improvement was observed, scores at the end of training (e.g. 34% for sentences) were significantly lower than the first group achieved in the reassignment conditions. Although it is not possible to say whether further training would have resulted in complete adaptation in the reassignment conditions, these results are consistent with those from other studies that have demonstrated a significant reduction of the detrimental effects of distorting the spectral representation of speech with training.\n",
    ""
   ]
  },
  "thiessen05_psp": {
   "authors": [
    [
     "Erik",
     "Thiessen"
    ]
   ],
   "title": "Distributional information and infants' use of phonemic contrasts for word learning",
   "original": "psp5_243",
   "page_count": 0,
   "order": 103,
   "p1": "243",
   "pn": "",
   "abstract": [
    "When engaged in word-learning tasks, infants between 14 and 17 months appear to be lesss sensitive to phonetic distinctions than they are in discrimination tasks (Stager & Werker, 1997; Werker, Fennell, Corcoran, & Stager, 2002). For example, while children can hear the difference between 'bih' and 'dih,' they treat 'bih' and 'dih' as interchangeable labels in word learning tasks.\n",
    "By 20 months, the disconnect between phonetic discrimination and word-learning has largely disappeared (Werker et al., 2002). One explanation proposed for this is that capacity increases between 14 and 20 months, allowing older children to attend to subtler phonetic distinctions in word-learning tasks. Another explanation is that characteristics of childrens' lexicons determine what distinctions they will use in word-learning tasks (e.g., Schafer & Mareschal, 2001; Shvachkin, 1954; Walley, 1993). To test these competing claims, we replicated Stager and Werker's (1997) results with new stimuli. Children familiarized with an object called a 'daw' accepted 'taw' as a label for that object.\n",
    "In two additional conditions, children not only learned a label for the daw-object, but the names of two additional objects. In one of the conditions, the other two objects were labeled as 'tawgoo' and a 'dawgoo.' In this condition (the Uninformative condition), 'daw' and 'taw' occurred in similar contexts in the two additional words. In the other condition (the Informative condition), the two additional objects were labeled a 'tawgoo' and a 'dawbow.' In this condition, the two additional objects had dissimilar names, and provided examples of 'daw' and 'taw' in distinct contexts.\n",
    "In the Uninformative condition, infants continued to treat 'daw' and 'taw' interchangeably, and accepted either as a label for the daw-object. In the Informative condition, infants dishabituated when the daw-object was referred to as 'taw.' Experience with 'daw' and 'taw' in distinct lexical contexts enabled infants to use the distinction in a wordlearning task. This suggests that the distribution of phonemes in the words with which infants are familiar plays a role in their subsequent use of those phonemes.\n",
    "These results indicate that the change in children's use of phonemic contrasts in word-learning tasks between 14 and 20 months may be explained by characteristics of their lexicon at those two ages. At 20 months, children know more words than 14- month-olds. Their larger vocabulary may provide them with more evidence about the distribution of phonemes in the words of their language.\n",
    ""
   ]
  },
  "tsukada05_psp": {
   "authors": [
    [
     "Kimiko",
     "Tsukada"
    ]
   ],
   "title": "Cross-language speech perception of final stops by Australian-English, Japanese and Thai listeners",
   "original": "psp5_244",
   "page_count": 4,
   "order": 104,
   "p1": "244",
   "pn": "247",
   "abstract": [
    "It is well known that the first language (L1) exerts a strong influence on the production/perception of non-native sounds. Theories of cross-language perception posit that perceived relation between native and non-native sounds plays an important role in predicting discrimination accuracy of nonnative sound contrasts. This study examined the ability of three groups of listeners differing in L1 to discriminate native and/or non-native stops. Native speakers of Australian English (AE) and Japanese and Thai speakers with varying degrees of experience with English participated and their discrimination of three stop place contrasts (/p/-/t/, /p/-/k/, /t/-/k/) in English and Thai was assessed.\n",
    "Listeners' L1s differed markedly with respect to the occurrence of word-final stops. All stop contrasts tested in this study occur in English and Thai and are known/familiar to these listeners but they differ in phonetic realization, i.e., variably released in English and invariably unreleased in Thai. In Japanese, on the other hand, the predominant syllable structure is /(C)V/ and English words such as 'cap' and 'cat' are expected to be difficult for Japanese listeners to discriminate. These cross-linguistic differences lead to the expectations that AE and Thai listeners would discriminate stop contrasts in English and Thai accurately while Japanese listeners would discriminate them poorly.\n",
    "Monosyllabic words ending with stop tokens were presented in triads to listeners whose task was to identify an odd item out if any. All three groups of listeners showed accurate discrimination of English stop contrasts, but only Thai listeners were able to discriminate Thai stop contrasts accurately. In general, AE listeners discriminated Thai stops significantly more accurately than Japanese listeners but less accurately than Thai listeners. AE and Japanese listeners' discrimination accuracy of Thai stimuli differed according to the contrast type tested and both groups were more variable than Thai listeners.\n",
    "Although Japanese listeners had no experience with word-final stops in the L1, they were able to discriminate English contrasts (but not Thai contrasts) accurately, demonstrating that non-native contrasts are learnable and that some aspects of speech perception remains plastic beyond early childhood. Taken together with the finding that AE listeners did not match Thai listeners in discriminating Thai stops despite their experience with unreleased stops in the L1 suggests that phonetic realization of sounds and/or the amount of acoustic information contained in the speech signal may influence accuracy with which sound contrasts are discriminated.\n",
    ""
   ]
  }
 },
 "sessions": [
  {
   "title": "Oral Sessions",
   "papers": [
    "flege05_psp",
    "pallier05_psp",
    "ramus05_psp",
    "dorman05_psp",
    "kuhl05_psp",
    "cutler05_psp",
    "gaskell05_psp",
    "moore05_psp",
    "scott05_psp",
    "iverson05_psp",
    "moore05b_psp",
    "rosen05_psp",
    "barry05_psp",
    "schellenberg05_psp",
    "saffran05_psp",
    "maye05_psp",
    "bosch05_psp"
   ]
  },
  {
   "title": "Poster Session #1",
   "papers": [
    "behne05_psp",
    "clarke05_psp",
    "cohen05_psp",
    "hawkins05_psp",
    "heeren05_psp",
    "johnson05_psp",
    "kitamura05_psp",
    "kushnerenko05_psp",
    "meunier05_psp",
    "mora05_psp",
    "nielsen05_psp",
    "nygaard05_psp",
    "panneton05_psp",
    "scobbie05_psp",
    "surowiecki05_psp",
    "taitelbaumswead05_psp",
    "wayland05_psp",
    "yasin05_psp",
    "yuen05_psp"
   ]
  },
  {
   "title": "Poster Session #2",
   "papers": [
    "vierudimulescu05_psp",
    "clayards05_psp",
    "davis05_psp",
    "evans05_psp",
    "gauthier05_psp",
    "hervaisadelman05_psp",
    "honbolygo05_psp",
    "lacerda05_psp",
    "mclennan05_psp",
    "mitterer05_psp",
    "pan05_psp",
    "pattamadilok05_psp",
    "sedin05_psp",
    "feest05_psp",
    "wagner05_psp"
   ]
  },
  {
   "title": "Poster Session #3",
   "papers": [
    "abelin05_psp",
    "brasileiro05_psp",
    "dellwo05_psp",
    "girard05_psp",
    "hjen05_psp",
    "johnsrude05_psp",
    "krebslazendic05_psp",
    "mayo05_psp",
    "mclennan05b_psp",
    "messaoudgalusi05_psp",
    "mueller05_psp",
    "nenonen05_psp",
    "pape05_psp",
    "fuchs05_psp",
    "sereno05_psp",
    "stacey05_psp",
    "uther05_psp",
    "winters05_psp",
    "yoo05_psp"
   ]
  },
  {
   "title": "Poster Session #4",
   "papers": [
    "roth05_psp",
    "chait05_psp",
    "creel05_psp",
    "fonteneau05_psp",
    "hamann05_psp",
    "hanpejaudier05_psp",
    "hazan05_psp",
    "hirata05_psp",
    "howell05_psp",
    "jones05_psp",
    "kalliorinne05_psp",
    "kotz05_psp",
    "nazzi05_psp",
    "nguyen05_psp",
    "shestakova05_psp",
    "thomson05_psp",
    "wang05_psp"
   ]
  },
  {
   "title": "Poster Session #5",
   "papers": [
    "agrawal05_psp",
    "biersack05_psp",
    "denham05_psp",
    "geng05_psp",
    "hay05_psp",
    "johnson05b_psp",
    "kishonrabin05_psp",
    "larsson05_psp",
    "mcqueen05_psp",
    "miller05_psp",
    "paulmann05_psp",
    "peelle05_psp",
    "pichorafuller05_psp",
    "purcell05_psp",
    "smith05_psp",
    "thiessen05_psp",
    "tsukada05_psp"
   ]
  }
 ]
}