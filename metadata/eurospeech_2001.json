{
 "title": "7th European Conference on Speech Communication and Technology (Eurospeech 2001)",
 "location": "Aalborg, Denmark",
 "startDate": "3/9/2001",
 "endDate": "7/9/2001",
 "chair": "General Chair: Paul Dalsgaard",
 "conf": "Eurospeech",
 "year": "2001",
 "name": "eurospeech_2001",
 "series": "Eurospeech",
 "SIG": "",
 "title1": "7th European Conference on Speech Communication and Technology",
 "title2": "(Eurospeech 2001)",
 "date": "3-7 September 2001",
 "booklet": "eurospeech_2001.pdf",
 "papers": {
  "pols01_eurospeech": {
   "authors": [
    [
     "Louis C. W.",
     "Pols"
    ]
   ],
   "title": "Acquiring and implementing phonetic knowledge",
   "original": "e01_k03",
   "page_count": 4,
   "order": 1,
   "p1": "K3-6",
   "pn": "",
   "abstract": [
    "Proper early acquisition of speech and language appears to be a necessary process to reach mature speech communication. In modelling the process of natural (and pathological) speech production and speech perception, we frequently concentrate on specific aspects of phonetic knowledge. But also to improve the performance of speech technological systems, an intelligent interpretation of the abundant, but sometimes also incomplete or absent, phonetic information is highly advisable. This keynote will use the above framework to discuss the progress made in speech science and technology over the past 30 years.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-1"
  },
  "neuvo01_eurospeech": {
   "authors": [
    [
     "Yrjö",
     "Neuvo"
    ]
   ],
   "title": "Mobile future",
   "original": "e01_k07",
   "page_count": 4,
   "order": 2,
   "p1": "K7-10",
   "pn": "",
   "abstract": [
    "Mobile communications has fast become one of the key industries globally, and currently the mobile revolution extending to the Internet. Third generation cellular systems will soon be taken in use, opening up new possibilites for mobile services and multimedia. This article gives an overview of the future development of mobile communications, and highlights some of the expectations that this development places on multimedia and speech applications.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-2"
  },
  "brennan01_eurospeech": {
   "authors": [
    [
     "Susan E.",
     "Brennan"
    ]
   ],
   "title": "How visual co-presence and joint attention shape speaking",
   "original": "e01_k11",
   "page_count": 2,
   "order": 3,
   "p1": "K11-12",
   "pn": "",
   "abstract": [
    "When people in conversation refer to things, they achieve a joint focus of attention that enables them to be confident that they are both talking about the same thing. When they cannot see one another, such as over a telephone, they must use verbal means to do so. For instance, they may reuse the same expression upon repeated referring, marking the mutual belief that they have achieved a joint perspective; this process has been called entrainment. In contrast, when they can see one another, such as when they are physically co-present in the same environment, they can use visual evidence and deictic (pointing) strategies to formulate and ground references to objects. In this way, visual co-presence shapes conversation.\n",
    ""
   ]
  },
  "greenberg01_eurospeech": {
   "authors": [
    [
     "Steven",
     "Greenberg"
    ]
   ],
   "title": "Whither speech technology? - a twenty-first century perspective",
   "original": "e01_p03",
   "page_count": 4,
   "order": 4,
   "p1": "P3-6",
   "pn": "",
   "abstract": [
    "Speech-technology research lies at an historic juncture. Commercialization of the technology is likely to accelerate dramatically over the coming decade, but its scientific foundation remains uncertain. A critical shortage of qualified speech scientists and engineers looms in the absence of well-funded, challenging programs for training speech technologists and timely intervention by universities, government agencies and speech-technology companies. The speech-technology industry should collaborate closely with academic and government partners to insure an orderly expansion of academic training and research facilities required to accommodate the inevitable surge in demand for spoken-language technology. In the absence of significant academic-industry-government collaboration the pace of scientific innovation in speech research is likely to slow dramatically.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-3"
  },
  "dobler01_eurospeech": {
   "authors": [
    [
     "Stefan",
     "Dobler"
    ],
    [
     "Hans",
     "Hermansson"
    ],
    [
     "Tor-Björn",
     "Minde"
    ]
   ],
   "title": "3g mobile networks and mobile internet as a promotor for new applications - challenges to industry and universities",
   "original": "e01_p07",
   "page_count": 4,
   "order": 5,
   "p1": "P7-10",
   "pn": "",
   "abstract": [
    "The intention of this article is to stimulate the discussion of relationships between university and industry research, especially regarding to applications in future 3rd generation mobile networks. Upcoming 3rd generation mobile networks will be briefly reviewed with respect to required speech technologies. 3G brings together high-speed radio access and IP-based services into one, powerful environment thus enabling to realize Mobile Internet. New location based services expand the capabilities of the traditional Internet, underlining that Mobile Internet is not just about making the Internet mobile. The characteristics of mobile terminals (small size, small keyboard) and the network (data rate) require e.g. automatic speech recognition with high demands on noise robustness, to create convenient user interfaces for the terminals as well as for services. On the other side, wide spread use of mobile communication opens opportunities for speech technologies for the mass market. New applications such as location-based services (e.g. finding nearest cinema and streaming of movie trailers) require to merge different technologies to catalyze these applications and to make them a success. Up to now this symbiosis is not sufficiently reflected in traditional collaboration between research at universities and in industry. An example to motivate interdisciplinary information exchange are the 'Ericsson University Days', which is a regular workshop of Ericsson research with its university partners.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-4"
  },
  "niiniluoto01_eurospeech": {
   "authors": [
    [
     "Ilkka",
     "Niiniluoto"
    ]
   ],
   "title": "Universities and industry: marriage or co-operation between independent partners?",
   "original": "e01_0011",
   "page_count": 2,
   "order": 6,
   "p1": "11",
   "pn": "12",
   "abstract": [
    "Michael Gibbons, the present secretary-general of the Association of Commonwealth Universities, published in Nature 402 (2 December 1999) an article on Sciences new social contract with society. Gibbons argues that \"the old image of science working autonomously will no longer suffice\": the clear demarcation lines between university science and industrial science are disappearing. According to the new \"social contract\", not only can science speak to society as a contributor of \"reliable knowledge\", but \"the society can now speak back to science\" by demanding \"various innovations\" and \"socially robust knowledge\". Such knowledge should be \"transparent and productive\" in the sense that its acceptability is tested \"not only against nature, but against (and hopefully also with) other people\".\n",
    "In my view, Gibbons refers to important developments within the relations between science and society, but his main conclusion is somewhat misleading. Academic research has for a long time had an impact on society through the applications of the results of basic research, as well as through the education of professional skills. In spite of the new prospects in co-operation between universities and industry, it still is important to distinguish basic and applied scientific research (which is open to public criticism within the scientific community and thereby accessible also to the society at large) from such forms of industrial or military research that serve economic or other practical interests and are not published in scientific journals. It is a fact that university science is today only a small fraction of all research activities. For example, in Finland, where the total R&D expenditure is about 3.1 % of GNP, almost 70 % of this R&D is financed by private corporations. Of the publicly funded 30 % of R&D, about one half consists of support for technological development in industrial firms and sectorial research carried out in governmental institutions under the auspices of different ministries. About 40% of the public resources go to the universities - either directly or via the Academy of Finland.\n",
    "Today academic research work in the universities increasingly relies on external funding - its sources include contract research with companies and the mission-oriented research programmes of the Academy of Finland and the European Union. Even though the residue of \"free\" academic basic research, founded upon the duties of university professors and research groups financed on a competitive basis, may seem to be small in comparison with other areas of R&D, it still has very important tasks in promoting science-based world views and education. Academic research should still serve as a source of new theories and innovative methods. The autonomy and integrity of academic research are important for the universities also in order to maintain their critical potential towards dominant cultural and social trends.\n",
    "Sectorial and mission-oriented research can be directed towards goals that are useful in planning the future, improving administration, protecting environment, and making rational decisions - and thereby they are instrumentally and socially relevant for the satisfaction of human needs. Many national programmes of science and technology policy look at the benefits of research only in terms of technological progress and economic wealth. Technology is seen as a way producing and distributing commodities in the free market. Science, both \"strategic\" basic research and applied research, serves as a basis of technological development. This motivates the treatment of scientific research and higher education as \"investments\" which should yield economic profit in the short or at least not-too-long run. This means that both science policy and technology policy are understood to be parts of the \"national innovation system\" which ultimately aims to promote commerce and industry.\n",
    "I agree that the universities, too, have a role in the creation of technological innovations - and this is one way in which they can be socially relevant for human life. But, in my view, the rationality of scientific inquiry should not be reduced to the commercial principles of technology policy. It is still significant to make a distinction between technology and science: technology does not produce knowledge by inquiry like science, but rather designs new artefacts, tools, and machines. Artefacts are not constrained by truth in the same way as knowledge claims, but by what is physically and economically possible. We do not decide what is real in nature, but we can choose what artefacts we wish to produce.\n",
    "For these reasons, I think it is misleading that Gibbons does not distinguish between the acceptability of a research project (e.g. the Superconducting Super Collider), a knowledge claim (e.g., whether genetically manipulated organisms or GMOs affect our health), and a technological artifact (e.g., whetherGMOs should be allowed to be sold in the market).\n",
    ""
   ]
  },
  "neuvo01b_eurospeech": {
   "authors": [
    [
     "Yrjö",
     "Neuvo"
    ]
   ],
   "title": "Considerations on what industry expects from universities",
   "original": "e01_0013",
   "page_count": 2,
   "order": 7,
   "p1": "13",
   "pn": "14",
   "abstract": [
    "Top-level knowledge: The fundamental expectation that industry has towards universities is that the research conducted in universities, and the resulting knowledge of the scientific field are at top level. Innovativeness: An atmosphere of innovativeness, a capability to generate new innovations from the research results, is one of the central expectations. Broad, cross-disciplinary understanding: The source of innovation is most often a matter of combining things in a new and unexpected way. To be able to do this, a cross-disciplinary understanding and ability to draw broader perspectives is underlined. This capability is underlined, as industries increasingly focus on developing end-to-end solutions. This also means that collaborative research projects will increasingly be such that the results may feed a range of cross-disciplinary projects. There is also a need for generalist doctors, people that have gone through the process of highest academic education with a broader scope than the traditional model usually expects. Being international, global: The ongoing globalization process touches first the most knowledge-intensive areas. Increasingly, competition is global also for universities. The research, the knowledge, and the education must be at top level also internationally in order for the university to be a relevant player. Excellent education: A key source of renewal for both universities and industry is new talent. To nurture this, the education of a university must be relevant and excellent. International competition also calls for this, as the most talented students and researches restrict themselves less and less to their home country when selecting their university. Although the primary focus of education obviously must be on the particular scientific field, the education must be able to incorporate training in human issues, interaction, people management, and so forth.\n",
    ""
   ]
  },
  "strong01_eurospeech": {
   "authors": [
    [
     "Gary",
     "Strong"
    ]
   ],
   "title": "A perspective on industry/university relationships in the US",
   "original": "e01_0015",
   "page_count": 2,
   "order": 8,
   "p1": "15",
   "pn": "16",
   "abstract": [
    "This perspective is from that of one who has served recently at both the US National Science Foundation (NSF) and the US Defense Advanced Research Projects Agency (DARPA). It is therefore biased from the point of view of the US Governments interest in fundamental research in information technology at these two agencies and other agencies that participate with them in Information Technology Research and Development. NSF has the mission of ensuring the health of science and engineering in US universities and rarely funds industry for research purposes. On the other hand, funded subcontracts from universities are possible in the Information Technology Research Program, if they contribute to the overall research goals of the university. Other than a Small Business Innovation Research Program, NSF does not usually fund industry directly.\n",
    "Part of the reason why this situation has remained thus for so long may be due to the existence of DARPA and its long-term partnership with NSF. Most of DARPAs IT Programs fund industry as well as universities. They also employ a wider range of funding vehicles than NSF that address issues such as intellectual property in various ways. University researchers in IT are often funded by both NSF and DARPA either at the same time or at different times in their career. DARPA funding allows them to interact closely with industrial researchers in the context of program activities, such as in the speech and natural language communities. While this is a neat division of responsibilities for the US Government, it emphasizes university-industry collaboration that has military relevance and tends to ignore such collaboration when it has educational or workforce relevance. This is because DARPA doesnt often fund projects with educational or curricular goals.\n",
    "While there has been increased emphasis on workforce development in recent years at NSF, the most diffi- cult problem in university-industrial research will be intellectual property. The US government is becoming more flexible in recent years on this issue, but universities have probably gone in the opposite direction. The need to deal with this issue may become a major driving force in future funding for fundamental research.\n",
    ""
   ]
  },
  "choukri01_eurospeech": {
   "authors": [
    [
     "Khalid",
     "Choukri"
    ]
   ],
   "title": "ELRA contribution to bridge the gap between industry and academia",
   "original": "e01_0017",
   "page_count": 2,
   "order": 9,
   "p1": "17",
   "pn": "18",
   "abstract": [
    "The European Language Resources Association (ELRA) was created in February 1995 to handle all issues related to Language Resources. ELRAs missions and activities include the collection, distribution, validation of speech, text, terminology resources and tools. Very recently ELRA has launched a new activity regarding the evaluation (of technologies, systems, prototypes, services, etc.).\n",
    ""
   ]
  },
  "maltese01_eurospeech": {
   "authors": [
    [
     "G.",
     "Maltese"
    ],
    [
     "P.",
     "Bravetti"
    ],
    [
     "H.",
     "Crépy"
    ],
    [
     "B. J.",
     "Grainger"
    ],
    [
     "M.",
     "Herzog"
    ],
    [
     "F.",
     "Palou"
    ]
   ],
   "title": "Combining word- and class-based language models: a comparative study in several languages using automatic and manual word-clustering techniques",
   "original": "e01_0021",
   "page_count": 4,
   "order": 10,
   "p1": "21",
   "pn": "24",
   "abstract": [
    "This paper compares various class-based language models when used in conjunction with a word-based trigram language model by means of linear interpolation. For class-based language models where classes are automatically derived we present a comparative analysis in five languages (French, British English, German, Italian, and Spanish). With regard to classes corresponding to parts-of-speech, we present results for three languages (British English, French, and Italian). For each language, we present results for varying training corpus size and test script complexity. We achieved significant perplexity and word error rate reduction for all five languages and for several language models and recognition tasks. This work extends previous research by covering more languages and showing positive impact of these techniques with very large corpora, whereas prior work mostly focused on addressing data sparseness issues caused by small corpora.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-5"
  },
  "isogai01_eurospeech": {
   "authors": [
    [
     "Shuntaro",
     "Isogai"
    ],
    [
     "Katsuhiko",
     "Shirai"
    ],
    [
     "Hirofumi",
     "Yamamoto"
    ],
    [
     "Yoshinori",
     "Sagisaka"
    ]
   ],
   "title": "Multi-class composite n-gram language model using multiple word clusters and word successions",
   "original": "e01_0025",
   "page_count": 4,
   "order": 11,
   "p1": "25",
   "pn": "28",
   "abstract": [
    "In this paper, a new language model, the Multi-Class Composite Ngram, is proposed to avoid a data sparseness problem in small amount of training data. The Multi-Class Composite N-gram maintains an accurate word prediction capability and reliability for sparse data with a compact model size based on multiple word clusters, so-called Multi-Classes. In the Multi-Class, the statistical connectivity at each position of the N-grams is regarded as word attributes, and one word cluster each is created to represent positional attributes. Furthermore, by introducing higher order word N-grams through the grouping of frequent word successions, Multi-Class N-grams are extended to Multi-Class Composite N-grams. In experiments, the Multi-Class Composite Ngrams result in 9.5% lower perplexity and a 16% lower word error rate in speech recognition with a 40% smaller parameter size than conventional word 3-grams.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-6"
  },
  "zitouni01_eurospeech": {
   "authors": [
    [
     "Imed",
     "Zitouni"
    ],
    [
     "Kamel",
     "Smaili"
    ],
    [
     "Jean-Paul",
     "Haton"
    ]
   ],
   "title": "Statistical language model based on a hierarchical approach: MCnv",
   "original": "e01_0029",
   "page_count": 4,
   "order": 12,
   "p1": "29",
   "pn": "32",
   "abstract": [
    "In this paper, we propose a new language model based on dependent word sequences organized in a multi-level hierarchy. We call this model MCnv, where n is the maximum number of words in a sequence and v is the maximum number of levels. The originality of this model is its capacity to take into account dependent variable-length sequences for very large vocabularies. In order to discover the variable-length sequences and to build the hierarchy, we use a set of 233 syntactic classes extracted from the 8 French elementary grammatical classes. The MCnv model learns hierarchical word patterns and uses them to reevaluate and filter the n-best utterance hypotheses outputted by our speech recognizer MAUD. The model has been trained on a corpus of 43 million words extracted from a French newspaper and uses a vocabulary of 20000 words. Tests have been conducted on 300 sentences. Results achieved 17% decrease in perplexity compared to an interpolated class trigram model. Rescoring the original n-best hypotheses resulted in an improvement of 5% in accuracy.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-7"
  },
  "whittaker01_eurospeech": {
   "authors": [
    [
     "E. W. D.",
     "Whittaker"
    ],
    [
     "Bhiksha",
     "Raj"
    ]
   ],
   "title": "Quantization-based language model compression",
   "original": "e01_0033",
   "page_count": 4,
   "order": 13,
   "p1": "33",
   "pn": "36",
   "abstract": [
    "This paper describes two techniques for reducing the size of statistical back-off N-gram language models in computer memory. Language model compression is achieved through a combination of quantizing language model probabilities and back-off weights and the pruning of parameters that are determined to be unnecessary after quantization. The recognition performance of the original and compressed language models is evaluated across three different language models and two different recognition tasks. The results show that the language models can be compressed by up to 60% of their original size with no significant loss in recognition performance. Moreover, the techniques that are described provide a principled method with which to compress language models further while minimising degradation in recognition performance.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-8"
  },
  "bloothooft01_eurospeech": {
   "authors": [
    [
     "Gerrit",
     "Bloothooft"
    ],
    [
     "Mieke van",
     "Wijck"
    ],
    [
     "Peter",
     "Pabon"
    ]
   ],
   "title": "Relations between vocal registers in voice breaks",
   "original": "e01_0039",
   "page_count": 4,
   "order": 14,
   "p1": "39",
   "pn": "42",
   "abstract": [
    "1783 modal-falsetto register breaks and 853 falsetto-modal register breaks, produced by seven untrained adult male subjects, were recorded and analyzed with respect to jumps in fundamental frequency and sound pressure level (SPL) using a computer phonetograph. SPL and relative positions of modal and falsetto registers were the most important factors underlying the results. Whereas sub- or supraglottal coupling certainly cannot explain the results, models of intrinsic non-linear behavior of the vocal folds may need to be extended for explanations of breaks outside the overlap area of the two registers.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-9"
  },
  "ramsay01_eurospeech": {
   "authors": [
    [
     "Gordon",
     "Ramsay"
    ]
   ],
   "title": "A quasi-one-dimensional model of aerodynamic and acoustic flow in the time-varying vocal tract: source and excitation mechanisms",
   "original": "e01_0043",
   "page_count": 4,
   "order": 15,
   "p1": "43",
   "pn": "46",
   "abstract": [
    "In this paper, the conservation laws of classical fluid mechanics are used to derive a quasi-one-dimensional model of fluid flow in an elastic tube of time-varying cross-sectional area, representing the human vocal tract. The global flow equations are then decomposed into aerodynamic and acoustic components, representing respiratory flow and sound propagation during speech. The nature of the coupling between the two systems is investigated, and a new interpretation of the traditional source-filter model of speech production is proposed.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-10"
  },
  "henrich01_eurospeech": {
   "authors": [
    [
     "Nathalie",
     "Henrich"
    ],
    [
     "Christophe",
     "d'Alessandro"
    ],
    [
     "Boris",
     "Doval"
    ]
   ],
   "title": "Spectral correlates of voice open quotient and glottal flow asymmetry : theory, limits and experimental data",
   "original": "e01_0047",
   "page_count": 4,
   "order": 16,
   "p1": "47",
   "pn": "50",
   "abstract": [
    "The effects of voice open quotient and glottal waveform asymmetry are studied in the spectral domain. The hypothesis that the amplitude difference of the first and second harmonics of the inverse-filtered voice signal (H1*-H2*) is a reliable spectral correlate of the open quotient is tested. Theoretical arguments and experiments are reported. In the theoretical part, analytical formulas are derived for the spectrum of several models (LF, R++,KLGLOTT88). Then it is shown that H1*-H2* is generally dependent on both open quotient and asymmetry. Domains for open quotient, asymmetry and H1*-H2* variations are given. In the experimental part, examples of voice and singing signals are analyzed. It is shown that a significant part of the spectral measurements obtained are out of the scope of the models studied.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-11"
  },
  "avanzini01_eurospeech": {
   "authors": [
    [
     "Federico",
     "Avanzini"
    ],
    [
     "Paavo",
     "Alku"
    ],
    [
     "Matti",
     "Karjalainen"
    ]
   ],
   "title": "One-delayed-mass model for efficient synthesis of glottal flow",
   "original": "e01_0051",
   "page_count": 4,
   "order": 17,
   "p1": "51",
   "pn": "54",
   "abstract": [
    "A lumped physical model of the glottal source is presented. Vocal folds are described as single masses but, unlike conventional one-mass models, vertical phase differences between upper and lower margins of the folds are taken into account. This is done by appropriately describing the non-linear interaction of the mechanical model with aerodynamics, resulting in a modified one-mass model, or a 'one-delayed-mass model'. Analysis on numerical simulations shows that the system behaves qualitatively as higher-dimensional ones (such as the two-mass model by Ishizaka and Flanagan); in particular, control over flow skewness is guaranteed, allowing for synthesis of realistic glottal flow waveforms. As only one degree of freedom (one mass) is needed in the model, structure and number of parameters are drastically reduced, thus making it suitable for real-time synthesis applications.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-12"
  },
  "zheng01_eurospeech": {
   "authors": [
    [
     "Fang",
     "Zheng"
    ],
    [
     "Zhanjiang",
     "Song"
    ],
    [
     "Pascale",
     "Fung"
    ],
    [
     "William",
     "Byrne"
    ]
   ],
   "title": "Modeling pronunciation variation using context-dependent weighting and b/s refined acoustic modeling",
   "original": "e01_0057",
   "page_count": 4,
   "order": 18,
   "p1": "57",
   "pn": "60",
   "abstract": [
    "The pronunciation variability is an important issue that must be faced with when developing practical automatic spontaneous speech recognition systems. By studying the initial/final (IF) characteristics of Chinese language and developing the Bayesian equation, we propose the concepts of generalized initial/final (GIF) and generalized syllable (GS), the GIF modeling method and the IF-GIF modeling method, as well as the context-dependent pronunciation weighting method. By using these approaches, the IF-GIF modeling reduces the Chinese syllable error rate (SER) by 6.3% and 4.2% compared with the GIF modeling and IF modeling respectively when the language modeling, such as syllable or word N-gram, is not used.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-13"
  },
  "bazzi01_eurospeech": {
   "authors": [
    [
     "Issam",
     "Bazzi"
    ],
    [
     "James",
     "Glass"
    ]
   ],
   "title": "Learning units for domain-independent out-of- vocabulary word modelling",
   "original": "e01_0061",
   "page_count": 4,
   "order": 19,
   "p1": "61",
   "pn": "64",
   "abstract": [
    "This paper describes our recent work on detecting and recognizing out-of-vocabulary (OOV) words for robust speech recognition and understanding. To allow for OOV recognition within a word-based recognizer, the in-vocabulary (IV) word network is augmented with an OOV model so that OOV words are considered simultaneously with IV words during recognition. We explore several configurations for the OOV model, the best of which utilizes a set of domain-independent, automatically derived, variable-length units. The units are created using an iterative bottom-up procedure where, at each iteration, the unit pairs with maximum mutual information are merged. When evaluating this method on a weather information domain, the false alarm rate of our baseline OOV model is reduced by over 60%. For example, with an OOV detection rate of 70%, the OOV false alarm rate is reduced from 8.5% to 3.2%, with only 3% relative degradation in word error rate on IV data.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-14"
  },
  "nakajima01_eurospeech": {
   "authors": [
    [
     "Hideharu",
     "Nakajima"
    ],
    [
     "Izumi",
     "Hirano"
    ],
    [
     "Yoshinori",
     "Sagisaka"
    ],
    [
     "Katsuhiko",
     "Shirai"
    ]
   ],
   "title": "Pronunciation variant analysis using speaking style parallel corpus",
   "original": "e01_0065",
   "page_count": 4,
   "order": 20,
   "p1": "65",
   "pn": "68",
   "abstract": [
    "To improve the recognition accuracy for spontaneous conversational speech, we collected a corpus to study how spontaneous conversational speech differs from read style speech. The corpus consists of two parts: 1) spontaneous conversational speech and 2) read speech with the same word transcriptions as the conversational speech. In word and phone recognition experiments, it was confirmed that, for the Japanese language, the recognition of spontaneous speech is harder than that of read speech. By comparing of recognition results, we found that, both in the occurrence of errors appearing with speaking style changes, and in the types of pronunciation variants, there are differences that depend on the linguistic categories that misrecognized words belong to. We confirmed that linguistic categories also affect pronunciation variants that deteriorate the recognition accuracy.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-15"
  },
  "kneissler01_eurospeech": {
   "authors": [
    [
     "Jan",
     "Kneissler"
    ],
    [
     "Dietrich",
     "Klakow"
    ]
   ],
   "title": "Speech recognition for huge vocabularies by using optimized sub-word units",
   "original": "e01_0069",
   "page_count": 4,
   "order": 21,
   "p1": "69",
   "pn": "72",
   "abstract": [
    "This paper describes approaches for decomposing words of huge vocabularies (up to 2 million) into smaller particles that are suitable for a recognition lexicon. Results on a Finnish dictation task and a flat list of German street names are given.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-16"
  },
  "lee01_eurospeech": {
   "authors": [
    [
     "Kyung-Tak",
     "Lee"
    ],
    [
     "Christian J.",
     "Wellekens"
    ]
   ],
   "title": "Dynamic lexicon using phonetic features",
   "original": "e01_1413",
   "page_count": 4,
   "order": 22,
   "p1": "1413",
   "pn": "1416",
   "abstract": [
    "In order to better model pronunciation variations, we present in this paper a method to build a lexicon whose content changes dynamically with the input speech. To achieve this goal, we proceeded in two steps. In the first step, a static augmented lexicon is created by adding new phone transcriptions to a basic lexicon. These new variants are derived from phonetic features that are automatically extracted from some training speech. Then in the second step, phonetic features are extracted again during recognition and help to select entries in the augmented lexicon that best match the phonetic characteristics of a given speech. These selected transcriptions constitute the dynamic lexicon, which is specific to each input utterance. Experiments showed a 16.0% relative reduction in WER compared to the baseline and 16.7% compared to when a static augmented lexicon is used.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-17"
  },
  "ziegenhain01_eurospeech": {
   "authors": [
    [
     "Ute",
     "Ziegenhain"
    ],
    [
     "Josef G.",
     "Bauer"
    ]
   ],
   "title": "Triphone tying techniques combining a-priori rules and data driven methods",
   "original": "e01_1417",
   "page_count": 4,
   "order": 23,
   "p1": "1417",
   "pn": "1420",
   "abstract": [
    "Tying of Hidden Markov Model states is an important issue for the use of triphones as modeling units in automatic speech recognition systems. This paper studies the application of a-priori rules for tying in combination with data driven methods. The baseline method features a combination of a-priori rules that reduce the theoretical number of units by an oder of magnitude and a simple back-off tying. Back-off tying is based on the frequency of units appearing in the training material. The use of the a-priori rules has practical advantages especially for the implementation of continuous phoneme recognition. This method is compared to the widely used decision tree based clustering that makes no use of a-priori rules. A third method is proposed that combines apriori rules with decision tree based clustering. Experiments on telephone data show that the combined method outperforms both other methods preserving the advantages of applying a-priori rules.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-18"
  },
  "bosch01_eurospeech": {
   "authors": [
    [
     "Louis F. M. ten",
     "Bosch"
    ],
    [
     "Nick",
     "Cremelie"
    ]
   ],
   "title": "Pronunciation modeling and lexical adaptation in midsize vocabulary ASR",
   "original": "e01_1421",
   "page_count": 4,
   "order": 24,
   "p1": "1421",
   "pn": "1424",
   "abstract": [
    "A computational-phonological method is presented to automatically adapt the phone transcriptions in a lexicon to improve ASR performance in a number of mid-size recognition tasks. The lexical adaptation approach is based on supervised phoneme loops using cd-HMM segments to find alternatives for the transcriptions, and can be considered as a counterpart of the K-means algorithm but on symbolic level. The word error rate in a limited task (digit string recognition) with dialect speakers is shown to drop by 20-25 percent relative, starting from non-dialect digit transcriptions. Since the method is computationally involving, it is only feasible for relatively small tasks.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-19"
  },
  "yi01_eurospeech": {
   "authors": [
    [
     "Liu",
     "Yi"
    ],
    [
     "Pascale",
     "Fung"
    ]
   ],
   "title": "Estimating pronunciation variations from acoustic likelihood score for HMM reconstruction",
   "original": "e01_1425",
   "page_count": 4,
   "order": 25,
   "p1": "1425",
   "pn": "1428",
   "abstract": [
    "It is widely acknowledged that pronunciation modeling is an efficient way to improve recognition performance in spontaneous speech. In pronunciation modeling, almost all methods of generating variation probability are based on relative frequency counting from DP alignment. In this paper, we investigate the local model mismatching caused by pronunciation variations and propose to estimate variation probability from acoustic likelihood score. According to estimated probability, we present a method of reconstructing pre-trained HMM models to include alternate pronunciations by sharing optimal mixture components instead of distributions. Experimental results show that using reconstructed HMM set reduces syllable error rate by 2.03% absolutely compared to the baseline system, also the accuracy improvement gained from proposed method is almost double with respect to that from previous DP alignment.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-20"
  },
  "bisani01_eurospeech": {
   "authors": [
    [
     "M.",
     "Bisani"
    ],
    [
     "Hermann",
     "Ney"
    ]
   ],
   "title": "Breadth-first search for finding the optimal phonetic transcription from multiple utterances",
   "original": "e01_1429",
   "page_count": 4,
   "order": 26,
   "p1": "1429",
   "pn": "1432",
   "abstract": [
    "Extending the vocabulary of a large vocabulary speech recognition system usually requires phonetic transcriptions for all words to be known. With automatic phonetic baseform determination acoustic samples of the words in question can substitute for the required expert knowledge. In this paper we follow a probabilitistic approach to this problem and present a novel breadth-first search algorithm which takes full advantage of multiple samples. An extension to the algorithm to genereate phone graphs as well as an EM based iteration scheme for estimating stochastic pronunciation models is presented. In preliminary experiments phoneme error rates below 5% with respect to the standard pronunciation are achieved without language or word specific prior knowledge.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-21"
  },
  "wolff01_eurospeech": {
   "authors": [
    [
     "Matthias",
     "Wolff"
    ],
    [
     "Matthias",
     "Eichner"
    ],
    [
     "Rüdiger",
     "Hoffmann"
    ]
   ],
   "title": "Improved data-driven generation of pronunciation dictionaries using an adapted word list",
   "original": "e01_1433",
   "page_count": 4,
   "order": 27,
   "p1": "1433",
   "pn": "1436",
   "abstract": [
    "Data-driven approaches to learning pronunciation variants for phonetic dictionaries have to deal with the problem of acquiring a sufficient amount of training data. The reason is not the size of the databases, but the unfavorable distribution of word frequencies in natural speech, which is known as Zipfs law. In this paper we suggest a method which reorganizes a phonetic dictionary according to a given speech database in order to maximize the number of word models for which pronunciation variants can be learned with this corpus. Reorganization takes place automatically by analyzing the orthographic and phonetic transcriptions of the corpus. The method produces an alternative word list consisting of units ranging from partial words to multi-words. The efficiency and the limits of the approach are discussed on the basis of experiments carried out on the German VERBMOBIL corpus.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-22"
  },
  "livescu01_eurospeech": {
   "authors": [
    [
     "Karen",
     "Livescu"
    ],
    [
     "James",
     "Glass"
    ]
   ],
   "title": "Segment-based recognition on the phonebook task: initial results and observations on duration modeling",
   "original": "e01_1437",
   "page_count": 4,
   "order": 28,
   "p1": "1437",
   "pn": "1440",
   "abstract": [
    "This paper describes preliminary recognition experiments on PhoneBook, a corpus of isolated, telephone-bandwidth, read words from a large (almost 8,000-word) vocabulary. We have chosen this corpus as a testbed for experiments on the language model-independent parts of a segment-based recognizer. We present results showing that a segment-based recognizer performs well on this task, and that a simple Gaussian mixture phone duration model significantly reduces the error rate. We compare context-independent, stress-dependent, and word position-dependent duration models and obtain relative error rate reductions of up to 12% on the test set. Finally, we make some observations regarding the effects of stress and word position in this isolated-word task and discuss our plans for further research using PhoneBook.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-23"
  },
  "riis01_eurospeech": {
   "authors": [
    [
     "Søren Kamaric",
     "Riis"
    ],
    [
     "Morten With",
     "Pedersen"
    ],
    [
     "Kare Jean",
     "Jensen"
    ]
   ],
   "title": "Multilingual text-to-phoneme mapping",
   "original": "e01_1441",
   "page_count": 4,
   "order": 29,
   "p1": "1441",
   "pn": "1444",
   "abstract": [
    "This paper introduces a novel approach for generating multilingual text-to-phoneme mappings for use in multilingual speech recognition systems. The multilingual mappings are based on the weighted outputs from a neural network text-to-phoneme model, trained on data mixed from several languages. The multilingual mappings used together with a branched grammar decoding scheme is able to capture both inter- and intra-language pronunciation variations which is ideal for multilingual speaker independent speech recognition systems. A significant improvement in overall system performance was obtained for a multilingual speaker independent name dialing task when applying multilingual instead of language dependent text-to-phoneme mapping.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-24"
  },
  "tsai01_eurospeech": {
   "authors": [
    [
     "Ming-yi",
     "Tsai"
    ],
    [
     "Fu-chiang",
     "Chou"
    ],
    [
     "Lin-shan",
     "Lee"
    ]
   ],
   "title": "Pronunciation variation analysis with respect to various linguistic levels and contextual conditions for Mandarin Chinese",
   "original": "e01_1445",
   "page_count": 4,
   "order": 30,
   "p1": "1445",
   "pn": "1448",
   "abstract": [
    "Chinese language has quite different characteristic structures from those of English. There are at least word, character, syllable, Initial-Final levels in Chinese, each carrying different levels of information with complicated correlations among them. In this paper, we investigate the dependency of pronunciation variation in conversational Mandarin speech on these different levels under various contextual conditions considering the structural features of the language. The influence of speaking rate and word frequency on such pronunciation variation is also analyzed. Different pruning methods, for including pronunciation variation in speech recognition were also evaluated, and the experimental results showed that improved accuracy is obtainable if the characteristics of the pronunciation variation found in the analysis can be properly taken into account. All discussions here are based on tests with the LDC Mandarin Call Home corpus.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-25"
  },
  "tomokiyo01_eurospeech": {
   "authors": [
    [
     "Laura Mayfield",
     "Tomokiyo"
    ]
   ],
   "title": "Hypothesis-driven accent discrimination",
   "original": "e01_1449",
   "page_count": 4,
   "order": 31,
   "p1": "1449",
   "pn": "1452",
   "abstract": [
    "Native and non-native use of language differs, depending on the proficiency of the speaker, in clear and quantifiable ways. It has been shown that customizing the acoustic and language models of a natural language understanding system can significantly improve handling of non-native input; in order to make such a switch, however, the nativeness status of the user must be known. In this paper, we show how the recognition hypothesis can be used to predict with very high accuracy whether the speaker is native. Effectiveness of both word-based and phone-based classification are evaluated, and a discussion of the primary discriminative features is presented. In an LVCSR system in which users are both native and non-native, we have achieved a 15.6% relative decrease in word error rate by integrating this classification method with speech recognition.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-26"
  },
  "ma01_eurospeech": {
   "authors": [
    [
     "Changxue",
     "Ma"
    ],
    [
     "Mark A.",
     "Randolph"
    ]
   ],
   "title": "An approach to automatic phonetic baseform generation based on Bayesian networks",
   "original": "e01_1453",
   "page_count": 4,
   "order": 32,
   "p1": "1453",
   "pn": "1457",
   "abstract": [
    "To improve the performance and the usability of the speech recognition devices, It is necessary for most applications to allow users to enter new words or personalize words to the system vocabulary. Voice-tagging technique is a simple example that use speaker dependent spoken sample to generate baseform transcriptions of the spoken words. More sophisticated techniques can use both spoken samples and texts of the new words to generate baseform transcriptions. In this paper, we propose a new approach to the problem. We use Bayesian networks to model the letter-to-sound rule probabilities. Compared to the common decision tree based method, This new approach shows a definite advantage.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-27"
  },
  "schramm01_eurospeech": {
   "authors": [
    [
     "Hauke",
     "Schramm"
    ],
    [
     "Peter",
     "Beyerlein"
    ]
   ],
   "title": "Towards discriminative lexicon optimization",
   "original": "e01_1457",
   "page_count": 4,
   "order": 33,
   "p1": "1457",
   "pn": "1460",
   "abstract": [
    "A lot of work has been done in deriving the pronunciation dictionary automatically from training data. These attempts focussed mainly on maximum likelihood or similar techniques. Due to the complexity and variability of the pronunciation process it is difficult to find an adequate pronunciation model. The model will deviate from the truth. Hence, the application of maximum likelihood techniques is likely to be suboptimal. For this reason we present an approach, where the pronunciation model is learned discriminatively from data. The corresponding theory utilizes (1) probabilistic weighting of pronunciation variants of words and (2) discriminative model combination (DMC) based on Viterbi-approximations. We will show that the derived theory adjusts the weighting of pronunciation variants with respect to the word error rate, to the frequency of occurence of the specific pronunciation in the training data, and to the likelihood of the acoustic observation sequence given the pronunciation.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-28"
  },
  "he01_eurospeech": {
   "authors": [
    [
     "Xiaodong",
     "He"
    ],
    [
     "Yunxin",
     "Zhao"
    ]
   ],
   "title": "Model complexity optimization for nonnative English speakers",
   "original": "e01_1461",
   "page_count": 4,
   "order": 34,
   "p1": "1461",
   "pn": "1464",
   "abstract": [
    "In this paper, a study is made on selecting existing acoustic models that are trained from native English speech for improving recognition of non-native English talkers speech. The problem is addressed from the perspective that foreign accents prevent detailed tri-phone models that are commonly used in high-performance speech recognition systems to match well with these talkers speech, and therefore an appropriate level of context-dependent acoustic modeling is needed for foreign accent speakers. In this work, model complexity selection is accomplished by empirically choosing a set of model tying thresholds and by using the principle of MDL. An experiment was performed on the Wall Street Journal task on three nonnative English talkers with Chinese accent (276 sentences). Compared to the result obtained from using the models optimized to native English speakers, the best model tying threshold and MDL yielded similar and significant reduction to recognition word errors by 23%.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-29"
  },
  "fegyo01_eurospeech": {
   "authors": [
    [
     "Tibor",
     "Fegyó"
    ],
    [
     "Péter",
     "Mihajlik"
    ],
    [
     "Péter",
     "Tatai"
    ],
    [
     "Géza",
     "Gordos"
    ]
   ],
   "title": "Pronunciation modeling in hungarian number recognition",
   "original": "e01_1465",
   "page_count": 4,
   "order": 35,
   "p1": "1465",
   "pn": "1468",
   "abstract": [
    "In Hungarian, as more or less in many other languages, a large percent of words and phrases can be pronounced in several, different, but correct ways. Introducing pronunciation alternatives for individual vocabulary elements may improve the efficiency of the recognition. But in connected word recognition tasks the modeling of inter-word phonetic changes has a greater significance. In this paper we introduce a rule-based method for the automatic generation of pronunciation alternatives used first for isolated words and later the method is extended to handle cross-word phonological changes in recognition networks, applying a special approach applicable for the Hungarian language. To evaluate the method it is tested in connected number recognition tests.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-30"
  },
  "swerts01_eurospeech": {
   "authors": [
    [
     "Marc",
     "Swerts"
    ],
    [
     "Hanne",
     "Kloots"
    ],
    [
     "Steven",
     "Gillis"
    ],
    [
     "Georges De",
     "Schutter"
    ]
   ],
   "title": "Factors affecting schwa-insertion in final consonant clusters in standard dutch",
   "original": "e01_0075",
   "page_count": 4,
   "order": 36,
   "p1": "75",
   "pn": "78",
   "abstract": [
    "The current paper describes a study that deals with the factors that determine the possible insertion of a schwa in final consonant clusters in Standard Dutch. Our study reveals that the absence or occurrence of such an extra vowel is dependent on regional, social and phonotactic determinants. We discuss how this finding, in combination with other results on speech variability, is important for speech technological applications, both for synthesis and recognition.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-31"
  },
  "hitchcock01_eurospeech": {
   "authors": [
    [
     "Leah",
     "Hitchcock"
    ],
    [
     "Steven",
     "Greenberg"
    ]
   ],
   "title": "Vowel height is intimately associated with stress accent in spontaneous american English discourse",
   "original": "e01_0079",
   "page_count": 4,
   "order": 37,
   "p1": "79",
   "pn": "82",
   "abstract": [
    "There is a systematic relationship between stress accent and vocalic identity in spontaneous English discourse (the Switchboard corpus of telephone dialogues). Low vowels are much more likely to be fully accented than their high vocalic counterparts. And conversely, high vowels are far more likely to lack stress accent than low or mid vocalic segments. Such patterns imply that stress accent and vowel height are bound together at some level of lexical representation. Vocalic duration appears to be the primary acoustic cue associated with stress accent, and the association between vowel height and accent level is most clearly observed in this dimension, particularly for diphthongs and the low, tense monophthongs. Together, the data suggest that vocalic duration plays an exceedingly important role in understanding spoken language.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-32"
  },
  "gibbon01_eurospeech": {
   "authors": [
    [
     "Dafydd",
     "Gibbon"
    ]
   ],
   "title": "Finite state prosodic analysis of african corpus resources",
   "original": "e01_0083",
   "page_count": 4,
   "order": 38,
   "p1": "83",
   "pn": "86",
   "abstract": [
    "The issue of efficient language documentation, particularly with regard to minority and endangered languages, has gained in importance in recent years, as witnessed by several major funding programmes and other human language technology initiatives in the field. An application of finite state technologies to the processing of lexical tone variation in annotated corpora of African languages is described. It is shown that finite state transducers can be constructed which not only provide adequate models for contextual variation in lexical tone (including automatic downstep, downdrift, and tonal assimilations, but also that the transducers provide intuitively satisfying explications of prosodic concepts in `metrical phonology' in terms of oscillations (iterative transitions). The technique has both theoretical value in formalising typological differences in African lexical tone languages and practical value in automatically generating markup enhancements for concordance-based corpus analysis and for fundamental frequency prediction in pitch modelling.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-33"
  },
  "schroder01_eurospeech": {
   "authors": [
    [
     "Marc",
     "Schröder"
    ],
    [
     "Roddy",
     "Cowie"
    ],
    [
     "Ellen",
     "Douglas-Cowie"
    ],
    [
     "Machiel",
     "Westerdijk"
    ],
    [
     "Stan",
     "Gielen"
    ]
   ],
   "title": "Acoustic correlates of emotion dimensions in view of speech synthesis",
   "original": "e01_0087",
   "page_count": 4,
   "order": 39,
   "p1": "87",
   "pn": "90",
   "abstract": [
    "In a database of emotional speech, dimensional descriptions of emotional states have been correlated with acoustic variables. Many stable correlations have been found. The predictions made by linear regression widely agree with the literature. The numerical form of the description and the choice of acoustic variables studied are particularly well suited for future implementation in a speech synthesis system, possibly allowing for the expression of gradual emotional states.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-34"
  },
  "ouden01_eurospeech": {
   "authors": [
    [
     "Hanny den",
     "Ouden"
    ],
    [
     "Jacques",
     "Terken"
    ]
   ],
   "title": "Measuring pitch range",
   "original": "e01_0091",
   "page_count": 4,
   "order": 40,
   "p1": "91",
   "pn": "94",
   "abstract": [
    "The literature offers at least two methods to annotators for characterizing the pitch range of a prosodic phrase. One method is in terms of the distance between the F0 maximum of the phrase (HiF0) and the speaker's utterance-final pitch (LoF0). The other method is in terms of the distance between pitch peaks and pitch valleys in the prosodic phrase. In this paper we address two questions. The first question concerns the reliability of the different methods. Five experienced phoneticians applied both methods on a set of forty utterances taken from read aloud text. We found that reliability was higher for HiF0 than for distances between pitch peaks and valleys. The second question is whether variation that is not captured by the first approach does actually occur in pitch contours. The results suggests that the HiF0 approach captures all the variation relevant to measuring pitch range that occurs in our small corpus. We conclude that the HiF0 method is methodologically more adequate, and at the same time sufficiently powerful to represent pitch range variation adequately for read aloud text.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-35"
  },
  "gibbon01b_eurospeech": {
   "authors": [
    [
     "Dafydd",
     "Gibbon"
    ],
    [
     "Ulrike",
     "Gut"
    ]
   ],
   "title": "Measuring speech rhythm",
   "original": "e01_0095",
   "page_count": 4,
   "order": 41,
   "p1": "95",
   "pn": "98",
   "abstract": [
    "We address the question of rhythm variation in typologically different languages (English, said to be a stress-timed language, Ibibio, said to be a syllable-timed language) and in different varieties of the same language (British and Nigerian English). Attempts to find correlates of different rhythm types in the acoustic signal have so far not been particularly successful. We examine a number of previous studies, in search of a promising measure of rhythm, and select a recently developed measure (the Pairwise Variability Index of Low & Grabe), with minor modifications and the addition of a binary classifier for focal and nonfocal components of rhythm units. The measure and the classifier are implemented as a software tool which takes esps/waves+ label files as input, and generates statistics on durations, duration differences, the rhythm measure, and a classification of the syllables in the labeled utterance. The results show distinct differences in stress-timing and syllable-timing between Ibibio and English.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-36"
  },
  "dimperio01_eurospeech": {
   "authors": [
    [
     "Mariapaola",
     "DImperio"
    ]
   ],
   "title": "Tonal alignment, scaling and slope in Italian question and statement tunes",
   "original": "e01_0099",
   "page_count": 4,
   "order": 42,
   "p1": "99",
   "pn": "102",
   "abstract": [
    "Unlike in languages such as English and Standard Italian, Neapolitan Italian yes/no questions and narrow focus statements share a rising-falling (LHL) tune [1, 2]. However, the alignment of the target H peak has been claimed to be later in questions. This study acoustically tested the hypothesis that all three tonal targets of the rise-fall are timed and scaled differently in questions and statements. Moreover, slope differences for both rise and fall were also tested by employing logistic regression modeling. Two speakers of Neapolitan Italian produced utterances whose target words differed in question/ statement modality, syllable structure and segmental environment. The results show that all three targets within the risefall are timed later in questions than in statements. By contrast, no systematic difference was found for the slope of the rise nor for the slope of the fall. The exact contribution of F0 height to signaling the contrast could not be determined, though. In fact, while one speaker marked the difference by producing higher peaks for statements, the other did not produce any difference.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-37"
  },
  "iivonen01_eurospeech": {
   "authors": [
    [
     "Antti",
     "Iivonen"
    ]
   ],
   "title": "Pragmatic temporal voice range profile as a tool in the research of speech styles",
   "original": "e01_0103",
   "page_count": 4,
   "order": 43,
   "p1": "103",
   "pn": "106",
   "abstract": [
    "Speakers pragmatic temporal voice range profile (TVRP) and F0 distribution histogram were used for comparing two radio speakers and two speaking styles in one speaker. It is assumed that the internal distribution of the temporal voice range, relative placement of the range on ST scale and the concentration of F0 points in certain semitone classes of the F0 histograms are connected with the speech style and the factors bedhind the style. Placing a single utterance against the background of TVRP its features can be better understood within the prosodic repertoire of the speaker.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-38"
  },
  "kim01_eurospeech": {
   "authors": [
    [
     "Wooil",
     "Kim"
    ],
    [
     "Taeyun",
     "Kim"
    ],
    [
     "Sungjoo",
     "Ahn"
    ],
    [
     "Hanseok",
     "Ko"
    ]
   ],
   "title": "Model based stress decision method",
   "original": "e01_0107",
   "page_count": 4,
   "order": 44,
   "p1": "107",
   "pn": "110",
   "abstract": [
    "This paper proposes an effective decision method focused to evaluate the \"stress position\". Conventional methods usually extract the acoustic parameters and compare them to reference in absolute scale, adversely producing unstable results as testing condition changes. To cope with the environmental dependency,the proposed method is designed to be model-based that determines the stressed interval by making relative comparison over candidates. The stressed/unstressed models are then induced from normal phone models by adaptive training. The experimental results indicate that the proposed method is promising and that it is useful for automatic detection of stress positions. The results also show that generating the stressed/unstressed model by adaptive training is effective.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-39"
  },
  "nordgard01_eurospeech": {
   "authors": [
    [
     "Torbjørn",
     "Nordgård"
    ],
    [
     "Arne Kjell",
     "Foldvik"
    ]
   ],
   "title": "Reduction of alternative pronunciations in the norwegian computational lexicon norkompleks",
   "original": "e01_0111",
   "page_count": 4,
   "order": 45,
   "p1": "111",
   "pn": "114",
   "abstract": [
    "This paper describes a method for selecting one single pronunciation from a set of alternatives. Two types of reductions are described - base form reductions and inflected form reductions. The strategies are different because the inflected forms have to take care of two properties of Norwegian: The language allows for many spelling alternatives, both in base forms and inflected forms, and many morphological inflection types license pronunciation alternatives. A comprehensive pronunciation lexicon with more than 65000 base forms is used, and it is demonstrated that it is possible for users of the lexicon to derive their own stylistic preferences.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-40"
  },
  "elordieta01_eurospeech": {
   "authors": [
    [
     "Gorka",
     "Elordieta"
    ],
    [
     "José Ignacio",
     "Hualde"
    ]
   ],
   "title": "The role of duration as a correlate of accent in lekeitio basque",
   "original": "e01_0115",
   "page_count": 4,
   "order": 46,
   "p1": "115",
   "pn": "118",
   "abstract": [
    "Northern Bizkaian Basque shares important prosodic features with Tokyo Japanese, including the existence of a lexical distinction between accented and unaccented content words, the presence of phrase-initial rises and the consistent realization of accents as tonal falls. In this paper we investigate whether NB Basque is also like Japanese in not making use of syllable duration as a correlate of accent, as has been suggested in recent work. The analysis of experimental data from 6 native speakers of the variety spoken in the Bizkaian town of Lekeitio confirms the hypothesis that the presence of an accent on a given syllable is not manifested in an increase of its duration in this language. Other things being equal, accented and unaccented syllables do not have significantly different durations in neutral declarative sentences. More tentatively, the same results are also established for two other sentential conditions: under narrow focus and in postfocus position.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-41"
  },
  "johansson01_eurospeech": {
   "authors": [
    [
     "Victoria",
     "Johansson"
    ],
    [
     "Merle",
     "Horne"
    ],
    [
     "Sven",
     "Strömqvist"
    ]
   ],
   "title": "Word final aspiration as a phrase boundary cue: data from spontaneous Swedish discourse",
   "original": "e01_0119",
   "page_count": 4,
   "order": 47,
   "p1": "119",
   "pn": "122",
   "abstract": [
    "The Swedish sound string /at/ (graphically: att) is associated with two grammatical functions: a) (part of) a subordinate conjunction and b) as an infinitive marker. Previous studies connect final lengthening and pauses with prosodic and syntactic boundaries in spoken discourse. Following these findings, this pilot study, with 5 short spontaneous discourses from 3 male speakers shows a correlation between pauses after att, and aspiration of /t/ in att. We also show a tendency for att with aspiration to be associated with the grammatical function of subordinate conjunction. Further, looking at the distribution of aspiration in the subordinate conjunction att, and in the infinitive marker att, we are able to show a tendency for the infinitive marker to be unaspirated in the normal case, while the subordinate conjunctions are characterized by final aspiration in 40 % of the cases.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-42"
  },
  "shen01_eurospeech": {
   "authors": [
    [
     "Xipeng",
     "Shen"
    ],
    [
     "Bo",
     "Xu"
    ]
   ],
   "title": "Study and auto-detection of stress based on tonal pitch range in Mandarin",
   "original": "e01_0123",
   "page_count": 4,
   "order": 48,
   "p1": "123",
   "pn": "126",
   "abstract": [
    "In Mandarin, there is a special acoustic feature¡ªtonal pitch range, which is relative to stress. In this paper, we present a novel concept-tonal range ratio (TRR), which is based on tonal pitch range, and make a study on the correlation between TRR and stress in Mandarin. And we developed a system to automatically detect stresses in words and sentences based on TRR in Mandarin. We obtained high success rate (92.67% in words and 82.0% in sentences). The results show that TRR has strong correlation with stress and is powerful in detecting stresses in Mandarin.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-43"
  },
  "amir01_eurospeech": {
   "authors": [
    [
     "Noam",
     "Amir"
    ],
    [
     "Ori",
     "Kerret"
    ],
    [
     "Dimitry",
     "Karlinski"
    ]
   ],
   "title": "Classifying emotions in speech: a comparison of methods",
   "original": "e01_0127",
   "page_count": 4,
   "order": 49,
   "p1": "127",
   "pn": "130",
   "abstract": [
    "A number of recent studies have attempted classification of emotional speech using various methods. In this paper we compare the performance of two algorithms: a classification algorithm based on euclidean distances, and a classification algorithm based on the use of neural networks. Both perform the classification using an identical feature set, on a database of emotional speech which has been validated through subjective listening tests.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-44"
  },
  "behne01_eurospeech": {
   "authors": [
    [
     "Dawn M.",
     "Behne"
    ],
    [
     "Peter E.",
     "Czigler"
    ],
    [
     "Kirk P.H.",
     "Sullivan"
    ]
   ],
   "title": "Development of vowel quantity perception in late childhood",
   "original": "e01_0133",
   "page_count": 4,
   "order": 50,
   "p1": "133",
   "pn": "136",
   "abstract": [
    "A distinction in vowel quantity is typically realized acoustically by vowel duration. Research on the perception of Swedish vowel quantity by adult native speakers supports this. It further suggests that when the duration of a vowel is relatively long (due, e.g., to inherent duration), listeners may also make use of vowel spectra to distinguish vowel quantities. The current project investigates the perceptual cues used to distinguish vowel quantities in language development by children 9 to 13 years old. Of particular interest is whether these developing listeners use spectral cues to identify the quantity of vowels which have a relatively long inherent duration. Results are compared with the findings for Swedish adults and the developmental use of vowel duration and spectra as cues for vowel quantity are described.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-45"
  },
  "yang01_eurospeech": {
   "authors": [
    [
     "Byunggon",
     "Yang"
    ]
   ],
   "title": "A study on the production-perception link of English vowels produced by native and non-native speakers",
   "original": "e01_0137",
   "page_count": 4,
   "order": 51,
   "p1": "137",
   "pn": "140",
   "abstract": [
    "This study explored the relationship between the production of the nine English vowels and the perception of synthesized vowels by thirty-five American, Chinese, and Korean, male and female speakers. The average formant values of the ten American English speakers were employed to synthesize the nine vowels that were presented to the thirty-five speakers. The center formant values of the highest and lowest formant boundary of the same vowel quality were collected and compared to the formant values of their productions. We found that there was a strong correlation between production and perception within and across the language groups. The American, Chinese and Korean groups perceived the stimuli about the same. Individual comparison by regressional analyses of the formant frequency data of the produced vowels and the center formant values of the perceptual test led to a very remarkable r-squared value. This suggests a very lawful relationship between production and perception.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-46"
  },
  "otake01_eurospeech": {
   "authors": [
    [
     "Takashi",
     "Otake"
    ],
    [
     "Yuka",
     "Yamaguchi"
    ]
   ],
   "title": "Japanese can be aware of syllables and morae: evidence from Japanese-English bilingual children",
   "original": "e01_0141",
   "page_count": 4,
   "order": 52,
   "p1": "141",
   "pn": "144",
   "abstract": [
    "This study investigated the metalinguistic knowledge of the internal structure of syllables by Japanese-English bilingual children. Our recent study revealed that Japanese-English adult bilinguals could be aware of two constituents of syllable structure, syllables and morae, depending upon the nature of input materials, while monolingual speakers were aware of morae irrespective of input materials. Three experiments were conducted with 10 bilingual Japanese children, using CVCVNCV materials in Japanese, English and Spanish in order to test whether the same phenomenon could be observed by bilingual children. The subjects were asked to identify the number of chunks within the materials which were presented aurally and to stamp the number of them on a test sheet.\n",
    "The results showed that they preferred moare in Japanese, but syllables in English and Spanish, suggesting that an ability to manipulate two languages freely may have a function to suppress moraic consciousness.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-47"
  },
  "callan01_eurospeech": {
   "authors": [
    [
     "Daniel",
     "Callan"
    ],
    [
     "Keiichi",
     "Tajima"
    ],
    [
     "Akiko",
     "Callan"
    ],
    [
     "Reiko",
     "Akahane-Yamada"
    ],
    [
     "Shinobu",
     "Masaki"
    ]
   ],
   "title": "Neural processes underlying perceptual learning of a difficult second language phonetic contrast",
   "original": "e01_0145",
   "page_count": 4,
   "order": 53,
   "p1": "145",
   "pn": "148",
   "abstract": [
    "Neural processes underlying the perceptual learning of the English /r-l/ phonetic contrast by native Japanese speakers before and after extensive perceptual identification training using feedback was investigated using fMRI. Relative to control conditions (English /b-v/ and /b-g/ contrasts), the /r-l/ contrast showed greater brain activity as well as functional connectivity (reflecting underlying global mappings) post- relative to pre- training bilaterally in frontal and temporal brain areas involved with speech processing as well as the cerebellum and the putamen.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-48"
  },
  "komatsu01_eurospeech": {
   "authors": [
    [
     "Masahiko",
     "Komatsu"
    ],
    [
     "Kazuya",
     "Mori"
    ],
    [
     "Takayuki",
     "Arai"
    ],
    [
     "Yuji",
     "Murahara"
    ]
   ],
   "title": "Human language identification with reduced segmental information: comparison between monolinguals and bilinguals",
   "original": "e01_0149",
   "page_count": 4,
   "order": 54,
   "p1": "149",
   "pn": "152",
   "abstract": [
    "We conducted human language identification experiments using signals with reduced segmental information with Japanese and bilingual subjects. American English and Japanese excerpts from the OGI_TS Corpus were processed by spectral-envelope removal (SER), vowel extraction from SER (VES) and temporal-envelope modulation (TEM). With the SER signal, where the spectral-envelope is eliminated, humans could still identify the languages fairly successfully. With the VES signal, which retains only vowel sections of the SER signal, the identification score was low. With the TEM signal, composed of whitenoise-driven intensity envelopes from several frequency bands, the identification score rose as the number of bands increased. Results varied depending on the stimulus language. Japanese and bilingual subjects demonstrated different scores from each other. These results indicate that humans can identify languages using a signal with drastically reduced segmental information. The results also suggest variation due to the phonetic attributes of languages and subjects knowledge.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-49"
  },
  "fernandez01_eurospeech": {
   "authors": [
    [
     "Santiago",
     "Fernández"
    ],
    [
     "Sergio",
     "Feijóo"
    ]
   ],
   "title": "Coarticulatory effects in perception",
   "original": "e01_0155",
   "page_count": 4,
   "order": 55,
   "p1": "155",
   "pn": "158",
   "abstract": [
    "The perceptual interaction between adjacent CV segments is studied in Fricative-Vowel syllables from a coarticulatory point of view. The results of a perceptual experiment with conflicting-cue stimuli are analyzed under the assumption, supported by the results, of the possible influence of F-to-V carryover coarticulation on the integration process. The DAC scale was used to estimate the degree of F-to-V carryover coarticulation in the original syllables. The magnitude of the coarticulatory effect permitted us to derive a prediction of the perceptual results based on the articulatory compatibility between the fricative and the vowel. The correlation between actual and predicted decrease in perceptual identification caused by the insertion of a conflicting transition was computed. The results show that the coarticulatory processes cannot explain the outcome of the perceptual experiment. Nevertheless, the perceptual role played by the /i/ transition can be effectively explained as a consequence of the F-to-V coarticulation.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-50"
  },
  "harding01_eurospeech": {
   "authors": [
    [
     "Sue",
     "Harding"
    ],
    [
     "Georg",
     "Meyer"
    ]
   ],
   "title": "A case for multi-resolution auditory scene analysis",
   "original": "e01_0159",
   "page_count": 4,
   "order": 56,
   "p1": "159",
   "pn": "162",
   "abstract": [
    "A commonly held view of auditory scene analysis is that complex auditory environments are segregated into separate perceptual streams using primitive cues that can be attended to separately. We argue that this view is inconsistent with the majority of perceptual data reported in the literature and propose an alternative model that is based on a primary, low resolution signal representation used in a passive pattern matching stage, augmented by secondary, high resolution representations that can be used in an active pattern matching stage to formulate hypotheses about the auditory scene.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-51"
  },
  "menard01_eurospeech": {
   "authors": [
    [
     "Lucie",
     "Ménard"
    ],
    [
     "Jean-Luc",
     "Schwartz"
    ],
    [
     "Louis-Jean",
     "Boë"
    ],
    [
     "Sonia",
     "Kandel"
    ],
    [
     "Nathalie",
     "Vallée"
    ]
   ],
   "title": "Perceptual identification and normalization of synthesized French vowels from birth to adulthood",
   "original": "e01_0163",
   "page_count": 4,
   "order": 57,
   "p1": "163",
   "pn": "166",
   "abstract": [
    "This paper aims at exploring the invariant parameters involved in auditory normalization of French vowels. A set of 490 stimuli, including the ten French vowels produced by an articulatory model simulating seven growth stages and seven fundamental frequency values, has been submitted as a perceptual test to 43 subjects. Results confirm the important effect of the tonality distance between F1 and F0 in perceived height. Regarding place of articulation, F2-F1, and F3-F2, in Bark, appear to be good predictors of the perceived front-back dimension. Roundedness is also examined and correlated to the effective second formant, involving spectral integration of higher formants within the 3.5-Bark critical distance.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-52"
  },
  "menard01b_eurospeech": {
   "authors": [
    [
     "Lucie",
     "Ménard"
    ],
    [
     "Louis-Jean",
     "Boë"
    ]
   ],
   "title": "Perceptual categorization of maximal vowel spaces from birth to adulthood simulated by an articulatory model",
   "original": "e01_0167",
   "page_count": 4,
   "order": 58,
   "p1": "167",
   "pn": "170",
   "abstract": [
    "This paper reports on an experiment aiming at determining the perceptual effects of non uniform vocal tract growth. An articulatory model was used to synthesize 342 stimuli, covering the maximal vowel space in the F1/F2 and F2/F3 dimensions, for 5 growth stages: a newborn, a 4-year-old, a 10-year-old, a 16-year-old, and a 21-year-old male speakers. Results of a categorization test of the stimuli by 40 French adult subjects reveal that for each vocal tract length, French phonological categories can be perceived. Furthermore, perceived front vowels cover a broader range in the acoustic F1/F2 space for very small vocal tracts, compared to adults. Data are interpreted in the light of the articulatory-to-acoustic mapping from a developmental point of view, and can shed light on the existence of perceptual constraints during vocal tract growth.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-53"
  },
  "eskenazi01_eurospeech": {
   "authors": [
    [
     "Maxine",
     "Eskenazi"
    ],
    [
     "Alan W.",
     "Black"
    ]
   ],
   "title": "A study on speech over the telephone and aging",
   "original": "e01_0171",
   "page_count": 4,
   "order": 59,
   "p1": "171",
   "pn": "174",
   "abstract": [
    "We describe an experiment to show how the comprehensibility of speech over the telephone is related to the age of the listener. Our intention is to show figures to prove the commonly-held belief that as we get older our hearing of information over the telephone degrades. The study was set up to determine, for all age groups from 20-29 to 80-89, whether comprehension degrades with age and with the type of speech (synthetic or natural). We gave subjects sentences containing target word pairs that they were to write down. The pairs contained more or less predictable words. Our findings, which we consider to be preliminary due to the sample size, show degradation of comprehension with age and degradation from natural speech to synthetic speech.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-54"
  },
  "chen01_eurospeech": {
   "authors": [
    [
     "Marcia",
     "Chen"
    ],
    [
     "Abeer",
     "Alwan"
    ]
   ],
   "title": "On the perception of voicing for plosives in noise",
   "original": "e01_0175",
   "page_count": 4,
   "order": 60,
   "p1": "175",
   "pn": "178",
   "abstract": [
    "Previous research has shown that the VOT and first formant transition are primary perceptual cues for the voicing distinction for syllable-initial plosives (SLP) in quiet environments. This study seeks to determine which cues are important for the perception of voicing for SLP in the presence of noise. Stimuli for the perceptual experiments consisted of naturally-spoken /CV/ syllables (six plosives in 3 vowel contexts) in varying levels of additive white Gaussian noise. In each experiment, plosives which share the same place of articulation (e.g. /p, b/) were presented to subjects in identification tasks. For each voiced/voiceless pair, a threshold SNR value was calculated. Threshold SNR values were then correlated with measurements of several acoustic parameters of the speech tokens. It was found that the VOT did not appear to influence the perception of voicing in noise as much as the first formant transition.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-55"
  },
  "jiang01_eurospeech": {
   "authors": [
    [
     "Jintao",
     "Jiang"
    ],
    [
     "Abeer",
     "Alwan"
    ],
    [
     "Edward T.",
     "Auer"
    ],
    [
     "Lynne E.",
     "Bernstein"
    ]
   ],
   "title": "Predicting visual consonant perception from physical measures",
   "original": "e01_0179",
   "page_count": 4,
   "order": 61,
   "p1": "179",
   "pn": "182",
   "abstract": [
    "The long term goal of our work is to predict visual confusion matrices from physical measurements. In this paper, four talkers were chosen to record 69 American-English Consonant-Vowel syllables with audio, video, and facial movements captured. During the recording, 20 markers were put on the face and an optical Qualisys system was used to track three-dimensional facial movements. The videotapes (with markers on the face and without sound) were presented to normal hearing viewers with average or above average lipreading ability, and visual confusion matrices were obtained. Results showed that the facial measurements were correlated with visual perception data by about 0.79 and account for about 63% of the variance.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-56"
  },
  "ainsworth01_eurospeech": {
   "authors": [
    [
     "William A.",
     "Ainsworth"
    ],
    [
     "T.",
     "Cervera"
    ]
   ],
   "title": "Effects of noise adaptation on the perception of voiced plosives in isolated syllables",
   "original": "e01_0371",
   "page_count": 4,
   "order": 62,
   "p1": "371",
   "pn": "374",
   "abstract": [
    "Speech is easier to understand in continuous noise than in noise which is switched on at the beginning of the speech and off at the end. It is suggested that this is due to some adaptation process. In order to test this hypothesis a series of experiments have been performed in which the intelligibility of plosives in isolated syllables was measured as a function of the duration of the preceding noise. The spectrum of the noise was also varied. It was found that the adaptation, as measured by the mean increase in intelligibility, increased as the duration of the noise preceding the syllable was lengthened. It was also found that the adaptation varied with the centre frequency of the spectrum of the noise. The amount of adaptation was negatively correlated with the threshold of hearing.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-57"
  },
  "hiroshige01_eurospeech": {
   "authors": [
    [
     "Makoto",
     "Hiroshige"
    ],
    [
     "Kenji",
     "Araki"
    ],
    [
     "Koji",
     "Tochinai"
    ]
   ],
   "title": "On differential limen of word-based local speechrate variation in Japanese expressed by duration ratio",
   "original": "e01_0375",
   "page_count": 4,
   "order": 63,
   "p1": "375",
   "pn": "378",
   "abstract": [
    "Fundamental studies about differential limen (DL) for word-based speech rate variations in Japanese are described. In our previous study, the DLs are expressed by subtractive difference of mora duration. In this report, however, to fit the expression for various global speech rate, the DLs are expressed by variation ratio of mora duration. We carry out auditory tests with stimuli made by equally lengthening or shortening a duration of a word in a sentence. The subjects' focus of attention is diffused to get DLs that are used in the normal natural conversations. The obtained DLs are approximately 0.85 for acceleration and 1.18 for deceleration in variation ratio of mora duration.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-58"
  },
  "tokuma01_eurospeech": {
   "authors": [
    [
     "Wan",
     "Tokuma"
    ]
   ],
   "title": "A multidimensional scaling study of fricatives; a comparison of perceptual and physical dimensions",
   "original": "e01_0379",
   "page_count": 4,
   "order": 64,
   "p1": "379",
   "pn": "382",
   "abstract": [
    "This study attempts to model the perceptual similarity data of natural English voiceless fricative syllables in terms of the auditory distance metrics using multidimensional scaling technique (MDS). First, it was proved that the perceptual configuration of nonspeech sounds is adequately modelled by Euclidean distance space. Next, the three distance metrics were analysed to show which metric is most efficient in modelling the perception of speech sounds. Finally, it was shown that the perceptual and physical configurations of five voiceless English fricatives were highly correlated. This result seems to support a model of speech perception based mainly on the general physical characteristics of speech.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-59"
  },
  "swerts01b_eurospeech": {
   "authors": [
    [
     "Marc",
     "Swerts"
    ],
    [
     "Emiel",
     "Krahmer"
    ]
   ],
   "title": "Reconstructing dialogue history",
   "original": "e01_0383",
   "page_count": 4,
   "order": 65,
   "p1": "383",
   "pn": "386",
   "abstract": [
    "This paper deals with a perceptual analysis of accent structure in Dutch to see to what extend listeners are able to reconstruct information from the previous discourse on the basis of prosodic properties of the current utterance. Using data collected in an an earlier dialogue game experiment, subjects were asked to perfom a perceptual task in which they had to reconstruct what the previous utterance was on the basis of input utterances with different accent patterns. Our results reveal that listeners are able to correctly guess the prior context for a significant number of cases, but that performance depends on the type of intonation contour of the input utterance.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-60"
  },
  "house01_eurospeech": {
   "authors": [
    [
     "David",
     "House"
    ],
    [
     "Jonas",
     "Beskow"
    ],
    [
     "Björn",
     "Granström"
    ]
   ],
   "title": "Timing and interaction of visual cues for prominence in audiovisual speech perception",
   "original": "e01_0387",
   "page_count": 4,
   "order": 66,
   "p1": "387",
   "pn": "390",
   "abstract": [
    "The timing of both eyebrow and head movements of a talking face was varied systematically in a test sentence using an audiovisual speech synthesizer. The audio speech signal was unchanged over all sentences. 33 listeners were given the task of identifying the most prominent word in the test sentence. Results indicate that both eyebrow and head movements are powerful visual cues for prominence and that perceptual sensitivity to timing is on the order of a typical syllable duration of 100-200 ms.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-61"
  },
  "komatsu01b_eurospeech": {
   "authors": [
    [
     "Masahiko",
     "Komatsu"
    ],
    [
     "Shinichi",
     "Tokuma"
    ],
    [
     "Won",
     "Tokuma"
    ],
    [
     "Takayuki",
     "Arai"
    ]
   ],
   "title": "Modelling the perceptual identification of Japanese consonants from LPC cepstral distances",
   "original": "e01_0391",
   "page_count": 4,
   "order": 67,
   "p1": "391",
   "pn": "394",
   "abstract": [
    "This study attempts to account for the perceptual phenomenon observed in Komatsu et al. [1] in terms of the spectral properties of the LPC resynthesised stimuli. To implement this, LPC cepstral distances between re-synthesised samples and their original samples are measured. The results of the acoustic analysis and their comparison with the perceptual data indicate that there is a striking similarity in patterns between the spectral property of the Japanese consonants and their perceptual scores. This suggests that the role played by spectral information in the perception of Japanese consonants is significant across all consonant types, and also implies that even in its crudest form, it contributes significantly to their perception.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-62"
  },
  "burnham01_eurospeech": {
   "authors": [
    [
     "Denis",
     "Burnham"
    ],
    [
     "Valter",
     "Ciocca"
    ],
    [
     "Stephanie",
     "Stokes"
    ]
   ],
   "title": "Auditory-visual perception of lexical tone",
   "original": "e01_0395",
   "page_count": 4,
   "order": 68,
   "p1": "395",
   "pn": "398",
   "abstract": [
    "Cantonese speakers were asked to identify spoken words as one of six Cantonese words differing only in tone. Words were presented in three modes: auditory-visual (AV), auditory only (AO), and visual only (VO). Performance was equivalent in the AO and AV conditions - there was no augmentation of auditory tone perception when visual information was added. Nevertheless, performance in the VO condition was significantly above chance under certain conditions: for perceivers without phonetic training, but not those with phonetic training; for tone carried on monophthongs, but not diphthongs; for tones spoken in running speech, but not citation form; and for contour tones (involving pitch movement over time), but not level tones (involving minimal pitch movement). Thus there is visual information for tone which is functionally relevant under certain circumstances.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-63"
  },
  "eriksson01_eurospeech": {
   "authors": [
    [
     "Anders",
     "Eriksson"
    ],
    [
     "Gunilla C.",
     "Thunberg"
    ],
    [
     "Hartmut",
     "Traunmüller"
    ]
   ],
   "title": "Syllable prominence: a matter of vocal effort, phonetic distinct-ness and top-down processing",
   "original": "e01_0399",
   "page_count": 4,
   "order": 69,
   "p1": "399",
   "pn": "402",
   "abstract": [
    "In this experiment, subjects had to rate the \"prominence\" of each of the syllables of 20 versions of the same utterance produced by men, women and children at various levels of vocal effort. The ratings were correlated with measurements of the SPL of the fundamental, spectral emphasis, vowel duration, F0max and F0 rise from the previous syllable. Together with ratings of the perceived vocal effort at which the utterances had been produced, these measurements were used to obtain the possible contributions of vocal effort, prosodic distinctness, and vowel duration to the perceived prominence. Together, these accounted for half of the variance. This was compared with the possible contribution of the linguistic structure of the utterance, which accounted for slightly more of the variance. The predictions of a model based on this analysis came closer to the mean than the average subject.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-64"
  },
  "mixdorff01_eurospeech": {
   "authors": [
    [
     "Hansjörg",
     "Mixdorff"
    ],
    [
     "Christina",
     "Widera"
    ]
   ],
   "title": "Perceived prominence in terms of a linguistically motivated quantitative intonation model",
   "original": "e01_0403",
   "page_count": 4,
   "order": 70,
   "p1": "403",
   "pn": "406",
   "abstract": [
    "The current study investigates the relationship between perceived syllable prominence and the F0 contour as described by a linguistically motivated model of German intonation based on the Fujisaki formula. A subcorpus of the Bonn Prosodic Database was analyzed using the F0 model, and normalized log syllable durations calculated. Analysis shows that, for accented syllables, prominences strongly correlate with the amplitude Aa of accent commands underlying the F0 movements in these syllables, whereas comparable F0 movements in unaccented syllables have little effect on prominence. The influence of Aa versus syllable duration on prominence is stronger for higher prominence levels. The fact that the F0 movement does not necessarily take place in the accented syllable proper, indicates that prominence judgment is partly guided by linguistic considerations. The results also show that F0 modeling in TTS needs to be especially accurate in accented syllables which supports the main rationale of the F0 model.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-65"
  },
  "hawkins01_eurospeech": {
   "authors": [
    [
     "Sarah",
     "Hawkins"
    ],
    [
     "Noël",
     "Nguyen"
    ]
   ],
   "title": "Perception of coda voicing from properties of the onset and nucleus of 'led' and 'let'",
   "original": "e01_0407",
   "page_count": 4,
   "order": 71,
   "p1": "407",
   "pn": "410",
   "abstract": [
    "Syllable-onset /l/ in British English is longer and often has different (usually lower) F2 frequency before a voiced coda. Five experiments explore the perceptual power of these properties and of f0. In each experiment, listeners identified as 'led' or 'let' synthetic syllables whose latter half was replaced by noise. The most reliable cue was /l/ duration; F2 frequency in the /l/ was influential mainly when the vowel quality was held constant. However, listeners learn which cues are most effective, and some choose /l/ duration rather than spectral properties relatively late in the procedure. The results support word recognition models with non-segmental lexical representation that is sensitive to systematic variation in phonetic fine detail.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-66"
  },
  "lin01_eurospeech": {
   "authors": [
    [
     "L.",
     "Lin"
    ],
    [
     "E.",
     "Ambikairajah"
    ],
    [
     "W. H.",
     "Holmes"
    ]
   ],
   "title": "Auditory filter bank design using masking curves",
   "original": "e01_0411",
   "page_count": 4,
   "order": 72,
   "p1": "411",
   "pn": "414",
   "abstract": [
    "It is very difficult and costly to experimentally observe the motion of the basilar membrane in a fully functional cochlea with the view to obtaining amplitude response at points along the membrane. This paper presents an inexpensive method of generating psychoacoustic tuning curves from the well-known masking curves in critical band rate. We present a method for designing critical band auditory filters from the tuning curves. It is also known that the auditory filter frequency response becomes broader with increasing input signal levels and becomes narrower with decreasing signal levels. We also propose a method for designing level dependent auditory filters. The proposed filter bank is applicable to various types of signal processing required to model human auditory filtering.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-67"
  },
  "erdenebat01_eurospeech": {
   "authors": [
    [
     "Dashtseren",
     "Erdenebat"
    ],
    [
     "Kitazawa",
     "Shigeyoshi"
    ],
    [
     "Kitamura",
     "Tatsuya"
    ]
   ],
   "title": "A new feature driven cochlear implant speech processing strategy",
   "original": "e01_0415",
   "page_count": 4,
   "order": 73,
   "p1": "415",
   "pn": "418",
   "abstract": [
    "Our study focuses on the development of a new feature driven speechprocessing strategy for cochlear implant system. In each cycle of stimulation of the cochlea, an electrode, corresponding to second formant frequency was chosen among 14 basilar electrodes. On the base of voiced/unvoiced decision of the speech, an electrode corresponding to first formant frequency was selected for stimulation among 6 apex electrodes. Additionally 4-6 electrodes were chosen for stimulation by maxima energy criteria. Speech intelligibility tests on multi syllable Japanese words within normal hearing listeners by acoustic simulation were provided to evaluate performance of the proposed strategy. Key words: Cochlear implant system, speech feature, acoustic simulation, speech intelligibility test.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-68"
  },
  "zhu01_eurospeech": {
   "authors": [
    [
     "Qifeng",
     "Zhu"
    ],
    [
     "Markus",
     "Iseli"
    ],
    [
     "Xiaodong",
     "Cui"
    ],
    [
     "Abeer",
     "Alwan"
    ]
   ],
   "title": "Noise robust feature extraction for ASR using the Aurora 2 database",
   "original": "e01_0185",
   "page_count": 4,
   "order": 74,
   "p1": "185",
   "pn": "188",
   "abstract": [
    "Four front-end processing techniques developed for noise robust speech recognition are tested with the Aurora 2 database. These techniques include three previously published algorithms: variable frame rate analysis [Zhu and Alwan, 2000], peak isolation [Strope and Alwan, 1997], and harmonic demodulation [Zhu and Alwan, 2000], and a new technique for peak-to-valley ratio locking. Our previous work has focused on isolated digit recognition. In this paper, these algorithms are modified for recognition of connected digits. Recognition results with the Aurora 2 database show that a combination of these four techniques results in 40% error rate reduction when compared to the baseline MFCC front-end for the clean training condition, with no significant increase in computational complexity.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-69"
  },
  "ellis01_eurospeech": {
   "authors": [
    [
     "Daniel P.W.",
     "Ellis"
    ],
    [
     "Manuel J. Reyes",
     "Gomez"
    ]
   ],
   "title": "Investigations into tandem acoustic modeling for the Aurora task",
   "original": "e01_0189",
   "page_count": 4,
   "order": 75,
   "p1": "189",
   "pn": "192",
   "abstract": [
    "In tandem acoustic modeling, signal features are first processed by a discriminantly-trained neural network, then the outputs of this network are treated as the feature inputs to a conventional distribution-modeling Gaussian-mixture model speech recognizer. This arrangement achieves relative error rate reductions of 30% or more on the Aurora task, as well as supporting feature stream combination at the posterior level, which can eliminate more than 50% of the errors compared to the HTK baseline. In this paper, we explore a number of variations on the tandem structure: We experiment with changing the subword units used in each model (neural net and GMM), varying the data subsets used to train each model, substituting the posterior calculations in the neural net with a second GMM, and a variety of feature condition such as deltas, normalization and PCA rank reduction in the `tandem domain' i.e. between the two models.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-70"
  },
  "andrassy01_eurospeech": {
   "authors": [
    [
     "Bernt",
     "Andrassy"
    ],
    [
     "Damjan",
     "Vlaj"
    ],
    [
     "Christophe",
     "Beaugeant"
    ]
   ],
   "title": "Recognition performance of the siemens front-end with and without frame dropping on the Aurora 2 database",
   "original": "e01_0193",
   "page_count": 4,
   "order": 76,
   "p1": "193",
   "pn": "196",
   "abstract": [
    "Following the objective of the Eurospeech special event, 'Noise Robust Recognition', the recognition results of a noise robust front-end, developed by Siemens, on the Aurora 2 database [1] are presented in this paper. The front-end was tested with and without a frame dropping algorithm. It is shown that the front-end improves the recognition results in high mismatch between training and testing by 43.90% over the reference front-end and works particularily well in conditions with high noise. Furthermore it is shown that the frame dropping mainly increases the performance of the front-end.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-71"
  },
  "kotnik01_eurospeech": {
   "authors": [
    [
     "Bojan",
     "Kotnik"
    ],
    [
     "Zdravko",
     "Kacic"
    ],
    [
     "Bogomir",
     "Horvat"
    ]
   ],
   "title": "A multiconditional robust front-end feature extraction with a noise reduction procedure based on improved spectral subtraction algorithm",
   "original": "e01_0197",
   "page_count": 4,
   "order": 77,
   "p1": "197",
   "pn": "200",
   "abstract": [
    "In this paper, the procedure for feature vector extraction in multiconditionally noisy environments is presented. Proposed front-end uses time and spectral domain processing for noise reduction as well as feature extraction to create mel-cepstrum parameters and achieves a trade-off between effective noise reduction and low computational load for real-time operations. First, a novel weighting function is used to reduce the rough noise in time domain, and then a spectral subtraction method based on minimum statistics is applied to decrease the effect of additive broadband noise on speech in the spectral domain. At final stage, a feature vector, which consists of 12 mel-cepstrum parameters and the energy, is created. For evaluation of improvement of speech recognition with presented front-end, the \"Aurora 2\" database together with the HTK recognition toolkit have been chosen. With proposed method an average improvement in performance of 24.75% relative to the current ETSI Aurora standard was achieved.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-72"
  },
  "veth01_eurospeech": {
   "authors": [
    [
     "Johan de",
     "Veth"
    ],
    [
     "Laurent",
     "Mauuary"
    ],
    [
     "Bernhard",
     "Noe"
    ],
    [
     "Febe de",
     "Wet"
    ],
    [
     "Jürgen",
     "Sienel"
    ],
    [
     "Louis",
     "Boves"
    ],
    [
     "Denis",
     "Jouvet"
    ]
   ],
   "title": "Feature vector selection to improve ASR robustness in noisy conditions",
   "original": "e01_0201",
   "page_count": 4,
   "order": 78,
   "p1": "201",
   "pn": "204",
   "abstract": [
    "It is well known that noise reduction schemes are beneficial in ASR to reduce training-test mismatch due to noise. However, a significant mismatch may still remain after noise reduction, especially in the nonspeech portions of the signals. To reduce the impact of this mismatch, two methods for discarding non-speech acoustic vectors at recognition time are investigated: variable frame rate processing and voice activity detection. Experiments are discussed for Aurora 2 and for SpeechDat Car Italian. Results show that both methods are highly effective for SpeechDat Car Italian. However, for Aurora 2, feature vector selection based on voice activity detection hardly gives a benefit, while variable frame rate processing actually lowers recognition accuracy somewhat. Several possible explanations of the different results observed for the two databases are discussed.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-73"
  },
  "macho01_eurospeech": {
   "authors": [
    [
     "Dusan",
     "Macho"
    ],
    [
     "Climent",
     "Nadeu"
    ]
   ],
   "title": "Comparison of spectral derivative parameters for robust speech recognition",
   "original": "e01_0205",
   "page_count": 4,
   "order": 79,
   "p1": "205",
   "pn": "208",
   "abstract": [
    "Recently, spectral first-derivative parameters obtained by frequency filtering (FF) have been successfully used in both clean and noisy HMM speech recognition. In this paper, two types of spectral derivative parameters, the usual FF features and the relative spectral difference (RSD) features, are compared both between them and with their second-derivative versions. Additionally, another kind of recently introduced robust speech features, the SBCOR parameters, are related theoretically with the second-derivative RSD. By experimentally comparing all those types of features in the Aurora 2.0 noisy database framework, we conclude that the first-derivative parameters are preferable to the second-derivative ones (and to the MFCC) for both clean and noisy speech recognition, and the RSD parameters show the best average performance.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-74"
  },
  "yapanel01_eurospeech": {
   "authors": [
    [
     "Umit",
     "Yapanel"
    ],
    [
     "John H. L.",
     "Hansen"
    ],
    [
     "Ruhi",
     "Sarikaya"
    ],
    [
     "Bryan",
     "Pellom"
    ]
   ],
   "title": "Robust digit recognition in noise: an evaluation using the AURORA corpus",
   "original": "e01_0209",
   "page_count": 4,
   "order": 80,
   "p1": "209",
   "pn": "212",
   "abstract": [
    "In this paper, a variety of techniques for robust digit recognition in noise are considered using the AURORA 2.0 corpus. Current recognizers perform as well as humans in small vocabulary tasks but computer recognition performance degrades substantially when noise is introduced into the speech, while human performance is much less sensitive. To make the recognizer robust, several methodologies are employed. These include, feature processing, enhancement before recognition and model adaptation. We considered a number of processing and adaptation scenarios depending on noise type. The best performance, as expected, was obtained in matched training conditions which in general has limited applicability in real world problems. As a feature processing step, using RCCs (Root Cepstrum Coeff.) instead of MFCCs gave substantial improvement. MFCC with front-end enhancement increased performance considerably, but results were far from that obtained with matched training. When we combine the RCC with enhancement, however, we get the best results. In the next step, we employed model adaptation techniques which outperformed MFCC+enhancement and gave much closer results to the matched condition limits. However, MFCC adaptation could not outperform RCC parameterization with front-end enhancement, which we show is much more computationally efficient than model adaptation.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-75"
  },
  "barker01_eurospeech": {
   "authors": [
    [
     "Jon",
     "Barker"
    ],
    [
     "Martin",
     "Cooke"
    ],
    [
     "Phil",
     "Green"
    ]
   ],
   "title": "Robust ASR based on clean speech models: an evaluation of missing data techniques for connected digit recognition in noise",
   "original": "e01_0213",
   "page_count": 4,
   "order": 81,
   "p1": "213",
   "pn": "217",
   "abstract": [
    "In this study, techniques for classification with missing or unreliable data are applied to the problem of noise-robustness in Automatic Speech Recognition (ASR). The techniques described make minimal assumptions about any noise background and rely instead on what is known about clean speech. A system is evaluated using the Aurora 2 connected digit recognition task. Using models trained on clean speech we obtain a 65% relative improvement over the Aurora clean training baseline system, a performance comparable with the Aurora baseline for multicondition training.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-76"
  },
  "droppo01_eurospeech": {
   "authors": [
    [
     "Jasha",
     "Droppo"
    ],
    [
     "Li",
     "Deng"
    ],
    [
     "Alex",
     "Acero"
    ]
   ],
   "title": "Evaluation of the SPLICE algorithm on the Aurora2 database",
   "original": "e01_0217",
   "page_count": 4,
   "order": 82,
   "p1": "217",
   "pn": "220",
   "abstract": [
    "This paper describes recent improvements to SPLICE, Stereo-based Piecewise Linear Compensation for Environments, which produces an estimate of cepstrum of undistorted speech given observed cepstrum of distorted speech. For distributed speech recognition applications, SPLICE can be placed at the server, thus limiting the processing that would take place at the client. We evaluated this algorithm on the Aurora2 task, which consists of digit sequences within the TIDigits database that have been digitally corrupted by passing them through a linear filter and/or by adding different types of realistic. On set A data, for which matched training data is available, we achieved a 66% decrease in word error rate over the baseline system with clean models. This preliminary result is of practical significance because in a server implementation, new noise conditions can be added as they are identified once the service is running.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-77"
  },
  "segura01_eurospeech": {
   "authors": [
    [
     "José C.",
     "Segura"
    ],
    [
     "Angel de la",
     "Torre"
    ],
    [
     "M. Carmen",
     "Benitez"
    ],
    [
     "Antonio M.",
     "Peinado"
    ]
   ],
   "title": "Model-based compensation of the additive noise for continuous speech recognition. experiments using the Aurora II database and tasks",
   "original": "e01_0221",
   "page_count": 4,
   "order": 83,
   "p1": "221",
   "pn": "224",
   "abstract": [
    "In this paper we apply a model-based compensation method to cancel the effect of the additive noise in Automatic Speech Recognition systems. The method is formulated in a statistical framework in order to perform the optimal compensation of the noise effect given the observed noisy speech, a model describing the statistics of the speech recorded in a clean reference environment and the estimation of the noise in the noisy recognition environment. The noise is estimated using the first frames of the sentence to be recognized and a frame-by-frame noise compensation algorithm is performed, so that the compensation procedure does not constrain real-time speech recognition systems and is compatible with emerging technologies based on distributed speech recognition. We have performed recognition experiments under noise conditions using the AURORA II database for the recognition tasks developed for thisdatabase as a standard reference. Experiments have been carried out including both, clean and multicondition training approaches. The experimental results show the improvements in the recognition performance when the proposed model-based compensation method is applied.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-78"
  },
  "morris01_eurospeech": {
   "authors": [
    [
     "Andrew",
     "Morris"
    ],
    [
     "Astrid",
     "Hagen"
    ],
    [
     "Hervé",
     "Bourlard"
    ]
   ],
   "title": "MAP combination of multi-stream HMM or HMM/ANN experts",
   "original": "e01_0225",
   "page_count": 4,
   "order": 84,
   "p1": "225",
   "pn": "228",
   "abstract": [
    "Automatic speech recognition (ASR) performance falls dramatically with the level of mismatch between training and test data. The human ability to recognise speech when a large proportion of frequencies are dominated by noise has inspired the \"missing data\" and \"multi-band\" approaches to noise robust ASR. \"Missing data\" ASR identifies low SNR spectral data in each data frame and then ignores it. Multi-band ASR trains a separate model for each position of missing data, estimates a reliability weight for each model, then combines model outputs in a weighted sum. A problem with both approaches is that local data reliability estimation is inherently inaccurate and also assumes that all of the training data was clean. In this article we present a model in which adaptive multi-band expert weighting is incorporated naturally into the maximum a posteriori (MAP) decoding process.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-79"
  },
  "jarc01_eurospeech": {
   "authors": [
    [
     "Bojan",
     "Jarc"
    ],
    [
     "Rudolf",
     "Babic"
    ]
   ],
   "title": "Second order statistics spectrum estimation method for robust speech recognition",
   "original": "e01_0229",
   "page_count": 4,
   "order": 85,
   "p1": "229",
   "pn": "232",
   "abstract": [
    "A second order statistics spectrum estimation (SOSSE) method for speech enhancement is presented. DFT amplitude spectral components of noisy signal are assumed to be random values. Upon first and second order statistic values estimation of noise-only spectrum, an enhancement of noisy signal spectrum was performed. As a reference, a fast discrete cosine transform based signal subspace (FDCTSS) method was realized. The Aurora 2 database of digit sequences was used, to show methods effectiveness in improvement of speech recognition. Both methods proved well under clean training condition. The total relative improvements of 30.75% (SOSSE) and 26.31% (FDCSS) in recognition accuracy were achieved. When the multi-condition training was done the proposed SOSSE method outperformed FDCTSS method. The total relative improvements of 17.50% (SOSSE) and -4.53% (FDCTSS) were achieved.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-80"
  },
  "yao01_eurospeech": {
   "authors": [
    [
     "Kaisheng",
     "Yao"
    ],
    [
     "Jingdong",
     "Chen"
    ],
    [
     "Kuldip K.",
     "Paliwal"
    ],
    [
     "Satoshi",
     "Nakamura"
    ]
   ],
   "title": "Feature extraction and model-based noise compensation for noisy speech recognition evaluated on AURORA 2 task",
   "original": "e01_0233",
   "page_count": 4,
   "order": 86,
   "p1": "233",
   "pn": "236",
   "abstract": [
    "We have evaluated several feature-based and a model-based method for robust speech recognition in noise. The evaluation was performed on Aurora 2 task. We show that after a sub-band based spectral subtraction, features can be more robust to additive noise. We also report a robust feature set derived from differential power spectrum (DPS), which is not only robust to additive noise, but also robust to spectrum colorization due to channel effects. When the clean training set is available, we show that a model-based noise compensation method can be effective to improve system robustness to noise. Given the testing sets, as a whole, the feature-based methods can yield about 22% relative improvement in accuracy for multi-condition training task, and the model-based method can have about 63% relative performance improvement when systems were trained on clean training set.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-81"
  },
  "federico01_eurospeech": {
   "authors": [
    [
     "Marcello",
     "Federico"
    ],
    [
     "Nicola",
     "Bertoldi"
    ]
   ],
   "title": "Broadcast news LM adaptation using contemporary texts",
   "original": "e01_0239",
   "page_count": 4,
   "order": 87,
   "p1": "239",
   "pn": "242",
   "abstract": [
    "This paper investigates the problem of dynamically updating the language model (LM) of a broadcast news speech recognition system, in order to cope with language and topic changes, typical of the news domain. Statistical adaptation methods are proposed that exploit written news sources which are daily available on the Internet, i.e. newswires and newspapers. Specifically, LM adaptation is performed by extending the basic lexicon, in order to minimize the out-of-vocabulary (OOV) rate, and by adapting the word probability distribution on the contemporary data. Experiments performed on 19 newscasts showed relative reductions of 58% on the OOV rate, 16% on the perplexity, and 4% on the word error rate.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-82"
  },
  "maucec01_eurospeech": {
   "authors": [
    [
     "Mirjam Sepesy",
     "Maucec"
    ],
    [
     "Zdravko",
     "Kacic"
    ]
   ],
   "title": "Topic detection for language model adaptation of highly-inflected languages by using a fuzzy comparison function",
   "original": "e01_0243",
   "page_count": 4,
   "order": 88,
   "p1": "243",
   "pn": "246",
   "abstract": [
    "A new framework is proposed to construct corpus-based topic-adapted language models for large vocabulary speech recognition of highly-inflected Slovenian language. The proposed techniques can be applied to other Slavic languages, where words are formed by many different inflectional affixatation. In this article an attempt to overcome two important difficulties of highly-inflected languages (high out-of-vocabulary rate and the problem of topic detection) is described. The first problem is solved by the decomposition of words into stems and endings, and topic detection is improved by a novel approach for feature extraction based on soft comparison of words. The results of experiments on the second largest Slovenian newspaper news corpus Vecer show the decrease in perplexity by 17% in average over a general word-based model.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-83"
  },
  "georgila01_eurospeech": {
   "authors": [
    [
     "Kallirroi",
     "Georgila"
    ],
    [
     "Nikos",
     "Fakotakis"
    ],
    [
     "George",
     "Kokkinakis"
    ]
   ],
   "title": "Efficient stochastic finite-state networks for language modelling in spoken dialogue systems",
   "original": "e01_0247",
   "page_count": 4,
   "order": 89,
   "p1": "247",
   "pn": "250",
   "abstract": [
    "In this paper we present a novel method for creating language models for Spoken Dialogue Systems (SDS). The idea is based on combining the linguistic structure and the limited requirements for training data of grammar-based models with the robustness of stochastic models regarding spontaneous speech. Our algorithm requires a set of sentences as input, in order to train a Hidden Markov Model (HMM). Classes containing words or phrases with semantic-syntactic similarities are formed automatically and simultaneously with the construction of the HMM. The states and observations of the HMM correspond to the word/phrase classes and words/phrases respectively. The resulting HMM incorporates grammatical structure provided by large context dependencies as well as coverage of ungrammatical spontaneous sentences provided by statistical estimations. The HMM is transformed to a Stochastic Finite-State Network (SFSN), which allows for variable history sizes with no specific upper limit. We used data from 3 different SDSs to evaluate the algorithm. The experiments carried out, resulted in precision and recall values regarding the classes formed, of 0.97 and 0.76 in average, respectively. There was also a reduction of perplexity (16.15% in average) compared to bigrams and a gain in recognition performance (keyword accuracy) of 6.2% compared to grammar-based models and 5.4% compared to bigrams.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-84"
  },
  "visweswariah01_eurospeech": {
   "authors": [
    [
     "Karthik",
     "Visweswariah"
    ],
    [
     "Harry",
     "Printz"
    ]
   ],
   "title": "Language models conditioned on dialog state",
   "original": "e01_0251",
   "page_count": 4,
   "order": 90,
   "p1": "251",
   "pn": "254",
   "abstract": [
    "We consider various techniques for using the state of the dialog in language modeling. The language models we built were for use in an automated airline travel reservation system. The techniques that we explored include (1) linear interpolation with state specific models and (2) incorporating state information using maximum entropy techniques. We also consider using the system prompt as part of the language model history. We show that using state results in about a 20% relative gain in perplexity and about a 9% percent relative gain in word error rate over a system using a language model with no information of the state.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-85"
  },
  "chen01b_eurospeech": {
   "authors": [
    [
     "Langzhou",
     "Chen"
    ],
    [
     "Jean-Luc",
     "Gauvain"
    ],
    [
     "Lori",
     "Lamel"
    ],
    [
     "Gilles",
     "Adda"
    ],
    [
     "Martine",
     "Adda"
    ]
   ],
   "title": "Using information retrieval methods for language model adaptation",
   "original": "e01_0255",
   "page_count": 4,
   "order": 91,
   "p1": "255",
   "pn": "258",
   "abstract": [
    "In this paper we report experiments on language model adaptation using information retrieval methods, drawing upon recent developments in information extraction and topic tracking. One of the problems is extracting reliable topic information with high confidence from the audio signal in the presence of recognition errors. The work in the information retrieval domain on information extraction and topic tracking suggested a new way to solve this problem. In this work, we make use of information retrieval methods to extract topic information in the word recognizer hypotheses, which are then used to automatically select adaptation data from a very large general text corpus. Two adaptive language models, a mixture based model and a MAP based model, have been investigated using the adaptation data. Experiments carried out with the LIMSI Mandarin broadcast news transcription system gives a relative character error rate reduction of 4.3% with this adaptation method.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-86"
  },
  "engwall01_eurospeech": {
   "authors": [
    [
     "Olov",
     "Engwall"
    ]
   ],
   "title": "Making the tongue model talk: merging MRI & EMA measurements",
   "original": "e01_0261",
   "page_count": 4,
   "order": 92,
   "p1": "261",
   "pn": "264",
   "abstract": [
    "Electromagnetic articulography (EMA) data collected with the Movetrack measurement system has been used to set the parameter values in a three-dimensional tongue model dynamically. The outputs from four of the receiver coils are used in the parameter control; the data from the coil on the lower incisor for the jaw height parameter and the three coils on the tongue, T1-T3, for the parameters of different parts of the tongue. The measurements of T1 control the raising and advancing of the tongue tip, those of T2 the tongue body and those of T3 the tongue dorsum movement. Rules to replicate the measured control sequences synthetically have been developed and the synthetic control sequences have been used to synthesize new fricative-vowel sequences.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-87"
  },
  "moen01_eurospeech": {
   "authors": [
    [
     "Inger",
     "Moen"
    ],
    [
     "Hanne Gram",
     "Simonsen"
    ],
    [
     "Morten",
     "Huseby"
    ],
    [
     "John",
     "Grue"
    ]
   ],
   "title": "The relationship between intraoral air pressure and tongue/palate contact during the articulation of norwegian /t/ and /d/",
   "original": "e01_0265",
   "page_count": 4,
   "order": 93,
   "p1": "265",
   "pn": "268",
   "abstract": [
    "Our paper addresses the question of covariation between intraoral air pressure and size of contact area between tongue and palate during the articulation of the Norwegian stop consonants /t/ and /d/. An EPG investigation of the two plosives shows a larger contact area between tongue and palate for /t/ than for /d/. An investigation of intraoral air pressure during the articulation of the two plosives shows higher air pressure for /t/ than for /d/. Presumably, the covariation between air pressure and contact area between tongue and palate may be accounted for in terms of general phonetic-physiological factors. In order to prevent air from escaping between the tongue and the palate during the closing stage of the plosive, and thus producing a fricative, a larger contact area is needed for the voiceless than for the voiced plosive since the air pressure is stronger for the voiceless than for the voiced plosive.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-88"
  },
  "elgendy01_eurospeech": {
   "authors": [
    [
     "Ahmed M.",
     "Elgendy"
    ],
    [
     "Louis C. W.",
     "Pols"
    ]
   ],
   "title": "Mechanical versus perceptual constraints as determinants of articulatory strategy",
   "original": "e01_0269",
   "page_count": 4,
   "order": 94,
   "p1": "269",
   "pn": "272",
   "abstract": [
    "This paper summarizes the results of a series of experiments conducted to investigate various aspects of normal pharyngeal articulation and the nature of pharyngeal coarticualtion. Video fiberscopic imaging, electromagnetography and acoustic analysis techniques were used to obtain empirical and quantitative data on the use of the pharynx in speech production. The overall results suggest that mechanical constraints determine to a great extent the articulatory strategy used by the speaker to achieve the perceptual/acoustic contrast essential for the process of speech encoding.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-89"
  },
  "gick01_eurospeech": {
   "authors": [
    [
     "Bryan",
     "Gick"
    ],
    [
     "Ian",
     "Wilson"
    ]
   ],
   "title": "Pre-liquid excrescent schwa: what happens when vocalic targets conflict",
   "original": "e01_0273",
   "page_count": 4,
   "order": 95,
   "p1": "273",
   "pn": "276",
   "abstract": [
    "Sequences of high tense vowel + liquid in English often result in the percept of an intervening schwa, as in, e.g., heel, hail, hire. We argue in this paper that this apparent schwa is simply the incidental acoustic result of the tongue moving through \"schwa-space\" (a schwa-like position) during the transition between conflicting tongue root targets. This conflict bears on both articulatory timing relationships in syllable codas and tongue root specification for tense vowels. We present two experiments: Experiment 1 shows that excrescent schwa does not correspond with greater duration of syllable rimes; Experiment 2 shows that the tongue moves through schwa space along its trajectory in the excrescent schwa cases. Our results support a timing model whereby coda timing is determined by the relationship between syllable peak and consonant closure, but where timing is unaffected by the number of intervening vocalic events.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-90"
  },
  "ouni01_eurospeech": {
   "authors": [
    [
     "Slim",
     "Ouni"
    ],
    [
     "Yves",
     "Laprie"
    ]
   ],
   "title": "Exploring the null space of the acoustic-to- articulatory inversion using a hypercube codebook",
   "original": "e01_0277",
   "page_count": 4,
   "order": 96,
   "p1": "277",
   "pn": "280",
   "abstract": [
    "Our acoustic to articulatory inversion method exploits an original codebook representing the articulatory space by hypercubes. The articulatory space is decomposed into regions where the articulatory-to-acoustic mapping is linear. Each region is represented by a hypercube. The inversion procedure retrieves articulatory vectors corresponding to an acoustic entry from the hypercube codebook. The main issue is about how all the possible inverse solutions in a given hypercube could be found. As the dimension of the articulatory space is greater than the dimension of the acoustic space, the corresponding null space is sampled by linear programming to retrieve all the possible solutions. Indeed, the sampling of the null space is a crucial point because it directly controls the smoothness of articulatory trajectories recovered from the original signal. This approach permits more realistic articulatory trajectories to be obtained.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-91"
  },
  "theunissen01_eurospeech": {
   "authors": [
    [
     "M. W.",
     "Theunissen"
    ],
    [
     "K.",
     "Scheffler"
    ],
    [
     "J. A. du",
     "Preez"
    ]
   ],
   "title": "Phoneme-based topic spotting on the switchboard corpus",
   "original": "e01_0283",
   "page_count": 4,
   "order": 97,
   "p1": "283",
   "pn": "286",
   "abstract": [
    "The field of topic spotting in conversational speech deals with the problem of identifying \"interesting\" conversations or speech extracts amongst large volumes of speech data. In this research, two phoneme-based topic spotting systems were evaluated on the Switchboard Corpus. Experiments [1,2] on the OGI Corpus suggested that the new Stochastic Method for the Automatic Recognition of Topics (SMART) yields a large improvement over the existing Euclidean Nearest Wrong Neighbours (ENWN) algorithm, which had outperformed competing systems in evaluations [3,4]. However, the small amount of data available for these experiments meant that more rigorous testing was required. We reimplemented the algorithm to run on the larger Switchboard Corpus, and report an improvement of SMART over ENWN characterised by a 35.8% reduction in ROC (receiver operating characteristic) error area. Statistical significance was demonstrated using a modified version of the McNemar test.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-92"
  },
  "franz01_eurospeech": {
   "authors": [
    [
     "Martin",
     "Franz"
    ],
    [
     "J. Scott",
     "McCarley"
    ],
    [
     "Todd",
     "Ward"
    ],
    [
     "Wei-Jing",
     "Zhu"
    ]
   ],
   "title": "Topic styles in IR and TDT: effect on system behavior",
   "original": "e01_0287",
   "page_count": 4,
   "order": 98,
   "p1": "287",
   "pn": "290",
   "abstract": [
    "The TREC Spoken Document Retrieval Track (SDR) and the Topic Detection and Tracking (TDT) project have annotated the same corpus with difference styles of relevance judgements, using differenct notions of topic. We compare the behavior of a topic tracking system using relevance judgements from TDT with that of the same system using relevance from the SDR in order to investigate the influence of differences document relevance judgements on the behavior of the tracking system.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-93"
  },
  "zweig01_eurospeech": {
   "authors": [
    [
     "Geoffrey",
     "Zweig"
    ],
    [
     "Jing",
     "Huang"
    ],
    [
     "Mukund",
     "Padmanabhan"
    ]
   ],
   "title": "Extracting caller information from voicemail",
   "original": "e01_0291",
   "page_count": 4,
   "order": 99,
   "p1": "291",
   "pn": "294",
   "abstract": [
    "In this paper we address the problem of extracting the identities and phone numbers of the callers in voicemail messages. Previous work in information extraction from speech includes spoken document retrieval and named entity detection. This task differs from the named entity task in that the information we are interested in is a subset of the named entities in the message, and consequently, the need to pick the correct subset makes the problem more difficult. Also, the caller's identity may include information that is not typically associated with a named entity. In this work, we present two information extraction methods, one based on hand-crafted rules, and one based on a maximum entropy model. We find that both systems give good performance when applied to manually-derived transcriptions, and that the maximum entropy system can reliably identify the time intervals containing phone numbers, even in the presence of significant decoding errors.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-94"
  },
  "kuo01_eurospeech": {
   "authors": [
    [
     "Hong-Kwang Jeff",
     "Kuo"
    ],
    [
     "Chin-Hui",
     "Lee"
    ]
   ],
   "title": "A portability study on natural language call steering",
   "original": "e01_0295",
   "page_count": 4,
   "order": 100,
   "p1": "295",
   "pn": "298",
   "abstract": [
    "In this paper we examine the portability of the vector-based call router to a new task involving calls to the operator in the UK. One component of the router was shown to require expert knowledge and hand-tuning: the stop word list. Stop word filtering involves replacing certain words with place markers and is necessary to reduce the number of features and parameters used by the classifier. Two specific approaches that eliminates the need for stop word filtering were investigated that led to comparable classification performance: (1) using trigram, bigram, and unigram features and using SVD to reduce the number of parameters, and (2) using only unigram features and applying discriminative training to boost the performance. After discriminative training, the classification error rate was reduced by 18-30% over the baseline unigram results. Increased robustness is demonstrated by a 24-48% reduction in error rate at 20% false rejection rate.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-95"
  },
  "chen01c_eurospeech": {
   "authors": [
    [
     "Berlin",
     "Chen"
    ],
    [
     "Hsin-min",
     "Wang"
    ],
    [
     "Lin-shan",
     "Lee"
    ]
   ],
   "title": "Improved spoken document retrieval by exploring extra acoustic and linguistic cues",
   "original": "e01_0299",
   "page_count": 4,
   "order": 101,
   "p1": "299",
   "pn": "302",
   "abstract": [
    "In this paper, we explored the use of various extra information to improve the performance of spoken document retrieval (SDR). From the speech recognition perspective, we incorporated the acoustic stress and word confusion information into the audio indexing. From the linguistic perspective, we applied the part-of-speech information in both the audio indexing and the query representation. From the information retrieval perspective, we integrated techniques such as the query expansion by word associations and the blind relevance feedback into the retrieval process. The SDR experiments were based on the Topic Detection and Tracking Corpora (TDT-2 and TDT-3). We used the Chinese newswire text stories as query exemplars and the Mandarin Chinese audio news stories as the spoken documents. With all the above acoustic and linguistic cues applied, the average precision was improved from 0.5122 to 0.6312 for the TDT-2 collection and from 0.6216 to 0.7172 for the TDT-3 collection.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-96"
  },
  "tsukada01_eurospeech": {
   "authors": [
    [
     "Kimiko",
     "Tsukada"
    ]
   ],
   "title": "Native vs non-native production of English vowels in spontaneous speech: an acoustic phonetic study",
   "original": "e01_0305",
   "page_count": 4,
   "order": 102,
   "p1": "305",
   "pn": "308",
   "abstract": [
    "This study aims to examine acoustic characteristics of English vowels produced by 1 Australian English talker and 3 Japanese learners of English in spontaneous speech. Primary stressed vowels in multi-syllabic words were extracted from five 15-minute interview sessions. While there was a considerable overlap between different vowel categories both in native and non-native vowel spaces, centroids were more clearly separated in the former than in the latter. All three Japanese learners' vowel spaces were widely spread in the F2 direction. The Australian talker showed a moderate spectral distinction in two pairs /i - sci/ and /a invv/. Although this appears contrary to the spectral overlap commonly reported for these pairs in Australian English, it is consistent with the notion that short vowels are more susceptible to reduction than their long counterparts which are less likely to be undershot in various consonantal contexts.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-97"
  },
  "goronzy01_eurospeech": {
   "authors": [
    [
     "Silke",
     "Goronzy"
    ],
    [
     "Marina",
     "Sahakyan"
    ],
    [
     "Wolfgang",
     "Wokurek"
    ]
   ],
   "title": "Is non-native pronunciation modelling necessary ?",
   "original": "e01_0309",
   "page_count": 4,
   "order": 103,
   "p1": "309",
   "pn": "312",
   "abstract": [
    "It is difficult to recognize non-native speech with speech recognition systems that are trained using native speech. While standard speaker adaptation techniques are often used in theses cases, they are not able to handle severe deviations from the expected pronunciation. Also, there has been a lot of interest in native pronunciation modelling recently. However, results often were not as good as expected. This paper investigates if a special treatment of non-native speakers is necessary. The effect of adding special pronunciation variants to the lexicon is examined. In contrast to native pronunciation modelling the results show that for the non-native case the enhanced dictionary is really necessary to obtain acceptable recognition rates. Recognition rates can be improved by up to 10% for German and even up to 28% for Italian learners of English. When combining this with MLLR adaptation, these results are further improved.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-98"
  },
  "laprie01_eurospeech": {
   "authors": [
    [
     "Yves",
     "Laprie"
    ],
    [
     "Anne",
     "Bonneau"
    ]
   ],
   "title": "Burst segmentation and evaluation of acoustic cues",
   "original": "e01_0313",
   "page_count": 4,
   "order": 104,
   "p1": "313",
   "pn": "316",
   "abstract": [
    "This paper investigates burst segmentation for the evaluation of acoustic cues used to identify unvoiced French stops. Unlike other works which utilize a fixed length window, our approach consists in segmenting bursts into transient and frication noise. The transient is found by minimizing the sum of spectral variances of transient and frication noise over the burst. The spectral variance criterion has the advantage of being sensitive both to energy deviations and spectral variations. Additional correction procedures augment the robustness of the segmentation against the presence of spurious noises during the closure and the determination of the voicing onset with delay. The relevance of our segmentation method has been evaluated by comparing the characteristics of the main spectral peak in the transient segmented by our method with those of the full burst. Our experiments showed that bursts segmented by our method allow a better discrimination between the three places of articulation.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-99"
  },
  "granser01_eurospeech": {
   "authors": [
    [
     "Theodor",
     "Granser"
    ],
    [
     "Sylvia",
     "Moosmüller"
    ]
   ],
   "title": "The schwa in albanian",
   "original": "e01_0317",
   "page_count": 4,
   "order": 105,
   "p1": "317",
   "pn": "320",
   "abstract": [
    "The schwa in Albanian Introduction: In Albanian, the schwa as a phoneme is restricted to the Tosk variety (south of the Shkumbin river, generally considered as basis for the standard), whereas it is described as a back, rounded vowel in the Gheg variety (north of the Shkumbin river). Method: Recordings of spontaneous speech of 7 male speakers have been analyzed. The first two formants were calculated. In total, 570 schwa-vowels have been analyzed, 5 articulation zones have been defined. Results: With respect to the realization of the schwa in stressed position, a significant difference could be observed with regard to \"within\" and \"outside\" the borders of the Republic of Albania. Within the borders of Albania, however, no differences could be observed. Speakers from all regions display a huge amount of variability, ranging from front to back articulation. In unstressed position, a tendency towards centralization can be observed.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-100"
  },
  "ashby01_eurospeech": {
   "authors": [
    [
     "Simone",
     "Ashby"
    ],
    [
     "Julie",
     "Carson-Berndsen"
    ],
    [
     "Gina",
     "Joue"
    ]
   ],
   "title": "A testbed for developing multilingual phonotactic descriptions",
   "original": "e01_0321",
   "page_count": 4,
   "order": 106,
   "p1": "321",
   "pn": "324",
   "abstract": [
    "This paper presents a testbed for developing multilingual phonotactic descriptions that employs finite state methods to represent the phonotactics of one or more languages. The motivation for this work is to make an extensive range of phonotactic descriptions of varying granularity available for speech technology applications. We discuss the design of the phonotactic testbed and how various modules may be used to generate finite state phonotactic descriptions. We provide an example multilingual application drawn from a partial sample of onset clusters spanning four language families, demonstrating how the commonalities of a broad spectrum of languages can be expressed using individual and generic phonotactic automata. We then discuss how these representations are extended via a three-tiered model to provide the basis for the feature- and event-based phonotactic automata.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-101"
  },
  "fung01_eurospeech": {
   "authors": [
    [
     "Wing-Nga",
     "Fung"
    ],
    [
     "Sze-Lok",
     "Lau"
    ]
   ],
   "title": "A physiological analysis of nasals and nasalization in Chinese",
   "original": "e01_0325",
   "page_count": 4,
   "order": 107,
   "p1": "325",
   "pn": "328",
   "abstract": [
    "This paper is a physiological investigation of the vowel nasalization in Chinese by analyzing the nasal and oral airflows for the (C):VN and (C)VN syllables in Shanghai (SH) and Hong Kong Cantonese (HKC). Results show that (i) the degree of nasalization is inversely correlated with the tongue height of the vowel followed by a syllable-final nasal in both SH and HKC; (ii) the duration of nasalization is positively correlated with the advancement of the oral closure for the syllable-final nasal in both SH and HKC; (iii) in general, 10% - 60% of the vowel is nasalized when followed by a nasal ending in SH, except for the schwa, the low-mid back vowel and the low vowel followed by a velar nasal; and (iv) in SH, the schwa, the low-mid back vowel and the low vowel followed by a velar nasal are fully nasalized.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-102"
  },
  "donovan01_eurospeech": {
   "authors": [
    [
     "Robert E.",
     "Donovan"
    ]
   ],
   "title": "A component by component listening test analysis of the IBM trainable speech synthesis system",
   "original": "e01_0329",
   "page_count": 4,
   "order": 108,
   "p1": "329",
   "pn": "332",
   "abstract": [
    "This paper reports on a listening test conducted to determine the impact on speech quality of each component in the IBM Trainable Speech Synthesiser. The study was originally conceived to direct future research effort to those components with the greatest potential for improvement. However, the results and conclusions regarding prosodic modification, concatenation unit length, and decision tree clustering are generally applicable and may be of wider interest.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-103"
  },
  "pan01_eurospeech": {
   "authors": [
    [
     "Shimei",
     "Pan"
    ],
    [
     "Kathleen",
     "McKeown"
    ],
    [
     "Julia",
     "Hirschberg"
    ]
   ],
   "title": "Semantic abnormality and its realization in spoken language",
   "original": "e01_0333",
   "page_count": 4,
   "order": 109,
   "p1": "333",
   "pn": "336",
   "abstract": [
    "In this paper we investigate the relationship between various lexical and prosodic features and 'semantic abnormality', the occurrence of unusual or unexpected events, in generating speech for MAGIC, which employs a Concept-to-Speech system to generate post-operative reports for patients who have undergone bypass surgery. Using the speech corpus collected for this application, we conducted empirical analysis to systematically discover significantly correlated prosodic and lexical features. The automatically learned abnormality model not only can be used in building comprehensive prosody prediction systems for Concept-to-Speech generation, but also help identify unusual information during speech analysis and understanding.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-104"
  },
  "campbell01_eurospeech": {
   "authors": [
    [
     "Nick",
     "Campbell"
    ]
   ],
   "title": "TALKING FOREIGN - concatenative speech synthesis and the language barrier",
   "original": "e01_0337",
   "page_count": 4,
   "order": 110,
   "p1": "337",
   "pn": "340",
   "abstract": [
    "This paper presents a solution to the problem of synthesising multilingual speech using waveform-concatenation speech synthesis. It presents methods for mapping the pronunciations of a target-language speaker onto the sounds available in the speech corpus of a native speaker so that the resulting synthesis produces speech which can accurately represent any foreign words encountered in a predominantly native-language text by use of multi-speaker synthesis. The methods differ depending on the language-pair and on the direction of the mapping, because in the case of one-to-many phonemic mappings, highlevel features can be used, but in the many-to-one case, a physical representation of the target speech signal is required. All mappings are automatic, and the use of rule-based procedures is minimised. In this way, the methods are extensible to any language combinations. Synthesised speech samples are included with the paper so that a subjective evaluation of the results can be made.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-105"
  },
  "jensen01_eurospeech": {
   "authors": [
    [
     "Christian",
     "Jensen"
    ]
   ],
   "title": "Schwa-assimilation in danish synthetic speech",
   "original": "e01_0341",
   "page_count": 4,
   "order": 111,
   "p1": "341",
   "pn": "344",
   "abstract": [
    "Assimilation of schwa into surrounding sonorant consonants is a vital feature of natural Danish speech. It varies with speaking rate and speaking style and is more likely to occur in some phonological contexts than in others. This presents some problems for the implementation of the process into a Danish text-to-speech system.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-106"
  },
  "tamura01_eurospeech": {
   "authors": [
    [
     "Masatsune",
     "Tamura"
    ],
    [
     "Takashi",
     "Masuko"
    ],
    [
     "Keiichi",
     "Tokuda"
    ],
    [
     "Takao",
     "Kobayashi"
    ]
   ],
   "title": "Text-to-speech synthesis with arbitrary speaker's voice from average voice",
   "original": "e01_0345",
   "page_count": 4,
   "order": 112,
   "p1": "345",
   "pn": "348",
   "abstract": [
    "This paper describes a technique for synthesizing speech with any desired voice. The technique is based on an HMM-based text-to-speech (TTS) system and MLLR adaptation algorithm. To generate speech of an arbitrarily given target speaker, speaker-independent speech units, i.e., average voice models, is adapted to the target speaker using MLLR framework. In addition to spectrum and pitch adaptation, we derive an algorithm for adaptation of state duration. We demonstrate that a few sentences uttered by a target speaker are sufficient to adapt not only voice characteristics but also prosodic features. Synthetic speech generated from adapted models using only four sentences is very close to that from speaker dependent models trained using a large amount of speech data.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-107"
  },
  "toda01_eurospeech": {
   "authors": [
    [
     "Tomoki",
     "Toda"
    ],
    [
     "Hiroshi",
     "Saruwatari"
    ],
    [
     "Kiyohiro",
     "Shikano"
    ]
   ],
   "title": "High quality voice conversion based on Gaussian mixture model with dynamic frequency warping",
   "original": "e01_0349",
   "page_count": 4,
   "order": 113,
   "p1": "349",
   "pn": "352",
   "abstract": [
    "In the voice conversion algorithm based on the Gaussian Mixture Model (GMM), quality of the converted speech is degraded because the converted spectrum is exceedingly smoothed. In this paper, we newly propose the GMM-based algorithm with the Dynamic Frequency Warping (DFW) to avoid the over-smoothing. We also propose that the converted spectrum is calculated by mixing the GMM-based converted spectrum and the DFW-based converted spectrum, to avoid the deterioration of conversion-accuracy on speaker individuality. Results of the evaluation experiments clarify that the converted speech quality is better than that of the GMM-based algorithm, and the conversionaccuracy on speaker individuality is the same as that of the GMM-based algorithm in the proposed algorithm with the proper weight for mixing spectra.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-108"
  },
  "tang01_eurospeech": {
   "authors": [
    [
     "Min",
     "Tang"
    ],
    [
     "Chao",
     "Wang"
    ],
    [
     "Stephanie",
     "Seneff"
    ]
   ],
   "title": "Voice transformations: from speech synthesis to mammalian vocalizations",
   "original": "e01_0353",
   "page_count": 4,
   "order": 114,
   "p1": "353",
   "pn": "356",
   "abstract": [
    "This paper describes a phase vocoder based technique for voice transformation. This method can flexibly manipulate various aspects of the input signal, e.g., pitch, duration, energy, and formant positions, without explicit F0 extraction. The modifications can be specific to any feature dimensions, and can vary over time. There are many potential applications for this technique. In concatenative speech synthesis, it can be applied to transform the voice characteristic of the speech corpus, or to smooth pitch or formant discontinuities between concatenation boundaries. The method can also be used in language learning. We can modify the prosody of the student's speech to match that from a native speaker, and use the result to guide improvements. The technique can also be used to convert other biological signals, such as killer whale vocalizations, to ones that are more appropriate for human auditory perception. Our experiments show encouraging results for all of these applications.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-109"
  },
  "gutierrezarriola01_eurospeech": {
   "authors": [
    [
     "J. M.",
     "Gutiérrez-Arriola"
    ],
    [
     "J. M.",
     "Montero"
    ],
    [
     "J. A.",
     "Vallejo"
    ],
    [
     "R.",
     "Córdoba"
    ],
    [
     "R.",
     "San-Segundo"
    ],
    [
     "Juan M.",
     "Pardo"
    ]
   ],
   "title": "A new multi-speaker formant synthesizer that applies voice conversion techniques",
   "original": "e01_0357",
   "page_count": 4,
   "order": 115,
   "p1": "357",
   "pn": "360",
   "abstract": [
    "We present a multi-speaker formant synthesizer based on parameter concatenation. The user can choose among three speakers, two males and one female. The synthesizer stores all the parameters for the basic speaker and linear transformation functions to synthesized the other two. The complete database for one speaker consists of 455 parameterized units (diphones, triphones,...) and the parameters used are pitch, formants and bandwidths and source parameters (four parameters for the LF model, and glottal noise). To get the converted speaker we store a linear transformation function for each spectral stable segment of each unit. Preliminary results show that the quality of the synthesizer is very good and that this system can help us to study and understand the speaker variability problem.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-110"
  },
  "mashimo01_eurospeech": {
   "authors": [
    [
     "Mikiko",
     "Mashimo"
    ],
    [
     "Tomoki",
     "Toda"
    ],
    [
     "Kiyohiro",
     "Shikano"
    ],
    [
     "Nick",
     "Campbell"
    ]
   ],
   "title": "Evaluation of cross-language voice conversion based on GMM and straight",
   "original": "e01_0361",
   "page_count": 4,
   "order": 116,
   "p1": "361",
   "pn": "364",
   "abstract": [
    "Voice conversion is a technique for producing utterances using any target speakers' voice from a single source speaker's utterance. In this paper, we apply cross-language voice conversion between Japanese and English to a system based on a Gaussian Mixture Model (GMM) method and STRAIGHT, a high quality vocoder. To investigate the effects of this conversion system across different languages, we recorded two sets of bilingual utterances and performed voice conversion experiments using a mapping function which converts parameters of acoustic features for a source speaker to those of a target speaker. The mapping functions were trained using bilingual databases of both Japanese and English speech. In an objective evaluation using Mel cepstrum distortion (Mel CD), it was confirmed that the system can perform cross-language voice conversion with the same performance as that within a single-language.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-111"
  },
  "coulston01_eurospeech": {
   "authors": [
    [
     "Rachel",
     "Coulston"
    ]
   ],
   "title": "Ejective reduction in chaha is conditioned by more than prosodic position",
   "original": "e01_0365",
   "page_count": 4,
   "order": 117,
   "p1": "365",
   "pn": "368",
   "abstract": [
    "This paper examines a neutralization asymmetry in Chaha ejectives, concluding that reduction is conditioned not by prosodic position alone, but also by place and manner of articulation. An acoustic examination of Chaha, a Gurage dialect of the Ethiopian Semitic language family, shows that its velar ejectives display a much stronger tendency to lose burst cues before another ejective than do alveolar ejectives in the same environment. This pattern of laryngeal neutralization provides important support for phonetically informed phonological theories. Purely prosody-based theories cannot account for this behavior but a viable alternative is found in a cue-based approach.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-112"
  },
  "kim01b_eurospeech": {
   "authors": [
    [
     "Hong Kook",
     "Kim"
    ],
    [
     "Richard C.",
     "Rose"
    ],
    [
     "Hong-Goo",
     "Kang"
    ]
   ],
   "title": "Acoustic feature compensation based on decomposition of speech and noise for ASR in noisy environments",
   "original": "e01_0421",
   "page_count": 4,
   "order": 118,
   "p1": "421",
   "pn": "424",
   "abstract": [
    "This paper presents a set of acoustic feature pre-processing techniques that are applied to improving automatic speech recognition (ASR) performance on the Aurora 2 noisy speech recognition task. The principal contribution of this paper is an approach for cepstrum domain feature compensation in ASR which is motivated by techniques for decomposing speech and noise that were originally developed for noisy speech enhancement. This approach is applied in combination with other feature compensation algorithms to compensating ASR features obtained from a mel-filterbank cepstrum coefficient (MFCC) front-end. Performance comparisons are made with respect to the application of the minimum mean squared error log spectral amplitude estimator (MMSELSA) based speech enhancement algorithm prior to feature analysis. An experimental study is presented where the feature compensation approaches described in the paper are found to reduce ASR word error rate by as much as 31% relative to uncompensated features under simulated environmental and channel mismatched conditions.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-113"
  },
  "cheng01_eurospeech": {
   "authors": [
    [
     "Yan Ming",
     "Cheng"
    ],
    [
     "Dusan",
     "Macho"
    ],
    [
     "Yuanjun",
     "Wei"
    ],
    [
     "Douglas",
     "Ealey"
    ],
    [
     "Holly",
     "Kelleher"
    ],
    [
     "David",
     "Pearce"
    ],
    [
     "William",
     "Kushner"
    ],
    [
     "Tenkasi",
     "Ramabadran"
    ]
   ],
   "title": "A robust front-end algorithm for distributed speech recognition",
   "original": "e01_0425",
   "page_count": 4,
   "order": 119,
   "p1": "425",
   "pn": "428",
   "abstract": [
    "This paper presents the robust front-end algorithm that was submitted by Motorola to the ETSI STQ-Aurora DSR working group as a proposal for the Advanced DSR front-end in January 2001. The algorithm consists of a two-stage mel-warped Wiener filter, a waveform processor, a channel-normalized mel-frequency cepstral calculation and a subsystem of post-cepstral processing according to the reliability of mel-spectral components, etc. The output of this algorithm, a set of Mel-Frequency Cepstral Coefficients (MFCC), is compressed, encoded and transmitted at 4800 bps. Compared to ETSI standard MFCC front-end, the proposed algorithm delivers an improvement of 47.58% on the Aurora 2 database, which is required by this Eurospeech special event. In this paper we also give further insights about the proposal by providing performances and analyses with the Aurora SpeechDat-Car databases.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-114"
  },
  "benitez01_eurospeech": {
   "authors": [
    [
     "M. Carmen",
     "Benitez"
    ],
    [
     "Lukas",
     "Burget"
    ],
    [
     "Barry",
     "Chen"
    ],
    [
     "Stephane",
     "Dupont"
    ],
    [
     "Hari",
     "Garudadri"
    ],
    [
     "Hynek",
     "Hermansky"
    ],
    [
     "Pratibha",
     "Jain"
    ],
    [
     "Sachin",
     "Kajarekar"
    ],
    [
     "Nelson",
     "Morgan"
    ],
    [
     "Sunil",
     "Sivadas"
    ]
   ],
   "title": "Robust ASR front-end using spectral-based and discriminant features: experiments on the Aurora tasks",
   "original": "e01_0429",
   "page_count": 4,
   "order": 120,
   "p1": "429",
   "pn": "432",
   "abstract": [
    "This paper describes an automatic speech recognition front-end that combines low-level robust ASR feature extraction techniques, and higher-level linear and non-linear feature transformations. The low-level algorithms use data-derived filters, mean and variance normalization of the feature vectors, and dropping of noise frames. The feature vectors are then linearly transformed using Principal Components Analysis (PCA). An Artificial Neural Network (ANN) is also used to compute features that are useful for classification of speech sounds. It is trained for phoneme probability estimation on a large corpus of noisy speech. These transformations lead to two feature streams whose vectors are concatenated and then used for speech recognition. This method was tested on the set of speech corpora used for the Aurora evaluation. Using the feature stream generated without the ANN yields an overall 41% reduction of the error rate over Mel-Frequency Cepstral Coefficients (MFCC) reference features. Adding the ANN stream further reduces the error rate yielding a 46% reduction over the reference features.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-115"
  },
  "noe01_eurospeech": {
   "authors": [
    [
     "Bernhard",
     "Noé"
    ],
    [
     "Jürgen",
     "Sienel"
    ],
    [
     "Denis",
     "Jouvet"
    ],
    [
     "Laurent",
     "Mauuary"
    ],
    [
     "Johan de",
     "Veth"
    ],
    [
     "Louis",
     "Boves"
    ],
    [
     "Febe de",
     "Wet"
    ]
   ],
   "title": "Noise reduction for noise robust feature extraction for distributed speech recognition",
   "original": "e01_0433",
   "page_count": 4,
   "order": 121,
   "p1": "433",
   "pn": "436",
   "abstract": [
    "This paper describes the noise robust feature extraction methods developed by France Telecom and Alcatel for the noise robust front-end standardisation of ETSI Aurora. It is shown that both noise reduction methods give a substantial improvement when compared to a standard MFCC feature extraction algorithm for speech recognition in noisy environments. In addition, blind equalisation and feature vector selection were used for further improvement of recognition performance. Results are discussed for the ETSI Aurora 2 task and the SDC-Italian task as well. It was found that the combination of noise reduction with the proposed methods is capable to achieve around 50% reduction of the error rate. In the context of the open ETSI Aurora standardisation, two proposals were submitted based on these methods, they achieved the best results among all the proposals.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-116"
  },
  "ealey01_eurospeech": {
   "authors": [
    [
     "Douglas",
     "Ealey"
    ],
    [
     "Holly",
     "Kelleher"
    ],
    [
     "David",
     "Pearce"
    ]
   ],
   "title": "Harmonic tunnelling: tracking non-stationary noises during speech",
   "original": "e01_0437",
   "page_count": 4,
   "order": 122,
   "p1": "437",
   "pn": "440",
   "abstract": [
    "This paper presents a novel noise robust front-end algorithm, evaluating its performance on the Aurora 2 database. Most noise robust algorithms for speech recognition assume stationary noise, i.e. that a noise estimate taken prior to the utterance will be accurate for the duration of that utterance. However, for non-stationary noises wherein the noise spectrum can change during the utterance, there can be substantial differences between the estimated and actual noise spectra for a given frame, resulting in poor performance. The algorithm presented here provides a continuous estimate of the noise, making use of the structure of the voiced speech spectrum to sample the gaps (or \"tunnels\") between the harmonic spectral peaks. Compared to the ETSI standard MFCC frontend, the proposed algorithm delivers an average improvement in performance of 43.93% on the Aurora 2 database.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-117"
  },
  "carter01_eurospeech": {
   "authors": [
    [
     "David",
     "Carter"
    ],
    [
     "Ian",
     "Gransden"
    ]
   ],
   "title": "Resource-limited sentence boundary detection",
   "original": "e01_0443",
   "page_count": 4,
   "order": 123,
   "p1": "443",
   "pn": "446",
   "abstract": [
    "We examine the practical constraints imposed on the task of sentence boundary detection in speech recognizer output, by the requirements of a system that supports large-scale commercial off-line transcription of dictations. We develop and evaluate a method that observes these constraints, reformulating the best technique previously reported in order to allow the use a smoothing technique directly tailored to boundary prediction. We then show how this method can be generalized and improved upon, demonstrating significantly better performance in three different domains.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-118"
  },
  "pargellis01_eurospeech": {
   "authors": [
    [
     "Andrew",
     "Pargellis"
    ],
    [
     "Eric",
     "Fosler-Lussier"
    ],
    [
     "Alexandros",
     "Potamianos"
    ],
    [
     "Chin-Hui",
     "Lee"
    ]
   ],
   "title": "Metrics for measuring domain independence of semantic classes",
   "original": "e01_0447",
   "page_count": 4,
   "order": 124,
   "p1": "447",
   "pn": "450",
   "abstract": [
    "The design of dialogue systems for a new domain requires semantic classes (concepts) to be identified and defined. This process could be made easier by importing relevant concepts from previously studied domains to the new one. We propose two methodologies, based on comparison of semantic classes across domains, for determining which concepts are domain-independent, and which are specific to the new task. The concept-comparison technique uses a context-dependent Kullback-Leibler distance measurement to compare all pairwise combinations of semantic classes, one from each domain. The conceptprojection method uses a similar metric to project a single semantic class from one domain into the lexical environment of another. Initial results show that both methods are good indicators of the degree of domain independence for a wide range of concepts, manually generated for three different tasks: Carmen (children's game), Movie (information retrieval) and Travel (flight reservations).\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-119"
  },
  "mou01_eurospeech": {
   "authors": [
    [
     "Xiaolong",
     "Mou"
    ],
    [
     "Stephanie",
     "Seneff"
    ],
    [
     "Victor",
     "Zue"
    ]
   ],
   "title": "Context-dependent probabilistic hierarchical sublexical modelling using finite state transducers",
   "original": "e01_0451",
   "page_count": 4,
   "order": 125,
   "p1": "451",
   "pn": "454",
   "abstract": [
    "This paper describes a unified architecture for integrating sub-lexical models with speech recognition, and a layered framework for contextdependent probabilistic hierarchical sub-lexical modelling using finite state transducers. Our major motivation for designing a unified architecture is to provide a framework such that probabilistic sublexical linguistic components can be integrated with other speech recognition components without sacrificing the flexibilities of their independent developments and configurations. We are also able to obtain a tightly coupled interface between recognizers and sub-lexical components. We present a view of using layered probabilistic models to augment contextfree grammars (CFGs). It captures context-dependent probabilistic information beyond the standard CFG formalism, and provides the flexibility of developing suitable probabilistic models independently for each sub-lexical layer. Experimental results show that such an approach can achieve comparable performance to pronunciation network approaches on in-vocabulary utterances, while being able to substantially reduce errors on utterances with previously unseen words.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-120"
  },
  "bellegarda01_eurospeech": {
   "authors": [
    [
     "Jerome R.",
     "Bellegarda"
    ],
    [
     "Kim E. A.",
     "Silverman"
    ]
   ],
   "title": "Data-driven semantic inference for unconstrained desktop command and control",
   "original": "e01_0455",
   "page_count": 4,
   "order": 126,
   "p1": "455",
   "pn": "458",
   "abstract": [
    "At ICSLP'00, we introduced the concept of data-driven semantic inference, an approach to command and control which in principle allows for any word constructs in command/query formulation. Unconstrained word strings are mapped onto the relevant action through an automated classification relying on latent semantic analysis: as a result, it is no longer necessary for users to memorize the exact syntax of every command. The objective of this paper is to further characterize the behavior of semantic inference, using a desktop command and control task involving 113 different actions. We consider various training scenarios of increasing scope to assess the influence of coverage on performance. Under realistic usage conditions, good classification results can be obtained at a level of coverage as low as 70%.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-121"
  },
  "jansche01_eurospeech": {
   "authors": [
    [
     "Martin",
     "Jansche"
    ]
   ],
   "title": "Information extraction via heuristics for a movie showtime query system",
   "original": "e01_0459",
   "page_count": 4,
   "order": 127,
   "p1": "459",
   "pn": "462",
   "abstract": [
    "Semantic interpretation for limited-domain spoken dialogue systems often amounts to extracting information from utterances. For a system that provides movie showtime information, queries are classified along four dimensions: question type, and movie titles, towns and theaters that were mentioned. Simple heuristics suffice for constructing highly accurate classifiers for the latter three attributes; classifiers for the question type attribute are induced from data using features tailored to spoken language phenomena. Since separate classifiers are used for the four attributes, which are not independent, certain errors can be detected and corrected, thus increasing robustness.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-122"
  },
  "otake01b_eurospeech": {
   "authors": [
    [
     "Takashi",
     "Otake"
    ],
    [
     "Anne",
     "Cutler"
    ]
   ],
   "title": "Recognition of (almost) spoken words: evidence from word play in Japanese",
   "original": "e01_0465",
   "page_count": 4,
   "order": 128,
   "p1": "465",
   "pn": "468",
   "abstract": [
    "Current models of spoken-word recognition assume automatic activation of multiple candidate words fully or partially compatible with the speech input. We propose that listeners make use of this concurrent activation in word play such as punning. Distortion in punning should ideally involve no more than a minimal contrastive deviation between two words, namely a phoneme. Moreover, we propose that this metric of similarity does not presuppose phonemic awareness on the part of the punster. We support these claims with an analysis of modern and traditional puns in Japanese (in which phonemic awareness in language users is not encouraged by alphabetic orthography). For both data sets, the results support the predictions. Punning draws on basic processes of spoken-word recognition, common across languages.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-123"
  },
  "colotte01_eurospeech": {
   "authors": [
    [
     "Vincent",
     "Colotte"
    ],
    [
     "Yves",
     "Laprie"
    ],
    [
     "Anne",
     "Bonneau"
    ]
   ],
   "title": "Perceptual experiments on enhanced and slowed down speech sentences for second language acquisition",
   "original": "e01_0469",
   "page_count": 4,
   "order": 129,
   "p1": "469",
   "pn": "473",
   "abstract": [
    "This paper investigates the perception of speech signals that have been enhanced and slowed down selectively, with the view of improving oral comprehension for second language acquisition. Our modifications are applied on a small number of acoustic cues, i.e. bursts of unvoiced stops, unvoiced fricative noises and rapid spectral transition regions. Bursts and frication noises were amplified, and spectral transitions were amplified and slowed down. We exploit energy and spectral criteria to localize bursts and frication noises, and spectral variation function to spot rapid transitions. The perception experiment involved students who learn French as a foreign language. The subjects were asked to fill in gaps in incomplete transcriptions of 50 French sentences. The average identification rate increases from 72% up to 81% when the enhancement is applied alone, and up to 86% when the two modifications are applied simultaneously. The strengths of our approach are the robustness of acoustic cue detection and the fully automatic strategy.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-124"
  },
  "greenberg01b_eurospeech": {
   "authors": [
    [
     "Steven",
     "Greenberg"
    ],
    [
     "Takayuki",
     "Arai"
    ]
   ],
   "title": "The relation between speech intelligibility and the complex modulation spectrum",
   "original": "e01_0473",
   "page_count": 4,
   "order": 130,
   "p1": "473",
   "pn": "476",
   "abstract": [
    "The amplitude and phase components of the modulation spectrum were dissociated in order to ascertain the importance of cross-spectral, envelope-modulation phase information for understanding spoken language. The dissociation was effected via local time reversals of the speech waveform (i.e., flipping the signal on its horizontal axis) at intervals ranging between 0 and 180 ms. Intelligibility declines progressively as the length of the time-reversed segment increases, down to an asymptotic trough in performance at 100 ms (4% of the words correct). Intelligibility does not correlate highly with the amplitude component of the modulation spectrum, but does coincide closely with the contour of the complex modulation spectrum, a representation that integrates the cross-spectral modulation phase and the conventional (amplitude-based) modulation spectrum into a unified representation. The results imply that intelligibility is based on both the phase and amplitude components of the modulation spectrum.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-125"
  },
  "crouzet01_eurospeech": {
   "authors": [
    [
     "Olivier",
     "Crouzet"
    ],
    [
     "William A.",
     "Ainsworth"
    ]
   ],
   "title": "Envelope information in speech processing: acoustic-phonetic analysis vs. auditory figure-ground segregation",
   "original": "e01_0477",
   "page_count": 4,
   "order": 131,
   "p1": "477",
   "pn": "480",
   "abstract": [
    "Long-term envelope modulations (<100Hz) influence the identification of speech in noise. It is not clear, however, whether this influence only takes place at the level of acoustic-phonetic analysis (phonetic identification) or if envelope fluctuations may also help in auditory figure-ground segregation (e.g. separation of speech from concurrent backgrounds). An experiment is presented in which the influence of long-term envelope modulations was investigated using signals mixed with either stationary or temporally modulated noise. The better performance observed when processing speech in modulated background may be related to the listeners' ability to use envelope information in trying to follow concurrent signals independently. It is therefore predicted that, if long-term envelope modulations help to segregate speech from noisy backgrounds, this effect should be stronger when envelope information is fully available.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-126"
  },
  "adank01_eurospeech": {
   "authors": [
    [
     "Patti",
     "Adank"
    ],
    [
     "Roeland van",
     "Hout"
    ],
    [
     "Roel",
     "Smits"
    ]
   ],
   "title": "A comparison between human vowel normalization strategies and acoustic vowel transformation techniques",
   "original": "e01_0481",
   "page_count": 4,
   "order": 132,
   "p1": "481",
   "pn": "484",
   "abstract": [
    "Perceptual and acoustic representations of vowel data were compared directly to evaluate the perceptual relevance of several speaker normalization transformations. The acoustic representations consisted of raw F0 and formant data. The perceptual representations were obtained through an experimental procedure, with phonetically trained listeners as subjects who had to judge vowel quality on three continuous scales: vowel height, vowel advancement and vowel rounding or spreading. The raw acoustic data were transformed according to several normalization schemes. The perceptual and the acoustic representations were compared using regression techniques. A zscore-transformation of the raw data appeared to resemble the perceptual data.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-127"
  },
  "ircing01_eurospeech": {
   "authors": [
    [
     "P.",
     "Ircing"
    ],
    [
     "P.",
     "Krbec"
    ],
    [
     "J.",
     "Hajic"
    ],
    [
     "J.",
     "Psutka"
    ],
    [
     "S.",
     "Khudanpur"
    ],
    [
     "Frederick",
     "Jelinek"
    ],
    [
     "William",
     "Byrne"
    ]
   ],
   "title": "On large vocabulary continuous speech recognition of highly inflectional language - czech",
   "original": "e01_0487",
   "page_count": 4,
   "order": 133,
   "p1": "487",
   "pn": "490",
   "abstract": [
    "A system for large vocabulary continuous speech recognition of highly inflectional language is introduced. Word-based recognition approach is compared with a morpheme-based recognition system. An experiment involving Czech N-best rescoring has been performed with encouraging results.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-128"
  },
  "shinozaki01_eurospeech": {
   "authors": [
    [
     "Takahiro",
     "Shinozaki"
    ],
    [
     "Chiori",
     "Hori"
    ],
    [
     "Sadaoki",
     "Furui"
    ]
   ],
   "title": "Towards automatic transcription of spontaneous presentations",
   "original": "e01_0491",
   "page_count": 4,
   "order": 134,
   "p1": "491",
   "pn": "494",
   "abstract": [
    "This paper reports various investigations on recognizing spontaneous presentation speech in connection with the \"Spontaneous Speech\" national project started in 1999. Presentation speech uttered by 10 male speakers of approximately 4.5 hours duration has been recognized. Experimental results show that acoustic and language modeling based on an actual spontaneous speech corpus is far more effective than conventional modeling based on read speech. The recognition accuracy has a wide speaker-to-speaker variability according to the speaking rate, the number of fillers, the number of repairs, etc. It was confirmed that unsupervised speaker adaptation of acoustic models was effective to improve the recognition accuracy. The recognition accuracy for spontaneous speech is, however, still rather low, and there remains a large number of research issues.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-129"
  },
  "siohan01_eurospeech": {
   "authors": [
    [
     "Olivier",
     "Siohan"
    ],
    [
     "Akio",
     "Ando"
    ],
    [
     "Mohamed",
     "Afify"
    ],
    [
     "Hui",
     "Jiang"
    ],
    [
     "Chin-Hui",
     "Lee"
    ],
    [
     "Qi",
     "Li"
    ],
    [
     "Feng",
     "Liu"
    ],
    [
     "Kazuo",
     "Onoe"
    ],
    [
     "Frank K.",
     "Soong"
    ],
    [
     "Qiru",
     "Zhou"
    ]
   ],
   "title": "A real-time Japanese broadcast news closed-captioning system",
   "original": "e01_0495",
   "page_count": 4,
   "order": 135,
   "p1": "495",
   "pn": "498",
   "abstract": [
    "This paper describes a collaboration between Bell Labs and NHK (Japan Broadcasting Corp.) STRL to develop a real-time large vocabulary speech recognition system for live closed-captioning of NHK news programs. Bell Labs broadcast news recognition engine consists of a two-pass decoder using bigram language models (LM) and right biphone models during the first pass, and trigram LM with within-word triphone models in the second pass. Various pruning strategies are used to achieve real time decoding, together with a noise compensation procedure aimed at improving recognition on noisy segments of the program. The system operates in a real-time mode and delivers less than 2% of word error rate (WER) on studio news conditions and about 5% of WER on noisy news and reporter speech when evaluated on a real broadcast news program.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-130"
  },
  "beyerlein01_eurospeech": {
   "authors": [
    [
     "Peter",
     "Beyerlein"
    ],
    [
     "X.",
     "Aubert"
    ],
    [
     "M.",
     "Harris"
    ],
    [
     "C.",
     "Meyer"
    ],
    [
     "Hauke",
     "Schramm"
    ]
   ],
   "title": "Investigations on conversational speech recognition",
   "original": "e01_0499",
   "page_count": 4,
   "order": 136,
   "p1": "499",
   "pn": "502",
   "abstract": [
    "Automatic speech recognition of real-life conversational speech is a precondition for building natural human-centered man-machine interfaces. Being able to extract speech utterances from real-life broadcast news audio streams and transcribing them with an overall word accuracy of 83% we are still faced with the problem of transcribing true conversational speech in real-life (i.e. bad) background conditions. The switchboard task focusses on the latter problem. The paper summarizes a set of experimental investigations on the switchboard corpus using the Philips LVCSR system.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-131"
  },
  "gao01_eurospeech": {
   "authors": [
    [
     "Yuqing",
     "Gao"
    ],
    [
     "Hakan",
     "Erdogan"
    ],
    [
     "Yongxin",
     "Li"
    ],
    [
     "Vaibhava",
     "Goel"
    ],
    [
     "Michael",
     "Picheny"
    ]
   ],
   "title": "Recent advances in speech recognition system for IBM DARPA communicator",
   "original": "e01_0503",
   "page_count": 4,
   "order": 137,
   "p1": "503",
   "pn": "506",
   "abstract": [
    "In this paper, we present methods to improve speech recognition performance of the IBM DARPA Communicator system. Our efforts for acoustic modeling include training a domain specific yet broad acoustic model, speaker clustering and speaker adaptation using feature space transforms. For language modeling, we achieved improvements by using compound words, carefully designed LM classes and adjusting the within class probabilities, using NLU state information to enhance the language model and building a language model with embedded grammar objects. Our efforts produced a relative error rate reduction of 34.6% on the test set that consists of 1173 utterances that IBM received during the NIST evaluation of the DARPA Communicator systems in June 2000. We also tested our decoding on the data from some other sites to further demonstrate the robustness of the system improvements.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-132"
  },
  "willett01_eurospeech": {
   "authors": [
    [
     "Daniel",
     "Willett"
    ],
    [
     "Erik",
     "McDermott"
    ],
    [
     "Yasuhiro",
     "Minami"
    ],
    [
     "Shigeru",
     "Katagiri"
    ]
   ],
   "title": "Time and memory efficient viterbi decoding for LVCSR using a precompiled search network",
   "original": "e01_0847",
   "page_count": 4,
   "order": 138,
   "p1": "847",
   "pn": "850",
   "abstract": [
    "In this paper, we present our recently developed time-synchronous speech recognition decoder, which adopts the idea of representing the search space of Large Vocabulary Continuous Speech Recognition (LVCSR) in a single precompiled network. In particular, we outline our approaches for time and memory efficient Viterbi decoding in this scenario. This includes reducing the fast memory needs by keeping the search network on disk and only loading the required parts on demand. Evaluations are carried out on a difficult Japanese LVCSR task which involves a back-off trigram language model and full cross-word dependent triphone acoustic models. Time and memory efficiency enables the real-time Viterbi decoding of entire lecture speeches in a single time-synchronous pass with a search error of less than 1%.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-133"
  },
  "liu01_eurospeech": {
   "authors": [
    [
     "Feng",
     "Liu"
    ],
    [
     "Mohamed",
     "Afify"
    ],
    [
     "Hui",
     "Jiang"
    ],
    [
     "Olivier",
     "Siohan"
    ]
   ],
   "title": "A new verification-based fast match approach to large vocabulary speech recognition",
   "original": "e01_0851",
   "page_count": 4,
   "order": 139,
   "p1": "851",
   "pn": "854",
   "abstract": [
    "This paper proposes a new fast match algorithm for large vocabulary continuous speech recogniton. By viewing the fast match as a verification problem we develop a likelihood ratio score to be used instead of conventional likelihood based fast matches. We also improve the computation through incremental calculation and the use of SSE for Intel instruction set. When used in a 20K Japanese broadcast news task the proposed fast match leads to about 30-40% improvement in speed with almost no degradation in the recognition accuracy.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-134"
  },
  "nakagawa01_eurospeech": {
   "authors": [
    [
     "Seiichi",
     "Nakagawa"
    ],
    [
     "Yukihisa",
     "Horibe"
    ]
   ],
   "title": "A fast calculation method in LVCSRS by time-skipping and clustering of probability density distributions",
   "original": "e01_0855",
   "page_count": 4,
   "order": 140,
   "p1": "855",
   "pn": "858",
   "abstract": [
    "In this paper, we propose a rapid output probability calculation method in HMM based large vocabulary continuous speech recognition systems (LVCSRS). This method is based on time-skipping of calculation, clustering of probability density distributions, and pruning of calculation. Only distributions covering input feature vectors with high probabilities are used to calculate output probabilities strictly, and representative distributions for other distributions are used to calculate them approximately. Here a skipping method for likelihood calculation is adopted in the time domain. Using the rapid calculation method by clustering of probability density distributions, the recognition time in a LVCSRS system was reduced by about 40%. Using a pruning method of likelihood calculations on the way, it was further reduced by 25%. Finally, using time-skipping, the calculation time, furthermore, was reduced by 15% without compromising recognition accuracy.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-135"
  },
  "homma01_eurospeech": {
   "authors": [
    [
     "Shinichi",
     "Homma"
    ],
    [
     "Akio",
     "Kobayashi"
    ],
    [
     "Shoei",
     "Sato"
    ],
    [
     "Toru",
     "Imai"
    ],
    [
     "Akio",
     "Ando"
    ]
   ],
   "title": "Speech recognition of Japanese news commentary",
   "original": "e01_0859",
   "page_count": 4,
   "order": 141,
   "p1": "859",
   "pn": "862",
   "abstract": [
    "This paper describes some improvements in speech recognition of broadcast news commentary in Japanese. Since news commentary speech has different linguistic and acoustic features from read speech, it gives lower word recognition accuracy. In this paper we apply to news manuscripts some rules which represent the linguistic features of news commentaries, and generate word sequences for language model adaptation. We also use a large volume of transcriptions of news programs as training texts. Acoustic models are speaker-adapted and their structures are changed so as to recognize relatively short phonemes, because we found the speech rate of news commentary is sometimes much faster than that of read speech. Furthermore, by using a decoder that can handle cross-word triphone models, we reduced the word error rate by 32%.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-136"
  },
  "cosi01_eurospeech": {
   "authors": [
    [
     "Piero",
     "Cosi"
    ],
    [
     "Fabio",
     "Tesser"
    ],
    [
     "Roberto",
     "Gretter"
    ],
    [
     "Cinzia",
     "Avesani"
    ],
    [
     "Mike",
     "Macon"
    ]
   ],
   "title": "Festival speaks Italian!",
   "original": "e01_0509",
   "page_count": 4,
   "order": 142,
   "p1": "509",
   "pn": "512",
   "abstract": [
    "Finally Festival speaks Italian. In this work, the development of the first Italian version of the Festival TTS system is described. One male and one female voice for three different speech engines are considered: the Festival-specific residual LPC synthesizer, the OGI residual LPC Plug-In for Festival and the MBROLA synthesizer. The new Italian voices will be freely available for download for non-commercial purposes together with specific software modules at http://nts.csrf.pd.cnr.it/IFD/Pages/Italian-TTS.htm This paper isdevotedly dedicated to the memory of Mike Macon, whose recent passing on was really a shock to all of his friends.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-137"
  },
  "monaghan01_eurospeech": {
   "authors": [
    [
     "Alex",
     "Monaghan"
    ],
    [
     "Mahmoud",
     "Kassaei"
    ],
    [
     "Mark",
     "Luckin"
    ],
    [
     "Mariscela",
     "Amador-Hernandez"
    ],
    [
     "Andrew",
     "Lowry"
    ],
    [
     "Dan",
     "Faulkner"
    ],
    [
     "Fred",
     "Sannier"
    ]
   ],
   "title": "Multilingual TTS for computer telephony: the aculab approach",
   "original": "e01_0513",
   "page_count": 4,
   "order": 143,
   "p1": "513",
   "pn": "516",
   "abstract": [
    "The requirements of the computer telephony (CT) industry place conflicting demands on text-to-speech (TTS) systems. Multilingual functionality and high quality output at telephone bandwidth requires detailed linguistic and acoustic analysis. At the same time, the need for robustness together with a high channel count and small memory footprint means that systems must be extremely efficient and databases must be kept small. We present a system which provides TTS for six languages, with 100 channels of highly natural output on a single DSP card.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-138"
  },
  "kiss01_eurospeech": {
   "authors": [
    [
     "Géza",
     "Kiss"
    ],
    [
     "Géza",
     "Németh"
    ],
    [
     "Gábor",
     "Olaszy"
    ],
    [
     "Géza",
     "Gordos"
    ]
   ],
   "title": "A flexible multilingual TTS development and speech research tool",
   "original": "e01_0517",
   "page_count": 4,
   "order": 144,
   "p1": "517",
   "pn": "520",
   "abstract": [
    "Diverse synthesis methods and text-to-speech (TTS) architectures are being developed and applied almost every day. This tendency raises the need for durable program systems that effectively assist research and development in this area. A flexible development system for multilingual text-to-speech and general speech research is introduced. The system was developed for use with the Multivox and Profivox concatenative speech synthesis systems, but its architecture makes it theoretically appropriate for a wide variety of purposes and different TTS systems. The system architecture and the functions of the development system are described. Keywords: TTS development system, speech research tools, system architecture, SGML derivative, object oriented design\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-139"
  },
  "klabbers01_eurospeech": {
   "authors": [
    [
     "Esther",
     "Klabbers"
    ],
    [
     "Karlheinz",
     "Stöber"
    ],
    [
     "Raymond",
     "Veldhuis"
    ],
    [
     "Petra",
     "Wagner"
    ],
    [
     "Stefan",
     "Breuer"
    ]
   ],
   "title": "Speech synthesis development made easy: the bonn open synthesis system",
   "original": "e01_0521",
   "page_count": 4,
   "order": 145,
   "p1": "521",
   "pn": "525",
   "abstract": [
    "This paper describes a new open source architecture for unit-selection based speech synthesis called BOSS (Bonn Open Synthesis System). It is built up modularly, with communications between modules taking place in a fixed format. This makes the addition, deletion and substitution of modules very easy. The strict separation between data and algorithms allows for the simple creation of new speech corpora for different domains and languages.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-140"
  },
  "olaszy01_eurospeech": {
   "authors": [
    [
     "Gábor",
     "Olaszy"
    ],
    [
     "Géza",
     "Németh"
    ],
    [
     "Péter",
     "Olaszi"
    ]
   ],
   "title": "Automatic prosody generation - a model for hungarian",
   "original": "e01_0525",
   "page_count": 4,
   "order": 146,
   "p1": "525",
   "pn": "528",
   "abstract": [
    "In our model a complex function set is described for the three prosody components of read speech. Each of them is modelled separately by a three-step procedure. A new method, based on indirect determination of specific sound durations was developed. Final duration values are calculated from the specific durations in two further steps. F0 changes are also modelled by three levels, starting with rules on sentence level, followed by the word and syllable level, and completed by the micro intonation level. Another three level model serves the intensity structure, i.e. rules applied on sounds, on words and on the complete sentence. The three component models have influence on each other during prosody generation. Cross effects among them are also mentioned. The model can be applied in speech research and in applications (synthesis and recognition). It was tested for Hungarian. Keywords: prosody generation, three-level model, specific sound durations, word-level duration map\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-141"
  },
  "herwijnen01_eurospeech": {
   "authors": [
    [
     "Olga van",
     "Herwijnen"
    ],
    [
     "Jacques",
     "Terken"
    ]
   ],
   "title": "Evaluation of PROS-3 for the assignment of prosodic structure, compared to assignment by human experts",
   "original": "e01_0529",
   "page_count": 4,
   "order": 147,
   "p1": "529",
   "pn": "532",
   "abstract": [
    "This paper describes the results of an evaluation of PROS-3, a system that assigns prosodic structure to text on the basis of the output of a syntactic parser. In order to evaluate the performance of PROS-3 as such and in combination with a revised algorithm for prosodic phrasing, we compare it to the prosodic structure as assigned by human experts. Also, the results of a perception experiment are presented, which show that listeners have the same preference of prosodic realization as we would expect on the basis of the comparison of the prosodic structures as assigned by PROS-3 and by human experts.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-142"
  },
  "yamashita01_eurospeech": {
   "authors": [
    [
     "Yoichi",
     "Yamashita"
    ],
    [
     "Tomoyoshi",
     "Ishida"
    ]
   ],
   "title": "Stochastic F0 contour model based on the clustering of F0 shapes of a syntactic unit",
   "original": "e01_0533",
   "page_count": 4,
   "order": 148,
   "p1": "533",
   "pn": "536",
   "abstract": [
    "This paper describes a stochastic modeling between an F0 contour and linguistic features of a sentence for speech synthesis. The F0 contour of a sentence is represented by concatenation of the F0 patterns of a Japanese syntactic unit, bunsetsu. A bunsetsu F0 pattern is composed of the F0 average and the F0 shape. The most probable sequence of bunsetsu F0 shapes for a sentence are found in the F0 shape database by a probabilistic measure. The probability that an F0 contour is observed for a sentence is defined by two kinds of probabilities, the F0 shape production and the F0 shape bigram. Several typical bunsetsu F0 shapes are extracted by clustering of training data and stored in the F0 shape database. The probability of the F0 shape production is computed for each bunsetsu based on the distribution of linguistic features in the cluster.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-143"
  },
  "sun01_eurospeech": {
   "authors": [
    [
     "Xuejing",
     "Sun"
    ],
    [
     "Ted H.",
     "Applebaum"
    ]
   ],
   "title": "Intonational phrase break prediction using decision tree and n-gram model",
   "original": "e01_0537",
   "page_count": 4,
   "order": 149,
   "p1": "537",
   "pn": "540",
   "abstract": [
    "In the current study, we propose and evaluate a new method for automatic intonational phrase break prediction based on sequences of parts-of-speech and word junctures. The proposed method uses decision trees to estimate the probability of a word juncture type (break or nonbreak) given a finite length window of part-of-speech values, and uses an n-gram to model the word juncture sequence. Trained on an 8,000 word database, our algorithm predicted breaks with F=77% and nonbreaks with F=93%, which represents a significant improvement over the commonly used approach, which uses decision trees alone.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-144"
  },
  "zaki01_eurospeech": {
   "authors": [
    [
     "A.",
     "Zaki"
    ],
    [
     "A.",
     "Rajouani"
    ],
    [
     "M.",
     "Najim"
    ]
   ],
   "title": "Synthesizing intonation of standard arabic language",
   "original": "e01_0541",
   "page_count": 4,
   "order": 150,
   "p1": "541",
   "pn": "545",
   "abstract": [
    "In this paper, we propose a model to generate fundamental frequency (F0) contours using neural networks. A learning procedure is proposed as an alternative to synthesis-by-rules. The generation of correct fundamental frequency contour is one of the important issues in the naturalness of automatic text-to-speech conversion systems. The proposed approach is based on a standard feed-forward multi-layer network that produces global F0 contours of sentences, directly from encoded linguistic features of standard Arabic language. Our model does not need syntactic information to produce suitable declarative intonation. TD-PSOLA synthesizer is used for validation of our results.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-145"
  },
  "xu01_eurospeech": {
   "authors": [
    [
     "Dawei",
     "Xu"
    ],
    [
     "Hiroki",
     "Mori"
    ],
    [
     "Hideki",
     "Kasuya"
    ]
   ],
   "title": "Invariance of relative F0 change field of Chinese disyllabic words",
   "original": "e01_0545",
   "page_count": 4,
   "order": 151,
   "p1": "545",
   "pn": "548",
   "abstract": [
    "In automatic voice response systems where a large number of words are inserted into fixed sentences, such as in voice-guided car navigation systems, one of the most important problems is the adjustment of the fundamental frequency (F0) contour of the inserted word to suit the F0 context of the fixed sentence. The effects of intonation and tone on the F0 contours of Chinese words can be described in terms of a word-level F0 range (WF0R) and an F0 change field (F0CF). WF0R in any position of a sentence is a tone-independent general F0 range, whereas F0CF is an F0 range taking the tone combination of words into account. Relative F0CF is regulated in reference to WF0R. If WF0R is used to represent the declination of a sentence, the relative F0CF should be invariant but dependent on the tone combination of a word. This paper examines the invariance of the relative F0CF among individuals. From an analysis of four native speakers' utterances of 160 words in the initial, middle and final parts of three carrier sentences, conducted over 2 days, we show that: (1) Chinese speakers read words in the same sentence position with stable relative F0 change; (2) the relative F0CFs in the middle position of a sentence are generally the same as those in the initial position, but slightly different from those in the final position; and (3) the relative F0CFs reveal that the effects of tone on F0 contour is individual independent.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-146"
  },
  "muller01_eurospeech": {
   "authors": [
    [
     "Achim F.",
     "Müller"
    ],
    [
     "Rüdiger",
     "Hoffmann"
    ]
   ],
   "title": "Accent label prediction by time delay neural networks using gating clusters",
   "original": "e01_0549",
   "page_count": 4,
   "order": 152,
   "p1": "549",
   "pn": "553",
   "abstract": [
    "In this paper a new neural network (NN) architecture for data driven prediction of accent labels---perceptual accents and pitch accents---for speech synthesis is presented. Within the proposed NN architecture, gating clusters are applied in a time delay (TD) framework. The gating clusters are used to adapt the network structure dynamically such that only available input feature vectors from the actual context window are treated. The proposed NN architecture has been successfully applied for accent label prediction on word level within our text-to-speech (TTS) system. Prediction accuracy for our German corpus was 86.1%. On an english corpus the achieved accuracy was 84.5%. This result is superior to results achieved on the same corpus with an approach based on classification and regression tree (CART) techniques[1]. The results were achieved with a simpler feature set than that used in[1]. [1] K. Ross and M. Ostendorf, \"Prediction of abstract prosodic labels for speech synthesis\"\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-147"
  },
  "henrichsen01_eurospeech": {
   "authors": [
    [
     "Peter Juel",
     "Henrichsen"
    ]
   ],
   "title": "Transformation-based learning of danish stress assignment",
   "original": "e01_0553",
   "page_count": 4,
   "order": 153,
   "p1": "553",
   "pn": "556",
   "abstract": [
    "In Danish, as in other languages, prosody assignment is fairly well described as a function of lexical and syntactic structure. So in principle, prosodic clue assignment should be open to machine learning techniques. This paper presents an experiment using transformationbased ML for unsupervised learning of Danish main stress assignment. The trained stress assigner is compared to the leading Danish text-tospeech system. In conclusion, ML for prosody assignment is advocated as an attractive alternative to naive word mapping as well as to labourintensive grammar writing.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-148"
  },
  "baumann01_eurospeech": {
   "authors": [
    [
     "Stefan",
     "Baumann"
    ],
    [
     "Jürgen",
     "Trouvain"
    ]
   ],
   "title": "On the prosody of German telephone numbers",
   "original": "e01_0557",
   "page_count": 4,
   "order": 154,
   "p1": "557",
   "pn": "560",
   "abstract": [
    "Spoken telephone numbers are prosodically structured. This is reflected on various levels, such as grouping, wording and accenting. Realisation strategies employed by German speakers are used to model the prosody of telephone number production. In a listening preference test using synthetic speech two strategies used by commercial inquiry systems proved to be less acceptable than the versions based on the proposed models. These models are proposed for use in speech-synthesis-based telephone number inquiry services.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-149"
  },
  "schroder01b_eurospeech": {
   "authors": [
    [
     "Marc",
     "Schröder"
    ]
   ],
   "title": "Emotional speech synthesis: a review",
   "original": "e01_0561",
   "page_count": 4,
   "order": 155,
   "p1": "561",
   "pn": "564",
   "abstract": [
    "Attempts to add emotion effects to synthesised speech have existed for more than a decade now. Several prototypes and fully operational systems have been built based on different synthesis techniques, and quite a number of smaller studies have been conducted. This paper aims to give an overview of what has been done in this field, pointing out the inherent properties of the various synthesis techniques used, summarising the prosody rules employed, and taking a look at the evaluation paradigms. Finally, an attempt is made to discuss interesting directions for future development.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-150"
  },
  "gustafson01_eurospeech": {
   "authors": [
    [
     "Kjell",
     "Gustafson"
    ],
    [
     "David",
     "House"
    ]
   ],
   "title": "Fun or boring? a web-based evaluation of expressive synthesis for children",
   "original": "e01_0565",
   "page_count": 4,
   "order": 156,
   "p1": "565",
   "pn": "568",
   "abstract": [
    "Prosodic features were varied in four sentences synthesized using a developmental version of the Infovox 330 concatenated diphone Swedish male voice. The sentences were part of an interactive evaluation test carried out on a commercial website for a period of three months. 78 girls and 56 boys between the ages of 5 and 15 rated the sentences on a qualitative four-point scale. Results indicate that both girls and boys interpret large-scale F0 manipulations as representing a fun voice while longer durations are generally regarded as boring, especially by the boys. The results also confirm the feasibility of using a website for remote evaluation even with children.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-151"
  },
  "chen01d_eurospeech": {
   "authors": [
    [
     "Jingdong",
     "Chen"
    ],
    [
     "Kuldip K.",
     "Paliwal"
    ],
    [
     "Satoshi",
     "Nakamura"
    ]
   ],
   "title": "Sub-band based additive noise removal for robust speech recognition",
   "original": "e01_0571",
   "page_count": 4,
   "order": 157,
   "p1": "571",
   "pn": "574",
   "abstract": [
    "To make an automatic speech recognition system robust with respect to noise, we will probably have to solve two problems. One is the detection and identification of noise. Another is the consideration of noise effect during recognition process. In this paper, we will investigate several noise estimation approaches, such as moving average, long-term average, long-term Fourier analysis, etc. We will then introduce a sub-band based scheme to remove the noise effect from corrupted speech to make recognition system immune to additive noise. We will report on experiments on TI digits database and NOISEX database to justify the proposed approach.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-152"
  },
  "tam01_eurospeech": {
   "authors": [
    [
     "Yik-Cheung",
     "Tam"
    ],
    [
     "Brian",
     "Mak"
    ]
   ],
   "title": "Development of an asynchronous multi-band system for continuous speech recognition",
   "original": "e01_0575",
   "page_count": 4,
   "order": 158,
   "p1": "575",
   "pn": "578",
   "abstract": [
    "Recently, multi-band automatic speech recognition (MBASR) has been proposed to combat environmental noises. We describe the two major efforts in the development of our asynchronous MBASR system for continuous speech recognition. Firstly, we successfully introduce asynchrony among sub-bands under the HMM composition framework. Secondly, the linear sub-band weightings are estimated by minimizing the string classification error among the N-best hypotheses using simulated noisy speech. When our asynchronous MBASR system is evaluated on connected TI digits with 0db additive low-pass white noise, compared with a full-band system, (1) our synchronous MBASR system reduces the absolute string error rate (SER) and word error rate (WER) by 19.8% and 14.1% respectively; (2) the introduction of asynchrony further reduces the absolute SER (WER) by 5.2%(2.5%); (3) an estimation of sub-band weightings using N-best string MCE training gives an additional reduction of absolute SER (WER) by 19.7% (5.1%).\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-153"
  },
  "jancovic01_eurospeech": {
   "authors": [
    [
     "Peter",
     "Jancovic"
    ],
    [
     "Ji",
     "Ming"
    ]
   ],
   "title": "A multi-band approach based on the probabilistic union model and frequency-filtering features for robust speech recognition",
   "original": "e01_0579",
   "page_count": 4,
   "order": 159,
   "p1": "579",
   "pn": "582",
   "abstract": [
    "Multi-band approach has recently been introduced for recognition of speech corrupted by frequency-localized noise, showing higher robustness than the traditional full-band approach. However, the multiband approach has been found to be less robust for wide-band noise than the full-band approach. In this paper, we present a multi-band recognition system based on the combination of the probabilistic union model and the frequency-filtering technique. The probabilistic union model is used to combine the features from the individual sub-bands without requiring information about the sub-band corruption. The frequency-filtering technique is used to produce the feature vector for each sub-band, which is similar to the usual cepstral feature but does not spead the frequency-localized noise over the sub-bands. We demonstrate that this combination results in a system that is equally effective for dealing with both frequency-localized noise and wide-band noise.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-154"
  },
  "gu01_eurospeech": {
   "authors": [
    [
     "Liang",
     "Gu"
    ],
    [
     "Kenneth",
     "Rose"
    ]
   ],
   "title": "Split-band perceptual harmonic cepstral coefficients as acoustic features for speech recognition",
   "original": "e01_0583",
   "page_count": 4,
   "order": 160,
   "p1": "583",
   "pn": "586",
   "abstract": [
    "This paper presents a significant modification of our previously proposed speech recognizer's front-end based on perceptual harmonic cepstral coefficients. The spectrum is split into two frequency bands, which correspond to the harmonic and non-harmonic components. A weighting function, which depends both on the voiced/unvoiced/ transitional classification and on the prominence of harmonic structures, is applied to the harmonic band, and ensures accurate representation of the voiced and transitional speech spectral envelope. Conventional smoothed spectrum is used in the non-harmonic band. The mixed spectrum undergoes mel-scaled band-pass filtering, and the log-energy of the filters' output is discrete cosine transformed to produce cepstral coefficients. Experiments with Mandarin digit and E-set databases show significant recognition gains over plain perceptual harmonic cepstral coefficients and considerable gains over standard techniques.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-155"
  },
  "hagen01_eurospeech": {
   "authors": [
    [
     "Astrid",
     "Hagen"
    ],
    [
     "Herve",
     "Bourlard"
    ]
   ],
   "title": "Error correcting posterior combination for robust multi-band speech recognition",
   "original": "e01_0587",
   "page_count": 4,
   "order": 161,
   "p1": "587",
   "pn": "590",
   "abstract": [
    "In human perception, the availability of context enhances recognition and renders it more robust to noise. Even if not all phonemes in a word (or words in a sentence etc.) are correctly perceived, humans can fill in missing parts with the help of cues from the surrounding speech parts. This was proven in studies on human speech perception where recognition of words in sentences under noise was shown to outperform recognition of words in isolation or, even more drastically, of nonsense syllables under noise. A new model for quantifying the influence of contextual information on human recognition performance was recently proposed. Although the authors state that it is not a model for the recognition process itself, we will see how the ideas behind this model can be used in automatic speech recognition to extend our formerly introduced multi-band recognition systems to incorporate frequency contextual information. We will compare the new set-up to our former models such as the full combination subband approach and its approximation.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-156"
  },
  "gajic01_eurospeech": {
   "authors": [
    [
     "Bojana",
     "Gajic"
    ],
    [
     "Kuldip K.",
     "Paliwal"
    ]
   ],
   "title": "Robust parameters for speech recognition based on subband spectral centroid histograms",
   "original": "e01_0591",
   "page_count": 4,
   "order": 162,
   "p1": "591",
   "pn": "594",
   "abstract": [
    "In this paper we propose a new speech parameterization framework that efficiently combines frequency and magnitude information from the short-term power spectrum of speech. This is achieved through computation of subband spectral centroid histograms (SSCH). Relationship between the proposed method and auditory based speech parameterization methods is discussed. An experimental study on an automatic speech recognition task has shown that the proposed method outperforms the conventional speech front-ends in presence of different types of additive noise, while it performs comparably in the noise-free conditions. In the case of car noise, our method also outperforms the computationally expensive auditory based methods, while having simplicity and low computational cost similar to the conventional frontends.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-157"
  },
  "edmondson01_eurospeech": {
   "authors": [
    [
     "William H.",
     "Edmondson"
    ],
    [
     "Li",
     "Zhang"
    ]
   ],
   "title": "Pseudo-articulatory representations and the recognition of syllable patterns in speech",
   "original": "e01_0595",
   "page_count": 4,
   "order": 163,
   "p1": "595",
   "pn": "598",
   "abstract": [
    "This paper presents an account of syllable structure as the basis for organizing articulatory activity. This contrasts with the serial organization of more conventional phonetic segments. It is demonstrated that working with syllables in this way can provide the basis for linguistically motivated speech recognition using the previously reported notion of the Pseudo-Articulatory Representation (PAR).\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-158"
  },
  "frankel01_eurospeech": {
   "authors": [
    [
     "Joe",
     "Frankel"
    ],
    [
     "Simon",
     "King"
    ]
   ],
   "title": "ASR - articulatory speech recognition",
   "original": "e01_0599",
   "page_count": 4,
   "order": 164,
   "p1": "599",
   "pn": "602",
   "abstract": [
    "We propose that using a continuous trajectory model to describe an articulatory-based feature set will address some of the shortcomings inherent in the hidden Markov model (HMM) as a model for speech recognition. The articulatory parameters allow us to explicitly model effects such as co-articulation and assimilation. A linear dynamic model (LDM) is used to capture the characteristics of each segment type. These models are well suited to describing smoothly varying, continuous, yet noisy trajectories, such as we find present in speech data. Experimentation has been based on data for a single speaker from the MOCHA corpus. This consists of parallel acoustic and recorded articulatory parameters for 460 TIMIT sentences. We report the results of classification and recognition tasks using both real and recovered articulatory parameters, on their own and in conjunction with acoustic features.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-159"
  },
  "ma01b_eurospeech": {
   "authors": [
    [
     "Jeff Z.",
     "Ma"
    ],
    [
     "Li",
     "Deng"
    ]
   ],
   "title": "Efficient decoding strategy for conversational speech recognition using state-space models for vocal-tract-resonance dynamics",
   "original": "e01_0603",
   "page_count": 4,
   "order": 165,
   "p1": "603",
   "pn": "606",
   "abstract": [
    "In this paper, we present an efficient strategy for likelihood computation and decoding in a continuous speech recognizer using underlying statespace dynamic models for the hidden speech dynamics. The state-space models have been constructed in a special way so as to be suitable for the conversational or casual style of speech where phonetic reduction abounds. The interacting multiple model (IMM) state estimation algorithm for switching state-space models is first introduced, which uses a merging strategy derived from Bayes's rule to meet the challenge of exponential growth in the switching combination. Then one specific dynamic-programming based decoding algorithm, incorporating the merging strategy, are derived. It successfully overcomes the exponential growth in the original search paths by using the path-merging strategy. Evaluation experiments on conversational speech using the Switchboard corpus demonstrate that the use of the new decoding strategy is capable of reducing the recognizer's word error rate compared with the baseline recognizers, including the HMM system and the state-space dynamic model using the HMM-produced phonetic boundaries.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-160"
  },
  "weber01_eurospeech": {
   "authors": [
    [
     "Katrin",
     "Weber"
    ],
    [
     "Samy",
     "Bengio"
    ],
    [
     "Hervé",
     "Bourlard"
    ]
   ],
   "title": "HMM2- extraction of formant structures and their use for robust ASR",
   "original": "e01_0607",
   "page_count": 4,
   "order": 166,
   "p1": "607",
   "pn": "610",
   "abstract": [
    "As recently introduced [ftp://ftp.idiap.ch/pub/reports/2000/rr00- 30.ps.gz], an HMM2 can be considered as a particular case of an HMM mixture in which the HMM emission probabilities (usually estimated through Gaussian mixtures or an artificial neural network) are modeled by state-dependent, feature-based HMM (referred to as frequency HMM). A general EM training algorithm for such a structure has already been developed [ftp://ftp.idiap.ch/pub/reports/2000/rr00-11.ps.gz]. Although there are numerous motivations for using such a structure, and many possible ways to exploit it, this paper will mainly focus on one particular instantiation of HMM2 in which the frequency HMM will be used to extract formant structure information, which will then be used as additional acoustic features in a standard Automatic Speech Recognition (ASR) system. While the fact that this architecture is able to automatically extract meaningful formant information is interesting by itself, empirical results will also show the robustness of these features to noise, and their potential to enhance state-of-the-art, noise-robust HMM-based ASR.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-161"
  },
  "yu01_eurospeech": {
   "authors": [
    [
     "Xiaoqing",
     "Yu"
    ],
    [
     "Wanggen",
     "Wan"
    ],
    [
     "Daniel P. K.",
     "Lun"
    ]
   ],
   "title": "Auditory model based speech recognition in noisy environment",
   "original": "e01_0611",
   "page_count": 4,
   "order": 167,
   "p1": "611",
   "pn": "614",
   "abstract": [
    "This paper presents a new speech feature, the ASBF speech feature based on the mathematical model of inner ear of human auditory system. This new speech feature is extracted using both mathematical model of inner ear and primary auditory nerve processing model of human auditory system, and it can track the speech formants effectively. In the experiment, the performance of MFCC and the ASBF are compared in both clean and noisy environments using left-to-right CDHMM with 6 states and 5 Gaussian mixtures. The experimental result shows that the ASBF is much more robust to noise than MFCC. When only 5 dimension is used in ASBF vector, the recognition rate is approximately 38.6% higher than the traditional MFCC with 39 dimension in the condition of S/N=10dB with white noise.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-162"
  },
  "wendt01_eurospeech": {
   "authors": [
    [
     "Sascha",
     "Wendt"
    ],
    [
     "Gernot A.",
     "Fink"
    ],
    [
     "Franz",
     "Kummert"
    ]
   ],
   "title": "Forward masking for increased robustness in automatic speech recognition",
   "original": "e01_0615",
   "page_count": 4,
   "order": 168,
   "p1": "615",
   "pn": "618",
   "abstract": [
    "In automatic speech recognition MFCC or LPCC are features commonly used today. However, their calculation considers only a few features of the auditory system. On the assumption that the human representation of speech is an optimal representation, considering more features of the auditory system might lead to a better performance of automatic speech recognition systems. In this paper a model proposed by Strope and Alwan (see references), which relies on the human acoustic perception and allows to consider the effect of forward masking, is incorporated after some modifications into an automatic speech recognition system with a MFCC-based front-end. The extended system is evaluated on recognition tasks, that are closer to real recognition than (connected) digit recognition commonly used in the literature. The evaluations show an increased robustness of the speech recognition system with forward masking on all recognition tasks, but especially on data recorded in noisy environments.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-163"
  },
  "li01_eurospeech": {
   "authors": [
    [
     "Qi",
     "Li"
    ],
    [
     "Frank K.",
     "Soong"
    ],
    [
     "Olivier",
     "Siohan"
    ]
   ],
   "title": "An auditory system-based feature for robust speech recognition",
   "original": "e01_0619",
   "page_count": 4,
   "order": 169,
   "p1": "619",
   "pn": "622",
   "abstract": [
    "An auditory feature extraction algorithm for robust speech recognition in adverse acoustic environments is presented. The feature computation is comprised of an outer-middle-ear transfer function, FFT, frequency conversion from linear to the Bark scale, auditory filtering, nonlinearity, and discrete cosine transform. The feature is evaluated in two tasks: connected-digit recognition and large vocabulary continuous speech recognition. The tested data were under various noise conditions, including handset and hands-free speech data in landline and wireless communications with additive car and babble noise. Compared with the LPCC, MFCC, MEL-LPCC, and PLP features, the proposed feature has an average 20% to 30% string error rate reduction on the connected-digit task, and 8% to 14% word error rate reduction on the Wall Street Journal task in various additive noise conditions.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-164"
  },
  "lieb01_eurospeech": {
   "authors": [
    [
     "Markus",
     "Lieb"
    ],
    [
     "Alexander",
     "Fischer"
    ]
   ],
   "title": "Experiments with the philips continuous ASR system on the AURORA noisy digits database",
   "original": "e01_0625",
   "page_count": 4,
   "order": 170,
   "p1": "625",
   "pn": "628",
   "abstract": [
    "With this work we evaluate the Philips continuous speech recognition system on the standardized AURORA noisy digit string recognition task. A variety of noise robust algorithms, ranging from spectral subtraction during the feature extraction stage, to adaptation techniques in the HMM-decoding stage, are applied and their effects are presented. Detailed experimental results show the contribution of the single approaches to the overall system performance. By thoroughly combining the best performing of the standard algorithms, we achieve significant improvements for the matched training as well as for the non-matched condition scenarios.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-165"
  },
  "saon01_eurospeech": {
   "authors": [
    [
     "George",
     "Saon"
    ],
    [
     "Juan M.",
     "Huerta"
    ],
    [
     "Ea-Ee",
     "Jan"
    ]
   ],
   "title": "Robust digit recognition in noisy environments: the IBM Aurora 2 system",
   "original": "e01_0629",
   "page_count": 4,
   "order": 171,
   "p1": "629",
   "pn": "632",
   "abstract": [
    "In this paper we describe some experiments on the Aurora 2 noisy digits database. The algorithms that we used can be broadly classified into noise robustness techniques based on a linear-channel model of the acoustic environment such as CDCN and its novel variant termed Alignment-based CDCN (ACDCN, proposed here), and techniques which do not assume any particular knowledge about the structure of the environment or noise conditions affecting the speech signal such as discriminant feature space transformations and speaker/channel adaptation. We present recognition experiments for both the clean training data and the multi-condition training data scenarios.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-166"
  },
  "afify01_eurospeech": {
   "authors": [
    [
     "Mohamed",
     "Afify"
    ],
    [
     "Hui",
     "Jiang"
    ],
    [
     "F.",
     "Korkmazskiy"
    ],
    [
     "Chin-Hui",
     "Lee"
    ],
    [
     "Qi",
     "Li"
    ],
    [
     "Olivier",
     "Siohan"
    ],
    [
     "Frank K.",
     "Soong"
    ],
    [
     "Arun C.",
     "Surendran"
    ]
   ],
   "title": "Evaluating the Aurora connected digit recognition task -- a bell labs approach",
   "original": "e01_0633",
   "page_count": 4,
   "order": 172,
   "p1": "633",
   "pn": "636",
   "abstract": [
    "Connected digit recognition has always been an ideal task for fundamental research in speech recognition due to its low complexity and potential applicaitons. In Bell Labs we have developed a number of techniques targeting directly or indirectly at connected digit recognition. For the Aurora task, we study a few such algorithms for the entire spectrum of the issues, including feature extraction, context-dependent digit modeling, minimum classification error acoustic modeling, unsupervised noise compensation, and utterance verification. We show how each component contributes to the reduction of digit recognition and verification errors. Average over all three test sets we obtained 84.6% and 91.3% digit accuracies for clean- and multi-condition training, respectively. This represents an average of 48.6% error rate reduction when compared to the official Aurora baseline results.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-167"
  },
  "fougeron01_eurospeech": {
   "authors": [
    [
     "Cécile",
     "Fougeron"
    ],
    [
     "J. P.",
     "Goldman"
    ],
    [
     "U. H.",
     "Frauenfelder"
    ]
   ],
   "title": "Liaison and schwa deletion in French: an effect of lexical frequency and competition?",
   "original": "e01_0639",
   "page_count": 4,
   "order": 173,
   "p1": "639",
   "pn": "642",
   "abstract": [
    "This study aims to determine whether the production of the lexical variants created by the phonological processes of liaison and schwa deletion in French are conditioned by factors linked to lexical recognition. We hypothesise that the realisation of these variants would be favoured for words which are lexically \"salient\" in term of frequency and in their lexical neighbourhoods. This claim was tested by examining a speech corpus for the effects of lexical frequency, neighbourhood density and neighbourhood frequency on the production of liaison (both in linking and linked words and their co-occurrence) and elision. Overall the results do not support our hypothesis: lexical frequency and competition do not appear to influence strongly whether liaison and elision are realised or not.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-168"
  },
  "zee01_eurospeech": {
   "authors": [
    [
     "Eric",
     "Zee"
    ],
    [
     "Wai-Sum",
     "Lee"
    ]
   ],
   "title": "An acoustical analysis of the vowels in beijing Mandarin",
   "original": "e01_0643",
   "page_count": 4,
   "order": 174,
   "p1": "643",
   "pn": "646",
   "abstract": [
    "The study is a spectral analysis of the vowels and syllabic approximants in Beijing Mandarin. It presents: (i) the average F1, F2, F3 values for the resonant sounds, (ii) the vowel ellipses for the resonant sounds, showing their relative positions in the F1/F2 plane, (iii) the vowel diagrams, showing the F-patterns of the first three formant frequencies of the resonant sounds, (iv) the formant trajectories for the rhotic schwa, showing that the vowel in the V syllables is actually a sequence of a plain schwa and a rhotic schwa, and (v) the diagrams of the average vowel positions for the vowels followed by a nasal ending, showing that the effect of the nasal ending on the F1 and F2 of the vowel sounds varies accoring to vowel type and nasal type.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-169"
  },
  "delvaux01_eurospeech": {
   "authors": [
    [
     "Veronique",
     "Delvaux"
    ],
    [
     "Alain",
     "Soquet"
    ]
   ],
   "title": "Discriminant analysis of nasal vs. oral vowels in French: comparison between different parametric representations",
   "original": "e01_0647",
   "page_count": 4,
   "order": 175,
   "p1": "647",
   "pn": "650",
   "abstract": [
    "The purpose of this paper is to investigate the realization of the [nasal] contrast in French by performing an acoustic analysis of naturally spoken nasal and oral vowels, and by carrying out discriminant analysis on these data. Results consistently show that generic parametric representations allow to reliably discriminate between nasals and orals. A specific issue addressed in this paper is the relationship between phonetic and phonological nasalization in French.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-170"
  },
  "demolin01_eurospeech": {
   "authors": [
    [
     "Didier",
     "Demolin"
    ],
    [
     "Véronique",
     "Delvaux"
    ]
   ],
   "title": "Whispery voiced nasal stops in rwanda",
   "original": "e01_0651",
   "page_count": 4,
   "order": 176,
   "p1": "651",
   "pn": "654",
   "abstract": [
    "The paper describes the main phonetic characteristics (acoustic, aerodynamic and articulatory) of Kinyarwanda prenasalized stops, focussing on voiceless consonants. We conclude from instrumental observations that the phonetic description of these sounds should be redefined. Consonants previously described as voiceless prenasalized stops in Rwanda are in fact whispery voiced nasal stops. Finally, the paper shows that the description of these sounds raises several important questions about nasal venting and the control of the velum closure.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-171"
  },
  "fant01_eurospeech": {
   "authors": [
    [
     "Gunnar",
     "Fant"
    ],
    [
     "Anita",
     "Kruckenberg"
    ],
    [
     "Johan",
     "Liljencrants"
    ],
    [
     "Antonis",
     "Botinis"
    ]
   ],
   "title": "Prominence correlates. a study of Swedish",
   "original": "e01_0657",
   "page_count": 4,
   "order": 177,
   "p1": "657",
   "pn": "660",
   "abstract": [
    "This is a summary of studies of word and syllable prominence in Swedish performed during several years. A unique feature is the correlation of observed acoustic data with a continuously scaled parameter of perceived prominence. Besides the established parameters of duration, F0, intensity, and spectral tilt we have also data on true subglottal pressure. Studies of co-variation within the set of acoustic parameters reveal some interesting relations, some of which can be related to the production mechanism. The major part of the material derives from prose reading, but we have also data from contrasting \"lab type\" sentences. Some systematic differences appear. Our findings have applications in the development of text-to-speech rules\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-172"
  },
  "ohno01_eurospeech": {
   "authors": [
    [
     "Sumio",
     "Ohno"
    ],
    [
     "Hiroya",
     "Fujisaki"
    ]
   ],
   "title": "Quantitative analysis of the effects of emphasis upon prosodic features of speech",
   "original": "e01_0661",
   "page_count": 4,
   "order": 178,
   "p1": "661",
   "pn": "664",
   "abstract": [
    "While it is known that emphasis is represented mainly by fundamental frequency, speech rate, and source intensity, few studies have been published on the relative roles of these variables in expressing the degree of emphasis. The present paper introduces the relative speech rate and the relative source intensity of a target utterance against a reference utterance, and formulates the processes of their generation by quantitative models that are in line with the model that has been established for the fundamental frequency contour. This makes it possible to compare the effects of emphasis on the three variables in quantitative terms, as well as to compare the effects of various degrees of emphasis. Analysis of English utterances by a native speaker and a non-native speaker indicated the influence of emphasis on the three variables in quantitative terms, and also clarified the difference between native and non-native speakers.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-173"
  },
  "dogil01_eurospeech": {
   "authors": [
    [
     "Grzegorz",
     "Dogil"
    ],
    [
     "Bernd",
     "Möbius"
    ]
   ],
   "title": "Towards a model of target oriented production of prosody",
   "original": "e01_0665",
   "page_count": 4,
   "order": 179,
   "p1": "665",
   "pn": "668",
   "abstract": [
    "A new paradigm for prosody research is presented, inspired by the speech production model recently proposed by Guenther, Perkell, and colleagues. This research paradigm aims at generalizing the production model by extending it from a predominantly segmental perspective to a new theory of the production of prosody. Speech movements in the prosodic domain are interpreted as intonational gestures that are planned to reach and traverse perceptual target regions. Evidence from F0 alignment studies suggests that the perceptual targets can be approximately represented by regions in a multidimensional acoustictemporal space. These studies also indicate that segmental, spectral, temporal, and prosodic structure are co-produced in such a way as to mutually support and enhance, and not impair, the perceptual targets. Furthermore, examples of multi-level mappings between invariant and variable targets in the domain of prosody are provided, and a dichotomy of phonemic and postural prosodic settings is discussed.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-174"
  },
  "shih01_eurospeech": {
   "authors": [
    [
     "Chilin",
     "Shih"
    ],
    [
     "Greg",
     "Kochanski"
    ]
   ],
   "title": "Prosody control for speaking and singing styles",
   "original": "e01_0669",
   "page_count": 4,
   "order": 180,
   "p1": "669",
   "pn": "672",
   "abstract": [
    "By proper control of prosody, text-to-speech systems already have the capability to imitate distinctive speaking styles. We show two examples where we can capture the critical features: the singing style of Dinah Shore and the speaking style of Martin Luther King Jr. The styles are described by Stem-ML tags (soft template mark-up language), which offers the flexibility needed to control accent shapes, phrasal pitch contours, and amplitude profiles, for speech as well as for singing.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-175"
  },
  "kochanski01_eurospeech": {
   "authors": [
    [
     "Greg",
     "Kochanski"
    ],
    [
     "Chilin",
     "Shih"
    ]
   ],
   "title": "Automated modeling of Chinese intonation in continuous speech",
   "original": "e01_0911",
   "page_count": 4,
   "order": 181,
   "p1": "911",
   "pn": "914",
   "abstract": [
    "We built and trained a model of intonation in continuous Mandarin speech based on the Stem-ML model of interacting accents. With this model, we found that we can accurately reproduce the intonation of the speaker using only one accent template for each lexical tone category. The resulting parameters are interpretable, and we find that the fitted model is consistent with linguistic expectations. Stem-ML is a phenomenological model of the muscle dynamics and planning process that controls the tension of the vocal folds. It describes the interactions between nearby tones or accents.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-176"
  },
  "frid01_eurospeech": {
   "authors": [
    [
     "Johan",
     "Frid"
    ]
   ],
   "title": "Prediction of intonation patterns of accented words in a corpus of read Swedish news through pitch contour stylization",
   "original": "e01_0915",
   "page_count": 4,
   "order": 182,
   "p1": "915",
   "pn": "918",
   "abstract": [
    "This paper describes an initial attempt at the construction of a data-driven model of Swedish intonation. The study is mainly concerned with model-building and prediction of the intonation patterns of accented words in a corpus of read news in Swedish. Extraction of pitch information is achieved by performing a stylization of the pitch contours. The information is used to build a model for the prediction of pitch patterns using linguistic features such as accent type and position of stress. The model is tested against unseen data from the same corpus. The evaluation is done by numerical comparisons. The RMSE between predicted and original contours for the different categories ranges between 3.7 and 31.4 Hz. The results are quite promising for future studies.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-177"
  },
  "alku01_eurospeech": {
   "authors": [
    [
     "Paavo",
     "Alku"
    ],
    [
     "Juha",
     "Vintturi"
    ],
    [
     "Erkki",
     "Vilkma"
    ]
   ],
   "title": "The use of fundamental frequency raising as a strategy for increasing vocal intensity in soft, normal, and loud phonation",
   "original": "e01_0919",
   "page_count": 4,
   "order": 183,
   "p1": "919",
   "pn": "922",
   "abstract": [
    "A method is presented to estimate the effect of intentional raising of fundamental frequency (F0) on vocal intensity. The method, Energy of the Synthesised Period (ESP), is based on computation of the energy of a hypothetical speech sound synthesised using a single period of the glottal volume velocity waveform and a digital filter that models the vocal tract. Both the glottal flow and the vocal tract filter are estimated by inverse filtering. The results show that, in producing loud voice, speakers use F0 to increase the number of glottal closures per time unit, which increases rapid fluctuations in the speech pressure waveform, which, in turn, raises vocal intensity. The average increase of sound pressure level due to this active use of F0 was approximately 4 dB in loud speech.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-178"
  },
  "botinis01_eurospeech": {
   "authors": [
    [
     "Antonis",
     "Botinis"
    ],
    [
     "Marios",
     "Fourakis"
    ],
    [
     "Robert",
     "Bannert"
    ]
   ],
   "title": "Prosodic interactions on segmental durations ingreek",
   "original": "e01_0923",
   "page_count": 4,
   "order": 184,
   "p1": "923",
   "pn": "926",
   "abstract": [
    "The present study is an experimental investigation of the effects of prosodic variables on segmental durations in Greek. Nonsense disyllabic CVCV words were produced in a carrier sentence under different conditions of stress, focus and tempo. The results indicate: (1) the intrinsic durations of vowels are rather canonical in the order /iu<eo<a/; (2) the adjacent consonant /s/ shows complementary duration tendencies; (3) stress has bigger effect on the vowel than the consonant; (4) focus has no major effects; (5) tempo has also bigger effect on the vowel than the consonant. In summary, stress has a bigger effect on both consonant and vowel durations than tempo whereas the effects of focus are in question.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-179"
  },
  "chu01_eurospeech": {
   "authors": [
    [
     "Min",
     "Chu"
    ],
    [
     "Yongqiang",
     "Feng"
    ]
   ],
   "title": "Study on factors influencing durations of syllables in Mandarin",
   "original": "e01_0927",
   "page_count": 4,
   "order": 185,
   "p1": "927",
   "pn": "930",
   "abstract": [
    "This paper studies factors that have influences on durations of syllables in Mandarn. Six factors are investigated by comparing means of duration in different categories in the preparatory study. The vector space is reduced from 299250 to 1000 by removing factors that do not cause significant changing in means and merging levels in each factor that have similar means. In order to remove influences from various initials and finals, durations of syllables are divided by the mean duration of the corresponding base syllables. The sum of squared residuals analysis is performed on the normalized duration. The results reveal that boundary index has the largest influence on syllable durations. Tone identity comes next. Though, position in word is not a very important single factor, it influences duration together with boundary index.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-180"
  },
  "gustafsoncapkova01_eurospeech": {
   "authors": [
    [
     "Sofia",
     "Gustafson-Capkova"
    ],
    [
     "Beata",
     "Megyesi"
    ]
   ],
   "title": "A comparative study of pauses in dialogues and read speech",
   "original": "e01_0931",
   "page_count": 4,
   "order": 186,
   "p1": "931",
   "pn": "934",
   "abstract": [
    "This study aims to investigate the length, frequency and position of various types of pauses in three different speech styles: elicited spontaneous dialogues, professional reading and non-professional reading.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-181"
  },
  "takamaru01_eurospeech": {
   "authors": [
    [
     "Keiichi",
     "Takamaru"
    ],
    [
     "Makoto",
     "Hiroshige"
    ],
    [
     "Kenji",
     "Araki"
    ],
    [
     "Koji",
     "Tochinai"
    ]
   ],
   "title": "Detecting Japanese local speech rate deceleration in spontaneous conversational speech using a variable threshold",
   "original": "e01_0935",
   "page_count": 4,
   "order": 187,
   "p1": "935",
   "pn": "938",
   "abstract": [
    "The variable threshold(VT), which detects the speech rate deceleration, is proposed. The VT varies dynamically depending upon the duration of previous mora in the utterance. The VT should not change rapidly because listener cannot perceive small variations of mora duration. Thus, a set of functions with time constants which decide response speed of the VT is introduced. We apply the VT to six sentences of spontaneous conversational speech. The auditory test of detecting local speech rate deceleration is carried out for the evaluation. The possibility of detecting the local speech rate deceleration by the VT is indicated.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-182"
  },
  "petersen01_eurospeech": {
   "authors": [
    [
     "Niels Reinholt",
     "Petersen"
    ]
   ],
   "title": "Modelling fundamental frequency in first post-tonic syllables in danish sentences",
   "original": "e01_0939",
   "page_count": 4,
   "order": 188,
   "p1": "939",
   "pn": "942",
   "abstract": [
    "The work reported in the paper continues previous research on the description of Danish intonation in mathematical terms using a linear regression model. The present paper focuses on the first post-tonic syllable which constitutes the fundamental frequency peak in the stress group in Standard Danish. The results indicate that--in contradistinction to the stressed syllables--the F0 variation over the sentence in the first post-tonics is explained mainly by their position in the sentence, and to a much lesser degree by their position in the prosodic phrase.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-183"
  },
  "savino01_eurospeech": {
   "authors": [
    [
     "Michelina",
     "Savino"
    ]
   ],
   "title": "Non-finality and pre-finality in bari Italian intonation: a preliminary account",
   "original": "e01_0943",
   "page_count": 4,
   "order": 189,
   "p1": "943",
   "pn": "946",
   "abstract": [
    "In this paper, a preliminary account of intonational strategies used by Bari Italian speakers in signal non-finality and pre-finality in discourse organisation is presented and discussed. Results obtained from auditory and instrumental analysis of speech material elicited with different methods (Map Task dialogues and monologues, lists readings) show that a rich inventory of intonational choices is available to Bari Italian speakers for conveying subordination relationships within a sequence of information (in a route describing task, this is normally a sequence of instructions and/or explanations), but also for signalling in advance the end of the sequence. Moreover, these results represent a further contribution to the development of an autosegmental-metrical account of the Bari Italian intonation system.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-184"
  },
  "mixdorff01b_eurospeech": {
   "authors": [
    [
     "Hansjörg",
     "Mixdorff"
    ],
    [
     "Oliver",
     "Jokisch"
    ]
   ],
   "title": "Building an integrated prosodic model of German",
   "original": "e01_0947",
   "page_count": 4,
   "order": 190,
   "p1": "947",
   "pn": "950",
   "abstract": [
    "The intellegibility and naturalness of synthetic speech strongly depends on its prosodic quality. Departing from works by Mixdorff on a linguistically motivated model of German intonation based on the Fujisaki model, the current paper presents statistical results concerning the relationship between linguistic and phonetic information underlying an utterance and its prosodic features. Statistical analysis yields, inter alia, the following pairs of strongest single factor - prosodic feature: boundary depths (right) - syllable duration; boundary depths (left) -phrase command magnitude Ap; accent type (intoneme) - accent command amplitude Aa. These results were employed for training an FFNN-based integrated prosodic model predicting syllable durations along with syllable-aligned Fujisaki control parameters. Correlations between trained and predicted parameters suggest synergy effects, as they are mostly higher than correlations yielded when predicting parameters individually from the same set of input features using a regression model. Informal listening tests with resynthesis examples showed encouraging results.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-185"
  },
  "ibrahim01_eurospeech": {
   "authors": [
    [
     "Omar A. G.",
     "Ibrahim"
    ],
    [
     "S.H.",
     "El-Ramly"
    ],
    [
     "N.S.",
     "Abdel-Kader"
    ]
   ],
   "title": "A model of F0 contour for arabic affirmative and interrogative sentences",
   "original": "e01_0951",
   "page_count": 4,
   "order": 191,
   "p1": "951",
   "pn": "954",
   "abstract": [
    "This Paper presents the results of analyzing the global contour of the fundamental frequency (F0) for Arabic sentences and developing a model that represents it. The work concentrated on analyzing only affirmative and interrogative isolated read-loud sentences. The work is divided into two parts: 1) Extracting the common characteristics and differences between the F0 plots of affirmative and interrogative Arabic sentences, and 2) Analyzing the effect of change in sentences length on the characteristics and differences extracted in the first part of the study. The model obtained can be easily implemented in speech synthesizers to improve its intonation.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-186"
  },
  "smith01_eurospeech": {
   "authors": [
    [
     "Caroline L.",
     "Smith"
    ],
    [
     "Lisa A.",
     "Hogan"
    ]
   ],
   "title": "Variation in final lengthening as a function of topic structure",
   "original": "e01_0955",
   "page_count": 4,
   "order": 192,
   "p1": "955",
   "pn": "958",
   "abstract": [
    "This experiment shows that for an English speaker reading aloud, the topic structure of the text affects the amount of lengthening in sentence-final words. The speaker lengthened words less at the end of sentences that were followed by another sentence elaborating on the topic of the first, than at the end of sentences where the subsequent sentence added new information or switched topics. These results show that speech durations are affected by larger-scale linguistic organization, in addition to the well-known effects of local and phrasal structure. Modeling variation at the text or discourse level could improve the comprehensibility of longer passages of synthesized text.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-187"
  },
  "herwijnen01b_eurospeech": {
   "authors": [
    [
     "Olga van",
     "Herwijnen"
    ],
    [
     "Jacques",
     "Terken"
    ]
   ],
   "title": "Do speakers realize the prosodic structure they say they do?",
   "original": "e01_0959",
   "page_count": 4,
   "order": 193,
   "p1": "959",
   "pn": "962",
   "abstract": [
    "In this paper we describe a study in which a comparison was made between prosodic structures as realized in a spoken version of a text and as assigned by annotators of this text on paper. The prosodic structures were assigned by experts. This study puts to test the strategy of annotating text on paper to obtain a HUMAN reference of the prosodic structure that would be assigned when reading text aloud. This strategy is less time consuming than the often used analysis of spoken versions to obtain the assigned prosodic structure. The results of the comparison described here show that speakers are fairly capable of predicting what prosodic structure they would assign when reading text aloud.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-188"
  },
  "tabain01_eurospeech": {
   "authors": [
    [
     "Marija",
     "Tabain"
    ],
    [
     "Guillaume",
     "Rolland"
    ],
    [
     "Christophe",
     "Savariaux"
    ]
   ],
   "title": "Coarticulatory effects at prosodic boundaries: some acoustic results",
   "original": "e01_0963",
   "page_count": 4,
   "order": 194,
   "p1": "963",
   "pn": "966",
   "abstract": [
    "Acoustic data are presented from a prosodic database containing data from 3 French speakers. The prosodic boundaries examined are the Utterance, the Intonational Phrase, the Accentual Phrase, and the Word. The aim is to study the interaction of coarticulatory effects with prosodic effects. The vowel /a/ before the prosodic boundary and the consonants /b d g f s S/ after the prosodic boundary are examined. It is found that the vowel duration is greatly affected by the strength of the prosodic boundary, but consonant duration less so. The duration of the fricative consonants is more stable than the stop consonants. Formant values suggest that /a/ is lower and more back the stronger the prosodic boundary, and that the vowel is more likely to reach its low target following a bilabial consonant /b f/. Based on an examination of formant values, the velar stop /g/ appears to have much variability in the frontback dimension. Finally, there is a strong negative correlation between duration and mean velocity of the formant transition, and this effect is related to the prosodic boundary.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-189"
  },
  "barbosa01_eurospeech": {
   "authors": [
    [
     "Plínio A.",
     "Barbosa"
    ]
   ],
   "title": "Generating duration from a cognitively plausible model of rhythm production",
   "original": "e01_0967",
   "page_count": 4,
   "order": 195,
   "p1": "967",
   "pn": "970",
   "abstract": [
    "A dynamical model of rhythm production is presented. The model is meant to generate segmental duration from the interplay between a dynamical rhythmic system and a gestural score representation. The rhythmic level is being implemented by a coupled-oscillator system (composed by a syllabic and a phrase stress oscillator) that delivers V-to- V-size beats to the gestural score. The model is able to automatically generate segment and pause acoustic duration according to speech rate input. The coupling of both oscillators as well as the interaction between the rhythmic system and a linguistic description of sentences is achieved by a recurrent neural network. The network delivers syllable-size normalized durations, which are then statistically distributed among the segments. The model exhibits cognitively plausible language universal and language-specific phonetic properties that are in complete disagreement with output-oriented techniques of speech generation which do not take into account the underlying speech production mechanism.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-190"
  },
  "stuttle01_eurospeech": {
   "authors": [
    [
     "M. N.",
     "Stuttle"
    ],
    [
     "M. J. F.",
     "Gales"
    ]
   ],
   "title": "A mixture of Gaussians front end for speech recognition",
   "original": "e01_0675",
   "page_count": 4,
   "order": 196,
   "p1": "675",
   "pn": "678",
   "abstract": [
    "This paper describes a feature extraction technique based on fitting a Gaussian mixture model (GMM) to the speech spectral envelope. The features obtained (the component means, variances and priors) represent both the the general shape of the spectrum and provide information on the position of the spectral peaks. As the features select peaks in the spectrum they are related to the formant amplitudes, locations and bandwidths. Results using the Resource Management corpus, a medium vocabulary task are presented. Although by themselves the GMM features do not outperform MFCC features, systems combining the GMM systems with a standard frontend are shown to give a reduction in word error rate.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-191"
  },
  "zheng01b_eurospeech": {
   "authors": [
    [
     "Jing",
     "Zheng"
    ],
    [
     "John",
     "Butzberger"
    ],
    [
     "Horacio",
     "Franco"
    ],
    [
     "Andreas",
     "Stolcke"
    ]
   ],
   "title": "Improved maximum mutual information estimation training of continuous density HMMs",
   "original": "e01_0679",
   "page_count": 4,
   "order": 197,
   "p1": "679",
   "pn": "682",
   "abstract": [
    "In maximum mutual information estimation (MMIE) training, the currently widely used update equations derive from the Extended Baum-Welch (EBW) algorithm, which was originally designed for the discrete hidden Markov model (HMM) and was extended to continuous Gaussian density HMMs through approximations. We derive a new set of equations for MMIE based on a quasi-Newton algorithm, without relying on EBW. We find that by adopting a generalized form of the MMIE criterion, the H-criterion, convergence speed and recognition performance can be improved. The proposed approach has been applied to a spelled-word recognition task leading to a 21.6% relative letter error rate reduction with respect to the standard Maximum Likelihood Estimation (MLE) training method, and showing advantages over the conventional MMIE approach in terms of both training speed and recognition accuracy.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-192"
  },
  "perronnin01_eurospeech": {
   "authors": [
    [
     "Florent",
     "Perronnin"
    ],
    [
     "Roland",
     "Kuhn"
    ],
    [
     "Patrick",
     "Nguyen"
    ],
    [
     "Jean-Claude",
     "Junqua"
    ]
   ],
   "title": "Maximum-likelihood training of a bipartite acoustic model for speech recognition",
   "original": "e01_0683",
   "page_count": 4,
   "order": 198,
   "p1": "683",
   "pn": "686",
   "abstract": [
    "This paper describes a context-dependent model that supports extremely rapid speaker adaptation. The model, called \"Eigencentroid plus Delta Trees\" (EDT), incorporates prior knowledge about speaker space and has modest memory requirements. The paper gives the formulae for training EDT models and performs a detailed entropy analysis to show how EDT and speaker-independent models trained on experimental data differ from each other. Phoneme recognition results on the TIMIT database are also given. EDT yields 12.1% relative error rate reduction (ERR) for supervised adaptation on three sentences, 11.2% ERR for unsupervised adaptation on three sentences, and 10.4% ERR for self-adaptation on a single sentence.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-193"
  },
  "sarikaya01_eurospeech": {
   "authors": [
    [
     "Ruhi",
     "Sarikaya"
    ],
    [
     "John H. L.",
     "Hansen"
    ]
   ],
   "title": "Analysis of the root-cepstrum for acoustic modeling and fast decoding in speech recognition",
   "original": "e01_0687",
   "page_count": 4,
   "order": 199,
   "p1": "687",
   "pn": "690",
   "abstract": [
    "Root-cepstral analysis has been proposed previously for speech recognition in car environments. In this paper, we focus on an alternative aspect of Root-cepstrum as it applies to discriminative acoustic modeling and fast speech recognizer decoding. We compare Rootcepstrum to Mel-Frequency cepstrum Coefficients (MFCC) in terms of their noise immunity during modeling and decoding speed. Our experiments use the SPINE~cite{HAN00} corpus which is composed of clean and noisy data with a 5K vocabulary size. Experiments were performed that allow pair-wise comparisons of acoustic models across different feature sets and acoustic units. We observed that for 84% of the phonemes, the average distance to all other acoustic units is increased in the Root-cepstrum domain compared to MFCC resulting in a sharp acoustic model set. Therefore, the ambiguity in the Root-cepstrum space is reduced. Large vocabulary noisy speech recognition experiments showed a 27.5% reduction in real--time processing factor (RTF) compared to MFCC features while improving overall recognition accuracy.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-194"
  },
  "eide01_eurospeech": {
   "authors": [
    [
     "Ellen",
     "Eide"
    ]
   ],
   "title": "Distinctive features for use in an automatic speech recognition system",
   "original": "e01_1613",
   "page_count": 4,
   "order": 200,
   "p1": "1613",
   "pn": "1616",
   "abstract": [
    "In this paper we develop a method of representing the speech waveform in terms of a set of abstract, linguistic distinctions in order to derive a set of discriminative features for use in a speech recognizer. By combining the distinctive feature representation with our original waveform representation we are able to achieve a reduction in word error rate of 33 percent on an automatic speech recognition task.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-195"
  },
  "zhang01_eurospeech": {
   "authors": [
    [
     "Jiyong",
     "Zhang"
    ],
    [
     "Fang",
     "Zheng"
    ],
    [
     "Jing",
     "Li"
    ],
    [
     "Chunhua",
     "Luo"
    ],
    [
     "Guoliang",
     "Zhang"
    ]
   ],
   "title": "Improved context-dependent acoustic modeling for continuous Chinese speech recognition",
   "original": "e01_1617",
   "page_count": 4,
   "order": 201,
   "p1": "1617",
   "pn": "1620",
   "abstract": [
    "This paper describes the new framework of context-dependent (CD) Initial/Final (IF) acoustic modeling using the decision tree based state tying for continuous Chinese speech recognition. The Extended Initial/Final (XIF) set is chosen as the basic speech recognition unit (SRU) set according to the Chinese language characteristics, which outperforms the standard IF set. An adaptive mixture increasing strategy is applied when splitting the single Gaussian into mixed Gaussians in each tied state after the decision tree has been constructed. Our experimental results show that these two improvements are helpful to the acoustic modeling of Chinese speech recognition and that the CD XIF model outperforms the baseline syllable model over 30%.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-196"
  },
  "duchateau01_eurospeech": {
   "authors": [
    [
     "Jacques",
     "Duchateau"
    ],
    [
     "Kris",
     "Demuynck"
    ],
    [
     "Dirk Van",
     "Compernolle"
    ],
    [
     "Patrick",
     "Wambacq"
    ]
   ],
   "title": "Class definition in discriminant feature analysis",
   "original": "e01_1621",
   "page_count": 4,
   "order": 202,
   "p1": "1621",
   "pn": "1624",
   "abstract": [
    "The aim of discriminant feature analysis techniques in the signal processing of speech recognition systems is to find a feature vector transformation which maps a high dimensional input vector onto a low dimensional vector while retaining a maximum amount of information in the feature vector to discriminate between predefined classes. This paper points out the significance of the definition of the classes in the discriminant feature analysis technique. Three choices for the definition of the classes are investigated: the phonemes, the states in context independent acoustic models and the tied states in context dependent acoustic models. These choices for the classes were applied to (1) standard LDA (linear discriminant analysis) for reference and to (2) MIDA, an improved, mutual information based discriminant analysis technique. Evaluation of the resulting linear feature transforms on a large vocabulary continuous speech recognition task shows, depending on the technique, the best choice for the classes.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-197"
  },
  "segura01b_eurospeech": {
   "authors": [
    [
     "Jose C.",
     "Segura"
    ],
    [
     "M. Carmen",
     "Benitez"
    ],
    [
     "Angel de la",
     "Torre"
    ],
    [
     "Antonio J.",
     "Rubio"
    ]
   ],
   "title": "Feature extraction from time-frequency matrices for robust speech recognition",
   "original": "e01_1625",
   "page_count": 4,
   "order": 203,
   "p1": "1625",
   "pn": "1628",
   "abstract": [
    "In this paper we present a study about time-frequency distribution of acoustic-phonetic information for the Spanish language. This is based on a large Spanish database automatically labeled, and we conclude that results are similar to those obtained for hand-labeled English databases. We use bidimensional LDA to extract discriminant features in timefrequency domain (TF) that are more robust in noise than the standard ones based on MFCC and time derivatives. We show that TF domain and its corresponding transformed domain (CTM) are equivalent from the point of view of LDA analysis and use this fact to reduce the dimensionality of the problem. Finally, cascade unidimensional LDA (CLDA) is applied first in frequency and then in time. This gives better estimates of projection vectors and better recognition performance. The proposed techniques are evaluated in a connected digit recognition task. Utterances have been artificially corrupted with additive real noises.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-198"
  },
  "peng01_eurospeech": {
   "authors": [
    [
     "Yu",
     "Peng"
    ],
    [
     "Wang",
     "Zuoying"
    ]
   ],
   "title": "Using spatial correlation information in speech recognition",
   "original": "e01_1629",
   "page_count": 4,
   "order": 204,
   "p1": "1629",
   "pn": "1632",
   "abstract": [
    "Acoustic model training is very important in speech recognition. But in traditional training algorithm, we take each state separately, and the relationship between different states is not considered. In this paper we bring forward a novel idea of using the correlation information between states, which is called \"spatial correlation\". We describe this correlation information as linear constraints. According to phonetic knowledge, we firstly divide states into small groups named ~{!0~}correlation sub-space~{!1~}. In every sub-space, we use eigen value decomposition to get linear constraints. The constraints are then used in a new training algorithm. Experiments of the new training algorithm show significant improvement over traditional training algorithm.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-199"
  },
  "bauer01_eurospeech": {
   "authors": [
    [
     "Josef G.",
     "Bauer"
    ]
   ],
   "title": "On the choice of classes in MCE based discriminative HMM-training for speech recognizers used in the telephone environment",
   "original": "e01_1633",
   "page_count": 4,
   "order": 205,
   "p1": "1633",
   "pn": "1636",
   "abstract": [
    "One of the most commonly used discriminative approaches in parameter estimation for Hidden Markov Models is the Minimum Classification Error (MCE) method . This paper studies possible choices for the classes (i.e. basic speech units) in MCE training and their application for several tasks suitable for speech driven dialog systems in the telephone environment. The considered choices of classes are HMM states, phonemes, words and sequences of words. The theoretical suitability and practical considerations for the different criteria are discussed. Using the different training criteria consistent experimental results are given for four tasks: non-task-specific training, training for small vocabulary isolated word recognition, training for connected digit recognition and for letter recognition. In all experiments not only the objective of the optimization but also the resulting word recognition performance is investigated. It shows that for the given setup only word and word string based criteria are capable to reduce the word error rate.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-200"
  },
  "keshet01_eurospeech": {
   "authors": [
    [
     "Joseph",
     "Keshet"
    ],
    [
     "Dan",
     "Chazan"
    ],
    [
     "Ben-Zion",
     "Bobrovsky"
    ]
   ],
   "title": "Plosive spotting with margin classifiers",
   "original": "e01_1637",
   "page_count": 4,
   "order": 206,
   "p1": "1637",
   "pn": "1640",
   "abstract": [
    "This paper presents a novel algorithm for precise spotting of plosives. The algorithm is based on a pattern matching technique implemented with margin classifiers, such as support vector machines (SVM). A special hierarchical treatment to overcome the problem of fricative and false silence detection is presented. It uses the loss-based multi-class decisions. Furthermore, a method for smoothing the overall decisions by sequential linear programming is described. The proposed algorithm was tested on the TIMIT corpus, which produced a very high spotting accuracy. The algorithm presented here is applied to plosives detection, but can easily be adapted to any class of phonemes.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-201"
  },
  "brugnara01_eurospeech": {
   "authors": [
    [
     "Fabio",
     "Brugnara"
    ]
   ],
   "title": "Model agglomeration for context-dependent acoustic modeling",
   "original": "e01_1641",
   "page_count": 4,
   "order": 207,
   "p1": "1641",
   "pn": "1644",
   "abstract": [
    "This work describes a method for generating back-off models for context-dependent unit modeling. The main characteristic of the approach is that of building generic models by gathering statistics of detailed models, collected during Baum-Welch reestimation. The construction of back-off models does not require additional processing of the training data, allowing to quickly build different models sets with different back-off criteria starting from the same set of trained models and their statistics. Experiments are reported on the TIMIT and Wall Street Journal corpora, that show the consistency of the approach and compare it with state tying based on Phonetic Decision Trees.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-202"
  },
  "levit01_eurospeech": {
   "authors": [
    [
     "M.",
     "Levit"
    ],
    [
     "A. L.",
     "Gorin"
    ],
    [
     "J. H.",
     "Wright"
    ]
   ],
   "title": "Multipass algorithm for acquisition of salient acoustic morphemes",
   "original": "e01_1645",
   "page_count": 4,
   "order": 208,
   "p1": "1645",
   "pn": "1648",
   "abstract": [
    "We are interested in spoken language understanding within the domain of automated telecommunication services. Our current methodology involves training statistical language models from large annotated corpora for recognition and understanding. Since the transcribing of large speech corpora is a resource consuming task, we are motivated to exploit speech without transcriptions. In particular, we learn the semantic associations for a task exploiting only phone-based sequences from the output of a task-independent ASR-system. In this paper we present a new multipass algorithm for acquiring salient phone sequences from untranscribed speech corpora and evaluate their utility for the HMIHY task. Compared to our previous strategy, this algorithm is shown to produce improved call-classification results while reducing up to 7-fold the number of salient phone-sequences selected for training.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-203"
  },
  "emori01_eurospeech": {
   "authors": [
    [
     "Tadashi",
     "Emori"
    ],
    [
     "Koichi",
     "Shinoda"
    ]
   ],
   "title": "Rapid vocal tract length normalization using maximum likelihood estimation",
   "original": "e01_1649",
   "page_count": 4,
   "order": 209,
   "p1": "1649",
   "pn": "1652",
   "abstract": [
    "Recently, vocal tract length normalization (VTLN) techniques have been developed for speaker normalization in speech recognition. This paper proposes a new VTLN method, in which the vocal tract length is normalized in the cepstrum space by means of linear mapping whose parameter is derived using maximum-likelihood estimation. The computational costs of this method are much lower than that of such conventional methods as ML-VTLN, in which the parameter for mapping is selected from among several parameters. Further, the new method offers greater precision in determining parameters for individual speakers. Experimental use of the method resulted in an error reduction rate of 7.1%. A combination of the proposed method with cepstrum mean normalization (CMN) method was also examined and found to reduce the error rate even more, by 14.6%.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-204"
  },
  "okuda01_eurospeech": {
   "authors": [
    [
     "Kozo",
     "Okuda"
    ],
    [
     "Tomoko",
     "Matsui"
    ],
    [
     "Satoshi",
     "Nakamura"
    ]
   ],
   "title": "Towards the creation of acoustic models for stressed Japanese speech",
   "original": "e01_1653",
   "page_count": 4,
   "order": 210,
   "p1": "1653",
   "pn": "1656",
   "abstract": [
    "In error recovery utterance, the user using the speech recognition system changes his or her speaking style to aid the system in recognizing the speech. However, this change leads the mismatch between the acoustic models and reduces the performance of the system. This degradation causes a serious problem of speech recognition for a dialog system or a speech translation system. In error recovery utterance in Japanese, the occurrence of syllable-stressed speech increases. In syllable-stressed speech, each syllable is uttered slowly and emphasized. The characteristics of each syllable are strongly altered by this modification and the speech recognition performance is reduced. This paper investigates how to create acoustic models robust in recognizing error recovery utterances, especially syllable-stressed speech. In this paper, we propose an acoustic modeling method for syllable-stressed speech by combining existing acoustic models. Our results indicate that the proposed method improves the system performance. Furthermore, the method does not need any expansion of the recognition dictionary or explicit model selection.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-205"
  },
  "baba01_eurospeech": {
   "authors": [
    [
     "Akira",
     "Baba"
    ],
    [
     "Shinichi",
     "Yoshizawa"
    ],
    [
     "Miichi",
     "Yamada"
    ],
    [
     "Akinobu",
     "Lee"
    ],
    [
     "Kiyohiro",
     "Shikano"
    ]
   ],
   "title": "Elderly acoustic model for large vocabulary continuous speech recognition",
   "original": "e01_1657",
   "page_count": 4,
   "order": 211,
   "p1": "1657",
   "pn": "1660",
   "abstract": [
    "In this paper, we evaluate elderly speaker acoustic models in LVCSR, which are trained by the 301 elderly speakers' database from the age of 60 to 90. Each speaker utters 200 sentences. The elderly speaker PTM (Phonetic Tied Mixture) acoustic model attains 88.9% word recognition rate, which is better than 86.0% word recognition rate by the usual adult (an average age of 28.6) PTM acoustic model. To achieve higher recognition rates, we use two types of speaker adaptation methods, which are a supervised MLLR and an unsupervised adaptation method based on the sufficient HMM statistics. In our experimental results, the elderly acoustic model is better as the adaptation baseline HMM model than the usual adult model for elderly speakers.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-206"
  },
  "zhang01b_eurospeech": {
   "authors": [
    [
     "Jin-Song",
     "Zhang"
    ],
    [
     "Shu-Wu",
     "Zhang"
    ],
    [
     "Yoshinori",
     "Sagisaka"
    ],
    [
     "Satoshi",
     "Nakamura"
    ]
   ],
   "title": "A hybrid approach to enhance task portability of acoustic models in Chinese speech recognition",
   "original": "e01_1661",
   "page_count": 4,
   "order": 212,
   "p1": "1661",
   "pn": "1664",
   "abstract": [
    "This paper presents our approach to enhance the portability of acoustic models by mitigating the phonetic mismatch arising from a new testing task which is rather different from the training data. The approach is a hybrid one which combines knowledge-based context categorization to generate a context rich set of subword units, and data-driven-based acoustic model clustering on the level of context category. Compared with the conventional approach of only phonetic decision tree based model clustering and unseen model generation, the new approach improved greatly the desired subword coverage for the new testing domain, and achieved an error rate reduction by 10.8% for Chinese character accuracy in the recognition experiments. Together with the effect of the newly adopted basic units of 9 glottal stops, we achieved a total 23.5% error rate reduction in the testing compared to the baseline system.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-207"
  },
  "rodriguez01_eurospeech": {
   "authors": [
    [
     "L. J.",
     "Rodriguez"
    ],
    [
     "I.",
     "Torres"
    ],
    [
     "A.",
     "Varona"
    ]
   ],
   "title": "Evaluation of sublexical and lexical models of acoustic disfluencies for spontaneous speech recognition in Spanish",
   "original": "e01_1665",
   "page_count": 4,
   "order": 213,
   "p1": "1665",
   "pn": "1668",
   "abstract": [
    "Spontaneous speech is full of acoustic disfluencies that rarely appear in read or laboratory speech. A very simple and straightforward approach is presented, in which acoustic disfluences are modelled by augmenting the inventory of sublexical units, which originally consisted of 23 context independent phones plus a special unit for silent pauses. This set was augmented with 12 additional units accounting for lengthenings of sounds, filled pauses and noises. Two speech databases, both in Spanish, were used in the experiments. A phonetically balanced database was used for initializing the acoustic models. A spontaneous speech database consisting of 227 dialogues was used both for training and testing purposes. Recognition rates, in terms of acoustic-phonetic accuracy and word accuracy, with and without filtering acoustic disfluencies prior to alignments, were obtained to evaluate the contribution of these models to speech recognition. Also, some specific but significant examples were explored and discussed. Experimental results showed that using explicit models of acoustic disfluencies clearly improved the performance of a spontaneous speech recognition system.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-208"
  },
  "deviren01_eurospeech": {
   "authors": [
    [
     "Murat",
     "Deviren"
    ],
    [
     "Khalid",
     "Daoudi"
    ]
   ],
   "title": "Structural learning of dynamic Bayesian networks in speech recognition",
   "original": "e01_1669",
   "page_count": 4,
   "order": 214,
   "p1": "1669",
   "pn": "1672",
   "abstract": [
    "We present a speech modeling methodolgy where no a priori assumption is made on the dependencies between the observed and hidden speech processes. Rather, dependencies are learned from data. This methodology guarantees improvement in modeling fidelity compared to HMMs. In addition, it gives the user a control on the trade-off between modeling accuracy and model complexity. Furthermore, the approach is technically very attractive because all the computational effort is made in the training phase.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-209"
  },
  "onishi01_eurospeech": {
   "authors": [
    [
     "Shigehiko",
     "Onishi"
    ],
    [
     "Hirofumi",
     "Yamamoto"
    ],
    [
     "Yoshinori",
     "Sagisaka"
    ]
   ],
   "title": "Structured language model for class identification of out-of-vocabulary words arising from multiple wordclasses",
   "original": "e01_0693",
   "page_count": 4,
   "order": 215,
   "p1": "693",
   "pn": "696",
   "abstract": [
    "A structured language model (STLM) is proposed to cope with out-of-vocabulary (OOV) words coming from multiple word-classes. The STLM aims at independently modeling the classes without interference and identifying the class of words arising from multiple word-classes. The STLM consists of the conventional word-class N-gram and the sets of the independent-trained class-specific sub-word N-grams. We made an experimental language model by using STLM for the two similar proper-noun classes and performed the speech recognition experiments. The results show that any OOV word of the one class is never misrecognized as that of the other class. The results show that the STLM could integrate the multiple different statistical language models with no interference.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-210"
  },
  "jitsuhiro01_eurospeech": {
   "authors": [
    [
     "Takatoshi",
     "Jitsuhiro"
    ],
    [
     "Hirofumi",
     "Yamamoto"
    ],
    [
     "Setsuo",
     "Yamada"
    ],
    [
     "Yoshinori",
     "Sagisaka"
    ]
   ],
   "title": "New language models using phrase structures extracted from parse trees",
   "original": "e01_0697",
   "page_count": 4,
   "order": 216,
   "p1": "697",
   "pn": "700",
   "abstract": [
    "This paper proposes a new speech recognition scheme using three linguistic constraints. Multi-class composite bigram models are used in the first and second passes to reflect word-neighboring characteristics as an extension of conventional word n-gram models. Trigram models with constituent boundary markers and word pattern models are both used in the third pass to utilize phrasal constraints and headword co-occurrences, respectively. These two models are made using a training text corpus with phrase structures given by an example-based Transfer-Driven Machine Translation (TDMT) parser. Speech recognition experiments show that the new recognition scheme reduces word errors 9.50% from the conventional scheme by using word-neighboring characteristics, that is only the multi-class composite bigram models.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-211"
  },
  "siciliagarcia01_eurospeech": {
   "authors": [
    [
     "E. I.",
     "Sicilia-Garcia"
    ],
    [
     "Ji",
     "Ming"
    ],
    [
     "F. J.",
     "Smith"
    ]
   ],
   "title": "Triggering individual word domains in n-gram language models",
   "original": "e01_0701",
   "page_count": 4,
   "order": 217,
   "p1": "701",
   "pn": "704",
   "abstract": [
    "We present a new method of introducing domain knowledge into an n-gram language model. It is based on a combination of language models for individual word domains. Each word model is built from an individual corpus which is formed by extracting those subsets of the entire training corpus which contain that significant word. When testing, significant words are extracted from a cache and their models are combined with a global language model. Different methods of combining the models are described; one simple method based on combining frequencies rather than probabilities gives promising results and provides a relatively simple method of introducing domain information into an n-gram language model. A 20% reduction in language model perplexity over the standard 3-gram approach is obtained which is similar to results obtained with other more complex domain models. The model also requires a small cache compared with other models requiring a cache.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-212"
  },
  "akiba01_eurospeech": {
   "authors": [
    [
     "Tomoyosi",
     "Akiba"
    ],
    [
     "Katunobu",
     "Itou"
    ]
   ],
   "title": "A structured statistical language model conditioned by arbitrarily abstracted grammatical categories based on GLR parsing",
   "original": "e01_0705",
   "page_count": 4,
   "order": 218,
   "p1": "705",
   "pn": "708",
   "abstract": [
    "This paper presents a new statistical language model for speech recognition, based on Generalized LR parsing. The proposed model, the Abstracted Probabilistic GLR (APGLR) model, is an extension of the existing structured language model known as the Probabilistic GLR (PGLR) model. It can predict next words from arbitrarily abstracted categories. The APGLR model is also a generalization of the original PGLR model, because PGLR can be considered to be a special case of APGLRs that predict the next words from the least abstracted grammatical categories, namely the terminal symbols. The selection of the abstraction level is arbitrary; we show several strategies to define the level. The experimental results show that the proposed model performs better than the original PGLR model for speech recognition.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-213"
  },
  "matsui01_eurospeech": {
   "authors": [
    [
     "Atsushi",
     "Matsui"
    ],
    [
     "Hiroyuki",
     "Segi"
    ],
    [
     "Akio",
     "Kobayashi"
    ],
    [
     "Toru",
     "Imai"
    ],
    [
     "Akio",
     "Ando"
    ]
   ],
   "title": "Speech recognition of broadcast sports news",
   "original": "e01_0709",
   "page_count": 4,
   "order": 219,
   "p1": "709",
   "pn": "712",
   "abstract": [
    "This paper shows that a domain-dependent language model and states-kipped HMMs can achieve improvements in word recognition accuracy on a broadcast sports news transcription task. Although a domain-dependent language model is much better than a general model in terms of word error rate, the smaller training corpus for a special topic relative to the general news corpus leads to problems especially in higher-order n-gram probability estimation. In this paper, we tried a linear interpolation technique to smooth out unreliable higher-order n-gram probabilities using more reliable lower-order n-gram probabilities. We also applied a language model adaptation technique by using news manuscripts on sports topics. For acoustic modeling, we added two state-skipping paths to three-state HMMs to deal with phonemes of duration less than three frames. Overall, we reduced the word error rate from 15.1% to 5.8%, and achieved sufficient performance to realize real-time subtitling services.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-214"
  },
  "mori01_eurospeech": {
   "authors": [
    [
     "Shinsuke",
     "Mori"
    ],
    [
     "Masafumi",
     "Nishimura"
    ],
    [
     "Nobuyasu",
     "Itoh"
    ]
   ],
   "title": "Improvement of a structured language model: arbori-context tree",
   "original": "e01_0713",
   "page_count": 4,
   "order": 220,
   "p1": "713",
   "pn": "716",
   "abstract": [
    "In this paper we present an extention of a context tree for a structured language model (SLM), which we call an arbori-context tree. The state-of-the-art SLM predicts the next word from a fixed partial tree of the history tree, such as two exposed heads, etc. An arbori-context tree allows us to select an optimum partial tree of a history tree for the next word prediction depending on the effectiveness in the similar way that a context tree selects the length of the history (n of n-gram). The experiment we conducted showed that the test set perplexity of the SLM based on an arbori-context tree (79.98) was lower than that of the SLM with a fixed history (101.56).\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-215"
  },
  "kim01c_eurospeech": {
   "authors": [
    [
     "Woosung",
     "Kim"
    ],
    [
     "Sanjeev",
     "Khudanpur"
    ],
    [
     "Jun",
     "Wu"
    ]
   ],
   "title": "Smoothing issues in the structured language model",
   "original": "e01_0717",
   "page_count": 4,
   "order": 221,
   "p1": "717",
   "pn": "720",
   "abstract": [
    "The Structured Language Model (SLM) recently introduced by Chelba and Jelinek is a powerful general formalism for exploiting syntactic dependencies in a left-to-right language model for applications such as speech and handwriting recognition, spelling correction, machine translation, etc. Unlike traditional N-gram models, optimal smoothing techniques -- discounting methods and hierarchical structures for back-off -- are still being developed for the SLM. In the SLM, the statistical dependencies of a word on immediately preceding words, preceding syntactic heads, non-terminal labels, etc., are parameterized as overlapping N-gram dependencies. Statistical dependencies in the parser and tagger used by the SLM also have N-gram like structure. Deleted interpolation has been used to combine these N-gram like models. We demonstrate on two different corpora -- WSJ and Switchboard -- that more recent modified back-off strategies and nonlinear interpolation methods considerably lower the perplexity of the SLM. Improvement in word error rate is also demonstrated on the Switchboard corpus.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-216"
  },
  "shen01b_eurospeech": {
   "authors": [
    [
     "Xipeng",
     "Shen"
    ],
    [
     "Bo",
     "Xu"
    ]
   ],
   "title": "The study of the effect of training set on statistical language modeling",
   "original": "e01_0721",
   "page_count": 4,
   "order": 222,
   "p1": "721",
   "pn": "724",
   "abstract": [
    "In this work, we make a study on the effect of training set on statistical language modeling (SLM). A corpus selection system based on perplexity is presented. It is tested in two experiments: one is to select optimal training corpus for generating a domain-specific SLM; the other one is for generating an optimal SLM for a LVCSR system. The results show that the training corpus is important for the capability of SLM and our corpus selection system is powerful for optimal corpus selection. With the help of this system, we generated a SLM for a LVCSR system, which contributed 14.5%--17.7% relative character error reduction.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-217"
  },
  "esteve01_eurospeech": {
   "authors": [
    [
     "Yannick",
     "Esteve"
    ],
    [
     "Frédéric",
     "Bechet"
    ],
    [
     "Alexis",
     "Nasr"
    ],
    [
     "Renato De",
     "Mori"
    ]
   ],
   "title": "Stochastic finite state automata language model triggered by dialogue states",
   "original": "e01_0725",
   "page_count": 4,
   "order": 223,
   "p1": "725",
   "pn": "728",
   "abstract": [
    "Within the framework of Natural Spoken Dialogue systems, this paper describes a method for dynamically adapting a Language Model (LM) to the dialogue states detected. This LM combines a standard n-gram model with Stochastic Finite State Automata (SFSAs). During the training process, the sentence corpus used to train the LM is split into several hierarchical clusters in a 2-step process which involves both explicit knowledge and statistical criteria. From the same sentence corpus, SFSAs are extracted in order to model longer contexts than the ones used in the standard n-gram model. A first decoding process calculates a word-graph as well as a first sentence hypothesis. This first hypothesis will be used to find the optimal sub-LM. Then, a rescoring process of the word graph using this LM is performed. By adapting the LM to the dialogue state detected, we show a statistically significant gain in WER on a dialogue corpus collected by France Telecom R&D.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-218"
  },
  "rayner01_eurospeech": {
   "authors": [
    [
     "Manny",
     "Rayner"
    ],
    [
     "John",
     "Dowding"
    ],
    [
     "Beth Ann",
     "Hockey"
    ]
   ],
   "title": "A baseline method for compiling typed unification grammars into context free language models",
   "original": "e01_0729",
   "page_count": 4,
   "order": 224,
   "p1": "729",
   "pn": "732",
   "abstract": [
    "This paper presents a minimal enumerative approach to the problem of compiling typed unification grammars into CFG language models, a prototype implementation and results of experiments in which it was used to compile some non-trivial unification grammars. We argue that enumerative methods are considerably more useful than has been previously believed. Also, the simplicity of enumerative methods makes them a natural baseline against which to compare alternative approaches.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-219"
  },
  "whittaker01b_eurospeech": {
   "authors": [
    [
     "E. W. D.",
     "Whittaker"
    ],
    [
     "Bhiksha",
     "Raj"
    ]
   ],
   "title": "Comparison of width-wise and length-wise language model compression",
   "original": "e01_0733",
   "page_count": 4,
   "order": 225,
   "p1": "733",
   "pn": "736",
   "abstract": [
    "In this paper we investigate the extent to which Katz back-off language models can be compressed through a combination of parameter quantization (width-wise compression) and parameter pruning (lengthwise compression) methods while preserving performance. We compare the compression and performance that is achieved using entropy-based pruning against that achieved using only parameter quantization. We then compare combinations of both methods. It is shown that a broadcast news language model can be compressed by up to 83% to only 12.6Mb with no loss in performance on a broadcast news task. Compressing the language model further by quantization to 10.3Mb resulted in only a 0.4% degradation in word error rate which is better than can be achieved through entropy-based pruning alone.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-220"
  },
  "siivola01_eurospeech": {
   "authors": [
    [
     "Vesa",
     "Siivola"
    ],
    [
     "Mikko",
     "Kurimo"
    ],
    [
     "Krista",
     "Lagus"
    ]
   ],
   "title": "Large vocabulary statistical language modeling for continuous speech recognition in finnish",
   "original": "e01_0737",
   "page_count": 4,
   "order": 226,
   "p1": "737",
   "pn": "740",
   "abstract": [
    "Statistical language modeling (SLM) is an essential part in any large-vocabulary continuous speech recognition (LVCSR) system. The development of the standard SLM methods has been strongly affected by the goals of LVCSR in English. The structure of Finnish is substantially different from English, so if the standard SLMs are directly applied, the success is by no means granted. In this paper we describe our first attempts of building a LVCSR for Finnish and the new SLMs that we have tried. One of our objective has been the indexing and recognition of broadcast news, so special issues of our interest are topic detection, word stemming and modeling words that are poorly covered in the training data. Our new methods are based on neural computing using the self-organizing map (SOM) which has recently been shown to successfully extract and approximate latent semantic structures from massive text collections.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-221"
  },
  "lopezcozar01_eurospeech": {
   "authors": [
    [
     "R.",
     "López-Cózar"
    ],
    [
     "D. H.",
     "Milone"
    ]
   ],
   "title": "A new technique based on augmented language models to improve the performance of spoken dialogue systems",
   "original": "e01_0741",
   "page_count": 4,
   "order": 227,
   "p1": "741",
   "pn": "744",
   "abstract": [
    "This paper presents a new technique that aims to improve the performance of spoken dialogue systems by using the so-called augmented language models. We define an augmented language model as a compound of a language model and a set of values concerning parameters that can influence the speech recognition when the language model is used. The diverse language models used by a dialogue system can be very different, in terms of perplexity for example. Then, the aim of the technique is to find and use the combination of values concerning the different parameters that leads to the best recognition results when the different language models are used by a dialogue system. The technique has been applied to a dialogue system for the fast food domain. The results show that when the augmented language models are used the system's performance is enhanced. In the experiments we have achieved a reduction of 9,33% in the word error rate and an increment of 11,26% in the sentence understanding.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-222"
  },
  "takagi01_eurospeech": {
   "authors": [
    [
     "Kazuyuki",
     "Takagi"
    ],
    [
     "Kazuhiko",
     "Ozeki"
    ]
   ],
   "title": "Pause information for dependency analysis of read Japanese sentences",
   "original": "e01_1041",
   "page_count": 4,
   "order": 228,
   "p1": "1041",
   "pn": "1044",
   "abstract": [
    "The work presented in this paper is devoted to the modeling of distribution of post-phrase pause duration for dependency analysis of read Japanese sentences. The pause information is incorporated into a dependency structure parser, which handles numerical information as part of linguistic knowledge. A series of our previous works has shown that the duration of pauses is constantly effective to improve the parsing accuracy among other prosodic features. This paper aims to improve the parsing accuracy by reforming pause distribution functions to better fit the actual distribution of pause duration. First, the pause distribution is divided into two parts, each of which is represented by a separate model: one by a discrete probability model, and the other by a bimodal p.d.f. Secondly, pause duration is calculated in log scale. All the experiments show that these modifications improve parsing accuracy.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-223"
  },
  "chen01e_eurospeech": {
   "authors": [
    [
     "Berlin",
     "Chen"
    ],
    [
     "Hsin-min",
     "Wang"
    ],
    [
     "Lin-shan",
     "Lee"
    ]
   ],
   "title": "An HMM/n-gram-based linguistic processing approach for Mandarin spoken document retrieval",
   "original": "e01_1045",
   "page_count": 4,
   "order": 229,
   "p1": "1045",
   "pn": "1048",
   "abstract": [
    "In this paper an HMM/N-gram-based linguistic processing approach for Mandarin spoken document retrieval is presented. The underlying characteristics and different structures of this approach were extensively investigated. The retrieval capabilities were verified by tests with indexing features of word- and syllable(subword)-levels and comparison with the conventional vector space model approach. To further improve the discrimination capabilities of the HMMs, both the expectation-maximization (EM) and minimum classification error (MCE) training algorithms were introduced in training. The information fusion of indexing features of word- and syllable-levels was also investigated. The spoken document retrieval experiments were performed on the Topic Detection and Tracking Corpora (TDT-2 and TDT-3). Very encouraging retrieval performance was obtained.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-224"
  },
  "lin01b_eurospeech": {
   "authors": [
    [
     "Yi-Chung",
     "Lin"
    ],
    [
     "Huei-Ming",
     "Wang"
    ]
   ],
   "title": "Probabilistic concept verification for language understanding in spoken dialogue systems",
   "original": "e01_1049",
   "page_count": 4,
   "order": 230,
   "p1": "1049",
   "pn": "1052",
   "abstract": [
    "In the past researches, several kinds of information have been explored to assess the confidence measure or to select the confidence tag for a word/phrase. However, the contextual confidence information is little touched. In this paper, we propose a concept-based probabilistic verification model to integrate the contextual confidence information. In this model, a concept is verified not only according to its acoustic confidence measure but also according to neighboring concepts and their confidence levels. Experimental results show that the proposed model significantly outperforms the model using only confidence measures. The error rate of confidence tag is reduced from 17.7% to 15.12%, which corresponds to an error reduction rate of 14.5%.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-225"
  },
  "kulekcy01_eurospeech": {
   "authors": [
    [
     "M. Oguzhan",
     "Külekcý"
    ],
    [
     "Mehmed",
     "Özkan"
    ]
   ],
   "title": "Turkish word segmentation using morphological analyzer",
   "original": "e01_1053",
   "page_count": 4,
   "order": 231,
   "p1": "1053",
   "pn": "1056",
   "abstract": [
    "This paper describes an algorithm to segment an input Turkish string without any spaces, which may be an output of a speech-to-text application, into words by using morphological analyser. It is quite possible to use the algorithm on other languages, which has a morphological analysis component, as well. Turkish morphological analyser is designed and implemented as the linguistic engine of the algorithm. The construction of the analyser proposes a technique that attempts to achieve group vise morpheme recognition instead of searching suffixes one by one in a word.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-226"
  },
  "tarsaku01_eurospeech": {
   "authors": [
    [
     "Pongthai",
     "Tarsaku"
    ],
    [
     "Virach",
     "Sornlertlamvanich"
    ],
    [
     "Rachod",
     "Thongprasirt"
    ]
   ],
   "title": "Thai grapheme-to-phoneme using probabilistic GLR parser",
   "original": "e01_1057",
   "page_count": 4,
   "order": 232,
   "p1": "1057",
   "pn": "1060",
   "abstract": [
    "Many difficulties in the Thai language such as the absence of boundary word, linking syllables in pronunciation, and homographs are challenging us in developing a Thai Grapheme-to-Phoneme (G2P) converter. Presently there are a couple Thai G2P systems which are proposed in ruled-based and decision-tree approach. The rule-based approach has a drawback in the limitation of employing the context. The decision-tree approach is somehow able to capture the local context for making the decision. On the contrary, the Probabilistic Generalized LR (PGLR) approach is reported that both the global and local context are efficiently captured in the probabilistic model. In this paper, we implement a Thai G2P system based on the PGLR approach. The result of experiment shows 90.44% of word accuracy in case of ignoring vowels length and 72.87% of word accuracy in case of exact match evaluation. These results are superior to those of rule-based and decision-tree approaches.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-227"
  },
  "blache01_eurospeech": {
   "authors": [
    [
     "Philippe",
     "Blache"
    ],
    [
     "Daniel",
     "Hirst"
    ]
   ],
   "title": "Aligning prosody and syntax in property grammars",
   "original": "e01_1061",
   "page_count": 4,
   "order": 233,
   "p1": "1061",
   "pn": "1064",
   "abstract": [
    "We propose in this paper a new approach for representing the prosody/syntax interface. We use for this a particular formalism, called {em Property Grammars}, in which all information is represented by means of constraints. We show how alignment constraints can implement such an interface. One of the interests of these constraints, in comparison with other approaches such as optimality theory, is the possibility of representing different information at the same level (allowing then a parallel treatment of prosody and syntax). This discussion is illustrated with the example of dislocated constructions.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-228"
  },
  "barkat01_eurospeech": {
   "authors": [
    [
     "Melissa",
     "Barkat"
    ],
    [
     "Ioana",
     "Vasilescu"
    ]
   ],
   "title": "From perceptual designs to linguistic typology and automatic language identification : overview and perspectives",
   "original": "e01_1065",
   "page_count": 4,
   "order": 234,
   "p1": "1065",
   "pn": "1068",
   "abstract": [
    "In the last years, researches in human identification of languages benefit from a special attention as an alternative to improve the robustness of automatic systems. In this scope, two fundamental goals are pursued: first, to highlight the perceptual strategies used by subjects during an experimental language and/or dialectal identification task and secondly, to identify a set of discriminative cues corresponding to different linguistic levels. Methodologically speaking, two different approaches emerge: one can find experimental designs using natural speech aiming at determining global discriminative cues and/or designs based on modified-speech aiming at isolating specific linguistic levels. These experiments lack of methodology as for the choice of studied languages as well as for subjects mother tongue. This aspect being all the more worsened by a hazy spotting and a limited exploitation of discriminative criteria. We suggest an original approach based on a two-step methodology integrating to perception \"genetic\" considerations and resulting into the modeling of perceptually identified discriminative cues.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-229"
  },
  "fitt01_eurospeech": {
   "authors": [
    [
     "Susan",
     "Fitt"
    ]
   ],
   "title": "Morphological approaches for an English pronunciation lexicon",
   "original": "e01_1069",
   "page_count": 4,
   "order": 235,
   "p1": "1069",
   "pn": "1072",
   "abstract": [
    "Most pronunciation lexica for speech synthesis in English take no account of morphology. Here we demonstrate the benefits of including a morphological breakdown in the transcription. These include maintaining consistency, developing the symbol set and providing the environmental description for allophones and phonetic variables. Our approach does not use a full morphological generator, but includes morphological boundaries in the lexicon.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-230"
  },
  "joue01_eurospeech": {
   "authors": [
    [
     "Gina",
     "Joue"
    ],
    [
     "Julie",
     "Carson-Berndsen"
    ]
   ],
   "title": "An embodiment paradigm for speech recognition systems",
   "original": "e01_1073",
   "page_count": 4,
   "order": 236,
   "p1": "1073",
   "pn": "1076",
   "abstract": [
    "The problems of conventional speech recognition approaches include incomplete linguistic knowledge and inability to deal with underspecification. These issues can be addressed by understanding the constraints of speech to predict speech tendencies. Understanding what constraints exist requires an embodied view of speech and that the traditional disembodied view of speech is the fundamental limitation on the robustness of many speech systems. Viewing speech as a form of embodied cognition, or within context of its production and use, provides important insights for speech recognition. In making this claim, this paper briefly outlines a strongly embodied account of cognition and develops from that an embodiment paradigm for speech recognition. The embodiment paradigm proposed leads to both an explanatory and descriptive account of linguistic structure. It simplifies the view of speech structure for automatic speech recognisers, by considering only the most directly relevant motivations or constraints influencing communication and thus speech.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-231"
  },
  "xu01b_eurospeech": {
   "authors": [
    [
     "Kui",
     "Xu"
    ],
    [
     "Fuliang",
     "Weng"
    ],
    [
     "Helen M.",
     "Meng"
    ],
    [
     "Po Chui",
     "Luk"
    ]
   ],
   "title": "Multi-parser architecture for query processing",
   "original": "e01_1077",
   "page_count": 4,
   "order": 237,
   "p1": "1077",
   "pn": "1080",
   "abstract": [
    "Natural language queries provide a natural means for common people to interact with computers and access to on-line information. Due to the complexity of natural language, the traditional way of using a single grammar for a single language parser leads to an inefficient, fragile, and often very big language processing system. Multi-Parser Architecture (MPA) intends to alleviate these problems, and the modularized MPA also has the advantage of easier portability to new domains and distributed computing. In this paper, we investigate the effect of using different types of parsers on different types of query data in MPA. Three data sets and two types of sub-parsers, particularly a predictive cascading composition for pre-compiled Earley parsers , have been examined. Results show that partitioning grammars leads to superior speed performance for the Earley-style parser across the three data sets. GLR parser is faster than Earley parser in the partitioned case, but it can lead to an excessive memory usage for the un-partitioned case.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-232"
  },
  "chen01f_eurospeech": {
   "authors": [
    [
     "Yi-Chia",
     "Chen"
    ],
    [
     "Yi-Chung",
     "Lin"
    ]
   ],
   "title": "Two-stage probabilistic approach to text segmentation",
   "original": "e01_1081",
   "page_count": 4,
   "order": 238,
   "p1": "1081",
   "pn": "1084",
   "abstract": [
    "For telephone-based spoken dialogue systems, the responses to users should be specific and short. Therefore, it is highly demanded to segment a topical text into specific event segments which can be use to answer users' queries. However, the lexical cohesion approach, which has been widely used to segment text into topics, is not suitable for segmenting text into smaller units, like events. In this paper, we present a two-stage approach to partition text into event segments. In the first stage, a trigram chunk tagger is used to label the segmentation tags. In the second stage, the unreliable segmentation tags are detected and then verified by a probabilistic verification model. Compared with the chunk tagger, the verification model can explore more contextual information and is less sensitive to the sparseness of training data. Experimental results show that the proposed two-stage approach significantly outperforms the chunk tagger approach. The improvements on precision and recall rates are 27% to 83% in different testing tasks.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-233"
  },
  "ordelman01_eurospeech": {
   "authors": [
    [
     "Roeland",
     "Ordelman"
    ],
    [
     "Arjan van",
     "Hessen"
    ],
    [
     "Franciska de",
     "Jong"
    ]
   ],
   "title": "Lexicon optimization for dutch speech recognition in spoken document retrieval",
   "original": "e01_1085",
   "page_count": 4,
   "order": 239,
   "p1": "1085",
   "pn": "1088",
   "abstract": [
    "In this paper, ongoing work concerning the language modelling and lexicon optimization of a Dutch speech recognition system for Spoken Document Retrieval is described: the collection and normalization of a training data set and the optimization of our recognition lexicon. Effects on lexical coverage of the amount of training data, of decompounding compound words and of different selection methods for proper names and acronyms are discussed.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-234"
  },
  "brndsted01_eurospeech": {
   "authors": [
    [
     "Tom",
     "Brøndsted"
    ]
   ],
   "title": "Evaluation of recent speech grammar standardization efforts",
   "original": "e01_1089",
   "page_count": 4,
   "order": 240,
   "p1": "1089",
   "pn": "1092",
   "abstract": [
    "The \"Voice Browser \" activity within the W3C consortium addresses the need for standards for speech grammars, dialogue descriptions etc. in distributed systems. This paper discusses the consortium's recent speech grammar working draft specification. The W3C specification is based on the Javatm Speech Grammar Format (JSGF) defined by Sun Microsystems and is - with all good and bad qualities - characterized by traditions of formal language theory. In a constructive spirit, we suggest some possible improvements based on natural language theory. The suggestions concern compound feature-based semantic presentations, lexicon structure, ambiguity, and exhaustive parsing,\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-235"
  },
  "brungart01_eurospeech": {
   "authors": [
    [
     "Douglas S.",
     "Brungart"
    ],
    [
     "Kimberly R.",
     "Scott"
    ],
    [
     "Brian D.",
     "Simpson"
    ]
   ],
   "title": "The influence of vocal effort on human speaker identification",
   "original": "e01_0747",
   "page_count": 4,
   "order": 241,
   "p1": "747",
   "pn": "750",
   "abstract": [
    "Although many of the acoustic cues used for speaker identification change systematically with the voice level of the talker, little is known about the influence vocal effort has on the identification of individual talkers by human listeners. In this experiment, listeners were trained to identify four different same-sex talkers speaking at one of three different levels of vocal effort (whispered, conversational, or shouted). They were then tested on their ability to identify the same four talkers speaking at the other two levels of vocal effort. The results show that the whispering talkers were harder to identify than the conversational talkers, and that the conversational talkers were harder to identify than the shouting talkers. The results also show that listeners who were trained to identify individual talkers speaking at one level of vocal effort had difficulty identifying the same talkers when they were speaking at a different level of vocal effort.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-236"
  },
  "faltlhauser01_eurospeech": {
   "authors": [
    [
     "Robert",
     "Faltlhauser"
    ],
    [
     "Günther",
     "Ruske"
    ]
   ],
   "title": "Improving speaker recognition using phonetically structured Gaussian mixture models",
   "original": "e01_0751",
   "page_count": 4,
   "order": 242,
   "p1": "751",
   "pn": "754",
   "abstract": [
    "Gaussian Mixture Models (GMM) are highly suitable for speaker identification and verification. Nevertheless these models try to represent primarily the distribution of the available training data - neglecting any possible phonetic information which might be of worth. In our paper we present a recognition system using multiple speaker GMMs based on phonetic classes. By introducing 'phonetic' mixture coefficients a weighting of phoneme classes with respect to speaker recognizability can be achieved. The implicit integration in the probability computation avoids the need for a phonetic labeling during recognition. The mixture weights can be learned in a training phase. Model training was examined applying MAP enrolment and the recently reported Eigenvoice approach. Especially for the latter a phonetic separation is advantageous. Recognition error reductions up to 15% relatively were achieved. Furthermore, the multiple GMM approach is particularly effective for speaker enrolment with sparse training data.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-237"
  },
  "sanderson01_eurospeech": {
   "authors": [
    [
     "Conrad",
     "Sanderson"
    ],
    [
     "Kuldip K.",
     "Paliwal"
    ]
   ],
   "title": "Information fusion for robust speaker verification",
   "original": "e01_0755",
   "page_count": 4,
   "order": 243,
   "p1": "755",
   "pn": "758",
   "abstract": [
    "In this paper we have studied two information fusion approaches, namely feature vector concatenation and decision fusion, for the task of reducing error rates in a speaker verification system used in mismatched conditions. Three types of features are fused: Mel Frequency Cepstral Coefficients (MFCC), MFCC with Cepstral Mean Subtraction (CMS) and Maximum Auto-Correlation Values (MACV). We have used the mismatch sensitivity of Linear Prediction Cepstral Coefficients (LPCC) as a speech quality measure for selecting the weight of the contribution of the MFCC modality in the adaptive decision fusion approach. We show that in most cases concatenation fusion is superior to decision fusion. The results lead us to propose a hybrid fusion approach in which two combinations of concatenation fusion are further fused using adaptive decision fusion. The hybrid system is shown to have the lowest error rates on both clean and noisy speech.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-238"
  },
  "satoh01_eurospeech": {
   "authors": [
    [
     "Takayuki",
     "Satoh"
    ],
    [
     "Takashi",
     "Masuko"
    ],
    [
     "Takao",
     "Kobayashi"
    ],
    [
     "Keiichi",
     "Tokuda"
    ]
   ],
   "title": "A robust speaker verification system against imposture using an HMM-based speech synthesis system",
   "original": "e01_0759",
   "page_count": 4,
   "order": 244,
   "p1": "759",
   "pn": "762",
   "abstract": [
    "This paper describes a text-prompted speaker verification system which is robust to imposture using synthetic speech generated by an HMM-based speech synthesis system. In the verification system, text and speaker are verified separately. Text verification is based on phoneme recognition using HMM, and speaker verification is based on GMM. To discriminate synthetic speech from natural speech, an average of inter-frame difference of the log likelihood is calculated, and input speech is judged to be synthetic when this value is smaller than a decision threshold. Experimental results show that the false acceptance rate for synthetic speech was reduced drastically without significant increase of the false acceptance and rejection rates for natural speech.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-239"
  },
  "surendran01_eurospeech": {
   "authors": [
    [
     "Arun C.",
     "Surendran"
    ]
   ],
   "title": "Sequential decisions for faster and more flexible verification",
   "original": "e01_0763",
   "page_count": 4,
   "order": 245,
   "p1": "763",
   "pn": "766",
   "abstract": [
    "Most speaker verification systems wait to collect a complete utterance from a speaker before making a decision. Faster verification can be achieved if decisions are made sequentially on smaller \"chunks\" of data. In this paper we present a sequential decision making algorithm in a connected digit application and discuss its properties. We show that sequential decisions, apart from requiring shorter utterances and fewer computations on the average, add another dimension of flexibility over current systems: within some limitations, they provide the ability to systematically tradeoff between the performance of the system and the amount of data needed to make a decision. Thus they make a speaker verification system work faster and be more flexible in real applications.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-240"
  },
  "tsai01b_eurospeech": {
   "authors": [
    [
     "Wei-Ho",
     "Tsai"
    ],
    [
     "Y. C.",
     "Chu"
    ],
    [
     "Chao-Shih",
     "Huang"
    ],
    [
     "Wen-Whei",
     "Chang"
    ]
   ],
   "title": "Background learning of speaker voices for textindependent speaker identification",
   "original": "e01_0767",
   "page_count": 4,
   "order": 246,
   "p1": "767",
   "pn": "771",
   "abstract": [
    "This study provides a novel learning mechanism, the so-called background learning, to the problem of text-independent speaker identification (speaker ID). Unlike the conventional speaker ID, the proposed system does not rely on enrollment data of clients in construction of speaker-specific models, but instead attempts to learn speakers' voices via clustering and parametric modeling of off-line collected data with no label of speaker identity. This eliminates the necessity of enrolling a large amount of speech data from clients. To permit such unsupervised learning, an efficient algorithm for blind clustering of speech utterances based on speaker characteristics is developed. Experimental results demonstrated that when very limited enrollment data is available, the speaker-ID performance achieved with the background learning could emulate that of using abundant enrollment data.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-241"
  },
  "tsai01c_eurospeech": {
   "authors": [
    [
     "Wei-Ho",
     "Tsai"
    ],
    [
     "Wen-Whei",
     "Chang"
    ],
    [
     "Chao-Shih",
     "Huang"
    ]
   ],
   "title": "Explicit exploitation of stochastic characteristics of test utterance for text-independent speaker identification",
   "original": "e01_0771",
   "page_count": 4,
   "order": 247,
   "p1": "771",
   "pn": "774",
   "abstract": [
    "In this paper, a novel speaker-identification (speaker-ID) technique based on explicit exploitation of stochastic characteristics of test utterance is proposed. Unlike the conventional approach which hypothesizes the identity of a test speaker by determining which client's model maximizes the likelihood for the test utterance, it is aimed to bilaterally compare test speaker's voices with client speakers' voices instead of simply taking the unilateral likelihoods into account. We study two approaches respectively based on cross likelihood ratio and Bayesian information criterion to accomplish this aim. Performance of the proposed approaches was evaluated by close-set text-independent speaker ID experiments and was shown to be superior to that of the conventional approach based on maximum likelihood decision rule.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-242"
  },
  "wutiwiwatchai01_eurospeech": {
   "authors": [
    [
     "Chai",
     "Wutiwiwatchai"
    ],
    [
     "Varin",
     "Achariyakulporn"
    ],
    [
     "Sawit",
     "Kasuriya"
    ]
   ],
   "title": "Improvement of speaker verification for Thai language",
   "original": "e01_0775",
   "page_count": 4,
   "order": 248,
   "p1": "775",
   "pn": "778",
   "abstract": [
    "There are many strategies proposed for speaker verification (SV) system, both in text-dependent (fixed-text) and text-independent (free-text) domains. To convey an appropriate algorithm for Thai speech, several consecutively improvement methods are compared in this paper including the dynamic time warping (DTW) matching and Gaussian mixture model (GMM) based systems. We firstly developed a system based on the conventional scoring algorithm. This system is improved by the incorporation of many scoring algorithms such as the cohort normalization, the global speaker model (GSM), and a new approach, namely, global anti-speaker model (GASM). Experiments are set up for Thai numeral speech and the results show an improving tendency of each algorithm.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-243"
  },
  "rodriguezsaeta01_eurospeech": {
   "authors": [
    [
     "Javier",
     "Rodríguez-Saeta"
    ],
    [
     "Christian",
     "Koechling"
    ],
    [
     "Javier",
     "Hernando"
    ]
   ],
   "title": "Speaker identification for car infotainment applications",
   "original": "e01_0779",
   "page_count": 4,
   "order": 249,
   "p1": "779",
   "pn": "782",
   "abstract": [
    "Car applications demand more and more the use of speech technologies. Drivers must concentrate on controlling the car and the non-use of hands makes the voice a valuable tool. Here we analyze the possibility of identifying the user of a car through her/his voice in order to develop some useful applications, and establish preferences, some of them related to music. The identification will be done in parallel to speech commands which will be given to devices in the car in the future. Once the user is identified, the system loads a personal profile. It includes music preferences which can be downloaded from the Internet databases using e.g. MPEG-7 .\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-244"
  },
  "schalk01_eurospeech": {
   "authors": [
    [
     "H.",
     "Schalk"
    ],
    [
     "Herbert",
     "Reininger"
    ],
    [
     "Stephan",
     "Euler"
    ]
   ],
   "title": "A system for text dependent speaker verification - field trial evaluation and simulation results",
   "original": "e01_0783",
   "page_count": 4,
   "order": 250,
   "p1": "783",
   "pn": "786",
   "abstract": [
    "In a speaker verification system, an identity claim is made by an unknown speaker. An utterance of this speaker and a model of the speaker whose identity is claimed is compared. If the model and the utterance match well the claim is accepted otherwise it is rejected. Thus, two classes of errors can occur in a speaker verification system: false acceptances and false rejections. The Equal Error Rate (EER) is often used as a performance measure of a verification system and results if the system parameters are adjusted in such a way that the two kinds of errors are equal. To model a speaker, reference utterances of this speaker are recorded in an enrollment session. Modelling itself can be done either in the signal domain using Dynamic Time Warping (DTW) or with a stochastic model of the speaker using Hidden Markov Models (HMM).\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-245"
  },
  "martin01_eurospeech": {
   "authors": [
    [
     "Alvin F.",
     "Martin"
    ],
    [
     "Mark A.",
     "Przybocki"
    ]
   ],
   "title": "Speaker recognition in a multi-speaker environment",
   "original": "e01_0787",
   "page_count": 4,
   "order": 251,
   "p1": "787",
   "pn": "790",
   "abstract": [
    "We discuss the multi-speaker tasks of detection, tracking, and segmentation of speakers as included in recent NIST Speaker Recognition Evaluations. We consider how performance for the two-speaker detection task is related to that for the corresponding one-speaker task. We examine the effects of target speaker speech duration and the gender mix within test segments on results for these tasks. We also relate performance results for the tracking and segmentation tasks, and look at factors affecting segmentation performance.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-246"
  },
  "ou01_eurospeech": {
   "authors": [
    [
     "Zhijian",
     "Ou"
    ],
    [
     "Zuoying",
     "Wang"
    ]
   ],
   "title": "A new DP-like speaker clustering algorithm",
   "original": "e01_0791",
   "page_count": 4,
   "order": 252,
   "p1": "791",
   "pn": "794",
   "abstract": [
    "In this paper we propose a new segment-synchronous speaker clustering algorithm based on the Bayesian Information Criterion (BIC), which is motivated by the Dynamic Programming (DP) idea. Compared with the commonly used agglomerative speaker clustering methods, the proposed algorithm is faster for lack of distance-matrix building and more reasonable as it avoids in some degree the simple irrevocable merging fashion. Moreover it facilitates online speaker clustering, which is important for real-time transcription applications (e.g., broadcast news, teleconferences etc.). In our experiments on 1997 Hub4 Mandarin broadcast news development data, unsupervised speaker adaptation with this DP-like clustering achieved 17.66% relative reduction in Character Error Rate (CER) from the baseline, as much as with the clustering by the true speaker identities.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-247"
  },
  "sivakumaran01_eurospeech": {
   "authors": [
    [
     "P.",
     "Sivakumaran"
    ],
    [
     "J.",
     "Fortuna"
    ],
    [
     "A. M.",
     "Ariyaeeinia"
    ]
   ],
   "title": "On the use of the Bayesian information criterion in multiple speaker detection",
   "original": "e01_0795",
   "page_count": 4,
   "order": 253,
   "p1": "795",
   "pn": "798",
   "abstract": [
    "An efficient scheme, based on the Bayesian information criterion (BIC), for the detection of speaker changes in an audio stream is introduced and investigated. BIC has been the subject of considerable attention in recent years due to its effectiveness for speaker change detection (SCD) as well as the detection of other forms of acoustic changes. A main difficulty in BIC-based SCD has been reported to be that of the computational complexity. The scheme proposed here tackles this problem by reducing the computational load in the previously proposed algorithms significantly, without compromising their effectiveness. The paper describes the new scheme thoroughly and analyses its performance. Experiments are based on 3 hours of broadcast news with 416 speaker changes. With this data, the proposed scheme has been found to be capable of running in about 0.06 times real-time whilst keeping the rate of each of misdetection and false alarm close to 9%.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-248"
  },
  "benarousse01_eurospeech": {
   "authors": [
    [
     "Laurent",
     "Benarousse"
    ],
    [
     "Edouard",
     "Geoffrois"
    ]
   ],
   "title": "Preliminary experiments on language identification using broadcast news recordings",
   "original": "e01_0799",
   "page_count": 4,
   "order": 254,
   "p1": "799",
   "pn": "802",
   "abstract": [
    "This article presents experiments on language identification using Broadcast News recordings, for which large amounts of data are available. The system uses a Broadcast News partitioner developed by LIMSI to extract the speech segments from raw signals. These segments are then transcribed using a language-independent HMM acoustic model. Phonotactic models are trained for each language, and used to score the transcription of the test signals. Training was conducted on recordings from three monolingual radios (about 17h of signal per language) and tests were made on signals from other radios. We also investigated a rejection strategy to improve the identification results. Without any rejection, the error rates range from 13.8% (5s segments) to 4.3% (45 s segments). Rejecting 1/3 of the data improves these rates by 78% for 10s segments.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-249"
  },
  "kirchhoff01_eurospeech": {
   "authors": [
    [
     "Katrin",
     "Kirchhoff"
    ],
    [
     "Sonia",
     "Parandekar"
    ]
   ],
   "title": "Multi-stream statistical n-gram modeling with application to automatic language identification",
   "original": "e01_0803",
   "page_count": 4,
   "order": 255,
   "p1": "803",
   "pn": "806",
   "abstract": [
    "Most state-of-the art automatic language identification systems are based on phonotactic information, i.e. languages are identified on the basis of probabilities of phone sequences extracted from the acoustic signal. This approach ignores the potential advantages to be gained from a richer representation of the acoustic signal in terms of parallel streams of subphonemic events. In this paper we develop an alternative approach to language identification which is based on parallel streams of phonetic features and sparse modeling of statistical dependencies between these streams. We present results on the OGI-TS database and show that the feature-based system outperforms a comparable phone-based system significantly while using fewer parameters. Moreover, the feature-based system exhibits a markedly better performance on very short test signals (< 3 seconds).\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-250"
  },
  "streefkerk01_eurospeech": {
   "authors": [
    [
     "Barbertje M.",
     "Streefkerk"
    ],
    [
     "Louis C. W.",
     "Pols"
    ],
    [
     "Louis F. M. ten",
     "Bosch"
    ]
   ],
   "title": "Up to what level can acoustical and textual features predict prominence",
   "original": "e01_0811",
   "page_count": 4,
   "order": 256,
   "p1": "811",
   "pn": "814",
   "abstract": [
    "In this paper both acoustical as well as textual correlates of prominence are discussed. Prominence, as we use it, is defined at the word level and is based on listener judgments. A selection of useful acoustic input features is tested for classification of prominent words, with the help of Feed Forward Nets. We use spoken sentences from many different speakers, taken from the Dutch Polyphone corpus of telephone speech. For an independent test set of 1,000 sentences about 72% of the words are correctly classified as prominent or not. At the text input level we also developed an algorithm, using linguistic/syntactical features derived from text only, to predict prominence. The prediction agrees with the perceived prominence in 82.6% of the cases.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-251"
  },
  "chung01_eurospeech": {
   "authors": [
    [
     "Hyunsong",
     "Chung"
    ],
    [
     "Mark A.",
     "Huckvale"
    ]
   ],
   "title": "Linguistic factors affecting timing in Korean with application to speech synthesis",
   "original": "e01_0815",
   "page_count": 4,
   "order": 257,
   "p1": "815",
   "pn": "818",
   "abstract": [
    "This paper describes the results of a study of the phonetic and phonological factors affecting the rhythm and timing of spoken Korean. Stepwise construction of a CART model was used to uncover the contribution and relative importance of phrasal, syllabic, and segmental contexts. The model was trained from a corpus of 671 read sentences, yielding 42,000 segments each annotated with 69 linguistic features. On reserved test data, the best model showed a correlation coefficient of 0.73 with a RMS prediction error of 26 ms. Analysis of the classification tree during and after construction shows that phrasal structure had the greatest influence on segmental duration. Strong lengthening effects were shown for the first and last syllable in the accentual phrase. Syllable structure and the manner features of surrounding segments had smaller effects on segmental duration. The model has application within Korean speech synthesis.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-252"
  },
  "schaeffler01_eurospeech": {
   "authors": [
    [
     "Felix",
     "Schaeffler"
    ]
   ],
   "title": "Measuring rhythmic deviation in second language speech",
   "original": "e01_0819",
   "page_count": 4,
   "order": 258,
   "p1": "819",
   "pn": "822",
   "abstract": [
    "This study deals with the question of whether recently provided methods to determine the rhythm class of languages can be transferred to foreign-accented speech. Therefore read German speech of Venezuelan Spanish native speakers was compared with read speech of a native German control group by means of four different measurements. Three of the four applied measurements showed significant differences between the two groups, with one of the differences contradicting earlier expectations. The study has shown that the measurements can be successfully transferred to foreign-accented speech, but slightly modified measurements are suggested.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-253"
  },
  "maddieson01_eurospeech": {
   "authors": [
    [
     "Ian",
     "Maddieson"
    ]
   ],
   "title": "Good timing: place-dependent voice onset time in ejective stops",
   "original": "e01_0823",
   "page_count": 4,
   "order": 259,
   "p1": "823",
   "pn": "826",
   "abstract": [
    "Voice onset time after voiceless unaspirated stops demonstrates a dependence on place of articulation, most reliably being shorter for labial and coronal than for velar stops. Some of the proposed explanations for this pattern suggest that a parallel dependence is not be expected for aspirated or ejective stops. However, similar patterns do occur with both aspirated and unaspirated stops. Cho and Ladefoged (1999) have suggested that ejectives do not follow the same trend, but they had little data on bilabial ejectives to compare with more plentiful data on velars. This paper contributes more material to this debate with expanded data on Yapese and the first published material on ejective VOT in Nez Perce. The results suggest that ejectives have a similar pattern to plosives and that therefore a unified explanation for all three types of stops should be sought.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-254"
  },
  "francois01_eurospeech": {
   "authors": [
    [
     "Helene",
     "Francois"
    ],
    [
     "Olivier",
     "Boeffard"
    ]
   ],
   "title": "Design of an optimal continuous speech database for text-to-speech synthesis considered as a set covering problem",
   "original": "e01_0829",
   "page_count": 4,
   "order": 260,
   "p1": "829",
   "pn": "832",
   "abstract": [
    "Text-to-speech synthesis can be carried out by concatenation of acoustic units obtained from a continuous speech database. This paper presents the optimization of such as database according to phonetic criteria. A large corpus of texts is assembled (311 572 sentences), phonetized automatically and condensed (12 217 sentences) to retain only 10 tokens of the most frequent triphonemes. This is a NP-hard problem of set covering. It has been solved in an approximate way using a greedy algorithm. The condensed database covers 25% of the initial distinct triphonemes, each being represented by 10 tokens at least, which allows 95% of the triphoneme tokens of the initial corpus to be covered. The distribution of the triphonemes remains proportional to their initial statistical appearance.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-255"
  },
  "vosnidis01_eurospeech": {
   "authors": [
    [
     "Christos",
     "Vosnidis"
    ],
    [
     "Vassilis",
     "Digalakis"
    ]
   ],
   "title": "Use of clustering information for coarticulation compensation in speech synthesis by word concatenation",
   "original": "e01_0833",
   "page_count": 4,
   "order": 261,
   "p1": "833",
   "pn": "836",
   "abstract": [
    "The Weather Report Synthesizer is a speech synthesis system for weather forecasts in Greek. Instead of trying to improve the synthesis quality of PSOLA based diphone concatenation speech synthesizers, we have chosen to use words as the synthesis units. This approach has the advantage of low complexity and quick implementation, and achieves better speech quality due to the fact that the synthesis units inherently possess the necessary prosodic feature diversity. The selection of the optimal sequence of words that form the synthesized speech, however, presents the greatest challenge in the synthesis process. Several features are taken into consideration during the selection, but we have identified Coarticulation at the edges of consecutive words to have the greatest effect on the quality of the synthesized utterance. We present a novel method for evaluating a measure on coarticulation effects among pairs of words, based on feature clustering information obtained from a current SR system.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-256"
  },
  "founda01_eurospeech": {
   "authors": [
    [
     "Maria",
     "Founda"
    ],
    [
     "George",
     "Tambouratzis"
    ],
    [
     "Aimilios",
     "Chalamandaris"
    ],
    [
     "George",
     "Carayannis"
    ]
   ],
   "title": "Reducing spectral mismatches in concatenative speech synthesis via systematic database enrichment",
   "original": "e01_0837",
   "page_count": 4,
   "order": 262,
   "p1": "837",
   "pn": "840",
   "abstract": [
    "This paper presents work performed for the Time-Domain TTS system, which is being developed at the ILSP for the Greek language. It focuses on the enhancement of the synthetic speech quality, by reducing the spectral mismatches between concatenated segments. To that end, a study has been performed to determine the distance that can best predict when a spectral mismatch is audible. Experimentation with different spectral distances has taken place and the distance with the best performance has been used in order to systematically enrich the segment database, which initially contained only one instance per segment. Results of this procedure indicate a substantial improvement in the synthetic speech quality.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-257"
  },
  "ferencz01_eurospeech": {
   "authors": [
    [
     "Attila",
     "Ferencz"
    ],
    [
     "Sung-Woo",
     "Choi"
    ],
    [
     "Ho-Eun",
     "Song"
    ],
    [
     "Myoung-Wan",
     "Koo"
    ]
   ],
   "title": "Hansori 2001 - corpus-based implementation of the Korean hansori text-to-speech synthesizer",
   "original": "e01_0841",
   "page_count": 4,
   "order": 263,
   "p1": "841",
   "pn": "844",
   "abstract": [
    "The improvement of Text-to-Speech (TTS) synthesizers' speech quality and naturalness is a continuous concern of researchers worldwide. The present paper gives a brief introduction of several previous Hansori TTS systems and is introducing our approach on experimenting, adopting and implementing corpus-based techniques for the system. We are focusing on corpus selection, on the optimal unit search criteria, on the missing unit (triphone unit) replacement handling, and we present some useful development tool options included in the trial version of the system.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-258"
  },
  "barry01_eurospeech": {
   "authors": [
    [
     "William",
     "Barry"
    ],
    [
     "Claus",
     "Nielsen"
    ],
    [
     "Ove",
     "Andersen"
    ]
   ],
   "title": "Must diphone synthesis be so unnatural?",
   "original": "e01_0975",
   "page_count": 4,
   "order": 264,
   "p1": "975",
   "pn": "978",
   "abstract": [
    "An English utterance was synthesized in four versions using sets of diphones produced under four different prosodic and contextual conditions. The synthesis used either accented di-phones only or appropriately located accented and unaccented diphones, with each of these conditions being repeated using neutral-context and differentiated-context diphones. They were presented to two listener groups, a native English and a non-native group for paired comparison acceptability judgements. The results show a massive preference for the stress- and context-differentiated condition. Both stress and context had a significant effect on acceptability judgements, but context-differentiation raised acceptability more strongly than stress-differentiation. Both the native and the main sub-group of non-native listeners judged the stimuli in essentially the same way.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-259"
  },
  "syrdal01_eurospeech": {
   "authors": [
    [
     "Ann K.",
     "Syrdal"
    ]
   ],
   "title": "Phonetic effects on listener detection of vowel concatenation",
   "original": "e01_0979",
   "page_count": 4,
   "order": 265,
   "p1": "979",
   "pn": "982",
   "abstract": [
    "Concatenative speech synthesis quality depends in part on the minimization of audible discontinuities between two successive concatenated units. This study focuses on human detection of concatenation discontinuities in synthetic speech. A phonetic analysis compared the perceptual results from two voices -- one female and one male. Neither a comprehensive phonetic analysis nor a comparison of discontinuity detection between voices has been reported previously. Although discontinuities were generally more detectable for the female than the male, there were many similarities between results obtained from the two speakers. A reliably higher detection rate was observed for diphthongs than for monophthong vowels. Post-vocalic consonants influenced concatenation discontinuities significantly more than prevocalic consonants, and post-vocalic sonorants were associated with higher detection rates than post-vocalic non-sonorants. The differences in discontinuity detection among vowels and consonantal contexts for both voices consistently suggest that highly audible discontinuity is related to concatenation in regions of spectral change.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-260"
  },
  "boeffard01_eurospeech": {
   "authors": [
    [
     "Olivier",
     "Boeffard"
    ]
   ],
   "title": "Variable-length acoustic units inference for text-to-speech synthesis",
   "original": "e01_0983",
   "page_count": 4,
   "order": 266,
   "p1": "983",
   "pn": "986",
   "abstract": [
    "The best voices in text-to-speech synthesis are currently obtained via acoustic units concatenation-based systems. In such systems, the choice of units whose concatenations will produce an acoustic message is a crucial stage. Moreover, it can be observed that current TTS systems use acoustic units which most often correspond to variable-length phonetic descriptions. In this article, an original framework is proposed which allows the automatic determination of an optimum set of variable-length acoustic units.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-261"
  },
  "bulyko01_eurospeech": {
   "authors": [
    [
     "Ivan",
     "Bulyko"
    ],
    [
     "Mari",
     "Ostendorf"
    ]
   ],
   "title": "Unit selection for speech synthesis using splicing costs with weighted finite state transducers",
   "original": "e01_0987",
   "page_count": 4,
   "order": 267,
   "p1": "987",
   "pn": "990",
   "abstract": [
    "In this paper we describe how unit selection for concatenative speech synthesis can be implemented efficiently for sub-phonetic units using weighted finite state transducers (WFST). We also introduce splicing costs as a measure to indicate which unit boundaries are particularly good or poor joint points. Splicing costs extend the flexibility offered by the unit selection paradigm. Through a perceptual experiment we demonstrate an improvement in speech quality achieved by using splicing costs during unit selection.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-262"
  },
  "law01_eurospeech": {
   "authors": [
    [
     "K. M.",
     "Law"
    ],
    [
     "Tan",
     "Lee"
    ],
    [
     "Wai",
     "Lau"
    ]
   ],
   "title": "Cantonese text-to-speech synthesis using sub-syllable units",
   "original": "e01_0991",
   "page_count": 4,
   "order": 268,
   "p1": "991",
   "pn": "994",
   "abstract": [
    "This paper describes our recent investigation on the use of both intra-syllable and cross-syllable acoustic units for Cantonese text-to-speech synthesis. In our previous work, isolated monosyllable units were used for concatenative speech synthesis of Cantonese. The synthetic speech was considered to be unnatural in such a way that there was an obvious lack of perceptual continuity. The proposed system adopts an acoustic inventory that covers all legitimate intra-syllable and cross-syllable acoustic units. Synthetic speech produced via concatenation of such sub-syllable units better captures the pertinent transitory effects that are crucial to perceived naturalness. Different strategies are used to concatenate speech segments with different acoustic-phonetic properties. Subjective listening test shows a noticeable performance improvement that is accounted for mainly by smoother transition between sonorant segments.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-263"
  },
  "wet01_eurospeech": {
   "authors": [
    [
     "Febe de",
     "Wet"
    ],
    [
     "Bert",
     "Cranen"
    ],
    [
     "Johan de",
     "Veth"
    ],
    [
     "Loe",
     "Boves"
    ]
   ],
   "title": "A comparison of LPC and FFT-based acoustic features for noise robust ASR",
   "original": "e01_0865",
   "page_count": 4,
   "order": 269,
   "p1": "865",
   "pn": "868",
   "abstract": [
    "Within the context of robust acoustic features for automatic speech recognition (ASR), we evaluated mel-frequency cepstral coefficients (MFCCs) derived from two spectral representation techniques, i.e. the fast Fourier transform (FFT) and linear predictive coding (LPC). ASR systems based on the two feature types were tested on a digit recognition task using continuous density hidden Markov phone models. System performance was determined in clean acoustic conditions as well as in different simulations of adverse acoustic conditions. The LPC-based MFCCs outperformed their FFT counterparts in most of the adverse acoustic conditions that were investigated in this study. A tentative explanation for this difference in recognition performance is given.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-264"
  },
  "yamada01_eurospeech": {
   "authors": [
    [
     "Miichi",
     "Yamada"
    ],
    [
     "Akira",
     "Baba"
    ],
    [
     "Shinichi",
     "Yoshizawa"
    ],
    [
     "Yuichiro",
     "Mera"
    ],
    [
     "Akinobu",
     "Lee"
    ],
    [
     "Hiroshi",
     "Saruwatari"
    ],
    [
     "Kiyohiro",
     "Shikano"
    ]
   ],
   "title": "Unsupervised noisy environment adaptation algorithm using MLLR and speaker selection",
   "original": "e01_0869",
   "page_count": 4,
   "order": 270,
   "p1": "869",
   "pn": "872",
   "abstract": [
    "An unsupervised acoustic model adaptation algorithm using MLLR and speaker selection for noisy environments is proposed. The proposed algorithm requires only one arbitrary utterance and environmental noise data. The adaptation procedure is composed of the following four steps. (1) Speaker selection from a large number of database speakers is carried out using GMM speaker models based on one arbitrary utterance. (2) Initial speaker adapted HMM acoustic models are calculated from the HMM sufficient statistics of the selected speakers, where the sufficient HMM statistics are pre-calculated and stored. (3) A small subset of the clean speech database from the selected speakers and the environment noise data are superimposed. (4) MLLR adaptation is carried out using the noise-superimposed speech database from the selected speakers. The proposed algorithm is evaluated in a 20k vocabulary dictation task for newspaper in noisy environments. We attain 85.7% word correct rate in 25dB SNR, which is slightly better than the matched model by the E-M training using noise superimposed whole speech database. The proposed algorithm is also 7% better than the HMM composition algorithm.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-265"
  },
  "tufekci01_eurospeech": {
   "authors": [
    [
     "Zekeriya",
     "Tufekci"
    ],
    [
     "John N.",
     "Gowdy"
    ],
    [
     "Sabri",
     "Gurbuz"
    ],
    [
     "E.",
     "Patterson"
    ]
   ],
   "title": "Applying parallel model compensation with mel-frequency discrete wavelet coefficients for noise-robust speech recognition",
   "original": "e01_0873",
   "page_count": 4,
   "order": 271,
   "p1": "873",
   "pn": "876",
   "abstract": [
    "Interfering noise severely degrades the performance of a speech recognition system. The Parallel Model Combination (PMC) technique is one of the most efficient techniques for dealing with such noise. Another method is to use features local in the frequency domain. Recently, we proposed Mel-Frequency Discrete Wavelet Coefficients (MFDWCs) as speech features local in frequency domain. In this paper, we discuss using PMC along with MFDWC features to take advantage of both noise compensation and local features (MFDWCs) to decrease the effect of noise on recognition performance. In addition we discuss the effect of increasing the number of the noise model mixture component on the performance of the Mel-Frequency Cepstral Coefficients (MFCCs) and MFDWCs. We evaluate the performance of MFDWCs using the NOISEX-92 database for various noise types and noise levels. We also compare the performance of these versus MFCCs and both using PMC for dealing with additive noise.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-266"
  },
  "hwang01_eurospeech": {
   "authors": [
    [
     "Tai-Hwei",
     "Hwang"
    ],
    [
     "Kuo-Hwei",
     "Yuo"
    ],
    [
     "Hsiao-Chuan",
     "Wang"
    ]
   ],
   "title": "Linear interpolation of cepstral variance for noisy speech recognition",
   "original": "e01_0877",
   "page_count": 4,
   "order": 272,
   "p1": "877",
   "pn": "880",
   "abstract": [
    "Speech model combination with the background noise has been shown effective to improve the pattern classification rate of noisy speech. The model combination can be performed by the addition of the spectral statistics such as the means and the variances. Since the speech feature for pattern classification has to be expressed in the cepstral domain, the combined spectral statistics have to be transferred into the cepstral domain for speech recognition. In our previous study, we have proposed a direct adaptation scheme of the cepstral variance that is without the mapping from the spectral domain to the cepstral domain. In this paper, an improved version to perform the adaptation is proposed. From the study, it is observed that the adapted variance can be expressed as a linear interpolation of the speech and the noise variances to obtain a comparable recognition rate that is obtained with the mapping process. Due to the direct adaptation of the variances, a lot of computation can be reduced to perform the environmental adaptation.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-267"
  },
  "matsumoto01_eurospeech": {
   "authors": [
    [
     "Hiroshi",
     "Matsumoto"
    ],
    [
     "Akihiko",
     "Shimizu"
    ],
    [
     "Kazumasa",
     "Yamamoto"
    ]
   ],
   "title": "Evaluation of a generalized dynamic cepstrum in distant speech recognition",
   "original": "e01_0881",
   "page_count": 4,
   "order": 273,
   "p1": "881",
   "pn": "884",
   "abstract": [
    "This paper examines the effectiveness of a generalized dynamic cepstrum in distant speech recognition. The generalized dynamic cepstrum (DyMFGC) is based upon the forward masking on the generalized logarithmic spectrum instead of the log-spectrum, which intends to make it robust to additive noise as well as convolutional noise. Digit recognition tests were carried out in a relatively quiet and small sized office environment. Under white noise environments, the DyMFGC outperforms the dynamic cepstrum on the logarithmic spectrum and the MFCC with cepstral mean normalization. It also maintains the word accuracy of 90% to 95% within a 1m distance from a source. In speech babble noise environments, the performance of the DyMFGC is approximately the same as that of the dynamic cepstrum on the logarithmic amplitude scale.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-268"
  },
  "martin01b_eurospeech": {
   "authors": [
    [
     "Arnaud",
     "Martin"
    ],
    [
     "Géraldine",
     "Damnati"
    ],
    [
     "Laurent",
     "Mauuary"
    ]
   ],
   "title": "Robust speech/non-speech detection using LDA applied to MFCC for continuous speech recognition",
   "original": "e01_0885",
   "page_count": 4,
   "order": 274,
   "p1": "885",
   "pn": "888",
   "abstract": [
    "Continuous speech recognition applications need precise detection because the number of words to recognize is unknown and vocabulary words can be short. The speech/non-speech detection must be robust to the boundary precision. In this work, a new approach to evaluate detection algorithm for continuous speech recognition is presented. The speech/non-speech detection using energy parameter combined with a Linear Discriminant Analysis (LDA) applied to Mel Frequency Cepstrum Coefficients (MFCC) is compared to the algorithm based on signal to noise ratio (SNR). The LDA applied to MFCC for speech/non-speech detection improves recognition performance in noisy environment and for continuous speech recognition applications.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-269"
  },
  "trentin01_eurospeech": {
   "authors": [
    [
     "Edmondo",
     "Trentin"
    ],
    [
     "Marco",
     "Gori"
    ]
   ],
   "title": "Toward noise-tolerant acoustic models",
   "original": "e01_0889",
   "page_count": 4,
   "order": 275,
   "p1": "889",
   "pn": "892",
   "abstract": [
    "Acoustic models relying on hidden Markov models (HMMs) are heavily noise-sensitive: recognition performance drops whenever a significant difference in acoustic conditions holds between training and test environments. The relevance of developing acoustic models that are intrinsically robust has to be stressed. Robustness to noise is related to the generalization capabilities of the model. Artificial neural networks (ANNs) appear to be a promising alternative, but they historically failed as a general paradigm for speech recognition. This paper faces the problem by (i) investigating the recognition performance of the ANN/HMM hybrid proposed by the authors over tasks with noisy signals, and (ii) proposing an explicit \"soft\" weight grouping technique, capable to improve its robustness. Experiments over noisy speaker-independent connected-digits strings are presented. In particular, results on the VODIS II/SpeechDatCar database, collected in a real car environment, show the dramatic gain over the standard HMM, as well as over Bourlard and Morgan's hybrid.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-270"
  },
  "evans01_eurospeech": {
   "authors": [
    [
     "Nicholas W. D.",
     "Evans"
    ],
    [
     "John S.",
     "Mason"
    ]
   ],
   "title": "Noise estimation without explicit speech, non-speech detection: a comparison of mean, modal and median based approaches",
   "original": "e01_0893",
   "page_count": 4,
   "order": 276,
   "p1": "893",
   "pn": "896",
   "abstract": [
    "Automatic speech recognition performance tends to be degraded in noisy conditions. Spectral subtraction is a simple, popular approach of noise compensation. In conventional spectral subtraction noise statistics are updated during speech gaps and subtracted from a corrupt signal during speech intervals. Some means of explicit speech, non-speech detection is therefore essential. Recent proposals have avoided the problem of speech, non-speech detection by continually updating noise estimates whether speech is present or not. In this paper, we evaluate two such approaches of noise estimation and compare their performance with standard noise estimation in hand-labelled speech gaps. Experimental results are reported with the conventional spectral subtraction framework on a 1500 speaker database. Results confirm that such approaches of noise estimation which do not rely on explicit speech, non-speech detection compare favourably with conventional noise estimation approaches.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-271"
  },
  "chengalvarayan01_eurospeech": {
   "authors": [
    [
     "Rathi",
     "Chengalvarayan"
    ]
   ],
   "title": "Evaluation of front-end features and noise compensation methods for robust Mandarin speech recognition",
   "original": "e01_0897",
   "page_count": 4,
   "order": 277,
   "p1": "897",
   "pn": "900",
   "abstract": [
    "This paper describes speaker-independent speech recognition experiments concerning acoustic front-end processing on a telephone database that was recorded in various dialect regions in China. In this paper, three different features based on human voice production, perception and auditory systems have been evaluated for Mandarin speech recognition. Experimental comparisons showed that auditory-filtered cepstral coefficients outperforms the other type of features. When speech recognizers are deployed in telephone services, they often encounter variable acoustic mismatches which significantly deteriorate their performance. Three different channel equalization techniques have been explored in this study to decrease this mismatch, hence improving the recognition accuracy. We present results with various noise compensation methods based on hierarchical cepstral mean subtaction and signal bias removal.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-272"
  },
  "frey01_eurospeech": {
   "authors": [
    [
     "Brendan J.",
     "Frey"
    ],
    [
     "Li",
     "Deng"
    ],
    [
     "Alex",
     "Acero"
    ],
    [
     "Trausti",
     "Kristjansson"
    ]
   ],
   "title": "ALGONQUIN: iterating laplace's method to remove multiple types of acoustic distortion for robust speech recognition",
   "original": "e01_0901",
   "page_count": 4,
   "order": 278,
   "p1": "901",
   "pn": "904",
   "abstract": [
    "We show how an iterative form of Laplace's method can be used to estimate the log-spectrum of clean speech from the log-spectrum of noisy, distorted speech, using a time-varying mixture model of the logspectra of the clean speech, noise, channel distortion and noisy speech. We use this method, called ALGONQUIN, to denoise speech features and then feed these features into a large vocabulary speech recognizer whose WER on the clean WSJ data is 4.9%. When 10dB of time-varying airplane engine noise is added to the data, the recognizer obtains a WER of 28.8%. ALGONQUIN reduces the WER to 12.6%, well below the WER of 25.0% obtained by spectral subtraction, and close to the WER of 9.7% obtained by retraining the recognizer on training data corrupted by the exact same noise. If ALGONQUIN is used to denoise the noisy training data before the recognizer is retrained, the WER drops to 8.5%. For 10dB of white noise, spectral subtraction reduces the WER from 55.1% to 33.8%. ALGONQUIN reduces the WER to 14.2%. The recognizer trained on noisy data obtains a WER of 14.0%, whereas the recognizer trained on noisy data denoised by ALGONQUIN obtains a WER of 9.9%.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-273"
  },
  "hansen01_eurospeech": {
   "authors": [
    [
     "John H. L.",
     "Hansen"
    ],
    [
     "Ruhi",
     "Sarikaya"
    ],
    [
     "Umit",
     "Yapanel"
    ],
    [
     "Bryan",
     "Pellom"
    ]
   ],
   "title": "Robust speech recognition in noise: an evaluation using the SPINE corpus",
   "original": "e01_0905",
   "page_count": 4,
   "order": 279,
   "p1": "905",
   "pn": "908",
   "abstract": [
    "In this paper, methodologies for effective speech recognition are considered along with evaluations of an NRL speech in noise corpus entitled SPINE. When speech is produced in adverse conditions that include high levels of noise, workload task stress, and Lombard effect, new challenges arise concerning how to best improve recognition performance. Here, we consider tradeoffs in (i) robust features, (ii) frontend noise suppression, (iii) model adaptation, and (iv) training and testing in the same conditions. The type of noise and recording conditions can significantly impact the type of signal processing and speech modeling methods that would be most effective in achieving robust speech recognition. We considered alternative frequency scales (M-MFCC, ExpoLog), feature processing (CMN, VCMN, LP-vs-FFT MFCCs), model adaptation (PMC), and combinations of gender dependent with gender independent models. For the purposes of achieving effective speech recognition performance, computational speed and availability of adaptation data greatly impacts final recognition performance. In particular, while reliable algorithm formulations for addressing specific types of distortion can improve recognition rates, these algorithms cannot reach their full potential without proper front-end algorithm data processing to direct compensation. While parallel banks of speech recognizers can improve recognition performance, their significant computational requirements can render the recognizer useless in actual speech applications.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-274"
  },
  "siu01_eurospeech": {
   "authors": [
    [
     "Manhung",
     "Siu"
    ],
    [
     "Yu-Chung",
     "Chan"
    ]
   ],
   "title": "Robust speech recognition against packet loss",
   "original": "e01_1095",
   "page_count": 4,
   "order": 280,
   "p1": "1095",
   "pn": "1098",
   "abstract": [
    "Recognizing speech transmitted over mobile or computer networks poses new challenges such as packet loss in transmission. Viterbi algorithm, the most common speech recognition approach, seaches for the most likely state sequence that explains all observation. However, because it implicitly sums the log observation probabilities, the resulting solution is sensitive to outlier frames. In this paper, we propose a robust approach that searches the state sequence that best explains x percent of the observation and is insensitive to the corruption of a limited number of frames. We evaluated the proposed algorithm on the TI-digits task. With 10% of the data loss, the proposed algorithm achieves improvement of 71.6% for isolated digit recognition and 32.2% for connected digit recognition.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-275"
  },
  "naito01_eurospeech": {
   "authors": [
    [
     "Masaki",
     "Naito"
    ],
    [
     "Shingo",
     "Kuroiwa"
    ],
    [
     "Tsuneo",
     "Kato"
    ],
    [
     "Tohru",
     "Shimizu"
    ],
    [
     "Norio",
     "Higuchi"
    ]
   ],
   "title": "Rapid CODEC adaptation for cellular phone speech recognition",
   "original": "e01_1099",
   "page_count": 4,
   "order": 281,
   "p1": "1099",
   "pn": "1102",
   "abstract": [
    "Along with the popularization of cellular phone, it becomes important issue to improve recognition accuracy for cellular phone speech input. However, the distortion caused by current low-bit rate speech corder is nonlinear. Therefore, it is difficult to compensate these distortion by only applying conventional CMN which assuming distortion as stationary linear transfer on spectrum domain. In this paper, to improve the accuracy of speech recognition over cellular-phone network, we investigate the use of CODEC-dependent acoustic model and rapid CODEC-adaptation using model selection based on maximum likelihood criterion. These method reduce degradation of recognition performance due to difference in CODEC by 33%.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-276"
  },
  "gallardoantolin01_eurospeech": {
   "authors": [
    [
     "Ascension",
     "Gallardo-Antolin"
    ],
    [
     "Carmen",
     "Pelaez-Moreno"
    ],
    [
     "Fernando",
     "Diaz-de-Maria"
    ]
   ],
   "title": "A robust front-end for ASR over IP snd GSM networks: an integrated scenario",
   "original": "e01_1103",
   "page_count": 4,
   "order": 282,
   "p1": "1103",
   "pn": "1106",
   "abstract": [
    "Both for the transmission over GSM and IP networks, voice must be encoded at the originating end and subsequently decoded at the receiving end. This lossy coding produces a quality deterioration, that though acceptable for a human being, seriously affects the performance of Automatic Speech Recognizers (ASR) when they are not specifically designed for operating under those conditions. The authors have already introduced and tested a new robust front-end which improves ASR performances by simulating both networks environments. Here we complete some previous results including realistic GSM models, compare both scenarios and put forward and integrated scenario where mobile GSM devices require the services of an ASR facility situated into an IP network.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-277"
  },
  "renevey01_eurospeech": {
   "authors": [
    [
     "Philippe",
     "Renevey"
    ],
    [
     "Rolf",
     "Vetter"
    ],
    [
     "Jens",
     "Krauss"
    ]
   ],
   "title": "Robust speech recognition using missing feature theory and vector quantization",
   "original": "e01_1107",
   "page_count": 4,
   "order": 283,
   "p1": "1107",
   "pn": "1110",
   "abstract": [
    "This paper addresses the problem of speech recognition in noisy conditions when low complexity is required like in embedded systems. In such systems, vector quantization is generally used to reduce the complexity of the recognition systems (e.g. HMMs). A novel approach for vector quantization based on the missing data theory is proposed. This approach allows to increase the robustness of the system against the noise perturbations with only a small increase of the computational requirements. The proposed algorithm is composed of two parts. The first part consists in dividing the spectral temporal features of the noisy signal into two subspaces: the unreliable (or missing) features and the reliable (or present) features. The second part of the proposed approach consists in defining a robust distance measure for vector quantization that compensates for the unreliable features. The proposed approach obtains similar results in noisy conditions than a more classical approach that consists in adapting the codebook of the vector quantization to the noisy conditions using model compensation. However the computation requirements are lower in the proposed approach and it is more suitable for a low complexity speech recognition system.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-278"
  },
  "ming01_eurospeech": {
   "authors": [
    [
     "Ji",
     "Ming"
    ],
    [
     "Peter",
     "Jancovic"
    ],
    [
     "Philip",
     "Hanna"
    ],
    [
     "Darryl",
     "Stewart"
    ]
   ],
   "title": "Modeling the mixtures of known noise and unknown unexpected noise for robust speech recognition",
   "original": "e01_1111",
   "page_count": 4,
   "order": 284,
   "p1": "1111",
   "pn": "1114",
   "abstract": [
    "Real-world noise may be a mixture of known or trainable noise and unknown unexpected noise. This paper investigates the combination of the conventional noise-reduction techniques with the probabilistic union model to deal with this type of mixed noise for robust speech recognition. In particular, we have developed a multi-environment system to remove the known or trainable acoustic mismatch across different environments. The novelty of this system, in contrast to other multi-environment models, is that the acoustic model for each environment is built upon the probabilistic union model, so that this system is also capable of accommodating further unknown unexpected noise within a specific environment. We have tested the new system for connected digit recognition in different environments, each involving an environment-specific noise and some unknown untrained noise. The results indicate that the new system offers significantly improved performance for the environments involving unknown additional noise, in comparison to a baseline multi-environment system.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-279"
  },
  "kawamura01_eurospeech": {
   "authors": [
    [
     "Takayoshi",
     "Kawamura"
    ],
    [
     "Kazuya",
     "Takeda"
    ],
    [
     "Fumitada",
     "Itakura"
    ]
   ],
   "title": "Robust speech recognition based on selective use of missing frequency band HMMs",
   "original": "e01_1115",
   "page_count": 4,
   "order": 285,
   "p1": "1115",
   "pn": "1118",
   "abstract": [
    "In this paper, we propose a multi-stream approach that selectively uses Missing Frequency Band HMMs (MFB-HMM) that is trained on the band-eliminated speech. This makes the model insensitive to the noise in the missing frequency band. With multiple MFB-HMMs of different missing frequency bands, the proposed recognition system is robust in various types of noise conditions. Recognition experiments show that the selective use of the MFB-HMMs is very effective in narrow band noise condition even if the noise is unstationary, however, the improvements of the performance to general noisy conditions, e.g. in-car noise and music sound, are not as high as in the narrow band noise case. The results of the experiments also show that the optimal selection of the MFB-HMM significantly improves the performance regardless of the type of the noise; therefore, the model selection measure is the key issue in this method.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-280"
  },
  "masudakatsuse01_eurospeech": {
   "authors": [
    [
     "Ikuyo",
     "Masuda-Katsuse"
    ]
   ],
   "title": "A new method for speech recognition in the presence of non-stationary, unpredictable and high-level noise",
   "original": "e01_1119",
   "page_count": 4,
   "order": 286,
   "p1": "1119",
   "pn": "1122",
   "abstract": [
    "We propose a new method for speech recognition in the presence of non-stationary, unpredictable and high-level noise by extending PreFEst (Predominant-F0 Estimation Method) which was developed to estimate melody and bass lines from music signals. The proposed method does not need to know about noise characteristics in advance and does not even estimate them in its process. A small set of evaluations demonstrates the feasibility of the method by showing a good performance even when the background noise is real and non-stationary noise and the SNR is less than 10 dB.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-281"
  },
  "kotnik01b_eurospeech": {
   "authors": [
    [
     "Bojan",
     "Kotnik"
    ],
    [
     "Zdravko",
     "Kacic"
    ],
    [
     "Bogomir",
     "Horvat"
    ]
   ],
   "title": "A computational efficient real time noise robust speech recognition based on improved spectral subtraction method",
   "original": "e01_1123",
   "page_count": 4,
   "order": 287,
   "p1": "1123",
   "pn": "1126",
   "abstract": [
    "In this paper, a speech enhancement method is presented, which uses spectral and time domain processing and achieves a trade-off between effective noise reduction and low computational load for real-time operations. First, a spectral subtraction method is used to reduce the effect of additive broadband noise on speech. Then, a novel weighting function is used to reduce a residual \"musical noise\" in time domain. This weighting function is a compound of a short-time zero crossing value and a short-time energy of speech signal. For evaluation of improvement of speech recognition the Slovenian SpeechDat FDB, the German SpeechDat FDB and SpeechDat-Car, as well as the Spanish SpeechDat FDB databases together with the HTK recognition toolkit were used. Word recognition accuracy in connected digits recognition task was improved by 8.7% for Slovenian FDB, by 5.1% for Spanish FDB, by 3.2% for German SpeechDat-Car, and by 2% for German SpeechDat FDB database.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-282"
  },
  "vlaj01_eurospeech": {
   "authors": [
    [
     "Damjan",
     "Vlaj"
    ],
    [
     "Zdravko",
     "Kacic"
    ],
    [
     "Bogomir",
     "Horvat"
    ]
   ],
   "title": "The use of noisy frame elimination and frequency spectrum magnitude reduction in noise robust speech recognition",
   "original": "e01_1127",
   "page_count": 4,
   "order": 288,
   "p1": "1127",
   "pn": "1130",
   "abstract": [
    "In this paper the procedure for feature vector extraction and the problems, which must be solved, by defining the feature vectors, which contain only the information about the speech signal are described. A new procedure of feature extraction which is based on the frame elimination and frequency spectrum reduction for the noisy part of the speech signal is proposed. For all tests the Slovenian telephone speech database SpeechDat II was used. The connected digits were used for both, training and testing. There were 800 speakers used for training and 200 for testing. The word recognition accuracy was increased for 3.1 percentage points with the new procedure, and this was achieved, when the number of Gaussian mixtures was four times smaller than with the ordinary method. The results obtained are especially encouraging for the systems where the size of the available memory and processing power are limited (for example, mobile phones).\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-283"
  },
  "chien01_eurospeech": {
   "authors": [
    [
     "Jen-Tzung",
     "Chien"
    ]
   ],
   "title": "Combined linear regression adaptation and Bayesian predictive classification for robust speech recognition",
   "original": "e01_1131",
   "page_count": 4,
   "order": 289,
   "p1": "1131",
   "pn": "1134",
   "abstract": [
    "The uncertainty in parameter estimation due to the adverse environments deteriorates the speech recognition performance. It becomes crucial to incorporate the parameter uncertainty into decision so that the classification robustness can be assured. In this paper, we propose a linear regression based Bayesian predictive classification (LRBPC) for robust speech recognition. This framework is constructed under the paradigm of linear regression adaptation of HMM's. Because the regression mapping between HMM's and adaptation data is ill posed, we properly characterize the uncertainty of regression parameters using a joint Gaussian distribution. A predictive distribution is derived to set up the LRBPC decision. Such decision is robust compared to the plug-in maximum a posteriori decision adopted in the maximum likelihood linear regression (MLLR). Since the specified distribution belongs to the conjugate prior family, the evolutionary hyperparameter is established. With the hyperparameter, the LRBPC achieves significantly better performance than MLLR adaptation in car speech recognition.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-284"
  },
  "hilger01_eurospeech": {
   "authors": [
    [
     "Florian",
     "Hilger"
    ],
    [
     "Hermann",
     "Ney"
    ]
   ],
   "title": "Quantile based histogram equalization for noise robust speech recognition",
   "original": "e01_1135",
   "page_count": 4,
   "order": 290,
   "p1": "1135",
   "pn": "1138",
   "abstract": [
    "This paper describes an approach to increase the noise robustness of automatic speech recognition systems by, transforming the signal after Mel scaled filtering, to make the cumulative density functions of the signal's values in recognition match the ones that where estimated on the training data. The cumulative density functions are approximated using a small number of quantiles. Recognition tests on several databases showed significant reductions of the word error rates. On a real life database recorded in driving cars with a large mismatch between the training and testing conditions the relative reductions of the word error rates where over 60%.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-285"
  },
  "yao01b_eurospeech": {
   "authors": [
    [
     "Kaisheng",
     "Yao"
    ],
    [
     "Kuldip K.",
     "Paliwal"
    ],
    [
     "Satoshi",
     "Nakamura"
    ]
   ],
   "title": "Sequential noise compensation by a sequential kullback proximal algorithm",
   "original": "e01_1139",
   "page_count": 4,
   "order": 291,
   "p1": "1139",
   "pn": "1142",
   "abstract": [
    "We present a sequential noise compensation method based on the sequential Kullback proximal algorithm, which uses the Kullback-Leibler divergence as a regularization function for the maximum likelihood estimation. The method is implemented as filters. In contrast to sequential noise compensation method based on the sequential EM algorithm, the convergence rate of the method and estimation error after convergence can be adjusted by a relaxation factor, where the sequential EM algorithm corresponds to the particular case of the presented algorithm. Through experiments on parameter estimation and speech recognition in noise, we verified the efficacy of the algorithm.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-286"
  },
  "koutras01_eurospeech": {
   "authors": [
    [
     "Athanasios",
     "Koutras"
    ],
    [
     "Evangelos",
     "Dermatas"
    ],
    [
     "George",
     "Kokkinakis"
    ]
   ],
   "title": "Blind speech separation of moving speakers using hybrid neural networks",
   "original": "e01_0997",
   "page_count": 4,
   "order": 292,
   "p1": "997",
   "pn": "1000",
   "abstract": [
    "In this paper we present a novel method for Blind Speech Separation of convolutive speech signals of moving speakers in highly reverberant rooms. The separation network used is a hybrid neural network, which performs separation of convolutive speech mixtures in the time domain, without any prior knowledge of the propagation media, based on the Maximum Likelihood Estimation (MLE) principle. The proposed method improves significantly (more than 13% in all adverse mixing situations) the performance of a phoneme-based continuous speech recognition system and therefore can be used as a front-end to separate simultaneous speech of speakers who are moving in reverberant rooms.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-287"
  },
  "herbordt01_eurospeech": {
   "authors": [
    [
     "W.",
     "Herbordt"
    ],
    [
     "H.",
     "Buchner"
    ],
    [
     "W.",
     "Kellermann"
    ]
   ],
   "title": "Computationally efficient frequency-domain combination of acoustic echo cancellation and robust adaptive beamforming",
   "original": "e01_1001",
   "page_count": 4,
   "order": 293,
   "p1": "1001",
   "pn": "1004",
   "abstract": [
    "For hands-free acoustical human/machine interfaces, e. g. for automatic speech recognition or teleconferencing systems, microphone arrays using robust Generalized Sidelobe Cancellers (GSCs) in conjunction with acoustic echo cancellation (AEC) can be efficiently applied for optimum communication. This contribution devises a new structure for combining AEC and GSC. It reduces the computational complexity by more than a factor of ten relative to a time-domain arrangement, increases convergence speed, and preserves positive synergies.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-288"
  },
  "seltzer01_eurospeech": {
   "authors": [
    [
     "Michael L.",
     "Seltzer"
    ],
    [
     "Bhiksha",
     "Raj"
    ]
   ],
   "title": "Calibration of microphone arrays for improved speech recognition",
   "original": "e01_1005",
   "page_count": 4,
   "order": 294,
   "p1": "1005",
   "pn": "1008",
   "abstract": [
    "We present a new microphone array calibration algorithm specifically designed for speech recognition. Currently, microphone-array-based speech recognition is performed in two independent stages: array processing, and then recognition. Array processing algorithms designed for speech enhancement process the waveforms before recognition. These systems make the assumption that the best array processing methods will result in the best recognition performance. However, recognition systems interpret a set of features extracted from the speech waveform, not the waveform itself. In our calibration method, the filter parameters of a filter-and-sum array processing scheme are optimized to maximize the likelihood of the recognition features extracted from the resulting output signal. By incorporating the speech recognition system into the design of the array processing algorithm, we are able to achieve improvements in word error rate of up to 37% over conventional array processing methods on both simulated and actual microphone array data.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-289"
  },
  "koutras01b_eurospeech": {
   "authors": [
    [
     "Athanasios",
     "Koutras"
    ],
    [
     "Evangelos",
     "Dermatas"
    ],
    [
     "George",
     "Kokkinakis"
    ]
   ],
   "title": "Improving simultaneous speech recognition in real room environments using overdetermined blind source separation",
   "original": "e01_1009",
   "page_count": 4,
   "order": 295,
   "p1": "1009",
   "pn": "1012",
   "abstract": [
    "In this paper we present a novel solution to the Overdetermined Blind Speech Separation (OBSS) problem for improving speech recognition accuracy of N simultaneous speakers in real room environments using M (M>N) microphones. The proposed OBSS system uses basic NxN Blind Speech Separation networks that process in parallel all different combinations of the available mixture signals in the frequency domain, resulting to lower computational complexity and faster convergence. Extensive experiments using an array of two to ten microphones and two simultaneous speakers in a simulated real room, showed that when the number of the microphones increases beyond two, the separation performance is improved and the phoneme recognition accuracy of an HMM based decoder increases drastically (more than 6%). Therefore, the introduction of more microphones than speakers is justified in order to improve speech recognition accuracy in multi simultaneous speaker environments.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-290"
  },
  "asano01_eurospeech": {
   "authors": [
    [
     "Futoshi",
     "Asano"
    ],
    [
     "Masataka",
     "Goto"
    ],
    [
     "Katunobu",
     "Itou"
    ],
    [
     "Hideki",
     "Asoh"
    ]
   ],
   "title": "Real-time sound source localization and separation system and its application to automatic speech recognition",
   "original": "e01_1013",
   "page_count": 4,
   "order": 296,
   "p1": "1013",
   "pn": "1016",
   "abstract": [
    "A real-time sound localization/separation system for near-field sound sources was constructed and evaluated in a real office environment. As for the sound localization, the experimental results showed that the direction of the two sources was estimated with high accuracy while the range of the sources was estimated with moderate accuracy. As for the sound separation, a recognition rate of 70% for an on-line recognizer on a network and of 90% for an off-line recognizer were achieved, respectively.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-291"
  },
  "lee01b_eurospeech": {
   "authors": [
    [
     "Joohun",
     "Lee"
    ],
    [
     "JinYoung",
     "Kim"
    ]
   ],
   "title": "An efficient lipreading method using the symmetry of lip",
   "original": "e01_1019",
   "page_count": 4,
   "order": 297,
   "p1": "1019",
   "pn": "1022",
   "abstract": [
    "An efficient method to reduce the amount of feature data for real-time automatic image-transform-based lipreading is proposed. Image-transform-based approach obtaining a compressed representation of image pixel values of speaker's mouth is reported to show superior lipreading performance. However, since this approach produces many feature vectors relevant to lip information, it requires much computation time for lipreading even when principal component analysis (PCA) is applied. To reduce the computational load efficiently, we propose an algorithm that utilizes the symmetry of the lip. The proposed method reduces the amount of required feature vectors up to 51% compared to the original one. Also, it improves the recognition rates by compensating the variation of illumination. With our database (22 words, 70 talkers) recorded in a natural environment, our method achieved an accuracy of 53.5% for visual-only speaker independent word recognition task. The extracted features are modeled by hidden Markov models with Gaussian mixture distributions.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-292"
  },
  "heckmann01_eurospeech": {
   "authors": [
    [
     "Martin",
     "Heckmann"
    ],
    [
     "Thorsten",
     "Wild"
    ],
    [
     "Frédéric",
     "Berthommier"
    ],
    [
     "Kristian",
     "Kroschel"
    ]
   ],
   "title": "Comparing audio- and a-posteriori-probability-based stream confidence measures for audio-visual speech recognition",
   "original": "e01_1023",
   "page_count": 4,
   "order": 298,
   "p1": "1023",
   "pn": "1026",
   "abstract": [
    "During the fusion of audio and video information for speech recognition, the estimation of the reliability of the noise affected audio channel is crucial to get meaningful recognition results. In this paper we compare two types of reliability measures. One is the use of the statistics of the phoneme a-posteriori probabilities and the other is the analysis of the audio signal itself. We implemented the entropy and the dispersion of the probabilities and, from the audio-based criteria, the so called Voicing Index. To test the criteria a hybrid ANN/HMM audio-visual recognition system was used and 5 different types of noise at 12 SNR levels each were added to the audio signal. The best sigmoidal fit for each criterion between the fusion parameter and the value of the criterion over all noise types and SNR values was performed. The resulting individual errors and the corresponding averaged relative errors are given.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-293"
  },
  "potamianos01_eurospeech": {
   "authors": [
    [
     "Gerasimos",
     "Potamianos"
    ],
    [
     "Chalapathy",
     "Neti"
    ],
    [
     "Giridharan",
     "Iyengar"
    ],
    [
     "Eric",
     "Helmuth"
    ]
   ],
   "title": "Large-vocabulary audio-visual speech recognition by machines and humans",
   "original": "e01_1027",
   "page_count": 4,
   "order": 299,
   "p1": "1027",
   "pn": "1030",
   "abstract": [
    "We compare automatic recognition with human perception of audiovisual speech, in the large-vocabulary, continuous speech recognition (LVCSR) domain. Specifically, we study the benefit of the visual modality for both machines and humans, when combined with audio degraded by speech-babble noise at various signal-to-noise ratios (SNRs). We first consider an automatic speechreading system with a pixel based visual front end that uses feature fusion for bimodal integration, and we compare its performance with an audio-only LVCSR system. We then describe results of human speech perception experiments, where subjects are asked to transcribe audio-only and audio-visual utterances at various SNRs. For both machines and humans, we observe approximately a 6 dB effective SNR gain compared to the audio-only performance at 10 dB, however such gains significantly diverge at other SNRs. Furthermore, automatic audio-visual recognition outperforms human audio-only speech perception at low SNRs.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-294"
  },
  "daubias01_eurospeech": {
   "authors": [
    [
     "Philippe",
     "Daubias"
    ],
    [
     "Paul",
     "Deleglise"
    ]
   ],
   "title": "Evaluation of an automatically obtained shape and appearance model for automatic audio visual speech recognition",
   "original": "e01_1031",
   "page_count": 4,
   "order": 300,
   "p1": "1031",
   "pn": "1034",
   "abstract": [
    "In this paper, we first present a shape and appearance model for Audio-Visual Automatic Speech Recognition. The shape model is a template (mean shape) and a set of deformation vectors to transform it into any possible shape. The global appearance model is a neural network trained to classify 5*5 colour image blocks as from skin, lips or inside of mouth. Both parts of this model were built automatically (without handlabelling). Appearance model was built using speech bimodality (acoustic information). We then propose several measures for quality evaluation of lip location. Finally, we show the classification results obtained using a hand-labelled and two automatically built appearance models of the lips.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-295"
  },
  "pelachaud01_eurospeech": {
   "authors": [
    [
     "C.",
     "Pelachaud"
    ],
    [
     "E.",
     "Magno-Caldognetto"
    ],
    [
     "C.",
     "Zmarich"
    ],
    [
     "Piero",
     "Cosi"
    ]
   ],
   "title": "An approach to an Italian talking head",
   "original": "e01_1035",
   "page_count": 4,
   "order": 301,
   "p1": "1035",
   "pn": "1038",
   "abstract": [
    "Our goal is to create a natural talking face with, in particular, lip-readable movements. Based on real data extracted from an Italian speaker with the ELITE system, we have approximated the data using radial basis functions. In this paper we present our 3D facial model based on MPEG-4 standard and our computational model of lip movements for Italian. Our experiment is based on some phonetic-phonological considerations on the parameters defining labial orifice, and on identification tests of visual articulatory movements.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-296"
  },
  "eriksson01b_eurospeech": {
   "authors": [
    [
     "Anders",
     "Eriksson"
    ],
    [
     "Gerrit",
     "Bloothooft"
    ]
   ],
   "title": "Education on the web: launch of three new websites",
   "original": "e01_1143",
   "page_count": 0,
   "order": 302,
   "p1": "0",
   "pn": "",
   "abstract": [
    "The ISCA Special Interest Group on Education will launch three websites during the presentation: (1) JEWELS (Joint European Website for Education in Language and Speech, sponsored by ELSNET and supported by ISCA, EACL and Socrates projects) which provides information on contents of studies, educational materials and tools, links to sites and courses, and information on European Educational support programmes, (2) a new professional website of the European Masters in Language and Speech, and (3) the website of EduSIG itself.\n",
    ""
   ]
  },
  "hirst01_eurospeech": {
   "authors": [
    [
     "Daniel",
     "Hirst"
    ],
    [
     "Bernard",
     "Bel"
    ],
    [
     "Nick",
     "Campbell"
    ]
   ],
   "title": "SProSIG: a special interest group on speech prosody",
   "original": "e01_1144",
   "page_count": 0,
   "order": 303,
   "p1": "0",
   "pn": "",
   "abstract": [
    "This presentation will present the activities of SProSIG since its creation in January 2000 including the setting up of an email list, dedicated web pages, and the planning of an International Conference on Speech Prosody (Speech Prosody 2002) to be held in Aix en Provence in April 2002. A number of ideas for future activities will also be presented for discussion.\n",
    ""
   ]
  },
  "bonastre01_eurospeech": {
   "authors": [
    [
     "Jean-François",
     "Bonastre"
    ],
    [
     "Ivan",
     "Magrin-Chagnolleau"
    ],
    [
     "Stephan",
     "Euler"
    ],
    [
     "François",
     "Pellegrino"
    ],
    [
     "Régine",
     "André-Obrecht"
    ],
    [
     "John S.",
     "Mason"
    ],
    [
     "Frédéric",
     "Bimbot"
    ]
   ],
   "title": "SPeaker and Language Characterization (spLC): a special interest group (SIG) of ISCA",
   "original": "e01_1145",
   "page_count": 4,
   "order": 304,
   "p1": "1145",
   "pn": "1148",
   "abstract": [
    "Last year, SPLC - an ISCA Special Interest Group centered around Speaker and Language Characterization born. The aims of this paper are to present the SPLC SIG, its the objectives and the work done during the first year.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-297"
  },
  "campbell01b_eurospeech": {
   "authors": [
    [
     "Nick",
     "Campbell"
    ],
    [
     "Wolfgang",
     "Hess"
    ],
    [
     "Bernd",
     "Möbius"
    ],
    [
     "Jan van",
     "Santen"
    ]
   ],
   "title": "The ISCA special interest group on speech synthesis",
   "original": "e01_1149",
   "page_count": 4,
   "order": 305,
   "p1": "1149",
   "pn": "1152",
   "abstract": [
    "This paper describes the constitution and activities of the ISCA Speech Synthesis Special Interest Group, SynSIG. It summarises past achievements and suggests ways in which future development could be maintained. The aims of the Special Interest Group on Speech Synthesis are to promote the study and diffusion of knowledge about speech synthesis in general, in a number of ways including: dedicated web pages, a mailing list, a bibliographic database, organisation of workshops on specific themes, exchange of students, and helping to co-ordinate sessions on speech synthesis in international conferences and workshops. The international and multi-disciplinary nature of the SIG also provides a means for diffusing information both to and from the different research communities involved in the synthesis of various languages.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-298"
  },
  "massaro01_eurospeech": {
   "authors": [
    [
     "Dominic W.",
     "Massaro"
    ]
   ],
   "title": "Auditory visual speech processing",
   "original": "e01_1153",
   "page_count": 4,
   "order": 306,
   "p1": "1153",
   "pn": "1156",
   "abstract": [
    "This paper provides an overview of the developments in Auditory Visual Speech Processing, a Special Interest Group within Eurospeech. I hope that this discussion will be informative and useful to readers in a variety of fields, including psychology, speech science, animation, psycholinguistics, human-machine interaction, hearing-impaired communication, and numerous other fields which also share in this fruitful intersection.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-299"
  },
  "bimbot01_eurospeech": {
   "authors": [
    [
     "Frédéric",
     "Bimbot"
    ],
    [
     "Jean-Francois",
     "Bonastre"
    ]
   ],
   "title": "The specificity of French speech processing (no proceedings paper)",
   "original": "e01_1344",
   "page_count": 0,
   "order": 307,
   "p1": "0",
   "pn": "",
   "abstract": [
    "A presentation of the SIG Groupe Francophone de la Communication Parleé.\n",
    ""
   ]
  },
  "dybkjr01_eurospeech": {
   "authors": [
    [
     "Laila",
     "Dybkjær"
    ]
   ],
   "title": "SIGdial - special interest group on discourse and dialogue",
   "original": "e01_1345",
   "page_count": 4,
   "order": 308,
   "p1": "1345",
   "pn": "1348",
   "abstract": [
    "This paper describes the ACL and ISCA Special Interest Group on Discourse and Dialogue (SIGdial). Objective, activities, people, and plans are presented.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-300"
  },
  "delcloque01_eurospeech": {
   "authors": [
    [
     "Philippe",
     "Delcloque"
    ]
   ],
   "title": "Integrating speech technology in language learning: an overview of the activities of inSTIL",
   "original": "e01_1349",
   "page_count": 4,
   "order": 309,
   "p1": "1349",
   "pn": "1352",
   "abstract": [
    "This presentation describes the activities of a Special Interest Group which focuses on the \"Integration of Speech Technology in (Language) traditions: Computer Assisted Language Learning (CALL) and Speech Science & Engineering. The history of the SIG is retraced since its foundation as CAPITAL (see later) in Edinburgh around 1994, past, present activities of the SIG are reviewed, its events, its publications, the origin and diversity of its membership, etc. Future activities are also mentioned. The success of the group has surprised its early members who thought that there were part of a very small minority group, this is a historic year for the SIG for two reasons, the first because it will write, present and publish the first \"Illustrated History of Speech Technology in Language Learning\" and second because it will be present for the first time at a EUROSPEECH conference.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-301"
  },
  "nadeu01_eurospeech": {
   "authors": [
    [
     "Climent",
     "Nadeu"
    ],
    [
     "Donncha",
     "ÓCróinín"
    ],
    [
     "Bojan",
     "Petek"
    ],
    [
     "Kepa",
     "Sarasola"
    ],
    [
     "Briony",
     "Williams"
    ]
   ],
   "title": "ISCA SALTMIL SIG: speech and language technology for minority languages",
   "original": "e01_1353",
   "page_count": 4,
   "order": 310,
   "p1": "1353",
   "pn": "1556",
   "abstract": [
    "This paper presents International Speech Communication Association (ISCA) Special Interest Group (SIG, http://www.iscaspeech. org/sig.html) on Speech And Language Technology for MInority Languages (SALTMIL). Overview of the group's mission, including its past and present activities are presented and discussed.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-302"
  },
  "chen01g_eurospeech": {
   "authors": [
    [
     "Weijun",
     "Chen"
    ],
    [
     "Fuzong",
     "Lin"
    ],
    [
     "Jianmin",
     "Li"
    ],
    [
     "Bo",
     "Zhang"
    ]
   ],
   "title": "Training prosodic phrasing rules for Chinese TTS systems",
   "original": "e01_1159",
   "page_count": 4,
   "order": 311,
   "p1": "1159",
   "pn": "1162",
   "abstract": [
    "This paper describes several experiments designed to train prosodic phrasing models for Chinese TTS systems and to investigate the underlying rules that control Chinese prosody. First, we collected 559 sentences from news programs and built a large corpus for modeling Chinese prosody. Second, we selected 20 features and used classification and regression trees (CART) and transformational rule-based learning (TRBL) techniques to generate phrasing rules automatically. Lastly, we propose a computer aided error-driven method of designing rule templates, and integrate it into the TRBL algorithm. The experimental results show that we achieve a high success rate of 94.5%, and we also get a set of well comprehensible rule templates which may give us insights into the relationship between Chinese syntax and prosody.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-303"
  },
  "heggtveit01_eurospeech": {
   "authors": [
    [
     "Per Olav",
     "Heggtveit"
    ],
    [
     "Jon Emil",
     "Natvig"
    ]
   ],
   "title": "Intonation modelling with a lexicon of natural F0 contours",
   "original": "e01_1163",
   "page_count": 4,
   "order": 312,
   "p1": "1163",
   "pn": "1166",
   "abstract": [
    "We describe a new approach for generating Norwegian intonation in text to speech synthesis. The method is based on a phonological representation of utterances. The overall f0 contour of an utterance is synthesised by concatenation of stored f0 contours corresponding to accent units. Candidate accent units are found by searching a lexicon derived from natural speech and selecting the unit that is the best match with respect to the properties of the target accent units of the utterance to be synthesised. A formal subjective test confirms that the new approach leads to more natural speech than a former rule based method, but the quality is still inferior to intonation copied from natural speech.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-304"
  },
  "silverman01_eurospeech": {
   "authors": [
    [
     "Kim E. A.",
     "Silverman"
    ],
    [
     "Jerime R.",
     "Bellegarda"
    ],
    [
     "Kevin A.",
     "Lenzo"
    ]
   ],
   "title": "Smooth contour estimation in data-driven pitch modelling",
   "original": "e01_1167",
   "page_count": 4,
   "order": 313,
   "p1": "1167",
   "pn": "1170",
   "abstract": [
    "Apple's next-generation text-to-speech system in MacOS X uses a superpositional pitch model, comprising a relatively smooth underlying F0 contour and a separate contribution from the influence of the phonetic segments. This paper focuses on the data-driven modelling of the underlying contour, based on electroglottographic signals obtained from a corpus of reiterant speech. F0 extraction from such signals leads to more accurate characteristic shapes, as objectively illustrated by a typically low mean absolute frequency deviation (between 2 and 3 Hz) between original and synthetic F0 contours. This in turn supports a better (both more complete and more realistic) model of F0 behavior. Experimental results illustrate the improved prosodic representation resulting from this F0 model.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-305"
  },
  "saito01_eurospeech": {
   "authors": [
    [
     "Takashi",
     "Saito"
    ],
    [
     "Masaharu",
     "Sakamoto"
    ]
   ],
   "title": "Generating F0 contours by statistical manipulation of natural F0 shapes",
   "original": "e01_1171",
   "page_count": 4,
   "order": 314,
   "p1": "1171",
   "pn": "1174",
   "abstract": [
    "This paper proposes a method of generating F0 contours from natural F0 segmental shapes for speech synthesis. The extracted shapes of F0 units are basically kept unchanged, by eliminating any averaging operation in the analysis phase and minimizing modification operations in the synthesis phase. The use of \"kept-unchanged\" F0 shapes has a great potential to incorporate a wide variety of speaking styles in the same framework, including not only read-out speech, but also dialogue and emotive speech. A linear-regression statistical model is proposed here to \"manipulate\" the stored raw F0 shapes for building them up to a sentential F0 contour. Through experimental evaluations, the proposed model turns out to provide a robust F0 contour prediction. By using the model, linguistically derived information of a sentence can be directly mapped, in a purely data-driven manner, to acoustic F0 values of the sentential intonation contour for a trained speaker.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-306"
  },
  "hirschberg01_eurospeech": {
   "authors": [
    [
     "Julia",
     "Hirschberg"
    ],
    [
     "Owen",
     "Rambow"
    ]
   ],
   "title": "Learning prosodic features using a tree representation",
   "original": "e01_1175",
   "page_count": 4,
   "order": 315,
   "p1": "1175",
   "pn": "1178",
   "abstract": [
    "We describe experiments designed to learn associations between two types of intonational features, pitch accent and phrasing, from a tree-based corpus annotated with various intonational and syntactic features, for a concept-to-speech system. We show that using novel tree-based features improves the quality of boundary prediction over using only the linear order-based features normally used in text-to-speech.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-307"
  },
  "gurbuz01_eurospeech": {
   "authors": [
    [
     "Sabri",
     "Gurbuz"
    ],
    [
     "Eric K.",
     "Patterson"
    ],
    [
     "Zekeriya",
     "Tufekci"
    ],
    [
     "John N.",
     "Gowdy"
    ]
   ],
   "title": "Lip-reading from parametric lip contours for audio- visual speech recognition",
   "original": "e01_1181",
   "page_count": 4,
   "order": 316,
   "p1": "1181",
   "pn": "1184",
   "abstract": [
    "This paper describes the incorporation of a visual lip tracking and lipreading algorithm that utilizes the affine-invariant Fourier descriptors from parametric lip contours to improve the audio-visual speech recognition systems. The audio-visual speech recognition system presented here uses parallel hidden Markov models (HMMs), where a joint decision, using an optimal decision rule, is made after processing. This work describes the extraction of affine-invariant Fourier descriptors (AI-FDs) from parametric lip contour data. Finally, this work validates the use of optimal weight selection, which is based on the noise type and signal-to-noise ratio (SNR) for joint audio-visual automatic speech recognition (JAV-ASR).\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-308"
  },
  "lucey01_eurospeech": {
   "authors": [
    [
     "Simon",
     "Lucey"
    ],
    [
     "Sridha",
     "Sridharan"
    ],
    [
     "Vinod",
     "Chandran"
    ]
   ],
   "title": "An investigation of HMM classifier combination strategies for improved audio-visual speech recognition",
   "original": "e01_1185",
   "page_count": 4,
   "order": 317,
   "p1": "1185",
   "pn": "1188",
   "abstract": [
    "The combining of independent audio and visual HMM classifiers (late integration) has been shown to out perform the combination of audio and visual features in a single HMM classifier (early integration) when either or both modalities are presented with distortion for the task of speech recognition. Theoretical foundations for the optimal combination of these audio and video classifiers are still unclear. In this paper a number of strategies for combining these classifiers are investigated. An argument for using a hybrid of the sum and product rules is made based on empirical, theoretical and heuristic evidence.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-309"
  },
  "bernsen01_eurospeech": {
   "authors": [
    [
     "Niels Ole",
     "Bernsen"
    ],
    [
     "Laila",
     "Dybkjær"
    ]
   ],
   "title": "Combining multi-party speech and text exchanges over the internet",
   "original": "e01_1189",
   "page_count": 4,
   "order": 318,
   "p1": "1189",
   "pn": "1192",
   "abstract": [
    "Bilateral or group text chatting over the Internet has become a favoured pastime for many people across the world. Yet it would seem that, in general, text chat is a severely impoverished mode of on-line communication compared to, e.g., fully situated human-human spoken conversation, video conferencing, or even speaking over the telephone. This paper explores what happens when on-line multi-speaker conversation over the Internet is added to text chat, creating what may become a widespread mode of communication in near future. The system used is called Magic Lounge. The paper presents rather clear-cut results on the respective communicative roles of speech and text chat from a series of user tests with the system in which different groups of users performed scenarios designed to explore the combined use of text chat and speech. The results reported may generalise to a wide range of applications which combine text and spoken information representation.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-310"
  },
  "nakadai01_eurospeech": {
   "authors": [
    [
     "Kazuhiro",
     "Nakadai"
    ],
    [
     "Ken-ichi",
     "Hidai"
    ],
    [
     "Hiroshi G.",
     "Okuno"
    ],
    [
     "Hiroaki",
     "Kitano"
    ]
   ],
   "title": "Real-time multiple speaker tracking by multi-modal integration for mobile robots",
   "original": "e01_1193",
   "page_count": 4,
   "order": 319,
   "p1": "1193",
   "pn": "1196",
   "abstract": [
    "In this paper, real-time multiple speaker tracking is addressed, because it is essential in robot perception and human-robot social interaction. The difficulty lies in treating a mixture of sounds, occlusion (some talkers are hidden) and real-time processing. Our approach consists of three components; (1) the extraction of the direction of each speaker by using interaural phase difference and interaural intensity difference, (2) the resolution of each speaker's direction by multi-modal integration of audition, vision and motion with canceling inevitable motor noises in motion in case of an unseen or silent speaker, and (3) the distributed implementation to three PCs connected by TCP/IP network to attain realtime processing. As a result, we attain robust real-time speaker tracking with 200 ms delay in a non-anechoic room, even when multiple speakers exist and the tracking person is visually occluded.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-311"
  },
  "nitta01_eurospeech": {
   "authors": [
    [
     "Tsuneo",
     "Nitta"
    ],
    [
     "Kouichi",
     "Katsurada"
    ],
    [
     "Hirobumi",
     "Yamada"
    ],
    [
     "Yusaku",
     "Nakamura"
    ],
    [
     "Satoshi",
     "Kobayashi"
    ]
   ],
   "title": "XISL: an attempt to separate multimodal interactions from XML contents",
   "original": "e01_1197",
   "page_count": 4,
   "order": 320,
   "p1": "1197",
   "pn": "1200",
   "abstract": [
    "In this paper we outline a multimodal interaction description language XISL (Extensible Interaction-Sheet Language) that is developed to describe multimodal interactions (MMI), and to separate the description of interactions from XML contents. XISL makes an XML document independent of interactions that may differ between each terminal, and so enables such seamless services as web-browsing to be constructed easily. Since XISL also provides various combinatorial usage of modalities, a developer can describe a MMI scenario easily. We implemented an interpreter of XISL on prototype systems for multimedia lectures with different types of MMI and proved the viability of XISL by experiments using the systems.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-312"
  },
  "gunawardana01_eurospeech": {
   "authors": [
    [
     "Asela",
     "Gunawardana"
    ],
    [
     "William",
     "Byrne"
    ]
   ],
   "title": "Discriminative speaker adaptation with conditional maximum likelihood linear regression",
   "original": "e01_1203",
   "page_count": 4,
   "order": 321,
   "p1": "1203",
   "pn": "1206",
   "abstract": [
    "We present a simplified derivation of the extended Baum-Welch procedure, which shows that it can be used for Maximum Mutual Information (MMI) of a large class of continuous emission density hidden Markov models (HMMs). We use the extended Baum-Welch procedure for discriminative estimation of MLLR-type speaker adaptation transformations. The resulting adaptation procedure, termed Conditional Maximum Likelihood Linear Regression (CMLLR), is used successfully for supervised and unsupervised adaptation tasks on the Switchboard corpus, yielding an improvement over MLLR. The interaction of unsupervised CMLLR with segmental minimum Bayes risk lattice voting procedures is also explored, showing that the two procedures are complimentary.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-313"
  },
  "kenny01_eurospeech": {
   "authors": [
    [
     "Patrick",
     "Kenny"
    ],
    [
     "Gilles",
     "Boulianne"
    ],
    [
     "Pierre",
     "Dumouchel"
    ]
   ],
   "title": "What is the best type of prior distribution for EMAP speaker adaptation?",
   "original": "e01_1207",
   "page_count": 4,
   "order": 322,
   "p1": "1207",
   "pn": "1210",
   "abstract": [
    "There are two types of prior distribution that can be viewed as natural for extended MAP (or EMAP) speaker adaptation. One arises from modeling the correlations between speakers (assumed to be constant across HMM Gaussians) and the other from modeling the correlations between HMM Gaussians (assumed to be constant across speakers). In this paper we present new results establishing the usefulness of correlations of the first type for speaker adaptation and we outline a tensor product construction which enables both types of correlation to be integrated in a common mathematical framework. We also present the results of some experiments which suggest that the two types of correlation are equally effective for speaker adaptation and that there is no incremental improvement to be gained by modeling both of them simultaneously.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-314"
  },
  "kim01d_eurospeech": {
   "authors": [
    [
     "Yoon",
     "Kim"
    ]
   ],
   "title": "Maximum-likelihood affine cepstral filtering (MLACF) technique for speaker normalization",
   "original": "e01_1211",
   "page_count": 4,
   "order": 323,
   "p1": "1211",
   "pn": "1214",
   "abstract": [
    "We present a novel technique of minimizing the acoustic variability of speakers by transforming the features extracted from the speaker's data to better fit the recognition model. The concept of maximum-likelihood affine cepstral filtering (MLACF) will be introduced for feature transformation, along with solutions for the transformation parameters that maximize the likelihood of the test data with respect to a given recognition model. It is shown that for log-concave distributions, the solution of the MLACF problem can be obtained using convex programming. HMM-based digit recognition on the TIDIGITS database is presented to demonstrate the flexibility of the transformation in compensating for large acoustic mismatches between the speakers in the training and test database. In addition, it will be shown that the technique requires estimation of far fewer transformation parameters compared to existing techniques, thus allowing fast, real-time compensation.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-315"
  },
  "zhou01_eurospeech": {
   "authors": [
    [
     "Bowen",
     "Zhou"
    ],
    [
     "John H. L.",
     "Hansen"
    ]
   ],
   "title": "A novel algorithm for rapid speaker adaptation based on structural maximum likelihood eigenspace mapping",
   "original": "e01_1215",
   "page_count": 4,
   "order": 324,
   "p1": "1215",
   "pn": "1218",
   "abstract": [
    "In this paper, we propose a novel algorithm for rapid speaker adaptation based on our Structural Maximum Likelihood Eigenspace Mapping (SMLEM). The proposed method constructs a binary-tree structured hierarchical Speaker Independent (SI) eigenspace at different levels from well-trained SI system models, and then dynamically constructs a new set of speaker dependent (SD) eigenspaces at corresponding levels, according to the availability of incoming adaptation data. By mapping the mixture Gaussian components from a SI eigenspace to SD eigenspaces in a maximum likelihood manner, the SI models are adapted towards SD models (EM algorithm is used to derive the eigenspace bias). Compared with conventional MLLR, the proposed algorithm is both computationally cheaper and more effective when only a very small amount (from 5 to 15 seconds) of adaptation data is available. In our simulations using the DARPA WSJ Spoke3 corpus, an average of 10.5% relative reduction in WER was achieved over MLLR adaptation when using 5 seconds data for adaptation.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-316"
  },
  "yoshizawa01_eurospeech": {
   "authors": [
    [
     "Shinichi",
     "Yoshizawa"
    ],
    [
     "Akira",
     "Baba"
    ],
    [
     "Kanako",
     "Matsunami"
    ],
    [
     "Yuichirou",
     "Mera"
    ],
    [
     "Miichi",
     "Yamada"
    ],
    [
     "Akinobu",
     "Lee"
    ],
    [
     "Kiyohiro",
     "Shikano"
    ]
   ],
   "title": "Evaluation on unsupervised speaker adaptation based on sufficient HMM statictics of selected speakers",
   "original": "e01_1219",
   "page_count": 4,
   "order": 325,
   "p1": "1219",
   "pn": "1222",
   "abstract": [
    "This paper describes an efficient method of unsupervised speaker adaptation. This method is based on (1) selecting a subset of speakers who are acoustically close to a test speaker, and (2) calculating adapted model parameters according to the previously stored sufficient statistics of the selected speakers' data. In this method, only a few unsupervised test speaker's data are necessary for the adaptation. Also, by using the sufficient HMM statistics of the selected speakers' data, a quick adaptation can be done. Compared with a pre-clustering method, the proposed method can obtain a more optimal cluster because the clustering result is determined according to test speaker's data on-line. Experimental results show that the proposed method attains better improvement than MLLR from the speaker-independent model. The proposed method is evaluated in details and discussed.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-317"
  },
  "lei01_eurospeech": {
   "authors": [
    [
     "Jia",
     "Lei"
    ],
    [
     "Xu",
     "Bo"
    ]
   ],
   "title": "A novel target-driven MLLR adaptation algorithm with multi-layer structure",
   "original": "e01_1225",
   "page_count": 4,
   "order": 326,
   "p1": "1225",
   "pn": "1228",
   "abstract": [
    "This paper presents a novel target-driven MLLR adaptation algorithm with multiply layer structure, which is based on the thorough analysis of MLLR using the generation of regression class trees. The new algorithm is constructed on the target-driven principal. It generates the regression class dynamically, basing on the outcome of the former MLLR transformation. The regression classes is defined in order to have the maximizing increase of the auxiliary function, which is in proportional to the likelihood of the occurrence of the adaptation data. Because of the new algorithm's special transformation structure, computation load in performing transformation is much reduced. In comparison with the conventional MLLR using the generation of regression class trees, the new algorithm give a further error reduction 10% and has only half computation time consuming.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-318"
  },
  "wallhoff01_eurospeech": {
   "authors": [
    [
     "Frank",
     "Wallhoff"
    ],
    [
     "Daniel",
     "Willett"
    ],
    [
     "Gerhard",
     "Rigoll"
    ]
   ],
   "title": "Scaled likelihood linear regression for hidden Markov model adaptation",
   "original": "e01_1229",
   "page_count": 4,
   "order": 327,
   "p1": "1229",
   "pn": "1232",
   "abstract": [
    "In the context of continuous Hidden Markov Model (HMM) based speech-recognition, linear regression approaches have become popular to adapt the acoustic models to the specific speaker's characteristics. The well known Maximum Likelihood Linear Regression (MLLR) and Maximum A Posteriori Linear Regression (MAPLR) are just two of them, which differ primarily in the training objective they are maximizing. However, besides the approaches mentioned above there exists another known training objective which is the Maximum Mutual Information (MMI). By combining this MMI-approach with the linear regression of the HMM's mean values, our research group developed a new adaptation technique that we call Scaled Likelihood Linear Regression (SLLR). In this approach, the distance of the correct model sequence against the wrong ones is discriminated framewise. Like all techniques using MMI objectives, this adaptation is computationally very expensive compared to techniques using ordinary ML based objectives. This paper therefore addresses the problem of an appropriate approximation technique to speed up this adaptation approach, by pruning the computation for tiny values in the discrimination objective. To further explore the potential of this adaptation technique and its approximation, the performance is measured on the LVCSR-system DUDeutsch developed by our research group at the Duisburg University and additionally on the 1993 WSJ adaptation tests of native and nonnative speakers for the supervised case.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-319"
  },
  "myrvoll01_eurospeech": {
   "authors": [
    [
     "Tor Andre",
     "Myrvoll"
    ],
    [
     "Kuldip K.",
     "Paliwal"
    ],
    [
     "Torbjørn",
     "Svendsen"
    ]
   ],
   "title": "Fast adaptation using constrained affine transformations with hierarchical priors",
   "original": "e01_1233",
   "page_count": 4,
   "order": 328,
   "p1": "1233",
   "pn": "1236",
   "abstract": [
    "In this paper we present an approach to transformation based model adaptation that combines a fast, closed form solution to the MAP estimation of our transforms with robust priors. The robust priors are found using the technique of hierarchical priors, and a closed form solution is achieved by choosing diagonally constrained affine transformations and a suitable family of prior distributions for these transformations. We show that the method gives results comparable to other algorithms, but with significantly reduced computational complexity and memory demands. Experiments are conducted on the SI Recognition Outlier task from the Wall Street Journal corpus, where speaker independent models have to be adapted to handle speech from non-native speakers.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-320"
  },
  "liu01b_eurospeech": {
   "authors": [
    [
     "Xiaoxing",
     "Liu"
    ],
    [
     "Baosheng",
     "Yuan"
    ],
    [
     "Yonghong",
     "Yan"
    ]
   ],
   "title": "A context adaptation approach for building context dependent models in LVCSR",
   "original": "e01_1237",
   "page_count": 4,
   "order": 329,
   "p1": "1237",
   "pn": "1240",
   "abstract": [
    "This paper introduces a new context adaptation framework for building context dependent HMM models in LVCSR. In this new framework, all states of each center phone are clustered into groups by the decision tree algorithm. All the tied states of context dependent HMM models were then derived by adapting the parameters of the multiple-mixture context independent model via data dependent MAP (maximum a posteriori probability)method using the training vectors corresponding to the tied state. An advantage of this approach is that it can maintain a high prediction and classification power given limited training data therefore the model trained in this framework is more reliable than in conventional framework. Experimental results on Wall Street Journal corpora demonstrate that the proposed approach leads to a significant improvement in recognition performance.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-321"
  },
  "lefevre01_eurospeech": {
   "authors": [
    [
     "Fabrice",
     "Lefevre"
    ],
    [
     "Jean-Luc",
     "Gauvain"
    ],
    [
     "Lori",
     "Lamel"
    ]
   ],
   "title": "Improving genericity for task-independent speech recognition",
   "original": "e01_1241",
   "page_count": 4,
   "order": 330,
   "p1": "1241",
   "pn": "1244",
   "abstract": [
    "Although there have been regular improvements in speech recognition technology over the past decade, speech recognition is far from being a solved problem. Recognition systems are usually tuned to a particular task and porting the system to a new task (or language) is both timeconsuming and expensive. In this paper, issues in speech recognizer portability are addressed through the development of generic core speech recognition technology. First, the genericity of wide domain models is assessed by evaluating performance on several tasks. Then, the use of transparent methods for adapting generic models to a specific taskis explored. Finally, further techniques are evaluated aiming at enhancing the genericity of the wide domain models. We show that unsupervised acoustic model adaptation and multi-source training can reduce the performance gap between task-independent and task-dependent acoustic models, and for some tasks even out-perform task-dependent acoustic models.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-322"
  },
  "matrouf01_eurospeech": {
   "authors": [
    [
     "Driss",
     "Matrouf"
    ],
    [
     "Olivier",
     "Bellot"
    ],
    [
     "Pascal",
     "Nocera"
    ],
    [
     "Georges",
     "Linares"
    ],
    [
     "Jean-Francois",
     "Bonastre"
    ]
   ],
   "title": "A posteriori and a priori transformations for speaker adaptation in large vocabulary speech recognition systems",
   "original": "e01_1245",
   "page_count": 4,
   "order": 331,
   "p1": "1245",
   "pn": "1248",
   "abstract": [
    "The speaker-dependent HMM-based recognizers gives lower word error rates in comparison with the corresponding speaker-independent recognizers. The aim of speaker adaptation techniques is to enhance the speaker-independent acoustic models to bring their recognition accuracy as close as possible to the one obtained with speaker-dependent models. In this paper, we propose a method using test and training data for acoustic model adaptation. This method operates in two steps. The first one performs an a priori adaptation using the transcribed training data of the closest training speakers to the test speaker. This adaptation is done with MAP procedure allowing reduced variances in the acoustic models. The second one performs an a posteriori adaptation using the MLLR procedure on the test data, allowing mapping of Gaussians means to match the test speaker's acoustic space. This adaptation strategy was evaluated in a large vocabulary speech recognition task. Our method leads to a relative gain of 15% with respect to the baseline system and 10% with respect to the MLLR adaptation.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-323"
  },
  "purnell01_eurospeech": {
   "authors": [
    [
     "Darryl W.",
     "Purnell"
    ],
    [
     "Elizabeth C.",
     "Botha"
    ]
   ],
   "title": "Bayesian methods for HMM speech recognition with limited training data",
   "original": "e01_1249",
   "page_count": 4,
   "order": 332,
   "p1": "1249",
   "pn": "1252",
   "abstract": [
    "This paper presents a Bayesian approach to learning for HMMs in speech recognition. The implementation of Bayesian learning for HMMs in speech recognition is discussed, including the requirement of maintaining the original HMM constraints, choice of prior and utterance recognition. This work shows that the Bayesian learning approach can be successfully applied to complex models when the amount of training data is small.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-324"
  },
  "wong01_eurospeech": {
   "authors": [
    [
     "Kwok-Man",
     "Wong"
    ],
    [
     "Brian",
     "Mak"
    ]
   ],
   "title": "Rapid speaker adaptation using MLLR and subspace regression classes",
   "original": "e01_1253",
   "page_count": 4,
   "order": 333,
   "p1": "1253",
   "pn": "1256",
   "abstract": [
    "In recent years, various adaptation techniques for hidden Markov modeling with mixture Gaussians have been proposed, most notably MAP estimation and MLLR transformation. When the amount of adaptation data is limited, adaptation can be done by grouping similar Gaussians together to form regression classes and then transforming the Gaussians in groups. The grouping of Gaussians is often determined at the full-space level. In this paper, we propose to group the Gaussians at a finer acoustic subspace level. The motivation is that clustering at subspaces of lower dimensions results in lower distortion. Besides, as the dimension of subspace Gaussians reduces, there are fewer parameters to estimate for the subsequent MLLR transformation matrix. This is particular attractive in fast adaptation. Speaker adaptation experiments on the Resource Management task with few seconds of speech show that the use of subspace regression classes is more effective than traditional full-space regression classes.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-325"
  },
  "yoma01_eurospeech": {
   "authors": [
    [
     "Nestor Becerra",
     "Yoma"
    ],
    [
     "Jorge",
     "Silva"
    ]
   ],
   "title": "Speaker adaptation of output probabilities and state duration distributions for speech recognition",
   "original": "e01_1257",
   "page_count": 4,
   "order": 334,
   "p1": "1257",
   "pn": "1260",
   "abstract": [
    "This paper presents a comparison of maximum a posteriori (MAP) speaker adaptation of state duration distributions and output probabilities in HMM. Both adaptation procedures are compared and then combined in recognition experiments with clean and noisy signals. The results here shown suggest that the state duration distribution adaptation can lead to higher improvements than the adaptation of output probabilities, and the reduction in the error rate when both adaptations are combined is as high as 50% or 60% using only a few samples per word.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-326"
  },
  "wu01_eurospeech": {
   "authors": [
    [
     "Jian",
     "Wu"
    ],
    [
     "Eric",
     "Chang"
    ]
   ],
   "title": "Cohorts based custom models for rapid speaker and dialect adaptation",
   "original": "e01_1261",
   "page_count": 4,
   "order": 335,
   "p1": "1261",
   "pn": "1264",
   "abstract": [
    "It is well known that speaker dependent acoustic models can achieve an error rate that is up to a factor of two smaller compared to well trained speaker independent acoustic models. Thus, for improved accuracy, many modern dictation systems require the user to perform enrollment sessions to adapt the acoustic model of the system. In this paper, we present an approach that uses as few as three sentences from the test speaker to select N closest speakers (cohorts) from both the original training set and newly available training speakers to construct customized models. By using such an approach, our adaptation scheme can be updated online without re-configuring anything that has been calculated before. When applying this approach to address dialectal differences, the cohort based user specific models constructed with 3 user sentences can obtain a lower error rate even when compared to user-adapted models based on 170 user sentences.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-327"
  },
  "vasilache01_eurospeech": {
   "authors": [
    [
     "Marcel",
     "Vasilache"
    ],
    [
     "Olli",
     "Viikki"
    ]
   ],
   "title": "Speaker adaptation of quantized parameter HMMs",
   "original": "e01_1265",
   "page_count": 4,
   "order": 336,
   "p1": "1265",
   "pn": "1268",
   "abstract": [
    "This paper extends the evaluation of Hidden Markov Models with quantized parameters (qHMM) presented in [5] to the case of speaker adaptive training. In speaker-independent speech recognition tasks, qHMMs were found to provide a similar performance as the original continuous density HMMs (CDHMM) with substantially reduced memory requirements. In this paper, we propose a Bayesian type of adaptation framework for qHMMs to improve the speaker-specific acoustic modeling accuracy. Experimental results indicate that the proposed qHMM adaptation scheme provides a comparable performance as obtained with the Bayesian adaptation of CDHMMs in a noise-free test environment. In the presence of noise, on the other hand, the performance improvement due to qHMM adaptation is lower than obtained in the CDHMM case. In general, the adaptation gains are on a similar scale fact that confers to qHMMs a great practical value.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-328"
  },
  "tsao01_eurospeech": {
   "authors": [
    [
     "Yu",
     "Tsao"
    ],
    [
     "Shang-Ming",
     "Lee"
    ],
    [
     "Fu-Chiang",
     "Chou"
    ],
    [
     "Lin-Shan",
     "Lee"
    ]
   ],
   "title": "Segmental eigenvoice for rapid speaker adaptation",
   "original": "e01_1269",
   "page_count": 4,
   "order": 337,
   "p1": "1269",
   "pn": "1272",
   "abstract": [
    "This paper presents a new approach to improve the conventional eigenvoice technique. In the conventional eigenvoice, an eigenspace is established by introducing a priori knowledge of training speakers via PCA. The adaptation data is then used to determine a group of coefficients with respect to the eigenspace and build the SD model for the testing speaker. In the proposed approach, the eigenspace in the conventional eigenvoice is segmented into N sub-eigenspaces. Each sub-eigenspace is established by those components in the training speaker SD models with similar properties to each other. With the adaptation data, N groups of coefficients corresponding to the N sub-eigenspaces can be determined to build SD model for the new testing speaker. Here, both mixture-based and feature-based segmentation of eigenspace were tested, and improved results compared to the conventional eigenvoice were obtained in both cases. Even better results were obtained when these approaches were properly combined.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-329"
  },
  "warakagoda01_eurospeech": {
   "authors": [
    [
     "Narada D.",
     "Warakagoda"
    ],
    [
     "Magne H.",
     "Johnsen"
    ]
   ],
   "title": "Speaker adaptation in an ASR system based on nonlinear dynamical systems",
   "original": "e01_1273",
   "page_count": 4,
   "order": 338,
   "p1": "1273",
   "pn": "1276",
   "abstract": [
    "The work presented here is centered around a speech production model called Chained Dynamical System Model (CDSM) which is motivated by the fundamental limitations of the mainstream ASR approaches. The CDSM is essentially a smoothly time varying continuous state nonlinear dynamical system, consisting of two sub dynamical systems coupled as a chain so that one system controls the parameters of the next system. The speech recognition problem is posed as inverting the CDSM, which is solved using the ideas borrowed from the theory of Embedding. The resulting architecture, which we call Inverted CDSM (ICDSM) is well suited for modeling variations of speaker and channel characteristics, by its nature. We have evaluated the ICDSM using a set of experiments involving speaker adaptation in a continuous speech recognition task on the TIMIT database. Results of these experiments confirm the feasibility and potential advantages of the approach.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-330"
  },
  "cordoba01_eurospeech": {
   "authors": [
    [
     "R.",
     "Córdoba"
    ],
    [
     "R.",
     "San-Segundo"
    ],
    [
     "J. M.",
     "Montero"
    ],
    [
     "J.",
     "Colás"
    ],
    [
     "J.",
     "Ferreiros"
    ],
    [
     "J.",
     "Macías-Guarasa"
    ],
    [
     "Juan M.",
     "Pardo"
    ]
   ],
   "title": "An interactive directory assistance service for Spanish with large-vocabulary recognition",
   "original": "e01_1279",
   "page_count": 4,
   "order": 339,
   "p1": "1279",
   "pn": "1282",
   "abstract": [
    "In the EU funded IDAS project (LE4-8315), demonstrators providing an automated interactive telephone-based directory assistance service have been developed by ten partners from Germany, Greece, Spain and Switzerland [6]. In this paper we will focus in the Spanish demonstrator. In particular, we will describe the following aspects: The general architecture of the system, paying special attention to the speech recognition module. We will present new alternatives for the estimation of continuous HMMs and the agglomerative clustering of context-dependent units. The most common problems encountered in the development of this kind of systems and their operation in a real environment. Impressions, opinions and scores from real-world users of the system. Keywords: large vocabulary recognition, telephone-based, directory assistance service, dialog.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-331"
  },
  "xu01c_eurospeech": {
   "authors": [
    [
     "Yunbiao",
     "Xu"
    ],
    [
     "Masahiro",
     "Araki"
    ],
    [
     "Yasuhisa",
     "Niimi"
    ]
   ],
   "title": "A multilingual-supporting dialog system using a common dialog controller",
   "original": "e01_1283",
   "page_count": 4,
   "order": 340,
   "p1": "1283",
   "pn": "1286",
   "abstract": [
    "It is well known that a speech dialog system can be regarded as an integration of a speech interface which runs in the front end and a dialog controller which runs in the back end. The former is obviously language-dependent while the later could be language-independent relatively. This paper describes an approach to constructing a multilingual spoken dialog system. In this approach, we extended a dialog controller for Japanese to a language-independent one and combined it with a Chinese speech interface. Experimental result shows that the proposed approach is effective in constructing quickly a multilingual-supporting dialog system using a common dialog controller.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-332"
  },
  "nouza01_eurospeech": {
   "authors": [
    [
     "Tomas",
     "Nouza"
    ],
    [
     "Jan",
     "Nouza"
    ]
   ],
   "title": "Graphic platform for designing and developing practical voice interaction systems",
   "original": "e01_1287",
   "page_count": 4,
   "order": 341,
   "p1": "1287",
   "pn": "1290",
   "abstract": [
    "A complete development environment for designing, building and running voice operated services has been created. It offers a system builder a graphic platform with several types of blocks, such as an ASR block, a TTS one, a switch block, a database query block, etc. Even a large dialogue scheme can be realized in very short time simply by placing blocks on the form, specifying their properties and aligning them into meaningful dialogue branches. Sequencing the blocks into branches is solved in a unique way without using any interconnecting lines, which makes the dialogue scheme easy for editing. The platform, named LOTOS, has been employed in building a large multi-domain information system with voice access via telephone.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-333"
  },
  "besacier01_eurospeech": {
   "authors": [
    [
     "Laurent",
     "Besacier"
    ],
    [
     "H.",
     "Blanchon"
    ],
    [
     "Y.",
     "Fouquet"
    ],
    [
     "J. P.",
     "Guilbaud"
    ],
    [
     "S.",
     "Helme"
    ],
    [
     "S.",
     "Mazenot"
    ],
    [
     "D.",
     "Moraru"
    ],
    [
     "D.",
     "Vaufreydaz"
    ]
   ],
   "title": "Speech translation for French in the NESPOLE! european project",
   "original": "e01_1291",
   "page_count": 4,
   "order": 342,
   "p1": "1291",
   "pn": "1294",
   "abstract": [
    "This paper presents CLIPS laboratory activities in the context of the NESPOLE! European project, exploring future applications of automatic speech to speech translation in e-commerce and e-service sectors. The scientific and technological research issues particularly addressed in order to improve current experimental speech-to-speech translation systems, are: robustness, scalability, and cross-domain portability. The general architecture of the whole speech to speech translation demonstrator is first presented and the Interchange Format (IF) strategy for translation adopted in the project is quickly described. The French database recorded during the project and the French Human Language Technology (HLT) modules (recognition, synthesis and translation) are then fully detailed. First results obtained and future perspectives of the project are also discussed in this article.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-334"
  },
  "hickey01_eurospeech": {
   "authors": [
    [
     "Marianne",
     "Hickey"
    ],
    [
     "Paul St John",
     "Brittan"
    ]
   ],
   "title": "Lessons from the development of a conversational interface",
   "original": "e01_1295",
   "page_count": 4,
   "order": 343,
   "p1": "1295",
   "pn": "1298",
   "abstract": [
    "The design of an effective mixed initiative dialogue system still presents great challenges. This paper reports on the experiences gained in the design and implementation of an experimental spoken dialogue system, MIZIK, which revolves around a new domain, the music charts. It describes the processes we went through to: determine the development approach for a robust system; specify the scope of the domain; select an appropriate architecture and speech and language technology; collect training data specific to the domain and the target user population and, finally, to develop the experimental system. The paper concludes with a number of key lessons learnt during these processes, many of which are equally applicable to the design and development of any conversational speech interface.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-335"
  },
  "hirschberg01b_eurospeech": {
   "authors": [
    [
     "Julia",
     "Hirschberg"
    ],
    [
     "Michiel",
     "Bacchiani"
    ],
    [
     "Don",
     "Hindle"
    ],
    [
     "Phil",
     "Isenhour"
    ],
    [
     "Aaron",
     "Rosenberg"
    ],
    [
     "Litza",
     "Stark"
    ],
    [
     "Larry",
     "Stead"
    ],
    [
     "Steve",
     "Whittaker"
    ],
    [
     "Gary",
     "Zamchick"
    ]
   ],
   "title": "SCANMail: browsing and searching speech data by content",
   "original": "e01_1299",
   "page_count": 4,
   "order": 344,
   "p1": "1299",
   "pn": "1302",
   "abstract": [
    "Increasing amounts of public, corporate, and private audio data are available for use, but limited in usefulness by the lack of tools to permit their browsing and search. In this paper, we describe SCANMail, a system that employs automatic speech recognition, information retrieval, information extraction, and human computer interaction technology to permit users to browse and search their voicemail messages by content through a graphical user interface interface. The SCANMail client also provides note-taking capabilities as well as browsing and querying features. A CallerId server also proposes caller names from existing caller acoustic models and is trained from user feedback. An Email server sends the original message plus its transcription to a mailing address specified in the user's profile.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-336"
  },
  "lo01_eurospeech": {
   "authors": [
    [
     "Wai-Kit",
     "Lo"
    ],
    [
     "Patrick",
     "Schone"
    ],
    [
     "Helen M.",
     "Meng"
    ]
   ],
   "title": "Multi-scale retrieval in MEI: an English-Chinese translingual speech retrieval system",
   "original": "e01_1303",
   "page_count": 4,
   "order": 345,
   "p1": "1303",
   "pn": "1306",
   "abstract": [
    "This paper presents a multi-scale retrieval approach in MEI (Mandarin-English Information), an English-Chinese cross-lingual spoken document retrieval (CL-SDR) system. It accepts an entire English news story (from newspaper text) as the input query, and automatically retrieves \"relevant\" Mandarin news stories (from broadcast audio). This allows the user to search for personally relevant content across the language and media barriers - a cross-lingual and cross-media retrieval task. MEI advocates a multi-scale paradigm for the retrieval task. Multiscale refers to the use of both words and subwords (Chinese characters and syllables) for retrieval. Words offer lexical knowledge to enhance precision, and subwords can potentially alleviate some prevailing problems in CL-SDR, e.g. open vocabularies in translation and recognition, out-of-vocabulary words in audio indexing, and ambiguities in Chinese homophones and word tokenizaiton. We present techniques for word-subword fusion, which improved retrieval performance in our experiments with the Topic Detection and Tracking collection.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-337"
  },
  "chien01b_eurospeech": {
   "authors": [
    [
     "Shih-Chieh",
     "Chien"
    ],
    [
     "Sen-Chia",
     "Chang"
    ]
   ],
   "title": "Compact word graph in spoken dialogue system",
   "original": "e01_1307",
   "page_count": 4,
   "order": 346,
   "p1": "1307",
   "pn": "1310",
   "abstract": [
    "In this paper, we introduce the multi-stage configuration for the interpretation of user's queries in our spoken dialogue system. In this configuration, a recovery mechanism is used to detect and recover the errors arising from speech recognition. To efficiently incorporate with this recovery mechanism, a recognition scheme that can provide a compact word graph is developed. The compact word graph is generated through a pruning method based on the N-best sentence score. Instead of setting threshold, we use the N-best sentence score to select word hypotheses of the compact word graph.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-338"
  },
  "sasajima01_eurospeech": {
   "authors": [
    [
     "Munehiko",
     "Sasajima"
    ],
    [
     "Takebhide",
     "Yano"
    ],
    [
     "Taishi",
     "Shimomori"
    ],
    [
     "Tatsuya",
     "Uehara"
    ]
   ],
   "title": "MINOS-II: a prototype car navigation system with mixed initiative turn taking dialogue",
   "original": "e01_1311",
   "page_count": 4,
   "order": 347,
   "p1": "1311",
   "pn": "1314",
   "abstract": [
    "Spoken dialogue systems are classified into three types from the viewpoint of turn taking. Dialogue can be led by the system (system initiative), the user (user initiative), and their mixture (mixed initiative). In this paper, EUROPA, a framework for developing spoken dialogue systems, is introduced. EUROPA is applied to prototyping a car navigation system called MINOS-II. MINOS-II deals with a car navigation task of mixed initiative dialogue. First, the system takes the initiative to lead the user to set a route for the destination. Next, while driving along the route, the user takes the initiative and retrieves information about the route freely. MINOS-II is built on a portable PC, can process over 2 million sentence patterns, and is able to respond to a user's question within a few seconds.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-339"
  },
  "kiriyama01_eurospeech": {
   "authors": [
    [
     "Shinya",
     "Kiriyama"
    ],
    [
     "Keikichi",
     "Hirose"
    ],
    [
     "Nobuaki",
     "Minematsu"
    ]
   ],
   "title": "Use of topic knowledge in spoken dialogue information retrieval system for academic documents",
   "original": "e01_1315",
   "page_count": 4,
   "order": 348,
   "p1": "1315",
   "pn": "1318",
   "abstract": [
    "An efficient search function based on topic estimation was integrated to our spoken dialogue system for academic document information retrieval. The following two points were mainly studied: 1) to properly categorize documents (to be retrieved) into related topics, and 2) to facilitate retrieval process using topic knowledge. For the first point, a method was developed to calculate recursively the relevance scores of retrieval words and documents for topics. Effects of the recursive process were proved through experimental results; better classification of retrieval words and documents into topics was realized. As for the second point, retrieval range was limited into topics estimated from retrieval words. It was shown through experiments of retrieval task solving that necessary number of dialogue turns (therefore, period of dialogue) could be largely reduced by the range limitation; a smooth retrieval process was proved to be realized using topic knowledge.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-340"
  },
  "komatani01_eurospeech": {
   "authors": [
    [
     "Kazunori",
     "Komatani"
    ],
    [
     "Katsuaki",
     "Tanaka"
    ],
    [
     "Hiroaki",
     "Kashima"
    ],
    [
     "Tatsuya",
     "Kawahara"
    ]
   ],
   "title": "Domain-independent spoken dialogue platform using key-phrase spotting based on combined language model",
   "original": "e01_1319",
   "page_count": 4,
   "order": 349,
   "p1": "1319",
   "pn": "1322",
   "abstract": [
    "We present a portable platform for spoken dialogue systems. Conventional development of speech interfaces involves much labor cost in either describing a task grammar or collecting a task corpus. Our platform automatically generates a lexicon and a language model of keyphrases based on task description and the domain database. By spotting key-phrases using both the generated grammar and word 2-gram model trained with dialogue corpora of similar domains, we realize flexible speech understanding on a variety of utterances. Furthermore, adopting a GUI that explicitly displays acceptable utterance patterns is effective in guiding user utterances within the system's capability. We evaluate the generated system using 24 novice users. The number of unacceptable utterances are significantly reduced with the simple phrase grammar and GUI. And the phrase spotter using the combined language model improves the semantic accuracy by 15.5% compared with the conventional method decoding the whole sentence with a fixed grammar.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-341"
  },
  "durston01_eurospeech": {
   "authors": [
    [
     "Peter J.",
     "Durston"
    ],
    [
     "Mark",
     "Farrell"
    ],
    [
     "David",
     "Attwater"
    ],
    [
     "James",
     "Allen"
    ],
    [
     "Hong-Kwang Jeff",
     "Kuo"
    ],
    [
     "Mohamed",
     "Afify"
    ],
    [
     "Eric",
     "Fosler-Lussier"
    ],
    [
     "Chin-Hui",
     "Lee"
    ]
   ],
   "title": "OASIS natural language call steering trial",
   "original": "e01_1323",
   "page_count": 4,
   "order": 350,
   "p1": "1323",
   "pn": "1326",
   "abstract": [
    "A recent trial of natural language call steering on live UK calls to the operator is described along with its results. The characteristics of the problem are described along with the acoustic, language, semantic and dialogue modelling approaches employed. Natural language call steering is found to be viable, with recognition and semantic accuracy the current limiting factors.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-342"
  },
  "azzini01_eurospeech": {
   "authors": [
    [
     "Ivano",
     "Azzini"
    ],
    [
     "Daniele",
     "Falavigna"
    ],
    [
     "Roberto",
     "Gretter"
    ],
    [
     "Giordano",
     "Lanzola"
    ],
    [
     "Marco",
     "Orlandi"
    ]
   ],
   "title": "First steps toward an adaptive spoken dialogue system in medical domain",
   "original": "e01_1327",
   "page_count": 4,
   "order": 351,
   "p1": "1327",
   "pn": "1330",
   "abstract": [
    "Recently the spoken dialog group of ITC-irst and the Dipartimento di Informatica e Sistemistica of the University of Pavia are working together to realize an intelligent spoken dialog system with adaptation capabilities. In this framework, some telemedicine services able to handle multimodal interactions are going to be investigated and developed. In the paper dialog \"adaptation\" will be defined according to the medical domains in which the system is going to be used: i.e. to assist chronic patients. The present architecture of the system will be described and some ideas for its future development will be discussed. Although the system is placed in medical domains, the basic concepts can, in principle, be exported towards different applications. The work reported in the paper is part of a more wide-ranging project aimed at efficiently merging patient and domain specific knowledge, with statistical knowledge (e.g. n-grams and/or concept probabilities) derived from real user interactions.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-343"
  },
  "nakano01_eurospeech": {
   "authors": [
    [
     "Mikio",
     "Nakano"
    ],
    [
     "Yasuhiro",
     "Minami"
    ],
    [
     "Stephanie",
     "Seneff"
    ],
    [
     "Timothy J.",
     "Hazen"
    ],
    [
     "D. Scott",
     "Cyphers"
    ],
    [
     "James",
     "Glass"
    ],
    [
     "Joseph",
     "Polifroni"
    ],
    [
     "Victor",
     "Zue"
    ]
   ],
   "title": "Mokusei: a telephone-based Japanese conversational system in the weather domain",
   "original": "e01_1331",
   "page_count": 4,
   "order": 352,
   "p1": "1331",
   "pn": "1334",
   "abstract": [
    "This paper describes Mokusei, an end-to-end Japanese version of our Jupiter weather information system. Mokusei delivers weather information over the phone through natural conversation with the user. For the most part, Mokusei uses the same components for recognition, understanding, and generation that Jupiter uses, and the database and the semantic frames for the weather information content are also shared. However, Mokusei motivated us to redesign our Genesis generation system, in order to improve the quality of translations of weather reports into Japanese. We also had to develop new ways to transcribe user utterances through morphological analysis. Mokusei is fully functional and has already been used for data collection with about 700 naive users. These data have been used for improvement and evaluation of Mokusei. This paper also presents the result of evaluating the current version of Mokusei.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-344"
  },
  "glass01_eurospeech": {
   "authors": [
    [
     "James",
     "Glass"
    ],
    [
     "Eugene",
     "Weinstein"
    ]
   ],
   "title": "Speechbuilder: facilitating spoken dialogue system development",
   "original": "e01_1335",
   "page_count": 4,
   "order": 353,
   "p1": "1335",
   "pn": "1338",
   "abstract": [
    "In this paper we report our attempts to facilitate the creation of mixed-initiative spoken dialogue systems for both novice and experienced developers of human language technology. Our efforts have resulted in the creation of a utility called SpeechBuilder, which allows developers to specify linguistic information about their domains, and rapidly create spoken dialogue interfaces to them. SpeechBuilder has been used to create domains providing access to structured information contained in a relational database, as well as to provide human language interfaces to control or transaction-based applications.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-345"
  },
  "rahim01_eurospeech": {
   "authors": [
    [
     "M.",
     "Rahim"
    ],
    [
     "Giuseppe Di",
     "Fabbrizio"
    ],
    [
     "C.",
     "Kamm"
    ],
    [
     "Marilyn",
     "Walker"
    ],
    [
     "A.",
     "Pokrovsky"
    ],
    [
     "P.",
     "Ruscitti"
    ],
    [
     "E.",
     "Levin"
    ],
    [
     "S.",
     "Lee"
    ],
    [
     "Ann K.",
     "Syrdal"
    ],
    [
     "K.",
     "Schlosser"
    ]
   ],
   "title": "Voice-IF: a mixed-initiative spoken dialogue system for AT&t conference services",
   "original": "e01_1339",
   "page_count": 4,
   "order": 354,
   "p1": "1339",
   "pn": "1342",
   "abstract": [
    "This paper presents the Voice-IF system; a mixed-initiative spoken dialogue system for AT&T conference services. One objective for creating Voice-IF is to provide a vehicle for evaluating our technologies in speech synthesis, recognition, understanding, dialogue and user interfaces on a real application with relatively novice users. Another objective is to design, build and test a set of tools that allow us to rapidly prototype applications. In this paper, we describe the performance of Voice-IF during its 6-week deployment period. In particular, we report a) results of perceptual evaluations of the synthesized speech, b) system performance and user satisfaction ratings, c) PARADISE analysis of the data, and d) comparisons with other systems, including the W99 conference registration system used at the ASRU'99 workshop and the Travel Communicator system.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-346"
  },
  "wahlster01_eurospeech": {
   "authors": [
    [
     "Wolfgang",
     "Wahlster"
    ],
    [
     "Norbert",
     "Reithinger"
    ],
    [
     "Anselm",
     "Blocher"
    ]
   ],
   "title": "Smartkom: multimodal communication with a life- like character",
   "original": "e01_1547",
   "page_count": 4,
   "order": 355,
   "p1": "1547",
   "pn": "1550",
   "abstract": [
    "SmartKom is a multimodal dialog system that combines speech, gesture, and mimics input and output. Spontaneous speech understanding is combined with the video-based recognition of natural gestures. One of the major scientific goals of SmartKom is to design new computational methods for the seamless integration and mutual disambiguation of multimodal input and output on a semantic and pragmatic level. SmartKom is based on the situated delegation-oriented dialog paradigm, in which the user delegates a task to a virtual communication assistant, visualized as a life-like character on a graphical display. We describe the SmartKom architecture, the use of an XML-based mark-up language for multimodal content, and some of the distinguishing features of the first fully operational SmartKom demonstrator.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-347"
  },
  "meng01_eurospeech": {
   "authors": [
    [
     "Helen M.",
     "Meng"
    ],
    [
     "Shuk Fong",
     "Chan"
    ],
    [
     "Yee Fong",
     "Wong"
    ],
    [
     "Cheong Chat",
     "Chan"
    ],
    [
     "Yiu Wing",
     "Wong"
    ],
    [
     "Tien Ying",
     "Fung"
    ],
    [
     "Wai Ching",
     "Tsui"
    ],
    [
     "Ke",
     "Chen"
    ],
    [
     "Lan",
     "Wang"
    ],
    [
     "Ting Yao",
     "Wu"
    ],
    [
     "Xiaolong",
     "Li"
    ],
    [
     "Tan",
     "Lee"
    ],
    [
     "Wing Nin",
     "Choi"
    ],
    [
     "P. C.",
     "Ching"
    ],
    [
     "Huisheng",
     "Chi"
    ]
   ],
   "title": "ISIS: a learning system with combined interaction and delegation dialogs",
   "original": "e01_1551",
   "page_count": 4,
   "order": 356,
   "p1": "1551",
   "pn": "1554",
   "abstract": [
    "This paper presents a progress update of our ISIS trilingual spoken dialog system. ISIS is a conversational system for the stocks domain, and supports interactions in the languages of our region - English and two dialects of Chinese (Mandarin and Cantonese). ISIS provides a system test-bed for our initial explorations with the CORBA architecture, and delegation to KQML agents. CORBA offers the advantages of interoperability, scalability and location transparency in client/server systems development. Users can delegate tasks to software agents to help monitor information (e.g. a drop in the price of a pre-specified stock), and generate user alert messages. Our current work presents new research directions in the context of ISIS: (i) automatic incorporation of newly listed stocks into our system's knowledge base; (ii) switching between on-line interaction and off-line delegation in a single dialog thread. We will also report on enhancements in the system's architecture and features.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-348"
  },
  "wang01_eurospeech": {
   "authors": [
    [
     "Ye-Yi",
     "Wang"
    ]
   ],
   "title": "Robust language understanding in mipad",
   "original": "e01_1555",
   "page_count": 4,
   "order": 357,
   "p1": "1555",
   "pn": "1558",
   "abstract": [
    "MiPad is an application prototype for the study of conversational, multi-modal interface in Microsoft Research. It has a Tap and Talk interface that allows users to effectively interact with a PDA device. The major Spoken Language Understanding (SLU) engine component behind MiPad is a robust chart parser. This paper discusses some novel features of the parser that enable it to take full advantage of the Tap and Talk interface and better support semantic based analysis. It also describes some implementation issues so that these new features can be accommodated without slowing down the parser. The new implementation speeds up the parser by a factor of three, making it more suitable for a SLU server.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-349"
  },
  "lemon01_eurospeech": {
   "authors": [
    [
     "Oliver",
     "Lemon"
    ],
    [
     "Anne",
     "Bracy"
    ],
    [
     "Alexander",
     "Gruenstein"
    ],
    [
     "Stanley",
     "Peters"
    ]
   ],
   "title": "The WITAS multi-modal dialogue system I",
   "original": "e01_1559",
   "page_count": 4,
   "order": 358,
   "p1": "1559",
   "pn": "1562",
   "abstract": [
    "We present the first demonstration version of the WITAS dialogue system for multi-modal conversations with autonomous mobile robots, and motivate several innovations currently in development for version II. The human-robot interaction setting is argued to present new challenges for dialogue system engineers, in comparison to previous work in dialogue systems under the travel-planning paradigm, in that dialogues must be asynchronous, mixed-initiative, open-ended, and involve a dynamic environment. We approached these general problems in a dialogue interface to the WITAS robot helicopter, or UAV (`Unmanned Aerial Vehicle'). We present this system and the modelling ideas behind it, and then motivate changes being made for version II of the system, involving more richly structured dialogue states and the use of automated reasoning systems over task, ability, and world-state models. We argue that these sorts of enhancement are vital to the future development of conversational systems.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-350"
  },
  "shriver01_eurospeech": {
   "authors": [
    [
     "Stefanie",
     "Shriver"
    ],
    [
     "Roni",
     "Rosenfeld"
    ],
    [
     "Xiaojin",
     "Zhu"
    ],
    [
     "Arthur",
     "Toth"
    ],
    [
     "Alexander I.",
     "Rudnicky"
    ],
    [
     "Markus",
     "Flueckiger"
    ]
   ],
   "title": "Universalizing speech: notes from the USI project",
   "original": "e01_1563",
   "page_count": 4,
   "order": 359,
   "p1": "1563",
   "pn": "1566",
   "abstract": [
    "This paper discusses progress in designing a standardized interface for speech interaction with simple machines  the Universal Speech Interface (USI) project. We discuss the motivation for such a design and issues that must be addressed by such an interface. We present our current proposals for handling these issues, and comment on the usability of these approaches based on user interactions with the system. Finally, we discuss future work and plans for the USI project.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-351"
  },
  "shriberg01_eurospeech": {
   "authors": [
    [
     "Elizabeth",
     "Shriberg"
    ],
    [
     "Andreas",
     "Stolcke"
    ],
    [
     "Don",
     "Baron"
    ]
   ],
   "title": "Observations on overlap: findings and implications for automatic processing of multi-party conversation",
   "original": "e01_1359",
   "page_count": 4,
   "order": 360,
   "p1": "1359",
   "pn": "1362",
   "abstract": [
    "We examine the distribution of overlapping speech in large multi-party conversations, including two different types of meetings, and two corpora of telephone conversations. Analyses are based on forced alignment and speech recognition using an identical recognizer across tasks. Three results are discussed. First, all corpora show high overall rates of overlap, with similar rates for meetings and telephone conversations. Second, speech recognition performance in non-overlapped regions of meetings is no worse than that for single-channel telephone conversations, while recognition in overlap regions degrades considerably. Finally, interrupt locations are associated with endpoints of word-level events in a speaker's turn, including back-channels, discourse markers, and disfluencies. Results suggest that overlaps are an important inherent characteristic of conversational speech that should not be ignored; on the contrary, they should be jointly modeled with acoustic and language model information in machine processing of conversation.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-352"
  },
  "beckham01_eurospeech": {
   "authors": [
    [
     "Jennifer L.",
     "Beckham"
    ],
    [
     "Giuseppe Di",
     "Fabbrizio"
    ],
    [
     "Nils",
     "Klarlund"
    ]
   ],
   "title": "Towards SMIL as a foundation for multimodal, multimedia applications",
   "original": "e01_1363",
   "page_count": 4,
   "order": 361,
   "p1": "1363",
   "pn": "1366",
   "abstract": [
    "Rich and interactive multimedia applications, where audio, video, graphics and text are precisely synchronized under timing constraints are becoming ubiquitous. Multimodal applications further extend the concept of user interaction combining different modalities, like speech recognition, speech synthesis and gestures. However, authoring dialog-capable multimodal, multimedia services is a very difficult task. In this paper, we argue that SMIL is an ideal substrate for extending multimedia applications with multimodal facilities. SMIL as it stands is not a general notation for controlling media and input mode resources. We show that all what is needed are few natural extensions to SMIL along with the addition of a simple reactive programming language that we call ReX. Our language is designed to be maximally compatible with existing W3C recommendations through a generic event system based on DOM and an expression language based on XPATH.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-353"
  },
  "kipp01_eurospeech": {
   "authors": [
    [
     "Michael",
     "Kipp"
    ]
   ],
   "title": "ANVIL - a generic annotation tool for multimodal dialogue",
   "original": "e01_1367",
   "page_count": 4,
   "order": 362,
   "p1": "1367",
   "pn": "1370",
   "abstract": [
    "ANVIL is a tool for the annotation of audiovisual material containing multimodal dialogue. Annotation takes place on freely definable, multiple layers (tracks) by inserting time-anchored elements that hold a number of typed attribute-value pairs. Higher-level elements (suprasegmental) consist of a sequence of elements. Attributes contain symbols or cross-level links to arbitrary other elements. ANVIL is highly generic (usable with different annotation schemes), platform-independent, XML-based and fitted with an intuitive graphical user interface. For project integration, ANVIL offers the import of speech transcription and export of textand table data for further statistical processing.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-354"
  },
  "walker01_eurospeech": {
   "authors": [
    [
     "Marilyn",
     "Walker"
    ],
    [
     "J.",
     "Aberdeen"
    ],
    [
     "J.",
     "Boland"
    ],
    [
     "E.",
     "Bratt"
    ],
    [
     "J.",
     "Garofolo"
    ],
    [
     "Lynette",
     "Hirschman"
    ],
    [
     "A.",
     "Le"
    ],
    [
     "S.",
     "Lee"
    ],
    [
     "Shrikanth",
     "Narayanan"
    ],
    [
     "K.",
     "Papineni"
    ],
    [
     "Bryan",
     "Pellom"
    ],
    [
     "Joseph",
     "Polifroni"
    ],
    [
     "Alexandros",
     "Potamianos"
    ],
    [
     "P.",
     "Prabhu"
    ],
    [
     "Alexander I.",
     "Rudnicky"
    ],
    [
     "G.",
     "Sanders"
    ],
    [
     "Stephanie",
     "Seneff"
    ],
    [
     "D.",
     "Stallard"
    ],
    [
     "Steve",
     "Whittaker"
    ]
   ],
   "title": "DARPA communicator dialog travel planning systems: the june 2000 data collection",
   "original": "e01_1371",
   "page_count": 4,
   "order": 363,
   "p1": "1371",
   "pn": "1374",
   "abstract": [
    "This paper describes results of an experiment with 9 different DARPA Communicator systems who participated in the June 2000 Data collection. All Systems supported travel planning and utilized some form of mixed initiative interaction. However they varied in several critical dimensions: (1) They targeted different back-end databases for travel information; (2) They used different modules for ASR, NLU, TTS and dialog management. We describe the experimental design, the approach to data collection, the metrics collected, and results comparing the systems.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-355"
  },
  "huang01_eurospeech": {
   "authors": [
    [
     "Chao",
     "Huang"
    ],
    [
     "Tao",
     "Chen"
    ],
    [
     "Stan",
     "Li"
    ],
    [
     "Eric",
     "Chang"
    ],
    [
     "Jianlai",
     "Zhou"
    ]
   ],
   "title": "Analysis of speaker variability",
   "original": "e01_1377",
   "page_count": 4,
   "order": 364,
   "p1": "1377",
   "pn": "1380",
   "abstract": [
    "Analysis and modeling of speaker variability, such as gender, accent, age, speech rate, and phones realizations, are important issues in speech recognition. It is known that existing feature representations describing speaker variations can be of very high dimension. In this paper, we introduce two powerful multivariate statistical analysis methods, namely, principal component analysis (PCA) and independent component analysis (ICA), as tools for analysis of such variability and extraction of low dimensional feature representation. Our findings are the following: (1) the first two principal components correspond to the gender and accent, respectively. The result that the second component corresponding to the accent has never been reported before, to the best of our knowledge. (2) It is shown that ICA based features yield better classification performance than PCA ones. Using 2-dimensional ICA representation, we achieved about 6.1% and 13.3% error rate in gender and accent classification, respectively, for 980 speakers.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-356"
  },
  "nishida01_eurospeech": {
   "authors": [
    [
     "M.",
     "Nishida"
    ],
    [
     "Y.",
     "Ariki"
    ]
   ],
   "title": "Speaker recognition by separating phonetic space and speaker space",
   "original": "e01_1381",
   "page_count": 4,
   "order": 365,
   "p1": "1381",
   "pn": "1384",
   "abstract": [
    "In speaker recognition, it is a problem that speech feature varies depending on sentences and time difference. This variation is mainly attributed to the variation of phonetic information and speaker information included in speech data. If these two kinds of information are separated each other, robust speaker recognition will be realized. In this study, we propose a speaker recognition method by separating the phonetic information and speaker information by a subspace method, under the assumption that a space with large within-speaker variance is a \"phonetic space\" and a space with small within-speaker variance is a \"speaker space\". We carried out comparative experiments of the proposed method with a conventional method based on GMM in an observation space as well as in a space transformed by LDA. As a result, we could construct a robust speaker model with a few model parameters using a few training data by the proposed method.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-357"
  },
  "wang01b_eurospeech": {
   "authors": [
    [
     "Nick J.-C.",
     "Wang"
    ],
    [
     "Wei-Ho",
     "Tsai"
    ],
    [
     "Lin-Shan",
     "Lee"
    ]
   ],
   "title": "Eigen-MLLR coefficients as new feature parameters for speaker identification",
   "original": "e01_1385",
   "page_count": 4,
   "order": 366,
   "p1": "1385",
   "pn": "1388",
   "abstract": [
    "Eigen-MLLR coefficients are proposed as new feature parameters for speaker-identification in this paper. By performing principle component analysis on MLLR parameters among training speakers, the eigen-MLLR coefficients (EMCs) are derived as the coefficients for the eigenvectors. The discriminating function of the new EMC features based on the Fisher criterion is found to be ten times larger than that of mel-frequency cepstral coefficient (MFCC) features, for distinguishing speakers. The speaker-identification accuracy using the EMC features are shown to be significantly better than that using MFCC features, especially when the quantity of enrollment data is limited. It is also shown that properly combining MFCC and EMC features can achieve a significant error rate reduction on the order of 50%-60% as compared to using MFCC features alone.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-358"
  },
  "navratil01_eurospeech": {
   "authors": [
    [
     "Jiri",
     "Navratil"
    ],
    [
     "Upendra V.",
     "Chaudhari"
    ],
    [
     "Ganesh N.",
     "Ramaswamy"
    ]
   ],
   "title": "Speaker verification using target and background dependent linear transforms and multi-system fusion",
   "original": "e01_1389",
   "page_count": 4,
   "order": 367,
   "p1": "1389",
   "pn": "1392",
   "abstract": [
    "This paper describes a GMM-based speaker verification system that uses speaker-dependent background models transformed by speaker-specific maximum likelihood linear transforms to achieve a sharper separation between the target and the nontarget acoustic region. The effect of tying, or coupling, Gaussian components between the target and the background model is studied and shown to be a relevant factor with respect to the desired operating point. A fusion of scores from multiple systems built on different acoustic features via a neural network with performance gains over linear combination is also presented. Results obtained on the 1999 speaker recognition evaluation set indicate reductions of the minimum detection cost of up to 13% and 25% for all tests and electret-only tests respectively, as compared to a baseline GMM system. The neural fusion of three systems gains further 5% cost reduction.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-359"
  },
  "caspers01_eurospeech": {
   "authors": [
    [
     "Johanneke",
     "Caspers"
    ]
   ],
   "title": "Testing the perceptual relevance of syntactic completion and melodic configuration for turn-taking in dutch",
   "original": "e01_1395",
   "page_count": 4,
   "order": 368,
   "p1": "1395",
   "pn": "1398",
   "abstract": [
    "The research presented in this paper focuses on the role of melodic configuration and syntactic completion in the turn-taking process in Dutch. Subjects were presented with fragments of task-oriented dialogue, in which syntactic completeness and four types of melodic configuration were systematically varied, asking them to indicate whether they expected the turn to change at a specific point or not. The number of expected speaker changes turns out to be very low when no possible syntactic completion point has been reached. A rising pitch accent followed by a level boundary tone (H* %) is generally interpreted as a signal that the speaker wishes to continue, while H* H%, H*L L% and H*L H% configurations at syntactic boundaries are expected to be followed by a speaker change in the majority of cases. The data support the view that syntactic and melodic completion play a major role in the projection of possible turn-transition places.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-360"
  },
  "rietveld01_eurospeech": {
   "authors": [
    [
     "Toni",
     "Rietveld"
    ],
    [
     "Patricia",
     "Vermillion"
    ]
   ],
   "title": "Cues for perceived pitch register",
   "original": "e01_1399",
   "page_count": 4,
   "order": 369,
   "p1": "1399",
   "pn": "1402",
   "abstract": [
    "The aim of this experiment was to assess empirically listeners' behaviours in characterising pitch contours with the label pitch register. The motivation for this assessment was initiated by the conflicting use of the term 'register' in speech science and intonology. The findings reported here indicate that pitch register would more appropriately be associated with position of the Low pitch targets and the mean value of the tonal contour. In addition, the importance of the F0-min and the distance between H and L targets have been found to be weaker than previously assumed.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-361"
  },
  "chen01h_eurospeech": {
   "authors": [
    [
     "Aoju",
     "Chen"
    ],
    [
     "Toni",
     "Rietveld"
    ],
    [
     "Carlos",
     "Gussenhoven"
    ]
   ],
   "title": "Language-specific effects of pitch range on the perception of universal intonational meaning",
   "original": "e01_1403",
   "page_count": 4,
   "order": 370,
   "p1": "1403",
   "pn": "1406",
   "abstract": [
    "Two groups of listeners, with Dutch and British English as their native language judged stimuli in Dutch and British English, respectively, on the scales CONFIDENT vs. NOT CONFIDENT and FRIENDLY vs. NOT FRIENDLY, two meanings derived from Ohala's universal Frequency Code. The stimuli, which were lexically equivalent, were varied in pitch contour and pitch range. In both languages, the perceived degree of confidence decreases and that of friendliness increases when the pitch range is raised, as predicted by the Frequency Code. However, at identical pitch ranges, British English is perceived as more confident and more friendly than Dutch. We argue that this difference in degree of the use of the Frequency Code is due to the difference in the standard pitch ranges of Dutch and British English.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-362"
  },
  "janse01_eurospeech": {
   "authors": [
    [
     "Esther",
     "Janse"
    ]
   ],
   "title": "Comparing word-level intelligibility after linear vs. non-linear time-compression",
   "original": "e01_1407",
   "page_count": 4,
   "order": 371,
   "p1": "1407",
   "pn": "1410",
   "abstract": [
    "In this paper the question is addressed whether the word-level intelligibility of time-compressed speech can be improved over linear compression by using a type of non-linear compression. Two options are tested: one type of compression which takes into account the natural timing of fast speech; and one other type of compression that saves the segmental intelligibility of short unstressed syllables. This is tested at two rates of speech: fast and very fast. The results of the perception experiments (an articulation test and a speech-interference test) show that, at both rates of speech, neither of the two types of non-linear time-compression improves intelligibility over linear compression. This suggests that both the prosodic pattern and the segmental intelligibility of both syllables contribute to word recognition in fast speech.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-363"
  },
  "koopmansvanbeinum01_eurospeech": {
   "authors": [
    [
     "Florien J.",
     "Koopmans-van Beinum"
    ],
    [
     "Chris J.",
     "Clement"
    ],
    [
     "Ineke Van den",
     "Dikkenberg-Pot"
    ]
   ],
   "title": "AMSTIVOC (AMsterdam system for transcription of infant VOCalizations) applied to utterances of deaf and normally hearing infants",
   "original": "e01_1471",
   "page_count": 4,
   "order": 372,
   "p1": "1471",
   "pn": "1474",
   "abstract": [
    "The need to transcribe infant sound productions from birth onwards by using universally applicable coding tools has been basic to the development of our AMSTIVOC classification system. In this system early infant vocalizations are described by means of a sensori-motor approach based on the source-filter model for speech production. We applied the AMSTIVOC classification system, among other things, to early vocalizations of 6 deaf and 6 hearing infants in order to answer the question whether and where the lack of auditory perception can be traced in the early sound productions of deaf infants. By using this classification system it can be demonstrated that auditory feedback is needed to coordinate the movements of the phonatory and the articulatory system. This coordination capacity is likely to be a prerequisite for the development of normal speech production.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-364"
  },
  "engwall01b_eurospeech": {
   "authors": [
    [
     "Olov",
     "Engwall"
    ]
   ],
   "title": "Using linguopalatal contact patterns to tune a 3d tongue model",
   "original": "e01_1475",
   "page_count": 4,
   "order": 373,
   "p1": "1475",
   "pn": "1478",
   "abstract": [
    "The six articulatory parameters of a three-dimensional tongue model were adjusted to replicate linguopalatal contact patterns measured with Electropalatography (EPG). The tongue model is based on artificially sustained articulations measured with MRI and the EPG data provides one possibility to tune the parameters to dynamic speech. A 3D model was generated of the palate and the electrode distribution, allowing the synthetic contact patterns to be calculated. The tongue parameters were then adjusted to minimise the deviation from the natural contact patterns. Substantial reduction of the false and missing electrode contacts was made in the tuning and the synthetic linguopalatal contact pattern is shown to replicate the total characteristics of the natural patterns rather well. The remaining error is often due to lateral asymmetry or central-to-edge contact variations.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-365"
  },
  "kaburagi01_eurospeech": {
   "authors": [
    [
     "Tokihiko",
     "Kaburagi"
    ],
    [
     "Masaaki",
     "Honda"
    ]
   ],
   "title": "Electromagnetic articulograph (EMA) based on a nonparametric representation of tthe magnetic field",
   "original": "e01_1479",
   "page_count": 4,
   "order": 374,
   "p1": "1479",
   "pn": "1482",
   "abstract": [
    "Electromagnetic articulograph (EMA) systems are useful to study the motor control of speech articulators and also to construct models of the speech production process. In the EMA system, the position of the receiver coil is predicted on the basis of a field function representing a spatial pattern of the magnetic field in relation to the relative position between the transmitter and receiver coils. This paper presents a new method of representing the magnetic field by using a multivariate spline function to overcome the problem of the field pattern having local fluctuations caused by interference between the transmitting signals. A procedure for determining the receiver position is also presented, and the piecewise property of the basis functions enables the spline function to flexibly approximate the field pattern and to attain a high measurement accuracy: the mean error in estimating the receiver position was less than 0.1 mm for a 14x14-cm measurement area.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-366"
  },
  "teixeira01_eurospeech": {
   "authors": [
    [
     "A.",
     "Teixeira"
    ],
    [
     "F.",
     "Vaz"
    ]
   ],
   "title": "European portuguese nasal vowels: an EMMA study",
   "original": "e01_1483",
   "page_count": 4,
   "order": 375,
   "p1": "1483",
   "pn": "1486",
   "abstract": [
    "In this paper new EMMA data regarding European Portuguese nasals is presented. Some details about corpus constitution, recording and annotation is given. First results from analysis are presented. Quantitative analysis of velum movement was done for nasal vowels between stops. For the other contexts representative examples are presented and qualitatively analysed. In all contexts nasal vowels are produced with an initial phase having an high velum position. This result supports our previous work conclusions, of nasal vowels viewed as dynamic sounds were beginning must have dominant lips radiation. Obtained knowledge has application in articulatory synthesis, our motivation for this study.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-367"
  },
  "fuchs01_eurospeech": {
   "authors": [
    [
     "Susanne",
     "Fuchs"
    ],
    [
     "Pascal",
     "Perrier"
    ],
    [
     "Christine",
     "Mooshammer"
    ]
   ],
   "title": "The role of the palate in tongue kinematics: an experimental assessment in v sequences from EPG and EMMA data",
   "original": "e01_1487",
   "page_count": 4,
   "order": 376,
   "p1": "1487",
   "pn": "1490",
   "abstract": [
    "The effect of palatal contact on tongue tip kinematics was investigated using simultaneous EMMA and EPG recordings. The material consisted of VC sequences, where C is a voiced or voiceless alveolar stop. The kinematic characteristics were studied by analyzing parameters of the velocity profile and the deceleration peaks of the closing gesture. No evidence could be found for a potential influence of lateral contacts. Central contacts, associated with the beginning of the consonantal closure, are strongly correlated in time with the velocity drop. It supports the hypothesis that for achieving a consonantal closure tongue tip kinematics is not controlled by a specific target on the palate, and that its deceleration phase is mostly influenced by the collision with the palate.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-368"
  },
  "aylett01_eurospeech": {
   "authors": [
    [
     "Matthew P.",
     "Aylett"
    ]
   ],
   "title": "Modelling care of articulation with HMMs is dangerous",
   "original": "e01_1491",
   "page_count": 4,
   "order": 377,
   "p1": "1491",
   "pn": "1494",
   "abstract": [
    "Changes in care of articulation (COA) affect both the spectral and durational characteristics of speech. This can have severe repercussions on both the success of speech recognition, and the quality of speech synthesis. Although auto-segmentation has proven useful for measuring the durational effects of COA, an automatic spectral measurement has proven more problematic. In this paper, we will explore the use of the acoustic log likelihoods generated by HMM autosegmentation as a measure of these changes in comparison with two phonetically motivated modeling systems based on vocalic F1/F2 values. When duration variation is controlled, the HMM output does not correlate with the human perception of vowel goodness, whereas, the phonetically motivated models do.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-369"
  },
  "murphy01_eurospeech": {
   "authors": [
    [
     "Peter J.",
     "Murphy"
    ]
   ],
   "title": "Spectral tilt as a perturbation-free measurement of noise levels in voice signals",
   "original": "e01_1495",
   "page_count": 4,
   "order": 378,
   "p1": "1495",
   "pn": "1498",
   "abstract": [
    "Acoustic analysis of voice quality proves useful in the objective assessment of voice disorders and for motivating new components for use in improving voice synthesis. A commonly used quantitative spectral index is the harmonics-to-noise ratio (HNR), which gives gross information regarding speech signal periodicity. However, as the measure is sensitive to all forms of waveform aperiodicities (not simply the additive random noise component of turbulent origin), it lacks specificity. Furthermore, the HNR of the radiated speech waveform has a fundamental frequency (f0) -dependence, increasing with fundamental frequency (for equal noise levels of the glottal source). Two spectral tilt measurements are applied to synthetically generated, aperiodic voice signals to investigate their sensitivity to the various forms of aperiodicity. The tilt measures are found to provide perturbation- (jitter and shimmer) free measures of noise levels in speech signals. However, for radiated speech waveforms the tilt measurements are strongly f0- dependent.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-370"
  },
  "schoentgen01_eurospeech": {
   "authors": [
    [
     "Jean",
     "Schoentgen"
    ]
   ],
   "title": "Estimation of the modulation frequency and modulation depth of the fundamental frequency owing to vocal micro-tremor of the voice source signal",
   "original": "e01_1499",
   "page_count": 4,
   "order": 379,
   "p1": "1499",
   "pn": "1502",
   "abstract": [
    "The aim of the article is to present a method for estimating the modulation frequency and modulation level owing to micro-tremor of the vocal fundamental frequency. Vocal micro-tremor designates the modulation of the fundamental frequency of the voice source signal owing to physiological tremor of normal speakers. The analysis is based on the spectral density function of the time series of the glottal cycle lengths. In the spectrum the modulation owing to micro-tremor gives rise to prominent spectral peaks which are positioned at the modulation frequencies. We discuss the results obtained for 38 normal male and female speakers and compare the modulation levels and frequencies to those obtained by others by means of a demodulation of the speech signal.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-371"
  },
  "dinther01_eurospeech": {
   "authors": [
    [
     "Ralph van",
     "Dinther"
    ],
    [
     "Raymond N.J.",
     "Veldhuis"
    ],
    [
     "Armin",
     "Kohlrausch"
    ]
   ],
   "title": "The perceptual relevance of glottal-pulse parameter variations",
   "original": "e01_1503",
   "page_count": 4,
   "order": 380,
   "p1": "1503",
   "pn": "1506",
   "abstract": [
    "The perceptual relevance of changes to glottal-pulse parameters is studied. First, it is demonstrated that a distance measure based on excitation patterns can predict audibility discrimination thresholds for small changes to the R parameters of the Liljencrants-Fant (LF) model. Next, by using this measure the perceptual relevance of the LF parameters is quantified. Results are presented for a number of sets of glottal-pulse parameters that were taken from literature, representing distinct voice qualities.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-372"
  },
  "ogner01_eurospeech": {
   "authors": [
    [
     "Marcel",
     "Ogner"
    ],
    [
     "Zdravko",
     "Kacic"
    ]
   ],
   "title": "Speaker normalization based on test to reference speaker mapping",
   "original": "e01_1507",
   "page_count": 4,
   "order": 381,
   "p1": "1507",
   "pn": "1510",
   "abstract": [
    "The paper presents the speaker normalization technique we implemented in a teach ing and training system for hearing handicapped children with the goal to reduce inter-speaker variability in time-frequency speech representation. In an effort to reduce variance caused by variation in vocal tract shape among speakers, a formant based nonlinear frequency warping approach to vocal tract normalization i s investigated. The proposed method can be efficiently realized in an Analysis by Synthesis framework. After the speech decomposition into the vocal tract envelope and excitation model, the vocal tract envelope is warped by the estimated frequency war ping function, while the excitation characteristics are mapped to the reference speaker excitation. The results have shown significant spectral distance decrease for correctly pronounced words between test and the reference speaker after the normalization has been applied, while for poor pronunciation by the test speaker the spectral distance remains relatively high.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-373"
  },
  "pitermann01_eurospeech": {
   "authors": [
    [
     "Michel",
     "Pitermann"
    ],
    [
     "Kevin G.",
     "Munhall"
    ]
   ],
   "title": "A face-to-muscle inversion of a biomechanical face model for audiovisual and motor control research",
   "original": "e01_1511",
   "page_count": 4,
   "order": 382,
   "p1": "1511",
   "pn": "1514",
   "abstract": [
    "Muscle-based models of the human face produce high quality animation but estimating modeled muscle activities has not been satisfying solved yet. In this paper we present a dynamic inversion of a muscle-based model that permits the animation to be created from kinematic recordings of facial movements. Using a nonlinear optimizer (Powell's algorithm) the inversion produces a muscle activity set for 16 muscle groups in the lower face that minimize the root mean square error between kinematic data recorded with OPTOTRAK and the corresponding nodes of the modeled facial mesh. This inverted muscle activity is then used to animate the facial model. The results of a first experiment showed that the inversion-synthesis method can accurately reproduce a synthetic facial animation, even for a partial sampling of the face. The results of a second experiment showed that the method is as successful for OPTOTRAK recording of a talker uttering a sentence.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-374"
  },
  "south01_eurospeech": {
   "authors": [
    [
     "Allan J",
     "South"
    ]
   ],
   "title": "A model of vowel production under positive pressure breathing",
   "original": "e01_1515",
   "page_count": 4,
   "order": 383,
   "p1": "1515",
   "pn": "1518",
   "abstract": [
    "Future combat aircraft using speech recognition in the cockpit interface will also use positive pressure breathing (PPB) to allow operation at high G levels. This paper describes work which extends the n-tube model of vowel production to include intra-oral pressure. The aim is to improve speech recogniser performance under these conditions. An 8-tube DRM model was used, with the assumption of uniform compliance in all regions of the vocal tract. A side branch was added to simulate the oesophagus. The model shows that as the pressure is increased, the vowel space in the F1/F2 plane shrinks towards the region of F1 = 400 Hz, F2 = 1200 Hz. Measurements made on real speech show a similar trend, but the reduction in the range of F2 is less than that predicted by the model, probably as a result of variation of compliance in different areas of the vocal tract.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-375"
  },
  "podhorski01_eurospeech": {
   "authors": [
    [
     "Adam",
     "Podhorski"
    ],
    [
     "Marek",
     "Czepulonis"
    ]
   ],
   "title": "Helium speech normalisation by codebook mapping",
   "original": "e01_1519",
   "page_count": 4,
   "order": 384,
   "p1": "1519",
   "pn": "1523",
   "abstract": [
    "In this paper we present a non-parametric approach to solving the helium speech problem. Properties of helium speech are replaced by those pertaining to normal speech by means of codebook mapping of spectral envelopes. This method eliminates the drawbacks inherent in the previous procedures of helium speech unscrambling as it requires neither model of helium speech production nor estimation of formant parameters. The only assumption is the general source-filter model required for linear prediction analysis. In the traditional approach spectral transformations were computed based on the assumed helium speech production model. And in the non-model approach it was assumed that helium speech distortion is speaker dependent, so all spectral transformations were calculated from formant parameters and F0 extracted directly from speech signals. In all previous methods the resulting speech was still retaining a nasal quality due to inaccurate modelling and speech processing schemes that were unable to guarantee independent manipulation of formant parameters. On the contrary our system results in speech that is completely free of the hyperbaric helium quality however its technical quality is still unsatisfactory as the mapping introduces noise into the corrected speech.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-376"
  },
  "campbell01c_eurospeech": {
   "authors": [
    [
     "Nick",
     "Campbell"
    ]
   ],
   "title": "Building a corpus of natural speech - and tools for the processing of expressive speech",
   "original": "e01_1525",
   "page_count": 4,
   "order": 385,
   "p1": "1525",
   "pn": "1528",
   "abstract": [
    "This paper details progress during the first year of the JST/CREST ESP Project, on the creation of natural-speech databases for the analysis and synthesis of \"Expressive Speech\", and the development of software tools for parameter-extraction and speech database labeling. The research is still in its initial stages, but we now have a clearer understanding of the types of speech data that will be necessary, and of the software and speech processing tools that are available for analysis and treatment of the data. The testing of applications and prototyping in real-world situations is planned as future work, and our current task is the collection of a 1000-hour corpus of natural conversational speech upon which future research will be based.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-377"
  },
  "broeder01_eurospeech": {
   "authors": [
    [
     "Daan",
     "Broeder"
    ],
    [
     "Hennie",
     "Brugman"
    ],
    [
     "Peter",
     "Wittenburg"
    ]
   ],
   "title": "Aspects of modern multi-modal/multi-media corpora exploitation environments",
   "original": "e01_1529",
   "page_count": 4,
   "order": 386,
   "p1": "1529",
   "pn": "1532",
   "abstract": [
    "This paper wants to discuss several aspects of multimodal/multimedia language resources such as the use of metadata descriptions for easy location purposes, their collaborative annotation and exploitation via Internet, the generation of synchronized media and text streams in distributed environments, and general annotation formats. These aspects that although they may be discussed independently have to fit together seamlessly to offer users an adequate exploitation environment that is up to the huge amount of data that is available in modern multi-media corpora and is able to exploit fully the current technology advancements.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-378"
  },
  "bigbee01_eurospeech": {
   "authors": [
    [
     "Tony",
     "Bigbee"
    ],
    [
     "Dan",
     "Loehr"
    ],
    [
     "Lisa",
     "Harper"
    ]
   ],
   "title": "Emerging requirements for multi-modal annotation and analysis tools",
   "original": "e01_1533",
   "page_count": 4,
   "order": 387,
   "p1": "1533",
   "pn": "1536",
   "abstract": [
    "We review existing capabilities of multi-modal annotation and analysis tools by presenting a survey of seven representative tools, and providing a sample annotation using one system. We discuss emerging requirements including handling electronic ink, eye-gaze tracking, and other time-based considerations. We briefly review aspects of empirically evaluating tool effectiveness and suggest that multimodal interfaces in future analytical tools may be desirable. We conclude by providing a tentative list of desired features for next-generation tools.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-379"
  },
  "altosaar01_eurospeech": {
   "authors": [
    [
     "Toomas",
     "Altosaar"
    ],
    [
     "Matti",
     "Karjalainen"
    ],
    [
     "Martti",
     "Vainio"
    ]
   ],
   "title": "Three-dimensional modelling of speech corpora: added value through visualisation",
   "original": "e01_1537",
   "page_count": 4,
   "order": 388,
   "p1": "1537",
   "pn": "1540",
   "abstract": [
    "Collections of annotated spoken language have formed an important basis for the development of speech technology. Their existence has promoted speech analysis research as well as enabled robust synthesis and recognition methods to be developed. However, many complex relationships remain unspecified within a corpus due to a lack of metadata that describes the raw information in sufficient detail as well as the inter-relationships between signals, recording conditions, talkers, etc. A deficit of standards and formats, needed to express complex relationships, has also hindered the potential use and value of available corpora. This paper presents a novel three-dimensional model for exploring temporal as well as atemporal information existing in speech corpora. Examined are the potential benefits that are gained through corpus visualisation during the phases of creation, editing, verification, use, and exploration. The paper suggests that by providing a threedimensional model of speech data, more of the inherent and potential value of a corpus can be utilised.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-380"
  },
  "turk01_eurospeech": {
   "authors": [
    [
     "Ulrich",
     "Türk"
    ]
   ],
   "title": "The technical processing in smartkom data collection: a case study",
   "original": "e01_1541",
   "page_count": 4,
   "order": 389,
   "p1": "1541",
   "pn": "1544",
   "abstract": [
    "This paper dicusses the specific technical features and processing steps of the multimodal data collection in the SmartKom project. It gives an overview of the goals of the project and the requirements to the multimodal corpus. The processing steps from the data recording to the final distribution of the data are detailed. We focus on the problem of recording temporal synchronous data from different sources and present our manual synchronization process based on standard software and hardware. In addition, we describe shortly the logistic system for organizing the working teams and managing the processing of the data.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-381"
  },
  "matassoni01_eurospeech": {
   "authors": [
    [
     "M.",
     "Matassoni"
    ],
    [
     "M.",
     "Omologo"
    ],
    [
     "P.",
     "Svaizer"
    ]
   ],
   "title": "Use of real and contaminated speech for training of a hands-free in-car speech recognizer",
   "original": "e01_1569",
   "page_count": 4,
   "order": 390,
   "p1": "1569",
   "pn": "1572",
   "abstract": [
    "A database of in-car speech for the Italian language was collected under the European projects SpeechDatCar and VODIS II. It consists of 600 sessions recorded under various noise and driving conditions and includes close-talk signals and far microphone signals for hands-free interaction. This paper describes some recognition experiments on two tasks conceived on a portion of this database: connected digit sequences and isolated command words. Recognition rate achieved by means of HMMs trained on real in-car speech is compared with that accomplished by a speech contamination approach, which aims at simulating in-car data starting from a clean speech corpus. Recognition performance is also analyzed as a function of the different noise conditions and of the consequent SNR at the far microphones. Finally, the effect of HMM adaptation is investigated in order to tune the recognizer on the conditions of the various sessions.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-382"
  },
  "plucienkowski01_eurospeech": {
   "authors": [
    [
     "Jay P.",
     "Plucienkowski"
    ],
    [
     "John H. L.",
     "Hansen"
    ],
    [
     "Pongtep",
     "Angkititrakul"
    ]
   ],
   "title": "Combined front-end signal processing for in-vehicle speech systems",
   "original": "e01_1573",
   "page_count": 4,
   "order": 391,
   "p1": "1573",
   "pn": "1576",
   "abstract": [
    "In this paper, we investigate the integration of two processing methods to improve speech quality for in-vehicle speech systems: multi-sensor beamforming and constrained iterative (Auto-LSP) speech enhancement. The intent is to establish an intelligent microphone array processing scheme in high noise environments by considering the effectiveness of a multi-sensor beamformer method and the Auto-LSP single channel speech enhancement method. The goal therefore is to design a system where the strengths of one method help compensate any potential weaknesses of the other. The noise cancellation method is an acoustic beamformer designed and constructed using a linear microphone array. The speech enhancement method is the constrained iterative Auto-LSP approach, previously considered for single channel enhancement. After establishing the combined processing scheme, evaluations are performed using speech and acoustic noise data collected in vehicles. Noise suppression levels by the beamformer is established for different road noise conditions. Quality improvement from the enhancement scheme is assessed using objective speech quality measures over a test speech corpus using TIMIT data. The results show that while beamforming alone can suppress background noise levels, the combination of beamforming and constrained enhancement can provide as much as a 63% improvement in objective quality, suggesting a potential single comprehensive solution for in-vehicle speech systems.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-383"
  },
  "selouani01_eurospeech": {
   "authors": [
    [
     "Sid-Ahmed",
     "Selouani"
    ],
    [
     "Hesham",
     "Tolba"
    ],
    [
     "Douglas",
     "OShaughnessy"
    ]
   ],
   "title": "Robust automatic speech recognition in low-SNR car environments by the application of a connectionist subspace-based approach to the melbased cepstral coefficients",
   "original": "e01_1577",
   "page_count": 4,
   "order": 392,
   "p1": "1577",
   "pn": "1580",
   "abstract": [
    "In this paper, the problem of robust continuous-speech recognition (CSR) in the presence of highly interfering car noise has been considered. Our approach is based on the noise reduction of the parameters that we use for recognition, that is, the Mel-based cepstral coefficients. This is achieved by the use of a Multilayer Perceptron (MLP) network for noise reduction in the cepstral domain in order to get less-variant parameters. Then, the obtained enhanced features are {it refined} via the Karhunen-Loève Transform (KLT) implemented using the Principal Component Analysis (PCA). Experiments show that the use of the enhanced parameters using such an approach increases the recognition rate of the CSR process in highly interfering car noise environments. Results show that the proposed hybrid technique when included in the front-end of an HTK-based CSR system, outperforms that of the conventional recognition process based on either a KLT- or an MLP-based preprocessing recognition in severe interfering car noise environments for a wide range of SNRs varying from 16 dB to -4 dB using a noisy version of the TIMIT database.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-384"
  },
  "korthauer01_eurospeech": {
   "authors": [
    [
     "Andreas",
     "Korthauer"
    ]
   ],
   "title": "Recognition of spelled city names in automotive environments",
   "original": "e01_1581",
   "page_count": 4,
   "order": 393,
   "p1": "1581",
   "pn": "1584",
   "abstract": [
    "This contribution presents the development and evaluation of a spelled letter recognizer for automotive environments. Specifically, the spoken language dialog for the navigation system requires reliable recognition of thousands of city names. In this context the recognition of spelling sequences is needed as fall-back strategy and for the disambiguation of similar sounding names. For that purpose we have developed a speaker-independent spelled letter recognizer on the basis of hidden Markov models using the HTK toolkit. Speech data which have been collected in real-world driving situations are used for the training of the hidden Markov models. Several feature extraction schemes were investigated and compared with regard to the recognition performance of the system. The best results for both arbitrary spelling sequences and constrained city name recognition are achieved by a system with two-channel LDA and integrated noise reduction.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-385"
  },
  "lleida01_eurospeech": {
   "authors": [
    [
     "Eduardo",
     "Lleida"
    ],
    [
     "Enrique",
     "Masgrau"
    ],
    [
     "Alfonso",
     "Ortega"
    ]
   ],
   "title": "Acoustic echo control and noise reduction for cabin car communication",
   "original": "e01_1585",
   "page_count": 4,
   "order": 394,
   "p1": "1585",
   "pn": "1588",
   "abstract": [
    "A Cabin Car Communication System (CCCS) has the goal of improving the communication among passengers inside the car. Wind, road and engine noise, the distance between passengers and other factors make difficult the communication inside vehicles. The driver must often look away from the road and passengers move out of normal seating positions. The CCCS makes use of a set of microphones to pick up the speech and the car-audio loudspeakers to reinforce the sound level. This scenario presents a great challenger for acoustic echo control and noise reduction. Acoustic echo control must prevent the overall system from howling and becoming unstable with the additional problem that the system must always work with double talk. The noise reduction must clean the microphone signal to avoid the reinforce of the noise inside the car. In this paper, we describe a combined acoustic echo control and noise reduction algorithm suitable for cabin car communication systems. A real-time system has been developed working together with the European Technological Center of Lear Corporation.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-386"
  },
  "hazen01_eurospeech": {
   "authors": [
    [
     "Timothy J.",
     "Hazen"
    ],
    [
     "I. Lee",
     "Hetherington"
    ],
    [
     "Alex",
     "Park"
    ]
   ],
   "title": "FST-based recognition techniques for multi-lingual and multi-domain spontaneous speech",
   "original": "e01_1591",
   "page_count": 4,
   "order": 395,
   "p1": "1591",
   "pn": "1594",
   "abstract": [
    "In this paper we present techniques for building multi-domain and multilingual recognizers within a finite-state transducer (FST) framework. The flexibility of the FST approach is also demonstrated on the task of incorporating networks modeling different types of non-speech events into an existing word lattice network. The ability to create robust multi-domain and/or multi-lingual recognizers for spontaneous speech will enable a conversational system to switch seamlessly and automatically among different domains and/or languages. Preliminary results using a bi-domain recognizer exhibit only small recognition accuracy degradation in comparison to domain-dependent recognition. Similarly promising results were observed using a bi-lingual recognizer which performs simultaneous language identification and recognition. When using the FST techniques to add non-speech models to the recognizer, experiments show a 10% reduction in word error rate across all utterances and a 30% reduction on utterances containing non-speech events.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-387"
  },
  "boulianne01_eurospeech": {
   "authors": [
    [
     "Gilles",
     "Boulianne"
    ],
    [
     "Pierre",
     "Ouellet"
    ],
    [
     "Pierre",
     "Dumouchel"
    ]
   ],
   "title": "A transducer approach to word graph generation",
   "original": "e01_1595",
   "page_count": 4,
   "order": 396,
   "p1": "1595",
   "pn": "1598",
   "abstract": [
    "We describe word graph generation in terms of transducer composition, and show that a simple modification to a Viterbi search avoids the usual assumptions of word-pair or phone-pair approximations when the search space is represented with a transducer detailed down to the level of HMM transitions. On a 20,000-word French language dictation task, this graph generation method increases recognition time by only 20%. The word graphs produced can be further reduced in size by applying automata minimization, and this operation can be done faster than realtime. When the resulting graphs are rescored using larger acoustic and language models, recognition rate remains near-optimal for word graph densities as low as 8 words per spoken word.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-388"
  },
  "hetherington01_eurospeech": {
   "authors": [
    [
     "I. Lee",
     "Hetherington"
    ]
   ],
   "title": "An efficient implementation of phonological rules using finite-state transducers",
   "original": "e01_1599",
   "page_count": 4,
   "order": 397,
   "p1": "1599",
   "pn": "1602",
   "abstract": [
    "Context-dependent phonological rules are used to model the mapping from phonemes to their varied phonetic surface realizations. Others, most notably Kaplan and Kay, have described how to compile general context-dependent phonological rewrite rules into finite-state transducers. Such rules are very powerful, but their compilation is complex and can result in very large nondeterministic automata. In this paper we present a simplified rewrite rule system and a technique to efficiently compile such a system into finite-state transducers.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-389"
  },
  "mohri01_eurospeech": {
   "authors": [
    [
     "Mehryar",
     "Mohri"
    ],
    [
     "Michael",
     "Riley"
    ]
   ],
   "title": "A weight pushing algorithm for large vocabulary speech recognition",
   "original": "e01_1603",
   "page_count": 4,
   "order": 398,
   "p1": "1603",
   "pn": "1606",
   "abstract": [
    "Weighted finite-state transducers provide a general framework for the representation of the components of speech recognition systems; language models, pronunciation dictionaries, context-dependent models, HMM-level acoustic models, and the output word or phone lattices can all be represented by weighted automata and transducers. In general, a representation is not unique and there may be different weighted transducers realizing the same mapping. In particular, even when they have exactly the same topology with the same input and output labels, two equivalent transducers may differ by the way the weights are distributed along each path. We present a \"weight pushing\" algorithm that modifies the weights of a given weighted transducer in a way such that the transition probabilities form a stochastic distribution. This results in an equivalent transducer whose weight distribution is more suitable for pruning and speech recognition. We demonstrate substantial improvements of the speed of our recognition system in several tasks based on the use of this algorithm. We report a 45% speedup at 83% word accuracy with a simple single-pass 40,000-word vocabulary North American Business News (NAB) recognition system on the DARPA Eval '95 test set. With the same technique, we report a 550% speedup at 88% word accuracy in rescoring NAB word lattices with more accurate 2nd-pass models. We finally report a 280% speedup at 68% word accuracy for 100,000 first name-last name pairs recognition.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-390"
  },
  "seward01_eurospeech": {
   "authors": [
    [
     "Alexander",
     "Seward"
    ]
   ],
   "title": "Transducer optimizations for tight-coupled decoding",
   "original": "e01_1607",
   "page_count": 4,
   "order": 399,
   "p1": "1607",
   "pn": "1612",
   "abstract": [
    "In this paper we apply a framework of finite-state transducers (FST) to uniformly represent various information sources and data-structures used in speech recognition. These source models include context-free language models, phonology models, acoustic model information (Hidden Markov Models), and pronunciation dictionaries. We will describe how this unified representation can serve as a single input model for the recognizer. We will demonstrate how the application of various levels of optimizations can lead to a more compact representation of these transducers and evaluate the effects on recognition performance, in terms of accuracy and computational complexity.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-391"
  },
  "wijngaarden01_eurospeech": {
   "authors": [
    [
     "Sander J. van",
     "Wijngaarden"
    ],
    [
     "Paula M.T.",
     "Smeele"
    ],
    [
     "Herman J.M.",
     "Steeneken"
    ]
   ],
   "title": "A new method for testing communication efficiency and user acceptability of speech communication channels",
   "original": "e01_1675",
   "page_count": 4,
   "order": 400,
   "p1": "1675",
   "pn": "1678",
   "abstract": [
    "The performance of speech communication channels featuring long delay times is usually subjectively experienced as lower than similar channels without delay. Yet most conventional speech intelligibility and speech quality tests are not sensitive to the effects of delay. Moreover, these conventional test do not take the effects of human compensating strategies into account, which help cope with adverse communication conditions by adapting our speech. Test types that do incorporate such effects are sometimes known as 'speech communicability' tests. Based on the lessons learned from literature on speech communicability testing, a list of requirements for the design of a good communicability test method was composed, followed by the actual design of a new test method combining attractive features of existing communicability tests. The suitability of the test design was verified by conducting a pilot experiment. The results of this experiment show that the new method is capable of measuring efficiency and acceptability, and is sufficiently sensitive to delay and background noise.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-392"
  },
  "cucchiarini01_eurospeech": {
   "authors": [
    [
     "Catia",
     "Cucchiarini"
    ],
    [
     "Diana",
     "Binnenpoorte"
    ],
    [
     "Simo",
     "Goddijn"
    ]
   ],
   "title": "Phonetic transcriptions in the spoken dutch corpus: how to combine efficiency and good transcription quality",
   "original": "e01_1679",
   "page_count": 4,
   "order": 401,
   "p1": "1679",
   "pn": "1682",
   "abstract": [
    "This paper reports on an experiment aimed at establishing how phonetic transcriptions for the large CGN corpus can be obtained most efficiently. This experiment explores the po-tential of an automatically generated transcription (AGT) by comparing an AGT with a reference transcription (Tref) of the same material, to determine whether and how the AGT can be improved to make it more similar to Tref. The results indicate that the AGT can be optimized through pronunciation variation modelling so as to make human corrections more efficient or even superfluous, at least for some speech styles.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-393"
  },
  "hutchinson01_eurospeech": {
   "authors": [
    [
     "Ben",
     "Hutchinson"
    ]
   ],
   "title": "A functional approach to speech recognition evaluation",
   "original": "e01_1683",
   "page_count": 4,
   "order": 402,
   "p1": "1683",
   "pn": "1686",
   "abstract": [
    "The paper describes a new evaluation measure for speech recognition in spoken language dialogue systems. The measure is based on the usefulness of the recognition for the system, and the usefulness is measured at the level of meaning representation. It is argued that the new measure is more useful than word error rate, and is more accurate than simpler functional measures.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-394"
  },
  "moller01_eurospeech": {
   "authors": [
    [
     "Sebastian",
     "Möller"
    ],
    [
     "Jens",
     "Berger"
    ]
   ],
   "title": "Instrumental derivation of equipment impairment factors for describing telephone speech codec degradations",
   "original": "e01_1687",
   "page_count": 4,
   "order": 403,
   "p1": "1687",
   "pn": "1690",
   "abstract": [
    "The impairment factor methodology has been adopted by the ITU-T for describing the relative impact of telephone transmission degradations on the overall quality of transmitted speech. Input parameters to this methodology are mainly instrumentally measurable characteristics of the transmission path, with the exception of low bit-rate codecs, whose perceptual characteristics still have to be determined in auditory tests. In this paper, we describe a new approach for deriving impairment factors for low bit-rate codecs in a purely instrumental way. Using instrumental quality prediction models like PESQ or TOSQA, quality estimations are obtained which can be combined with other degradations in order to obtain an overall quality estimation for the whole transmission channel. A comparison with defined values for well-known codecs shows a high correlation of instrumentally derived impairment factors with the corresponding defined values, as well as with auditory test data.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-395"
  },
  "lee01c_eurospeech": {
   "authors": [
    [
     "Akinobu",
     "Lee"
    ],
    [
     "Tatsuya",
     "Kawahara"
    ],
    [
     "Kiyohiro",
     "Shikano"
    ]
   ],
   "title": "Julius --- an open source real-time large vocabulary recognition engine",
   "original": "e01_1691",
   "page_count": 4,
   "order": 404,
   "p1": "1691",
   "pn": "1694",
   "abstract": [
    "Julius is a high-performance, two-pass LVCSR decoder for researchers and developers. Based on word 3-gram and context-dependent HMM, it can perform almost real-time decoding on most current PCs in 20k word dictation task. Major search techniques are fully incorporated such as tree lexicon, N-gram factoring, cross-word context dependency handling, enveloped beam search, Gaussian pruning, Gaussian selection, etc. Besides search efficiency, it is also modularized carefully to be independent from model structures, and various HMM types are supported such as shared-state triphones and tied-mixture models, with any number of mixtures, states, or phones. Standard formats are adopted to cope with other free modeling toolkit. The main platform is Linux and other Unix workstations, and partially works on Windows. Julius is distributed with open license together with source codes, and has been used by many researchers and developers in Japan.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-396"
  },
  "toledano01_eurospeech": {
   "authors": [
    [
     "Doroteo Torre",
     "Toledano"
    ],
    [
     "Luis A. Hernández",
     "Gómez"
    ]
   ],
   "title": "Local refinement of phonetic boundaries: a general framework and its application using different transition models",
   "original": "e01_1695",
   "page_count": 4,
   "order": 405,
   "p1": "1695",
   "pn": "1698",
   "abstract": [
    "In the last few years we have been experimenting with an automatic phonetic segmentation and labeling system based on a modified HMM phonetic recognizer followed by a local phonetic boundary refinement system. During this period we have used different approaches for the local refinement, including fuzzy rules and neural networks. In this paper we present a unified framework for the local refinement of phonetic boundaries that has allowed us to thoroughly evaluate and compare these approaches and yet another one based on gaussian mixture models. Results show that neural networks outperform the rest of the approaches in speaker dependent mode, achieving a precision almost equal to a manual segmentation. In speaker independent mode, however, neural networks and fuzzy rules achieve almost the same performance, a bit worse than a manual segmentation.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-397"
  },
  "ludwig01_eurospeech": {
   "authors": [
    [
     "Thorsten",
     "Ludwig"
    ],
    [
     "Ulrich",
     "Heute"
    ]
   ],
   "title": "Detection of digital transmission systems for voice quality measurements",
   "original": "e01_1699",
   "page_count": 4,
   "order": 406,
   "p1": "1699",
   "pn": "1702",
   "abstract": [
    "In-service, Non-intrusive Measurement Devices (INMD) estimate the perceived quality of the telephone link by extracting quality-defining criteria like echo attenuation, echo delay, active speech level, noise level, frame losses and transient failures from a telephone call. In addition, the quality depends on the used digital transmission systems (codec systems). This paper proposes a method to distinguish between two codec classes. With the help of features determined from the speech signal, a classifier decides about the class affiliation of the signal. The recognition rate for signals with 16 seconds of active speech is about 97%.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-398"
  },
  "lewis01_eurospeech": {
   "authors": [
    [
     "Eric",
     "Lewis"
    ],
    [
     "Mark",
     "Tatham"
    ]
   ],
   "title": "Automatic segmentation of recorded speech into syllables for speech synthesis",
   "original": "e01_1703",
   "page_count": 4,
   "order": 407,
   "p1": "1703",
   "pn": "1706",
   "abstract": [
    "Concatenated waveform text-to-speech synthesis systems require an inventory of stored waveforms from which units of speech can be extracted for subsequent rearrangement and concatenation as needed. In previous papers [1], [2] we have argued that for natural sounding speech the syllable should be the preferred unit. The mark-up of the stored waveforms for segmentation into syllables must be precise and for our MeteoSPRUCE limited domain system the mark-up has been done by manual editing. In this paper we describe how most of the segmentation can be done automatically, leaving only those waveforms which would be prone to error to be segmented manually. With automatic labelling of both the pitch periods and the syllables the task of generating different synthetic voices to order becomes feasible.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-399"
  },
  "teixeira01b_eurospeech": {
   "authors": [
    [
     "João Paulo",
     "Teixeira"
    ],
    [
     "Diamantino",
     "Freitas"
    ],
    [
     "Daniela",
     "Braga"
    ],
    [
     "Maria João",
     "Barros"
    ],
    [
     "Vagner",
     "Latsch"
    ]
   ],
   "title": "Phonetic events from the labeling the european portuguese database for speech synthesis, FEUP/IPBDB",
   "original": "e01_1707",
   "page_count": 4,
   "order": 408,
   "p1": "1707",
   "pn": "1710",
   "abstract": [
    "In this paper a labeled new speech signal database (FEUP/IPB-DB) in Standard European Portuguese is presented. The objective of this work is, on one hand, to provide phonetic material for TTS systems construction, either from the start or to improve the quality of existing ones, and, on the other hand, to place at service of the European Portuguese scientific community a phonetically and prosodically valuable speech corpus, essential for Speech Synthesis or Phonetics research. The main features of the database will be described as well as some basic statistical aspects. A discussion of some methodological problems and some observed phenomena in experimental phonetics deriving from the speech signal labeling is also done. The approach in our work is to produce a resource that can be further improved in subsequent steps with minimal re-work. The phonetic, linguistic and technical consistency are guaranteed through the involvement of a multidisciplinary team.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-400"
  },
  "nefti01_eurospeech": {
   "authors": [
    [
     "Samir",
     "Nefti"
    ],
    [
     "Olivier",
     "Boeffard"
    ]
   ],
   "title": "Acoustical and topological experiments for an HMM-based speech segmentation system",
   "original": "e01_1711",
   "page_count": 4,
   "order": 409,
   "p1": "1711",
   "pn": "1714",
   "abstract": [
    "Several specific tasks in the field of text-to-speech synthesis requires a huge amount of labeled speech corpora. Mostly, these labels correspond to phone marks aligned on the speech waveform. Different kind of solutions have been applied to this problem from rule-based systems to stochastic-based ones. We validate here a solution based on Hidden Makov Models. Various test configurations are proposed. At the acoustic level, we compare LSP to MFCC coefficients and the fitness of multigaussians for this segmentation task. At the topological level, we compare standard left-to-right models to phonological dependent topologies. The best configuration we found is related to an MFCC analysis with standard left-to-right models and with diagonal multi-gaussians per state. For this configuration the overall root mean squared error on the test database is 18 +/- 0.3 ms within a 99% confidence interval.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-401"
  },
  "zhou01b_eurospeech": {
   "authors": [
    [
     "Qiru",
     "Zhou"
    ],
    [
     "Jinsong",
     "Zheng"
    ],
    [
     "Chin-Hui",
     "Lee"
    ]
   ],
   "title": "TclBLASR: an automatic speech recognition extension for tcl",
   "original": "e01_1715",
   "page_count": 4,
   "order": 410,
   "p1": "1715",
   "pn": "1718",
   "abstract": [
    "We present TclBLASR, a framework to integrate a proprietary speech recognition engine, an open source script language, such as Tcl/Tk and an open source sound analysis toolkit, such as Snack from KTH, into a user friendly platform that a user can write a Tcl/Tk script application quickly for speech recognition evaluation, speech data collection and automatic annotation, and speech technology demonstration. This framework is extremely useful for third party customer evaluation of speech technologies that do not involve heavy C/C++ program development and extensive knowledge on low-level speech engine APIs. Using the Bell Labs Automatic Speech Recognition (BLASR) engine, coupled with the real-time audio I/O and visualization provided by Snack and the flexible graphical user interface tools embedded in Tcl/Tk, the TclBLASR platform proves to be a useful framework for quick packaging of ASR engines for customer evaluation of the technology without extensive customization of interfaces to meet different needs from a wide range of customers.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-402"
  },
  "kessens01_eurospeech": {
   "authors": [
    [
     "Judith M.",
     "Kessens"
    ],
    [
     "Helmer",
     "Strik"
    ]
   ],
   "title": "Lower WERs do not guarantee better transcriptions",
   "original": "e01_1721",
   "page_count": 4,
   "order": 411,
   "p1": "1721",
   "pn": "1724",
   "abstract": [
    "The goal of this paper is to investigate the effect of various properties of the CSR on automatic transcription. To this end, we used various versions of a continuous speech recognizer (CSR) to make automatic transcriptions. Our results show that changing certain properties of the CSR affects the resulting automatic transcriptions. The best results were obtained when 'short' hidden Markov models (HMMs), and context-independent HMMs were used. Furthermore, we found that minimizing the amount of contamination in the HMMs improves the quality of the automatic transcriptions. Another important result is that there does not appear to be a straightforward relation between word error rate (WER) and the transcription quality. In other words: A CSR with a lower WER does not always guarantee better transcriptions.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-403"
  },
  "chang01_eurospeech": {
   "authors": [
    [
     "Shuangyu",
     "Chang"
    ],
    [
     "Steven",
     "Greenberg"
    ],
    [
     "Mirjam",
     "Wester"
    ]
   ],
   "title": "An elitist approach to articulatory-acoustic feature classification",
   "original": "e01_1725",
   "page_count": 4,
   "order": 412,
   "p1": "1725",
   "pn": "1728",
   "abstract": [
    "A novel framework for automatic articulatory-acoustic feature extraction has been developed for enhancing the accuracy of place- and manner-of-articulation classification in spoken language. The \"elitist\" approach focuses on frames for which neural network (MLP) classifiers are highly confident, and discards the rest. Using this method, it is possible to achieve a frame-level accuracy of 93% for manner information on a corpus of American English sentences passed through a telephone network (NTIMIT). Place information is extracted for each manner class independently, resulting in an appreciable gain in place-feature classification relative to performance for a manner-independent system. The elitist framework provides a potential means of automatically annotating a corpus at the phonetic level without recourse to a word-level transcript and could thus be of utility for developing training materials for automatic speech recognition and speech synthesis applications, as well as aid the empirical study of spoken language.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-404"
  },
  "wester01_eurospeech": {
   "authors": [
    [
     "Mirjam",
     "Wester"
    ],
    [
     "Steven",
     "Greenberg"
    ],
    [
     "Shuangyu",
     "Chang"
    ]
   ],
   "title": "A dutch treatment of an elitist approach to articulatory-acoustic feature classification",
   "original": "e01_1729",
   "page_count": 4,
   "order": 413,
   "p1": "1729",
   "pn": "1732",
   "abstract": [
    "A novel approach to articulatory-acoustic feature extraction has been developed for enhancing the accuracy of classification associated with place and manner of articulation information. This \"elitist\" approach is tested on a corpus of spontaneous Dutch using two different systems, one trained on a subset of the same corpus, the other trained on a corpus from a different language (American English). The feature dimensions, voicing and manner of articulation transfer relatively well between the two languages. However, place information transfers less well. Manner-specific training can be used to improve classification of articulatory place information.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-405"
  },
  "galley01_eurospeech": {
   "authors": [
    [
     "Michel",
     "Galley"
    ],
    [
     "Eric",
     "Fosler-Lussier"
    ],
    [
     "Alexandros",
     "Potamianos"
    ]
   ],
   "title": "Hybrid natural language generation for spoken dialogue systems",
   "original": "e01_1735",
   "page_count": 4,
   "order": 414,
   "p1": "1735",
   "pn": "1738",
   "abstract": [
    "The natural language generation component of most dialogue systems is based on templates. Template-based generators are hard to maintain and reuse, and the sentences they produce lack the variability and robustness needed by conversational systems. In this paper, a flexible and domain-independent natural language generator for spoken dialogue systems is proposed which combines fixed surface expressions with freely generated text. The generation algorithm follows a hybrid approach, combining finite state machine (FSM) grammars and corpus-based language models. In this approach, the FSM grammar (a reversible parser grammar) is constrained by a word and concept n-gram that takes terminals and non-terminal co-occurrences into account. The n-gram grammar helps prevent inappropriate derivations, therefore improving the quality of the generated texts. The proposed algorithm achieves faster than real-time performance because of the limited number of derivations.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-406"
  },
  "cook01_eurospeech": {
   "authors": [
    [
     "Nicholas J.",
     "Cook"
    ],
    [
     "Ian D.",
     "Benest"
    ]
   ],
   "title": "The generation of speech for a search guide",
   "original": "e01_1739",
   "page_count": 4,
   "order": 415,
   "p1": "1739",
   "pn": "1742",
   "abstract": [
    "A major problem with any interface to a hierarchical information system, however shallow the hierarchy might be, is that information below the current level is hidden from view. To determine whether there is useful information at any level below the current one, requires an inference-based look-and-ponder process followed by a tedious point-click-waitread-back process of manipulation. This equally applies to the results obtained from a search engine. An alternative is to provide a search interface that offers oral cues to buried information and relies on the intelligence of the user to recognise the usefulness behind the cues. The result will be a conversational search guide and this paper addresses the production of speech utterances, using pre-recorded speech, so that the guide remains almost as fresh as a human guide.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-407"
  },
  "araki01_eurospeech": {
   "authors": [
    [
     "Masahiro",
     "Araki"
    ],
    [
     "Tasuku",
     "Ono"
    ],
    [
     "Kiyoshi",
     "Ueda"
    ],
    [
     "Takuya",
     "Nishimoto"
    ],
    [
     "Yasuhisa",
     "Niimi"
    ]
   ],
   "title": "An automatic dialogue system generator from the internet information contents",
   "original": "e01_1743",
   "page_count": 4,
   "order": 416,
   "p1": "1743",
   "pn": "1746",
   "abstract": [
    "We propose a semi-automatic dialogue system generator from the Internet information contents. We classify the practical Web site into three classes of task: slot-filling, database search, and explanation. Using three levels of dialogue library for each task, our generator translates XML based Web site into VoiceXML, which controls a conversation between a user and a computer system. In this paper, we explain an outline of our project and report implementation examples.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-408"
  },
  "rogati01_eurospeech": {
   "authors": [
    [
     "Monica",
     "Rogati"
    ],
    [
     "Marilyn",
     "Walker"
    ],
    [
     "Owen",
     "Rambow"
    ]
   ],
   "title": "Training a sentence planner for spoken dialog: the impact of syntactic and planning features",
   "original": "e01_1747",
   "page_count": 4,
   "order": 417,
   "p1": "1747",
   "pn": "1750",
   "abstract": [
    "The dialog manager of a spoken dialog system often performs domain dependent functions as well as general dialog tasks. It is possible to separate the domain specific knowledge from knowledge about language using techniques from natural language generation. However a natural language generator often has to be tuned for particular applications. In this work, we describe a new method for automatically training the natural language generator and examine the role that domain specific and domain independent features have on performance. We show that although the general features have the largest impact, the use of domain specific features improves performance, while still retaining the benefits of automatic domain customization through training.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-409"
  },
  "vivaracho01_eurospeech": {
   "authors": [
    [
     "Carlos E.",
     "Vivaracho"
    ],
    [
     "Javier",
     "Ortega-García"
    ],
    [
     "Luis",
     "Alonso"
    ],
    [
     "Quiliano I.",
     "Moro"
    ]
   ],
   "title": "A comparative study of MLP-based artificial neural networks in text-independent speaker verification against GMM-based systems",
   "original": "e01_1753",
   "page_count": 4,
   "order": 418,
   "p1": "1753",
   "pn": "1757",
   "abstract": [
    "Text-independent speaker verification is an interesting task where the use of Gaussian Mixture Models is almost a must. Nevertheless, some preliminar encouraging results obtained in previous works using ANN in speaker verification have led us to consider to perform a direct comparison between these different methods. In this sense, this paper is only focused on the classification stage of both GMM-based and ANNbased speaker verification systems. Experiments are accomplish making use of the AHUMADA/GAUDI spanish speech database, specially oriented for speaker-recognition tasks as it contains multisession and multichannel data of about 500 speakers. Results confirm a better performance when using GMM-based system and microphonic speech but, on the other hand, when testing in specific conditions and with real telephone speech ANN outperforms GMM results.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-410"
  },
  "fine01_eurospeech": {
   "authors": [
    [
     "Shai",
     "Fine"
    ],
    [
     "Jiri",
     "Navratil"
    ],
    [
     "Ramesh A.",
     "Gopinath"
    ]
   ],
   "title": "Enhancing GMM scores using SVM \"hints\"",
   "original": "e01_1757",
   "page_count": 4,
   "order": 419,
   "p1": "1757",
   "pn": "1760",
   "abstract": [
    "This paper proposes a classification scheme that combines statistical models and support vector machines. It exploits the fact that GMM and SVM classifiers with roughly the same level of performance produce uncorrelated errors. We describe a novel scheme which employs an SVM classifier as an ``advisor'' to the GMM classifier in uncertain cases. The utility of the combined generative/discriminative approach is demonstrated on standard text-independent speaker verification and speaker identification tasks in matched and mismatched training and test conditions. Results indicate significant improvements in performance without much computational overhead.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-411"
  },
  "kharroubi01_eurospeech": {
   "authors": [
    [
     "Jamal",
     "Kharroubi"
    ],
    [
     "Dijana",
     "Petrovska-Delacretaz"
    ],
    [
     "Gerard",
     "Chollet"
    ]
   ],
   "title": "Combining GMM's with suport vector machines for text-independent speaker verification",
   "original": "e01_1761",
   "page_count": 4,
   "order": 420,
   "p1": "1761",
   "pn": "1764",
   "abstract": [
    "Current best performing speaker recognition algorithms are based on Gaussian Mixture Models (GMM). Their results are not satisfactory for all experimental conditions, especially for the mismatched between train and test conditions. Support Vector Machine is a new and very promissing technique in statistical learning theory. Recently, this technique produced very interesting results in image processing and for the fusion of experts in biometric authentification. In this paper we address the issue of using the Support Vector Learning technique in combination with the currently well performing GMM models, in order to improve speaker verification results.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-412"
  },
  "gu01b_eurospeech": {
   "authors": [
    [
     "Yong",
     "Gu"
    ],
    [
     "Trevor",
     "Thomas"
    ]
   ],
   "title": "A text-independent speaker verification system using support vector machines classifier",
   "original": "e01_1765",
   "page_count": 4,
   "order": 421,
   "p1": "1765",
   "pn": "1768",
   "abstract": [
    "In the recent years the technology for speaker verification or call authentication has received an increasing amount of attention in IVR industry. However due to the complexity of speaker information embedded in the speech signals the current technology still can not produce the verification accuracy to meet the requirement for some applications. In this paper we introduce a new pattern classification approach, support vector machines (SVM) for the text-independent speaker verification. The SVM is a new way of statistical learning based on a principle of structural risk minimisation. In the paper various evaluation results for the SVM verification system are presented and a comparison with a baseline GMM approach is also given. The results demonstrate that the SVM approach perform much better than the GMM approach. On the same training and testing data set the SVM approach gives an EER 1.2% versus 3.9% EER from the GMM approach.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-413"
  },
  "stapert01_eurospeech": {
   "authors": [
    [
     "Robert P.",
     "Stapert"
    ],
    [
     "John S.",
     "Mason"
    ]
   ],
   "title": "A segmental mixture model for speaker recognition",
   "original": "e01_2509",
   "page_count": 4,
   "order": 422,
   "p1": "2509",
   "pn": "2512",
   "abstract": [
    "Standard Gaussian mixture modelling does not possess time sequence information (TSI) other than that which might be embedded in the acoustic features. Dynamic time warping relates directly to TSI, time-warping two sequences of features into alignment. Here, a hybrid system embedding DTW into a GMM is presented. Improved automatic speaker verification performance is demonstrated. Testing 1000 speakers in a fully text independent, world-model-adapted mode shows an equal error improvement over a standard GMM from 4.1% to 3.8%.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-414"
  },
  "blouet01_eurospeech": {
   "authors": [
    [
     "Raphael",
     "Blouet"
    ],
    [
     "Frédéric",
     "Bimbot"
    ]
   ],
   "title": "Tree based score computation for speaker verification",
   "original": "e01_2513",
   "page_count": 4,
   "order": 423,
   "p1": "2513",
   "pn": "2516",
   "abstract": [
    "This paper proposes an original approach to the task of speaker verification, in which the training process consists in a direct modeling of the score function. It divides the parameter space in disjoint regions where a score can be obtained as a simple function of the vector position in the region. The aim of this approach is, on the one hand to overcome some undesirable properties of the gaussian mixture models (GMMs), and on the other hand, to speed up the decision process. First, we present the formalism of probabilistic speaker verification and we discuss some motivations for exploring alternative approaches. We then describe a method currently under investigation, which is based on a binary recursive partition of the acoustic parameter space into regions to which an elementary scoring function is associated. Finally, we provide illustrations and preliminary results of the method, together with conclusions and perspectives.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-415"
  },
  "andrews01_eurospeech": {
   "authors": [
    [
     "Walter D.",
     "Andrews"
    ],
    [
     "Mary A.",
     "Kohler"
    ],
    [
     "Joseph P.",
     "Campbell"
    ]
   ],
   "title": "Phonetic speaker recognition",
   "original": "e01_2517",
   "page_count": 4,
   "order": 424,
   "p1": "2517",
   "pn": "2520",
   "abstract": [
    "This paper introduces a novel language-independent speaker-recognition system based on differences among speakers in dynamic realization of phonetic features (i.e., pronunciation) rather than spectral differences in voice quality. The system exploits phonetic information from six languages to perform text independent speaker recognition. All experiments were performed on the NIST 2001 Speaker Recognition Evaluation Extended Data Task. Recognition results are provided for each of the six language front ends and for various fusions. The fusion results demonstrate that speaker recognition capability for speech in languages outside the system is successful.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-416"
  },
  "doddington01_eurospeech": {
   "authors": [
    [
     "George",
     "Doddington"
    ]
   ],
   "title": "Speaker recognition based on idiolectal differences between speakers",
   "original": "e01_2521",
   "page_count": 4,
   "order": 425,
   "p1": "2521",
   "pn": "2524",
   "abstract": [
    "Familiar speaker information is explored using non-acoustic features in NIST's new extended data speaker detection task. Word unigrams and bigrams, used in a traditional target/background likelihood ratio framework, are shown to give surprisingly good performance. Performance continues to improve with additional training and/or test data. Bigram performance is also found to be a function of target/model sex and age difference. These initial experiments strongly suggest that further exploration of familiar speaker characteristics will likely be an extremely interesting and valuable research direction for recognition of speakers in conversational speech.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-417"
  },
  "hori01_eurospeech": {
   "authors": [
    [
     "Chiori",
     "Hori"
    ],
    [
     "Sadaoki",
     "Furui"
    ]
   ],
   "title": "Advances in automatic speech summarization",
   "original": "e01_1771",
   "page_count": 4,
   "order": 426,
   "p1": "1771",
   "pn": "1774",
   "abstract": [
    "This paper reports recent advances in automatic speech summarization method. In our proposed method, a set of words maximizing a summarization score is extracted from automatically transcribed speech. This extraction is performed according to a target compression ratio using a dynamic programming technique. The extracted set of words is then connected to build a summarized sentence. The summarization score consists of a word significance measure, a confidence measure, linguistic likelihood, and a word concatenation probability which is determined by a dependency structure in the original speech given by Stochastic Dependency Context Free Grammar. Japanese broadcast news speech transcribed using a large vocabulary continuous speech recognition system is summarized and evaluated in comparison with manual summarization by human subjects. The manual summarization results are combined to build a word network, and word accuracy of each automatic summarization result is calculated comparing with the most similar word string in the network.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-418"
  },
  "hacioglu01_eurospeech": {
   "authors": [
    [
     "Kadri",
     "Hacioglu"
    ],
    [
     "Wayne",
     "Ward"
    ]
   ],
   "title": "A word graph interface for a flexible concept based speech understanding framework",
   "original": "e01_1775",
   "page_count": 4,
   "order": 427,
   "p1": "1775",
   "pn": "1778",
   "abstract": [
    "In this paper, we introduce a word graph interface between speech and natural language processing systems within a flexible speech understanding framework based on stochastic concept modeling augmented with background \"filler\" models. Each concept represents a set of phrases ( written as a context free grammar (CFG)) with the same meaning, and is compiled into a stochastic recursive transition network (SRTN). The arcs (or rules) are tagged with probabilities after training. The filler models are used for phrases that are not covered by the concept networks. The structure in concept+filler sequences is captured by n-grams. The interface is implemented within the context of CU Communicator spoken dialog system. We investigate the effect of several different filler models and interpolation of complementary language models on the system performance. We report notable performance improvements compared to the baseline system. The gain in performance along with the efficiency and flexibility of the method motivates future work on the implementation of a tighter interface.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-419"
  },
  "knight01_eurospeech": {
   "authors": [
    [
     "Sylvia",
     "Knight"
    ],
    [
     "Genevieve",
     "Gorrell"
    ],
    [
     "Manny",
     "Rayner"
    ],
    [
     "David",
     "Milward"
    ],
    [
     "Rob",
     "Koeling"
    ],
    [
     "Ian",
     "Lewin"
    ]
   ],
   "title": "Comparing grammar-based and robust approaches to speech understanding: a case study",
   "original": "e01_1779",
   "page_count": 4,
   "order": 428,
   "p1": "1779",
   "pn": "1782",
   "abstract": [
    "Previous work has demonstrated the success of statistical language models when enough training data is available, but despite that, grammar-based systems are proving the preferred choice in successful commercial systems such as HeyAnita, BeVocal and Tellme, largely due to the difficulty involved in obtaining a corpus of training data. Here we trained an SLM on data obtained using a grammar-based system and compared the performance of the two systems with regards to recognition. We also parsed the output of the SLM using a robust parser and compared the accuracy of the semantic output of the systems. The SLM/robust parser showed considerable improvement on unconstrained input, and similar precision/recall (per slot value) on utterances provided by trained users.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-420"
  },
  "abdou01_eurospeech": {
   "authors": [
    [
     "Sherif",
     "Abdou"
    ],
    [
     "Michael",
     "Scordilis"
    ]
   ],
   "title": "Integrating multiple knowledge sources for improved speech understanding",
   "original": "e01_1783",
   "page_count": 4,
   "order": 429,
   "p1": "1783",
   "pn": "1786",
   "abstract": [
    "In spoken dialog systems it is often the case that the sentence produced by the decoder with the highest recognition probability may not be the best choice for extracting the intended concepts. Lower ranking hypotheses may present better alternatives. In this paper, we show how to integrate multiple knowledge sources for the decision of selecting one of these hypotheses. A scoring schema combining information from the recognizer output, the parser, an utterance type classifier and dialog context is used. The scaling weights of the combined scores are determined automatically by an optimization procedure. Finally, we show the results of testing this approach and its performance compared to the approach of selecting the best recognition hypothesis.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-421"
  },
  "litichever01_eurospeech": {
   "authors": [
    [
     "Zeev",
     "Litichever"
    ],
    [
     "Dan",
     "Chazan"
    ]
   ],
   "title": "Classification of transition sounds with application to automatic speech recognition",
   "original": "e01_1789",
   "page_count": 4,
   "order": 430,
   "p1": "1789",
   "pn": "1792",
   "abstract": [
    "This paper addresses the problem of classification of speech transition sounds. A number of non parametric classifiers are compared, and it is shown that some non-parametric classifiers have considerable advantages over traditional hidden Markov models. Among the non parametric classifiers, support vector machines were found the most suitable and the easiest to tune. Some of the reasons for the superiority of non parametric classifiers will be discussed. The algorithm was tested on the voiced stop consonant phones extracted from the TIMIT corpus and resulted in very low error rates.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-422"
  },
  "faizakov01_eurospeech": {
   "authors": [
    [
     "Avi",
     "Faizakov"
    ],
    [
     "Arnon",
     "Cohen"
    ],
    [
     "Tzur",
     "Vaich"
    ]
   ],
   "title": "Gaussian subtraction (GS) algorithms for word spotting in continuous speech",
   "original": "e01_1793",
   "page_count": 4,
   "order": 431,
   "p1": "1793",
   "pn": "1796",
   "abstract": [
    "In this paper, a novel approach for the design of cohort models for word spotting in continuous speech is presented. This new approach is based on modifying the probability density function of a conventional filler so that regions in the feature space that are related to the keyword will be reduced or removed. By modifying these regions, the filler and keyword models become more orthogonal in the sense that they represent different areas in the feature space, making the filler appropriate to be used as a cohort model. The algorithms, named Gaussian Subtraction (GS) and Gaussian Removal (GR), may be considered discriminative training algorithms.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-423"
  },
  "shire01_eurospeech": {
   "authors": [
    [
     "Michael L.",
     "Shire"
    ]
   ],
   "title": "Relating frame accuracy with word error in hybrid ANN-HMM ASR",
   "original": "e01_1797",
   "page_count": 4,
   "order": 432,
   "p1": "1797",
   "pn": "1800",
   "abstract": [
    "Frame accuracy is a common and natural summary statistic to use in neural-network-based ASR. It is often used as an indication of the performance of the neural network probability estimator and in the stopping criterion during its training. Though considered an important factor for word recognition, the frame accuracy presents an incomplete and sometimes deficient indicator of performance for the overall task of word recognition, as with many such summary statistics. Many in the ASR community have seen instances where an improvement in the acoustic posterior probability estimation yielded a disappointing effect on word recognition. We conducted experiments in an effort to illustrate some of the variability in word-recognition performance associated with frame accuracy. Our experiments attempt to shed light on some of the factors that might give rise to instances where frame accuracy and word error correlate. Some of the results are confirmation of intuitive or commonly known trends.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-424"
  },
  "zhang01c_eurospeech": {
   "authors": [
    [
     "Guoliang",
     "Zhang"
    ],
    [
     "Fang",
     "Zheng"
    ],
    [
     "Wenhu",
     "Wu"
    ]
   ],
   "title": "A two-layer lexical tree based beam search in continuous Chinese speech recognition",
   "original": "e01_1801",
   "page_count": 4,
   "order": 433,
   "p1": "1801",
   "pn": "1804",
   "abstract": [
    "In this paper, an approach to continuous speech recognition based on a two-layer lexical tree is proposed. The search network is maintained by the two-layer lexical tree, in which the first layer reflects the word net and the phone net while the second layer the dynamic programming (DP). Because the acoustic information is tied in the second layer, the memory cost is so small that it has the ability to process some complicated applications, such as the use of cross-word context-dependent (CD) triphone models, the Chinese fuzzy syllable mapping and the pronunciation modeling. The search algorithm based on the two-layer lexical tree is also proposed, which is derived from the token-passing algorithm. Finally, an implementation of the two-layer lexical tree using the cross-word context-dependent triphone models is presented, and the experimental results show that the highly efficient decoding can be achieved without too much memory cost.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-425"
  },
  "itoh01_eurospeech": {
   "authors": [
    [
     "Yoshiaki",
     "Itoh"
    ],
    [
     "Kazuyo",
     "Tanaka"
    ]
   ],
   "title": "Automatic labeling and digesting for lecture speech utilizing repeated speech by shift CDP",
   "original": "e01_1805",
   "page_count": 4,
   "order": 434,
   "p1": "1805",
   "pn": "1808",
   "abstract": [
    "This paper proposes an automatic labeling and digesting method for lecture speech. The method utilizes same sections, such as same words or same phrases that are thought to be important and are repeated in the speech. To extract the same sections, we have proposed a new efficient algorithm, called Shift Continuous DP, because it is an extension of Continuous DP and realizes fast matching between arbitrary sections in two speech data sets frame-synchronously. Shift CDP is extended to extract same sections in single long speech data in this paper. This paper describes ways to apply the algorithm to labeling and digesting for a lecture speech. We conduct some preliminary experiments to show the method can extract same sections and a sequence of extracted sections can be regarded as a digest of the speech.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-426"
  },
  "hori01b_eurospeech": {
   "authors": [
    [
     "Takaaki",
     "Hori"
    ],
    [
     "Yoshiaki",
     "Noda"
    ],
    [
     "Shoichi",
     "Matsunaga"
    ]
   ],
   "title": "Improved phoneme-history-dependent search for large-vocabulary continuous-speech recognition",
   "original": "e01_1809",
   "page_count": 4,
   "order": 435,
   "p1": "1809",
   "pn": "1813",
   "abstract": [
    "This paper describes an improved phoneme-history-dependent (PHD) search algorithm. This method is an optimum algorithm under the assumption that the starting time of a word depends on only a few preceding phonemes (phoneme history). The computational cost and number of recognition errors made by a multi-pass-based recognizer can be reduced if the PHD search of the first decoding pass uses re-selection of the preceding word and the optimum length of phoneme histories. These improvements increase the speed of the first decoding pass and help that the word lattice has the correct word sequence. Consequently search errors can be reduced in the second decoding pass. In 65k-word domain-independent Japanese read-speech dictation task and 1000-word spontaneous-speech airplane-reservation task, the improved PHD search was 1.2-2.0 times faster than a traditional word-dependent search under the condition of equal word accuracy.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-427"
  },
  "psutka01_eurospeech": {
   "authors": [
    [
     "Josef",
     "Psutka"
    ],
    [
     "Ludek",
     "Müller"
    ],
    [
     "Josef V.",
     "Psutka"
    ]
   ],
   "title": "Comparison of MFCC and PLP parameterizations in the speaker independent continuous speech recognition task",
   "original": "e01_1813",
   "page_count": 4,
   "order": 436,
   "p1": "1813",
   "pn": "1816",
   "abstract": [
    "The authors of this paper wish to contribute to the discussion about an optimal parameterization of speech signals in speech recognition systems. Our experiments deal with a telephone-based speaker independent continuous speech recognition task in which the MFCC and PLP parameterizations were tested and compared. The benefit of an adjustment of the filters used in the MFCC and PLP parameterizations to the critical bandwidth of hearing was explored and the impact of the number of filters and enumerated parameters to the recognition accuracy was tested. The results of these experiments showed that the MFCC parameterization is less sensitive to satisfying the theory of the critical bandwidth of hearing than the PLP parameterization. Experiments also proved that 5 PLP-cepstral (including derived 5 delta + 5 delta-delta) coefficients do not afford the best results as could be deduced from recent work. However, after optimal setting both parameterization techniques provided almost comparable results\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-428"
  },
  "pusateri01_eurospeech": {
   "authors": [
    [
     "Ernest",
     "Pusateri"
    ],
    [
     "J.M. Van",
     "Thong"
    ]
   ],
   "title": "N-best list generation using word and phoneme recognition fusion",
   "original": "e01_1817",
   "page_count": 4,
   "order": 437,
   "p1": "1817",
   "pn": "1820",
   "abstract": [
    "This paper describes an approach for combining phoneme and word recognition to produce an accurate N-best list of hypotheses. We run two decoding threads in parallel. The first performs phoneme recognition, while the other performs word recognition on the same recorded utterance. The output of the word recognition thread is returned as the most likely hypothesis, and the result of the phoneme recognition thread is used to lookup a list of words for the rest of the N-best list. The algorithm is simple to implement and efficient. In our evaluation, we found that this approach has similar performance to the classical lattice-based N-best search methods on isolated word recognition. This method has the potential to improve existing ASR systems or can be used in interactive multi-modal applications.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-429"
  },
  "ahn01_eurospeech": {
   "authors": [
    [
     "Dong-Hoon",
     "Ahn"
    ],
    [
     "Minhwa",
     "Chung"
    ]
   ],
   "title": "A one pass semi-dynamic network decoder based on language model network",
   "original": "e01_1821",
   "page_count": 4,
   "order": 438,
   "p1": "1821",
   "pn": "1824",
   "abstract": [
    "Decoding in a precompiled static network, compared with one in a dynamically managed network, is easier to implement and faster enough to yield a near real time response. However, when the recognition system handles a complex task, it has a problem of intensive memory usage. To overcome this weakness, we present a new decoding strategy that combines the advantages of static and dynamic network architectures. In this strategy, we first define a language model (LM) network that can represent an arbitrary back-off N-gram in a finite state network (FSN). The LM network enables constructing a precompiled static network and partitioning the whole network into subnetworks using LM histories. Then the recognition network can be dynamically created and destroyed on the subnetwork¡6s basis. To make dynamic management of networks as simple as possible, we also devise a data structure for network representation that self-structures its nodes and arcs. The final decoder maintains subnetworks as needed, but does not need to maintain nodes and arcs. Experimental results show that this semi-dynamic management of networks dramatically reduces memory usage at the cost of less than 10% increase of recognition time.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-430"
  },
  "macherey01_eurospeech": {
   "authors": [
    [
     "W.",
     "Macherey"
    ],
    [
     "D.",
     "Keysers"
    ],
    [
     "J.",
     "Dahmen"
    ],
    [
     "Hermann",
     "Ney"
    ]
   ],
   "title": "Improving automatic speech recognition using tangent distance",
   "original": "e01_1825",
   "page_count": 4,
   "order": 439,
   "p1": "1825",
   "pn": "1828",
   "abstract": [
    "In this paper we present a new approach to variance modelling in automatic speech recognition (ASR) that is based on tangent distance (TD). Using TD, classifiers can be made invariant w.r.t. small transformations of the data. Such transformations generate a manifold in a high dimensional feature space when applied to an observation vector. While conventional classifiers determine the distance between an observation and a prototype vector, TD approximates the minimum distance between their manifolds, resulting in classification that is invariant w.r.t. the underlying transformation. Recently, this approach was successfully applied in image object recognition. In this paper we describe how TD can be incorporated into ASR systems based on Gaussian mixture densities (GMD). The proposed method is embedded into a probabilistic framework. Experiments on the SieTill corpus for telephone line recorded digit strings show a significant improvement in comparison with a conventional GMD approach using comparable amounts of model parameters.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-431"
  },
  "chotimongkol01_eurospeech": {
   "authors": [
    [
     "Ananlada",
     "Chotimongkol"
    ],
    [
     "Alexander I.",
     "Rudnicky"
    ]
   ],
   "title": "N-best speech hypotheses reordering using linear regression",
   "original": "e01_1829",
   "page_count": 4,
   "order": 440,
   "p1": "1829",
   "pn": "1832",
   "abstract": [
    "We propose a hypothesis reordering technique to improve speech recognition accuracy in a dialog system. For such systems, additional information external to the decoding process itself is available, in particular features derived from the parse and the dialog. Such features can be combined with recognizer features by means of a linear regression model to predict the most likely entry in the hypothesis list. We introduce the use of concept error rate as an alternative accuracy measurement and compare it withy the use of word error rate. The proposed model performs better than human subjects performing the same hypothesis reordering task.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-432"
  },
  "deligne01_eurospeech": {
   "authors": [
    [
     "Sabine",
     "Deligne"
    ],
    [
     "Ellen",
     "Eide"
    ],
    [
     "Ramesh",
     "Gopinath"
    ],
    [
     "Dimitri",
     "Kanevsky"
    ],
    [
     "Benoit",
     "Maison"
    ],
    [
     "Peder",
     "Olsen"
    ],
    [
     "Harry",
     "Printz"
    ],
    [
     "Jan",
     "Sedivy"
    ]
   ],
   "title": "Low-resource hidden Markov model speech recognition",
   "original": "e01_1833",
   "page_count": 4,
   "order": 441,
   "p1": "1833",
   "pn": "1836",
   "abstract": [
    "We describe techniques for enhancing the accuracy, efficiency and features of a low-resource, medium-vocabulary, grammar-based speech recognition system, which uses hidden Markov models. Among the issues and techniques we explore are reducing computation via silence detection, applying the Bayesian information criterion (BIC) to build smaller and better acoustic models, minimizing finite state grammars, using hybrid maximum likelihood and discriminative models, and automatically generating baseforms from single new-word utterances. We report WER figures where appropriate.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-433"
  },
  "hirsch01_eurospeech": {
   "authors": [
    [
     "H. G.",
     "Hirsch"
    ],
    [
     "K.",
     "Hellwig"
    ],
    [
     "S.",
     "Dobler"
    ]
   ],
   "title": "Speech recognition at multiple sampling rates",
   "original": "e01_1837",
   "page_count": 4,
   "order": 442,
   "p1": "1837",
   "pn": "1840",
   "abstract": [
    "A feature extraction scheme is presented that analyzes speech signals sampled at different sampling rates. This will be needed in the future because of terminals in the telecom network that will transmit speech information also in the frequency region above 4 kHz. A cepstral analysis scheme is applied in the frequency range up to 4 kHz to create a common set of acoustic parameters for all sampling rates. Additional parameters are determined describing the subband energy in the frequency region above 4 kHz. As the major advantage of this feature extraction no individual recognizer has to be trained for each sampling frequency. It is shown with a recognition experiment that terminals and recognition systems can be combined without a remarkable loss in recognition performance with the terminal operating at a different sampling frequency than the recognizer has been trained on.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-434"
  },
  "shimodaira01_eurospeech": {
   "authors": [
    [
     "Hiroshi",
     "Shimodaira"
    ],
    [
     "Ken-ichi",
     "Noma"
    ],
    [
     "Mitsuru",
     "Nakai"
    ],
    [
     "Shigeki",
     "Sagayama"
    ]
   ],
   "title": "Support vector machine with dynamic time-alignment kernel for speech recognition",
   "original": "e01_1841",
   "page_count": 4,
   "order": 443,
   "p1": "1841",
   "pn": "1844",
   "abstract": [
    "A new class of Support Vector Machine (SVM) which is applicable to sequential-pattern recognition is developed by incorporating an idea of non-linear time alignment into the kernel. Since time-alignment operation of sequential pattern is embedded in the kernel evaluation, same algorithms with the original SVM for training and classification can be employed without modifications. Furthermore, frame-wise evaluation of kernel in the proposed SVM (DTAK-SVM) enables frame-synchronous recognition of sequential pattern, which is suitable for continuous speech recognition. Preliminary experiments of speaker-dependent 6 voiced-consonants recognition demonstrated excellent recognition performance of more than 98% in correct classification rate, whereas 93% by hidden Markov models (HMMs).\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-435"
  },
  "srinivasamurthy01_eurospeech": {
   "authors": [
    [
     "Naveen",
     "Srinivasamurthy"
    ],
    [
     "Antonio",
     "Ortega"
    ],
    [
     "Shrikanth",
     "Narayanan"
    ]
   ],
   "title": "Efficient scalable speech compression for scalable speech recognition",
   "original": "e01_1845",
   "page_count": 4,
   "order": 444,
   "p1": "1845",
   "pn": "1848",
   "abstract": [
    "We propose a scalable recognition system for reducing recognition complexity. Scalable recognition can be combined with scalable compression in a distributed speech recognition (DSR) application to reduce both the computational load and the bandwidth requirement at the server. A low complexity pre-processor is used to eliminate the unlikely classes so that the complex recognizer can use the reduced subset of classes to recognize the unknown utterance. It is shown that by using our system it is fairly straightforward to trade-off reductions in complexity for performance degradation. Results of preliminary experiments using the TI-46 word digit database show that the proposed scalable approach can provide a 40% speed up, while operating under 1.05 kbps, compared to the baseline recognition using uncompressed speech.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-436"
  },
  "stadermann01_eurospeech": {
   "authors": [
    [
     "J.",
     "Stadermann"
    ],
    [
     "V.",
     "Stahl"
    ],
    [
     "G.",
     "Rose"
    ]
   ],
   "title": "Voice activity detection in noisy environments",
   "original": "e01_1851",
   "page_count": 4,
   "order": 445,
   "p1": "1851",
   "pn": "1854",
   "abstract": [
    "The subject of this paper is robust voice activity detection (VAD) in noisy environments, especially in car environments. We present a comparison between several frame based VAD feature extraction algorithms in combination with different classifiers. Experiments are carried out under equal test conditions using clean speech, clean speech with added car noise and speech recorded in car environments. The lowest error rate is achieved applying features based on a likelihood ratio test which assumes normal distribution of speech and noise and a perceptron classifier. We propose modifications of this algorithm which reduce the frame error rate by approximately 30% relative in our experiments compared to the original algorithm.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-437"
  },
  "sheikhzadeh01_eurospeech": {
   "authors": [
    [
     "Hamid",
     "Sheikhzadeh"
    ],
    [
     "Hamid Reza",
     "Abutalebi"
    ]
   ],
   "title": "An improved wavelet-based speech enhancement system",
   "original": "e01_1855",
   "page_count": 4,
   "order": 446,
   "p1": "1855",
   "pn": "1858",
   "abstract": [
    "The problem of speech enhancement using wavelet thresholding algorithm is considered. Major problems in applying the basic algorithm are discussed and modifications are proposed to improve the method. First, we propose the use of different thresholds for different wavelet bands. Next, by employing a pause detection algorithm, noise profile is estimated and the thresholds are adapted. This enables the modified enhancement system to handle colored and non-stationary noises. Finally, a wavelet-based voiced/unvoiced classification is proposed and implemented that can further improve the performance of the enhancement system. To evaluate the system performance, we have used real-life noise types such as multi-talker babble and low-pass noises. Subjective and objective evaluations show that the proposed system improves the performance the wavelet thresholding algorithm.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-438"
  },
  "ramabadran01_eurospeech": {
   "authors": [
    [
     "Tenkasi",
     "Ramabadran"
    ],
    [
     "Jeff",
     "Meunier"
    ],
    [
     "Mark",
     "Jasiuk"
    ],
    [
     "Bill",
     "Kushner"
    ]
   ],
   "title": "Enhancing distributed speech recognition with back- end speech reconstruction",
   "original": "e01_1859",
   "page_count": 4,
   "order": 447,
   "p1": "1859",
   "pn": "1862",
   "abstract": [
    "In this paper, we present a method to enhance the usefulness of a Distributed Speech Recognition (DSR) system by providing it the capability to reconstruct speech at the back-end. Speech reconstruction is achieved using the standard DSR parameters, viz., Mel-Frequency Cepstral Coefficients (MFCC) and log-energy, and some additional parameters, viz., voicing class, pitch period, and (optionally) higher-resolution energy information. From the MFCC parameters and energy information, the spectral magnitudes at the harmonics of the pitch frequency are estimated. Based on the class information, the harmonic phases are appropriately modeled. The harmonic magnitudes and phases are used to reconstruct speech according to the well-known sinusoidal model for speech synthesis. Transmission of the additional parameters for speech reconstruction increases the DSR bit rate by less than 20%. Evaluation by Mean-Opinion-Score (MOS) test and Diagnostic Rhyme Test (DRT) show that speech reconstructed as above is of reasonable quality and quite intelligible.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-439"
  },
  "tihelka01_eurospeech": {
   "authors": [
    [
     "Jiri",
     "Tihelka"
    ],
    [
     "Pavel",
     "Sovka"
    ]
   ],
   "title": "Implementation effective one-channel noise reduction system",
   "original": "e01_1863",
   "page_count": 4,
   "order": 448,
   "p1": "1863",
   "pn": "1866",
   "abstract": [
    "This contribution addresses the problem of additive noise reduction using one-channel noise suppression system. A new implementation effective method is suggested and evaluated. The method consists of two independent parts. The noise estimation part is based on the noise matched filter producing an estimation of background noise without the need of a voice activity detector. The noise reduction part uses short-time spectral attenuation technique. The main idea reducing computational costs lies in the use of reduced number of frequency bands for computation of attenuation factors. Except of reduced number of operations this approach decreases fluctuations of estimated spectral gains, and therefore the speech distortion is low. Thus the suggested system eliminates the need of any enhanced speech postprocessing. A new effective approach is used for the inverse frequency transformation. In spite of the simplicity of the suggested method its performance is comparable with other existing one-channel noise reduction methods.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-440"
  },
  "kim01e_eurospeech": {
   "authors": [
    [
     "Hyoung-Gook",
     "Kim"
    ],
    [
     "Klaus",
     "Obermayer"
    ],
    [
     "Mathias",
     "Bode"
    ],
    [
     "Dietmar",
     "Ruwisch"
    ]
   ],
   "title": "Efficient speech enhancement by diffusive gain factors (DGF)",
   "original": "e01_1867",
   "page_count": 4,
   "order": 449,
   "p1": "1867",
   "pn": "1870",
   "abstract": [
    "In this paper we propose a very simple but highly effective algorithm for single channel noise reduction of speech signals. One of the main objectives is to find a balanced tradeoff between noise reduction and speech distortion in the processed signal. This is accomplished by a system based on spectral minimum detection and diffusive gain factors. Our approach to speech enhancement is capable of distinguishing between speech and noise interference in the microphone signal, even when they are located in the same frequency band.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-441"
  },
  "mahe01_eurospeech": {
   "authors": [
    [
     "Gaël",
     "Mahé"
    ],
    [
     "André",
     "Gilloire"
    ]
   ],
   "title": "Correction of the voice timbre distortions on telephone network",
   "original": "e01_1871",
   "page_count": 4,
   "order": 450,
   "p1": "1871",
   "pn": "1874",
   "abstract": [
    "In a telephone link, the voice timbre is affected by the loss of low frequencies components and distortions due to the analog lines. We analyze first how the quantization noise limits the restoration of the timbre. Within this limitation, a method of equalization, inspired by the cepstral subtraction, is then proposed to correct the timbre and is validated by experimental results.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-442"
  },
  "lee01d_eurospeech": {
   "authors": [
    [
     "Yunjung",
     "Lee"
    ],
    [
     "Joohun",
     "Lee"
    ],
    [
     "Ki Yong",
     "Lee"
    ],
    [
     "Katsuhiko",
     "Shirai"
    ]
   ],
   "title": "Speech enhancement based on IMM with NPHMM",
   "original": "e01_1875",
   "page_count": 4,
   "order": 451,
   "p1": "1875",
   "pn": "1878",
   "abstract": [
    "The nonlinear speech enhancement method with interactive parallel-extended Kalman filter is applied to speech contaminated by additive white noise. To represent the nonlinear and nonstationary nature of speech, we assume that speech is the output of a nonlinear prediction HMM (NPHMM) combining both neural network and HMM. The NPHMM is a nonlinear autoregressive process whose time-varying parameters are controlled by a hidden Markov chain. The simulation results shows that the proposed method offers better performance gains relative to the previous results [6] with slightly increased complexity.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-443"
  },
  "fujimoto01_eurospeech": {
   "authors": [
    [
     "M.",
     "Fujimoto"
    ],
    [
     "Y.",
     "Ariki"
    ]
   ],
   "title": "Speech recognition under musical environments using kalman filter and iterative MLLR adaptation",
   "original": "e01_1879",
   "page_count": 4,
   "order": 452,
   "p1": "1879",
   "pn": "1882",
   "abstract": [
    "In this paper, we propose a speech recognition method under non-stationary musical environments using Kalman filtering speech signal estimation method and iterative unsupervised MLLR adaptation. Our proposing method estimates the speech signal under non-stationary noisy environments such as musical background by applying speech state transition model to Kalman filtering estimation. The speech state transition model represents the state transition of speech component in non-stationary noisy speech and is modeled by using Taylor expansion. In this model, the state transition of noise is estimated by using linear predictive estimation. Furthermore, to obtain higher recognition accuracy, we consider to adapt the acoustic models by using iterative unsupervised MLLR adaptation to speech spectra distorted by Kalman filtering residual noise. In order to evaluate the proposed method, we carried out large vocabulary continuous speech recognition experiments under 3 types of music. As a result, the proposed method obtained the significant improvement in word accuracy.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-444"
  },
  "vetter01_eurospeech": {
   "authors": [
    [
     "Rolf",
     "Vetter"
    ],
    [
     "Philippe",
     "Renevey"
    ],
    [
     "Jens",
     "Krauss"
    ]
   ],
   "title": "Dual channel speech enhancement using coherence function and MDL-based subspace approach in bark domain",
   "original": "e01_1883",
   "page_count": 4,
   "order": 453,
   "p1": "1883",
   "pn": "1886",
   "abstract": [
    "A novel algorithm for dual channel speech enhancement is presented. It combines the coherence function and a subspace approach in the Bark domain together with an optimal subspace selection through the minimum description length (MDL) criterion. The coherence function allows one to exploit the spatial diversity of the sound field. The processing in the Bark domain permits to take into account of masking properties of the human auditory system while the MDL-based subspace approach ensures statistical robustness. Performance evaluation in real sound fields has highlighted the ability of the algorithm to enhance noisy signals and improve intelligibility for various experimental conditions.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-445"
  },
  "renevey01b_eurospeech": {
   "authors": [
    [
     "Philippe",
     "Renevey"
    ],
    [
     "Andrzej",
     "Drygajlo"
    ]
   ],
   "title": "Entropy based voice activity detection in very noisy conditions",
   "original": "e01_1887",
   "page_count": 4,
   "order": 454,
   "p1": "1887",
   "pn": "1890",
   "abstract": [
    "This paper addresses the problem of robust voice activity detection (VAD) capable for working at very low signal-to-noise ratios (SNR<10dB). A new algorithm that we propose is based on entropy estimation measures of the time-frequency magnitude spectrum. The problem of the estimation of the distribution of noise in detected non-speech segments of analysed signal is also presented. It is shown that the new entropy based VAD significantly outperforms the commonly used energy-based algorithms in all (stationary, non-stationary, white and coloured) noise conditions at SNRs from 10 dB down to -10 dB and below. One of the main advantages of the method proposed in this paper is that it is not very sensitive to the changing level of noise.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-446"
  },
  "karneback01_eurospeech": {
   "authors": [
    [
     "Stefan",
     "Karnebäck"
    ]
   ],
   "title": "Discrimination between speech and music based on a low frequency modulation feature",
   "original": "e01_1891",
   "page_count": 4,
   "order": 455,
   "p1": "1891",
   "pn": "1894",
   "abstract": [
    "The possibility to discriminate between speech and music signals by using a feature based on low frequency modulation has been investigated. Three different low frequency modulation parameters have been extracted and tested concerning the ability of discrimination. The low frequency modulation amplitudes calculated over 20 critical bands and their standard deviations were found to be good features for this discrimination task even with VQ models. They were also found to be less sensitive to channel quality and model size than MFCC features.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-447"
  },
  "cheng01b_eurospeech": {
   "authors": [
    [
     "Yiou-Wen",
     "Cheng"
    ],
    [
     "Lin-Shan",
     "Lee"
    ]
   ],
   "title": "Credibility proof for speech content and speaker verification by fragile watermarking with consecutive frame-based processing",
   "original": "e01_1895",
   "page_count": 4,
   "order": 456,
   "p1": "1895",
   "pn": "1898",
   "abstract": [
    "With rapid growth in real-world speech-based transactions via communication networks, the need for a reliable mechanism to prove the credibility of speech content is highly desired. This paper presents a fragile watermarking technique for such purposes. The proposed approach is also useful for speaker verification. It breaks the speech signal into a series of non-overlapping frames and encodes the watermark into those frames consecutively. The watermark sequence for each frame is dependent on the statistical characteristics of the previous frame, therefore any signal discontinuity caused by malicious purposes will be detected and the speaker can be verified as well by the watermark. Experiments showed very encouraging results, including reasonable detection rate even under signal compression and filter attacks.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-448"
  },
  "potamitis01_eurospeech": {
   "authors": [
    [
     "I.",
     "Potamitis"
    ],
    [
     "Nikos",
     "Fakotakis"
    ],
    [
     "George",
     "Kokkinakis"
    ]
   ],
   "title": "Map estimation for on-line noise compensation of time trajectories of spectral coefficients",
   "original": "e01_1899",
   "page_count": 4,
   "order": 457,
   "p1": "1899",
   "pn": "1902",
   "abstract": [
    "This paper presents a novel data driven compensation technique that modifies on-line the incoming spectral representation of degraded speech in order to approximate the features of high quality speech used to train a classifier. We apply the Bayesian inference framework to the degraded spectral coefficients based on the modeling of clean speech linear-spectrum with appropriate non-Gaussian distributions that allow maximum a-posteriori (MAP) closed form solution. The MAP solution leads to spectral magnitude estimation adapted to the spectral characteristics and noise variance of each spectral band. We perform extensive evaluation of our algorithm using white and coloured Gaussian noise on the task of improving the quality of speech perception as well as Automatic Speech Recognition (ASR), and demonstrate its robustness at very low SNRs. The enhancement process comes at little to no extra computational overhead for ASR systems, thus achieving real time performance.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-449"
  },
  "attias01_eurospeech": {
   "authors": [
    [
     "Hagai",
     "Attias"
    ],
    [
     "Li",
     "Deng"
    ],
    [
     "Alex",
     "Acero"
    ],
    [
     "John C.",
     "Platt"
    ]
   ],
   "title": "A new method for speech denoising and robust speech recognition using probabilistic models for clean speech and for noise",
   "original": "e01_1903",
   "page_count": 4,
   "order": 458,
   "p1": "1903",
   "pn": "1906",
   "abstract": [
    "We present a new method for speech denoising and robust speech recognition. Using the framework of probabilistic models allows us to integrate detailed speech models and models of realistic non-stationary noise signals in a principled manner. The framework transforms the denoising problem into a problem of Bayes-optimal signal estimation, producing minimum mean square error estimators of desired features of clean speech from noisy data. We describe a fast and efficient implementation of an algorithm that computes these estimators. The effectiveness of this algorithm is demonstrated in robust speech recognition experiments, using the Wall Street Journal speech corpus and Microsoft Whisper large-vocabulary continuous speech recognizer. Results show significantly lower word error rates than those under noisy matched condition. In particular, when the denoising algorithm is applied to the noisy training data and subsequently the recognizer is retrained, very low error rates are obtained.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-450"
  },
  "kienappel01_eurospeech": {
   "authors": [
    [
     "Anne K.",
     "Kienappel"
    ],
    [
     "Reinhard",
     "Kneser"
    ]
   ],
   "title": "Designing very compact decision trees for grapheme-to-phoneme transcription",
   "original": "e01_1911",
   "page_count": 4,
   "order": 459,
   "p1": "1911",
   "pn": "1914",
   "abstract": [
    "Decision trees are a popular technique for automatic generation of a phonetic transcription for a given word spelling. We investigate different methods of decision tree design to obtain more compact trees and at the same time better grapheme-to-phoneme transcription quality. We evaluate different approaches to decision tree question selection and pruning using one English and two German grapheme-to-phoneme transcription tasks. In particular, we present a method of automatic generation of decision tree questions from the training data that significantly improves decision tree design.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-451"
  },
  "mana01_eurospeech": {
   "authors": [
    [
     "Franco",
     "Mana"
    ],
    [
     "Paolo",
     "Massimino"
    ],
    [
     "Alberto",
     "Pacchiotti"
    ]
   ],
   "title": "Using machine learning techniques for grapheme to phoneme transcription",
   "original": "e01_1915",
   "page_count": 4,
   "order": 460,
   "p1": "1915",
   "pn": "1918",
   "abstract": [
    "The renewed interest in grapheme to phoneme conversion (G2P), due to the need of developing multilingual speech synthesizers and recognizers, suggests new approaches more efficient than the traditional rule&exception ones. A number of studies have been performed to investigate the possible use of machine learning techniques to extract phonetic knowledge in a automatic way starting from a lexicon. In this paper, we present the results of our experiments in this research field. Starting from the state of art, our contribution is in the development of a language-independent learning scheme for G2P based on Classification and Regression Trees (CART). To validate our approach, we realized G2P converters for the following languages: British English, American English, French and Brazilian Portuguese.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-452"
  },
  "llitjos01_eurospeech": {
   "authors": [
    [
     "Ariadna Font",
     "Llitjos"
    ],
    [
     "Alan W.",
     "Black"
    ]
   ],
   "title": "Knowledge of language origin improves pronunciation accuracy of proper names",
   "original": "e01_1919",
   "page_count": 4,
   "order": 461,
   "p1": "1919",
   "pn": "1922",
   "abstract": [
    "As it is impossible to have a lexicon with complete coverage, and a high proportion of unknown words are proper names, this paper addresses the issue of automatically finding pronunciations of unseen proper names in US English. Proper names, especially in the US, may come from a large range of ethnic backgrounds. We present a model and results showing that including ethnic origin of words in a statistical model can improve pronunciation results. We used a lexicon of 56,000 proper names from CMUDICT, and gathered data (text and proper names) from 26 languages to built statistical models that provide an estimate of word origin. Tests against held out data showed a 7.6% absolute improvement from a baseline of 54.8% when language based features were added to our CART-based model. Our user studies show a 17% preference for the model with language features compared to the baseline.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-453"
  },
  "boulademareuil01_eurospeech": {
   "authors": [
    [
     "Philippe",
     "Boula de Mareüil"
    ],
    [
     "Franck",
     "Floricic"
    ]
   ],
   "title": "On the pronunciation of acronyms in French and in Italian",
   "original": "e01_1923",
   "page_count": 4,
   "order": 462,
   "p1": "1923",
   "pn": "1926",
   "abstract": [
    "Acronyms are used more and more nowadays: this article describes their pronunciation in French and in Italian. Rules are proposed, so that a text-to-speech converter may know whether it must spell or read acronyms, and which way. Our analysis, which gave rise to a computational realisation, leans on the number of letters which constitute acronyms, on the vowel/consonant opposition, as well as on the distribution between continuous and momentary consonants.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-454"
  },
  "shin01_eurospeech": {
   "authors": [
    [
     "Vladimir I.",
     "Shin"
    ],
    [
     "Doh-Suk",
     "Kim"
    ],
    [
     "Moo Young",
     "Kim"
    ],
    [
     "Jeongsu",
     "Kim"
    ]
   ],
   "title": "Enhancement of noisy speech by using improved global soft decision",
   "original": "e01_1929",
   "page_count": 4,
   "order": 463,
   "p1": "1929",
   "pn": "1932",
   "abstract": [
    "We propose a novel speech enhancement algorithm, termed improved global soft decision (IGSD). IGSD is a unified framework for global soft decision on speech absence/presence, noise spectrum estimation, spectral gain modification based on Ephraim-Malah noise suppression. In IGSD, speech absence probability (SAP) is the most important factor, and we propose an efficient and novel SAP estimation in which the SAP is derived based on the general hypothesis for speech absence/presence. In IGSD, the global SAP based on the global hypothesis for speech absence/presence is used to prevent from the problem caused by insufficient amount of data, but more general hypothesis is utilized in the derivation of global SAP estimation. The performance of IGSD is evaluated both subjectively and objectively, and the quality of speech is improved significantly, compared with conventional GSD speech enhancement algorithm.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-455"
  },
  "cohen01_eurospeech": {
   "authors": [
    [
     "Israel",
     "Cohen"
    ]
   ],
   "title": "Enhancement of speech using bark-scaled wavelet packet decomposition",
   "original": "e01_1933",
   "page_count": 4,
   "order": 464,
   "p1": "1933",
   "pn": "1936",
   "abstract": [
    "In this paper, we propose a speech enhancement system, which integrates a bark-scaled wavelet packet decomposition (BS-WPD), a soft-decision gain modification and a \"magnitude\" decision-directed estimation technique. The BS-WPD provides an overcomplete auditory representation, having a higher frequency resolution than the critical band decomposition. Speech is estimated by Wiener filtering in the wavelet packet domain, modified by the signal presence probability. We introduce a \"magnitude\" decision-directed estimator for the variance of speech, which is closely related to the decision-directed estimator of Ephraim and Malah. This estimator achieves, in the established process, a better tradeoff between noise reduction and signal distortion. The proposed enhancement algorithm is tested with various noise types, and compared to a conventional log-spectral amplitude estimator. We show that noise can be further suppressed, while preserving its natural structure and the intelligibility and quality of the speech components.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-456"
  },
  "bahoura01_eurospeech": {
   "authors": [
    [
     "Mohammed",
     "Bahoura"
    ],
    [
     "Jean",
     "Rouat"
    ]
   ],
   "title": "A new approach for wavelet speech enhancement",
   "original": "e01_1937",
   "page_count": 4,
   "order": 465,
   "p1": "1937",
   "pn": "1940",
   "abstract": [
    "We propose a new approach to improve the performance of speech enhancement techniques based on wavelet thresholding. First, space-adaptation of the threshold is obtained by extending the principle of the level-dependent threshold to the Wavelet Packet Transform (WPT). Next, the time-adaptation is introduced using the Teager Energy Operator (TEO) of the wavelets coefficients. Finally, the time-space adapted threshold is proposed. Comparisons with the Ephraim and Malah Filter are reported.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-457"
  },
  "yoon01_eurospeech": {
   "authors": [
    [
     "Sukhyun",
     "Yoon"
    ],
    [
     "Chang D.",
     "Yoo"
    ]
   ],
   "title": "Speech/noise-dominant decision for speech enhancement",
   "original": "e01_1941",
   "page_count": 4,
   "order": 466,
   "p1": "1941",
   "pn": "1944",
   "abstract": [
    "A novel method to reduce additive non-stationary noise is proposed. The proposed method requires neither the statistical assumption about noise nor the estimate of the noise statistics from any pause regions. The enhancement is performed on a band-by-band basis for each time frame. Based on both the decision on whether a particular band in a frame is speech or noise dominant and the masking property of the human auditory system, an appropriate amount of noise is reduced using modified spectral subtraction. The proposed method was tested on various noisy conditions - car noise, F16 noise, white Gaussian noise, pink noise, tank noise and babble noise. On the basis of comparing segmental SNR with spectral subtraction proposed by Boll with pause detection for estimating noise, and visually inspecting the enhanced spectrograms and listening to the enhanced speech, the proposed method was found to effectively reduce various noise while minimizing distortion to speech.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-458"
  },
  "wang01c_eurospeech": {
   "authors": [
    [
     "Fan",
     "Wang"
    ],
    [
     "Fang",
     "Zheng"
    ],
    [
     "Wenhu",
     "Wu"
    ]
   ],
   "title": "An MCE based classification tree using hierarchical feature-weighting in speech recognition",
   "original": "e01_1947",
   "page_count": 4,
   "order": 467,
   "p1": "1947",
   "pn": "1950",
   "abstract": [
    "In this paper a hierarchical classification framework using the feature-weighting tree for the objective of applying diverse weighting to acoustic features is proposed for speech recognition. The hierarchical feature-weighting tree with a flexible structure complexity can be constructed optimally with the optimal splitting for the recognition confusion graph. Based on the minimum classification error principle, the subset-dependent training and the multi-level recognition method are proposed, where the feature weighting can be automatically trained without normalization in recognition. Both the mathematical analysis and the experimental results show that such a supervised hierarchical classification tree based on the feature weighting is efficient to reduce the speech recognition error.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-459"
  },
  "zhou01c_eurospeech": {
   "authors": [
    [
     "Jianlai",
     "Zhou"
    ],
    [
     "Eric",
     "Chang"
    ],
    [
     "Chao",
     "Huang"
    ]
   ],
   "title": "Selective MCE training strategy in Mandarin speech recognition",
   "original": "e01_1951",
   "page_count": 4,
   "order": 468,
   "p1": "1951",
   "pn": "1954",
   "abstract": [
    "In this paper, selective strategy about MCE based discriminative training method,in particular for mandarin syllable loop recognition,is introduced. The basic idea is that since the decoding errors occur in parts of the models in whole decoded sentence, it is reasonable to adjust the parameters of the \"wrong models\". As a result, weighted MCE formulation is derived, which can provide more effective convergence property and about 10% error rate reduction for a large training set is achieved. On the other hand, from our experiments, we observed that although the whole performance of recognition system is improved, some original correct recognition results are misrecognized after discriminative training, divide and conquer strategy is proposed to solve it. Combining above two methods, we got more than 14.5% error reduction in syllable loop recognition experiments.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-460"
  },
  "wu01b_eurospeech": {
   "authors": [
    [
     "Chung-Hsien",
     "Wu"
    ],
    [
     "Gwo-Lang",
     "Yan"
    ]
   ],
   "title": "Discriminative disfluency modeling for spontaneous speech recognition",
   "original": "e01_1955",
   "page_count": 4,
   "order": 469,
   "p1": "1955",
   "pn": "1958",
   "abstract": [
    "Most automatic speech recognizers (ASRs) have concentrated on read speech, which is different from speech with the presence of disfluencies. These ASRs cannot handle the speech with a high rate of disfluencies such as filled pauses, repetition, repairs, false starts, and silence pauses in actual spontaneous speech or dialogues. In this paper, we focus on the modeling of the filled pauses \"uh\" and \"um\". The filled pauses contain the characteristics of nasal and lengthening, and the acoustic parameters for these characteristics are analyzed and adopted for disfluency modeling. A Gaussian mixture model (GMM), trained by a discriminative training algorithm that minimizes the recognition error, is proposed. A transition probability density function is defined from the GMM and used to weight the transition probability between the boundaries of fluency and disfluency models in the one-stage algorithm. Experimental result shows that the proposed method yields an improvement rate of 27.3% for disfluency compared to the baseline system.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-461"
  },
  "hung01_eurospeech": {
   "authors": [
    [
     "Jeih-weih",
     "Hung"
    ],
    [
     "Hsin-min",
     "Wang"
    ],
    [
     "Lin-shan",
     "Lee"
    ]
   ],
   "title": "Comparative analysis for data-driven temporal filters obtained via principal component analysis (PCA) and linear discriminant analysis (LDA) in speech recognition",
   "original": "e01_1959",
   "page_count": 4,
   "order": 470,
   "p1": "1959",
   "pn": "1962",
   "abstract": [
    "The Linear Discriminant Analysis (LDA) has been widely used to derive the data-driven temporal filtering of speech feature vectors. In this paper, we proposed that the Principal Component Analysis (PCA) can also be used in the optimization process just as LDA to obtain the temporal filters, and detailed comparative analysis between these two approaches are presented and discussed. It's found that the PCA-derived temporal filters significantly improve the recognition performance of the original MFCC features as LDA-derived filters do. Also, while PCA/LDA filters are combined with the conventional temporal filters, RASTA or CMS, the recognition performance will be further improved regardless the training and testing environments are matched or mismatched, compressed or noise corrupted.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-462"
  },
  "heikkinen01_eurospeech": {
   "authors": [
    [
     "Ari",
     "Heikkinen"
    ],
    [
     "Vesa T.",
     "Ruoppila"
    ],
    [
     "Samuli",
     "Pietilä"
    ]
   ],
   "title": "Coding method for successive pitch periods",
   "original": "e01_1965",
   "page_count": 4,
   "order": 471,
   "p1": "1965",
   "pn": "1968",
   "abstract": [
    "This paper presents a coding method for successive pitch periods of a speech signal. In the proposed method, a priori knowledge of the statistical properties of successive pitch periods is used by designing a shaped lattice structure that covers the most probable points in the pitch space. We also present briefly a pitch search algorithm for the shaped lattice. An example implementation of shaped lattice coding is given for a modified IS-641 speech coder. Based on the simulation results, the proposed scheme achieves capacity savings compared to the conventional methods\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-463"
  },
  "nurminen01_eurospeech": {
   "authors": [
    [
     "Jani",
     "Nurminen"
    ],
    [
     "Ari",
     "Heikkinen"
    ],
    [
     "Jukka",
     "Saarinen"
    ]
   ],
   "title": "Objective evaluation of methods for quantization of variable-dimension spectral vectors in WI speech coding",
   "original": "e01_1969",
   "page_count": 4,
   "order": 472,
   "p1": "1969",
   "pn": "1972",
   "abstract": [
    "In this paper, we present a comprehensive evaluation of five quantization techniques for variable-dimension spectral vectors in a waveform interpolation speech coder. Each technique included in the evaluation is based on dimension conversions. The conversions are performed using zero-pad and truncation, frequency bins, band-limited interpolation, discrete cosine transform, and polynomial approximation. In addition to assessing quantization accuracy, this study considers the complexity of the techniques. The evaluation indicates that the selection of the optimal quantization technique is a trade-off between coding accuracy, complexity, and memory requirements. According to our results, the technique based on discrete cosine transform appears to be a strong candidate for many applications.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-464"
  },
  "pobloth01_eurospeech": {
   "authors": [
    [
     "Harald",
     "Pobloth"
    ],
    [
     "W. Bastiaan",
     "Kleijn"
    ]
   ],
   "title": "Squared error as a measure of phase distortion",
   "original": "e01_1973",
   "page_count": 4,
   "order": 473,
   "p1": "1973",
   "pn": "1976",
   "abstract": [
    "In this article, we investigate how accurately the squared error captures perceptual errors introduced by Fourier phase spectrum changes. We measure the perceptual error using the Auditory Image Model by Patterson et al.. The squared error is found to represent the perceptual error well for low squared errors but it saturates. Thus, a further increase in squared error does on average not lead to any further increase in perceptual error. This suggests that encoding phase using squared-error trained codebooks only improves perceived quality when operating at high bit rates. To verify this, phase was encoded with codebooks of different sizes. As expected, increasing the codebook size has very little influence on the average perceptual error for low rates, which is confirmed by listening tests. Our results suggest that a direct phase codebook is an inefficient representation of the relevant information contained in phase.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-465"
  },
  "faundezzanuy01_eurospeech": {
   "authors": [
    [
     "Marcos",
     "Faundez-Zanuy"
    ]
   ],
   "title": "Non-linear predictive vector quantization of speech",
   "original": "e01_1977",
   "page_count": 4,
   "order": 474,
   "p1": "1977",
   "pn": "1980",
   "abstract": [
    "In this paper we propose a Non-Linear Predictive Vector quantizer (PVQ) for speech coding, based on Multi-Layer Perceptrons. We also propose a method to evaluate if a quantizer is well designed, and if it exploits the correlation between consecutive outputs. Although the results of the Non-linear PVQ do not improve the results of the nonlinear scalar predictor, we check that there is some room for the PVQ improvement.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-466"
  },
  "katugampala01_eurospeech": {
   "authors": [
    [
     "Nilantha",
     "Katugampala"
    ],
    [
     "Ahmet M.",
     "Kondoz"
    ]
   ],
   "title": "A variable rate hybrid coder based on a synchronized harmonic excitation",
   "original": "e01_1981",
   "page_count": 4,
   "order": 475,
   "p1": "1981",
   "pn": "1984",
   "abstract": [
    "A novel synchronization technique is proposed for hybrid coders employing harmonic and waveform coding. A new classification technique based on analysis by synthesis to distinguish between stationary and transitional segments is also proposed. Harmonic excitation is synchronized with the LPC residual by transmitting the location of the pitch pulse closest to the frame boundary and a phase value that represents the shape of the corresponding pitch pulse. A hybrid coder is designed to demonstrate the new techniques, which has three modes: scaled white noise excitation colored by LPC for unvoiced, ACELP for transitions, and harmonic excitation for stationary segments. Subjective listening tests show that the speech quality of the variable rate hybrid coder outperforms the quality of ITU G.723.1 coders, at maximum bit rates lower than those of G.723.1 coders.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-467"
  },
  "ho01_eurospeech": {
   "authors": [
    [
     "M. S.",
     "Ho"
    ],
    [
     "D. J.",
     "Molyneux"
    ],
    [
     "B. M. G.",
     "Cheetham"
    ]
   ],
   "title": "A hybrid sub-band sinusoidal coding scheme",
   "original": "e01_1985",
   "page_count": 4,
   "order": 476,
   "p1": "1985",
   "pn": "1988",
   "abstract": [
    "This paper describes a hybrid sub-band speech coding scheme based on sinusoidal coding and CELP. Purely voiced speech is encoded using sinusoidal coding techniques and phase information is selectively transmitted. For mixed and unvoiced speech, the lower band is processed by sinusoidal coding algorithms while the upper band is encoded using CELP. To accommodate the extra bandwidth required by the encoded CELP parameters, the phase information is disregarded. The proposed coder is enhanced by sub-band discrete all-pole modeling and a voicing detection technique based on an analysis-by-synthesis approach. An efficient adaptive spectral shaping technique based on bandwidth widening in the LSP domain is employed. The proposed technique is capable of producing high quality speech at 4.1 kbit/s.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-468"
  },
  "lukasiak01_eurospeech": {
   "authors": [
    [
     "J.",
     "Lukasiak"
    ],
    [
     "I. S.",
     "Burnett"
    ],
    [
     "C. H.",
     "Ritz"
    ]
   ],
   "title": "Low rate speech coding incorporating simultaneously masked spectrally weighted linear prediction",
   "original": "e01_1989",
   "page_count": 4,
   "order": 477,
   "p1": "1989",
   "pn": "1992",
   "abstract": [
    "Linear prediction (LP) is the cornerstone of most modern speech compression algorithms. Previously it has been shown that incorporating a weighting function based on the simultaneous masking property of the ear into the calculation of the LP coefficients (SMWLPC) allows the filter to better model the unmasked sections of the input spectrum. This paper conducts a detailed analysis of the implementation of SMWLPC in low rate speech codecs. The analysis allows the cause of inconsistencies in the technique to be identified and solutions formulated. Experimental results show that when combined with the proposed changes, the SMWLPC technique is suitable for implementation in any low rate LP based speech codec and the net result is an improvement in the perceptual quality of synthesised speech for all speakers.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-469"
  },
  "najafzadeh01_eurospeech": {
   "authors": [
    [
     "Hossein",
     "Najaf-Zadeh"
    ],
    [
     "Peter",
     "Kabal"
    ]
   ],
   "title": "Narrowband perceptual audio coding: enhancements for speech",
   "original": "e01_1993",
   "page_count": 4,
   "order": 478,
   "p1": "1993",
   "pn": "1996",
   "abstract": [
    "This paper presents a bi-modal coding paradigm to compress narrowband audio signals at 8 kbit/s. In the general mode, the Enhanced Narrowband Audio Coder (ENPAC) exploits the characteristics of the human hearing system to adaptively code the perceptually important spectral components of the input audio. The other mode is employed to handle audio inputs with a strong harmonic structure. In that mode, the input block is represented by its audible harmonics. The spectral magnitude is modeled by the linear prediction analysis in the time domain. The phase of each harmonic is predicted and the phase residues are quantized using an adaptive bit allocation algorithm. This paper introduces a perceptually-based upper bound for phase errors of spectral components. The ENPAC encoder delivers good quality for narrowband speech and non-speech inputs.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-470"
  },
  "bessette01_eurospeech": {
   "authors": [
    [
     "B.",
     "Bessette"
    ],
    [
     "Roch",
     "Lefebvre"
    ],
    [
     "R.",
     "Salami"
    ],
    [
     "M.",
     "Jelinek"
    ],
    [
     "J.",
     "Vainio"
    ],
    [
     "J.",
     "Rotola-Pukkila"
    ],
    [
     "H.",
     "Mikkola"
    ],
    [
     "K.",
     "Jarvinen"
    ]
   ],
   "title": "Techniques for high-quality ACELP coding of wideband speech",
   "original": "e01_1997",
   "page_count": 4,
   "order": 479,
   "p1": "1997",
   "pn": "2000",
   "abstract": [
    "We present in this paper new methods for achieving high-quality wideband speech at low rates using the ACELP algorithm. Several innovations are introduced to optimize the quality and minimize the complexity of the coder. A multi-rate wideband speech encoding algorithm based on these techniques was recently selected by 3GPP as the standard for AMR-WB, and is currently one of the candidates for the ITU-T wideband speech coder standard at around 16 kbit/sec. This standard was jointly developed by VoiceAge and Nokia.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-471"
  },
  "pujalte01_eurospeech": {
   "authors": [
    [
     "Sílvia",
     "Pujalte"
    ],
    [
     "Asunción",
     "Moreno"
    ]
   ],
   "title": "Wideband ACELP at 16 kb/s with multi-band excitation",
   "original": "e01_2001",
   "page_count": 4,
   "order": 480,
   "p1": "2001",
   "pn": "2004",
   "abstract": [
    "This paper describes two wideband CELP coders at 16 kb/s. Their main feature is fast searching, achieving quality comparable to G.722 at 56 and 48 kb/s. Both the excitations derive from a multi-band algebraic structure in order to reduce computational complexity and bit allocation. The first one, the faster, has a full band excitation with two gains. The second one, which provides better quality, has a full band excitation with emphasized high frequencies. Analysis and error minimization are done over the full band.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-472"
  },
  "lee01e_eurospeech": {
   "authors": [
    [
     "Seung Won",
     "Lee"
    ],
    [
     "Keun Sung",
     "Bae"
    ]
   ],
   "title": "Wideband speech coding algorithm with application of discrete wavelet transform to upper band",
   "original": "e01_2005",
   "page_count": 4,
   "order": 481,
   "p1": "2005",
   "pn": "2008",
   "abstract": [
    "In this paper, we propose a new wideband speech coder which combines the European standard of a narrowband speech coder, i.e., GSM-EFR, and a transform coder using the discrete wavelet transform. Input speech is first split into two bands with equal bandwidth. A subband coder with wavelet transformed speech is designed for a upper band coder, and a GSM-EFR coder is adopted as a lower band coder. The total bit rate of the proposed coder is 18.9 kbps, and informal listening test results have shown that the proposed coder has comparable speech quality to that of G.722 with 56 kbps.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-473"
  },
  "satheesh01_eurospeech": {
   "authors": [
    [
     "S.",
     "Satheesh"
    ],
    [
     "T. V.",
     "Sreenivas"
    ]
   ],
   "title": "A switched DPCM/subband coder for pre-echo reduction",
   "original": "e01_2009",
   "page_count": 4,
   "order": 482,
   "p1": "2009",
   "pn": "2012",
   "abstract": [
    "Recently, adaptive subband coders based on wavelet packet decomposition and psychoacoustic modelling have been proposed to achieve transparent quality compression of audio signals. While these coders perform well for stationary signals, there is no special mechanism in the coder to prevent the pre-echo artifact when transient signals are encoded. In this paper, we propose a switched DPCM/Subband structure to remove the pre-echo problem. This is achieved through a novel temporally varying bit allocation scheme which is based on the temporal masking properties of the human auditory system. The proposed coder/decoder output is found to be free from pre-echo artifact even at a lower bitrate than the adaptive subband coder.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-474"
  },
  "etemoglu01_eurospeech": {
   "authors": [
    [
     "Cagri Özgenc",
     "Etemoglu"
    ],
    [
     "Vladimir",
     "Cuperman"
    ]
   ],
   "title": "A generalized multistage VQ approach for spectral magnitude quantization",
   "original": "e01_2013",
   "page_count": 4,
   "order": 483,
   "p1": "2013",
   "pn": "2016",
   "abstract": [
    "This paper presents a novel vector quantization (VQ) technique in which the quantized vector is formed by adding the transformed outputs of a multistage codebook rather than just adding the outputs of the stages as in regular multistage vector quantization (MSVQ). The transformations are selected from a family of linear transformations represented by a codebook of matrices. This technique can be viewed as a generalized form of MSVQ. If the transformations are constrained to be the identity transformation, this technique becomes identical to the regular MSVQ. The design algorithm is based on joint optimization of the linear transformations and the stage codebooks. It is shown that the proposed technique yields high quality spectral magnitude quantization with performance exceeding that of multistage vector quantization (MSVQ) of similar complexity and bit rate.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-475"
  },
  "jung01_eurospeech": {
   "authors": [
    [
     "Sung-Kyo",
     "Jung"
    ],
    [
     "Young-Cheol",
     "Park"
    ],
    [
     "Sung-Wan",
     "Youn"
    ],
    [
     "Kyoung-Tae",
     "Kim"
    ],
    [
     "Dae-Hee",
     "Youn"
    ]
   ],
   "title": "Efficient implementation of ITU-t g.723.1 speech coder for multichannel voice transmission and storage",
   "original": "e01_2017",
   "page_count": 4,
   "order": 484,
   "p1": "2017",
   "pn": "2020",
   "abstract": [
    "This paper presents an efficient implementation of G.723.1 speech coder. To simplify the excitation quantization procedure which is the most computationally demanding, we propose fast algorithms for adaptive codebook and fixed codebook search. In the fast adaptive codebook search, pitch delay and pitch gains are computed sequentially. In the fast fixed codebook search, the codebook structure is redesigned based on the interleaved single-pulse permutation (ISPP) design at high rate mode and the depth-first tree search is applied instead of nested-loop search at low rate mode. A real-time implementation is achieved using a 16-bit fixedpoint TMS320C62x DSP. The implemented G.723.1 speech coder requires 8.70 and 10.29 MHz clock cycles at low and high rate, respectively, 57.8 kByte of program memory and 55 kByte of data memory. Thus, more than 16 channels of G.723.1 coder can be operated in real-time using a single TMS320C62x DSP.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-476"
  },
  "hansen01b_eurospeech": {
   "authors": [
    [
     "John H. L.",
     "Hansen"
    ],
    [
     "Pongtep",
     "Angkititrakul"
    ],
    [
     "Jay",
     "Plucienkowski"
    ],
    [
     "Stephen",
     "Gallant"
    ],
    [
     "Umit",
     "Yapanel"
    ],
    [
     "Bryan",
     "Pellom"
    ],
    [
     "Wayne",
     "Ward"
    ],
    [
     "Ron",
     "Cole"
    ]
   ],
   "title": "CU-move : analysis & corpus development for interactive in-vehicle speech systems",
   "original": "e01_2023",
   "page_count": 4,
   "order": 485,
   "p1": "2023",
   "pn": "2026",
   "abstract": [
    "In this paper, we present our recent work in the analysis and formulation of a new acoustic speech corpus for developing in-vehicle interactive systems for route planning and navigation. The CU-Move Corpus development is partitioned into two phases: [I] acoustic noise collection and analysis across vehicles, and [II] data collection consisting of +1000 speakers from across the United States. We present results from Phase I acoustic noise data analysis across vehicles to determine guidelines for Phase II large-scale data collection using a single vehicle type. A total of 14 noise conditions are identified for analysis across 6 vehicles. We also present our plan for Phase II collection including speakers, dialect regions, data collection hardware, prompts and dialog domains. Since previous studies in speech recognition have shown significant loses in performance when speakers are under task stress, it is important to develop conversational systems that minimize operator stress for the driver. This will be the first U.S. based corpus of its kind consisting of multi-channel data, intended for use in developing mixed-initiative dialog speech systems; the initial application being route planning and navigation through a wireless information retrieval sub-system connected to the WWW.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-477"
  },
  "kawaguchi01_eurospeech": {
   "authors": [
    [
     "Nobuo",
     "Kawaguchi"
    ],
    [
     "Shigeki",
     "Matsubara"
    ],
    [
     "Kazuya",
     "Takeda"
    ],
    [
     "Fumitada",
     "Itakura"
    ]
   ],
   "title": "Multimedia data collection of in-car speech communication",
   "original": "e01_2027",
   "page_count": 4,
   "order": 486,
   "p1": "2027",
   "pn": "2030",
   "abstract": [
    "This paper reports the details of the collection of the multimedia data such as audio, video and auxiliary information of the vehicle during a spoken dialogue in a moving car. The system specially built in a Data CollectionVehicle (DCV) supports synchronous recording of multi-channel audio data from 16 microphones, 3-channel video data and the vehicle related data. Multimedia data has been collected for three sessions of spoken dialogue in about a 60-minute drive by each of 200 subjects. Data has been collected for two dialogue modes:(1) prompted dialogue between the driver and an accompanying operator and (2) natural dialogue between the driver and a telephone operator for information access over a cellular phone while driving a car. The corpus can be used for analysis of multimedia data in a moving car environment and also for modeling spoken dialogue in scenarios such as information access while driving a car.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-478"
  },
  "heeman01_eurospeech": {
   "authors": [
    [
     "Peter A.",
     "Heeman"
    ],
    [
     "David",
     "Cole"
    ],
    [
     "Andrew",
     "Cronk"
    ]
   ],
   "title": "The u.s. speechdat-car data collection",
   "original": "e01_2031",
   "page_count": 4,
   "order": 487,
   "p1": "2031",
   "pn": "2034",
   "abstract": [
    "The SpeechDat-Car data collection effort is an ambitious effort to collect data from multiple languages in an in-car setting. This paper describes the U.S. data collection effort. We discuss problems we had implementing the collection procedure; and changes we made to improve the procedure. This paper should benefit future in-car data collections.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-479"
  },
  "nemeth01_eurospeech": {
   "authors": [
    [
     "Géza",
     "Németh"
    ],
    [
     "Csaba",
     "Zainkó"
    ]
   ],
   "title": "Word unit based multilingual comparative analysis of text corpora",
   "original": "e01_2035",
   "page_count": 4,
   "order": 488,
   "p1": "2035",
   "pn": "2038",
   "abstract": [
    "Parallel study of three linguistically different languages - Hungarian. German and English - using text corpora of a similar size gives a possibility for the exploration of both similarities and differences. Corpora of publicly available Internet sources was used. Besides traditional corpus coverage, word length and occurence statistics, some new features about prosodic boundaries (sentence beginning and final positions, preceding and following a comma) were also computed. Among others, it was found, that the coverage of corpora by the most frequent words follows a parallel logarithmic rule for all languages in the 40-85% coverage range. The functions are much nearer for English and German than for Hungarian. The results can be applied in such diverse domains as predictive text input, word hyphenation, language modeling in speech recognition, corpus-based speech synthesis, etc. Keywords: text corpora, corpus analysis, multilinguality, word length, sentence length, unit based analysis, language modeling, corpus-based speech synthesis\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-480"
  },
  "backfried01_eurospeech": {
   "authors": [
    [
     "Gerhard",
     "Backfried"
    ],
    [
     "Robert",
     "Hecht"
    ],
    [
     "Sabine",
     "Loots"
    ],
    [
     "Norbert",
     "Pfannerer"
    ],
    [
     "Jürgen",
     "Riedler"
    ],
    [
     "Christian",
     "Schiefer"
    ]
   ],
   "title": "Creating a european English broadcast news transcription corpus and system",
   "original": "e01_2039",
   "page_count": 4,
   "order": 489,
   "p1": "2039",
   "pn": "2042",
   "abstract": [
    "Based on BBN's Rough'n'Ready suite of technologies used in the DARPA Hub-4 evaluations we describe the Sail-Labs Media Indexer system aiming at processing European English television broadcasts. We discuss the development of a European English broadcast news corpus, suitable for measuring performance of system components, such as speaker identification and speech recognition. We further report evaluation results on our multi-purpose test set, and outline the integration of real-time indexing into a spoken document retrieval system.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-481"
  },
  "burger01_eurospeech": {
   "authors": [
    [
     "Susanne",
     "Burger"
    ],
    [
     "Laurent",
     "Besacier"
    ],
    [
     "Paolo",
     "Coletti"
    ],
    [
     "Florian",
     "Metze"
    ],
    [
     "Céline",
     "Morel"
    ]
   ],
   "title": "The nespole! voIP dialogue database",
   "original": "e01_2043",
   "page_count": 4,
   "order": 490,
   "p1": "2043",
   "pn": "2046",
   "abstract": [
    "This paper presents the status of the NESPOLE! data collection as of end of February, 2001. A multilingual VoIP (Voice over Internet Protocol networks) database consisting of 200 dialogues in 4 languages (English, German, Italian and French) was recorded and transcribed. Dialogue speakers were connected via a H323 video-conferencing terminal. We describe the task, the technical architecture, the recording procedure and the transcription process of the NESPOLE! data collection. We provide some statistics concerning the data and, finally, we address problems that arose during the collection and annotation process.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-482"
  },
  "matousek01_eurospeech": {
   "authors": [
    [
     "Jindrich",
     "Matousek"
    ],
    [
     "Josef",
     "Psutka"
    ],
    [
     "Jiri",
     "Kruta"
    ]
   ],
   "title": "Design of speech corpus for text-to-speech synthesis",
   "original": "e01_2047",
   "page_count": 4,
   "order": 491,
   "p1": "2047",
   "pn": "2050",
   "abstract": [
    "This paper deals with the design of a speech corpus for a concatenation-based text-to-speech (TTS) synthesis. Several aspects of the design process are discussed here. We propose a sentence selection algorithm to choose sentences (from a large text corpus) which will be read and stored in a speech corpus. The selected sentences should include all possible triphones in a sufficient number of occurrences. Some notes on recording the speech are also discussed to ensure a quality speech corpus. As some popular speech synthesis techniques require knowing the moments of principal excitation of vocal tract during the speech, pitch-mark detection is also a subject of our attention. Several automatic pitch-mark detection methods are discussed here and a comparison test is performed to find out the best method.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-483"
  },
  "son01_eurospeech": {
   "authors": [
    [
     "Rob J. J. H. van",
     "Son"
    ],
    [
     "Diana",
     "Binnenpoorte"
    ],
    [
     "Henk van den",
     "Heuvel"
    ],
    [
     "Louis C. W.",
     "Pols"
    ]
   ],
   "title": "The IFA corpus: a phonemically segmented dutch \"open source\" speech database",
   "original": "e01_2051",
   "page_count": 4,
   "order": 492,
   "p1": "2051",
   "pn": "2054",
   "abstract": [
    "An open source database of hand-segmented Dutch speech was constructed with off-the-shelf software using speech from 8 speakers in a variety of speaking styles. For a total of 50,000 words, speech acquisition and preparation took around 3 person-weeks per speaker. Hand segmentation took 1,000 hours of labeling altogether. The asymptotic segmentation speed was about one word, or four boundaries, per minute. An evaluation showed that the Median Absolute Difference of the segment boundaries was 6 ms between labelers, and 4 ms within labelers. Label differences (substitutions, insertions, and deletions) were found in 8% of the segments between labelers and 5% within labelers. Compiled data are available in relational database format for querying with SQL.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-484"
  },
  "louw01_eurospeech": {
   "authors": [
    [
     "Philippa H.",
     "Louw"
    ],
    [
     "Justus C.",
     "Roux"
    ],
    [
     "Elizabeth C.",
     "Botha"
    ]
   ],
   "title": "African speech technology (AST) telephone speech databases: corpus design and contents",
   "original": "e01_2055",
   "page_count": 4,
   "order": 493,
   "p1": "2055",
   "pn": "2058",
   "abstract": [
    "The African Speech Technology project is developing telephone speech databases for five of South Africas eleven official languages, i.e. South African English, Afrikaans, Zulu, Xhosa, and Southern Sotho. These databases will be fully transcribed - orthographically and phonetically - and will be used for the training and testing of phoneme-based, speaker-independent speech recognition systems. The project aims to deliver a telephone speech application developers software toolkit. A prototype multilingual enquiry and booking system for the hotel industry will be developed as a first application. This paper describes the design and contents of the speech corpus that is currently being collected over both mobile and fixed networks. In particular language coverage is discussed within the framework of the multilingual character of the South African population. Some language specific differences with regards to the contents of the different databases are noted. Methods and tools applied in the acquisition of phonetic information are discussed.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-485"
  },
  "heuvel01_eurospeech": {
   "authors": [
    [
     "Henk van den",
     "Heuvel"
    ],
    [
     "Jerome",
     "Boudy"
    ],
    [
     "Zsolt",
     "Bakcsi"
    ],
    [
     "Jan",
     "Cernocky"
    ],
    [
     "Valery",
     "Galunov"
    ],
    [
     "Julia",
     "Kochanina"
    ],
    [
     "Wojciech",
     "Majewski"
    ],
    [
     "Petr",
     "Pollak"
    ],
    [
     "Milan",
     "Rusko"
    ],
    [
     "Jerzy",
     "Sadowski"
    ],
    [
     "Piotr",
     "Staroniewicz"
    ],
    [
     "Herbert S.",
     "Tropf"
    ]
   ],
   "title": "Speechdat-e: five eastern european speech databases for voice-operated teleservices completed",
   "original": "e01_2059",
   "page_count": 4,
   "order": 494,
   "p1": "2059",
   "pn": "2062",
   "abstract": [
    "In the Speechdat-E project five medium large telephone speech databases have been collected for Czech, Hungarian, Polish, Russian, and Slovak. The project was recently concluded. This paper reports briefly on the contents of the databases, elaborates on experiences gained from the data recordings and from the validation of the databases. The availability of the databases to the public is addressed, too.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-486"
  },
  "gibbon01c_eurospeech": {
   "authors": [
    [
     "Dafydd",
     "Gibbon"
    ],
    [
     "Thorsten",
     "Trippel"
    ],
    [
     "Serge",
     "Sharoff"
    ]
   ],
   "title": "Concordancing for parallel spoken language corpora",
   "original": "e01_2063",
   "page_count": 4,
   "order": 495,
   "p1": "2063",
   "pn": "2066",
   "abstract": [
    "Concordancing is one of the oldest corpus analysis tools, especially for written corpora. In NLP concordancing appears in training of speech-recognition system. Additionally, comparative studies of different languages result in parallel corpora. Concordancing for these corpora in a NLP context is a new approach. We propose to combine these fields of interest for a multi-purpose concordance for Spoken Language Data, opening the opportunity of combining corpus-linguistic and NLP methods resulting in a broader empirical basis for NLP research. Theoretic models for audio-concordances are discussed. Principles of the structure and design of a parallel audio concordance are given, coding by means of XML to ensure reusability and flexibility, using time stamps for referencing from annotations to the signal.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-487"
  },
  "psutka01b_eurospeech": {
   "authors": [
    [
     "Josef",
     "Psutka"
    ],
    [
     "Vlasta",
     "Radova"
    ],
    [
     "Ludek",
     "Müller"
    ],
    [
     "Jindrich",
     "Matousek"
    ],
    [
     "Pavel",
     "Ircing"
    ],
    [
     "David",
     "Graff"
    ]
   ],
   "title": "Large broadcast news and read speech corpora of spoken czech",
   "original": "e01_2067",
   "page_count": 4,
   "order": 496,
   "p1": "2067",
   "pn": "2070",
   "abstract": [
    "This paper presents the first annotated and phonetically transcribed large speech corpora developed for spoken Czech. All corpora were collected during the last two years at the Department of Cybernetics, University of West Bohemia (UWB) in Pilsen. The first two collections are broadcast news, the third corpus is a high-quality read-speech database. This paper describes the collection conditions, annotation and phonetic transcription process related to each corpus. The basic phonetic and lexical characteristics of all corpora will be given and compared mutually.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-488"
  },
  "yablonsky01_eurospeech": {
   "authors": [
    [
     "Serge A.",
     "Yablonsky"
    ]
   ],
   "title": "Development of Russian lexical databases, corpora and supporting tools for speech products",
   "original": "e01_2071",
   "page_count": 4,
   "order": 497,
   "p1": "2071",
   "pn": "2074",
   "abstract": [
    "The situation with regard to Russian language resources is fragmented and disorganized. For this reason, it is important to promote for Russian the development of its basic resources in one package that could be used for development of speech products. The paper presents a design of the Russian lexical databases, corpora and supporting tools (system for construction and support of lexical databases, system for transcription, morphological analyzer and normalyzer) developed for wide usage in speech engineering.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-489"
  },
  "fotinea01_eurospeech": {
   "authors": [
    [
     "Stavroula-Evita F.",
     "Fotinea"
    ],
    [
     "George D.",
     "Tambouratzis"
    ],
    [
     "George V.",
     "Carayannis"
    ]
   ],
   "title": "Constructing a segment database for greek time domain speech synthesis",
   "original": "e01_2075",
   "page_count": 4,
   "order": 498,
   "p1": "2075",
   "pn": "2078",
   "abstract": [
    "In this article, a methodology is presented regarding the design of a segment database for use with a time-domain speech synthesis system for the Greek language. The main issue of this process is the systematic generation of a corpus containing all possible instances of the segments for the specific language. Particular issues such as the phonetic coverage, the sentence selection as well as iterative evaluation techniques employing custom-built tools are discussed. The resulting corpus is characterised by a near-minimal size, provides a complete coverage of the Greek language and its distribution of phonemes is similar to that of natural corpora. A typical spoken acquisition procedure may then be performed, resulting in a segment database for use with a time-domain Greek synthesizer. The corpus creation procedure allows for the fine-tuning of the segment database's language-dependent characteristics and thus assists in the generation of high-quality text-to-speech synthesis.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-490"
  },
  "hone01_eurospeech": {
   "authors": [
    [
     "Kate S.",
     "Hone"
    ],
    [
     "Robert",
     "Graham"
    ]
   ],
   "title": "Subjective assessment of speech-system interface usability",
   "original": "e01_2083",
   "page_count": 4,
   "order": 499,
   "p1": "2083",
   "pn": "2086",
   "abstract": [
    "Methods for the evaluation of the efficiency and effectiveness of speech input / output systems are well established. However, user subjective reactions to speech interfaces may well be a more important predictor of real world success. A review of existing subjective measures of speech system usability reveals a number of limitations in the approaches taken. This paper then summarises the work we have conducted in developing a new measure for the subjective assessment of speech system interfaces (SASSI).\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-491"
  },
  "chu01b_eurospeech": {
   "authors": [
    [
     "Min",
     "Chu"
    ],
    [
     "Hu",
     "Peng"
    ]
   ],
   "title": "An objective measure for estimating MOS of synthesized speech",
   "original": "e01_2087",
   "page_count": 4,
   "order": 500,
   "p1": "2087",
   "pn": "2090",
   "abstract": [
    "This paper proposes an average concatenative cost function as the objective measure for naturalness of synthesized speech. All its seven component-costs can be derived directly from the input text and the scripts of speech database. A formal Mean Opinion Score (MOS) experiment shows that the average concatenative cost and its seven components are all highly correlated with MOS obtained subjectively. The correlation coefficient between the objective measure and subjective measure is 0.872. The mean of errors in MOS estimation for individual waveforms is 0.32 with 0.40 RMSE. When estimating the overall MOS for TTS systems, the mean error is smaller than 0.05. With the proposed objective measure, it becomes possible and easy for us to track the performance in naturalness regularly. The proposed cost function could also serve as criteria for optimizing the algorithms for unit selecting and speech database pruning.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-492"
  },
  "strik01_eurospeech": {
   "authors": [
    [
     "Helmer",
     "Strik"
    ],
    [
     "Catia",
     "Cucchiarini"
    ],
    [
     "Judith M.",
     "Kessens"
    ]
   ],
   "title": "Comparing the performance of two CSRs: how to determine the significance level of the differences",
   "original": "e01_2091",
   "page_count": 4,
   "order": 501,
   "p1": "2091",
   "pn": "2094",
   "abstract": [
    "When two CSRs are compared, it is important to test what the significance level of the difference is. For this purpose a metric and a statistical test are needed. In this paper we compare several combinations of a metric with a statistical test, in order to find a combination which is suitable for this task. Four combinations which are introduced in this paper appear to be suitable for this task.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-493"
  },
  "terashima01_eurospeech": {
   "authors": [
    [
     "Ryuta",
     "Terashima"
    ],
    [
     "Hiroyuki",
     "Hoshino"
    ],
    [
     "Toshihiro",
     "Wakita"
    ]
   ],
   "title": "Prediction of low recognition rate words for isolated word recognition system",
   "original": "e01_2095",
   "page_count": 4,
   "order": 502,
   "p1": "2095",
   "pn": "2098",
   "abstract": [
    "This paper describes an efficient method to predict words whose recognition rates are low. This method is based on the idea that the minimum value of the word pair recognition rates corresponds to the word recognition rate. The word pair recognition rates can be calculated by the measured distributions of phoneme log likelihood difference. The proposed method was evaluated by recognition experiments using about 3000 word pairs. The correlation coefficient between the predicted and the measured recognition rates was 0.87 when the phoneme lengths of both words of the word pair were equal. Furthermore, we also estimated a 95% confidence interval for the measured recognition rates, and the percentage of the predicted words that were contained in the confidence interval was 94.8%. The results showed the effectiveness of the proposed method for predicting the word pair recognition rates.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-494"
  },
  "batusek01_eurospeech": {
   "authors": [
    [
     "Robert",
     "Batusek"
    ]
   ],
   "title": "An objective measure for assessment of the concatenative TTS segment inventories",
   "original": "e01_2099",
   "page_count": 4,
   "order": 503,
   "p1": "2099",
   "pn": "2102",
   "abstract": [
    "In the paper we present a method for assessment of the segment inventories for concatenative text-to-speech synthesis. We argue that the overall comprehensibility of the synthesized speech depends on the length of the segments - longer segments imply more intelligible speech. The problem of minimum text cover by the given segment set is formulated in the paper as well as an algorithm finding the solution. Some improvements speeding up the algorithm are discussed in the rest of the paper.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-495"
  },
  "zhang01d_eurospeech": {
   "authors": [
    [
     "Rong",
     "Zhang"
    ],
    [
     "Alexander I.",
     "Rudnicky"
    ]
   ],
   "title": "Word level confidence annotation using combinations of features",
   "original": "e01_2105",
   "page_count": 4,
   "order": 504,
   "p1": "2105",
   "pn": "2108",
   "abstract": [
    "This paper describes the development of a word-level confidence metric suitable for use in a dialog system. Two aspects of the problems are investigated: the identification of useful features and the selection of an effective classifier. We find that two parse-level features, Parsing-Mode and Slot-Backoff-Mode, provide annotation accuracy comparable to that observed for decoder-level features. However, both decoder-level and parse-level features independently contribute to confidence annotation accuracy. In comparing different classification techniques, we found that Support Vector Machines (SVMs) appear to provide the best accuracy. Overall we achieve 39.7% reduction in annotation uncertainty for a binary confidence decision in a travel-planning domain.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-496"
  },
  "moreno01_eurospeech": {
   "authors": [
    [
     "Pedro J.",
     "Moreno"
    ],
    [
     "Beth",
     "Logan"
    ],
    [
     "Bhiksha",
     "Raj"
    ]
   ],
   "title": "A boosting approach for confidence scoring",
   "original": "e01_2109",
   "page_count": 4,
   "order": 505,
   "p1": "2109",
   "pn": "2112",
   "abstract": [
    "In this paper we present the application of a boosting classification algorithm to confidence scoring. We derive feature vectors from speech recognition lattices and feed them into a boosting classifier. This classifier combines hundreds of very simple `weak learners' and derives classification rules that can reduce the confidence error rate by up to 34%. We compare our results to those obtained using two other standard classification techniques, Support Vector Machines (SVMs) and Classification and Regression Trees (CART), and show significant improvements. Furthermore, the nature of the boosting algorithm allows us to combine the best single classifier and improve its performance. We present experimental results on real world corpora derived from our SpeechBot Web index http://www.speechbot.com) and from the HUB4 DARPA evaluation sets. We believe these results have wide applicability to audio indexing and to acoustic and language modeling adaptation where word confidence scores can be used in iterative adaptation schemes.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-497"
  },
  "charlet01_eurospeech": {
   "authors": [
    [
     "Delphine",
     "Charlet"
    ],
    [
     "Guy",
     "Mercier"
    ],
    [
     "Denis",
     "Jouvet"
    ]
   ],
   "title": "On combining confidence measures for improved rejection of incorrect data",
   "original": "e01_2113",
   "page_count": 4,
   "order": 506,
   "p1": "2113",
   "pn": "2116",
   "abstract": [
    "In this paper, techniques for combining confidence measures are proposed and evaluated. Confidence measures are useful for rejecting incorrect data, which is an important issue in speech recognition based interactive systems. Many ways of computing individual confidence measures have already been investigated. A detailed analysis of various confidence measures shows that they behave differently for what concerns rejection of incorrect data on various field data subsets (substitution errors, out-of-vocabulary data & noise tokens) collected from a vocal directory task. Two combination methods are then presented. One combines confidence measures by means of a neural network and the other through logistic regression. Evaluations shows that both combination techniques are efficient, and both take the best of the various individual confidence measures involved on each data subset.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-498"
  },
  "palmer01_eurospeech": {
   "authors": [
    [
     "David D.",
     "Palmer"
    ],
    [
     "Mari",
     "Ostendorf"
    ]
   ],
   "title": "Improved word confidence estimation using long range features",
   "original": "e01_2117",
   "page_count": 4,
   "order": 507,
   "p1": "2117",
   "pn": "2120",
   "abstract": [
    "This paper describes experiments in improving word confidence estimation using document- and task-level features of the hypothesized word sequence from a recognizer. The improved confidence estimates are shown to improve information extraction performance, specifically named entity (NE) recognition. The detected names can then be used to further improve confidence estimation in a multi-pass NE recognition framework.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-499"
  },
  "carpenter01_eurospeech": {
   "authors": [
    [
     "Paul",
     "Carpenter"
    ],
    [
     "Chun",
     "Jin"
    ],
    [
     "Daniel",
     "Wilson"
    ],
    [
     "Rong",
     "Zhang"
    ],
    [
     "Dan",
     "Bohus"
    ],
    [
     "Alexander I.",
     "Rudnicky"
    ]
   ],
   "title": "Is this conversation on track?",
   "original": "e01_2121",
   "page_count": 4,
   "order": 508,
   "p1": "2121",
   "pn": "2124",
   "abstract": [
    "Confidence annotation allows a spoken dialog system to accurately assess the likelihood of misunderstanding at the utterance level and to avoid breakdowns in interaction. We describe experiments that assess the utility of features from the decoder, parser and dialog levels of processing. We also investigate the effectiveness of various classifiers, including Bayesian Networks, Neural Networks, SVMs, Decision Trees, AdaBoost and Naive Bayes, to combine this information into an utterance-level confidence metric. We found that a combination of a subset of the features considered produced promising results with several of the classification algorithms considered, e.g., our Bayesian Network classifier produced a 45.7% relative reduction in confidence assessment error and a 29.6% reduction relative to a handcrafted rule.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-500"
  },
  "nisimura01_eurospeech": {
   "authors": [
    [
     "Ryuichi",
     "Nisimura"
    ],
    [
     "Kumiko",
     "Komatsu"
    ],
    [
     "Yuka",
     "Kuroda"
    ],
    [
     "Kentaro",
     "Nagatomo"
    ],
    [
     "Akinobu",
     "Lee"
    ],
    [
     "Hiroshi",
     "Saruwatari"
    ],
    [
     "Kiyohiro",
     "Shikano"
    ]
   ],
   "title": "Automatic n-gram language model creation from web resources",
   "original": "e01_2127",
   "page_count": 4,
   "order": 509,
   "p1": "2127",
   "pn": "2130",
   "abstract": [
    "This paper describes an automatic building of N-gram language models from Web texts for large vocabulary continuous speech recognition. Although a huge amount of well-formed texts are needed to train a model, collecting and organizing such text corpus for every task by hand needs a great labor. We need the language model to update frequently to cover the current topics. To deal with this problem, we propose an automatic language model creation method by collecting Web texts via keyword-based Web search engines. We can build a task-dependent language model by selecting suitable keywords for the task. A text filtering algorithm based on character perplexity is developed to extract proper Japanese texts from Web texts. A language model for a medical consulting task created by the proposed method shows the higher word recognition rate by 11.4% than that of a conventional newspaper language model.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-501"
  },
  "caseiro01_eurospeech": {
   "authors": [
    [
     "Diamantino",
     "Caseiro"
    ],
    [
     "Isabel",
     "Trancoso"
    ]
   ],
   "title": "On integrating the lexicon with the language model",
   "original": "e01_2131",
   "page_count": 4,
   "order": 510,
   "p1": "2131",
   "pn": "2134",
   "abstract": [
    "The goal of this work was to develop an algorithm for the integration of the lexicon with the language model which would be computationally efficient in terms of memory requirements, even in the case of large trigram models. Two specialized versions of the algorithm for transducer composition were implemented. The first one is basically a composition algorithm that uses the precomputed set of the output labels that can be reached from a particular epsilon edge of the lexicon; the second includes an \"on the fly\" implementation of the pushing of weights and output labels. Very significant memory savings were obtained with the proposed algorithms compared with the general determinization algorithm for weighted transducers.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-502"
  },
  "varona01_eurospeech": {
   "authors": [
    [
     "A.",
     "Varona"
    ],
    [
     "I.",
     "Torres"
    ]
   ],
   "title": "Back-off smoothing evaluation over syntactic language models",
   "original": "e01_2135",
   "page_count": 4,
   "order": 511,
   "p1": "2135",
   "pn": "2138",
   "abstract": [
    "Continuous Speech Recognition systems require a Language Model (LM) to represent the syntactic constraints of the language. In LMs development a smoothing technique needs to be applied to also consider events not represented in the training corpus. In this work, several back-off smoothing approaches have been compared: classical discounting-distribution schema including Witten-Bell, Absolute and Linear discounting and a new proposal, the Delimited discounting. Delimited discounting deals with the Turing discounting problems while keeping the Katz's smoothing scheme. The experimental evaluation was carried out over a Spanish speech application task, showing that an increase of the test set perplexity of a LM does not always mean a degradation in the model performance when integrated into a CSR system. Besides, there is a strong dependence between the amount of probability reserved by the smoothing technique to be assigned to unseen events and the value of the balance parameter applied to the LM probabilities in the Bayes's rule needed to get the best system performance.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-503"
  },
  "wu01c_eurospeech": {
   "authors": [
    [
     "Genqing",
     "Wu"
    ],
    [
     "Fang",
     "Zheng"
    ],
    [
     "Ling",
     "Jin"
    ],
    [
     "Wenhu",
     "Wu"
    ]
   ],
   "title": "An online incremental language model adaptation method",
   "original": "e01_2139",
   "page_count": 4,
   "order": 512,
   "p1": "2139",
   "pn": "2142",
   "abstract": [
    "In this paper, an online incremental language model adaptation method is proposed, which is different from the traditional offline language model adaptation method. There are some problems in the online incremental adaptation. The first one is how to adjust the model parameters online and modify the model incrementally. The second one is how to induce new words and assign initial probabilities to the n-grams related to them. In our application for Chinese character input method editor, the language model is divided into two parts, corresponding to the background (general-purpose) model and the user model, respectively. A modified maximum a posterior method is proposed for adapting the user model dynamically. Experiments are done to test the proposed method on an Chinese sentence input system and the results show that a satisfying word error rate reduction is obtained when the input articles are of similar topics.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-504"
  },
  "samuelsson01_eurospeech": {
   "authors": [
    [
     "Christer",
     "Samuelsson"
    ],
    [
     "James L.",
     "Hieronymus"
    ]
   ],
   "title": "Using boosting and POS word graph tagging to improve speech recognition",
   "original": "e01_2143",
   "page_count": 4,
   "order": 513,
   "p1": "2143",
   "pn": "2146",
   "abstract": [
    "The word graphs produced by a large vocabulary speech recognition system usually contain a path labelled with the correct utterance, but this is not always the highest scoring path. Boosting increases the probability of words which occur often in the word graph, which are in some sense robust. Adding syntactic information allows rescoring of arc probabilities with the possibility that more grammarical word sequences will also be the correct ones. A theory is developed which allows general probabilistic syntactic models to be used to rescore word lattices. Experiments conducted on the Wall Street Journal (WSJ) corpus with a version of the AT&T 1995 FST LVSR system with part of speech (POS) trigram sequences show that using only POS leads to a loss in performance. Boosting alone provides an improvement in performance which is not statistically significant. Cascading the two methods, boosting first and then using syntactic information improves performance 4.5 % relative on a large portion of the 1995 DARPA test set.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-505"
  },
  "yan01_eurospeech": {
   "authors": [
    [
     "Pengju",
     "Yan"
    ],
    [
     "Fang",
     "Zheng"
    ],
    [
     "Mingxing",
     "Xu"
    ]
   ],
   "title": "Robust parsing in spoken dialogue systems",
   "original": "e01_2149",
   "page_count": 4,
   "order": 514,
   "p1": "2149",
   "pn": "2152",
   "abstract": [
    "The rule-based parsing is a prevalent method for the natural language understanding (NLU) and has been introduced in dialogue systems for spoken language processing (SLP). However, additional measures must be taken to cope with the severe spoken linguistic phenomena, such as garbage, repetition, ellipsis, word disordering, fragment and ill form, which frequently occur in the spoken language. We propose in this paper a robust parsing scheme, which integrates the following methods. Keywords are used as terminal symbols; hence the symbol set of the grammar is purely within the semantical category. The definition of the grammar is extended to accommodate four types of rules, called up-tying, by-passing, up-messing, and over-crossing respectively. An improved chart parser, named marionette, is designed to parse the semantic grammar instance. The robust parsing scheme has been adopted in an air traveling information service system, called EasyFlight, and has achieved a high performance when dealing with the spontaneous speech.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-506"
  },
  "huang01b_eurospeech": {
   "authors": [
    [
     "Yinfei",
     "Huang"
    ],
    [
     "Fang",
     "Zheng"
    ],
    [
     "Yi",
     "Su"
    ],
    [
     "Fang",
     "Li"
    ],
    [
     "Wenhu",
     "Wu"
    ]
   ],
   "title": "A theme structure method for the ellipsis resolution",
   "original": "e01_2153",
   "page_count": 4,
   "order": 515,
   "p1": "2153",
   "pn": "2156",
   "abstract": [
    "The purpose of this paper is to solve the contextual ellipsis problem that is popular in our Chinese spoken dialogue system named EasyNav. A Theme Structure is proposed to describe the attentional state. Its dynamic generation feature makes it suitable to model the topic transition in user-initiative dialogues. By studying the differences and the similarities between the ellipsis and the anaphora phenomena, we extend the resolution procedure and the theory from anaphora to ellipsis. The ellipsis resolution is now based on the semantic knowledge and the discourse factor other than the syntactic information. A Theme Structure Method proposed in this paper for the ellipsis resolution is uniform to not only all kinds of elliptical elements but also some particular ellipsis types such as the fragmental ellipsis and the default ellipsis.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-507"
  },
  "haase01_eurospeech": {
   "authors": [
    [
     "Martin",
     "Haase"
    ],
    [
     "Werner",
     "Kriechbaum"
    ],
    [
     "Gregor",
     "Möhler"
    ],
    [
     "Gerhard",
     "Stenzel"
    ]
   ],
   "title": "Deriving document structure from prosodic cues",
   "original": "e01_2157",
   "page_count": 4,
   "order": 516,
   "p1": "2157",
   "pn": "2160",
   "abstract": [
    "This study presents an approach for prosody-driven segmentation of speech data. The model is based solely on F0 contours and RMS envelopes. Phoneme or word information from a speech recognizer is unneccesary. Using data from German broadcast news, we show how this prosodic information can be exploited to retrieve structural information of the spoken text. The suitability of the CART-like algorithm for utterance boundary prediction has been evaluated on 7 five-minutes-news- reports, using 28 reports as training material for the classification tree. Sentence boundaries were predicted with a precision of 93%, at a recall of 88%.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-508"
  },
  "su01_eurospeech": {
   "authors": [
    [
     "Yi",
     "Su"
    ],
    [
     "Fang",
     "Zheng"
    ],
    [
     "Yinfei",
     "Huang"
    ]
   ],
   "title": "Design of a semantic parser with support to ellipsis resolution in a Chinese spoken language dialogue system",
   "original": "e01_2161",
   "page_count": 4,
   "order": 517,
   "p1": "2161",
   "pn": "2164",
   "abstract": [
    "In this paper, a semantic parser with support to ellipsis resolution in a Chinese spoken language dialogue system is proposed. The grammar and parsing strategy of this parser is designed to address the characteristics of spoken language and to support the ellipsis resolution. Namely, it parses the user utterance with a domain-specific semantic grammar based on a template-filling approach. Syntactic constraints extracted by a Generalized LR parser are also used in the parsing process. With a paradigm of two-state bottom-up parsing and a scoring scheme, the ellipsis resolution module is integrated into the parser seamlessly. The parsing result is represented by a linked structure of semantic frames, which is convenient to both the parser and its successive components of the dialogue system.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-509"
  },
  "sansegundo01_eurospeech": {
   "authors": [
    [
     "R.",
     "San-Segundo"
    ],
    [
     "J. M.",
     "Montero"
    ],
    [
     "J.",
     "Colás"
    ],
    [
     "J.",
     "Gutiérrez"
    ],
    [
     "J. M.",
     "Ramos"
    ],
    [
     "Juan M.",
     "Pardo"
    ]
   ],
   "title": "Methodology for dialogue design in telephone-based spoken dialogue systems: a Spanish train information system",
   "original": "e01_2165",
   "page_count": 4,
   "order": 518,
   "p1": "2165",
   "pn": "2168",
   "abstract": [
    "In this paper, we propose a new methodology for designing dialogue managers in telephone-based spoken dialogue systems. This methodology comprises five steps: database analysis, design by intuition, design by observation, simulation and iterative improvement. At each step, several measures to evaluate the designing alternatives are presented. We introduce confidence measures in recognition to define an efficient confirmation strategy in each case. The use of user-modeling techniques adapts the system to the user ability. This methodology is applied for designing a telephone-based system that provides rail travel information for the main Spanish intercity connections, including timetables, simulated fares and reservations. With 30 users completing 4 scenarios, the average duration for a fully automatic call is 204 seconds. The users validated the applicability and usability of the system with a global score of 3.9 (out of 5).\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-510"
  },
  "zhang01e_eurospeech": {
   "authors": [
    [
     "Bo",
     "Zhang"
    ],
    [
     "Qingsheng",
     "Cai"
    ],
    [
     "Jianfeng",
     "Mao"
    ],
    [
     "Eric",
     "Chang"
    ],
    [
     "Baining",
     "Guo"
    ]
   ],
   "title": "Spoken dialogue management as planning and acting under uncertainty",
   "original": "e01_2169",
   "page_count": 4,
   "order": 519,
   "p1": "2169",
   "pn": "2172",
   "abstract": [
    "Some stochastic models like Markov decision process (MDP) are used to model the dialogue manager. MDP-based system degrades fast when uncertainty about user¡6s intention increases. We propose a novel dialogue model based on the partially observable Markov decision process (POMDP). We use hidden system states and user intentions as the state set, parser results and low-level information as the observation set, domain actions and dialogue repair actions as the action set. Here the low-level information is extracted from different input modals using Bayesian networks. Because of the limitation of exact algorithms, we focus on heuristic methods and their applicability in dialogue management.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-511"
  },
  "matsusaka01_eurospeech": {
   "authors": [
    [
     "Yosuke",
     "Matsusaka"
    ],
    [
     "Shinya",
     "Fujie"
    ],
    [
     "Tetsunori",
     "Kobayashi"
    ]
   ],
   "title": "Modeling of conversational strategy for the robot participating in the group conversation",
   "original": "e01_2173",
   "page_count": 4,
   "order": 520,
   "p1": "2173",
   "pn": "2176",
   "abstract": [
    "This paper describes a strategy for the conversation system to take part in human-to-human group conversation. One big characteristic of the group conversation system is that it can choose whether to observe or to take turn in the conversation. We implement the computational model combined with speech and gaze recognizers to keep the rules in turn taking, and define an interruption decision strategy based on an analysis of human needs. And finally, we realized a human-friendly group conversation system by combining multi-modal information processing/expression abilities of humanoid robot ROBITA.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-512"
  },
  "terken01_eurospeech": {
   "authors": [
    [
     "Jacques",
     "Terken"
    ],
    [
     "Saskia te",
     "Riele"
    ]
   ],
   "title": "Supporting the construction of a user model in speech-only interfaces by adding multi-modality",
   "original": "e01_2177",
   "page_count": 4,
   "order": 521,
   "p1": "2177",
   "pn": "2180",
   "abstract": [
    "Comparing to graphical user interfaces, speech-only interfaces face several problems: robustness, making clear what functionality is available, and making clear how the functionality may be accessed. We explore a potential solution for these problems by presenting a visual representation of the domain of discourse and of the state of the dialogue. We describe an experiment in which uni-modal and multi-modal interfaces are compared in terms of effectiveness, efficiency and satisfaction. The results of the experiment show a strong learning effect. Subjects who start using the multi-modal interface subsequently have a strong advantage when switching to the uni-modal (speech-only) interface, compared to subjects who start by using the uni-modal interface, switching to the multi-modal interface later on. The results are discussed in terms of the need to establish an appropriate user model as early as possible. We discuss implications of this interpretation for interaction design.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-513"
  },
  "tseng01_eurospeech": {
   "authors": [
    [
     "Shu-Chuan",
     "Tseng"
    ]
   ],
   "title": "A word- and turn-oriented approach to exploring the structure of Mandarin dialogues",
   "original": "e01_2181",
   "page_count": 4,
   "order": 522,
   "p1": "2181",
   "pn": "2184",
   "abstract": [
    "This paper investigates the structure of Mandarin spoken dialogues by analysing the distribution of words and turns used in dialogues. The results of an empirical quantitative study show that independent of speakers, there exists a kind of basic vocabulary for daily Mandarin conversations. It is proposed that this is the minimal set of a lexicon for the use of spoken Mandarin. Moreover, a number of words in the basic vocabulary were specifically used for marking various constituent boundaries in discourse, such as turn-initial and utterance-final. Discourse markers and disfluency are also taken into consideration for their highly frequent occurrences and their function of marking significant positions in spoken discourse. By means of lexical distribution and its interaction with turn taking, this paper demonstrates a new attempt to analyse the structure of Mandarin spoken dialogues.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-514"
  },
  "niimi01_eurospeech": {
   "authors": [
    [
     "Yasuhisa",
     "Niimi"
    ],
    [
     "Tomoki",
     "Oku"
    ],
    [
     "Takuya",
     "Nishimoto"
    ],
    [
     "Masahiro",
     "Araki"
    ]
   ],
   "title": "A rule based approach to extraction of topics and dialog acts in a spoken dialog system",
   "original": "e01_2185",
   "page_count": 4,
   "order": 523,
   "p1": "2185",
   "pn": "2188",
   "abstract": [
    "This paper presents a rule based approach to extraction of dialog acts and topics from utterances in a spoken dialog system with a task-independent dialog controller based on an extension of the frame-driven method. We demonstrated it could control dialogs in several different task domains, only given a set of topic frames and a set of rules manually designed for the discourse analysis which were both task-dependent. In this paper we report an approach to semiautomatic derivation of a set of rules for the discourse analysis from information needed to specify a task, for example, a set of topic frames, a conceptual tree of those words and case frames of those verbs which are likely to be used in the task domain. This method was examined with a corpus of twenty dialogs. Correct extraction rates were 82% for the topic and 80% for the dialog act.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-515"
  },
  "turunen01_eurospeech": {
   "authors": [
    [
     "Markku",
     "Turunen"
    ],
    [
     "Jaakko",
     "Hakulinen"
    ]
   ],
   "title": "Agent-based error handling in spoken dialogue systems",
   "original": "e01_2189",
   "page_count": 4,
   "order": 524,
   "p1": "2189",
   "pn": "2192",
   "abstract": [
    "In this paper, we introduce an agent-based error handling architecture for spoken dialogue systems. In this architecture, all the parts of the error-handling process on the different interaction levels (input, dialogue and output) are explicitly modeled. Error handling is divided into individual, preferably application independent components. The proposed architecture makes it possible to construct adaptive and reusable error handling components and entire error-handling toolkits. The architecture is especially suitable for multilingual applications. The architecture is implemented as part of the Jaspis speech application development environment and it uses Jaspis' agent-based interaction model.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-516"
  },
  "degerstedt01_eurospeech": {
   "authors": [
    [
     "Lars",
     "Degerstedt"
    ],
    [
     "Arne",
     "Jönsson"
    ]
   ],
   "title": "Iterative implementation of dialogue system modules",
   "original": "e01_2193",
   "page_count": 4,
   "order": 525,
   "p1": "2193",
   "pn": "2196",
   "abstract": [
    "This paper presents an approach to the implementation of modules for dialogue systems. The implementation method is divided into two distinct, but correlated, steps; Conceptual design and Framework customisation. Conceptual design and framework customisation are two mutually dependent sides of the same phenomena, where the former is an on-paper activity that results in a design document and the latter results in the actual implementation. The method is iterative and applicable in various phases of dialogue system development and also for different dialogue system modules. We also present the development of the dialogue management module in more detail. The development space for such modules involves issues on modularisation, knowledge representation and interface functionality internally, and between modules. Orthogonal to this are the various types of re-use for framework customisation; tools, framework template and code patterns. Taken together they form a scheme which is explored during the implementation process.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-517"
  },
  "oppermann01_eurospeech": {
   "authors": [
    [
     "Daniela",
     "Oppermann"
    ],
    [
     "Florian",
     "Schiel"
    ],
    [
     "Silke",
     "Steininger"
    ],
    [
     "Nicole",
     "Beringer"
    ]
   ],
   "title": "Off-talk - a problem for human-machine-interaction?",
   "original": "e01_2197",
   "page_count": 4,
   "order": 526,
   "p1": "2197",
   "pn": "2200",
   "abstract": [
    "This paper is concerned with the definition and description of the phenomenon Off-Talk in human-machine-interaction. This phenomenon is considered to cause problems due to non-relevant information that is conveyed within these utterances. Besides the definition of Off-Talk our work aims to provide an analysis of transcribed audio data that is part of the SmartKom data collection. In the search for features that could indicate the occurrence of Off-Talk we looked at several speech levels e.g. acoustics, lexicon and prosody. Due to the small amount of available data only three features were examined, as there are: loudness, word frequency and filled pauses. The analysis revealed that a correlation might exist between Off-Talk and all features, so that they may serve as indicators for this phenomenon.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-518"
  },
  "schwarz01_eurospeech": {
   "authors": [
    [
     "Jana",
     "Schwarz"
    ],
    [
     "Vaclav",
     "Matousek"
    ]
   ],
   "title": "Automatic analysis of real dialogues and generating of training corpora",
   "original": "e01_2201",
   "page_count": 4,
   "order": 527,
   "p1": "2201",
   "pn": "2204",
   "abstract": [
    "The development of computerized information retrieval dialogue systems communicating with the user in natural language requires the implementation of an effective training procedure with the aid of which the main modules of the dialogue system have to be partly automatically developed. The presented paper describes an attempt to create the generating sentence templates automatically, using a special program package implementing an especially developed method of a quantitative linguistic analysis of transcribed real dialogues. Firstly, the program package generates a set of formulas (templates) consisting of a special grammar and describing the syntactic structure of required sentences. Secondly, it generates a large corpus of unique training sentences using the sentence templates and a stochastic context-free grammar. The experimentally created corpus was used for the training of modules of a city information dialogue system.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-519"
  },
  "macherey01b_eurospeech": {
   "authors": [
    [
     "Klaus",
     "Macherey"
    ],
    [
     "Franz Josef",
     "Och"
    ],
    [
     "Hermann",
     "Ney"
    ]
   ],
   "title": "Natural language understanding using statistical machine translation",
   "original": "e01_2205",
   "page_count": 4,
   "order": 528,
   "p1": "2205",
   "pn": "2208",
   "abstract": [
    "Over the past years, automatic dialogue systems have received increasing attention. In addition to a speech recognizer, such systems include a natural language understanding (NLU) component. One of the most investigated approaches to NLU are rule-based methods as stochastic grammars which are often written manually. However, the sole usage of rule-based methods can turn out to be inflexible when extending or changing the application's domain. Therefore, techniques are desirable which help to reduce the manual effort when creating an NLU component for a new domain. In this paper we investigate an approach to NLU which is derived from the field of statistical machine translation. Starting from a bilingual annotated corpus, we describe the problem of NLU as the translation from a source to a target sentence. Experiments were performed on the TABA corpus which is a text corpus in the domain of a German train timetable information system.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-520"
  },
  "zhang01f_eurospeech": {
   "authors": [
    [
     "Jianping",
     "Zhang"
    ],
    [
     "Wayne",
     "Ward"
    ],
    [
     "Bryan",
     "Pellom"
    ],
    [
     "Xiuyang",
     "Yu"
    ],
    [
     "Kadri",
     "Hacioglu"
    ]
   ],
   "title": "Improvements in audio processing and language modeling in the CU communicator",
   "original": "e01_2209",
   "page_count": 4,
   "order": 529,
   "p1": "2209",
   "pn": "2212",
   "abstract": [
    "This paper presents some up-to-date audio processing techniques which have been developed and integrated into the University of Colorado (CU) communicator system. The CU Communicator is an interactive human-machine dialogue system for airline, hotel and rental car information. The baseline system was fully functional in June 1999. Since then, many improvements have been made. The paper will concentrate on acoustic echo cancellation, voice activity detection (VAD) and language modeling techniques and provide a paradigm for speech and audio processing in a dialog system with barge-in capabilities. Specifically, a real-time block least-mean-square (LMS) algorithm is discussed. A robust voice activity detector using energy threshold is applied to detect user voice. Experimental results are presented and some real-time implementation issues are addressed.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-521"
  },
  "tsai01d_eurospeech": {
   "authors": [
    [
     "Augustine",
     "Tsai"
    ],
    [
     "Andrew N.",
     "Pargellis"
    ],
    [
     "Chin-Hui",
     "Lee"
    ],
    [
     "Joseph P.",
     "Olive"
    ]
   ],
   "title": "Dialogue session: management using voiceXML",
   "original": "e01_2213",
   "page_count": 4,
   "order": 530,
   "p1": "2213",
   "pn": "2216",
   "abstract": [
    "A spoken dialogue system capable of maintaining a human-machine conversation over a telephone must simultaneously maintain many dialogue sessions with different callers and third-party servers running applications over the Internet. There are three main issues to be addressed. First, the caller's identity has to be passed between each dialogue turn. Secondly, the dialogue state and connections to third party servers have to be continuously maintained. Thirdly, the system interfaces have to follow some standards, especially Internet protocols. We use session tickets and session object hashes to address these issues. Each dialogue turn is encoded in a session data object and VXML page. The session tickets contain the session ID and security-related information passed between the VoiceXML interpreter and document server. The session object hash stores the third party connection handle, which is retrieved for subsequent dialogue turns.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-522"
  },
  "ammicht01_eurospeech": {
   "authors": [
    [
     "Egbert",
     "Ammicht"
    ],
    [
     "Alexandros",
     "Potamianos"
    ],
    [
     "Eric",
     "Fosler-Lussier"
    ]
   ],
   "title": "Ambiguity representation and resolution in spoken dialogue systems",
   "original": "e01_2217",
   "page_count": 4,
   "order": 531,
   "p1": "2217",
   "pn": "2220",
   "abstract": [
    "Spoken natural language often contains ambiguities that must be addressed by a spoken dialogue system. In this work, we present the internal semantic representation and resolution strategy of a dialogue system designed to understand ambiguous input. These mechanisms are domain independent; task-specific knowledge is represented in parameterizable data structures. Speech input is processed through the speech recognizer, parser, interpreter, context tracker, pragmatic analyzer and pragmatic scorer. The context tracker combines dialogue context and parser output to yield raw attribute-value (AV) pairs from which candidate values are derived. The pragmatic analyzer adjusts the confidence associated with each AV candidate based on system intent, e.g., implicit confirmation, and on user input. Pragmatic confidence scores are introduced to measure the dialogue managers confidence for each AV: MYCIN-like scoring is used to merge multiple information sources. Pragmatic analysis and scoring is combined with explicit error correction capabilities to achieve efficient ambiguity resolution. The proposed strategies greatly improve dialogue interaction, eliminating about half of the errors in dialogues from a travel reservation task.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-523"
  },
  "popovici01_eurospeech": {
   "authors": [
    [
     "C.",
     "Popovici"
    ],
    [
     "M.",
     "Andorno"
    ],
    [
     "P.",
     "Laface"
    ],
    [
     "L.",
     "Fissore"
    ],
    [
     "M.",
     "Nigra"
    ],
    [
     "C.",
     "Vair"
    ]
   ],
   "title": "Learning of user formulations for business listings in automatic directory assistance",
   "original": "e01_2325",
   "page_count": 4,
   "order": 532,
   "p1": "2325",
   "pn": "2328",
   "abstract": [
    "Automatic Directory Assistance (DA) for business listings poses many application specific problems. One of the main problem is that customers formulate their requests for the same listing with great variability. We present the results of a study aiming at automatic learning, from field data, of expressions typically used by customers to formulate their requests for the most frequent business listings. We use a clustering procedure that exploits the association of the phonetic string produced by a lexical unconstrained search for a given denomination pronounced by the user and the phone number provided by the system or by the human operator, in case of failure of the automatic DA service. We show that an unsupervised approach allows to detect user formulations that were not foreseen by the designers, and that can be added, as variants, to the denominations already included in the system to reduce its failures.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-524"
  },
  "louloudis01_eurospeech": {
   "authors": [
    [
     "D.",
     "Louloudis"
    ],
    [
     "A.",
     "Tsopanoglou"
    ],
    [
     "Nikos",
     "Fakotakis"
    ],
    [
     "George",
     "Kokkinakis"
    ]
   ],
   "title": "Mathematical modeling of spoken human - machine dialogues including erroneous confirmations",
   "original": "e01_2329",
   "page_count": 4,
   "order": 533,
   "p1": "2329",
   "pn": "2332",
   "abstract": [
    "In this paper, we present a method that enables the designer to investigate a spoken dialogue systems performance by employing diagnostic evaluation during the initial phases of the systems development. Results from glass box assessment (e.g.. recognition success rate), combined with the dialogue strategy in the proposed mathematical model, can be used to predict the systems performance. The model incorporates erroneous confirmations by the user, which affect the overall performance.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-525"
  },
  "lewin01_eurospeech": {
   "authors": [
    [
     "Ian",
     "Lewin"
    ]
   ],
   "title": "Limited enquiry negotiation dialogues",
   "original": "e01_2333",
   "page_count": 4,
   "order": 534,
   "p1": "2333",
   "pn": "2336",
   "abstract": [
    "We define a new dialogue management strategy Limited Enquiry Negotiation Dialogues designed for enabling simple man-machine dialogues in which the parameters (for which the user will supply values) of a query to a database are negotiated. The choice of which query to make next is also not pre-ordained. The strategy is simple and intuitive but permits interestingly complex dialogue behaviour. We propose it as an addition to a dialogue designers standard components toolbox along with other well-known ideas such as menu-traversal and slot-filling. We illustrate the strategy by examining how it accounts for interesting but by no means rare data in a Wizard of Oz corpus of business trip planning dialogues. Finally, we discuss some more theoretical issues arising from the model.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-526"
  },
  "cox01_eurospeech": {
   "authors": [
    [
     "Stephen",
     "Cox"
    ],
    [
     "Ben",
     "Shahshahani"
    ]
   ],
   "title": "A comparison of some different techniques for vector based call-routing",
   "original": "e01_2337",
   "page_count": 4,
   "order": 535,
   "p1": "2337",
   "pn": "2340",
   "abstract": [
    "Two approaches to vector-based call-routing are described, one based on matching queries to routes and the other on matching queries directly to stored queries. We argue that there are some problems with the former approach, both when used directly and when latent semantic analysis (LSA) is used to reduce the dimensionality of the vectors. However, the second approach imposes a higher computational load than the first and we have experimented with reducing the number of reference vectors (using the multi-edit and condense algorithm) and the dimensionality of the vectors (using linear discriminant analysis (LDA)). Results are presented for the task of routing queries on banking and financial services to one of thirty-two destinations. Best results (5.1% routing error) were obtained by first using LSA to smooth the query vectors followed by LDA to increase discrimination and reduce vector dimensionality.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-527"
  },
  "niklfeld01_eurospeech": {
   "authors": [
    [
     "Georg",
     "Niklfeld"
    ],
    [
     "Robert",
     "Finan"
    ],
    [
     "Michael",
     "Pucher"
    ]
   ],
   "title": "Architecture for adaptive multimodal dialog systems based on voiceXML",
   "original": "e01_2341",
   "page_count": 4,
   "order": 536,
   "p1": "2341",
   "pn": "2344",
   "abstract": [
    "This paper describes application oriented research on architectural building blocks and constraints for adaptive multimodal dialog systems that use VoiceXML as a component technology. The VoiceXML standard is well supported and promises to make the development of speech-enabled applications so easy that everyone with general web programming skills can accomplish it. The paper proposes a software architecture for multimodal interfaces that emphasizes modularity, in order to strengthen this potential by clearly separating different types of development tasks in a multimodal dialog system. The paper argues that adaptivity is a central design concern for multimodal dialog systems, but that adaptivity is not facilitated by the current VoiceXML standard. This and other examined limitations of VoiceXML for building multimodal dialog systems should be repaired in upcoming work on a successor standard that will explicitly target multimodal applications.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-528"
  },
  "tsuzaki01_eurospeech": {
   "authors": [
    [
     "Minoru",
     "Tsuzaki"
    ]
   ],
   "title": "Feature extraction by auditory modeling for unit selection in concatenative speech synthesis",
   "original": "e01_2223",
   "page_count": 4,
   "order": 537,
   "p1": "2223",
   "pn": "2226",
   "abstract": [
    "A comprehensive computational model of the human auditory peripherals was applied to extract basic features of speech sounds. The auditory model extracts features by the auditory temporal coding mechanism in addition to features by the auditory place coding mechanism which has traditionally been used as spectral features. It also considers the nonlinearity of human auditory responses. Several speech databases of different talkers for a concatenative synthesis system were analyzed by the auditory model, and segmental characteristics were estimated by calculating the averages, standard deviations, and trends of individual feature parameters. The results were compared with results obtained by a physical model. A preliminary perceptual test suggested an advantage of auditory-based distances over physical distances.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-529"
  },
  "lee01f_eurospeech": {
   "authors": [
    [
     "Minkyu",
     "Lee"
    ]
   ],
   "title": "Perceptual cost functions for unit searching in large corpus-based text-to-speech",
   "original": "e01_2227",
   "page_count": 4,
   "order": 538,
   "p1": "2227",
   "pn": "2230",
   "abstract": [
    "In large corpus-based concatenative Text-to-Speech, unit selection is critical for the quality of synthetic speech. Dynamic programming algorithms have been used for unit-searching by minimizing a total cost (1) between target specification and candidate units and (2) between candidate units for concatenation. The cost function is often a weighted sum of sub-costs, which are the costs for each of the acoustic and phonetic features of units. The weights control the individual contribution of the sub-costs to the total cost. They also determine the relative sensitivity of a feature to the quality degradation when signal processing is applied to modify the feature. However, determining the weights for the cost function has not been a simple task. In this paper, we propose a new method for unit-searching based on a perceptual preference test. The proposed algorithm is designed to find the weights in more systematic and meaningful way. The algorithm searches for a set of weights that can produce a ranking of renditions, that is close to the perceptual test results. The downhill simplex method is used for the multi-dimensional search of the weights. A dissimilarity measure is proposed to evaluate the closeness of two rankings. About 83 percent of the cases, the unit selection algorithm using the optimal set of weights choose the same rendition that human listeners prefer. The results show that the proposed weight optimization algorithm can successfully predict the human preference pattern. The synthetic speech using the optimal weights consistantly showed smoother transition and higher voice quality than the one using manually determined weights.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-530"
  },
  "kim01f_eurospeech": {
   "authors": [
    [
     "Sanghun",
     "Kim"
    ],
    [
     "Youngjik",
     "Lee"
    ],
    [
     "Keikichi",
     "Hirose"
    ]
   ],
   "title": "Pruning of redundant synthesis instances based on weighted vector quantization",
   "original": "e01_2231",
   "page_count": 4,
   "order": 539,
   "p1": "2231",
   "pn": "2234",
   "abstract": [
    "A new method of pruning redundant synthesis unit instances in a largescale synthesis database was proposed based on weighted vector quantization (WVQ). WVQ takes relative importance of each instance into account when clustering the similar instances using vector quantization (VQ) technique. The proposed method was compared with two conventional pruning methods through objective and subjective evaluations of synthetic speech quality: one to simply limit maximum number of instance, and the other based on normal VQ-based clustering. For the same reduction rate of instance number, the proposed method showed the best performance. The synthetic speech with reduction rate 50% sounded with no perceptible degradation as compared to that without instance reduction.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-531"
  },
  "fitt01b_eurospeech": {
   "authors": [
    [
     "Susan",
     "Fitt"
    ]
   ],
   "title": "Using real words for recording diphones",
   "original": "e01_2235",
   "page_count": 4,
   "order": 540,
   "p1": "2235",
   "pn": "2238",
   "abstract": [
    "This paper focuses on the creation of word-lists for making diphone recordings for speech synthesis. Such lists often consist of nonsense words, which has the advantage that the phonetic environment can be constrained, and it is easy to produce lists containing all possible combinations. However, this approach has the disadvantage that nonexperts may find it difficult to read the nonsense-word transcriptions. For this reason, we investigate here the issues associated with the use of real words in creating diphone recordings.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-532"
  },
  "dines01_eurospeech": {
   "authors": [
    [
     "John",
     "Dines"
    ],
    [
     "Sridha",
     "Sridharan"
    ],
    [
     "Miles",
     "Moody"
    ]
   ],
   "title": "Application of the trended hidden Markov model to speech synthesis",
   "original": "e01_2239",
   "page_count": 4,
   "order": 541,
   "p1": "2239",
   "pn": "2242",
   "abstract": [
    "This paper presents our work on a speech synthesis system that utilises the trended Hidden Markov Model to represent the basic synthesis unit. We draw upon both speech recognition and speech synthesis research to develop a system that is able to synthesise intelligible and natural sounding speech. Acoustic units are clustered using the decision tree technique and speech data corresponding to these clusters is used for the training of trended Hidden Markov Model synthesis units. The overall system has been implemented in a PSOLA synthesiser and the resultant speech has been compared with that produced by a conventional diphone synthesiser to yield very encouraging results.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-533"
  },
  "sandri01_eurospeech": {
   "authors": [
    [
     "Stefano",
     "Sandri"
    ],
    [
     "Enrico",
     "Zovato"
    ]
   ],
   "title": "Two features to check phonetic transcriptions in text to speech systems",
   "original": "e01_2243",
   "page_count": 4,
   "order": 542,
   "p1": "2243",
   "pn": "2246",
   "abstract": [
    "The paper describes a framework to overcome some problems in the analysis of speech corpora used in text-to-speech systems. In particular two kinds of errors that can produce disagreeable effect at synthesis level have been examined. The first of them is the incorrect transcription of pauses (and more generally low energy intervals) and the second one is the mismatch between voiced intervals and the phonetic symbol that should represent them. For the first problem a statistical approach has been used, by comparing some features of the detected low energy intervals (LE) with those of trained data. The second problem has been faced extracting the voiced/unvoiced intervals (VU) and checking the coherence with the phonetic transcription and segmentation.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-534"
  },
  "xydas01_eurospeech": {
   "authors": [
    [
     "Gerasimos",
     "Xydas"
    ],
    [
     "Georgios",
     "Kouroupetroglou"
    ]
   ],
   "title": "Text-to-speech scripting interface for appropriate vocalisation of e-texts",
   "original": "e01_2247",
   "page_count": 4,
   "order": 543,
   "p1": "2247",
   "pn": "2250",
   "abstract": [
    "Electronic texts carry important meta-information (such as tags in HTML) that most of the current Text-to-Speech (TtS) systems ignore during the production of the speech. We propose an approach to exploit this meta-information in order to achieve a detailed auditory representation of an e-text. The e-Text to Speech and Audio (e-TSA) Composer has been designed and developed as an XML based scripting framework that can be adopted by existing TtS, with minor or major modifications. It provides a mechanism to create scripts using combined elements from e-texts and TtS systems. The e-TSA Composer can manipulate the behaviour of a TtS (e.g. the applied prosody) in order to define a finest vocalisation in response to specific e-texts.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-535"
  },
  "rojc01_eurospeech": {
   "authors": [
    [
     "Matej",
     "Rojc"
    ],
    [
     "Zdravko",
     "Kacic"
    ]
   ],
   "title": "Representation of large lexica using finite-state transducers for the multilingual text-to-speech synthesis systems",
   "original": "e01_2251",
   "page_count": 4,
   "order": 544,
   "p1": "2251",
   "pn": "2254",
   "abstract": [
    "Large external language resources used for multilingual text processing in TTS systems represent a big problem because of needed space and slow look-up time. Representation of large lexica using finite-state transducers is mainly motivated by considerations of space and time efficiency. In the paper we present a method and results of compiling large German phonetic and morphology lexica (CISLEX) [4] into corresponding finite-state transducers (FSTs), both with about 300.000 words. For both lexica a great reduction in size and optimal access time was achieved. The starting size for German phonetic lexicon was 12.526 MB and 18.49 MB for morphology lexicon. The final size of the corresponding FST was only 2.78 MB for the phonetic lexicon and 6.33 MB for the morphology lexicon. At the same time the look-up time is optimal, since it depends only on the length of the input word and not on the size of the lexicon.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-536"
  },
  "hirose01_eurospeech": {
   "authors": [
    [
     "Keikichi",
     "Hirose"
    ],
    [
     "Masaya",
     "Eto"
    ],
    [
     "Nobuaki",
     "Minematsu"
    ],
    [
     "Atsuhiro",
     "Sakurai"
    ]
   ],
   "title": "Corpus-based synthesis of fundamental frequency contours based on a generation process model",
   "original": "e01_2255",
   "page_count": 4,
   "order": 545,
   "p1": "2255",
   "pn": "2258",
   "abstract": [
    "A mode-constrained corpus-based synthesis strategy was developed for F0 contours of Japanese sentences. In the training phase, the relationship between linguistic factors and the command values of F0 contour generation process model was learned using neural networks. Input parameters consist of linguistic information related to accentual phrases that can be automatically driven from text, such as the number of morae, and so on. In the synthesis phase, the network is used to generate the command values. The synthesis method was also realized based on multiple linear regression analysis to examine how each input parameter contributes to the F0 contour generation. The use of the parametric model restricts the degrees of freedom of the mapping between linguistic and prosodic features, and thus enables to generate appropriate values even with limited training data. Experimental results showed that the method could generate F0 contours quite close to those by the rule-based method.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-537"
  },
  "tychtl01_eurospeech": {
   "authors": [
    [
     "Zbyn.ek",
     "Tychtl"
    ],
    [
     "Josef",
     "Psutka"
    ]
   ],
   "title": "Corpus-based database of residual excitations used for speech reconstruction from MFCCs",
   "original": "e01_2259",
   "page_count": 4,
   "order": 546,
   "p1": "2259",
   "pn": "2262",
   "abstract": [
    "This paper proposes a new approach to extraction of a corpus-based database of residual signal segments that are used as excitations of a production model to replay MFCC encoded speech signal with natural sound. Neither extra information besides the MFCCs (like F0, voiced/unvoiced flag etc.) nor modification and/or extension of a MFCC computation algorithm is needed. The MFCC algorithm is considered to be in a commonly accepted form that was implemented for example in the HTK software. Because of mentioned restrictions we don't aim to achieve exact reconstruction of original signal but we seek to replay the speech signal in an intelligible and as natural as possible way. Moreover, the 'low-demanding' solution based on pulse/noise excitation is offered that employs a new method for making voiced/unvoiced decision using the MFCC vector only.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-538"
  },
  "yoshimura01_eurospeech": {
   "authors": [
    [
     "Takayoshi",
     "Yoshimura"
    ],
    [
     "Keiichi",
     "Tokuda"
    ],
    [
     "Takashi",
     "Masuko"
    ],
    [
     "Takao",
     "Kobayashi"
    ],
    [
     "Tadashi",
     "Kitamura"
    ]
   ],
   "title": "Mixed excitation for HMM-based speech synthesis",
   "original": "e01_2263",
   "page_count": 4,
   "order": 547,
   "p1": "2263",
   "pn": "2266",
   "abstract": [
    "This paper describes improvements on the excitation model of an HMM-based text-to-speech system. In our previous work, natural spectral and pitch parameters have been generated from HMM by using a speech parameter generation algorithm. However, synthesized speech has a typical quality of ``vocoded speech'' since the system used a traditional excitation model with either a periodic impulse train or white noise. In this paper, in order to reduce the synthetic quality, a mixed excitation model used in MELP is incorporated into the system. Excitation parameters used in mixed excitation are modeled by HMMs, and generated from HMMs by a parameter generation algorithm in the synthesis phase. The result of a listening test shows that the mixed excitation model significantly improves quality of synthesized speech as compared with the traditional excitation model.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-539"
  },
  "ohtsuka01_eurospeech": {
   "authors": [
    [
     "Takahiro",
     "Ohtsuka"
    ],
    [
     "Hideki",
     "Kasuya"
    ]
   ],
   "title": "Aperiodicity control in ARX-based speech analysis-synthesis method",
   "original": "e01_2267",
   "page_count": 4,
   "order": 548,
   "p1": "2267",
   "pn": "2270",
   "abstract": [
    "We present an improved algorithm for a robust speech analysis-synthesis method based on an auto-regressive with exogenous input (ARX) speech production model proposed previously. The speech analysis-synthesis method is capable of making an automatic estimation of vocal tract (formant) and voice source parameters from a speech utterance, generating accurate formant values even for very high-pitched voices. The improved algorithm presented in this paper incorporates aperiodic components included in the voice source signal, taking the dynamic nature of the speech production process into account. Perceptual experiments show that implementation of the aperiodic components in the analysis-synthesis is very effective in improving the perceived quality of synthetic speech, particularly for soft voices, typical of female voice quality.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-540"
  },
  "karjalainen01_eurospeech": {
   "authors": [
    [
     "Matti",
     "Karjalainen"
    ],
    [
     "Tuomas",
     "Paatero"
    ]
   ],
   "title": "Generalized source-filter structures for speech synthesis",
   "original": "e01_2271",
   "page_count": 4,
   "order": 549,
   "p1": "2271",
   "pn": "2274",
   "abstract": [
    "In this paper we discuss various digital filter principles as models for synthetic speech generation. Warped linear prediction (WLP) and frequency-warped filters have been introduced earlier as a method to reduce the filter order in high-quality wideband speech synthesis. In addition to analyzing WLP and frequency-warped filters we introduce new related structures and techniques for arbitrary frequency resolution allocation. Kautz filters can be considered as generalized structures for pole-zero modeling. This study focuses on residual-excited synthesis and diphone-oriented reconstruction of speech signals. Control strategies for text-to-speech synthesis are discussed briefly.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-541"
  },
  "wypych01_eurospeech": {
   "authors": [
    [
     "Mikolaj",
     "Wypych"
    ]
   ],
   "title": "The speech synthesis environment and parametric modeling of coarticulation",
   "original": "e01_2275",
   "page_count": 3,
   "order": 550,
   "p1": "2275",
   "pn": "2278",
   "abstract": [
    "A general description of the environment for speech processing called SLOPE is presented. The main area of application of the SLOPE environment is mid and low-level speech synthesis - the area between prosody modeling and a speech waveform generation. The final part of the article describes the idea of obtaining a strict parametric form of the utterance from the string of phones and prosodic information implemented on the basis of SLOPE\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-542"
  },
  "carsonberndsen01_eurospeech": {
   "authors": [
    [
     "Julie",
     "Carson-Berndsen"
    ],
    [
     "Michael",
     "Walsh"
    ]
   ],
   "title": "Defining constraints for multilinear speech processing",
   "original": "e01_2281",
   "page_count": 4,
   "order": 551,
   "p1": "2281",
   "pn": "2284",
   "abstract": [
    "This paper presents a constraint model for the interpretation of multilinear representations of speech utterances which can provide important fine-grained information for speech recognition applications. The model uses explicit structural constraints specifying overlap and precedence relations between features in both the phonological and the phonetic domains in order to recognise well-formed syllable structures. In the phonological domain, these constraints together form a complete phonotactic description of the language, while in the phonetic domain, the constraints define the internal structure of phonologial features based on phonetic realisations. The constraints are enhanced by a constraint relaxation procedure to cater for underspecified input and allows output representations to be extrapolated based on the phonetic and phonological information contained in the constraints and the rankings which have been assigned to them. This approach thus addresses issues of robustness in speech recognition.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-543"
  },
  "batliner01_eurospeech": {
   "authors": [
    [
     "Anton",
     "Batliner"
    ],
    [
     "Bernd",
     "Möbius"
    ],
    [
     "Gregor",
     "Möhler"
    ],
    [
     "Antje",
     "Schweitzer"
    ],
    [
     "Elmar",
     "Nöth"
    ]
   ],
   "title": "Prosodic models, automatic speech understanding, and speech synthesis: towards the common ground",
   "original": "e01_2285",
   "page_count": 4,
   "order": 552,
   "p1": "2285",
   "pn": "2288",
   "abstract": [
    "Automatic speech understanding and speech synthesis, two of the major speech processing applications, impose strikingly different constraints and requirements on prosodic models. The prevalent models of prosody and intonation fail to offer a unified solution to these conflicting constraints. As a consequence, prosodic models have been applied only occasionally in end-to-end automatic speech understanding systems; in contrast, they have been applied extensively in speech synthesis systems. In this paper we want to discuss the reasons for this state of affairs as well as possible strategies to overcome the shortcomings of the use of prosodic modelling in automatic speech processing.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-544"
  },
  "christensenyz01_eurospeech": {
   "authors": [
    [
     "Heidi",
     "Christensenyz"
    ],
    [
     "Børge",
     "Lindbergy"
    ],
    [
     "Ove",
     "Anderseny"
    ]
   ],
   "title": "Introducing phonetically motivated information into ASR",
   "original": "e01_2289",
   "page_count": 4,
   "order": 553,
   "p1": "2289",
   "pn": "2292",
   "abstract": [
    "In this paper we present an approach to introducing more phonetically motivated information into automatic speech recognition in the form of a phonetic `expert'. To avoid the curse of dimensionality problem, the expert information is introduced at the level of the acoustic model. Two types of experts are used each providing discriminative information regarding groups of phonetically related phonemes. The phonetic expert is implemented using an MLP. A numbers recognition task shows that, when using the expert in conjunction with both a fullband and a multiband system speech recognition performances are increased.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-545"
  },
  "gravier01_eurospeech": {
   "authors": [
    [
     "Guillaume",
     "Gravier"
    ],
    [
     "Francois",
     "Yvon"
    ],
    [
     "Bruno",
     "Jacob"
    ],
    [
     "Frédéric",
     "Bimbot"
    ]
   ],
   "title": "Integrating contextual phonological rules in a large vocabulary decoder",
   "original": "e01_2293",
   "page_count": 4,
   "order": 554,
   "p1": "2293",
   "pn": "2296",
   "abstract": [
    "This paper presents an approach to the integratation of contextual phonological rules in the beam-search algorithm of a large vocabulary speech recognition system. The main interest of contextual transcription rules is that they implement constraints on pronunciations sequences which complement the bigram constraints on word sequences. As such, they should help avoiding acoustic confusions and reduce the search space. In our approach, contextual transcription do not incur any augmentation of the lexicon size. This approach is evaluated on a dictation task in French for two different sets of contextual phonological rules. Our results show that, given the current resources, the introduction of contextual rule deteriorates the recognition rate. We discuss the possible factors explaining this surprising result and outline the problems of defining a set of contextual phonological rules and integrating them in the search algorithm.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-546"
  },
  "pastorigadea01_eurospeech": {
   "authors": [
    [
     "M.",
     "Pastor-i-Gadea"
    ],
    [
     "F.",
     "Casacuberta"
    ]
   ],
   "title": "Automatic learning of finite state automata for pronunciation modeling",
   "original": "e01_2297",
   "page_count": 4,
   "order": 555,
   "p1": "2297",
   "pn": "2300",
   "abstract": [
    "The great variability of word pronunciations in spontaneous speech is one of the reasons for the low performance of present speech recognition systems. The generation of dictionaries that take into account this variability can increase the robustness of such systems. A word pronunciation is a possible phone sequence that can appear in a real utterance, and represents a possible acoustic realization of the word. Here, word pronunciations are modeled using finite state automata. The use of such models allow for the application of grammatical inference methods and an easy integration with the others sources of acknowledge. The training samples are obtained from the alignment between the phone decodification of each training utterance and the corresponding canonical transcription. Models proposed in this work were applied in a translation-oriented speech task. The improvements achieved by these models were in the range between 2.7 to 0.6 points depending on the language model used.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-547"
  },
  "rotolapukkila01_eurospeech": {
   "authors": [
    [
     "J.",
     "Rotola-Pukkila"
    ],
    [
     "J.",
     "Vainio"
    ],
    [
     "H.",
     "Mikkola"
    ],
    [
     "K.",
     "Järvinen"
    ],
    [
     "B.",
     "Bessette"
    ],
    [
     "Roch",
     "Lefebvre"
    ],
    [
     "R.",
     "Salami"
    ],
    [
     "M.",
     "Jeline"
    ]
   ],
   "title": "AMR wideband codec - leap in mobile communication voice quality",
   "original": "e01_2303",
   "page_count": 4,
   "order": 556,
   "p1": "2303",
   "pn": "2306",
   "abstract": [
    "The Third Generation Partnership Project (3GPP) and European Telecommunications Standards Institute (ETSI) have carried out development and standardisation of a wideband speech codec for GSM and the third generation mobile communication WCDMA system since 1999. The Adaptive Multi-Rate Wideband (AMR-WB) codec algorithm was selected in December 2000, and the corresponding specifications were approved in March 2001. The AMR-WB codec was jointly developed by Nokia and VoiceAge. AMR-WB extends the audio bandwidth from 3.4 kHz to 7 kHz and gives superior speech quality and voice naturalness compared to existing 2nd and 3rd generation mobile communication systems. The wideband speech service provided by the AMR-WB codec will give mobile communication speech quality that even exceeds (narrowband) wireline quality.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-548"
  },
  "farrugia01_eurospeech": {
   "authors": [
    [
     "Maria",
     "Farrugia"
    ],
    [
     "Ahmet M.",
     "Kondoz"
    ]
   ],
   "title": "Combined speech and audio coding with bit rate and bandwidth scalability",
   "original": "e01_2307",
   "page_count": 4,
   "order": 557,
   "p1": "2307",
   "pn": "2310",
   "abstract": [
    "The growing demand for streaming multimedia services over the Internet and recently also over mobile networks has initiated a great interest in coding algorithms which are able to adapt to different transmission environments and to operate under multiple constraints of bit rate, complexity, delay, robustness to bit errors and diversity of input signals. In the light of these recent developments, we present a novel scalable representation for speech and audio signals with low delay. The algorithm operates in four modes, each based on backward-adaptive linear predictive coding (BA LPC). The first mode is referred to as the base-line narrowband (0--4kHz) coder. Wideband speech and audio signals (0--8kHz) are efficiently represented by the second mode which employs a QMF to split the spectrum into two equal bands. The remaining two modes use a two-stage QMF structure to decompose the bandwidth of 32kHz sampled signals into four bands. Scalability is achieved by means of discrete quantisation layers representing various levels of enhancements for each band and also flexibility in terms of complexity and bit allocation requirements depending on the particular application and on the network resources. The resulting bit rates range from 12 to 64kb/s. The performance of the coder is evaluated by comparing it to MPEG and ITU standards.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-549"
  },
  "fek01_eurospeech": {
   "authors": [
    [
     "Mark",
     "Fék"
    ],
    [
     "Annamária R.",
     "Várkonyi-Kóczy"
    ],
    [
     "Jean-Marc",
     "Boucher"
    ]
   ],
   "title": "Joint speech and audio coding combining sinusoidal modeling and wavelet packets",
   "original": "e01_2311",
   "page_count": 4,
   "order": 558,
   "p1": "2311",
   "pn": "2314",
   "abstract": [
    "This paper presents a joint speech and audio coding algorithm combining sinusoidal modeling and a perceptually adapted Wavelet Packet Transform (WPT). The input signal is limited to the band of 50-7000 Hz, and sampled at 16 kHz. The sinusoidal modeling uses a Sinusoidal Similarity Measure (SSM) to find stable sinusoidal components. A novel pitch harmonics based encoding is applied to encode the sinusoidal frequencies. The residual is obtained by extracting the re-synthesized sinusoids from the input, and is processed by a WPT simulating the critical bands of the Human Auditory System. Perceptual Noise Substitution (PNS) is applied in noisy WPT sub-bands to reduce the bit rate. The method provides nearly transparent quality for both speech and audio inputs. The mean bit rate of the compressed signal varies between 32-62 kbps depending on the input.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-550"
  },
  "ritz01_eurospeech": {
   "authors": [
    [
     "C. H.",
     "Ritz"
    ],
    [
     "I. S.",
     "Burnett"
    ]
   ],
   "title": "Temporal decomposition: a promising approach to low rate wideband speech compression",
   "original": "e01_2315",
   "page_count": 4,
   "order": 559,
   "p1": "2315",
   "pn": "2318",
   "abstract": [
    "In this paper, we present new results on Temporal Decomposition (TD) applied to the Line Spectral Frequencies (LSFs) derived for wideband speech. The paper shows that by incorporating a dynamic programming search algorithm into TD, near transparent quantisation of wideband LSFs can be obtained at approximately 1 kbps. We also show that TD performs significantly better than Split Vector Quantisation at low bit rates. We propose that TD is a promising approach to low rate wideband speech coding for applications such as unicast streaming.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-551"
  },
  "ragot01_eurospeech": {
   "authors": [
    [
     "Stephane",
     "Ragot"
    ],
    [
     "Hassan",
     "Lahdili"
    ],
    [
     "Roch",
     "Lefebvre"
    ]
   ],
   "title": "Wideband LSF quantization by generalized voronoi codes",
   "original": "e01_2319",
   "page_count": 4,
   "order": 560,
   "p1": "2319",
   "pn": "2322",
   "abstract": [
    "Presented a method for quantizing the wideband line spectrum frequencies (LSF) with a specific class of near-ellipsoidal lattice codes referred to as \"generalized Voronoi codes\". Optimization procedures are described with respect to a weighted mean-square error (WMSE). The lattices D16, RE16 or RLambda16 are applied to quantize the LSF with no frequency splitting. Results indicate that near-ellipsoidal lattice quantization allows to develop efficient one-stage algebraic wideband LSF quantization at competitive bit rates.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-552"
  },
  "rigazio01_eurospeech": {
   "authors": [
    [
     "Luca",
     "Rigazio"
    ],
    [
     "Patrick",
     "Nguyen"
    ],
    [
     "David",
     "Kryze"
    ],
    [
     "Jean-Claude",
     "Junqua"
    ]
   ],
   "title": "Separating speaker and environment variabilities for improved recognition in non-stationary conditions",
   "original": "e01_2347",
   "page_count": 4,
   "order": 561,
   "p1": "2347",
   "pn": "2350",
   "abstract": [
    "In this paper we address the problem of speaker adaptation in noisy environments. We estimate speaker adapted models from noisy data by combining unsupervised speaker adaptation with noise compensation. We aim at using the resulting speaker adapted models in environments that differ from the adaptation environment, without a significant loss in performance. The key idea is to separate speaker and environment variabilities and associate them to independent models. We show that linear models for both speaker and environment are critical for achieving this goal. Experiments for 2000 and 4000 isolated word tasks on real car noise show that unsupervised speaker adaptation combined with noise compensation can provide more than 20% error rate reduction compared with noise compensation only, and more than 50% error rate reduction compared with speaker adaptation only.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-553"
  },
  "rose01_eurospeech": {
   "authors": [
    [
     "Richard C.",
     "Rose"
    ],
    [
     "Hong Kook",
     "Kim"
    ],
    [
     "Don",
     "Hindle"
    ]
   ],
   "title": "Robust speech recognition techniques applied to a speech in noise task",
   "original": "e01_2351",
   "page_count": 4,
   "order": 562,
   "p1": "2351",
   "pn": "2355",
   "abstract": [
    "This paper describes the design and evaluation of an automatic speech recognition (ASR) system on the Naval Research Laboratory Speech In Noise (SPINE) speech corpus. This corpus represents a task which involves human-human interaction on a constrained problem solving scenario under six different simulated noisy environments. Acoustic and language modeling were performed using a dataset taken entirely from a subset of the acoustic environments. Speech recognition was performed on continuous conversations by detecting speech utterances, performing acoustic feature analysis and normalization, and adapting HMM models in multiple passes over each conversation-side. The ASR word accuracy (WAC) ranged from 77 percent in an office environment to 61 percent in conditions that include significant levels of background speech and noise. An overall average WAC of 69.0 percent was obtained across all noise conditions.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-554"
  },
  "afify01b_eurospeech": {
   "authors": [
    [
     "Mohamed",
     "Afify"
    ],
    [
     "Olivier",
     "Siohan"
    ],
    [
     "Chin-Hui",
     "Lee"
    ]
   ],
   "title": "Minimax classification with parametric neighborhoods for noisy speech recognition",
   "original": "e01_2355",
   "page_count": 4,
   "order": 563,
   "p1": "2355",
   "pn": "2358",
   "abstract": [
    "In this paper we derive upper and lower bounds on the mean of speech signals corrupted by additive noise. The bounds are derived in the log spectral domain. Approximate bounds on the first and second order time derivatives are also developed. It is then shown how to transform these bounds to the MFCC domain to be used by conventional cepstrum-based speech recognizers. The proposed bounds define the mismatch neighborhood for minimax classification. Speech recognition experiments, using artificially added noise, and a real-life mismatch scenario, illustrate that this parametric neighborhood works quite well in practice. We also beleive that the proposed bounds will find various applications in noisy speech recognition.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-555"
  },
  "padmanabhan01_eurospeech": {
   "authors": [
    [
     "M.",
     "Padmanabhan"
    ],
    [
     "S.",
     "Dharanipragada"
    ]
   ],
   "title": "Maximum likelihood non-linear transformation for environment adaptation in speech recognition",
   "original": "e01_2359",
   "page_count": 4,
   "order": 564,
   "p1": "2359",
   "pn": "2362",
   "abstract": [
    "In this paper, we describe an adaptation method for speech recognition systems that is based on a piecewise-linear approximation to a non-linear transformation of the feature space. The method extends a previously proposed non-linear transformation (NLT) technique by making the transformation function more sophisticated and by computing the transformation to maximize the likelihood of the adaptation data given its transcription. This method also differs from other linear techniques (such as MLLR, linear feature space transforms, etc.) in two ways - first, the computed transformation is non-linear, second, the tying structure of the transformation depends not on the phonetic class but rather on the location in the feature space. Experimental results show that the method performs well for the case of limited adaptation data, and the performance gains appear to be additive to those provided by MLLR -yielding upto 3.4% relative improvement over MLLR.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-556"
  },
  "turunen01b_eurospeech": {
   "authors": [
    [
     "Jari",
     "Turunen"
    ],
    [
     "Damjan",
     "Vlaj"
    ]
   ],
   "title": "A study of speech coding parameters in speech recognition",
   "original": "e01_2363",
   "page_count": 4,
   "order": 565,
   "p1": "2363",
   "pn": "2366",
   "abstract": [
    "Speech recognition over different transmission channels will set demands to the parametric encoded/decoded speech. The effects of different types of noise have been studied a lot and the effects of the parameterization process in speech has been known to cause degradation in decoded speech when compared to the original speech. But does the encoding/decoding process modify the speech so much that it will cause degradation in the speech recognition result? If it does what may cause the speech recognition degradation? We have studied the effect of the parameterization and the causes of the nine different codec configurations to isolated word recognition.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-557"
  },
  "garciamateo01_eurospeech": {
   "authors": [
    [
     "Carmen",
     "Garcia-Mateo"
    ],
    [
     "Laura",
     "Docio-Fernandez"
    ],
    [
     "Antonio",
     "Cardenal-Lopez"
    ]
   ],
   "title": "Some practical considerations in the deployment of a wireless-communication interactive voice response system",
   "original": "e01_2369",
   "page_count": 4,
   "order": 566,
   "p1": "2369",
   "pn": "2372",
   "abstract": [
    "In this paper, we describe the design procedure for a wireless communication interactive voice response (IVR) system. The application must work in a very noisy environment which has imposed many design constraints. We will address the sensible aspects of three components of the application: the voice activity detector (VAD), the automatic speech recognition (ASR) system, and the confidence measure (CM) determination. In order to get a satisfactory product, it has been necessary to reduce the important mismatch between available linguistic and acoustic resources and the operational environment. Adaptation techniques for the acoustic models of the speech recognition system have proven to be effective to speed up the application deployment time.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-558"
  },
  "rosenberg01_eurospeech": {
   "authors": [
    [
     "Aaron",
     "Rosenberg"
    ],
    [
     "Julia",
     "Hirschberg"
    ],
    [
     "Michiel",
     "Bacchiani"
    ],
    [
     "S.",
     "Parthasarathy"
    ],
    [
     "Philip",
     "Isenhour"
    ],
    [
     "Larry",
     "Stead"
    ]
   ],
   "title": "Caller identification for the SCANMail voicemail browser",
   "original": "e01_2373",
   "page_count": 4,
   "order": 567,
   "p1": "2373",
   "pn": "2376",
   "abstract": [
    "SCANMail is a prototype system developed to provide useful tools for managing and searching through voicemail messages. Content is extracted from voicemail messages using various speech and text processing tools. One such content category is the identity of the message caller. This paper describes CallerID, the sever tool attached to SCANMail to provide caller labels for voicemail messages. CallerID makes use of text independent speaker recognition techniques. Two kinds of requests are handled by the CallerID server. A request triggered by the arrival of a new voicemail message results in the processing of the message to score it agains the models of callers assigned to the user (recipient) in order to propose the identity of the caller. A second request is initiated by a user who provides a caller label for a message he/she has reviewed. CallerID processes the message and uses it to train or adapt a speaker model for the caller.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-559"
  },
  "koumpis01_eurospeech": {
   "authors": [
    [
     "Konstantinos",
     "Koumpis"
    ],
    [
     "Steve",
     "Renals"
    ],
    [
     "Mahesan",
     "Niranjan"
    ]
   ],
   "title": "Extractive summarization of voicemail using lexical and prosodic feature subset selection",
   "original": "e01_2377",
   "page_count": 4,
   "order": 568,
   "p1": "2377",
   "pn": "2380",
   "abstract": [
    "This paper presents a novel data-driven approach to summarizing spoken audio transcripts utilizing lexical and prosodic features. The former are obtained from a speech recognizer and the latter are extracted automatically from speech waveforms. We employ a feature subset selection algorithm, based on ROC curves, which examines different combinations of features at different target operating conditions. The approach is evaluated on the IBM Voicemail corpus, demonstrating that it is possible and desirable to avoid complete commitment to a single best classifier or feature set.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-560"
  },
  "scharenborg01_eurospeech": {
   "authors": [
    [
     "Odette",
     "Scharenborg"
    ],
    [
     "Janienke",
     "Sturm"
    ],
    [
     "Lou",
     "Boves"
    ]
   ],
   "title": "Business listings in automatic directory assistance",
   "original": "e01_2381",
   "page_count": 4,
   "order": 569,
   "p1": "2381",
   "pn": "2384",
   "abstract": [
    "So far most attempts to automate Directory Assistance services focused on private listings, because it is not known precisely how callers will refer to a business listings. The research described in this paper, carried out in the SMADA project, tries to fill this gap. The aim of the research is to model the expressions people use when referring to a business listing by means of rules, in order to automatically create a vocabulary, which can be part of an automated DA service. In this paper a rule-base procedure is proposed, which derives rules from the expressions people use. These rules are then used to automatically create expressions from directory listings. Two categories of businesses, viz. hospitals and the hotel and catering industry, are used to explain this procedure. Results for these two categories are used to discuss the problem of the over- and undergeneration of expressions.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-561"
  },
  "pastorigadea01b_eurospeech": {
   "authors": [
    [
     "M.",
     "Pastor-i-Gadea"
    ],
    [
     "A.",
     "Sanchis"
    ],
    [
     "F.",
     "Casacuberta"
    ],
    [
     "E.",
     "Vidal"
    ]
   ],
   "title": "Eutrans: a speech-to-speech translator prototype",
   "original": "e01_2385",
   "page_count": 4,
   "order": 570,
   "p1": "2385",
   "pn": "2388",
   "abstract": [
    "EuTrans system is a telephone speech input translation prototype capable of translating telephone calls from one language to another. It assumes a human to human communication, each one speaking a different language, assisted by a system with translation capabilities. The prototype has been developed as a demonstrator for the European project with the same name. EuTrans achieves a response time close to real time for speaker-independent, medium complexity tasks (a few thousand words) and offers competitive accuracy. The acoustic, language and translation models are finite-state networks that are automatically learnt form training samples, this makes the system easily adaptable to news tasks. It runs on a standard PC with audio capability and a cheap modem. The system is currently available for two translation tasks: FUB task (Italian-English) and Traveler task (Spanish-English).\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-562"
  },
  "metze01_eurospeech": {
   "authors": [
    [
     "Florian",
     "Metze"
    ],
    [
     "John",
     "McDonough"
    ],
    [
     "Hagen",
     "Soltau"
    ]
   ],
   "title": "Speech recognition over netmeeting connections",
   "original": "e01_2389",
   "page_count": 4,
   "order": 571,
   "p1": "2389",
   "pn": "2392",
   "abstract": [
    "In this paper we evaluate the performance of the ISL's German Verbmobil spontaneous speech recognizer on the Nespole! database. In this task, people talk to an agent in a tourist office to plan their holidays via a NetMeeting connection, also sharing screen contents (web-pages). Stereo recordings were made both before and after speech transmission over an IP connection using the G.711 codec, so that we are able to directly measure the loss in LVCSR performance due to NetMeeting's segmentation and compression. The aim of this work is to quantify this loss, which is a consequence of using protocols which were not designed for speech recognition purposes. We report on techniques employed to port our existing clean-speech recognizer to this new data quality, using about 1.5h of labeled adaptation data, but avoiding a complete retraining of the system.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-563"
  },
  "martin01c_eurospeech": {
   "authors": [
    [
     "Juan C. Díaz",
     "Martín"
    ],
    [
     "Juan L. García",
     "Zapata"
    ],
    [
     "José M. Rodríguez",
     "García"
    ],
    [
     "José F. Álvarez",
     "Salgado"
    ],
    [
     "Pablo Espada",
     "Bueno"
    ],
    [
     "Pedro Gómez",
     "Vilda"
    ]
   ],
   "title": "DIARCA: a component approach to voice recognition",
   "original": "e01_2393",
   "page_count": 4,
   "order": 572,
   "p1": "2393",
   "pn": "2396",
   "abstract": [
    "Current voice recognition systems tend to be implemented as a PC desktop facility. This model is not suitable for the growing complexities of present and future developments: It is single-user, it is non portable, and it assumes the workstation model, where all the CPU resources are supposed to be locally available. This work researches how a high performance speech recognition system can be redesigned and implemented as a time-critical network service shared through ordinary data transmission media with three main design goals: Scalability, predictability and POSIX portability. The whole idea has been tested by rebuilding IVORY, a well known robust desktop voice recognition methodology, as a distributed component.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-564"
  },
  "kyung01_eurospeech": {
   "authors": [
    [
     "Y. J.",
     "Kyung"
    ],
    [
     "J. O.",
     "Jung"
    ],
    [
     "S. M.",
     "Sohn"
    ],
    [
     "H. J.",
     "Chun"
    ],
    [
     "S. Y.",
     "Moon"
    ],
    [
     "M. H.",
     "Kim"
    ],
    [
     "W. H.",
     "Sull"
    ]
   ],
   "title": "The mvprotek : m-commerce voice verification system",
   "original": "e01_2397",
   "page_count": 4,
   "order": 573,
   "p1": "2397",
   "pn": "2400",
   "abstract": [
    "In this paper, we developed speaker verification system for m-commerce (mobile commerce) via wireless internet and WAP. We implemented the system as client-server architecture. The clients are mobile phone simulator and PDA. As the needs for wireless Internet service is increasing, the needs for secure m-commerce is also increasing. Conventional security technique are reinforced by biometric security technique. This paper utilized the voice as biometric security techniques. The verification results are obtained by integrating the mVprotek system with SK Telecom's CDMA system. Utilizing F-Ratio and Virtual cohort model normalization showed much better performance than conventional background model normalization technique.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-565"
  },
  "alm01_eurospeech": {
   "authors": [
    [
     "Norman",
     "Alm"
    ],
    [
     "Mamoru",
     "Iwabuchi"
    ],
    [
     "Peter N.",
     "Andreasen"
    ],
    [
     "Kenryu",
     "Nakamura"
    ],
    [
     "Iain R.",
     "Murray"
    ]
   ],
   "title": "Real-time multilingual communication by means of prestored conversational units",
   "original": "e01_2401",
   "page_count": 4,
   "order": 574,
   "p1": "2401",
   "pn": "2404",
   "abstract": [
    "A computer mediated communication system has been developed which can offer real time multilingual communication, as long as users stay within the boundaries of prestored conversational units. The system was designed originally to give non-speaking people a multi-lingual capability. However, the system could also be used by people whose only communication disadvantage is not being able to speak a foreign language. It is based on research into conversational modelling and utterance prediction, making use of prestored material. In comparison with a multi-lingual phrase book, the system helped users to have more natural conversation, and to take more control of the interaction. This project is an interesting example of the way in which systems developed for people with severe disabilities can often have useful general applications.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-566"
  },
  "murray01_eurospeech": {
   "authors": [
    [
     "Iain R.",
     "Murray"
    ],
    [
     "John L.",
     "Arnott"
    ],
    [
     "Norman",
     "Alm"
    ],
    [
     "Richard",
     "Dye"
    ],
    [
     "Gillian",
     "Harper"
    ]
   ],
   "title": "Writing script-based dialogues for AAC",
   "original": "e01_2405",
   "page_count": 4,
   "order": 575,
   "p1": "2405",
   "pn": "2409",
   "abstract": [
    "AAC (Augmentative and Alternative Communication) devices are often used by disabled people who are non-speaking, in order to assist them to communicate. However, many such systems require the user to build up an utterance word-by-word each time, and are thus often laborious for the user and slow (and thus less effective) in communication. For this reason, an AAC system was developed which relies on pre-stored scripts and an engaging user interface to predict and guide the user through many standard dialogue situations with a minimum of effort. In order to allow individuality in using the system, an authoring package has been developed. This allows users (or their carers) to modify existing scripts or to add new scripts into the system, and has also been designed to facilitate exchange of scripts between users. The systems developed utilise speech synthesisers for output, and are commercially available in English, Dutch and German versions.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-567"
  },
  "iida01_eurospeech": {
   "authors": [
    [
     "Akemi",
     "Iida"
    ],
    [
     "Yosuke",
     "Sakurada"
    ],
    [
     "Nick",
     "Campbell"
    ],
    [
     "Michiaki",
     "Yasumura"
    ]
   ],
   "title": "Communication aid for non-vocal people using corpusbased concatenative speech synthesis",
   "original": "e01_2409",
   "page_count": 4,
   "order": 576,
   "p1": "2409",
   "pn": "2412",
   "abstract": [
    "This paper reports on the development of Chatako-AID, a communication aid for non-vocal people using corpus-based cocatenative speech synthesis by creating a speech corpus especially designed for such use. The concept of Chatako-AID; synthesis with the user's voice, which makes use of precomposed texts, is highly appreciated by the target user. This confirms that the recording of a minimum set of phonetically balanced sentences is insufficient for speech synthesis in the proposed method and that a combination of the above recording and a recording of well-read continuous-text material produces more natural sounded synthesised speech.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-568"
  },
  "suzuki01_eurospeech": {
   "authors": [
    [
     "Noriko",
     "Suzuki"
    ],
    [
     "Kazuhiko",
     "Kakehi"
    ],
    [
     "Yugo",
     "Takeuchi"
    ],
    [
     "Michio",
     "Okada"
    ]
   ],
   "title": "Social effects on vocal rate with echoic mimicry using prosody-only voice",
   "original": "e01_2413",
   "page_count": 4,
   "order": 577,
   "p1": "2413",
   "pn": "2416",
   "abstract": [
    "We have been studying some of the essential factors that constitute interpersonal relations between humans and computers by focusing on social bonding in proto-communications (the interaction between adults and human infants or pets). From this viewpoint, this paper presents psychological experiments on the interaction between humans and animated characters that mimic the human voice at the prosodic level using prosody-only voice under different vocal rate of character's voice: (a) faster than normal, (b) normal speed, and (c) slower than normal. We examine the subjects' impression towards animated characters with the above conditions of their voice using post-questionnaire and analyse the change of the speech rate of subjects. The results indicate that most humans may prefer an animated character with a faster voice to that with a slower voice. Moreover, the speech rate of humans changes to opposite of vocal rate of animated characters' voice. I.e. the speech rate of subjects becomes slower when the voice of character becomes faster, and vice versa.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-569"
  },
  "castelli01_eurospeech": {
   "authors": [
    [
     "Eric",
     "Castelli"
    ],
    [
     "Dan",
     "Istrate"
    ]
   ],
   "title": "Everyday life sounds and speech analysis for a medical telemonitoring system",
   "original": "e01_2417",
   "page_count": 4,
   "order": 578,
   "p1": "2417",
   "pn": "2420",
   "abstract": [
    "In order to improve patients life conditions and to reduce the costs of the long hospitalization, the medicine is more and more interested in the telemonitoring techniques. These will allow the old people or the high risk patients to stay at home, and to benefit from a remotely and automated medical supervision. We develop in collaboration with TIMCIMAG laboratory, a system of telemonitoring in a habitat equipped with physiological sensors, position encoders of the person, and microphones. The originality of our approach consists in replacing the video camera monitoring, hardly accepted by the patients by microphones recording the sounds (speech or noises) in the apartment. The microphones carry out a multichannel sound acquisition system which, thanks to the sound information coupled with physical information, will enable us to identify a situation of distress. We describe the practical solutions chosen for the acquisition system and the recorded corpus of situations.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-570"
  },
  "draxler01_eurospeech": {
   "authors": [
    [
     "Christoph",
     "Draxler"
    ],
    [
     "Klaus",
     "Bengler"
    ],
    [
     "Christina",
     "Olaverri-Monreal"
    ]
   ],
   "title": "Speaking while driving - preliminary results on spellings in the German speechdat-car database",
   "original": "e01_2421",
   "page_count": 4,
   "order": 579,
   "p1": "2421",
   "pn": "2424",
   "abstract": [
    "Abstract Voice-operated devices are of particular interest in mobile environments, e.g. vehicles. They promise a natural and intuitive interface to devices and services, and they offer hands-free operation, a legal prerequisite for in-car usage in many European countries. Spelling is a common task for the operation of voice operated devices, especially under unfavorable communication conditions. This paper presents a first analysis of the error and fluency rate for 4502 utterances from the German SpeechDat-Car database. The error rate was found to be between 1.7% and 4.4% for the spelling of natural items, and between 3.6% and 7.9% for artificial letter sequences. Only 3.6% of the utterances contained hesitations. These results suggest that spelling while driving might be a suitable means of fallback interaction if specific error recovery mechanisms are implemented.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-571"
  },
  "chazan01_eurospeech": {
   "authors": [
    [
     "Dan",
     "Chazan"
    ],
    [
     "Meir (Zibulski)",
     "Tzur"
    ],
    [
     "Ron",
     "Hoory"
    ],
    [
     "Gilad",
     "Cohen"
    ]
   ],
   "title": "Efficient periodicity extraction based on sine-wave representation and its application to pitch determination of speech signals",
   "original": "e01_2427",
   "page_count": 4,
   "order": 580,
   "p1": "2427",
   "pn": "2430",
   "abstract": [
    "This paper presents a novel low-complexity method for extracting periodicity of signals based on their sine-wave representation. In this representation, the signal is modeled as a finite sum of sine-waves, with time-varying amplitudes, phases and frequencies. We describe how one can modify the familiar spectral-comb analysis method to obtain a guaranteed and effective procedure to find the fundamental-frequency which gives the best harmonic approximation of the signal spectrum. The search is efficiently carried out in the frequency domain. The procedure obtains a successive refinement of possible pitch values which are consistent with an increasing number of sine wave components. Other pitch intervals are pruned at an early stage of the search. The advantage of this algorithm is its high accuracy achieved at a relatively low complexity. We also briefly describe one possible application in the area of pitch determination of speech signals.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-572"
  },
  "shdaifat01_eurospeech": {
   "authors": [
    [
     "I.",
     "Shdaifat"
    ],
    [
     "R.",
     "Grigat"
    ],
    [
     "Stefan",
     "Lütgert"
    ]
   ],
   "title": "Viseme recognition using multiple feature matching",
   "original": "e01_2431",
   "page_count": 4,
   "order": 581,
   "p1": "2431",
   "pn": "2434",
   "abstract": [
    "In this paper, we present a technique for the extraction of the five main visemes produced in natural speech for German. The method belongs to the LDA (Linear Discriminant Analysis) family. The intensity, the edges, and the line segments are used to locate the lips automatically and for viseme classification. Using many features in the recognition maximizes the probability of recognition rate. The corners of the mouth are used in case of small rotation and scale. An experiment has been carried out on different people, to understand the part of the speech that the human being use. The people grouped the phonemes into five different visemes. The number of distinguished visemes is not the same for each speaker. Everyone express the speech in a different visemes. Good recognition rate has been achieved on different speaker.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-573"
  },
  "hirtum01_eurospeech": {
   "authors": [
    [
     "A. Van",
     "Hirtum"
    ],
    [
     "D.",
     "Berckmans"
    ]
   ],
   "title": "The fundamental frequency of cough by autocorrelation analysis",
   "original": "e01_2435",
   "page_count": 4,
   "order": 582,
   "p1": "2435",
   "pn": "2438",
   "abstract": [
    "The presented research evaluates the quantitative characterization of human cough sounds by estimating the fundamental frequency or pitch. The fundamental frequency was determined by autocorrelation analysis on both the rough time-signal and the linear predicted time-signal. Differences between `spontaneous' and `voluntary' cough sounds are put forward. The experimental cough database was registered in the free acoustical field on respectively 3 pathological and 9 healthy nonsmoking subjects.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-574"
  },
  "ishimoto01_eurospeech": {
   "authors": [
    [
     "Yuichi",
     "Ishimoto"
    ],
    [
     "Masashi",
     "Unoki"
    ],
    [
     "Masato",
     "Akagi"
    ]
   ],
   "title": "A fundamental frequency estimation method for noisy speech based on instantaneous amplitude and frequency",
   "original": "e01_2439",
   "page_count": 4,
   "order": 583,
   "p1": "2439",
   "pn": "2442",
   "abstract": [
    "This paper proposes a robust and accurate F0 estimation method for noisy speech. This method uses two different principles: (1) an F0 estimation based on periodicity and harmonicity of instantaneous amplitude for a robust estimation in noisy environments, and (2) an F0 estimation based on stability of instantaneous frequency as an accurate estimation method. The proposed method also uses a comb filter with controllable pass-bands to combine the two estimation methods. Simulations were carried out to estimate F0s from real speech in noisy environments and to compare the proposed method with other methods. The results showed that this method can not only estimate F0s for clean speech with similar accuracy as the method using only instantaneous frequency but also robustly estimate F0s from noisy speech in comparison with the other methods such as the cepstrum method.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-575"
  },
  "sasou01_eurospeech": {
   "authors": [
    [
     "Akira",
     "Sasou"
    ],
    [
     "Kazuyo",
     "Tanaka"
    ]
   ],
   "title": "Robust LP analysis using glottal source HMM with application to high-pitched and noise corrupted speech",
   "original": "e01_2443",
   "page_count": 4,
   "order": 584,
   "p1": "2443",
   "pn": "2446",
   "abstract": [
    "This paper presents a robust feature extraction method effective to speech signal with high fundamental frequency and/or corrupted by additive white noise. The method represents the glottal source wave using HMM in order to model the non-stationary properties. The nodes of HMM are concatenated in a ring state to represent the periodicity of voiced sounds. The method can accurately extract glottal source wave and vocal tract characteristics from speech signals even in high fundamental frequency as ranging up to 750Hz. From identification theory, estimation of vocal tract characteristics from speech corrupted by additive noise requires glottal source wave that can not be observed directly, so that it needs to be estimated. Therefore, estimation accuracy of vocal tract characteristics highly depends on the estimation accuracy of glottal source wave. We apply the glottal source HMM to extracting the glottal source wave from corrupted speech, and confirmed the feasibility of the method.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-576"
  },
  "choi01_eurospeech": {
   "authors": [
    [
     "Yong-Soo",
     "Choi"
    ],
    [
     "Dae-Hee",
     "Youn"
    ]
   ],
   "title": "Fast harmonic estimation using a low resolution pitch for low bit rate harmonic coding",
   "original": "e01_2447",
   "page_count": 4,
   "order": 585,
   "p1": "2447",
   "pn": "2450",
   "abstract": [
    "This paper describes a fast harmonic estimation, referred to Delta Adjustment (DA), using a low resolution pitch. The presented DA method is based on modification of the Generalized Dual Excitation (GDE) technique [1] which was proposed to improve speech enhancement performance. We introduce the GDE technique and modify it to be suitable for low bit rate harmonic coding that uses only an integer pitch estimate. Unlike the GDE, the DA matches a frequency-warped version of the original spectrum that conforms to a fixed pitch at all harmonic bands. In addition, complexity and performance of the presented method are described in comparison with those of the conventional Fractional Pitch (FP) based harmonic estimation. Experimental results showed that the DA algorithm significantly reduces the complexity of the FP method while maintaining the performance.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-577"
  },
  "cheveigne01_eurospeech": {
   "authors": [
    [
     "Alain de",
     "Cheveigné"
    ],
    [
     "Hideki",
     "Kawahara"
    ]
   ],
   "title": "Comparative evaluation of F0 estimation algorithms",
   "original": "e01_2451",
   "page_count": 4,
   "order": 586,
   "p1": "2451",
   "pn": "2454",
   "abstract": [
    "This paper reports the comparative evaluation of several speech F0 evaluation algorithms over a wide database of laryngograph-labeled speech. Included are several classic algorithms that are available in software on the net, as well as two new algorithms that offer greatly reduced error rates. Particular attention is given to the methodology of evaluation.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-578"
  },
  "ishi01_eurospeech": {
   "authors": [
    [
     "Carlos Toshinori",
     "Ishi"
    ],
    [
     "Nobuaki",
     "Minematsu"
    ],
    [
     "Ryuji",
     "Nishide"
    ],
    [
     "Keikichi",
     "Hirose"
    ]
   ],
   "title": "Identification of accent and intonation in sentences for CALL systems",
   "original": "e01_2455",
   "page_count": 4,
   "order": 587,
   "p1": "2455",
   "pn": "2458",
   "abstract": [
    "In order to construct a CALL (Computer Aided Language Learning) system that can teach learners accent and intonation of Japanese, it's necessary to automatically identify accent types and intonation types in sentence utterances. For this purpose, several acoustic (prosodic) features of speech were investigated taking their effects on human perception into account. For the accent type identification method, the use of average values of F0 in mora and target values of F0 in mora final was evaluated in CV and VC units. Average values of VC units and target values of CV units showed better performance in the identification task. As for the intonation identification, several acoustic features were investigated to represent 6 types of sentence final tones, each conveying different information of intention and perceptual impression. The proposed acoustic features for relative duration and sentence final pitch change showed good correspondence to perceptual features.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-579"
  },
  "kawahara01_eurospeech": {
   "authors": [
    [
     "Hideki",
     "Kawahara"
    ],
    [
     "Parham",
     "Zolfaghari"
    ]
   ],
   "title": "Systematic F0 glitches around nasal-vowel transitions",
   "original": "e01_2459",
   "page_count": 4,
   "order": 588,
   "p1": "2459",
   "pn": "2462",
   "abstract": [
    "High-resolution F0 analysis using a speech database with simultaneously recorded EGG (Electroglottogram) signals indicated that there are systematic F0 glitches around nasal-vowel transitions. The durations of the glitches are 10 to 20 ms and they introduce 5 to 10 Hz F0 shifts. A detailed series of analyses of these glitches indicated that the major contributing factor of these glitches is sudden changes of group delay values of the vocal tract transfer function in the vicinity of the fundamental frequency at nasal-vowel transitions. It is also suggested that the Doppler effects due to apparent changes of vocal tract length are marginal, even if they exist. Finally, issues in evaluating high resolution F0 extraction algorithms and applications to high quality speech manipulation methods are discussed.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-580"
  },
  "wojdel01_eurospeech": {
   "authors": [
    [
     "Jacek C.",
     "Wojdel"
    ],
    [
     "Leon J. M.",
     "Rothkrantz"
    ]
   ],
   "title": "Using aerial and geometric features in automatic lip-reading",
   "original": "e01_2463",
   "page_count": 4,
   "order": 589,
   "p1": "2463",
   "pn": "2466",
   "abstract": [
    "In this paper we present the lip-reading experiments with different sets of the features extracted from the video sequence. In our experiments we use a simple color based filtering techniques to extract the feature vectors from the incoming video signal. Some of those features are directly related to the geometrical properties of the lips (their position and visible thickness). Other features represent the information that relates to the visibility of the other components of the speech production system. The visibility of the teeth and vocal tract for example is described by means of the area they occupy in the image, we call them therefore the aerial features.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-581"
  },
  "schnell01_eurospeech": {
   "authors": [
    [
     "Karl",
     "Schnell"
    ],
    [
     "Arild",
     "Lacroix"
    ]
   ],
   "title": "Inverse filtering of tube models with frequency dependent tube terminations",
   "original": "e01_2467",
   "page_count": 4,
   "order": 590,
   "p1": "2467",
   "pn": "2470",
   "abstract": [
    "The tube model, realized by lattice filters in discrete time, can be used to describe the propagation of plane sound waves through the vocal tract. The tube model which is treated in this contribution contains two prescribed terminations. One for the lip opening and one for the constriction at the glottis. These two terminations are frequency dependent. To estimate the parameters of this tube model, standard algorithms like the Burg-method and related methods are not applicable. Therefore a procedure is proposed to estimate the parameters of these tube models in an adequate way. The procedure is based on inverse filtering, which is carried out iteratively. The analysis of consonants shows, that the corresponding constrictions in the vocal tract area functions can be observed. Using these estimated constrictions it is possible to synthesize VCV transitions too, which implies the typical formant movements.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-582"
  },
  "ouni01b_eurospeech": {
   "authors": [
    [
     "Kaïs",
     "Ouni"
    ],
    [
     "Zied",
     "Lachiri"
    ],
    [
     "Noureddine",
     "Ellouze"
    ]
   ],
   "title": "Formant estimation using gammachirp filterbank",
   "original": "e01_2471",
   "page_count": 4,
   "order": 591,
   "p1": "2471",
   "pn": "2474",
   "abstract": [
    "This paper proposes a new method for formants estimation using a decomposition of speech signal in Gammachirp functions base. It is a spectral analysis method performed by a gammachirp filterbank. A similar approach to the modified spectrum estimation, which allows a smooth and an average spectrum is adopted. In fact, instead of using an uniform window commonly used in short-time Fourier analysis, a bank of gammachirp filters is applied on the signal. A temporal average of the estimated spectra is then applied to obtain one spectrum highlighting the formants structure. This approach is validated by its application on synthesized vowels. The formants are detected with good estimation in comparison with the values given in synthesis. In the same way, this analysis is applied on natural vowels. All the results are compared to three traditional methods, LPC, cepstral and spectral ones and also to a same analysis given by a gammatone filterbank. The tracking of formants shows that this method, which based on gammachirp filters, gives a correct estimation of the formants compared to traditional methods.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-583"
  },
  "potamitis01b_eurospeech": {
   "authors": [
    [
     "I.",
     "Potamitis"
    ],
    [
     "Nikos",
     "Fakotakis"
    ]
   ],
   "title": "Autoregressive time-frequency interpolation in the context of missing data theory for impulsive noise compensation",
   "original": "e01_2475",
   "page_count": 4,
   "order": 592,
   "p1": "2475",
   "pn": "2478",
   "abstract": [
    "The present paper reports on a novel technique for the identification and replacement of spectral coefficients degraded by impulsive noise. The problem is viewed from the perspective of Missing Feature Theory (MFT). The analysis is carried out in the linear spectrum prior to, or after applying the mel-scale filter-bank depending on whether one aims at improving the quality of perception of speech recordings or at Automatic Speech Recognition (ASR). Each filter-bank output is considered to be a time series drawn from an Auto-Regressive process (AR). A validation corpus of undistorted recordings is used to derive a-priori bounds on the expected prediction error of each AR model. In operational conditions, the prediction procedure is monitored and the violation of the statistical bounds indicates band corruption and entails the substitution of the degraded spectral coefficients by the prediction of the corresponding AR model. ASR experiments and informal listening tests demonstrate large improvement in terms of word recognition performance and Itakura-Saito divergence at very low Signal to Impulsive Noise Ratios (SINRs). Data, and implementation code can be found at: ftp://wcl.ee.upatras.gr/\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-584"
  },
  "petrinovic01_eurospeech": {
   "authors": [
    [
     "D.",
     "Petrinovic"
    ],
    [
     "Vladimir",
     "Cuperman"
    ]
   ],
   "title": "Analysis of the voiced speech using the generalized fourier transform with quadratic phase",
   "original": "e01_2479",
   "page_count": 4,
   "order": 593,
   "p1": "2479",
   "pn": "2482",
   "abstract": [
    "One significant problem with sinusoidal modeling of the speech signal is due to the use of standard Fourier Transform for a quasi-periodic signal. The analysis accuracy is severely limited by the lack of stationarity of the analyzed segment, since the analysis is based on the conventional Fourier Transform. An improved analysis technique based on the Generalized Fourier Transform (GFT) with quadratic phase will be discussed in this paper. Speech signal is modeled as a sum of harmonic cosines but with nonlinear phases. A technique for estimation of the time-varying model parameters from the GFT spectrum is proposed. It will be shown that the modeling gain can be improved significantly by inclusion of a single additional parameter in the analysis procedure.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-585"
  },
  "greenberg01c_eurospeech": {
   "authors": [
    [
     "Steven",
     "Greenberg"
    ]
   ],
   "title": "From here to utility - melding phonetic insight with speech technology",
   "original": "e01_2485",
   "page_count": 4,
   "order": 594,
   "p1": "2485",
   "pn": "2488",
   "abstract": [
    "An historic tension exists between science and technology with respect to spoken language. Over the coming decades this tension is likely to dissolve into a collaborative relationship melding linguistic knowledge with machine-learning and statistical methods as a means of developing mature science and technology pertaining to human-machine communication. In the process many mysteries surrounding the form and substance of spoken language are likely to be solved through the concerted efforts of scientists and engineers focused on the creation of \"flawless\" speech technology.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-586"
  },
  "park01_eurospeech": {
   "authors": [
    [
     "Sang-Wook",
     "Park"
    ],
    [
     "Young-Cheol",
     "Park"
    ],
    [
     "Dae-Hee",
     "Youn"
    ]
   ],
   "title": "Speech quality measure for voIP using wavelet based bark coherence function",
   "original": "e01_2491",
   "page_count": 4,
   "order": 595,
   "p1": "2491",
   "pn": "2494",
   "abstract": [
    "The Bark Coherence Function (BCF) defines a coherence function with loudness speech as a new cognition module, robust to linear distortions due to the analog interface of digital mobile system. Preliminary experiments have shown the superiority of BCF over current measures. In this paper, a new BCF suitable for VoIP is developed. The new BCF is based on the wavelet series expansion that provides good frequency resolution while keeping good time locality. The proposed Wavelet based Bark Coherence Function (WBCF) is robust to variable delay often observed in internet telephony such as VoIP. We also show that the refinement of time synchronization after signal decomposition can improve the performance of the WBCF. The regression analysis was performed with VoIP speech data. The correlation coefficients and the standard error of estimates computed using the WBCF showed noticeable improvement over the PSQM that is recommended by ITU-T.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-587"
  },
  "wijngaarden01b_eurospeech": {
   "authors": [
    [
     "Sander J. van",
     "Wijngaarden"
    ],
    [
     "Herman J. M.",
     "Steeneken"
    ]
   ],
   "title": "A proposed method for measuring language dependency of narrow band voice coders",
   "original": "e01_2495",
   "page_count": 4,
   "order": 596,
   "p1": "2495",
   "pn": "2498",
   "abstract": [
    "Narrow band voice coders that use vector quantization techniques may suffer from language dependency: the performance of the coder (in terms of speech intelligibility) may depend on the language spoken. For multinational applications, this is undesirable. A test method is proposed that may be used to determine to which extent a vocoder is language dependent. The proposed method, based on a subjective speech intelligibility test in multiple languages, is shown to be feasible by application on known language dependent 'systems': non-native (human) speakers and listeners. The method is shown to be able to significantly prove differences in language dependency, even when using only three languages and nine speaker/listener combinations.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-588"
  },
  "yoon01b_eurospeech": {
   "authors": [
    [
     "Sung Wan",
     "Yoon"
    ],
    [
     "Sung Kyo",
     "Jung"
    ],
    [
     "Young Cheol",
     "Park"
    ],
    [
     "Dae Hee",
     "Youn"
    ]
   ],
   "title": "An efficient transcoding algorithm for g.723.1 and g.729a speech coders",
   "original": "e01_2499",
   "page_count": 4,
   "order": 597,
   "p1": "2499",
   "pn": "2502",
   "abstract": [
    "To set a valid communication channel between two endpoints employing different speech coders, decoder and encoder of each endpoint need to be placed in tandem. However, tandem coding is often associated with problems such as poor speech quality, high computational load, and additional transmission delay. In this paper, we propose an efficient transcoding algorithm for a legitimate communication between 5.3 kbps G.723.1 and 8 kbps G.729A coders. The proposed transcoding algorithm is composed of four parts: LSP conversion, open-loop pitch conversion, fast adaptive codebook search, and fast fixed codebook search. In each part of the transcoding algorithm, parameters of the target coder are obtained directly from the parameters of the source coder. The efficient transcoding algorithm is supported via the computational reduction of about 25-35% in the encoding part. Subjective preference tests as well as objective quality evaluation confirmed that the proposed transcoding algorithm can produce equivalent speech quality to the tandem coding with the shorter processing delay and less computational complexity.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-589"
  },
  "perezcordoba01_eurospeech": {
   "authors": [
    [
     "Jose L.",
     "Perez-Cordoba"
    ],
    [
     "Antonio J.",
     "Rubio"
    ],
    [
     "Antonio M.",
     "Peinado"
    ],
    [
     "Angel de la",
     "Torre"
    ]
   ],
   "title": "Joint source-channel coding for low bit-rate coding of LSP parameters",
   "original": "e01_2503",
   "page_count": 4,
   "order": 598,
   "p1": "2503",
   "pn": "2506",
   "abstract": [
    "This work presents a quantization technique for LSP parameters which results in a low bit-rate transmission while providing protection against channel errors. As a generalization of the so called Channel Optimized Vector Quantization (COVQ), Channel Optimized Matrix Quantization (COMQ) can remove intraframe and interframe LSP redundancy with the target of protecting the information sent through a channel in the presence of noise. Split COMQ is used in order to reduce storage requirements and complexity. Results show that Split COMQ gives better performance under certain error conditions and a lower bit rate transmission in all channel conditions compared to the reference quantization techniques.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-590"
  },
  "wrede01_eurospeech": {
   "authors": [
    [
     "Britta",
     "Wrede"
    ],
    [
     "Gernot A.",
     "Fink"
    ],
    [
     "Gerhard",
     "Sagerer"
    ]
   ],
   "title": "An investigation of modelling aspects for ratedependent speech recognition",
   "original": "e01_2527",
   "page_count": 4,
   "order": 599,
   "p1": "2527",
   "pn": "2530",
   "abstract": [
    "For the modelling of speech rate variation in speech recognition many approaches have been suggested. However, the training of speech-rate dependent models has by far received most of the attention. In order to investigate problematic aspects related with the classification of the speech data which represents one of the major problems of these approaches extensive experiments were carried out on a German corpus of read speech. The results indicate that while the kind of the model-driven speech-rate measure is only of minor importance a data-driven classification of the speech data significantly improves the performance of rate-dependent models. Further results suggest a detailed modelling of speech rate based on more general models. This means that it might be possible to model speech rate adaptation by means of a transformation based on a continuous measure.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-591"
  },
  "nanjo01_eurospeech": {
   "authors": [
    [
     "Hiroaki",
     "Nanjo"
    ],
    [
     "Kazuomi",
     "Kato"
    ],
    [
     "Tatsuya",
     "Kawahara"
    ]
   ],
   "title": "Speaking rate dependent acoustic modeling for spontaneous lecture speech recognition",
   "original": "e01_2531",
   "page_count": 4,
   "order": 600,
   "p1": "2531",
   "pn": "2534",
   "abstract": [
    "The paper addresses large vocabulary spontaneous speech recognition focusing on acoustic modeling that considers the speaking rate. Using the real lecture speech corpus collected under the priority research project in Japan, we have made baseline acoustic model, and evaluated on the automatic transcription of oral presentations by experienced speakers and obtained word accuracy of 58.2%. Compared with read speech, we have observed significant difference in the speaking rate. To handle fast and poorly articulated phone segments, several extensions of the modeling are explored. Specifically, we introduce state-skipping modeling, speech rate-dependent model, and syllable sub-word modeling. As a result, we reduced the word error rate by absolute 0.8%-2.0%. We also address a language modeling especially on effective use of various large text corpora.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-592"
  },
  "fabian01_eurospeech": {
   "authors": [
    [
     "Tibor",
     "Fábián"
    ],
    [
     "Thilo",
     "Pfau"
    ],
    [
     "Günther",
     "Ruske"
    ]
   ],
   "title": "Analysis of n-best output hypotheses for fast speech in large vocabulary continuous speech recognition",
   "original": "e01_2535",
   "page_count": 4,
   "order": 601,
   "p1": "2535",
   "pn": "2538",
   "abstract": [
    "The performance of speech recognition systems often deteriorate considerably with fast speech. Particularly when the recognizer is run in mismatched conditions, e.g. fast speech, the performance can be improved by properly selecting one of the N-best recognition output hypotheses. For the selection of the best hypothesis, different speech rate measures were taken into account. First, to show the potential of the speech rate as a selection criterion, an \"ideal\" speech rate value is assumed, which is calculated from the known transcription. Phoneme and vowel rate are compared. Second, a phoneme recognizer is used to estimate the speaking rates of unknown sentences. Tests on the spontaneously spoken German Verbmobil material showed a relative decrease of 6.6% in the word error rate for fast speech, when taking the estimated vowel rate which is almost as good as using the \"ideal\" vowel rate (relative improvement of 7.64%). The most accurate match of N-best output hypotheses shows that the word error rate could ideally be decreased by 26.75%.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-593"
  },
  "farinas01_eurospeech": {
   "authors": [
    [
     "Jérôme",
     "Farinas"
    ],
    [
     "François",
     "Pellegrino"
    ]
   ],
   "title": "Automatic rhythm modeling for language identification",
   "original": "e01_2539",
   "page_count": 4,
   "order": 602,
   "p1": "2539",
   "pn": "2542",
   "abstract": [
    "This paper deals with an approach to Automatic Language Identification based on rhythmic modeling. Beside phonetics and phonotactics, rhythm is actually one of the most promising features to be considered for language identification, but significant problems are unresolved for its modeling. In this paper, an algorithm of rhythm extraction is described. Experiments are performed on read speech for 5 European languages. They show that salient features may be automatically extracted and efficiently modeled from the raw signal: a Gaussian mixture modeling of the extracted features results in a 81% percent of correct language identification for the 5 languages, using 20 s duration utterances.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-594"
  },
  "zhang01g_eurospeech": {
   "authors": [
    [
     "Yaxin",
     "Zhang"
    ],
    [
     "Raymond",
     "Lee"
    ],
    [
     "Anton",
     "Madievski"
    ]
   ],
   "title": "Confidence measure (CM) estimation for large vocabulary speaker-independent continuous speech recognition system",
   "original": "e01_2545",
   "page_count": 4,
   "order": 603,
   "p1": "2545",
   "pn": "2548",
   "abstract": [
    "In this paper we report a study for confidence measure estimation in a large vocabulary speaker-independent continuous speech recognition system. A hybrid confidence measure estimation algorithm was developed. The final confidence measure consists of a number of confidence parameters which are generated from the different processing levels of the recognition system. A Parameter Reliability Analysis (PRA) algorithm was proposed to combine the confidence parameters to form the final confidence measure. The approach was applied to a large vocabulary speaker-independent continuous speech recognition system and obtained superior performance.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-595"
  },
  "kodama01_eurospeech": {
   "authors": [
    [
     "Yasuhiro",
     "Kodama"
    ],
    [
     "Takehito",
     "Utsuro"
    ],
    [
     "Hiromitsu",
     "Nishizaki"
    ],
    [
     "Seiichi",
     "Nakagawa"
    ]
   ],
   "title": "Experimental evaluation on confidence of agreement among multiple Japanese LVCSR models",
   "original": "e01_2549",
   "page_count": 4,
   "order": 604,
   "p1": "2549",
   "pn": "2552",
   "abstract": [
    "For many practical applications of speech recognition systems, it is quite desirable to have an estimate of confidence for each hypothesized word. Unlike previous works on confidence measures, this paper studies features for confidence measures that are extracted from outputs of {it more than one} LVCSR models. More specifically, this paper experimentally evaluates the agreement among the outputs of multiple Japanese LVCSR models, with respect to whether it is effective as an estimate of confidence for each hypothesized word. The results of experimental evaluation show that the agreement between the outputs with two acoustic models which have different units in HMMs, such as phonemes and syllables, can achieve quite reliable confidence.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-596"
  },
  "sansegundo01b_eurospeech": {
   "authors": [
    [
     "R.",
     "San-Segundo"
    ],
    [
     "J.",
     "Macías-Guarasa"
    ],
    [
     "J.",
     "Ferreiros"
    ],
    [
     "P.",
     "Martín"
    ],
    [
     "Juan M.",
     "Pardo"
    ]
   ],
   "title": "Detection of recognition errors and out of the spelling dictionary names in a spelled name recognizer for Spanish",
   "original": "e01_2553",
   "page_count": 4,
   "order": 605,
   "p1": "2553",
   "pn": "2556",
   "abstract": [
    "This paper deals with improved confidence assessment for detecting recognition errors and out of dictionary names in a Spanish Recognizer of continuously spelled names over the telephone. We present a hypothesis-verification approach for spelled name recognition. We evaluate the system for several dictionaries, obtaining more than 90.0% recognition rate for a 10,000 name dictionary. For confidence scoring, we consider several features obtained from the different recognition stages. The paper investigates the ability of each feature set to detect recognition errors and names out of the spelling dictionary. We use a neural network to combine all the features in order to obtain the best confidence annotation. Using the data collected from 1,000 phone calls, it is shown that 57.9% incorrectly recognized names and 68.3% out of the spelling dictionary names are detected at a 5% false rejection rate.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-597"
  },
  "mengusoglu01_eurospeech": {
   "authors": [
    [
     "Erhan",
     "Mengusoglu"
    ],
    [
     "Christophe",
     "Ris"
    ]
   ],
   "title": "Use of acoustic prior information for confidence measure in ASR applications",
   "original": "e01_2557",
   "page_count": 4,
   "order": 606,
   "p1": "2557",
   "pn": "2560",
   "abstract": [
    "In this paper, we propose a new acoustic confidence measure of ASR hypothesis and compare it to approaches proposed in the literature. This approach takes into account prior information on the acoustic model performance specific to each phoneme. The new method is tested on two types of recognition errors: the out-of-vocabulary words and the errors due to additive noise. We then propose an efficient way to interpret the raw confidence measure as a correctness prior probability.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-598"
  },
  "ferrer01_eurospeech": {
   "authors": [
    [
     "Luciana",
     "Ferrer"
    ],
    [
     "Claudio",
     "Estienne"
    ]
   ],
   "title": "Improving performance of a keyword spotting system by using a new confidence measure",
   "original": "e01_2561",
   "page_count": 4,
   "order": 607,
   "p1": "2561",
   "pn": "2564",
   "abstract": [
    "This work describes a HMM-based keyword spotting system. In this system, keywords are modeled as concatenations of phoneme models, consequently, no specific databases are needed to train the system. In addition no filler models are required, therefore small computational requirements are necessary. Two main stages define the whole system. The first stage extracts segments from the utterance corresponding to possible keywords based on the maximization of a confidence measure. Those segments are used as input hypotheses for the second stage in order to get a new confidence measure. This second measure is determined by comparing the vector of emission probabilities for an hypothesis over the keyword model and the vector of emission probabilities for the best sequence of phonemes, in the segment where the hypothesis was detected. The first measure is linearly combined with the second one resulting in a new confidence measure which performs significatively better than that one.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-599"
  },
  "tan01_eurospeech": {
   "authors": [
    [
     "Beng T.",
     "Tan"
    ],
    [
     "Yong",
     "Gu"
    ],
    [
     "Trevor",
     "Thomas"
    ]
   ],
   "title": "Word level confidence measures using n-best sub-hypotheses likelihood ratio",
   "original": "e01_2565",
   "page_count": 4,
   "order": 608,
   "p1": "2565",
   "pn": "2568",
   "abstract": [
    "This paper proposes an efficient confidence measure applied at the word level by combining various likelihood ratio tests. The estimates are derived from the local N-best sub-hypotheses. This approach allows the confidence measures to take into account the effect of neighboring words and still provides the estimate localized around the word to be verified. It produces an effective confidence measure that is usable for various tasks. We compared the results with other likelihood ratio based confidence measures including garbage model, N-best homogeneity and online garbage models. The proposed method gave more than 30% relative false accept rate reduction over other methods and the rejection performance was less task-dependent.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-600"
  },
  "goel01_eurospeech": {
   "authors": [
    [
     "Vaibhava",
     "Goel"
    ],
    [
     "Shankar",
     "Kumar"
    ],
    [
     "William",
     "Byrne"
    ]
   ],
   "title": "Confidence based lattice segmentation and minimum Bayes-risk decoding",
   "original": "e01_2569",
   "page_count": 4,
   "order": 609,
   "p1": "2569",
   "pn": "2572",
   "abstract": [
    "Minimum Bayes Risk (MBR) speech recognizers have been shown to yield improvements over the conventional maximum a-posteriori probability (MAP) decoders in the context of N-best list rescoring and A* search over recognition lattices. Segmental MBR (SMBR) procedures have been developed to simplify implementation of MBR recognizers, by segmenting the N-best list or lattice, to reduce the size of the search space over which MBR recognition is carried out. In this paper we describe lattice cutting as a method to segment recognition word lattices into regions of low confidence and high confidence. We present two SMBR decoding procedures that can be applied on low confidence segment sets. Results obtained on the Switchboard conversational telephone speech corpus show modest but significant improvements relative to MAP decoders.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-601"
  },
  "jiang01b_eurospeech": {
   "authors": [
    [
     "Hui",
     "Jiang"
    ],
    [
     "Frank K.",
     "Soong"
    ],
    [
     "Chin-Hui",
     "Lee"
    ]
   ],
   "title": "A data selection strategy for utterance verification in continuous speech recognition",
   "original": "e01_2573",
   "page_count": 4,
   "order": 610,
   "p1": "2573",
   "pn": "2576",
   "abstract": [
    "In this paper, we propose the concept of rival for verifying hypothesis in speech recognition. A likelihood ratio test, based on the rivals model, are investigated for utterance verification in continuous speech recognition. We present an efficient method to train rival model automatically from training data as well as a single pass strategy of utterance verification, namely verification-in-search, for continuous speech recognition. Some preliminary experiments on DARPA Communicator travel task have shown the rival models give better verification performance in terms of identifying mis-recognized words from the output of our baseline recognizer.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-602"
  },
  "ogata01_eurospeech": {
   "authors": [
    [
     "J.",
     "Ogata"
    ],
    [
     "Y.",
     "Ariki"
    ]
   ],
   "title": "Improved speech recognition using iterative decoding based on confidence measures",
   "original": "e01_2577",
   "page_count": 4,
   "order": 611,
   "p1": "2577",
   "pn": "2580",
   "abstract": [
    "In this paper, a decoding method incorporating word-level confidence measures for improved speech recognition is presented. At first, we focus on the estimation of confidence measures from the word graph and evaluate them in word graph rescoring (2nd-pass in 2-pass search system). Next, we propose the lexical tree search (1st-pass in 2-pass search system) incorporating the word-level confidence measures and an iterative decoding based on the confidence measures, resulting in the reconstrucion of the word graph. The experimental results showed that this method achieved a slight improvement at word accuracy.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-603"
  },
  "schaaf01_eurospeech": {
   "authors": [
    [
     "Thomas",
     "Schaaf"
    ]
   ],
   "title": "Detection of OOV words using generalized word models and a semantic class language model",
   "original": "e01_2581",
   "page_count": 4,
   "order": 612,
   "p1": "2581",
   "pn": "2584",
   "abstract": [
    "This paper describes an approach to detect out-of-vocabulary words in spontaneous speech using a language model built on semantic categories and a new type of generalized word models consisting of a mixture of specific and general acoustic units. We demonstrate the construction of the generalized word models as replacements for surnames in a German spontaneous travel planning task GSST. We show that the use of our generalized word models improves recognition accuracy in cases where out-of-vocabulary words appear and does not lead to a degradation of the overall recognition accuracy. In our experiments we measured recall and precision rates of OOV-detection which are close to their theoretic optimum. Furthermore, we compared the effect of using cross-word-triphones vs. using context-independent cross-word models. We show that when using generalized word models with cross-word-triphones, the expected number of consequential errors following an OOV word can be reduced significantly by 37%.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-604"
  },
  "bouwman01_eurospeech": {
   "authors": [
    [
     "Gies",
     "Bouwman"
    ],
    [
     "Janienke",
     "Sturm"
    ],
    [
     "Lou",
     "Boves"
    ]
   ],
   "title": "Effects of OOV rates on keyphrase rejection schemes",
   "original": "e01_2585",
   "page_count": 4,
   "order": 613,
   "p1": "2585",
   "pn": "2588",
   "abstract": [
    "Recognising directory listings for national telephone number inquiry is slowly getting within reach for modern ASR technology. Two key factors for a successful system design are (1) optimal extent of lexical modelling and (2) an effective utterance rejection method. In this paper we show how a choice for the first has consequences for the second. We have taken the approach of building a lexicon with multiword expressions for the most frequently requested telephone listings, stepwise extended with filler words and less frequently addressed listings. In doing so, we keep track of the consequences that different Out of Vocabulary (OOV) rates have on two diverging keyphrase rejection schemes. Experimental results on field data clearly show that tasks with high OOV rates benefit most from acoustic confidence measures, while tasks with low OOV rates are better off with N-best list-based rejection schemes.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-605"
  },
  "sanchezbote01_eurospeech": {
   "authors": [
    [
     "J. L.",
     "Sánchez-Bote"
    ],
    [
     "J.",
     "González-Rodríguez"
    ],
    [
     "D.",
     "Simón-Zorita"
    ]
   ],
   "title": "A new auditory based microphone array and objective evaluation using e-RASTI",
   "original": "e01_2591",
   "page_count": 4,
   "order": 614,
   "p1": "2591",
   "pn": "2594",
   "abstract": [
    "Two are the goals of the work presented in this paper. The first one is the implementation of a new method of speech enhancement using microphone arrays. This method gets noise reduction of speech signal using the masking properties of the human auditory system. The second goal of the paper is to use RASTI index (RApid Speech Transmission Index) for objective evaluation of speech signal quality through E-RASTI evaluation. What is new is that E-RASTI is applied to speech signals and no to RASTI-like signals. The E-RASTI index is specially suited to test reverberant speech and has been used here to evaluate the reverberation reduction produced by a microphone array based on all-pass and minimum-phase decomposition or multichannel liftering. Noise reduction evaluation has been performed with the E-RASTI index and also with more traditional methods, based on Signal to Noise Ratios (SNR). Results have demonstrated the good performance of the noise suppressor and the E-RASTI objective quality evaluator.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-606"
  },
  "araki01b_eurospeech": {
   "authors": [
    [
     "Shoko",
     "Araki"
    ],
    [
     "Shoji",
     "Makino"
    ],
    [
     "Ryo",
     "Mukai"
    ],
    [
     "Hiroshi",
     "Saruwatari"
    ]
   ],
   "title": "Equivalence between frequency domain blind source separation and frequency domain adaptive null beamformers",
   "original": "e01_2595",
   "page_count": 4,
   "order": 615,
   "p1": "2595",
   "pn": "2598",
   "abstract": [
    "Frequency domain Blind Source Separation (BSS) is shown to be equivalent to two sets of frequency domain adaptive microphone arrays, that is, Adaptive Null Beamformers (ANB). The unmixing matrix of the BSS and the filter coefficients of the ANB converge to the same solution in the mean square error sense if the two source signals are ideally independent. This understanding clearly explains the poor performance of the BSS in a real room with long reverberation. The fundamental difference exists in the adaptation period when they should adapt. That is, the ANB can adapt in the presence of a jammer but the absence of a target, whereas the BSS can adapt in the presence of a target and jammer, and also in the presence of only a target.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-607"
  },
  "mukai01_eurospeech": {
   "authors": [
    [
     "Ryo",
     "Mukai"
    ],
    [
     "Shoko",
     "Araki"
    ],
    [
     "Shoji",
     "Makino"
    ]
   ],
   "title": "Separation and dereverberation performance of frequency domain blind source separation for speech in a reverberant environment",
   "original": "e01_2599",
   "page_count": 4,
   "order": 616,
   "p1": "2599",
   "pn": "2602",
   "abstract": [
    "In this paper, we investigate the performance of an unmixing system obtained by frequency domain Blind Source Separation (BSS) based on Independent Component Analysis (ICA). Since ICA is based on statistics, i.e., it only attempts to make outputs independent, it is not easy to predict what is going on in a BSS system. We therefore investigate the detailed components in the processed signals of a whole BSS system by measuring four impulse responses of the system. In particular, we focus on the direct sound and reverberation in the target and jammer signals. As a result, we reveal that the direct sound and reverberation of the jammer can be reduced compared to a null beamformer (NBF), while the reverberation of the target can not be reduced.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-608"
  },
  "saruwatari01_eurospeech": {
   "authors": [
    [
     "Hiroshi",
     "Saruwatari"
    ],
    [
     "Toshiya",
     "Kawamura"
    ],
    [
     "Kiyohiro",
     "Shikano"
    ]
   ],
   "title": "Blind source separation for speech based on fast-convergence algorithm with ICA and beamforming",
   "original": "e01_2603",
   "page_count": 4,
   "order": 617,
   "p1": "2603",
   "pn": "2606",
   "abstract": [
    "We propose a new algorithm for blind source separation (BSS), in which independent component analysis (ICA) and beamforming are combined to resolve the low-convergence problem through optimization in ICA. The proposed method consists of the following three parts: (1) frequency-domain ICA with direction-of-arrival (DOA) estimation, (2) null beamforming based on the estimated DOA, and (3) integration of (1) and (2) based on the algorithm diversity in both iteration and frequency domain. The inverse of the mixing matrix obtained by ICA is temporally substituted by the matrix based on null beamforming through iterative optimization, and the temporal alternation between ICA and beamforming can realize fast- and high-convergence optimization. The results of the signal separation experiments reveal that the signal separation performance of the proposed algorithm is superior to that of the conventional ICA-based BSS method, even under reverberant conditions.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-609"
  },
  "mizumachi01_eurospeech": {
   "authors": [
    [
     "Mitsunori",
     "Mizumachi"
    ],
    [
     "Satoshi",
     "Nakamura"
    ]
   ],
   "title": "Noise reduction using paired-microphones for both far-field and near-field sound sources",
   "original": "e01_2607",
   "page_count": 4,
   "order": 618,
   "p1": "2607",
   "pn": "2610",
   "abstract": [
    "The near-field problem is a source of anxiety with beamforming techniques. We propose a strategy to solve the near-field problem by using a subtractive beamforming technique. The authors earlier proposed a method for noise reduction using paired-microphones. In this paper, the method is improved for near-field sound sources. The proposed method can maintain a high performance regardless of the distance between the sound source and an array, but the performance of a Delayand-Sum beamformer declines even if the amplitude of the target signal is normalized. The concept of \"paired-microphones\" in the proposed method is the key for solving the near-field problem.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-610"
  },
  "nishiura01_eurospeech": {
   "authors": [
    [
     "Takanobu",
     "Nishiura"
    ],
    [
     "Satoshi",
     "Nakamura"
    ],
    [
     "Kiyohiro",
     "Shikano"
    ]
   ],
   "title": "Statistical sound source identification in a real acoustic environment for robust speech recognition using a microphone array",
   "original": "e01_2611",
   "page_count": 4,
   "order": 619,
   "p1": "2611",
   "pn": "2614",
   "abstract": [
    "It is very important for a hands-free speech interface to capture distant talking speech with high quality. A microphone array is an ideal candidate for this purpose. However, this approach requires localizing the target talker. To cope with this problem, we propose a new talker localization method consisting of two algorithms. One algorithm is for multiple sound source localization based on CSP (Cross-power Spectrum Phase) analysis. The other algorithm is for sound source identification among localized multiple sound sources towards talker localization. In this paper, we particularly focus on the latter statistical sound source identification among localized multiple sound sources with statistical speech and environmental sound models based on GMMs (Gaussian Mixture Models) and a microphone array towards talker localization.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-611"
  },
  "alvarezmarquina01_eurospeech": {
   "authors": [
    [
     "A.",
     "Álvarez-Marquina"
    ],
    [
     "P.",
     "Gómez-Vilda"
    ],
    [
     "R.",
     "Martínez-Olalla"
    ],
    [
     "V.",
     "Nieto-Lluís"
    ],
    [
     "V.",
     "Rodellar-Biarge"
    ]
   ],
   "title": "Speech enhancement and source separation based on binaural negative beamforming",
   "original": "e01_2615",
   "page_count": 4,
   "order": 620,
   "p1": "2615",
   "pn": "2618",
   "abstract": [
    "Negative Beamformers are well known for their high angular selectivity, which makes them potentially suitable for speech enhancement applications in noisy backgrounds and for directional source separation. On the other hand, Spectral Subtraction is a well-known method for removing noise from a noise-corrupted speech signal. The scheme that is proposed in this paper combines both techniques in order to obtain large gains in the SNR at a reasonable low computational cost. This method may be used to eliminate or enhance a specific signal using a binaural array. The fundamentals of the technique are reviewed, and a structure to control and improve its angular selectivity is presented. Results obtained in a real situation are also commented. Applications of this technique may be found in Security Systems, Domotic Control and also, to improve Speech Recognition.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-612"
  },
  "gomezvilda01_eurospeech": {
   "authors": [
    [
     "P.",
     "Gómez-Vilda"
    ],
    [
     "A.",
     "Álvarez-Marquina"
    ],
    [
     "V.",
     "Nieto-Lluís"
    ],
    [
     "V.",
     "Rodellar-Biarge"
    ],
    [
     "R.",
     "Martínez-Olalla"
    ]
   ],
   "title": "Multiple source separation in the frequency domain using negative beamforming",
   "original": "e01_2619",
   "page_count": 4,
   "order": 621,
   "p1": "2619",
   "pn": "2622",
   "abstract": [
    "The localization of acoustic sources in a room is essential in many applications, as security monitoring, video conferencing, automatic scene analysis, reverberation canceling, or robust Speech Recognition under multiple-party effect. Through the present paper the design and operation of a negative beamformer for multiple source speech separation will be presented. The problems found for its proper operation when multiple sources are present on the same band will be pointed out and the solutions found will be commented and discussed showing the results of real experiments carried out on a recording scenario.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-613"
  },
  "martin01d_eurospeech": {
   "authors": [
    [
     "Rainer",
     "Martin"
    ],
    [
     "Alexey",
     "Petrovsky"
    ],
    [
     "Thomas",
     "Lotter"
    ]
   ],
   "title": "Planar superdirective microphone arrays for speech acquisition in the car",
   "original": "e01_2623",
   "page_count": 4,
   "order": 622,
   "p1": "2623",
   "pn": "2626",
   "abstract": [
    "In this paper we investigate a small broadside planar (2D) superdirective microphone array for speech acquisition in the car and compare its performance to linear arrays. The objective of this investigation is to replace an expensive directional microphone by a small array of inexpensive omnidirectional sensors. Since the array was designed to be used in the car environment it has to satisfy restrictions with respect to size and to the number of microphones. For all array configurations we present theoretical gains, actual measured gains using low-cost microphones, and beam patterns. For a fixed number of microphones and fixed array dimensions we show that the planar design leads to slightly superior array gains.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-614"
  },
  "kinnunen01_eurospeech": {
   "authors": [
    [
     "Tomi",
     "Kinnunen"
    ],
    [
     "Ismo",
     "Kärkkäinen"
    ],
    [
     "Pasi",
     "Fränti"
    ]
   ],
   "title": "Is speech data clustered? - statistical analysis of cepstral features",
   "original": "e01_2627",
   "page_count": 4,
   "order": 623,
   "p1": "2627",
   "pn": "2630",
   "abstract": [
    "Speech analysis applications are typically based on short-term spectral analysis of the speech signal. Feature extraction process outputs one feature vector per frame. The features are further processed by application-dependent techniques, such as hidden Markov models or vector quantization. Independent from the application, it is often desirable that the feature vectors form separable clusters in the feature space. In this work, we study whether data is really clustered in the feature space and, if so, what is the number of the clusters in typical speech data. We consider different forms of the widely used cepstral features. Keywords: Speech analysis, pattern recognition, short-term features, cluster analysis.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-615"
  },
  "nokas01_eurospeech": {
   "authors": [
    [
     "George",
     "Nokas"
    ],
    [
     "Evangelos",
     "Dermatas"
    ],
    [
     "George",
     "Kokkinakis"
    ]
   ],
   "title": "Maximum likelihood adaptation for distant speech recognition of stationary and moving speakers in reverberant environments",
   "original": "e01_2631",
   "page_count": 4,
   "order": 624,
   "p1": "2631",
   "pn": "2634",
   "abstract": [
    "In this paper, a feature transformation method is presented for distant speech recognition in reverberant and noisy environments. In the Maximum Likelihood framework the optimum bias parameters are obtained on-line, using a small number of successive speech frames. The stochastic matching is achieved by assuming a mixture of Gaussians pdf for the clean speech features. The proposed method was evaluated on the Mel-scaled Frequency Cepstral Coefficient (MFCC) features as well as on MFCC after cepstral mean subtraction and after RASTA filtering. The experiments, carried out in several adverse conditions including room acoustics and additive factory noise for stationary and moving speakers, have shown significant improvement of the recognition accuracy for isolated word speech recognition. In the experiments, the proposed method improves the recognition score of a standing speaker by more than 50%, when SNR is higher than 10db. In the case of the moving speaker the improvement is 8.6% using MFCC while the score reach 91.05% using RASTA fetures.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-616"
  },
  "couvreur01_eurospeech": {
   "authors": [
    [
     "Laurent",
     "Couvreur"
    ],
    [
     "Christophe",
     "Ris"
    ],
    [
     "Christophe",
     "Couvreur"
    ]
   ],
   "title": "Model-based blind estimation of reverberation time: application to robust ASR in reverberant environments",
   "original": "e01_2635",
   "page_count": 4,
   "order": 625,
   "p1": "2635",
   "pn": "2638",
   "abstract": [
    "This paper presents a method for blind estimation of reverberation times in reverberant enclosures. The proposed algorithm is based on a statistical model of short-term log-energy sequences for echo-free speech. Given a speech utterance recorded in a reverberant room, it computes a Maximum Likelihood estimate of the room full-band reverberation time. The estimation method is shown to require little data and to perform satisfactorily. The method has been successfully applied to robust automatic speech recognition in reverberant environments by model selection. For this application, the reverberation time is first estimated from the reverberated speech utterance to be recognized. The estimation is then used to select the best acoustic model out of a library of models trained in various artificial reverberant conditions. Speech recognition experiments in simulated and real reverberant environments show the efficiency of our approach which outperforms standard channel normalization techniques.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-617"
  },
  "momomura01_eurospeech": {
   "authors": [
    [
     "Yasunori",
     "Momomura"
    ],
    [
     "Kenji",
     "Okada"
    ],
    [
     "Takayuki",
     "Arai"
    ],
    [
     "Noboru",
     "Kanedera"
    ],
    [
     "Yuji",
     "Murahara"
    ]
   ],
   "title": "Using the modulation complex wavelet transform for feature extraction in automatic speech recognition",
   "original": "e01_2639",
   "page_count": 4,
   "order": 626,
   "p1": "2639",
   "pn": "2642",
   "abstract": [
    "In this paper we examine robust feature extraction methods for automatic speech recognition (ASR) in noise-distorted environments. Previous research showed that combining the coefficients of multi-resolutional modulation frequency band. We show that this multi-resolutional approach can be achieved using a wavelet transform instead of the Fourier transform. Taking the FFT phase into consideration, we applied the Gabor function, which is a complex function, as mother wavelet. This approach yielded a 1.7% increase in recognition accuracy compared to the FFT-based multi-resolutional approach.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-618"
  },
  "okuno01_eurospeech": {
   "authors": [
    [
     "Hiroshi G.",
     "Okuno"
    ],
    [
     "Kazuhiro",
     "Nakadai"
    ],
    [
     "Tino",
     "Lourens"
    ],
    [
     "Hiroaki",
     "Kitano"
    ]
   ],
   "title": "Separating three simultaneous speeches with two microphones by integrating auditory and visual processing",
   "original": "e01_2643",
   "page_count": 4,
   "order": 627,
   "p1": "2643",
   "pn": "2646",
   "abstract": [
    "This paper addresses the problem of automatic recognition of three simultaneous speeches with two microphones, that is, that of sound source separation where the number of sound sources is greater than that of microphones. The approach used is the {it direction-pass filter}, which is implemented by hypothetical reasoning on the interaural phase difference (IPD) and interaural intensity difference (IID). Auditory processing calculates IPD and IID for each subband, and generates hypotheses for precalculated IPD and IID for every direction including one obtained by visual processing. Then the system calculates the belief factor of hypothesis by Dempster-Shafer theory and determines the direction of each subband. Subbands of the specific direction are collected and then converted to a wave form by inverse FFT. With 200 benchmarks of three simultaneous utterances of Japanese words, the average 1-best and 10-best recognition rates of extracted speeches are 60% and 81%, respectively.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-619"
  },
  "funaki01_eurospeech": {
   "authors": [
    [
     "Keiichi",
     "Funaki"
    ]
   ],
   "title": "A time-varying complex AR speech analysis based on GLS and ELS method",
   "original": "e01_2649",
   "page_count": 4,
   "order": 628,
   "p1": "2649",
   "pn": "2652",
   "abstract": [
    "We have already developed three kinds of time-varying complex AR (TV-CAR) parameter estimation algorithms for analytic speech signal, which are based on minimizing mean square error (MMSE), Huber's robust M-estimation and Instrumental Variable (IV) method. This paper presents novel robust TV-CAR model parameter estimation algorithms on the basis of a Generalized Least Square (GLS) and Extended Least Square (ELS) method, in which the equation error is modeled by complex AR model with white Gaussian input to whiten the equation error. The experiments with natural speech corrupted by white Gaussian demonstrate that the proposed methods achieve robust spectral estimation against additive white Gaussian.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-620"
  },
  "pitz01_eurospeech": {
   "authors": [
    [
     "Michael",
     "Pitz"
    ],
    [
     "Sirko",
     "Molau"
    ],
    [
     "Ralf",
     "Schlüter"
    ],
    [
     "Hermann",
     "Ney"
    ]
   ],
   "title": "Vocal tract normalization equals linear transformation in cepstral space",
   "original": "e01_2653",
   "page_count": 4,
   "order": 629,
   "p1": "2653",
   "pn": "2656",
   "abstract": [
    "We show that vocal tract normalization (VTN) frequency warping results in a linear transformation in the cepstral domain. For the special case of a piece-wise linear warping function, the transformation matrix is analytically calculated. This approach enables us to compute the Jacobian determinant of the transformation matrix, which allows the normalization of the probability distributions used in speaker-normalization for automatic speech recognition.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-621"
  },
  "yu01b_eurospeech": {
   "authors": [
    [
     "An-Tze",
     "Yu"
    ],
    [
     "Hsiao-Chuan",
     "Wang"
    ]
   ],
   "title": "An algorithm for finding line spectrum frequencies of added speech signals and its application to robust speech recognition",
   "original": "e01_2657",
   "page_count": 4,
   "order": 630,
   "p1": "2657",
   "pn": "2660",
   "abstract": [
    "Line Spectrum Frequencies (LSFs) are efficient and popular for representing the spectral envelope in low bit-rate (LBR) speech coding. It is also attractive to use LSFs in the task of speech or speaker recognition. Although, the LBR speech coding does not deteriorate the recognition performance substantially, additive noise does degrade the performance. This paper presents an algorithm for finding LSFs of the addition of two speech signals. This algorithm can be used to adapt the model of noisy speech in LSFs domain, thereby improve the robustness of speech recognition in noisy environments. The experiments on Mandarin digits recognition have proved the effectiveness of the proposed algorithm.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-622"
  },
  "gonon01_eurospeech": {
   "authors": [
    [
     "G.",
     "Gonon"
    ],
    [
     "S.",
     "Montrésor"
    ],
    [
     "M.",
     "Baudry"
    ]
   ],
   "title": "Improved entropic gain for speech signals analysis/synthesis based on an adaptive time-frequency segmentation scheme",
   "original": "e01_2661",
   "page_count": 4,
   "order": 631,
   "p1": "2661",
   "pn": "2664",
   "abstract": [
    "In the search for adaptive representation of speech signals, the Wavelet Packet Decomposition (WPD) has been proved to be a efficient tool because of its frequency adaptation skills through the best basis search algorithm. The entropic minimization of this algorithm is bounded by two artifacts : the dyadic structure of the decomposition and the lack of temporal segmentation. We propose here a low cost extended tree in the WPD which improves the best basis search by reducing the entropy of the base and which is still compatible with the classical WPD. The decomposition also allows perfect reconstruction. The entropic test is updated to take into account the new basis. The preliminary use of a temporal segmentation, based on the Local Entropic Criterion highly improves the entropic gain of the global analysis. The results are shown on experimental speech signals comparing the gain of our scheme versus a usual WPD.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-623"
  },
  "lucke01_eurospeech": {
   "authors": [
    [
     "Helmut",
     "Lucke"
    ],
    [
     "Masanori",
     "Omote"
    ]
   ],
   "title": "Automatic word acquisition from continuous speech",
   "original": "e01_2667",
   "page_count": 4,
   "order": 632,
   "p1": "2667",
   "pn": "2670",
   "abstract": [
    "A method for learning lexical representations of unknown words in an unsupervised manner is described. The unknown words are automatically extracted from continuous speech and a clustering algorithm is used to derive word clusters and lexical representations based on the set of phonetic units used in the system. In experiments, we verify the robustness of the approach. An interesting feature is that extraction errors usually do no harm, as wrongly extracted words tend to inhabit clusters by themselves and thus do not adversely effect the modeling of correctly extracted words.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-624"
  },
  "li01b_eurospeech": {
   "authors": [
    [
     "Qun",
     "Li"
    ],
    [
     "Martin J.",
     "Russell"
    ]
   ],
   "title": "Why is automatic recognition of children's speech difficult?",
   "original": "e01_2671",
   "page_count": 4,
   "order": 633,
   "p1": "2671",
   "pn": "2674",
   "abstract": [
    "This paper is concerned with automatic recognition of childrens speech. The paper begins with a comparison of vowel formant frequencies for adult and childrens speech, and notes that in many cases, the average value of F3 for children is greater than 4kHz. Next it is shown that recognition accuracy for childrens speech degrades rapidly as bandwidth is reduced to less that 6kHz. Finally, it is demonstrated that the choice of front-end signal processing parameters such as analysis window length, and mel-scale filter widths, have little effect on recognition accuracy for childrens speech. It is concluded that bandwidth reduction is a major contributor to the difficulty of recognition of childrens speech.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-625"
  },
  "arunachalam01_eurospeech": {
   "authors": [
    [
     "Sudha",
     "Arunachalam"
    ],
    [
     "Dylan",
     "Gould"
    ],
    [
     "Elaine",
     "Andersen"
    ],
    [
     "Dani",
     "Byrd"
    ],
    [
     "Shrikanth",
     "Narayanan"
    ]
   ],
   "title": "Politeness and frustration language in child-machine interactions",
   "original": "e01_2675",
   "page_count": 4,
   "order": 634,
   "p1": "2675",
   "pn": "2678",
   "abstract": [
    "Children represent a potentially crucial user segment for conversational interfaces. Computer systems interacting with children need to be tailored for these users so that they will understand child intent and so that the child will have a positive and successful experience with the system. This study focuses on discourse analysis of spoken-language child- machine interactions. In particular, politeness and frustration markers were analyzed using a database of child-machine conversations obtained from 160 children using a computer game in a wizard-of-Oz set up. Results indicate that younger children less likely to use overt politeness markers and more polite information requests compared to the older ones, with no apparent gender differences. Younger children, on the other hand, expressed frustration verbally more than the older ones; furthermore, frustration language was more predominant in male children.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-626"
  },
  "nogueiras01_eurospeech": {
   "authors": [
    [
     "Albino",
     "Nogueiras"
    ],
    [
     "Asunción",
     "Moreno"
    ],
    [
     "Antonio",
     "Bonafonte"
    ],
    [
     "José B.",
     "Mariño"
    ]
   ],
   "title": "Speech emotion recognition using hidden Markov models",
   "original": "e01_2679",
   "page_count": 4,
   "order": 635,
   "p1": "2679",
   "pn": "2682",
   "abstract": [
    "This paper introduces a first approach to emotion recognition using RAMSES, the UPC's speech recognition system. The approach is based on standard speech recognition technology using hidden semi-continuous Markov models. Both the selection of low level features and the design of the recognition system are addressed. Results are given on speaker dependent emotion recognition using the Spanish corpus of INTERFACE emotional speech synthesis database. The accuracy recognising seven different emotions---the six ones defined in MPEG-4 plus neutral style---exceeds 80% using the best combination of low level features and HMM structure. This result is very similar to that obtained with the same database in subjective evaluation by human judges.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-627"
  },
  "ibrahim01b_eurospeech": {
   "authors": [
    [
     "Aseel",
     "Ibrahim"
    ],
    [
     "Jonas",
     "Lundberg"
    ],
    [
     "Jenny",
     "Johansson"
    ]
   ],
   "title": "Speech enhanced remote control for media terminal",
   "original": "e01_2685",
   "page_count": 4,
   "order": 636,
   "p1": "2685",
   "pn": "2688",
   "abstract": [
    "A media terminal box combines digital television and services on the World Wide Web. This device will be available in many homes and the interaction with it occurs via a remote control and a visual presentation. The problem is the navigation difficulties among the huge number of television channels. The aim of this study is to investigate whether spoken commands could solve the navigation problem. In this study two input techniques were tested: remote control and speech input. The results showed that speech input was more effective as steps to complete tasks were less and shortcuts were used more often in the speech condition. However, the subjective data showed that the subjects were more satisfied with the remote control input. In conclusion, we recommend multimodal interaction where of speech input to complement the remote control unit.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-628"
  },
  "amaral01_eurospeech": {
   "authors": [
    [
     "Rui",
     "Amaral"
    ],
    [
     "Thibault",
     "Langlois"
    ],
    [
     "Hugo",
     "Meinedo"
    ],
    [
     "Joao",
     "Neto"
    ],
    [
     "Nuno",
     "Souto"
    ],
    [
     "Isabel",
     "Trancoso"
    ]
   ],
   "title": "The development of a portuguese version of a media watch system",
   "original": "e01_2689",
   "page_count": 4,
   "order": 637,
   "p1": "2689",
   "pn": "2692",
   "abstract": [
    "This paper summarizes the work that has been done concerning the Portuguese language in the scope of the ALERT project during its first year. The media watch system that is the goal of this project comprises many different modules, some of them common among the three languages of the project. This paper concentrates on the definition and collection of the necessary linguistic resources for Portuguese, and the development of the speech recognition, topic and jingle detection modules. The first version of the ALERT demo for European Portuguese is also described.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-629"
  },
  "roach01_eurospeech": {
   "authors": [
    [
     "Matthew",
     "Roach"
    ],
    [
     "John S.",
     "Mason"
    ]
   ],
   "title": "Classification of video genre using audio",
   "original": "e01_2693",
   "page_count": 4,
   "order": 638,
   "p1": "2693",
   "pn": "2696",
   "abstract": [
    "In this paper we propose an approach to high-level classification of video into genre: sport, cartoon, news, commercial and music. An important issue for automatic high-level classification systems is the amount of time needed to classify a video. Here we investigate classification performance as a function of the test sequence length. In addition we present performance against different orders and combinations of static and dynamic mel-frequency cepstral coefficients (MFCC). We find that static and delta MFCCs perform well for this classification task. A test sequence length of approximately 25 seconds for the 5 class problem gives approximately 80% correct identification.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-630"
  },
  "horiuchi01_eurospeech": {
   "authors": [
    [
     "Yasuo",
     "Horiuchi"
    ],
    [
     "Akira",
     "Ichikawa"
    ]
   ],
   "title": "Prosody in finger braille and teletext receiver for finger braille",
   "original": "e01_2697",
   "page_count": 4,
   "order": 639,
   "p1": "2697",
   "pn": "2702",
   "abstract": [
    "In this paper, we introduce durational rules in text-to-Finger-Braille. Finger Braille is one of the communication methods for the deaf blind and it seems to be the medium most suited to real-time communication and for expressing the feelings of the speaker because of its prosody existing similarly to spoken languages. First, we analyzed duration between two Braille codes in Finger Braille and found that it can be changed according to the structure and meaning of the sentences. Second, we construct durational rules in Finger Braille based on these results. Third, the effectiveness of the rule was examined in listening experiments with a deaf blind person. As a result, it is suggested that durational prosody help listeners to have clear understanding. Finally, we made a prototype of Finger Braille receiver for teletext broadcasting system as a practical application applying this rule.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-631"
  },
  "bernard01_eurospeech": {
   "authors": [
    [
     "Alexis",
     "Bernard"
    ],
    [
     "Abeer",
     "Alwan"
    ]
   ],
   "title": "Joint channel decoding - Viterbi recognition for wireless applications",
   "original": "e01_2703",
   "page_count": 4,
   "order": 640,
   "p1": "2703",
   "pn": "2706",
   "abstract": [
    "We introduce the concept of joint channel decoding and Viterbi recognition, by which the Viterbi recognizer is modified to take into account the confidence in the decoded feature after channel transmission. We present a metric for evaluating such confidence based on soft decision decoding. As a case study, we quantize MFCCs using predictive VQ. The overall source-channel coding scheme operating at a combined rate of 1 kbps is shown to provide good recognition accuracy over a wide range of Rayleigh fading channels.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-632"
  },
  "peinado01_eurospeech": {
   "authors": [
    [
     "Antonio M.",
     "Peinado"
    ],
    [
     "Victoria",
     "Sanchez"
    ],
    [
     "José C.",
     "Segura"
    ],
    [
     "José L.",
     "Perez-Cordoba"
    ]
   ],
   "title": "MMSE-based channel error mitigation for distributed speech recognition",
   "original": "e01_2707",
   "page_count": 4,
   "order": 641,
   "p1": "2707",
   "pn": "2710",
   "abstract": [
    "Recently, the first version of an ETSI standard for Distributed Speech Recognition has been proposed. The main benefit of this approach is the possibility of maintaining a high recognition performance when accessing remote information systems. The use of a digital channel for transmission of the encoded speech parameters implies the introduction of several channel distortions. Our paper deals with the mitigation of such distortions. We study the application of MMSE estimation to this problem and propose a new MMSE procedure that obtains the probabilities needed for MMSE from a forward-backward algorithm. We show that MMSE estimation obtains better performance than the mitigation algorithm described in the ETSI standard under different channel conditions.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-633"
  },
  "stadermann01b_eurospeech": {
   "authors": [
    [
     "J.",
     "Stadermann"
    ],
    [
     "R.",
     "Meermeier"
    ],
    [
     "Gerhard",
     "Rigoll"
    ]
   ],
   "title": "Distributed speech recognition using traditional and hybrid modeling techniques",
   "original": "e01_2711",
   "page_count": 4,
   "order": 642,
   "p1": "2711",
   "pn": "2714",
   "abstract": [
    "We compare the performance of different acoustic modeling techniques on the task of distributed speech recognition (DSR). The DSR technology is interesting for speech recognition tasks in mobile environments, where features are sent from a thin client to a server where the actual recognition is performed. The evaluation is done on the TI digits database which consists of single digits and digit-chains spoken by American-English talkers. We investigate clean speech and speech added with white noise. Our results show that new hybrid or discrete modeling techniques can outperform standard continuous systems on this task.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-634"
  },
  "riskin01_eurospeech": {
   "authors": [
    [
     "Eve A.",
     "Riskin"
    ],
    [
     "Constantinos",
     "Boulis"
    ],
    [
     "Scott",
     "Otterson"
    ],
    [
     "Mari",
     "Ostendorf"
    ]
   ],
   "title": "Graceful degradation of speech recognition performance over lossy packet networks",
   "original": "e01_2715",
   "page_count": 4,
   "order": 643,
   "p1": "2715",
   "pn": "2718",
   "abstract": [
    "This paper explores packet loss recovery in client-server Automatic Speech Recognition (ASR) systems. A forward error correction (FEC) system is designed and tested over several channel loss models, at variable amounts of data acquisition delay. In experiments with simulated packet loss, the FEC system provides robust ASR performance which degrades gracefully as packet loss rates increase. Comparing this scheme to several alternatives under low and medium loss channel conditions, we found one approach (multiple transmission plus interpolation) that yielded similar performance, but the FEC system should scale better to lower bit rate conditions.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-635"
  },
  "schultz01_eurospeech": {
   "authors": [
    [
     "Tanja",
     "Schultz"
    ],
    [
     "Alex",
     "Waibel"
    ]
   ],
   "title": "Experiments on cross-language acoustic modeling",
   "original": "e01_2721",
   "page_count": 4,
   "order": 644,
   "p1": "2721",
   "pn": "2724",
   "abstract": [
    "With the distribution of speech products all over the world, the portability to new target languages becomes a practical concern. As a consequence our research focuses on rapid transfer of LVCSR systems to other languages. In former studies we evaluated the performance if limited adaptation data is available. Particularly for very time constrained tasks and minority languages, it is even reasonable that no training data is available at all. In this paper we examine what performance can be expected in this scenario. All experiments are run in the framework of the GlobalPhone project which investigates LVCSR systems in 15 languages.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-636"
  },
  "zgank01_eurospeech": {
   "authors": [
    [
     "Andrej",
     "Zgank"
    ],
    [
     "Bojan",
     "Imperl"
    ],
    [
     "Finn Tore",
     "Johansen"
    ],
    [
     "Zdravko",
     "Kacic"
    ],
    [
     "Bogomir",
     "Horvat"
    ]
   ],
   "title": "Crosslingual speech recognition with multilingual acoustic models based on agglomerative and tree-based triphone clustering",
   "original": "e01_2725",
   "page_count": 4,
   "order": 645,
   "p1": "2725",
   "pn": "2729",
   "abstract": [
    "The paper describes our ongoing work on crosslingual speech recognition based on multilingual triphone hidden Markov models. Multilingual acoustic models were built using two different clustering procedures: agglomerative triphone clustering and tree-based triphone clustering. The agglomerative clustering procedure is based on measuring the similarity of triphones on a phoneme level where the monophone similarity is estimated by the Houtgast algorithm. The treebased clustering procedure is based on common broad classes. The Slovenian, German and Spanish 1000 FDB SpeechDat(II) databases were used for training. The crosslingual speech recognition was performed on the Norwegian 1000 FDB SpeechDat(II) database. No adaptation or training with the Norwegian database was used. The mapping of Norwegian phonemes was done with the IPA scheme. Five different Norwegian recognition vocabularies were generated. The best crosslingual system achieved a recognition rate of 45.03%, while the reference Norwegian system achieved 78.32%.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-637"
  },
  "harju01_eurospeech": {
   "authors": [
    [
     "Mikko",
     "Harju"
    ],
    [
     "Petri",
     "Salmela"
    ],
    [
     "Jussi",
     "Leppänen"
    ],
    [
     "Olli",
     "Viikki"
    ],
    [
     "Jukka",
     "Saarinen"
    ]
   ],
   "title": "Comparing parameter tying methods for multilingual acoustic modelling",
   "original": "e01_2729",
   "page_count": 4,
   "order": 646,
   "p1": "2729",
   "pn": "2732",
   "abstract": [
    "In this paper, we compare the state-level and model-level tying of continuous density hidden Markov models for the multilingual acoustic modelling. Using the model-level tying technique, the number of the language dependent (LD) phoneme models of five European languages were reduced to the desired number. This tying was based on dissimilarity measure between the LD phoneme models in a bottom-up agglomerative clustering technique. This system provided 87.3% word recognition accuracy on the test set, while a comparable multilingual recognition based on the SAMPA phone inventory obtained 84.6% accuracy on the same set. The above model-level tying technique was also used for obtaining an alternative phone inventory to SAMPA such that both inventories have an equal number of phones for these five languages. The multilingual recognition systems trained for the SAMPA and alternative phone invetonries obtained 80.9% and 83.7% word accuracies on the same test set, when state-level tying was used for reducing the number of the parameters from 199k to 76k in both systems. The original LD recognition systems obtained 89.0% recognition rate with the same test set, which contained approximately 200 isolated words from SpeechDat(II) databases for each of the five languages. In this paper, the test set results are also given for the recognition systems after performing MAP language adaptation for the multilingual phone models.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-638"
  },
  "chengalvarayan01b_eurospeech": {
   "authors": [
    [
     "Rathi",
     "Chengalvarayan"
    ]
   ],
   "title": "Accent-independent universal HMM-based speech recognizer for american, australian and british English",
   "original": "e01_2733",
   "page_count": 4,
   "order": 647,
   "p1": "2733",
   "pn": "2736",
   "abstract": [
    "This paper addresses the problem of speech recognition under accent variations in English language. It has been demonstrated in previous research efforts that the multi-transitional model architecture is one of the solutions for robust speech recognition. In this study, we describe an universal hybrid system that is trained with data from American, Australian, and British accented speech. Experimental results on connected-digit recognition task show an average string error rate reduction of about 62% and 8% when compared to our best monolingual and multi-transitional systems respectively. The result indicates that the universal model is about three times faster and half time smaller than the multi-transitional or multilingual models and this makes it an ideal choice for practical accent-independent speech recognition applications.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-639"
  },
  "chen01i_eurospeech": {
   "authors": [
    [
     "Fang",
     "Chen"
    ],
    [
     "Jonas",
     "Sääv"
    ]
   ],
   "title": "The effect of time stress on automatic speech recognition accuracy when using second language",
   "original": "e01_2737",
   "page_count": 4,
   "order": 648,
   "p1": "2737",
   "pn": "2740",
   "abstract": [
    "The purpose of the present study is to compare the ASR performance when Swedish people speaking Swedish and English under time-stress and due-task performance. Fifteen university students (20 to 40 years of age, native Swedish language speaking) participated in the experiment. Three factors were studied: time-stress, which was manipulated by PWSP program. Two models of presenting the commands, one is by displaying the text on the screen and another is by headphone voice. Swedish and English languages were tested on Philips FreeSpeech 2000 speech recognition system. There is no individual voice file training and pre-designed grammar file for the speech recognition system. The results show that there are no interactions between any of the factors. The individual differences are large. There is a significant decrease of recognition accuracy (p<0.05) for both languages during stress. The recognition accuracy on Swedish language is significant higher (p<0.01) than English Language due to the Swedish accents.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-640"
  },
  "wong01b_eurospeech": {
   "authors": [
    [
     "Yiu Wing",
     "Wong"
    ],
    [
     "Eric",
     "Chang"
    ]
   ],
   "title": "The effect of pitch and lexical tone on different Mandarin speech recognition tasks",
   "original": "e01_2741",
   "page_count": 4,
   "order": 649,
   "p1": "2741",
   "pn": "2744",
   "abstract": [
    "Tone is an important component in Mandarin speech recognition. It is necessary to recognize the five lexical tones to disambiguate between confusing words. Tone is acoustically characterized by the pitch contour. The use of pitch has been shown to be helpful in Mandarin syllable recognition. In this paper, a comprehensive set of investigations on the effect of pitch on diverse Mandarin speech recognition tasks, namely large vocabulary continuous speech recognition (LVCSR) and isolated word recognition, is reported. In this paper, various techniques to utilize pitch in acoustic modeling are examined. In particular, modeling of tone context dependence and normalization of pitch value are investigated. The experimental result shows that with the incorporation of pitch, an error reduction of 26% can be achieved in tonal syllable recognition. The same level of error reduction is attained in isolated word recognition. On the other hand, the gain from using pitch in an LVCSR task is less. The result suggests that without a language model, the use of pitch is more beneficial in Mandarin speech recognition, thus speech recognizers may be designed to dynamically make use of the pitch feature to obtain the best tradeoff between accuracy and computation.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-641"
  },
  "stemmer01_eurospeech": {
   "authors": [
    [
     "Georg",
     "Stemmer"
    ],
    [
     "Elmar",
     "Nöth"
    ],
    [
     "Heinrich",
     "Niemann"
    ]
   ],
   "title": "Acoustic modeling of foreign words in a German speech recognition system",
   "original": "e01_2745",
   "page_count": 4,
   "order": 650,
   "p1": "2745",
   "pn": "2748",
   "abstract": [
    "The paper deals with the development of acoustic models of foreign words for a German speech recognizer. The recognition quality of foreign words is crucial for the overall performance of a system in application fields like spoken dialogue systems, when foreign words occur as proper names. One of the main problems in the modeling of foreign words is the limitation of training data, which must contain samples of the non-native pronunciation of the foreign sounds. In order to obtain robust acoustic models, which are still precise enough, we compare several methods to map or to merge the models of phonemes, which are pronounced in a similar way by German speakers. We utilize an entropy-based distance measure between sets of phoneme models. The best approach yields a reduction of 16.5 % word error rate, when compared to a baseline system.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-642"
  },
  "siu01b_eurospeech": {
   "authors": [
    [
     "K. C.",
     "Siu"
    ],
    [
     "Helen M.",
     "Meng"
    ]
   ],
   "title": "Semi-automatic grammar induction for bi-directional English-Chinese machine translation",
   "original": "e01_2749",
   "page_count": 4,
   "order": 651,
   "p1": "2749",
   "pn": "2752",
   "abstract": [
    "We have previously designed a methodology for semi-automatic grammar induction from un-annotated corpora belonging to a restricted domain. The induced grammar contains both semantic and syntactic structures, and experiments with the Air Travel Information Service (ATIS-3) corpus demonstrated the viability of our approach [1] for natural language understanding. This work explores the portability of our grammar induction approach to Chinese, based on a corpus of translated ATIS-3 queries. To assess grammar quality, we developed a framework bi-directional English-Chinese example-based machine translation using the induced grammars. Our translation framework can handle word order differences between the language pair during translation. Translations based on the ATIS-3 test sets showed a high percentage (76% to 91%) of user-accepted translations.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-643"
  },
  "charnvivit01_eurospeech": {
   "authors": [
    [
     "Patavee",
     "Charnvivit"
    ],
    [
     "Somchai",
     "Jitapunkul"
    ],
    [
     "Visarut",
     "Ahkuputra"
    ],
    [
     "Ekkarit",
     "Maneenoi"
    ],
    [
     "Umavasee",
     "Thathong"
    ],
    [
     "Boonchai",
     "Thampanitchawong"
    ]
   ],
   "title": "F0 feature extraction by polynomial regression function for monosyllabic Thai tone recognition",
   "original": "e01_2753",
   "page_count": 4,
   "order": 652,
   "p1": "2753",
   "pn": "2756",
   "abstract": [
    "This paper presents a monosyllabic Thai tone recognition system. The system is composed of three processes, fundamental frequency (F0) extraction from input speech signal, analysis of F0 contour for feature extraction, and classification of each tone using the extracted features. In the F0 feature extraction, the polynomial regression functions are employed to fit the segmented F0 curve where its coefficients are used as a feature vector. In tone recognition, we used the maximum a posteriori probability classifier (MAP) to classify a tone. The vocabulary set is composed of the short vowel words, the long vowel words and have the effect of initial and final consonant on the shape of F0 contour. The experimental results show that by using the system as a speaker-dependent system, the maximum recognition rate is 96.20% using three-dimension feature vector. The speaker-independent recognition rates are 79.99% for male and 82.80% for female using four-dimension feature vector.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-644"
  },
  "kim01g_eurospeech": {
   "authors": [
    [
     "Ji-Hwan",
     "Kim"
    ],
    [
     "P. C.",
     "Woodland"
    ]
   ],
   "title": "The use of prosody in a combined system for punctuation generation and speech recognition",
   "original": "e01_2757",
   "page_count": 4,
   "order": 653,
   "p1": "2757",
   "pn": "2760",
   "abstract": [
    "In this paper, we discuss a combined system for punctuation generation and speech recognition. This system incorporates prosodic information with acoustic and language model information. Experiments are conducted for both the reference transcriptions and speech recogniser outputs. For the reference transcription case, prosodic information is shown to be more useful than language model information. When these information sources are combined, we can obtain an F-measure of up to 0.7830 for punctuation recognition. A few straightforward modifications of a conventional speech recogniser allow the system to produce punctuation and speech recognition hypotheses simultaneously. The multiple hypotheses are produced by the automatic speech recogniser and are re-scored by prosodic information. When prosodic information is incorporated, the F-measure can be improved by 19% relative. At the same time, small reductions in word error rate are obtained.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-645"
  },
  "wang01d_eurospeech": {
   "authors": [
    [
     "Chao",
     "Wang"
    ],
    [
     "Stephanie",
     "Seneff"
    ]
   ],
   "title": "Lexical stress modeling for improved speech recognition of spontaneous telephone speech in the jupiter domain",
   "original": "e01_2761",
   "page_count": 4,
   "order": 654,
   "p1": "2761",
   "pn": "2765",
   "abstract": [
    "This paper examines an approach of using lexical stress models to improve the speech recognition performance on spontaneous telephone speech. We analyzed the correlation of various pitch, energy, and duration measurements with lexical stress on a large corpus of spontaneous English utterances, and identified the most informative features of stress using classification experiments. We incorporated the stress models into the recognizer first-pass Viterbi search and obtained modest but statistically significant improvements over a state-of-the-art real-time performance on the Jupiter weather information domain.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-646"
  },
  "stephenson01_eurospeech": {
   "authors": [
    [
     "Todd A.",
     "Stephenson"
    ],
    [
     "M.",
     "Mathew"
    ],
    [
     "Herve",
     "Bourlard"
    ]
   ],
   "title": "Modeling auxiliary information in Bayesian network based ASR",
   "original": "e01_2765",
   "page_count": 4,
   "order": 655,
   "p1": "2765",
   "pn": "2768",
   "abstract": [
    "Automatic speech recognition bases its models on the acoustic features derived from the speech signal. Some have investigated replacing or supplementing these features with information that can not be precisely measured (articulator positions, pitch, gender, etc.) automatically. Consequently, automatic estimations of the desired information would be generated. This data can degrade performance due to its imprecisions. In this paper, we describe a system that treats pitch as an auxiliary information within the framework of Bayesian networks, resulting in improved performance.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-647"
  },
  "chen01j_eurospeech": {
   "authors": [
    [
     "Feili",
     "Chen"
    ],
    [
     "Eric",
     "Chang"
    ]
   ],
   "title": "A new dynamic HMM model for speech recognition",
   "original": "e01_2769",
   "page_count": 4,
   "order": 656,
   "p1": "2769",
   "pn": "2772",
   "abstract": [
    "In this paper, we describe a new method to do speech recognition based on Dynamic HMM architecture. Pitch values are treated as hidden layer and used to modify the parameter of observation probability functions. The results show that the new model achieves approximately 10 percent relative error reductions both in base-syllable recognition task and tonal syllable recognition task. The new method can be used compatibly with conventional HMM based EM training algorithm and Viterbi decoding algorithm.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-648"
  },
  "wang01e_eurospeech": {
   "authors": [
    [
     "Wern-Jun",
     "Wang"
    ],
    [
     "Chun-Jen",
     "Lee"
    ],
    [
     "Eng-Fong",
     "Huang"
    ],
    [
     "Sin-Horng",
     "Chen"
    ]
   ],
   "title": "Multi-keyword spotting of telephone speech using orthogonal transform-based SBR and RNN prosodic model",
   "original": "e01_2773",
   "page_count": 4,
   "order": 657,
   "p1": "2773",
   "pn": "2776",
   "abstract": [
    "In this paper, orthogonal transform-based signal bias removal (OTSBR) approach and RNN prosodic model are proposed for multi-keyword spotting of telephone speech. OTSBR is employed in the pre-processing stage of acoustic decoding and aimed at channel bias estimation to eliminate the acoustic mismatch between training and testing environments. The RNN prosodic model is adopted in the post-processing stage of the acoustic decoding to detect word boundaries for reordering the keyword candidates from the keyword spotter. Simulations on the real speech database collected from the Phone Directory Assistant Service developed in Chunghwa Telecommunication Laboratories (CTL-PDAS) were performed to evaluate the proposed methods. Experimental results showed that 71.0% of keyword detection rate and 81.8% of top 5 keywords inclusion rate can be attained by incorporating OTSBR and RNN prosodic model into the system.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-649"
  },
  "iskra01_eurospeech": {
   "authors": [
    [
     "Andrej",
     "Iskra"
    ],
    [
     "Bojan",
     "Petek"
    ],
    [
     "Tom",
     "Brøndsted"
    ]
   ],
   "title": "Recognition of slovenian speech: within and cross-language experiments on monophones using the speechdat(II)",
   "original": "e01_2777",
   "page_count": 4,
   "order": 658,
   "p1": "2777",
   "pn": "2780",
   "abstract": [
    "Though the Slovenian SpeechDat(II) database is the largest spoken language resources for Slovenian ever recorded, it belongs to the smaller speech data collections made available by the European LE2-4001 project (http://www.speechdat.org/). The aim of this paper is to analyze this new Slovenian resource and explore the possibilities of supplementing it with data recorded for other languages. The donor languages being considered are English, German, and Danish. For each of these languages four time as much speech data has been recorded (4000 speakers compared to the Slovenian 1000 speaker database). Our purely data-driven cross language tests show that serious problems are involved when porting data across languages. The problems are partly due to differences in the recording conditions (telephone line noise). Other problems can be explained by the different phonological structures of the analyzed languages.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-650"
  },
  "batliner01b_eurospeech": {
   "authors": [
    [
     "Anton",
     "Batliner"
    ],
    [
     "Jan",
     "Buckow"
    ],
    [
     "Richard",
     "Huber"
    ],
    [
     "Volker",
     "Warnke"
    ],
    [
     "Elmar",
     "Nöth"
    ],
    [
     "Heinrich",
     "Niemann"
    ]
   ],
   "title": "Boiling down prosody for the classification of boundaries and accents in German and English",
   "original": "e01_2781",
   "page_count": 4,
   "order": 659,
   "p1": "2781",
   "pn": "2784",
   "abstract": [
    "In the focus of this paper is a comparison of the most relevant prosodic features/feature classes for the classification of boundaries and accents in German and in English. Principal components were computed based on a large prosodic feature vector; these principal components were used as predictor variables in a Linear Discriminant analysis as well as in a Classification and Regression Tree. The number of the most relevant principal components was between three and five; for both languages and for boundary and accent classification alike, most important were principal components modelling duration, in combination with energy, followed by pauses and F0.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-651"
  },
  "drygajlo01_eurospeech": {
   "authors": [
    [
     "Andrzej",
     "Drygajlo"
    ],
    [
     "Gary",
     "Garcia Molina"
    ]
   ],
   "title": "Javaspeakerrecognition - interactive workbench for visualizing speaker recognition concepts on the WWW",
   "original": "e01_2787",
   "page_count": 4,
   "order": 660,
   "p1": "2787",
   "pn": "2790",
   "abstract": [
    "The purpose of this paper is to introduce a user-friendly computer assisted learning (CAL) workbench in order to support traditional teaching in the Speaker Recognition area. The workbench (an interactive on-line laboratory) is based on Java and Java-enabled Web browser. The first prototype demonstrator developed at the Swiss Federal Institute of Technology Lausanne (EPFL) for speaker identification training consists of four modules: Dynamic Time Warping (DTW), hidden Markov Modelling (HMM), Vector Quantization (VQ) and Gaussian Mixture Modelling (GMM). These four modules aim at presenting, visualizing and investigating in a uniform way basic concepts of speaker recognition in a single user-friendly environment and allow for easy and highly illustrative learning through experiments with real speech data. They can be used for conventional classroom experiments, in the students' laboratory or can provide self-study means for distance learning applications or for further free exploration.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-652"
  },
  "arai01_eurospeech": {
   "authors": [
    [
     "Takayuki",
     "Arai"
    ],
    [
     "Nobuyuki",
     "Usuki"
    ],
    [
     "Yuji",
     "Murahara"
    ]
   ],
   "title": "Prototype of a vocal-tract model for vowel production designed for education in speech science",
   "original": "e01_2791",
   "page_count": 4,
   "order": 661,
   "p1": "2791",
   "pn": "2794",
   "abstract": [
    "We present a manipulative tool for use in education in speech science. The tool consists of several, square, plastic disks each of which has holes of various diameters. Combined, the holes in 10-17 disks simulate the vocal tract by creating an acoustic tube. Students may study the effect the shape of an acoustic tube has on acoustic output by reordering the disks. After demonstrating the disks in an actual classroom, results show that the tool helped students grasp the relationship between vocal tract configuration and acoustic output. This suggests that students are better able to understand abstractions regarding the acoustics of speech if they have access to a 3-dimensional model such as ours.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-653"
  },
  "cooke01_eurospeech": {
   "authors": [
    [
     "Martin",
     "Cooke"
    ],
    [
     "Maria Luisa",
     "Garcia-Lecumberri"
    ],
    [
     "John",
     "Maidment"
    ]
   ],
   "title": "A tool for automatic feedback on phonemic transcription",
   "original": "e01_2795",
   "page_count": 4,
   "order": 662,
   "p1": "2795",
   "pn": "2798",
   "abstract": [
    "A tool which provides relevant feedback on learners attempts at phonemic transcription is described. The tool aims to complement courses in transcription which are currently taught in both linguistics and language learning settings. A variety of types of feedback are provided. These can be staged by a tutor in order to support customization for different groups of learners and course levels. The tool consists of two similar standalone applications (for tutors and learners). The system performs an optimal alignment of student versus model transcriptions using a dynamic programming algorithm, modified to handle optional and alternative pronunciations. As a result, it computes a summary of errors and their locations within the attempt. Portability and internationalization are key design goals, supported in practice through the use of Java and XML. The tool is currently being tested in a controlled experiment which will provide considerable information on its actual usefulness and necessary refinements.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-654"
  },
  "chang01b_eurospeech": {
   "authors": [
    [
     "Eric",
     "Chang"
    ],
    [
     "Yu",
     "Shi"
    ],
    [
     "Jianlai",
     "Zhou"
    ],
    [
     "Chao",
     "Huang"
    ]
   ],
   "title": "Speech lab in a box: a Mandarin speech toolbox to jumpstart speech related research",
   "original": "e01_2799",
   "page_count": 4,
   "order": 663,
   "p1": "2799",
   "pn": "2802",
   "abstract": [
    "The necessity of gathering data has been an impediment for researchers and students who are interested in getting started in the fields related to speech recognition. We are proposing a new approach of distributing data that is designed to quickly help researchers and students achieve a set of baseline results to build upon. Furthermore, by leveraging publicly available programs, all researchers will be able to exactly reproduce results that are described in this paper. We also aim to facilitate comparison of recognition results in the field of Mandarin speech recognition by including a testing set in the toolbox. We describe a toolbox that includes Mandarin speech data from 125 speakers, suitable language model, scripts and data files required for recreating a set of baseline experiments, and a copy of Microsoft SAPI 5.0 SDK that can help professors and students who wish to jumpstart research programs in speech technologies. By lowering the barrier of entry to the field, we hope to encourage more participation in the study of Mandarin speech recognition.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-655"
  },
  "jong01_eurospeech": {
   "authors": [
    [
     "John H. A. L. de",
     "Jong"
    ],
    [
     "Jared",
     "Bernstein"
    ]
   ],
   "title": "Relating phonepass scores overall scores to the council of europe framework level descriptors",
   "original": "e01_2803",
   "page_count": 4,
   "order": 664,
   "p1": "2803",
   "pn": "2806",
   "abstract": [
    "This study is a preliminary report on an experiment relating PhonePass SET-10 scores to the scale of level descriptors in the Council of Europe Framework. This scale describes the content and level of second language proficiency from a functional communicative perspective. Speech samples from each of 121 non-native English speakers were (1) scored in SET-10, an automatic speaking test, and (2) rated under the European Framework through the conciliation of three independent raters. Rater reliability in using the Council of Europe scale and the comparability of the human and automatic measures are reported.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-656"
  },
  "vicsi01_eurospeech": {
   "authors": [
    [
     "Klára",
     "Vicsi"
    ],
    [
     "Peter",
     "Roach"
    ],
    [
     "Anne-Marie",
     "Öster"
    ],
    [
     "Zdravko",
     "Kacic"
    ],
    [
     "F.",
     "Csatári"
    ],
    [
     "A.",
     "Sfakianaki"
    ],
    [
     "R.",
     "Veronik"
    ],
    [
     "Géza",
     "Gordos"
    ]
   ],
   "title": "A multilingual, multimodal, speech training system, SPECO",
   "original": "e01_2807",
   "page_count": 4,
   "order": 665,
   "p1": "2807",
   "pn": "2810",
   "abstract": [
    "The SPECO Project was funded by the EU through the INCOCOPERNICUS program (Contract no. 977126) in 1999. In the frame of the project a system has been developed which is an audio-visual pronunciation teaching and training system for 5-10 year old children. Correction of disordered aspects of speech is done by real time visual presentation of the speech parameters, in a way that is understandable and interesting for young children, while remaining correct from the acoustic-phonetic point of view. The development of the speech by our teaching method is made mainly on the basis of visual information using the intact visual channel of the hearing impaired child. However during practice we use their limited auditory channel too, by giving auditory information synchronised with the visual one. This multi modal training and teaching system has been developed for languages of all the SPECO partners; these are English, Swedish, Slovenian and Hungarian.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-657"
  },
  "nakamura01_eurospeech": {
   "authors": [
    [
     "Naoki",
     "Nakamura"
    ],
    [
     "Nobuaki",
     "Minematsu"
    ],
    [
     "Seiichi",
     "Nakagawa"
    ]
   ],
   "title": "Instantaneous estimation of accentuation habits for Japanese students to learn English pronunciation",
   "original": "e01_2811",
   "page_count": 4,
   "order": 666,
   "p1": "2811",
   "pn": "2814",
   "abstract": [
    "More and more efforts have been recently made to apply speech technologies to language learning. The authors have been especially focusing on Japanese manners of generating English word stress. This is because accentuation habits inevitable to Japanese learners can be easily found in their stress generation. In our previous studies, a stressed syllable detector and an accentuation habit estimator were developed, where the estimated habits of individual learners accorded well with their English pronunciation proficiency rated by English teachers. However, the estimation methods in our previous studies required several dozens of word utterances or a relatively large amount of computation even when a single word utterance enabled the estimation. In this paper, we investigated a method which required only a single word utterance with a small computation cost. Results showed that similar tendencies can be found between the habits estimated in our previous study and in the current one.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-658"
  },
  "tanaka01_eurospeech": {
   "authors": [
    [
     "Takashi",
     "Tanaka"
    ],
    [
     "Kazumasa",
     "Mori"
    ],
    [
     "Satoshi",
     "Kobayashi"
    ],
    [
     "Seiichi",
     "Nakagawa"
    ]
   ],
   "title": "Automatic construction of CALL system from TV news program with captions",
   "original": "e01_2815",
   "page_count": 4,
   "order": 667,
   "p1": "2815",
   "pn": "2818",
   "abstract": [
    "Many language learning materials have been published in Japan. However, they are limited in their scope and content. In addition, we doubt whether the speech sounds found there are natural in various situations. These days, some TV news programs (by NHK, CNN, ABC, etc.) have closed/open captions corresponding to the speech of the announcer. We have developed a system that automatically makes CALL (Computer Assisted Language Learning) materials from such captioned newscasts. Materials compiled by this system have the following functions: repetition listening, consulting an electronic dictionary, and automatic construction of a dictation test. The materials have the following advantages: polite and natural speech sound, various and timely topics, and abundant materials. In this paper, we describe the organization of our new system.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-659"
  },
  "arcienega01_eurospeech": {
   "authors": [
    [
     "Mijail",
     "Arcienega"
    ],
    [
     "Andrzej",
     "Drygajlo"
    ]
   ],
   "title": "Pitch-dependent GMMs for text-independent speaker recognition systems",
   "original": "e01_2821",
   "page_count": 4,
   "order": 668,
   "p1": "2821",
   "pn": "2825",
   "abstract": [
    "Gaussian mixture models (GMMs) and ergodic hidden Markov models (HMMs) have been successfully applied to model short-term acoustic vectors for speaker recognition systems. Prosodic features are known to carry information concerning the speaker's identity and they can be combined with the short-term acoustic vectors in order to increase the performance of the speaker recognition system. In this paper, a statistical approach using pitch-dependent GMMs for modeling speakers is presented. This new approach is capable of simultaneously modeling the statistical distributions of the short-term acoustic vectors and long-term prosodic features\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-660"
  },
  "ezzaidi01_eurospeech": {
   "authors": [
    [
     "Hassan",
     "Ezzaidi"
    ],
    [
     "Jean",
     "Rouat"
    ],
    [
     "Douglas",
     "OShaughnessy"
    ]
   ],
   "title": "Towards combining pitch and MFCC for speaker recognition systems",
   "original": "e01_2825",
   "page_count": 4,
   "order": 669,
   "p1": "2825",
   "pn": "2828",
   "abstract": [
    "Usually, speaker recognition systems do not take into account the dependence between the vocal source and the vocal tract. A feasibility study that retains this dependence is presented here. A model of joint probability functions of the pitch and the feature vectors is proposed. Three strategies are designed and compared for all female speakers taken from the SPIDRE corpus. The first operates on all voiced and unvoiced speech segments (baseline strategy). The second strategy considers only the voiced speech segments and the last includes the pitch information along with thestandard MFCC. We use two pattern recognizers: LVQ--SLP and GMM. In all cases, we observe an increase in the identification rates and more specifically when using a time duration of 500ms (6% higher).\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-661"
  },
  "kim01h_eurospeech": {
   "authors": [
    [
     "Yu-Jin",
     "Kim"
    ],
    [
     "Hea-Kyoung",
     "Jung"
    ],
    [
     "Jae-Ho",
     "Chung"
    ]
   ],
   "title": "Formant-broadened CMS using peak-picking in LOG spectrum",
   "original": "e01_2829",
   "page_count": 4,
   "order": 670,
   "p1": "2829",
   "pn": "2832",
   "abstract": [
    "In this paper, we propose a method to remove the residual speech effects of the channel cepstrum for speaker recognition in the Cepstral Mean Subtraction framework. The proposed Formant-Broadened CMS(FBCMS) is based on the facts that the formants can be found easily in log spectrum which is transformed from the cepstrum and the formants correspond to the dominant poles of all-pole model which is usually modeled vocal tract. The FBCMS evaluates only poles to be broadening from the log spectrum without polynomial factorization and makes a formant-broadened cepstrum by broadening the bandwidths of formant poles. Using 8 simulated telephone channels, we compared the relative errors of estimating channel cepstrum, speaker identification and computational efficiency for CMS, PFCMS, and the proposed method respectively on two databases. The proposed method has shown to yield improved speaker recognition rates with lower computational burden.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-662"
  },
  "mashao01_eurospeech": {
   "authors": [
    [
     "Daniel J.",
     "Mashao"
    ],
    [
     "N. Tinyiko",
     "Baloyi"
    ]
   ],
   "title": "Improvements in the speaker identification rate using feature-sets",
   "original": "e01_2833",
   "page_count": 4,
   "order": 671,
   "p1": "2833",
   "pn": "2836",
   "abstract": [
    "In this paper we look at the parameterized feature-set that has been used in connected alpha-digit speech recognition and evaluate it on a speaker identification system. Compared to the popular mel-scaled feature-set (MFCC) the parameterized feature-set gives over 21% improvement in identification rate on the NTIMIT database in some cases. On average it shows a 14.0% improvement in identification rate. This demonstrates the improvement in performance that can be obtained using feature-sets.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-663"
  },
  "miyajima01_eurospeech": {
   "authors": [
    [
     "Chiyomi",
     "Miyajima"
    ],
    [
     "Keiichi",
     "Tokuda"
    ],
    [
     "Tadashi",
     "Kitamura"
    ]
   ],
   "title": "Minimum classification error training for speaker identification using Gaussian mixture models based on multi-space probability distribution",
   "original": "e01_2837",
   "page_count": 4,
   "order": 672,
   "p1": "2837",
   "pn": "2840",
   "abstract": [
    "In our previous work, we have proposed a speaker modeling technique using spectral and pitch features for text-independent speaker identification based on Multi-Space Probability Distribution Gaussian Mixture Models (MSD-GMMs). We have presented a maximum likelihood (ML) estimation procedure for the MSD-GMM parameters and demonstrated its high recognition performance. In this paper, we describe an minimum classification error (MCE) training procedure for the MSD-GMM speaker models. MCE training is also applied to automatically estimate mixture-dependent stream weights for spectral and pitch streams. The MCE-based MSD-GMM speaker models are evaluated for a text-independent speaker identification task. Experimental results show that MCE training of the MSD-GMM parameters significantly reduces identification errors and system performance is further improved by appropriately weighting spectral and pitch streams using MCE training.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-664"
  },
  "yadong01_eurospeech": {
   "authors": [
    [
     "Wu",
     "Yadong"
    ],
    [
     "Li",
     "Zhizhu"
    ]
   ],
   "title": "Speaker recognition based on feature space trace",
   "original": "e01_2841",
   "page_count": 4,
   "order": 673,
   "p1": "2841",
   "pn": "2844",
   "abstract": [
    "This paper presents a multiple templates matching algorithm based on feature space trace, which is used in speaker recognition. It extracts the cepstrum coefficient as feature parameter. We normalize the sequence of feature parameter based on feature space trace. The fuzzy c-means method is adopted in generating the multiple templates and the multiple matching method is applied to match the templates. Experiments show this method is a robust, high recognizable and valuable way to implement text-dependent speaker recognition.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-665"
  },
  "yoma01b_eurospeech": {
   "authors": [
    [
     "Nestor Becerra",
     "Yoma"
    ],
    [
     "Miguel Villar",
     "Fernandez"
    ]
   ],
   "title": "Additive and convolutional noise canceling in speaker verification using a stochastic weighted viterbi algorithm",
   "original": "e01_2845",
   "page_count": 4,
   "order": 674,
   "p1": "2845",
   "pn": "2848",
   "abstract": [
    "This paper replaces the ordinary output probability with its expected value if the addition of noise is modeled as a stochastic process, which in turn is merged with the HMM in the Viterbi algorithm. The method, which can be seen as a weighted matching algorithm, is applied in combination with spectral subtraction and RASTA to improve the robustness to additive and convolutional noise of a text-dependent speaker verification system. Reductions around 10% or 20% in the error rates and improvements as high as 30% or 50% in the stability of the decision thresholds are reported when the ordinary Viterbi algorithm is replaced with the weighted one. When compared with the baseline system, reductions of 70% or 80% are shown.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-666"
  },
  "yoshida01_eurospeech": {
   "authors": [
    [
     "Kenichi",
     "Yoshida"
    ],
    [
     "Kazuyuki",
     "Takagi"
    ],
    [
     "Kazuhiko",
     "Ozeki"
    ]
   ],
   "title": "A multi-SNR subband model for speaker identification under noisy environments",
   "original": "e01_2849",
   "page_count": 4,
   "order": 675,
   "p1": "2849",
   "pn": "2852",
   "abstract": [
    "The model presented in this paper consists of a set of subband GMMs trained on speech data corrupted with white Gaussian noise at several SNRs. In the recognition stage, an optimal GMM that yields the maximum accumulated likelihood on the whole input frames is selected for each subband. Then the likelihood is recombined over the subbands to give a speaker identification score. To evaluate the performance of this model, text independent speaker identification experiments were conducted under 5 different noisy environments. For comparison, performance evaluation was also conducted on 3 other models: a subband model trained on clean speech, a multi-SNR fullband model, and a fullband model trained on clean speech. Results show that the multi-SNR subband model is very effective under a wide variety of noisy environments. Additional improvement was observed when an optimal GMM was selected on a short term basis instead of a whole input basis.\n",
    ""
   ],
   "doi": "10.21437/Eurospeech.2001-667"
  }
 },
 "sessions": [
  {
   "title": "Keynotes",
   "papers": [
    "pols01_eurospeech",
    "neuvo01_eurospeech",
    "brennan01_eurospeech"
   ]
  },
  {
   "title": "What do Industry and Universities Expect from Each Other? (Special Session)",
   "papers": [
    "greenberg01_eurospeech",
    "dobler01_eurospeech",
    "niiniluoto01_eurospeech",
    "neuvo01b_eurospeech",
    "strong01_eurospeech",
    "choukri01_eurospeech"
   ]
  },
  {
   "title": "Linguistic Modelling: Language Model Compression",
   "papers": [
    "maltese01_eurospeech",
    "isogai01_eurospeech",
    "zitouni01_eurospeech",
    "whittaker01_eurospeech"
   ]
  },
  {
   "title": "Speech Production: Voice Source",
   "papers": [
    "bloothooft01_eurospeech",
    "ramsay01_eurospeech",
    "henrich01_eurospeech",
    "avanzini01_eurospeech"
   ]
  },
  {
   "title": "Speech Recognition and Understanding: Pronunciation and Subword Units",
   "papers": [
    "zheng01_eurospeech",
    "bazzi01_eurospeech",
    "nakajima01_eurospeech",
    "kneissler01_eurospeech",
    "lee01_eurospeech",
    "ziegenhain01_eurospeech",
    "bosch01_eurospeech",
    "yi01_eurospeech",
    "bisani01_eurospeech",
    "wolff01_eurospeech",
    "livescu01_eurospeech",
    "riis01_eurospeech",
    "tsai01_eurospeech",
    "tomokiyo01_eurospeech",
    "ma01_eurospeech",
    "schramm01_eurospeech",
    "he01_eurospeech",
    "fegyo01_eurospeech"
   ]
  },
  {
   "title": "Phonetics and Phonology: Prosody and Others",
   "papers": [
    "swerts01_eurospeech",
    "hitchcock01_eurospeech",
    "gibbon01_eurospeech",
    "schroder01_eurospeech",
    "ouden01_eurospeech",
    "gibbon01b_eurospeech",
    "dimperio01_eurospeech",
    "iivonen01_eurospeech",
    "kim01_eurospeech",
    "nordgard01_eurospeech",
    "elordieta01_eurospeech",
    "johansson01_eurospeech",
    "shen01_eurospeech",
    "amir01_eurospeech"
   ]
  },
  {
   "title": "Speech Perception: First and Second Language Learning",
   "papers": [
    "behne01_eurospeech",
    "yang01_eurospeech",
    "otake01_eurospeech",
    "callan01_eurospeech",
    "komatsu01_eurospeech"
   ]
  },
  {
   "title": "Speech Perception: Miscellaneous",
   "papers": [
    "fernandez01_eurospeech",
    "harding01_eurospeech",
    "menard01_eurospeech",
    "menard01b_eurospeech",
    "eskenazi01_eurospeech",
    "chen01_eurospeech",
    "jiang01_eurospeech",
    "ainsworth01_eurospeech",
    "hiroshige01_eurospeech",
    "tokuma01_eurospeech",
    "swerts01b_eurospeech",
    "house01_eurospeech",
    "komatsu01b_eurospeech",
    "burnham01_eurospeech",
    "eriksson01_eurospeech",
    "mixdorff01_eurospeech",
    "hawkins01_eurospeech",
    "lin01_eurospeech",
    "erdenebat01_eurospeech"
   ]
  },
  {
   "title": "Noise Robust Recognition: Frontend and Compensation Algorithms (Special Session)",
   "papers": [
    "zhu01_eurospeech",
    "ellis01_eurospeech",
    "andrassy01_eurospeech",
    "kotnik01_eurospeech",
    "veth01_eurospeech",
    "macho01_eurospeech",
    "yapanel01_eurospeech",
    "barker01_eurospeech",
    "droppo01_eurospeech",
    "segura01_eurospeech",
    "morris01_eurospeech",
    "jarc01_eurospeech",
    "yao01_eurospeech"
   ]
  },
  {
   "title": "Linguistic Modelling: Language Model Adaptation",
   "papers": [
    "federico01_eurospeech",
    "maucec01_eurospeech",
    "georgila01_eurospeech",
    "visweswariah01_eurospeech",
    "chen01b_eurospeech"
   ]
  },
  {
   "title": "Speech Production: Articulation",
   "papers": [
    "engwall01_eurospeech",
    "moen01_eurospeech",
    "elgendy01_eurospeech",
    "gick01_eurospeech",
    "ouni01_eurospeech"
   ]
  },
  {
   "title": "Speech Recognition and Understanding: Topic Detection and Information Retrieval",
   "papers": [
    "theunissen01_eurospeech",
    "franz01_eurospeech",
    "zweig01_eurospeech",
    "kuo01_eurospeech",
    "chen01c_eurospeech"
   ]
  },
  {
   "title": "Phonetics and Phonology: Segmentals and Synthesis",
   "papers": [
    "tsukada01_eurospeech",
    "goronzy01_eurospeech",
    "laprie01_eurospeech",
    "granser01_eurospeech",
    "ashby01_eurospeech",
    "fung01_eurospeech",
    "donovan01_eurospeech",
    "pan01_eurospeech",
    "campbell01_eurospeech",
    "jensen01_eurospeech",
    "tamura01_eurospeech",
    "toda01_eurospeech",
    "tang01_eurospeech",
    "gutierrezarriola01_eurospeech",
    "mashimo01_eurospeech",
    "coulston01_eurospeech"
   ]
  },
  {
   "title": "Noise Robust Recognition: Frontend (Special Session)",
   "papers": [
    "kim01b_eurospeech",
    "cheng01_eurospeech",
    "benitez01_eurospeech",
    "noe01_eurospeech",
    "ealey01_eurospeech"
   ]
  },
  {
   "title": "Linguistic Modelling: Semantic Modelling",
   "papers": [
    "carter01_eurospeech",
    "pargellis01_eurospeech",
    "mou01_eurospeech",
    "bellegarda01_eurospeech",
    "jansche01_eurospeech"
   ]
  },
  {
   "title": "Speech Perception: Recognition and Intelligibility",
   "papers": [
    "otake01b_eurospeech",
    "colotte01_eurospeech",
    "greenberg01b_eurospeech",
    "crouzet01_eurospeech",
    "adank01_eurospeech"
   ]
  },
  {
   "title": "Speech Recognition and Understanding: LVCSR",
   "papers": [
    "ircing01_eurospeech",
    "shinozaki01_eurospeech",
    "siohan01_eurospeech",
    "beyerlein01_eurospeech",
    "gao01_eurospeech",
    "willett01_eurospeech",
    "liu01_eurospeech",
    "nakagawa01_eurospeech",
    "homma01_eurospeech"
   ]
  },
  {
   "title": "Speech Synthesis: Systems and Prosody",
   "papers": [
    "cosi01_eurospeech",
    "monaghan01_eurospeech",
    "kiss01_eurospeech",
    "klabbers01_eurospeech",
    "olaszy01_eurospeech",
    "herwijnen01_eurospeech",
    "yamashita01_eurospeech",
    "sun01_eurospeech",
    "zaki01_eurospeech",
    "xu01_eurospeech",
    "muller01_eurospeech",
    "henrichsen01_eurospeech",
    "baumann01_eurospeech",
    "schroder01b_eurospeech",
    "gustafson01_eurospeech"
   ]
  },
  {
   "title": "Speech Recognition and Understanding: Articulatory and Perceptual Approaches to ASR",
   "papers": [
    "chen01d_eurospeech",
    "tam01_eurospeech",
    "jancovic01_eurospeech",
    "gu01_eurospeech",
    "hagen01_eurospeech",
    "gajic01_eurospeech",
    "edmondson01_eurospeech",
    "frankel01_eurospeech",
    "ma01b_eurospeech",
    "weber01_eurospeech",
    "yu01_eurospeech",
    "wendt01_eurospeech",
    "li01_eurospeech"
   ]
  },
  {
   "title": "Noise Robust Recognition: Robust Systems - What Helps? (Special Session)",
   "papers": [
    "lieb01_eurospeech",
    "saon01_eurospeech",
    "afify01_eurospeech"
   ]
  },
  {
   "title": "Phonetics and Phonology: Segmentals",
   "papers": [
    "fougeron01_eurospeech",
    "zee01_eurospeech",
    "delvaux01_eurospeech",
    "demolin01_eurospeech"
   ]
  },
  {
   "title": "Speech Production: Prosody",
   "papers": [
    "fant01_eurospeech",
    "ohno01_eurospeech",
    "dogil01_eurospeech",
    "shih01_eurospeech",
    "kochanski01_eurospeech",
    "frid01_eurospeech",
    "alku01_eurospeech",
    "botinis01_eurospeech",
    "chu01_eurospeech",
    "gustafsoncapkova01_eurospeech",
    "takamaru01_eurospeech",
    "petersen01_eurospeech",
    "savino01_eurospeech",
    "mixdorff01b_eurospeech",
    "ibrahim01_eurospeech",
    "smith01_eurospeech",
    "herwijnen01b_eurospeech",
    "tabain01_eurospeech",
    "barbosa01_eurospeech"
   ]
  },
  {
   "title": "Speech Recognition and Understanding: Acoustic Modelling - I",
   "papers": [
    "stuttle01_eurospeech",
    "zheng01b_eurospeech",
    "perronnin01_eurospeech",
    "sarikaya01_eurospeech",
    "eide01_eurospeech",
    "zhang01_eurospeech",
    "duchateau01_eurospeech",
    "segura01b_eurospeech",
    "peng01_eurospeech",
    "bauer01_eurospeech",
    "keshet01_eurospeech",
    "brugnara01_eurospeech",
    "levit01_eurospeech",
    "emori01_eurospeech",
    "okuda01_eurospeech",
    "baba01_eurospeech",
    "zhang01b_eurospeech",
    "rodriguez01_eurospeech",
    "deviren01_eurospeech"
   ]
  },
  {
   "title": "Linguistic Modelling: Language Models",
   "papers": [
    "onishi01_eurospeech",
    "jitsuhiro01_eurospeech",
    "siciliagarcia01_eurospeech",
    "akiba01_eurospeech",
    "matsui01_eurospeech",
    "mori01_eurospeech",
    "kim01c_eurospeech",
    "shen01b_eurospeech",
    "esteve01_eurospeech",
    "rayner01_eurospeech",
    "whittaker01b_eurospeech",
    "siivola01_eurospeech",
    "lopezcozar01_eurospeech",
    "takagi01_eurospeech",
    "chen01e_eurospeech",
    "lin01b_eurospeech",
    "kulekcy01_eurospeech",
    "tarsaku01_eurospeech",
    "blache01_eurospeech",
    "barkat01_eurospeech",
    "fitt01_eurospeech",
    "joue01_eurospeech",
    "xu01b_eurospeech",
    "chen01f_eurospeech",
    "ordelman01_eurospeech",
    "brndsted01_eurospeech"
   ]
  },
  {
   "title": "Speaker Recognition: Identification, Verification and Tracking. Speech Recognition and Understanding: Language Identification",
   "papers": [
    "brungart01_eurospeech",
    "faltlhauser01_eurospeech",
    "sanderson01_eurospeech",
    "satoh01_eurospeech",
    "surendran01_eurospeech",
    "tsai01b_eurospeech",
    "tsai01c_eurospeech",
    "wutiwiwatchai01_eurospeech",
    "rodriguezsaeta01_eurospeech",
    "schalk01_eurospeech",
    "martin01_eurospeech",
    "ou01_eurospeech",
    "sivakumaran01_eurospeech",
    "benarousse01_eurospeech",
    "kirchhoff01_eurospeech"
   ]
  },
  {
   "title": "Phonetics and Phonology: Prominence and Timing",
   "papers": [
    "streefkerk01_eurospeech",
    "chung01_eurospeech",
    "schaeffler01_eurospeech",
    "maddieson01_eurospeech"
   ]
  },
  {
   "title": "Speech Synthesis: Concatenation",
   "papers": [
    "francois01_eurospeech",
    "vosnidis01_eurospeech",
    "founda01_eurospeech",
    "ferencz01_eurospeech",
    "barry01_eurospeech",
    "syrdal01_eurospeech",
    "boeffard01_eurospeech",
    "bulyko01_eurospeech",
    "law01_eurospeech"
   ]
  },
  {
   "title": "Speech Recognition and Understanding: Noise Robustness",
   "papers": [
    "wet01_eurospeech",
    "yamada01_eurospeech",
    "tufekci01_eurospeech",
    "hwang01_eurospeech",
    "matsumoto01_eurospeech",
    "martin01b_eurospeech",
    "trentin01_eurospeech",
    "evans01_eurospeech",
    "chengalvarayan01_eurospeech",
    "frey01_eurospeech",
    "hansen01_eurospeech",
    "siu01_eurospeech",
    "naito01_eurospeech",
    "gallardoantolin01_eurospeech",
    "renevey01_eurospeech",
    "ming01_eurospeech",
    "kawamura01_eurospeech",
    "masudakatsuse01_eurospeech",
    "kotnik01b_eurospeech",
    "vlaj01_eurospeech",
    "chien01_eurospeech",
    "hilger01_eurospeech",
    "yao01b_eurospeech"
   ]
  },
  {
   "title": "Signal Analysis: Microphone Arrays & Source Localisation",
   "papers": [
    "koutras01_eurospeech",
    "herbordt01_eurospeech",
    "seltzer01_eurospeech",
    "koutras01b_eurospeech",
    "asano01_eurospeech"
   ]
  },
  {
   "title": "Speech Recognition and Understanding: Audio-Visual Processing",
   "papers": [
    "lee01b_eurospeech",
    "heckmann01_eurospeech",
    "potamianos01_eurospeech",
    "daubias01_eurospeech",
    "pelachaud01_eurospeech"
   ]
  },
  {
   "title": "SIGshow (Special Session)",
   "papers": [
    "eriksson01b_eurospeech",
    "hirst01_eurospeech",
    "bonastre01_eurospeech",
    "campbell01b_eurospeech",
    "massaro01_eurospeech",
    "bimbot01_eurospeech",
    "dybkjr01_eurospeech",
    "delcloque01_eurospeech",
    "nadeu01_eurospeech"
   ]
  },
  {
   "title": "Speech Synthesis: Prosody",
   "papers": [
    "chen01g_eurospeech",
    "heggtveit01_eurospeech",
    "silverman01_eurospeech",
    "saito01_eurospeech",
    "hirschberg01_eurospeech"
   ]
  },
  {
   "title": "Applications: Multimodal Applications",
   "papers": [
    "gurbuz01_eurospeech",
    "lucey01_eurospeech",
    "bernsen01_eurospeech",
    "nakadai01_eurospeech",
    "nitta01_eurospeech"
   ]
  },
  {
   "title": "Speech Recognition and Understanding: Speaker Adaptation",
   "papers": [
    "gunawardana01_eurospeech",
    "kenny01_eurospeech",
    "kim01d_eurospeech",
    "zhou01_eurospeech",
    "yoshizawa01_eurospeech"
   ]
  },
  {
   "title": "Speech Recognition and Understanding: Adaptation",
   "papers": [
    "lei01_eurospeech",
    "wallhoff01_eurospeech",
    "myrvoll01_eurospeech",
    "liu01b_eurospeech",
    "lefevre01_eurospeech",
    "matrouf01_eurospeech",
    "purnell01_eurospeech",
    "wong01_eurospeech",
    "yoma01_eurospeech",
    "wu01_eurospeech",
    "vasilache01_eurospeech",
    "tsao01_eurospeech",
    "warakagoda01_eurospeech"
   ]
  },
  {
   "title": "Dialogue Systems: Project Descriptions",
   "papers": [
    "cordoba01_eurospeech",
    "xu01c_eurospeech",
    "nouza01_eurospeech",
    "besacier01_eurospeech",
    "hickey01_eurospeech",
    "hirschberg01b_eurospeech",
    "lo01_eurospeech",
    "chien01b_eurospeech",
    "sasajima01_eurospeech",
    "kiriyama01_eurospeech",
    "komatani01_eurospeech",
    "durston01_eurospeech",
    "azzini01_eurospeech",
    "nakano01_eurospeech",
    "glass01_eurospeech",
    "rahim01_eurospeech",
    "wahlster01_eurospeech",
    "meng01_eurospeech",
    "wang01_eurospeech",
    "lemon01_eurospeech",
    "shriver01_eurospeech"
   ]
  },
  {
   "title": "Dialogue Systems: Resources",
   "papers": [
    "shriberg01_eurospeech",
    "beckham01_eurospeech",
    "kipp01_eurospeech",
    "walker01_eurospeech"
   ]
  },
  {
   "title": "Speaker Recognition: Features and Transforms",
   "papers": [
    "huang01_eurospeech",
    "nishida01_eurospeech",
    "wang01b_eurospeech",
    "navratil01_eurospeech"
   ]
  },
  {
   "title": "Speech Perception: Prosody",
   "papers": [
    "caspers01_eurospeech",
    "rietveld01_eurospeech",
    "chen01h_eurospeech",
    "janse01_eurospeech"
   ]
  },
  {
   "title": "Speech Production: Miscellaneous",
   "papers": [
    "koopmansvanbeinum01_eurospeech",
    "engwall01b_eurospeech",
    "kaburagi01_eurospeech",
    "teixeira01_eurospeech",
    "fuchs01_eurospeech",
    "aylett01_eurospeech",
    "murphy01_eurospeech",
    "schoentgen01_eurospeech",
    "dinther01_eurospeech",
    "ogner01_eurospeech",
    "pitermann01_eurospeech",
    "south01_eurospeech",
    "podhorski01_eurospeech"
   ]
  },
  {
   "title": "Existing and Future Corpora: Next Generation Speech Resources (Special Session)",
   "papers": [
    "campbell01c_eurospeech",
    "broeder01_eurospeech",
    "bigbee01_eurospeech",
    "altosaar01_eurospeech",
    "turk01_eurospeech"
   ]
  },
  {
   "title": "Signal Analysis: Speech Processing in Car Environments",
   "papers": [
    "matassoni01_eurospeech",
    "plucienkowski01_eurospeech",
    "selouani01_eurospeech",
    "korthauer01_eurospeech",
    "lleida01_eurospeech"
   ]
  },
  {
   "title": "Speech Recognition and Understanding: Finite State Transducers for ASR",
   "papers": [
    "hazen01_eurospeech",
    "boulianne01_eurospeech",
    "hetherington01_eurospeech",
    "mohri01_eurospeech",
    "seward01_eurospeech"
   ]
  },
  {
   "title": "Resources, Assessment and Standards: Assessment Tools & Methodology",
   "papers": [
    "wijngaarden01_eurospeech",
    "cucchiarini01_eurospeech",
    "hutchinson01_eurospeech",
    "moller01_eurospeech",
    "lee01c_eurospeech",
    "toledano01_eurospeech",
    "ludwig01_eurospeech",
    "lewis01_eurospeech",
    "teixeira01b_eurospeech",
    "nefti01_eurospeech",
    "zhou01b_eurospeech"
   ]
  },
  {
   "title": "Existing and Future Corpora: Automated Analysis of Speech Resources (Special Session)",
   "papers": [
    "kessens01_eurospeech",
    "chang01_eurospeech",
    "wester01_eurospeech"
   ]
  },
  {
   "title": "Dialogue Systems: Dialogue Systems and Generation",
   "papers": [
    "galley01_eurospeech",
    "cook01_eurospeech",
    "araki01_eurospeech",
    "rogati01_eurospeech"
   ]
  },
  {
   "title": "Speaker Recognition: Alternative Trends in Verification",
   "papers": [
    "vivaracho01_eurospeech",
    "fine01_eurospeech",
    "kharroubi01_eurospeech",
    "gu01b_eurospeech",
    "stapert01_eurospeech",
    "blouet01_eurospeech",
    "andrews01_eurospeech",
    "doddington01_eurospeech"
   ]
  },
  {
   "title": "Speech Recognition and Understanding: Speech Understanding",
   "papers": [
    "hori01_eurospeech",
    "hacioglu01_eurospeech",
    "knight01_eurospeech",
    "abdou01_eurospeech"
   ]
  },
  {
   "title": "Speech Recognition and Understanding: Algorithms and Architectures",
   "papers": [
    "litichever01_eurospeech",
    "faizakov01_eurospeech",
    "shire01_eurospeech",
    "zhang01c_eurospeech",
    "itoh01_eurospeech",
    "hori01b_eurospeech",
    "psutka01_eurospeech",
    "pusateri01_eurospeech",
    "ahn01_eurospeech",
    "macherey01_eurospeech",
    "chotimongkol01_eurospeech",
    "deligne01_eurospeech",
    "hirsch01_eurospeech",
    "shimodaira01_eurospeech",
    "srinivasamurthy01_eurospeech"
   ]
  },
  {
   "title": "Signal Analysis: Speech Enhancement and Noise Processing",
   "papers": [
    "stadermann01_eurospeech",
    "sheikhzadeh01_eurospeech",
    "ramabadran01_eurospeech",
    "tihelka01_eurospeech",
    "kim01e_eurospeech",
    "mahe01_eurospeech",
    "lee01d_eurospeech",
    "fujimoto01_eurospeech",
    "vetter01_eurospeech",
    "renevey01b_eurospeech",
    "karneback01_eurospeech",
    "cheng01b_eurospeech",
    "potamitis01_eurospeech",
    "attias01_eurospeech"
   ]
  },
  {
   "title": "Speech Synthesis: Grapheme-to-Phoneme Conversion",
   "papers": [
    "kienappel01_eurospeech",
    "mana01_eurospeech",
    "llitjos01_eurospeech",
    "boulademareuil01_eurospeech"
   ]
  },
  {
   "title": "Signal Analysis: Speech Enhancement",
   "papers": [
    "shin01_eurospeech",
    "cohen01_eurospeech",
    "bahoura01_eurospeech",
    "yoon01_eurospeech"
   ]
  },
  {
   "title": "Speech Recognition and Understanding: Discriminative Training",
   "papers": [
    "wang01c_eurospeech",
    "zhou01c_eurospeech",
    "wu01b_eurospeech",
    "hung01_eurospeech"
   ]
  },
  {
   "title": "Speech Coding: Advances in Speech Coding",
   "papers": [
    "heikkinen01_eurospeech",
    "nurminen01_eurospeech",
    "pobloth01_eurospeech",
    "faundezzanuy01_eurospeech",
    "katugampala01_eurospeech",
    "ho01_eurospeech",
    "lukasiak01_eurospeech",
    "najafzadeh01_eurospeech",
    "bessette01_eurospeech",
    "pujalte01_eurospeech",
    "lee01e_eurospeech",
    "satheesh01_eurospeech",
    "etemoglu01_eurospeech",
    "jung01_eurospeech"
   ]
  },
  {
   "title": "Resources, Assessment and Standards: Corpora",
   "papers": [
    "hansen01b_eurospeech",
    "kawaguchi01_eurospeech",
    "heeman01_eurospeech",
    "nemeth01_eurospeech",
    "backfried01_eurospeech",
    "burger01_eurospeech",
    "matousek01_eurospeech",
    "son01_eurospeech",
    "louw01_eurospeech",
    "heuvel01_eurospeech",
    "gibbon01c_eurospeech",
    "psutka01b_eurospeech",
    "yablonsky01_eurospeech",
    "fotinea01_eurospeech"
   ]
  },
  {
   "title": "Resources, Assessment and Standards: Assessment Methodology",
   "papers": [
    "hone01_eurospeech",
    "chu01b_eurospeech",
    "strik01_eurospeech",
    "terashima01_eurospeech",
    "batusek01_eurospeech"
   ]
  },
  {
   "title": "Speech Recognition and Understanding: Confidence Measures",
   "papers": [
    "zhang01d_eurospeech",
    "moreno01_eurospeech",
    "charlet01_eurospeech",
    "palmer01_eurospeech",
    "carpenter01_eurospeech"
   ]
  },
  {
   "title": "Speech Recognition and Understanding: Language Modelling",
   "papers": [
    "nisimura01_eurospeech",
    "caseiro01_eurospeech",
    "varona01_eurospeech",
    "wu01c_eurospeech",
    "samuelsson01_eurospeech"
   ]
  },
  {
   "title": "Dialogue Systems: Techniques and Strategies",
   "papers": [
    "yan01_eurospeech",
    "huang01b_eurospeech",
    "haase01_eurospeech",
    "su01_eurospeech",
    "sansegundo01_eurospeech",
    "zhang01e_eurospeech",
    "matsusaka01_eurospeech",
    "terken01_eurospeech",
    "tseng01_eurospeech",
    "niimi01_eurospeech",
    "turunen01_eurospeech",
    "degerstedt01_eurospeech",
    "oppermann01_eurospeech",
    "schwarz01_eurospeech",
    "macherey01b_eurospeech",
    "zhang01f_eurospeech",
    "tsai01d_eurospeech",
    "ammicht01_eurospeech",
    "popovici01_eurospeech",
    "louloudis01_eurospeech",
    "lewin01_eurospeech",
    "cox01_eurospeech",
    "niklfeld01_eurospeech"
   ]
  },
  {
   "title": "Speech Synthesis: Miscellaneous",
   "papers": [
    "tsuzaki01_eurospeech",
    "lee01f_eurospeech",
    "kim01f_eurospeech",
    "fitt01b_eurospeech",
    "dines01_eurospeech",
    "sandri01_eurospeech",
    "xydas01_eurospeech",
    "rojc01_eurospeech",
    "hirose01_eurospeech",
    "tychtl01_eurospeech",
    "yoshimura01_eurospeech",
    "ohtsuka01_eurospeech",
    "karjalainen01_eurospeech",
    "wypych01_eurospeech"
   ]
  },
  {
   "title": "Integration of Phonetic Knowledge in Speech Technology: Experiments and Experiences (Special Session)",
   "papers": [
    "carsonberndsen01_eurospeech",
    "batliner01_eurospeech",
    "christensenyz01_eurospeech",
    "gravier01_eurospeech",
    "pastorigadea01_eurospeech"
   ]
  },
  {
   "title": "Speech Coding: Wideband Speech Coding",
   "papers": [
    "rotolapukkila01_eurospeech",
    "farrugia01_eurospeech",
    "fek01_eurospeech",
    "ritz01_eurospeech",
    "ragot01_eurospeech"
   ]
  },
  {
   "title": "Speech Recognition and Understanding: Robust ASR",
   "papers": [
    "rigazio01_eurospeech",
    "rose01_eurospeech",
    "afify01b_eurospeech",
    "padmanabhan01_eurospeech",
    "turunen01b_eurospeech"
   ]
  },
  {
   "title": "Applications: Miscellaneous Applications",
   "papers": [
    "garciamateo01_eurospeech",
    "rosenberg01_eurospeech",
    "koumpis01_eurospeech",
    "scharenborg01_eurospeech",
    "pastorigadea01b_eurospeech",
    "metze01_eurospeech",
    "martin01c_eurospeech",
    "kyung01_eurospeech",
    "alm01_eurospeech",
    "murray01_eurospeech",
    "iida01_eurospeech",
    "suzuki01_eurospeech",
    "castelli01_eurospeech",
    "draxler01_eurospeech"
   ]
  },
  {
   "title": "Signal Analysis: Pitch and Speech Analysis",
   "papers": [
    "chazan01_eurospeech",
    "shdaifat01_eurospeech",
    "hirtum01_eurospeech",
    "ishimoto01_eurospeech",
    "sasou01_eurospeech",
    "choi01_eurospeech",
    "cheveigne01_eurospeech",
    "ishi01_eurospeech",
    "kawahara01_eurospeech",
    "wojdel01_eurospeech",
    "schnell01_eurospeech",
    "ouni01b_eurospeech",
    "potamitis01b_eurospeech",
    "petrinovic01_eurospeech"
   ]
  },
  {
   "title": "Integration of Phonetic Knowledge in Speech Technology: Is Phonetic Knowledge any use? Panel discussion (Special Session)",
   "papers": [
    "greenberg01c_eurospeech"
   ]
  },
  {
   "title": "Speech Coding: Speech Transmission Systems",
   "papers": [
    "park01_eurospeech",
    "wijngaarden01b_eurospeech",
    "yoon01b_eurospeech",
    "perezcordoba01_eurospeech"
   ]
  },
  {
   "title": "Speech Recognition and Understanding: Rhythm and Timing in ASR",
   "papers": [
    "wrede01_eurospeech",
    "nanjo01_eurospeech",
    "fabian01_eurospeech",
    "farinas01_eurospeech"
   ]
  },
  {
   "title": "Speech Recognition and Understanding: Confidence Measures and OOV",
   "papers": [
    "zhang01g_eurospeech",
    "kodama01_eurospeech",
    "sansegundo01b_eurospeech",
    "mengusoglu01_eurospeech",
    "ferrer01_eurospeech",
    "tan01_eurospeech",
    "goel01_eurospeech",
    "jiang01b_eurospeech",
    "ogata01_eurospeech",
    "schaaf01_eurospeech",
    "bouwman01_eurospeech"
   ]
  },
  {
   "title": "Signal Analysis: Source Localisation and Beam Forming",
   "papers": [
    "sanchezbote01_eurospeech",
    "araki01b_eurospeech",
    "mukai01_eurospeech",
    "saruwatari01_eurospeech",
    "mizumachi01_eurospeech",
    "nishiura01_eurospeech",
    "alvarezmarquina01_eurospeech",
    "gomezvilda01_eurospeech",
    "martin01d_eurospeech",
    "kinnunen01_eurospeech",
    "nokas01_eurospeech",
    "couvreur01_eurospeech",
    "momomura01_eurospeech",
    "okuno01_eurospeech"
   ]
  },
  {
   "title": "Signal Analysis: Speech Features and Modelling",
   "papers": [
    "funaki01_eurospeech",
    "pitz01_eurospeech",
    "yu01b_eurospeech",
    "gonon01_eurospeech"
   ]
  },
  {
   "title": "Speech Recognition and Understanding: Kids, Toys and Emotions",
   "papers": [
    "lucke01_eurospeech",
    "li01b_eurospeech",
    "arunachalam01_eurospeech",
    "nogueiras01_eurospeech"
   ]
  },
  {
   "title": "Applications: Media Applications",
   "papers": [
    "ibrahim01b_eurospeech",
    "amaral01_eurospeech",
    "roach01_eurospeech",
    "horiuchi01_eurospeech"
   ]
  },
  {
   "title": "Speech Recognition and Understanding: Distributed Speech Recognition",
   "papers": [
    "bernard01_eurospeech",
    "peinado01_eurospeech",
    "stadermann01b_eurospeech",
    "riskin01_eurospeech"
   ]
  },
  {
   "title": "Speech Recognition and Understanding: Prosody and Cross-Language in ASR",
   "papers": [
    "schultz01_eurospeech",
    "zgank01_eurospeech",
    "harju01_eurospeech",
    "chengalvarayan01b_eurospeech",
    "chen01i_eurospeech",
    "wong01b_eurospeech",
    "stemmer01_eurospeech",
    "siu01b_eurospeech",
    "charnvivit01_eurospeech",
    "kim01g_eurospeech",
    "wang01d_eurospeech",
    "stephenson01_eurospeech",
    "chen01j_eurospeech",
    "wang01e_eurospeech",
    "iskra01_eurospeech",
    "batliner01b_eurospeech"
   ]
  },
  {
   "title": "Education: Education and Training",
   "papers": [
    "drygajlo01_eurospeech",
    "arai01_eurospeech",
    "cooke01_eurospeech",
    "chang01b_eurospeech",
    "jong01_eurospeech",
    "vicsi01_eurospeech",
    "nakamura01_eurospeech",
    "tanaka01_eurospeech"
   ]
  },
  {
   "title": "Speaker Recognition: Features and Robustness",
   "papers": [
    "arcienega01_eurospeech",
    "ezzaidi01_eurospeech",
    "kim01h_eurospeech",
    "mashao01_eurospeech",
    "miyajima01_eurospeech",
    "yadong01_eurospeech",
    "yoma01b_eurospeech",
    "yoshida01_eurospeech"
   ]
  }
 ],
 "doi": "10.21437/Eurospeech.2001"
}