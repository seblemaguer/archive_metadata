{
  "title": "Interspeech 2021",
  "location": "Brno, Czechia",
  "startDate": "30/08/2021",
  "endDate": "3/09/2021",
  "URL": "http://www.interspeech2021.org/",
  "chair": "General Chairs: Hynek He\u0159mansk\u00fd, Honza \u010cernock\u00fd; Technical Chairs: Luk\u00e1\u0161 Burget, Lori Lamel, Odette Scharenborg, Petr Motlicek",
  "conf": "Interspeech",
  "year": "2021",
  "name": "interspeech_2021",
  "series": "Interspeech",
  "SIG": "",
  "title1": "Interspeech 2021",
  "date": "30 August - 3 September 2021",
  "papers": {
    "pucher21_interspeech": {
      "authors": [
        [
          "Michael",
          "Pucher"
        ],
        [
          "Thomas",
          "Woltron"
        ]
      ],
      "title": "Conversion of Airborne to Bone-Conducted Speech with Deep Neural Networks",
      "original": "0473",
      "page_count": 5,
      "order": 1,
      "p1": "1",
      "pn": "5",
      "abstract": [
        "It is a common experience of most speakers that the playback of one&#8217;s\nown voice sounds strange. This can be mainly attributed to the missing\nbone-conducted speech signal that is not present in the playback signal.\nIt was also shown that some phonemes have a high bone-conducted relative\nto air-conducted sound transmission, which means that the bone-conduction\nfilter is phone-dependent. To achieve such a phone-dependent modeling\nwe train different speaker dependent and speaker adaptive speech conversion\nsystems using airborne and bone-conducted speech data from 8 speakers\n(5 male, 3 female), which allow for the conversion of airborne speech\nto bone-conducted speech. The systems are based on Long Short-Term\nMemory (LSTM) deep neural networks, where the speaker adaptive versions\nwith speaker embedding can be used without bone-conduction signals\nfrom the target speaker. Additionally we also used models that apply\na global filtering. The different models are then evaluated by an objective\nerror metric and a subjective listening experiment, which show that\nthe LSTM based models outperform the global filters.\n"
      ],
      "doi": "10.21437/Interspeech.2021-473",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "rezackova21_interspeech": {
      "authors": [
        [
          "Mark\u00e9ta",
          "\u0158ez\u00e1\u010dkov\u00e1"
        ],
        [
          "Jan",
          "\u0160vec"
        ],
        [
          "Daniel",
          "Tihelka"
        ]
      ],
      "title": "T5G2P: Using Text-to-Text Transfer Transformer for Grapheme-to-Phoneme Conversion",
      "original": "0546",
      "page_count": 5,
      "order": 2,
      "p1": "6",
      "pn": "10",
      "abstract": [
        "Despite the increasing popularity of end-to-end text-to-speech (TTS)\nsystems, the correct grapheme-to-phoneme (G2P) module is still a crucial\npart of those relying on a phonetic input. In this paper, we, therefore,\nintroduce a T5G2P model, a Text-to-Text Transfer Transformer (T5) neural\nnetwork model which is able to convert an input text sentence into\na phoneme sequence with a high accuracy. The evaluation of our trained\nT5 model is carried out on English and Czech, since there are different\nspecific properties of G2P, including homograph disambiguation, cross-word\nassimilation and irregular pronunciation of loanwords. The paper also\ncontains an analysis of a homographs issue in English and offers another\napproach to Czech phonetic transcription using the detection of pronunciation\nexceptions.\n"
      ],
      "doi": "10.21437/Interspeech.2021-546"
    },
    "perrotin21_interspeech": {
      "authors": [
        [
          "Olivier",
          "Perrotin"
        ],
        [
          "Hussein El",
          "Amouri"
        ],
        [
          "G\u00e9rard",
          "Bailly"
        ],
        [
          "Thomas",
          "Hueber"
        ]
      ],
      "title": "Evaluating the Extrapolation Capabilities of Neural Vocoders to Extreme Pitch Values",
      "original": "1547",
      "page_count": 5,
      "order": 3,
      "p1": "11",
      "pn": "15",
      "abstract": [
        "Neural vocoders are systematically evaluated on homogeneous train and\ntest databases. This kind of evaluation is efficient to compare neural\nvocoders in their &#8220;comfort zone&#8221;, yet it hardly reveals\ntheir limits towards unseen data during training. To compare their\nextrapolation capabilities, we introduce a methodology that aims at\nquantifying the robustness of neural vocoders in synthesising unseen\ndata, by precisely controlling the ranges of seen/unseen data in the\ntraining database. By focusing in this study on the pitch (F<SUB>0</SUB>)\nparameter, our methodology involves a careful splitting of a dataset\nto control which F<SUB>0</SUB> values are seen/unseen during training,\nfollowed by both global (utterance) and local (frame) evaluation of\nvocoders. Comparison of four types of vocoders (autoregressive, sourcefilter,\nflows, GAN) displays a wide range of behaviour towards unseen input\npitch values, including excellent extrapolation (WaveGlow); widely-spread\nF<SUB>0</SUB> errors (WaveRNN); and systematic generation of the training\nset median F<SUB>0</SUB> (LPCNet, Parallel WaveGAN). In contrast, fewer\ndifferences between vocoders were observed when using homogeneous train\nand test sets, thus demonstrating the potential and need for such evaluation\nto better discriminate the neural vocoders abilities to generate out-of-training-range\ndata.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1547",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "do21_interspeech": {
      "authors": [
        [
          "Phat",
          "Do"
        ],
        [
          "Matt",
          "Coler"
        ],
        [
          "Jelske",
          "Dijkstra"
        ],
        [
          "Esther",
          "Klabbers"
        ]
      ],
      "title": "A Systematic Review and Analysis of Multilingual Data Strategies in Text-to-Speech for Low-Resource Languages",
      "original": "1565",
      "page_count": 5,
      "order": 4,
      "p1": "16",
      "pn": "20",
      "abstract": [
        "We provide a systematic review of past studies that use multilingual\ndata for text-to-speech (TTS) of low-resource languages (LRLs). We\nfocus on the strategies used by these studies for incorporating multilingual\ndata and how they affect output speech quality. To investigate the\ndifference in output quality between corresponding monolingual and\nmultilingual models, we propose a novel measure to compare this difference\nacross the included studies and their various evaluation metrics. This\nmeasure, called the Multilingual Model Effect (MLME), is found to be\naffected by: acoustic model architecture, the difference ratio of target\nlanguage data between corresponding multilingual and monolingual experiments,\nthe balance ratio of target language data to total data, and the amount\nof target language data used. These findings can act as reference for\ndata strategies in future experiments with multilingual TTS models\nfor LRLs. Language family classification, despite being widely used,\nis not found to be an effective criterion for selecting source languages.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1565",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "talkar21_interspeech": {
      "authors": [
        [
          "Tanya",
          "Talkar"
        ],
        [
          "Nancy Pearl",
          "Solomon"
        ],
        [
          "Douglas S.",
          "Brungart"
        ],
        [
          "Stefanie E.",
          "Kuchinsky"
        ],
        [
          "Megan M.",
          "Eitel"
        ],
        [
          "Sara M.",
          "Lippa"
        ],
        [
          "Tracey A.",
          "Brickell"
        ],
        [
          "Louis M.",
          "French"
        ],
        [
          "Rael T.",
          "Lange"
        ],
        [
          "Thomas F.",
          "Quatieri"
        ]
      ],
      "title": "Acoustic Indicators of Speech Motor Coordination in Adults With and Without Traumatic Brain Injury",
      "original": "1581",
      "page_count": 5,
      "order": 5,
      "p1": "21",
      "pn": "25",
      "abstract": [
        "A traumatic brain injury (TBI) can lead to various long-term effects\non memory, attention, and mood, as well as the occurrence of headaches,\nspeech, and hearing problems. There is a need to better understand\nthe long-term effects of a TBI for objective tracking of an individual&#8217;s\nrecovery, which could be used to determine intervention trajectories.\nThis study utilizes acoustic features derived from recordings of speech\ntasks completed by active-duty service members and veterans (SMVs)\nenrolled in the Defense and Veterans Brain Injury (DVBIC)/Traumatic\nBrain Injury Center of Excellence (TBICoE) 15-Year Longitudinal TBI\nStudy. We hypothesize that the individuals diagnosed with moderate\nto severe TBI would demonstrate motor speech impairments through decreased\ncoordination of the speech production subsystems as compared to individuals\nwith no history of TBI. Speech motor coordination is measured through\ncorrelations of acoustic feature time series representing speech subsystems.\nEigenspectra derived from these correlations are utilized in machine\nlearning models to discriminate between the two groups. The fusion\nof correlation features derived from the recordings achieves an AUC\nof 0.78. This suggests that residual motor impairments from moderate\nto severe TBI could be detectable through objective measures of speech\nmotor coordination.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1581",
      "author_area_id": "13",
      "author_area_label": " Speech, Voice, and Hearing Disorders"
    },
    "vasquezcorrea21_interspeech": {
      "authors": [
        [
          "J.C.",
          "V\u00e1squez-Correa"
        ],
        [
          "Julian",
          "Fritsch"
        ],
        [
          "J.R.",
          "Orozco-Arroyave"
        ],
        [
          "Elmar",
          "N\u00f6th"
        ],
        [
          "Mathew",
          "Magimai-Doss"
        ]
      ],
      "title": "On Modeling Glottal Source Information for Phonation Assessment in Parkinson&#8217;s Disease",
      "original": "1084",
      "page_count": 5,
      "order": 6,
      "p1": "26",
      "pn": "30",
      "abstract": [
        "Parkinson&#8217;s disease produces several motor symptoms, including\ndifferent speech impairments that are known as hypokinetic dysarthria.\nSymptoms associated to dysarthria affect different dimensions of speech\nsuch as phonation, articulation, prosody, and intelligibility. Studies\nin the literature have mainly focused on the analysis of articulation\nand prosody because they seem to be the most prominent symptoms associated\nto dysarthria severity. However, phonation impairments also play a\nsignificant role to evaluate the global speech severity of Parkinson&#8217;s\npatients. This paper proposes an extensive comparison of different\nmethods to automatically evaluate the severity of specific phonation\nimpairments in Parkinson&#8217;s patients. The considered models include\nthe computation of perturbation and glottal-based features, in addition\nto features extracted from a zero frequency filtered signals. We consider\nas well end-to-end models based on 1D CNNs, which are trained to learn\nfeatures from the raw speech waveform, reconstructed glottal signals,\nand zero-frequency filtered signals. The results indicate that it is\npossible to automatically classify between speakers with low versus\nhigh phonation severity due to the presence of dysarthria and at the\nsame time to evaluate the severity of the phonation impairments on\na continuous scale, posed as a regression problem.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1084",
      "author_area_id": "13",
      "author_area_label": " Speech, Voice, and Hearing Disorders"
    },
    "daoudi21_interspeech": {
      "authors": [
        [
          "Khalid",
          "Daoudi"
        ],
        [
          "Biswajit",
          "Das"
        ],
        [
          "Solange Milh\u00e9 de Saint",
          "Victor"
        ],
        [
          "Alexandra",
          "Foubert-Samier"
        ],
        [
          "Anne Pavy-Le",
          "Traon"
        ],
        [
          "Olivier",
          "Rascol"
        ],
        [
          "Wassilios G.",
          "Meissner"
        ],
        [
          "Virginie",
          "Woisard"
        ]
      ],
      "title": "Distortion of Voiced Obstruents for Differential Diagnosis Between Parkinson&#8217;s Disease and Multiple System Atrophy",
      "original": "0223",
      "page_count": 5,
      "order": 7,
      "p1": "31",
      "pn": "35",
      "abstract": [
        "Parkinson&#8217;s disease (PD) and the parkinsonian variant of Multiple\nSystem Atrophy (MSA-P) are two neurodegenerative diseases which share\nsimilar clinical features, particularly in early disease stages. The\ndifferential diagnosis can be thus very challenging. Dysarthria is\nknown to be a frequent and early clinical feature of PD and MSA. It\ncan be thus used as a vehicle to provide a vocal biomarker which could\nhelp in the differential diagnosis. In particular, distortion of consonants\nis known to be a frequent impairment in these diseases. The aim of\nthis study is to investigate distinctive patterns in the distortion\nof voiced obstruents (plosives and fricatives). It is the first study\nwhich attempts to examine such distortions in the French language for\nthe purpose of the differential diagnosis between PD and MSA-P (and\namong the very few studies if we consider all languages). We carry\nout a perceptual and objective analysis of voiced obstruents extracted\nfrom isolated pseudo-words initials. We first show that devoicing is\na significant impairment which predominates in MSA-P. We then show\nthat voice onset time (VOT) of voiced plosives (prevoicing duration)\ncan be a complementary feature to improve the accuracy in discrimination\nbetween PD and MSA-P.\n"
      ],
      "doi": "10.21437/Interspeech.2021-223",
      "author_area_id": "13",
      "author_area_label": " Speech, Voice, and Hearing Disorders"
    },
    "wang21_interspeech": {
      "authors": [
        [
          "Pu",
          "Wang"
        ],
        [
          "Bagher",
          "BabaAli"
        ],
        [
          "Hugo",
          "Van hamme"
        ]
      ],
      "title": "A Study into Pre-Training Strategies for Spoken Language Understanding on Dysarthric Speech",
      "original": "1720",
      "page_count": 5,
      "order": 8,
      "p1": "36",
      "pn": "40",
      "abstract": [
        "End-to-end (E2E) spoken language understanding (SLU) systems avoid\nan intermediate textual representation by mapping speech directly into\nintents with slot values. This approach requires considerable domain-specific\ntraining data. In low-resource scenarios this is a major concern, e.g.,\nin the present study dealing with SLU for dysarthric speech. Pretraining\npart of the SLU model for automatic speech recognition targets helps\nbut no research has shown to which extent SLU on dysarthric speech\nbenefits from knowledge transferred from other dysarthric speech tasks.\nThis paper investigates the efficiency of pre-training strategies for\nSLU tasks on dysarthric speech. The designed SLU system consists of\na TDNN acoustic model for feature encoding and a capsule network for\nintent and slot decoding. The acoustic model is pre-trained in two\nstages: initialization with a corpus of normal speech and finetuning\non a mixture of dysarthric and normal speech. By introducing the intelligibility\nscore as a metric of the impairment severity, this paper quantitatively\nanalyzes the relation between generalization and pathology severity\nfor dysarthric speech.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1720",
      "author_area_id": "13",
      "author_area_label": " Speech, Voice, and Hearing Disorders"
    },
    "turrisi21_interspeech": {
      "authors": [
        [
          "Rosanna",
          "Turrisi"
        ],
        [
          "Arianna",
          "Braccia"
        ],
        [
          "Marco",
          "Emanuele"
        ],
        [
          "Simone",
          "Giulietti"
        ],
        [
          "Maura",
          "Pugliatti"
        ],
        [
          "Mariachiara",
          "Sensi"
        ],
        [
          "Luciano",
          "Fadiga"
        ],
        [
          "Leonardo",
          "Badino"
        ]
      ],
      "title": "EasyCall Corpus: A Dysarthric Speech Dataset",
      "original": "0549",
      "page_count": 5,
      "order": 9,
      "p1": "41",
      "pn": "45",
      "abstract": [
        "This paper introduces a new dysarthric speech command dataset in Italian,\ncalled EasyCall corpus. The dataset consists of 21386 audio recordings\nfrom 24 healthy and 31 dysarthric speakers, whose individual degree\nof speech impairment was assessed by neurologists through the Therapy\nOutcome Measure. The corpus aims at providing a resource for the development\nof ASR-based assistive technologies for patients with dysarthria. In\nparticular, it may be exploited to develop a voice-controlled contact\napplication for commercial smartphones, aiming at improving dysarthric\npatients&#8217; ability to communicate with their family and caregivers.\nBefore recording the dataset, participants were administered a survey\nto evaluate which commands are more likely to be employed by dysarthric\nindividuals in a voice-controlled contact application. In addition,\nthe dataset includes a list of non-commands (i.e., words near/inside\ncommands or phonetically close to commands) that can be leveraged to\nbuild a more robust command recognition system. At present commercial\nASR systems perform poorly on the EasyCall Corpus as we report in this\npaper. This result corroborates the need for dysarthric speech corpora\nfor developing effective assistive technologies. To the best of our\nknowledge, this database represents the richest corpus of dysarthric\nspeech to date.\n"
      ],
      "doi": "10.21437/Interspeech.2021-549",
      "author_area_id": "13",
      "author_area_label": " Speech, Voice, and Hearing Disorders"
    },
    "bie21_interspeech": {
      "authors": [
        [
          "Xiaoyu",
          "Bie"
        ],
        [
          "Laurent",
          "Girin"
        ],
        [
          "Simon",
          "Leglaive"
        ],
        [
          "Thomas",
          "Hueber"
        ],
        [
          "Xavier",
          "Alameda-Pineda"
        ]
      ],
      "title": "A Benchmark of Dynamical Variational Autoencoders Applied to Speech Spectrogram Modeling",
      "original": "0256",
      "page_count": 5,
      "order": 10,
      "p1": "46",
      "pn": "50",
      "abstract": [
        "The Variational Autoencoder (VAE) is a powerful deep generative model\nthat is now extensively used to represent high-dimensional complex\ndata via a low-dimensional latent space learned in an unsupervised\nmanner. In the original VAE model, input data vectors are processed\nindependently. In recent years, a series of papers have presented different\nextensions of the VAE to process sequential data, that not only model\nthe latent space, but also model the temporal dependencies within a\nsequence of data vectors and corresponding latent vectors, relying\non recurrent neural networks. We recently performed a comprehensive\nreview of those models and unified them into a general class called\nDynamical Variational Autoencoders (DVAEs). In the present paper, we\npresent the results of an experimental benchmark comparing six of those\nDVAE models on the speech analysis-resynthesis task, as an illustration\nof the high potential of DVAEs for speech modeling.\n"
      ],
      "doi": "10.21437/Interspeech.2021-256",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "yurt21_interspeech": {
      "authors": [
        [
          "Metehan",
          "Yurt"
        ],
        [
          "Pavan",
          "Kantharaju"
        ],
        [
          "Sascha",
          "Disch"
        ],
        [
          "Andreas",
          "Niedermeier"
        ],
        [
          "Alberto N.",
          "Escalante-B"
        ],
        [
          "Veniamin I.",
          "Morgenshtern"
        ]
      ],
      "title": "Fricative Phoneme Detection Using Deep Neural Networks and its Comparison to Traditional Methods",
      "original": "0645",
      "page_count": 5,
      "order": 11,
      "p1": "51",
      "pn": "55",
      "abstract": [
        "Accurate phoneme detection and processing can enhance speech intelligibility\nin hearing aids and audio &amp; speech codecs. As fricative phonemes\nhave an important part of their energy concentrated in high frequency\nbands, frequency lowering algorithms are used in hearing aids to improve\nfricative intelligibility for people with high-frequency hearing loss.\nIn traditional audio codecs, while processing speech in blocks, spectral\nsmearing around fricative phoneme borders results in pre and post echo\nartifacts. Hence, detecting the fricative borders and adapting the\nprocessing accordingly could enhance the quality of speech. Until recently,\nphoneme detection and analysis were mostly done by extracting features\nspecific to the class of phonemes. In this paper, we present a deep\nlearning based fricative phoneme detection algorithm that exceeds the\nstate-of-the-art fricative phoneme detection accuracy on the TIMIT\nspeech corpus. Moreover, we compare our method to other approaches\nthat employ classical signal processing for fricative detection and\nalso evaluate it on the TIMIT files coded with AAC codec followed by\nbandwidth limitation. Reported results of our deep learning approach\non original TIMIT files are reproducible and come with an easy to use\ncode that could serve as a baseline for any future research on this\ntopic.\n"
      ],
      "doi": "10.21437/Interspeech.2021-645",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "prasad21_interspeech": {
      "authors": [
        [
          "RaviShankar",
          "Prasad"
        ],
        [
          "Mathew",
          "Magimai-Doss"
        ]
      ],
      "title": "Identification of F1 and F2 in Speech Using Modified Zero Frequency Filtering",
      "original": "1598",
      "page_count": 5,
      "order": 12,
      "p1": "56",
      "pn": "60",
      "abstract": [
        "Formants are major resonances in the vocal tract system. Identification\nof formants is important for study of speech. In the literature, formants\nare typically identified by first deriving formant frequency candidates\n(e.g., using linear prediction) and then applying a tracking mechanism.\nIn this paper, we propose a simple tracking-free formant identification\napproach based on zero frequency filtering. More precisely, formants\nF1-F2 are identified by modifying the trend removal operation in zero\nfrequency filtering and picking simply the dominant peak in the short-term\ndiscrete Fourier transform spectra. We demonstrate the potential of\nthe approach by comparing it against state-of-the-art formant identification\napproaches on a typical speech data set (TIMIT-VTR) and an atypical\nspeech data set (PC-GITA).\n"
      ],
      "doi": "10.21437/Interspeech.2021-1598",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "teytaut21_interspeech": {
      "authors": [
        [
          "Yann",
          "Teytaut"
        ],
        [
          "Axel",
          "Roebel"
        ]
      ],
      "title": "Phoneme-to-Audio Alignment with Recurrent Neural Networks for Speaking and Singing Voice",
      "original": "1676",
      "page_count": 5,
      "order": 13,
      "p1": "61",
      "pn": "65",
      "abstract": [
        "Phoneme-to-audio alignment is the task of synchronizing voice recordings\nand their related phonetic transcripts. In this work, we introduce\na new system to forced phonetic alignment with Recurrent Neural Networks\n(RNN). With the Connectionist Temporal Classification (CTC) loss as\ntraining objective, and an additional reconstruction cost, we learn\nto infer relevant per-frame phoneme probabilities from which alignment\nis derived. The core of the neural architecture is a context-aware\nattention mechanism between mel-spectrograms and side information.\nWe investigate two contexts given by either phoneme sequences (model\n PhAtt) or spectrograms themselves (model  SpAtt). Evaluations show\nthat these models produce precise alignments for both speaking and\nsinging voice. Best results are obtained with the model  PhAtt, which\noutperforms baseline reference with an average imprecision of 16.3ms\nand 29.8ms on speech and singing, respectively. The model  SpAtt also\nappears as an interesting alternative, capable of aligning longer audio\nfiles without requiring phoneme sequences on small audio segments.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1676",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "kim21_interspeech": {
      "authors": [
        [
          "Seong-Hu",
          "Kim"
        ],
        [
          "Yong-Hwa",
          "Park"
        ]
      ],
      "title": "Adaptive Convolutional Neural Network for Text-Independent Speaker Recognition",
      "original": "0065",
      "page_count": 5,
      "order": 14,
      "p1": "66",
      "pn": "70",
      "abstract": [
        "In text-independent speaker recognition, each speech is composed of\ndifferent phonemes depending on spoken text. The conventional neural\nnetworks for speaker recognition are static models, so they do not\nreflect this phoneme-varying characteristic well. To tackle this limitation,\nwe propose an adaptive convolutional neural network (ACNN) for text-independent\nspeaker recognition. The utterance is divided along the time axis into\nshort segments with small fluctuating phonemes. Frame-level features\nare extracted by applying input-dependent kernels adaptive to each\nsegment. By applying time average pooling and linear layers, utterance-level\nembeddings extraction and speaker recognition are performed. Adaptive\nVGG-M using 0.356 seconds segmentation shows better speaker recognition\nperformance than baseline models, with a Top-1 of 86.51% and an EER\nof 5.68%. It extracts more accurate frame-level embeddings for vowel\nand nasal phonemes compared to the conventional method without overfitting\nand large parameters. This framework for text-independent speaker recognition\neffectively utilizes phonemes and text-varying characteristic of speech.\n"
      ],
      "doi": "10.21437/Interspeech.2021-65",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "qi21_interspeech": {
      "authors": [
        [
          "Jiajun",
          "Qi"
        ],
        [
          "Wu",
          "Guo"
        ],
        [
          "Bin",
          "Gu"
        ]
      ],
      "title": "Bidirectional Multiscale Feature Aggregation for Speaker Verification",
      "original": "0111",
      "page_count": 5,
      "order": 15,
      "p1": "71",
      "pn": "75",
      "abstract": [
        "In this paper, we propose a novel bidirectional multiscale feature\naggregation (BMFA) network with attentional fusion modules for text-independent\nspeaker verification. The feature maps from different stages of the\nbackbone network are iteratively combined and refined in both a bottom-up\nand top-down manner. Furthermore, instead of simple concatenation or\nelementwise addition of feature maps from different stages, an attentional\nfusion module is designed to compute the fusion weights. Experiments\nare conducted on the NIST SRE16 and VoxCeleb1 datasets. The experimental\nresults demonstrate the effectiveness of the bidirectional aggregation\nstrategy and show that the proposed attentional fusion module can further\nimprove the performance.\n"
      ],
      "doi": "10.21437/Interspeech.2021-111",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "zhang21_interspeech": {
      "authors": [
        [
          "Yu-Jia",
          "Zhang"
        ],
        [
          "Yih-Wen",
          "Wang"
        ],
        [
          "Chia-Ping",
          "Chen"
        ],
        [
          "Chung-Li",
          "Lu"
        ],
        [
          "Bo-Cheng",
          "Chan"
        ]
      ],
      "title": "Improving Time Delay Neural Network Based Speaker Recognition with Convolutional Block and Feature Aggregation Methods",
      "original": "0356",
      "page_count": 5,
      "order": 16,
      "p1": "76",
      "pn": "80",
      "abstract": [
        "In this paper, we develop a system that integrates multiple ideas and\ntechniques inspired by the convolutional block and feature aggregation\nmethods. We begin with the state-of-the-art speaker-embedding model\nfor speaker recognition, namely the model of Emphasized Channel Attention,\nPropagation, and Aggregation in Time Delay Neural Network, and then\ngradually experiment with the proposed network modules, including bottleneck\nresidual blocks, attention mechanisms, and feature aggregation methods.\nIn our final model, we replace the Res2Block with SC-Block and we use\na hierarchical architecture for feature aggregation. We evaluate the\nperformance of our model on the VoxCeleb1 test set and the 2020 VoxCeleb\nSpeaker Recognition Challenge (VoxSRC20) validation set. The relative\nimprovement of the proposed models over ECAPA-TDNN is 22.8% on VoxCeleb1\nand 18.2% on VoxSRC20.\n"
      ],
      "doi": "10.21437/Interspeech.2021-356",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "wu21_interspeech": {
      "authors": [
        [
          "Yanfeng",
          "Wu"
        ],
        [
          "Junan",
          "Zhao"
        ],
        [
          "Chenkai",
          "Guo"
        ],
        [
          "Jing",
          "Xu"
        ]
      ],
      "title": "Improving Deep CNN Architectures with Variable-Length Training Samples for Text-Independent Speaker Verification",
      "original": "0559",
      "page_count": 5,
      "order": 17,
      "p1": "81",
      "pn": "85",
      "abstract": [
        "Deep Convolutional Neural Network (CNN) based speaker embeddings, such\nas r-vectors, have shown great success in text-independent speaker\nverification (TI-SV) task. However, previous deep CNN models usually\nuse fixed-length samples for training and employ variable-length utterances\nfor speaker embeddings, which generates a mismatch between training\nand embedding. To address this issue, we investigate the effect of\nemploying variable-length training samples on CNN-based TI-SV systems\nand explore two approaches to improve the performance of deep CNN architectures\non TI-SV through capturing variable-term contexts. Firstly, we present\nan improved selective kernel convolution which allows the networks\nto adaptively switch between short-term and long-term contexts based\non variable-length utterances. Secondly, we propose a multi-scale statistics\npooling method to aggregate multiple time-scale features from different\nlayers of the networks. We build a novel ResNet34 based architecture\nwith two proposed approaches. Experiments are conducted on the VoxCeleb\ndatasets. The results demonstrate that the effect of using variable-length\nsamples is diverse in different networks and the architecture with\ntwo proposed approaches achieves significant improvement over r-vectors\nbaseline system.\n"
      ],
      "doi": "10.21437/Interspeech.2021-559",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "zhu21_interspeech": {
      "authors": [
        [
          "Tinglong",
          "Zhu"
        ],
        [
          "Xiaoyi",
          "Qin"
        ],
        [
          "Ming",
          "Li"
        ]
      ],
      "title": "Binary Neural Network for Speaker Verification",
      "original": "0600",
      "page_count": 5,
      "order": 18,
      "p1": "86",
      "pn": "90",
      "abstract": [
        "Although deep neural networks are successful for many tasks in the\nspeech domain, the high computational and memory costs of deep neural\nnetworks make it difficult to directly deploy high-performance Neural\nNetwork systems on low-resource embedded devices. There are several\nmechanisms to reduce the size of the neural networks i.e. parameter\npruning, parameter quantization, etc. This paper focuses on how to\napply binary neural networks to the task of speaker verification. The\nproposed binarization of training parameters can largely maintain the\nperformance while significantly reducing storage space requirements\nand computational costs. Experiment results show that, after binarizing\nthe Convolutional Neural Network, the ResNet34-based network achieves\nan EER of around 5% on the Voxceleb1 testing dataset and even outperforms\nthe traditional real number network on the text-dependent dataset:\nXiaole while having a 32&#215; memory saving.\n"
      ],
      "doi": "10.21437/Interspeech.2021-600",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "tu21_interspeech": {
      "authors": [
        [
          "Youzhi",
          "Tu"
        ],
        [
          "Man-Wai",
          "Mak"
        ]
      ],
      "title": "Mutual Information Enhanced Training for Speaker Embedding",
      "original": "1436",
      "page_count": 5,
      "order": 19,
      "p1": "91",
      "pn": "95",
      "abstract": [
        "Mutual information (MI) is useful in unsupervised and self-supervised\nlearning. Maximizing the MI between the low-level features and the\nlearned embeddings can preserve meaningful information in the embeddings,\nwhich can contribute to performance gains. This strategy is called\ndeep InfoMax (DIM) in representation learning. In this paper, we follow\nthe DIM framework so that the speaker embeddings can capture more information\nfrom the frame-level features. However, a straightforward implementation\nof DIM may pose a dimensionality imbalance problem because the dimensionality\nof the frame-level features is much larger than that of the speaker\nembeddings. This problem can lead to unreliable MI estimation and can\neven cause detrimental effects on speaker verification. To overcome\nthis problem, we propose to squeeze the frame-level features before\nMI estimation through some global pooling methods. We call the proposed\nmethod squeeze-DIM. Although the squeeze operation inevitably introduces\nsome information loss, we empirically show that the squeeze-DIM can\nachieve performance gains on both Voxceleb1 and VOiCES-19 tasks. This\nsuggests that the squeeze operation facilitates the MI estimation and\nmaximization in a balanced dimensional space, which helps learn more\ninformative speaker embeddings.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1436",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "zhu21b_interspeech": {
      "authors": [
        [
          "Ge",
          "Zhu"
        ],
        [
          "Fei",
          "Jiang"
        ],
        [
          "Zhiyao",
          "Duan"
        ]
      ],
      "title": "Y-Vector: Multiscale Waveform Encoder for Speaker Embedding",
      "original": "1707",
      "page_count": 5,
      "order": 20,
      "p1": "96",
      "pn": "100",
      "abstract": [
        "State-of-the-art text-independent speaker verification systems typically\nuse cepstral features or filter bank energies as speech features. Recent\nstudies attempted to extract speaker embeddings directly from raw waveforms\nand have shown competitive results. In this paper, we propose a novel\nmulti-scale waveform encoder that uses three convolution branches with\ndifferent time scales to compute speech features from the waveform.\nThese features are then processed by squeeze-and-excitation blocks,\na multi-level feature aggregator, and a time delayed neural network\n(TDNN) to compute speaker embedding. We show that the proposed embeddings\noutperform existing raw-waveform-based speaker embeddings on speaker\nverification by a large margin. A further analysis of the learned filters\nshows that the multi-scale encoder attends to different frequency bands\nat its different scales while resulting in a more flat overall frequency\nresponse than any of the single-scale counterparts.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1707",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "liu21_interspeech": {
      "authors": [
        [
          "Yan",
          "Liu"
        ],
        [
          "Zheng",
          "Li"
        ],
        [
          "Lin",
          "Li"
        ],
        [
          "Qingyang",
          "Hong"
        ]
      ],
      "title": "Phoneme-Aware and Channel-Wise Attentive Learning for Text Dependent Speaker Verification",
      "original": "2137",
      "page_count": 5,
      "order": 21,
      "p1": "101",
      "pn": "105",
      "abstract": [
        "This paper proposes a multi-task learning network with phoneme-aware\nand channel-wise attentive learning strategies for text-dependent Speaker\nVerification (SV). In the proposed structure, the frame-level multi-task\nlearning along with the segment-level adversarial learning is adopted\nfor speaker embedding extraction. The phoneme-aware attentive pooling\nis exploited on frame-level features in the main network for speaker\nclassifier, with the corresponding posterior probability for the phoneme\ndistribution in the auxiliary subnet. Further, the introduction of\nSqueeze and Excitation (SE-block) performs dynamic channel-wise feature\nrecalibration, which improves the representational ability. The proposed\nmethod exploits speaker idiosyncrasies associated with pass-phrases,\nand is further improved by the phoneme-aware attentive pooling and\nSE-block from temporal and channel-wise aspects, respectively. The\nexperiments conducted on RSR2015 Part 1 database confirm that the proposed\nsystem achieves outstanding results for text-dependent SV.\n"
      ],
      "doi": "10.21437/Interspeech.2021-2137",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "zhu21c_interspeech": {
      "authors": [
        [
          "Hongning",
          "Zhu"
        ],
        [
          "Kong Aik",
          "Lee"
        ],
        [
          "Haizhou",
          "Li"
        ]
      ],
      "title": "Serialized Multi-Layer Multi-Head Attention for Neural Speaker Embedding",
      "original": "2210",
      "page_count": 5,
      "order": 22,
      "p1": "106",
      "pn": "110",
      "abstract": [
        "This paper proposes a serialized multi-layer multi-head attention for\nneural speaker embedding in text-independent speaker verification.\nIn prior works, frame-level features from one layer are aggregated\nto form an utterance-level representation. Inspired by the Transformer\nnetwork, our proposed method utilizes the hierarchical architecture\nof stacked self-attention mechanisms to derive refined features that\nare more correlated with speakers. Serialized attention mechanism contains\na stack of self-attention modules to create fixed-dimensional representations\nof speakers. Instead of utilizing multi-head attention in parallel,\nthe proposed serialized multi-layer multi-head attention is designed\nto aggregate and propagate attentive statistics from one layer to the\nnext in a serialized manner. In addition, we employ an input-aware\nquery for each utterance with the statistics pooling. With more layers\nstacked, the neural network can learn more discriminative speaker embeddings.\nExperiment results on VoxCeleb1 dataset and SITW dataset show that\nour proposed method outperforms other baseline methods, including x-vectors\nand other x-vectors + conventional attentive pooling approaches by\n9.7% in EER and 8.1% in DCF10<SUP>-2</SUP>.\n"
      ],
      "doi": "10.21437/Interspeech.2021-2210",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "gong21_interspeech": {
      "authors": [
        [
          "Cheng",
          "Gong"
        ],
        [
          "Longbiao",
          "Wang"
        ],
        [
          "Ju",
          "Zhang"
        ],
        [
          "Shaotong",
          "Guo"
        ],
        [
          "Yuguang",
          "Wang"
        ],
        [
          "Jianwu",
          "Dang"
        ]
      ],
      "title": "TacoLPCNet: Fast and Stable TTS by Conditioning LPCNet on Mel Spectrogram Predictions",
      "original": "0852",
      "page_count": 5,
      "order": 23,
      "p1": "111",
      "pn": "115",
      "abstract": [
        "The combination of the recently proposed LPCNet vocoder and a seq-to-seq\nacoustic model, i.e., Tacotron, has successfully achieved lightweight\nspeech synthesis systems. However, the quality of synthesized speech\nis often unstable because the precision of the pitch parameters predicted\nby acoustic models is insufficient, especially for some tonal languages\nlike Chinese and Japanese. In this paper, we propose an end-to-end\nspeech synthesis system, TacoLPCNet, by conditioning LPCNet on Mel\nspectrogram predictions. First, we extend LPCNet for the Mel spectrogram\ninstead of using explicit pitch information and pitch-related network.\nFurthermore, we optimize the system by model pruning, multi-frame inference,\nand increasing frame length, to enable it to meet the conditions required\nfor real-time applications. The objective and subjective evaluation\nresults for various languages show that the proposed system is more\nstable for tonal languages within the proposed optimization strategies.\nThe experimental results also verify that our model improves synthesis\nruntime by 3.12 times than that of the baseline on a standard CPU while\nmaintaining naturalness.\n"
      ],
      "doi": "10.21437/Interspeech.2021-852",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "bak21_interspeech": {
      "authors": [
        [
          "Taejun",
          "Bak"
        ],
        [
          "Jae-Sung",
          "Bae"
        ],
        [
          "Hanbin",
          "Bae"
        ],
        [
          "Young-Ik",
          "Kim"
        ],
        [
          "Hoon-Young",
          "Cho"
        ]
      ],
      "title": "FastPitchFormant: Source-Filter Based Decomposed Modeling for Speech Synthesis",
      "original": "0866",
      "page_count": 5,
      "order": 24,
      "p1": "116",
      "pn": "120",
      "abstract": [
        "Methods for modeling and controlling prosody with acoustic features\nhave been proposed for neural text-to-speech (TTS) models. Prosodic\nspeech can be generated by conditioning acoustic features. However,\nsynthesized speech with a large pitch-shift scale suffers from audio\nquality degradation, and speaker characteristics deformation. To address\nthis problem, we propose a feed-forward Transformer based TTS model\nthat is designed based on the source-filter theory. This model, called\n<i>FastPitchFormant</i>, has a unique structure that handles text and\nacoustic features in parallel. With modeling each feature separately,\nthe tendency that the model learns the relationship between two features\ncan be mitigated. Owing to its structural characteristics, FastPitchFormant\nis robust and accurate for pitch control and generates prosodic speech\npreserving speaker characteristics. The experimental results show that\nproposed model outperforms the baseline FastPitch.\n"
      ],
      "doi": "10.21437/Interspeech.2021-866",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "nakamura21_interspeech": {
      "authors": [
        [
          "Taiki",
          "Nakamura"
        ],
        [
          "Tomoki",
          "Koriyama"
        ],
        [
          "Hiroshi",
          "Saruwatari"
        ]
      ],
      "title": "Sequence-to-Sequence Learning for Deep Gaussian Process Based Speech Synthesis Using Self-Attention GP Layer",
      "original": "0896",
      "page_count": 5,
      "order": 25,
      "p1": "121",
      "pn": "125",
      "abstract": [
        "This paper presents a speech synthesis method based on deep Gaussian\nprocess (DGP) and sequence-to-sequence (Seq2Seq) learning toward high-quality\nend-to-end speech synthesis. Feed-forward and recurrent models using\nDGP are known to produce more natural synthetic speech than deep neural\nnetworks (DNNs) because of Bayesian learning and kernel regression.\nHowever, such DGP models consist of a pipeline architecture of independent\nmodels, acoustic and duration models, and require a high level of expertise\nin text processing. The proposed model is based on Seq2Seq learning,\nwhich enables a unified training of acoustic and duration models. The\nencoder and decoder layers are represented by Gaussian process regressions\n(GPRs) and the parameters are trained as a Bayesian model. We also\npropose a self-attention mechanism with Gaussian processes to effectively\nmodel character-level input in the encoder. The subjective evaluation\nresults show that the proposed Seq2Seq-SA-DGP can synthesize more natural\nspeech than DNNs with self-attention and recurrent structures. Besides,\nSeq2Seq-SA-DGP reduces the smoothing problems of recurrent structures\nand is effective when a simple input for an end-to-end system is given.\n"
      ],
      "doi": "10.21437/Interspeech.2021-896",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "kakegawa21_interspeech": {
      "authors": [
        [
          "Naoto",
          "Kakegawa"
        ],
        [
          "Sunao",
          "Hara"
        ],
        [
          "Masanobu",
          "Abe"
        ],
        [
          "Yusuke",
          "Ijima"
        ]
      ],
      "title": "Phonetic and Prosodic Information Estimation from Texts for Genuine Japanese End-to-End Text-to-Speech",
      "original": "0914",
      "page_count": 5,
      "order": 26,
      "p1": "126",
      "pn": "130",
      "abstract": [
        "The biggest obstacle to develop end-to-end Japanese text-to-speech\n(TTS) systems is to estimate phonetic and prosodic information (PPI)\nfrom Japanese texts. The following are the reasons: (1) the Kanji characters\nof the Japanese writing system have multiple corresponding pronunciations,\n(2) there is no separation mark between words, and (3) an accent nucleus\nmust be assigned at appropriate positions. In this paper, we propose\nto solve the problems by neural machine translation (NMT) on the basis\nof encoder-decoder models, and compare NMT models of recurrent neural\nnetworks and the Transformer architecture. The proposed model handles\ntexts on token (character) basis, although conventional systems handle\nthem on word basis. To ensure the potential of the proposed approach,\nNMT models are trained using pairs of sentences and their PPIs that\nare generated by a conventional Japanese TTS system from 5 million\nsentences. Evaluation experiments were performed using PPIs that are\nmanually annotated for 5,142 sentences. The experimental results showed\nthat the Transformer architecture has the best performance, with 98.0%\naccuracy for phonetic information estimation and 95.0% accuracy for\nPPI estimation. Judging from the results, NMT models are promising\ntoward end-to-end Japanese TTS.\n"
      ],
      "doi": "10.21437/Interspeech.2021-914"
    },
    "dai21_interspeech": {
      "authors": [
        [
          "Xudong",
          "Dai"
        ],
        [
          "Cheng",
          "Gong"
        ],
        [
          "Longbiao",
          "Wang"
        ],
        [
          "Kaili",
          "Zhang"
        ]
      ],
      "title": "Information Sieve: Content Leakage Reduction in End-to-End Prosody Transfer for Expressive Speech Synthesis",
      "original": "1011",
      "page_count": 5,
      "order": 27,
      "p1": "131",
      "pn": "135",
      "abstract": [
        "Expressive neural text-to-speech (TTS) systems incorporate a style\nencoder to learn a latent embedding as the style information. However,\nthis embedding process may encode redundant textual information. This\nphenomenon is called content leakage. Researchers have attempted to\nresolve this problem by adding an ASR or other auxiliary supervision\nloss functions. In this study, we propose an unsupervised method called\nthe &#8220;information sieve&#8221; to reduce the effect of content\nleakage in prosody transfer. The rationale of this approach is that\nthe style encoder can be forced to focus on style information rather\nthan on textual information contained in the reference speech by a\nwell-designed downsample-upsample filter, i.e., the extracted style\nembeddings can be downsampled at a certain interval and then upsampled\nby duplication. Furthermore, we used instance normalization in convolution\nlayers to help the system learn a better latent style space. Objective\nmetrics such as the significantly lower word error rate (WER) demonstrate\nthe effectiveness of this model in mitigating content leakage. Listening\ntests indicate that the model retains its prosody transferability compared\nwith the baseline models such as the original GST-Tacotron and ASR-guided\nTacotron.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1011",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "dou21_interspeech": {
      "authors": [
        [
          "Qingyun",
          "Dou"
        ],
        [
          "Xixin",
          "Wu"
        ],
        [
          "Moquan",
          "Wan"
        ],
        [
          "Yiting",
          "Lu"
        ],
        [
          "Mark J.F.",
          "Gales"
        ]
      ],
      "title": "Deliberation-Based Multi-Pass Speech Synthesis",
      "original": "1405",
      "page_count": 5,
      "order": 28,
      "p1": "136",
      "pn": "140",
      "abstract": [
        "Sequence-to-sequence (seq2seq) models have achieved state-of-the-art\nperformance in a wide range of tasks including Neural Machine Translation\n(NMT) and Text-To-Speech (TTS). These models are usually trained with\nteacher forcing, where the reference back-history is used to predict\nthe next token. This makes training efficient, but limits performance,\nbecause during inference the free-running back-history must be used.\nTo address this problem, deliberation-based multi-pass seq2seq has\nbeen used in NMT. Here the output sequence is generated in multiple\npasses, each one conditioned on the initial input and the free-running\noutput of the previous pass. This paper investigates, and compares,\ndeliberation-based multi-pass seq2seq for TTS and NMT. For NMT the\nsimplest form of multi-pass approaches, where the free-running first-pass\noutput is combined with the initial input, improves performance. However,\napplying this scheme to TTS is challenging: the multi-pass model tends\nto converge to the standard single-pass model, ignoring the previous\noutput. To tackle this issue, a guided attention loss is added, enabling\nthe system to make more extensive use of the free-running output. Experimental\nresults confirm the above analysis and demonstrate that the proposed\nTTS model outperforms a strong baseline.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1405",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "elias21_interspeech": {
      "authors": [
        [
          "Isaac",
          "Elias"
        ],
        [
          "Heiga",
          "Zen"
        ],
        [
          "Jonathan",
          "Shen"
        ],
        [
          "Yu",
          "Zhang"
        ],
        [
          "Ye",
          "Jia"
        ],
        [
          "R.J.",
          "Skerry-Ryan"
        ],
        [
          "Yonghui",
          "Wu"
        ]
      ],
      "title": "Parallel Tacotron 2: A Non-Autoregressive Neural TTS Model with Differentiable Duration Modeling",
      "original": "1461",
      "page_count": 5,
      "order": 29,
      "p1": "141",
      "pn": "145",
      "abstract": [
        "This paper introduces <i>Parallel Tacotron 2</i>, a non-autoregressive\nneural text-to-speech model with a fully differentiable duration model\nwhich does not require supervised duration signals. The duration model\nis based on a novel attention mechanism and an iterative reconstruction\nloss based on Soft Dynamic TimeWarping, this model can learn token-frame\nalignments as well as token durations automatically. Experimental results\nshow that Parallel Tacotron 2 outperforms baselines in subjective naturalness\nin several diverse multi speaker evaluations.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1461",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "wu21b_interspeech": {
      "authors": [
        [
          "Chunyang",
          "Wu"
        ],
        [
          "Zhiping",
          "Xiu"
        ],
        [
          "Yangyang",
          "Shi"
        ],
        [
          "Ozlem",
          "Kalinli"
        ],
        [
          "Christian",
          "Fuegen"
        ],
        [
          "Thilo",
          "Koehler"
        ],
        [
          "Qing",
          "He"
        ]
      ],
      "title": "Transformer-Based Acoustic Modeling for Streaming Speech Synthesis",
      "original": "1655",
      "page_count": 5,
      "order": 30,
      "p1": "146",
      "pn": "150",
      "abstract": [
        "Transformer models have shown promising results in neural speech synthesis\ndue to their superior ability to model long-term dependencies compared\nto recurrent networks. The computation complexity of transformers increases\nquadratically with sequence length, making it impractical for many\nreal-time applications. To address the complexity issue in speech synthesis\ndomain, this paper proposes an efficient transformer-based acoustic\nmodel that is constant-speed regardless of input sequence length, making\nit ideal for streaming speech synthesis applications. The proposed\nmodel uses a transformer network that predicts the prosody features\nat phone rate and then an Emformer network to predict the frame-rate\nspectral features in a streaming manner. Both the transformer and Emformer\nin the proposed architecture use a self-attention mechanism that involves\nexplicit long-term information, thus providing improved speech naturalness\nfor long utterances. In our experiments, we use a WaveRNN neural vocoder\nthat takes in the predicted spectral features and generates the final\naudio. The overall architecture achieves human-like speech quality\nboth on short and long utterances while maintaining a low latency and\nlow real-time factor. Our mean opinion score (MOS) evaluation shows\nthat for short utterances, the proposed model achieves a MOS of 4.213\ncompared to ground-truth with MOS of 4.307; and for long utterances,\nit also produces high-quality speech with a MOS of 4.201 compared to\nground-truth with MOS of 4.360.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1655",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "jia21_interspeech": {
      "authors": [
        [
          "Ye",
          "Jia"
        ],
        [
          "Heiga",
          "Zen"
        ],
        [
          "Jonathan",
          "Shen"
        ],
        [
          "Yu",
          "Zhang"
        ],
        [
          "Yonghui",
          "Wu"
        ]
      ],
      "title": "PnG BERT: Augmented BERT on Phonemes and Graphemes for Neural TTS",
      "original": "1757",
      "page_count": 5,
      "order": 31,
      "p1": "151",
      "pn": "155",
      "abstract": [
        "This paper introduces <i>PnG BERT</i>, a new encoder model for neural\nTTS. This model is augmented from the original BERT model, by taking\nboth phoneme and grapheme representations of text as input, as well\nas the word-level alignment between them. It can be pre-trained on\na large text corpus in a self-supervised manner, and fine-tuned in\na TTS task. Experimental results show that a neural TTS model using\na pre-trained PnG BERT as its encoder yields more natural prosody and\nmore accurate pronunciation than a baseline model using only phoneme\ninput with no pre-training. Subjective side-by-side preference evaluations\nshow that raters have no statistically significant preference between\nthe speech synthesized using a PnG BERT and ground truth recordings\nfrom professional speakers.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1757",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "ge21_interspeech": {
      "authors": [
        [
          "Zhenhao",
          "Ge"
        ],
        [
          "Lakshmish",
          "Kaushik"
        ],
        [
          "Masanori",
          "Omote"
        ],
        [
          "Saket",
          "Kumar"
        ]
      ],
      "title": "Speed up Training with Variable Length Inputs by Efficient Batching Strategies",
      "original": "2100",
      "page_count": 5,
      "order": 32,
      "p1": "156",
      "pn": "160",
      "abstract": [
        "In the model training with neural networks, although the model performance\nis always the first priority to optimize, training efficiency also\nplays an important role in model deployment. There are many ways to\nspeed up training with minimal performance loss, such as training with\nmore GPUs, or with mixed precisions, optimizing training parameters,\nor making features more compact but more representable. Since mini-batch\ntraining is now the go-to approach for many machine learning tasks,\nminimizing the zero-padding to incorporate samples of different lengths\ninto one batch, is an alternative approach to save training time. Here\nwe propose a batching strategy based on semi-sorted samples, with dynamic\nbatch sizes and batch randomization. By replacing the random batching\nwith the proposed batching strategies, it saves more than 40% training\ntime without compromising performance in training seq2seq neural text-to-speech\nmodels based on the Tacotron framework. We also compare it with two\nother batching strategies and show it performs similarly in terms of\nsaving time and maintaining performance, but with a simpler concept\nand a smoother tuning parameter to balance between zero-padding and\nrandomness level.\n"
      ],
      "doi": "10.21437/Interspeech.2021-2100",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "sun21_interspeech": {
      "authors": [
        [
          "Yuhang",
          "Sun"
        ],
        [
          "Linju",
          "Yang"
        ],
        [
          "Huifeng",
          "Zhu"
        ],
        [
          "Jie",
          "Hao"
        ]
      ],
      "title": "Funnel Deep Complex U-Net for Phase-Aware Speech Enhancement",
      "original": "0010",
      "page_count": 5,
      "order": 33,
      "p1": "161",
      "pn": "165",
      "abstract": [
        "The emergence of deep neural networks has made speech enhancement well\ndeveloped. Most of the early models focused on estimating the magnitude\nof spectrum while ignoring the phase, this gives the evaluation result\na certain upper limit. Some recent researches proposed deep complex\nnetwork, which can handle complex inputs, and realize joint estimation\nof magnitude spectrum and phase spectrum by outputting real and imaginary\nparts respectively. The encoder-decoder structure in Deep Complex U-net\n(DCU) has been proven to be effective for complex-valued data. To further\nimprove the performance, in this paper, we design a new network called\nFunnel Deep Complex U-net (FDCU), which could process magnitude information\nand phase information separately through one-encoder-two-decoders structure.\nMoreover, in order to achieve better training effect, we define negative\nstretched-SI-SNR as the loss function to avoid errors caused by the\nnegative vector angle. Experimental results show that our FDCU model\noutperforms state-of-the-art approaches in all evaluation metrics.\n"
      ],
      "doi": "10.21437/Interspeech.2021-10",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "zhang21b_interspeech": {
      "authors": [
        [
          "Qiquan",
          "Zhang"
        ],
        [
          "Qi",
          "Song"
        ],
        [
          "Aaron",
          "Nicolson"
        ],
        [
          "Tian",
          "Lan"
        ],
        [
          "Haizhou",
          "Li"
        ]
      ],
      "title": "Temporal Convolutional Network with Frequency Dimension Adaptive Attention for Speech Enhancement",
      "original": "0046",
      "page_count": 5,
      "order": 34,
      "p1": "166",
      "pn": "170",
      "abstract": [
        "Despite much progress, most temporal convolutional networks (TCN) based\nspeech enhancement models are mainly focused on modeling the long-term\ntemporal contextual dependencies of speech frames, without taking into\naccount the distribution information of speech signal in frequency\ndimension. In this study, we propose a frequency dimension adaptive\nattention (FAA) mechanism to improve TCNs, which guides the model selectively\nemphasize the frequency-wise features with important speech information\nand also improves the representation capability of network. Our extensive\nexperimental investigation demonstrates that the proposed FAA mechanism\nis able to consistently provide significant improvements in terms of\nspeech quality (PESQ), intelligibility (STOI) and three other composite\nmetrics. More promisingly, it has better generalization ability to\nreal-world noisy environment.\n"
      ],
      "doi": "10.21437/Interspeech.2021-46",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "pan21_interspeech": {
      "authors": [
        [
          "Changjie",
          "Pan"
        ],
        [
          "Feng",
          "Yang"
        ],
        [
          "Fei",
          "Chen"
        ]
      ],
      "title": "Perceptual Contributions of Vowels and Consonant-Vowel Transitions in Understanding Time-Compressed Mandarin Sentences",
      "original": "0058",
      "page_count": 5,
      "order": 35,
      "p1": "171",
      "pn": "175",
      "abstract": [
        "Many early studies reported the importance of vowels and vowel-consonant\ntransitions to speech intelligibility. The present work assessed their\nperceptual impacts to the understanding of time-compressed sentences,\nwhich could be used to measure the temporal acuity during speech understanding.\nMandarin sentences were edited to selectively preserve vowel centers\nor vowel-consonant transitional segments, and compress the rest regions\nwith equipment time compression rates (TCRs) up to 3, including conditions\nonly preserving vowel centers or vowel-consonant transitions. The processed\nstimuli were presented to normal-hearing listeners to recognize. Results\nshowed that, consistent with the segmental contributions in understanding\nuncompressed speech, the vowel-only time-compressed stimuli were highly\nintelligible (i.e., intelligibility score &#62;85%) at a TCR around\n3, and vowel-consonant transitions carried important intelligibility\ninformation in understanding time-compressed sentences. The time-compression\nconditions in the present work provided higher intelligibility scores\nthan their counterparties in understanding the PSOLA-processed time-compressed\nsentences with TCRs around 3. The findings in this work suggested that\nthe design of time compression processing could be guided towards selectively\npreserving perceptually important speech segments (e.g., vowels) in\nthe future.\n"
      ],
      "doi": "10.21437/Interspeech.2021-58",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "biswas21_interspeech": {
      "authors": [
        [
          "Ritujoy",
          "Biswas"
        ],
        [
          "Karan",
          "Nathwani"
        ],
        [
          "Vinayak",
          "Abrol"
        ]
      ],
      "title": "Transfer Learning for Speech Intelligibility Improvement in Noisy Environments",
      "original": "0150",
      "page_count": 5,
      "order": 36,
      "p1": "176",
      "pn": "180",
      "abstract": [
        "In a recent work [1], a novel Delta Function-based Formant Shifting\napproach was proposed for speech intelligibility improvement. The underlying\nprinciple is to dynamically relocate the formants based on their occurrence\nin the spectrum away from the region of noise. The manner in which\nthe formants are shifted is decided by the parameters of the Delta\nFunction, the optimal values of which are evaluated using Comprehensive\nLearning Particle Swarm Optimization (CLPSO). Although effective, CLPSO\nis computationally expensive to the extent that it overshadows its\nmerits in intelligibility improvement. As a solution to this, the current\nwork aims to improve the Short-Time Objective Intelligibility (STOI)\nof (target) speech using a Delta Function that has been generated using\na different (source) language. This transfer learning is based upon\nthe relative positioning of the formant frequencies and pitch values\nof the source &amp; target language datasets. The proposed approach\nis demonstrated and validated by subjecting it to experimentation with\nthree different languages under variable noisy conditions.\n"
      ],
      "doi": "10.21437/Interspeech.2021-150",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "yamamoto21_interspeech": {
      "authors": [
        [
          "Ayako",
          "Yamamoto"
        ],
        [
          "Toshio",
          "Irino"
        ],
        [
          "Kenichi",
          "Arai"
        ],
        [
          "Shoko",
          "Araki"
        ],
        [
          "Atsunori",
          "Ogawa"
        ],
        [
          "Keisuke",
          "Kinoshita"
        ],
        [
          "Tomohiro",
          "Nakatani"
        ]
      ],
      "title": "Comparison of Remote Experiments Using Crowdsourcing and Laboratory Experiments on Speech Intelligibility",
      "original": "0174",
      "page_count": 5,
      "order": 37,
      "p1": "181",
      "pn": "185",
      "abstract": [
        "Many subjective experiments have been performed to develop objective\nspeech intelligibility measures, but the novel coronavirus outbreak\nhas made it difficult to conduct experiments in a laboratory. One solution\nis to perform remote testing using crowdsourcing; however, because\nwe cannot control the listening conditions, it is unclear whether the\nresults are entirely reliable. In this study, we compared the speech\nintelligibility scores obtained from remote and laboratory experiments.\nThe results showed that the mean and standard deviation (SD) of the\nremote experiments&#8217; speech reception threshold (SRT) were higher\nthan those of the laboratory experiments. However, the variance in\nthe SRTs across the speech-enhancement conditions revealed similarities,\nimplying that remote testing results may be as useful as laboratory\nexperiments to develop an objective measure. We also show that practice\nsession scores are correlated with SRT values. This is a priori information\nbefore performing the main tests and would be useful for data screening\nto reduce the variability of the SRT distribution.\n"
      ],
      "doi": "10.21437/Interspeech.2021-174",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "liu21b_interspeech": {
      "authors": [
        [
          "Wenzhe",
          "Liu"
        ],
        [
          "Andong",
          "Li"
        ],
        [
          "Yuxuan",
          "Ke"
        ],
        [
          "Chengshi",
          "Zheng"
        ],
        [
          "Xiaodong",
          "Li"
        ]
      ],
      "title": "Know Your Enemy, Know Yourself: A Unified Two-Stage Framework for Speech Enhancement",
      "original": "0238",
      "page_count": 5,
      "order": 38,
      "p1": "186",
      "pn": "190",
      "abstract": [
        "Traditional spectral subtraction-type single channel speech enhancement\n(SE) algorithms often need to estimate interference components including\nnoise and/or reverberation before subtracting them while deep neural\nnetwork-based SE methods often aim to realize the end-to-end target\nmapping. In this paper, we show that both denoising and dereverberation\ncan be unified into a common problem by introducing a two-stage paradigm,\nnamely for interference components estimation and speech recovery.\nIn the first stage, we propose to explicitly extract the magnitude\nof interference components, which serves as the prior information.\nIn the second stage, with the guidance of this estimated magnitude\nprior, we can expect to better recover the target speech. In addition,\nwe propose a transform module to facilitate the interaction between\ninterference components and the desired speech modalities. Meanwhile,\na temporal fusion module is designed to model long-term dependencies\nwithout ignoring short-term details. We conduct the experiments on\nthe WSJ0-SI84 corpus and the results on both denoising and dereverberation\ntasks show that our approach outperforms previous advanced systems\nand achieves state-of-the-art performance in terms of many objective\nmetrics.\n"
      ],
      "doi": "10.21437/Interspeech.2021-238",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "kong21_interspeech": {
      "authors": [
        [
          "Qiuqiang",
          "Kong"
        ],
        [
          "Haohe",
          "Liu"
        ],
        [
          "Xingjian",
          "Du"
        ],
        [
          "Li",
          "Chen"
        ],
        [
          "Rui",
          "Xia"
        ],
        [
          "Yuxuan",
          "Wang"
        ]
      ],
      "title": "Speech Enhancement with Weakly Labelled Data from AudioSet",
      "original": "0259",
      "page_count": 5,
      "order": 39,
      "p1": "191",
      "pn": "195",
      "abstract": [
        "Speech enhancement is a task to improve the intelligibility and perceptual\nquality of degraded speech signals. Recently, neural network-based\nmethods have been applied to speech enhancement. However, many neural\nnetwork-based methods require users to collect clean speech and background\nnoise for training, which can be time-consuming. In addition, speech\nenhancement systems trained on particular types of background noise\nmay not generalize well to a wide range of noise. To tackle those problems,\nwe propose a speech enhancement framework trained on weakly labelled\ndata. We first apply a pretrained sound event detection system to detect\nanchor segments that contain sound events in audio clips. Then, we\nrandomly mix two detected anchor segments as a mixture. We build a\nconditional source separation network using the mixture and a conditional\nvector as input. The conditional vector is obtained from the audio\ntagging predictions on the anchor segments. In inference, we input\na noisy speech signal with the one-hot encoding of &#8220;Speech&#8221;\nas a condition to the trained system to predict enhanced speech. Our\nsystem achieves a PESQ of 2.28 and an SSNR of 8.75 dB on the VoiceBank-DEMAND\ndataset, outperforming the previous SEGAN system of 2.16 and 7.73 dB\nrespectively.\n"
      ],
      "doi": "10.21437/Interspeech.2021-259",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "hsieh21_interspeech": {
      "authors": [
        [
          "Tsun-An",
          "Hsieh"
        ],
        [
          "Cheng",
          "Yu"
        ],
        [
          "Szu-Wei",
          "Fu"
        ],
        [
          "Xugang",
          "Lu"
        ],
        [
          "Yu",
          "Tsao"
        ]
      ],
      "title": "Improving Perceptual Quality by Phone-Fortified Perceptual Loss Using Wasserstein Distance for Speech Enhancement",
      "original": "0582",
      "page_count": 5,
      "order": 40,
      "p1": "196",
      "pn": "200",
      "abstract": [
        "Speech enhancement (SE) aims to improve speech quality and intelligibility,\nwhich are both related to a smooth transition in speech segments that\nmay carry linguistic information, e.g. phones and syllables. In this\nstudy, we propose a novel phone-fortified perceptual loss (PFPL) that\ntakes phonetic information into account for training SE models. To\neffectively incorporate the phonetic information, the PFPL is computed\nbased on latent representations of the <i>wav2vec</i> model, a powerful\nself-supervised encoder that renders rich phonetic information. To\nmore accurately measure the distribution distances of the latent representations,\nthe PFPL adopts the Wasserstein distance as the distance measure. Our\nexperimental results first reveal that the PFPL is more correlated\nwith the perceptual evaluation metrics, as compared to signal-level\nlosses. Moreover, the results showed that the PFPL can enable a deep\ncomplex U-Net SE model to achieve highly competitive performance in\nterms of standardized quality and intelligibility evaluations on the\nVoice Bank&#8211;DEMAND dataset.\n"
      ],
      "doi": "10.21437/Interspeech.2021-582",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "fu21_interspeech": {
      "authors": [
        [
          "Szu-Wei",
          "Fu"
        ],
        [
          "Cheng",
          "Yu"
        ],
        [
          "Tsun-An",
          "Hsieh"
        ],
        [
          "Peter",
          "Plantinga"
        ],
        [
          "Mirco",
          "Ravanelli"
        ],
        [
          "Xugang",
          "Lu"
        ],
        [
          "Yu",
          "Tsao"
        ]
      ],
      "title": "MetricGAN+: An Improved Version of MetricGAN for Speech Enhancement",
      "original": "0599",
      "page_count": 5,
      "order": 41,
      "p1": "201",
      "pn": "205",
      "abstract": [
        "The discrepancy between the cost function used for training a speech\nenhancement model and human auditory perception usually makes the quality\nof enhanced speech unsatisfactory. Objective evaluation metrics which\nconsider human perception can hence serve as a bridge to reduce the\ngap. Our previously proposed MetricGAN was designed to optimize objective\nmetrics by connecting the metric with a discriminator. Because only\nthe scores of the target evaluation functions are needed during training,\nthe metrics can even be non-differentiable. In this study, we propose\na MetricGAN+ in which three training techniques incorporating domain-knowledge\nof speech processing are proposed. With these techniques, experimental\nresults on the VoiceBank-DEMAND dataset show that MetricGAN+ can increase\nPESQ score by 0.3 compared to the previous MetricGAN and achieve state-of-the-art\nresults (PESQ score = 3.15).\n"
      ],
      "doi": "10.21437/Interspeech.2021-599",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "edraki21_interspeech": {
      "authors": [
        [
          "Amin",
          "Edraki"
        ],
        [
          "Wai-Yip",
          "Chan"
        ],
        [
          "Jesper",
          "Jensen"
        ],
        [
          "Daniel",
          "Fogerty"
        ]
      ],
      "title": "A Spectro-Temporal Glimpsing Index (STGI) for Speech Intelligibility Prediction",
      "original": "0605",
      "page_count": 5,
      "order": 42,
      "p1": "206",
      "pn": "210",
      "abstract": [
        "We propose a monaural intrusive speech intelligibility prediction (SIP)\nalgorithm called STGI based on detecting <i>glimpses</i> in short-time\nsegments in a spectro-temporal modulation decomposition of the input\nspeech signals. Unlike existing glimpse-based SIP methods, the application\nof STGI is not limited to additive uncorrelated noise; STGI can be\nemployed in a broad range of degradation conditions. Our results show\nthat STGI performs consistently well across 15 datasets covering degradation\nconditions including modulated noise, noise reduction processing, reverberation,\nnear-end listening enhancement, checkerboard noise, and gated noise.\n"
      ],
      "doi": "10.21437/Interspeech.2021-605",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "qiu21_interspeech": {
      "authors": [
        [
          "Yuanhang",
          "Qiu"
        ],
        [
          "Ruili",
          "Wang"
        ],
        [
          "Satwinder",
          "Singh"
        ],
        [
          "Zhizhong",
          "Ma"
        ],
        [
          "Feng",
          "Hou"
        ]
      ],
      "title": "Self-Supervised Learning Based Phone-Fortified Speech Enhancement",
      "original": "0734",
      "page_count": 5,
      "order": 43,
      "p1": "211",
      "pn": "215",
      "abstract": [
        "For speech enhancement, deep complex network based methods have shown\npromising performance due to their effectiveness in dealing with complex-valued\nspectrums. Recent speech enhancement methods focus on further optimization\nof network structures and hyperparameters, however, ignore inherent\nspeech characteristics (e.g., phonetic characteristics), which are\nimportant for networks to learn and reconstruct speech information.\nIn this paper, we propose a novel self-supervised learning based phone-fortified\n(SSPF) method for speech enhancement. Our method explicitly imports\nphonetic characteristics into a deep complex convolutional network\nvia a Contrastive Predictive Coding (CPC) model pre-trained with self-supervised\nlearning. This operation can greatly improve speech representation\nlearning and speech enhancement performance. Moreover, we also apply\nthe self-attention mechanism to our model for learning long-range dependencies\nof a speech sequence, which further improves the performance of speech\nenhancement. The experimental results demonstrate that our SSPF method\noutperforms existing methods and achieves state-of-the-art performance\nin terms of speech quality and intelligibility.\n"
      ],
      "doi": "10.21437/Interspeech.2021-734",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "nayem21_interspeech": {
      "authors": [
        [
          "Khandokar Md.",
          "Nayem"
        ],
        [
          "Donald S.",
          "Williamson"
        ]
      ],
      "title": "Incorporating Embedding Vectors from a Human Mean-Opinion Score Prediction Model for Monaural Speech Enhancement",
      "original": "1844",
      "page_count": 5,
      "order": 44,
      "p1": "216",
      "pn": "220",
      "abstract": [
        "Objective measures of success, such as the perceptual evaluation of\nspeech quality (PESQ), signal-to-distortion ratio (SDR), and short-time\nobjective intelligibility (STOI), have recently been used to optimize\ndeep-learning based speech enhancement algorithms, in an effort to\nincorporate perceptual constraints into the learning process. Optimizing\nwith these measures, however, may be sub-optimal, since the objective\nscores do not always strongly correlate with a listener&#8217;s evaluation.\nThis motivates the need for approaches that either are optimized with\nscores that are strongly correlated with human assessments or that\nuse alternative strategies for incorporating perceptual constraints.\nIn this work, we propose an attention-based approach that uses learned\nspeech embedding vectors from a mean-opinion score (MOS) prediction\nmodel and a speech enhancement module to jointly enhance noisy speech.\nOur loss function is jointly optimized with signal approximation and\nMOS prediction loss terms. We train the model using real-world noisy\nspeech data that has been captured in everyday environments. The results\nshow that our proposed model significantly outperforms other approaches\nthat are optimized with objective measures.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1844",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "zhang21c_interspeech": {
      "authors": [
        [
          "Jianwei",
          "Zhang"
        ],
        [
          "Suren",
          "Jayasuriya"
        ],
        [
          "Visar",
          "Berisha"
        ]
      ],
      "title": "Restoring Degraded Speech via a Modified Diffusion Model",
      "original": "1889",
      "page_count": 5,
      "order": 45,
      "p1": "221",
      "pn": "225",
      "abstract": [
        "There are many deterministic mathematical operations (e.g. compression,\nclipping, downsampling) that degrade speech quality considerably. In\nthis paper we introduce a neural network architecture, based on a modification\nof the DiffWave model, that aims to restore the original speech signal.\nDiffWave, a recently published diffusion-based vocoder, has shown state-of-the-art\nsynthesized speech quality and relatively shorter waveform generation\ntimes, with only a small set of parameters. We replace the mel-spectrum\nupsampler in DiffWave with a deep CNN upsampler, which is trained to\nalter the degraded speech mel-spectrum to match that of the original\nspeech. The model is trained using the original speech waveform, but\nconditioned on the degraded speech mel-spectrum. Post-training, only\nthe degraded mel-spectrum is used as input and the model generates\nan estimate of the original speech. Our model results in improved speech\nquality (original DiffWave model as baseline) on several different\nexperiments. These include improving the quality of speech degraded\nby LPC-10 compression, AMR-NB compression, and signal clipping. Compared\nto the original DiffWave architecture, our scheme achieves better performance\non several objective perceptual metrics and in subjective comparisons.\nImprovements over baseline are further amplified in a out-of-corpus\nevaluation setting.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1889",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "nguyen21_interspeech": {
      "authors": [
        [
          "Hoang Long",
          "Nguyen"
        ],
        [
          "Vincent",
          "Renkens"
        ],
        [
          "Joris",
          "Pelemans"
        ],
        [
          "Srividya Pranavi",
          "Potharaju"
        ],
        [
          "Anil Kumar",
          "Nalamalapu"
        ],
        [
          "Murat",
          "Akbacak"
        ]
      ],
      "title": "User-Initiated Repetition-Based Recovery in Multi-Utterance Dialogue Systems",
      "original": "1536",
      "page_count": 5,
      "order": 46,
      "p1": "226",
      "pn": "230",
      "abstract": [
        "Recognition errors are common in human communication. Similar errors\noften lead to unwanted behaviour in dialogue systems or virtual assistants.\nIn human communication, we can recover from them by repeating misrecognized\nwords or phrases; however in human-machine communication this recovery\nmechanism is not available. In this paper, we attempt to bridge this\ngap and present a system that allows a user to correct speech recognition\nerrors in a virtual assistant by repeating misunderstood words. When\na user repeats part of the phrase the system rewrites the original\nquery to incorporate the correction. This rewrite allows the virtual\nassistant to understand the original query successfully. We present\nan end-to-end 2-step attention pointer network that can generate the\nthe rewritten query by merging together the incorrectly understood\nutterance with the correction follow-up. We evaluate the model on data\ncollected for this task and compare the proposed model to a rule-based\nbaseline and a standard pointer network. We show that rewriting the\noriginal query is an effective way to handle repetition-based recovery\nand that the proposed model outperforms the rule based baseline, reducing\nWord Error Rate by 19% relative at 2% False Alarm Rate on annotated\ndata.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1536",
      "author_area_id": "11",
      "author_area_label": "Spoken Dialog Systems and Conversational Analysis"
    },
    "chen21_interspeech": {
      "authors": [
        [
          "Nuo",
          "Chen"
        ],
        [
          "Chenyu",
          "You"
        ],
        [
          "Yuexian",
          "Zou"
        ]
      ],
      "title": "Self-Supervised Dialogue Learning for Spoken Conversational Question Answering",
      "original": "0120",
      "page_count": 5,
      "order": 47,
      "p1": "231",
      "pn": "235",
      "abstract": [
        "In spoken conversational question answering (SCQA), the answer to the\ncorresponding question is generated by retrieving and then analyzing\na fixed spoken document, including multi-part conversations. Most SCQA\nsystems have considered only retrieving information from ordered utterances.\nHowever, the sequential order of dialogue is important to build a robust\nspoken conversational question answering system, and the changes of\nutterances order may severely result in low-quality and incoherent\ncorpora. To this end, we introduce a self-supervised learning approach,\nincluding <i>incoherence discrimination, insertion detection</i>, and\n<i>question prediction</i>, to explicitly capture the coreference resolution\nand dialogue coherence among spoken documents. Specifically, we design\na joint learning framework where the auxiliary self-supervised tasks\ncan enable the pre-trained SCQA systems towards more coherent and meaningful\nspoken dialogue learning. We also utilize the proposed self-supervised\nlearning tasks to capture intra-sentence coherence. Experimental results\ndemonstrate that our proposed method provides more coherent, meaningful,\nand appropriate responses, yielding superior performance gains compared\nto the original pre-trained language models. Our method achieves state-of-the-art\nresults on the Spoken-CoQA dataset.\n"
      ],
      "doi": "10.21437/Interspeech.2021-120",
      "author_area_id": "11",
      "author_area_label": "Spoken Dialog Systems and Conversational Analysis"
    },
    "su21_interspeech": {
      "authors": [
        [
          "Ruolin",
          "Su"
        ],
        [
          "Ting-Wei",
          "Wu"
        ],
        [
          "Biing-Hwang",
          "Juang"
        ]
      ],
      "title": "Act-Aware Slot-Value Predicting in Multi-Domain Dialogue State Tracking",
      "original": "0138",
      "page_count": 5,
      "order": 48,
      "p1": "236",
      "pn": "240",
      "abstract": [
        "As an essential component in task-oriented dialogue systems, dialogue\nstate tracking (DST) aims to track human-machine interactions and generate\nstate representations for managing the dialogue. Representations of\ndialogue states are dependent on the domain ontology and the user&#8217;s\ngoals. In several task-oriented dialogues with a limited scope of objectives,\ndialogue states can be represented as a set of slot-value pairs. As\nthe capabilities of dialogue systems expand to support increasing naturalness\nin communication, incorporating dialogue act processing into dialogue\nmodel design becomes essential. The lack of such consideration limits\nthe scalability of dialogue state tracking models for dialogues having\nspecific objectives and ontology. To address this issue, we formulate\nand incorporate dialogue acts, and leverage recent advances in machine\nreading comprehension to predict both categorical and non-categorical\ntypes of slots for multi-domain dialogue state tracking. Experimental\nresults show that our models can improve the overall accuracy of dialogue\nstate tracking on the MultiWOZ 2.1 dataset, and demonstrate that incorporating\ndialogue acts can guide dialogue state design for future task-oriented\ndialogue systems.\n"
      ],
      "doi": "10.21437/Interspeech.2021-138",
      "author_area_id": "11",
      "author_area_label": "Spoken Dialog Systems and Conversational Analysis"
    },
    "chiba21_interspeech": {
      "authors": [
        [
          "Yuya",
          "Chiba"
        ],
        [
          "Ryuichiro",
          "Higashinaka"
        ]
      ],
      "title": "Dialogue Situation Recognition for Everyday Conversation Using Multimodal Information",
      "original": "0171",
      "page_count": 5,
      "order": 49,
      "p1": "241",
      "pn": "245",
      "abstract": [
        "In recent years, dialogue systems have been applied to daily living.\nSuch systems should be able to associate conversations with dialogue\nsituations, such as a place where a dialogue occurs and the relationship\nbetween participants. In this study, we propose a dialogue situation\nrecognition method that understands the perspective of dialogue scenes.\nThe target dialogue situations contain dialogue styles, places, activities,\nand relations between participants. We used the Corpus of Everyday\nJapanese Conversation (CEJC), which records natural everyday conversations\nin various situations for experiments. We experimentally verified the\neffectiveness of our proposed method using multimodal information for\nsituation recognition.\n"
      ],
      "doi": "10.21437/Interspeech.2021-171",
      "author_area_id": "11",
      "author_area_label": "Spoken Dialog Systems and Conversational Analysis"
    },
    "yamazaki21_interspeech": {
      "authors": [
        [
          "Yoshihiro",
          "Yamazaki"
        ],
        [
          "Yuya",
          "Chiba"
        ],
        [
          "Takashi",
          "Nose"
        ],
        [
          "Akinori",
          "Ito"
        ]
      ],
      "title": "Neural Spoken-Response Generation Using Prosodic and Linguistic Context for Conversational Systems",
      "original": "0381",
      "page_count": 5,
      "order": 50,
      "p1": "246",
      "pn": "250",
      "abstract": [
        "Spoken dialogue systems have become widely used in daily life. Such\na system must interact with the user socially to truly operate as a\npartner with humans. In studies of recent dialogue systems, neural\nresponse generation led to natural response generation. However, these\nstudies have not considered the acoustic aspects of conversational\nphenomena, such as the adaptation of prosody. We propose a spoken-response\ngeneration model that extends a neural conversational model to deal\nwith pitch control signals. Our proposed model is trained using multimodal\ndialogue between humans. The generated pitch control signals are input\nto a speech synthesis system to control the pitch of synthesized speech.\nOur experiment shows that the proposed system can generate synthesized\nspeech with an appropriate F0 contour as an utterance in context compared\nto the output of a system without pitch control, although language\ngeneration remains an issue.\n"
      ],
      "doi": "10.21437/Interspeech.2021-381",
      "author_area_id": "11",
      "author_area_label": "Spoken Dialog Systems and Conversational Analysis"
    },
    "xu21_interspeech": {
      "authors": [
        [
          "Weiyuan",
          "Xu"
        ],
        [
          "Peilin",
          "Zhou"
        ],
        [
          "Chenyu",
          "You"
        ],
        [
          "Yuexian",
          "Zou"
        ]
      ],
      "title": "Semantic Transportation Prototypical Network for Few-Shot Intent Detection",
      "original": "0548",
      "page_count": 5,
      "order": 51,
      "p1": "251",
      "pn": "255",
      "abstract": [
        "Few-shot intent detection is a problem that only a few annotated examples\nare available for unseen intents, and deep models could suffer from\nthe overfitting problem because of scarce data. Existing state-of-the-art\nfew-shot model, Prototypical Network (PN), mainly focus on computing\nthe similarity between examples in a metric space by leveraging sentence-level\ninstance representations. However, sentence-level representations may\nincorporate highly noisy signals from unrelated words which leads to\nperformance degradation. In this paper, we propose Semantic Transportation\nPrototypical Network (STPN) to alleviate this issue. Different from\nthe original PN, our approach takes word-level representation as input\nand uses a new distance metric to obtain better sample matching result.\nAnd we reformulate the few-shot classification task into an instance\nof optimal matching, in which the key word semantic information between\nexamples are expected to be matched and the matching cost is treated\nas similarity. Specifically, we design Mutual-Semantic mechanism to\ngenerate word semantic information, which could reduce the unrelated\nword noise and enrich key word information. Then, Earth Mover&#8217;s\nDistance (EMD) is applied to find an optimal matching solution. Comprehensive\nexperiments on two benchmark datasets are conducted to validate the\neffectiveness and generalization of our proposed model.\n"
      ],
      "doi": "10.21437/Interspeech.2021-548",
      "author_area_id": "11",
      "author_area_label": "Spoken Dialog Systems and Conversational Analysis"
    },
    "tang21_interspeech": {
      "authors": [
        [
          "Li",
          "Tang"
        ],
        [
          "Yuke",
          "Si"
        ],
        [
          "Longbiao",
          "Wang"
        ],
        [
          "Jianwu",
          "Dang"
        ]
      ],
      "title": "Domain-Specific Multi-Agent Dialog Policy Learning in Multi-Domain Task-Oriented Scenarios",
      "original": "0887",
      "page_count": 5,
      "order": 52,
      "p1": "256",
      "pn": "260",
      "abstract": [
        "Traditional dialog policy learning methods train a generic dialog agent\nto address all situations. However, when the dialog agent encounters\na complicated task that involves more than one domain, it becomes difficult\nto perform concordant actions due to the hybrid information in the\nmulti-domain ontology. Inspired by a real-life scenario at a bank,\nthere are always several specialized departments that deal with different\nbusinesses. In this paper, we propose Domain-Specific Multi-Agent Dialog\nPolicy Learning (DSMADPL), in which the dialog system is composed of\na set of agents where each agent represents a specialized skill in\na particular domain. Every domain-specific agent is first pretrained\nwith supervised learning using a dialog corpus, and then they are jointly\nimproved with multi-agent reinforcement learning. When the dialog system\ninteracts with the user, in each turn the system action is decided\nby the actions of relevant agents. Experiments conducted on the commonly\nused MultiWOZ dataset prove the effectiveness of the proposed method,\nin which dialog success rate increases from 55.0% for the traditional\nmethod to 67.2% for our method in multi-domain scenarios.\n"
      ],
      "doi": "10.21437/Interspeech.2021-887",
      "author_area_id": "11",
      "author_area_label": "Spoken Dialog Systems and Conversational Analysis"
    },
    "wang21b_interspeech": {
      "authors": [
        [
          "Haoyu",
          "Wang"
        ],
        [
          "John",
          "Chen"
        ],
        [
          "Majid",
          "Laali"
        ],
        [
          "Kevin",
          "Durda"
        ],
        [
          "Jeff",
          "King"
        ],
        [
          "William",
          "Campbell"
        ],
        [
          "Yang",
          "Liu"
        ]
      ],
      "title": "Leveraging ASR N-Best in Deep Entity Retrieval",
      "original": "1370",
      "page_count": 5,
      "order": 53,
      "p1": "261",
      "pn": "265",
      "abstract": [
        "Entity Retrieval (ER) in spoken dialog systems is a task that retrieves\nentities in a catalog for the entity mentions in user utterances. ER\nsystems are susceptible to upstream errors, with Automatic Speech Recognition\n(ASR) errors being particularly troublesome. In this work, we propose\na robust deep learning based ER system by leveraging ASR N-best hypotheses.\nSpecifically, we evaluate different neural architectures to infuse\nASR N-best through an attention mechanism. On 750 hours of audio data\ntaken from live traffic, our best model achieves 11.07% relative error\nreduction while maintaining the same performance on rejecting out-of-domain\nER requests.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1370",
      "author_area_id": "11",
      "author_area_label": "Spoken Dialog Systems and Conversational Analysis"
    },
    "zhang21d_interspeech": {
      "authors": [
        [
          "Shuai",
          "Zhang"
        ],
        [
          "Jiangyan",
          "Yi"
        ],
        [
          "Zhengkun",
          "Tian"
        ],
        [
          "Ye",
          "Bai"
        ],
        [
          "Jianhua",
          "Tao"
        ],
        [
          "Xuefei",
          "Liu"
        ],
        [
          "Zhengqi",
          "Wen"
        ]
      ],
      "title": "End-to-End Spelling Correction Conditioned on Acoustic Feature for Code-Switching Speech Recognition",
      "original": "1242",
      "page_count": 5,
      "order": 54,
      "p1": "266",
      "pn": "270",
      "abstract": [
        "In this work, we propose a new end-to-end (E2E) spelling correction\nmethod for post-processing of code-switching automatic speech recognition\n(ASR). Existing E2E spelling correction models take the hypotheses\nof ASR as inputs and annotated text as the targets. Due to the powerful\nmodeling capabilities of the E2E model, the training of the correction\nsystem is extremely prone to over-fitting. It usually requires sufficient\ndata diversity for reliable training. Therefore, it is difficult to\napply the E2E correction models to the code-switching ASR task because\nof the data shortage. In this paper, we introduce the acoustic features\ninto the spelling correction model. Our method can alleviate the problem\nof over-fitting and has better performance. Meanwhile, because the\nacoustic features are encode-free, our proposed model can be applied\nto the ASR model without significantly increasing the computational\ncost. The experimental results on ASRU 2019 Mandarin-English Code-switching\nChallenge data set show that the proposed method achieves 11.14% relative\nerror rate reduction compared with baseline.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1242",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "siminyu21_interspeech": {
      "authors": [
        [
          "Kathleen",
          "Siminyu"
        ],
        [
          "Xinjian",
          "Li"
        ],
        [
          "Antonios",
          "Anastasopoulos"
        ],
        [
          "David R.",
          "Mortensen"
        ],
        [
          "Michael R.",
          "Marlo"
        ],
        [
          "Graham",
          "Neubig"
        ]
      ],
      "title": "Phoneme Recognition Through Fine Tuning of Phonetic Representations: A Case Study on Luhya Language Varieties",
      "original": "1434",
      "page_count": 5,
      "order": 55,
      "p1": "271",
      "pn": "275",
      "abstract": [
        "Models pre-trained on multiple languages have shown significant promise\nfor improving speech recognition, particularly for low-resource languages.\nIn this work, we focus on phoneme recognition using Allosaurus, a method\nfor multilingual recognition based on phonetic annotation, which incorporates\nphonological knowledge through a language-dependent allophone layer\nthat associates a universal narrow phone-set with the phonemes that\nappear in each language. To evaluate in a challenging real-world scenario,\nwe curate phone recognition datasets for Bukusu and Saamia, two varieties\nof the Luhya language cluster of western Kenya and eastern Uganda.\nTo our knowledge, these datasets are the first of their kind. We carry\nout similar experiments on the dataset of an endangered Tangkhulic\nlanguage, East Tusom, a Tibeto-Burman language variety spoken mostly\nin India. We explore both zero-shot and few-shot recognition by fine-tuning\nusing datasets of varying sizes (10 to 1000 utterances). We find that\nfine-tuning of Allosaurus, even with just 100 utterances, leads to\nsignificant improvements in phone error rates.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1434",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "loweimi21_interspeech": {
      "authors": [
        [
          "Erfan",
          "Loweimi"
        ],
        [
          "Zoran",
          "Cvetkovic"
        ],
        [
          "Peter",
          "Bell"
        ],
        [
          "Steve",
          "Renals"
        ]
      ],
      "title": "Speech Acoustic Modelling Using Raw Source and Filter Components",
      "original": "0053",
      "page_count": 5,
      "order": 56,
      "p1": "276",
      "pn": "280",
      "abstract": [
        "Source-filter modelling is among the fundamental techniques in speech\nprocessing with a wide range of applications. In acoustic modelling,\nfeatures such as MFCC and PLP which parametrise the filter component\nare widely employed. In this paper, we investigate the efficacy of\nbuilding acoustic models from the raw filter and source components.\nThe raw magnitude spectrum, as the primary information stream, is decomposed\ninto the excitation and vocal tract information streams via cepstral\nliftering. Then, acoustic models are built via multi-head CNNs which,\namong others, allow for processing each individual stream via a sequence\nof bespoke transforms and fusing them at an optimal level of abstraction.\nWe discuss the possible advantages of such information factorisation\nand recombination, investigate the dynamics of these models and explore\nthe optimal fusion level. Furthermore, we illustrate the CNN&#8217;s\nlearned filters and provide some interpretation for the captured patterns.\nThe proposed approach with optimal fusion scheme results in up to 14%\nand 7% relative WER reduction in WSJ and Aurora-4 tasks.\n"
      ],
      "doi": "10.21437/Interspeech.2021-53",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "fujimoto21_interspeech": {
      "authors": [
        [
          "Masakiyo",
          "Fujimoto"
        ],
        [
          "Hisashi",
          "Kawai"
        ]
      ],
      "title": "Noise Robust Acoustic Modeling for Single-Channel Speech Recognition Based on a Stream-Wise Transformer Architecture",
      "original": "0225",
      "page_count": 5,
      "order": 57,
      "p1": "281",
      "pn": "285",
      "abstract": [
        "This paper addresses a noise-robust automatic speech recognition (ASR)\nmethod under the constraints of real-time, one-pass, and single-channel\nprocessing. Under such strong constraints, single-channel speech enhancement\nbecomes a key technology because methods with multiple-passes or batch\nprocessing, such as acoustic model adaptation, are not suitable for\nuse. However, single-channel speech enhancement often degrades ASR\nperformance due to speech distortion. To overcome this problem, we\npropose a noise robust acoustic modeling method based on the stream-wise\ntransformer model. The proposed method accepts multi-stream features\nobtained by multiple single-channel speech enhancement methods as input\nand selectively uses an appropriate feature stream according to the\nnoise environment by paying attention to the noteworthy stream on the\nbasis of multi-head attention. The proposed method considers the attention\nfor the stream direction instead of the time series direction, and\nit is thus capable of real-time and low-latency processing. Comparative\nevaluations reveal that the proposed method successfully improves the\naccuracy of ASR in noisy environments and reduces the number of model\nparameters even under strong constraints.\n"
      ],
      "doi": "10.21437/Interspeech.2021-225",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "ratnarajah21_interspeech": {
      "authors": [
        [
          "Anton",
          "Ratnarajah"
        ],
        [
          "Zhenyu",
          "Tang"
        ],
        [
          "Dinesh",
          "Manocha"
        ]
      ],
      "title": "IR-GAN: Room Impulse Response Generator for Far-Field Speech Recognition",
      "original": "0230",
      "page_count": 5,
      "order": 58,
      "p1": "286",
      "pn": "290",
      "abstract": [
        "We present a Generative Adversarial Network (GAN) based room impulse\nresponse generator (IR-GAN) for generating realistic synthetic room\nimpulse responses (RIRs). IR-GAN extracts acoustic parameters from\ncaptured real-world RIRs and uses these parameters to generate new\nsynthetic RIRs. We use these generated synthetic RIRs to improve far-field\nautomatic speech recognition in new environments that are different\nfrom the ones used in training datasets. In particular, we augment\nthe far-field speech training set by convolving our synthesized RIRs\nwith a clean LibriSpeech dataset [1]. We evaluate the quality of our\nsynthetic RIRs on the far-field LibriSpeech test set created using\nreal-world RIRs from the BUT ReverbDB [2] and AIR [3] datasets. Our\nIR-GAN reports up to an 8.95% lower error rate than Geometric Acoustic\nSimulator (GAS) in far-field speech recognition benchmarks. We further\nimprove the performance when we combine our synthetic RIRs with synthetic\nimpulse responses generated using GAS. This combination can reduce\nthe word error rate by up to 14.3% in far-field speech recognition\nbenchmarks.\n"
      ],
      "doi": "10.21437/Interspeech.2021-230",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "chen21b_interspeech": {
      "authors": [
        [
          "Junqi",
          "Chen"
        ],
        [
          "Xiao-Lei",
          "Zhang"
        ]
      ],
      "title": "Scaling Sparsemax Based Channel Selection for Speech Recognition with ad-hoc Microphone Arrays",
      "original": "0419",
      "page_count": 5,
      "order": 59,
      "p1": "291",
      "pn": "295",
      "abstract": [
        "Recently, speech recognition with ad-hoc microphone arrays has received\nmuch attention. It is known that channel selection is an important\nproblem of ad-hoc microphone arrays, however, this topic seems far\nfrom explored in speech recognition yet, particularly with a large-scale\nad-hoc microphone array. To address this problem, we propose a <i>Scaling\nSparsemax</i> algorithm for the channel selection problem of the speech\nrecognition with large-scale ad-hoc microphone arrays. Specifically,\nwe first replace the conventional Softmax operator in the stream attention\nmechanism of a multichannel end-to-end speech recognition system with\nSparsemax, which conducts channel selection by forcing the channel\nweights of noisy channels to zero. Because Sparsemax punishes the weights\nof many channels to zero harshly, we propose Scaling Sparsemax which\npunishes the channels mildly by setting the weights of very noisy channels\nto zero only. Experimental results with ad-hoc microphone arrays of\nover 30 channels under the conformer speech recognition architecture\nshow that the proposed Scaling Sparsemax yields a word error rate of\nover 30% lower than Softmax on simulation data sets, and over 20% lower\non semi-real data sets, in test scenarios with both matched and mismatched\nchannel numbers.\n"
      ],
      "doi": "10.21437/Interspeech.2021-419",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "chang21_interspeech": {
      "authors": [
        [
          "Feng-Ju",
          "Chang"
        ],
        [
          "Martin",
          "Radfar"
        ],
        [
          "Athanasios",
          "Mouchtaris"
        ],
        [
          "Maurizio",
          "Omologo"
        ]
      ],
      "title": "Multi-Channel Transformer Transducer for Speech Recognition",
      "original": "0655",
      "page_count": 5,
      "order": 60,
      "p1": "296",
      "pn": "300",
      "abstract": [
        "Multi-channel inputs offer several advantages over single-channel,\nto improve the robustness of on-device speech recognition systems.\nRecent work on multi-channel transformer, has proposed a way to incorporate\nsuch inputs into end-to-end ASR for improved accuracy. However, this\napproach is characterized by a high computational complexity, which\nprevents it from being deployed in on-device systems. In this paper,\nwe present a novel speech recognition model, <i>Multi-Channel Transformer\nTransducer (MCTT)</i>, which features end-to-end multi-channel training,\nlow computation cost, and low latency so that it is suitable for streaming\ndecoding in on-device speech recognition. In a far-field in-house dataset,\nour MCTT outperforms stagewise multi-channel models with transformer-transducer\nup to 6.01% relative WER improvement (WERR). In addition, MCTT outperforms\nthe multi-channel transformer up to 11.62% WERR, and is 15.8 times\nfaster in terms of inference speed. We further show that we can improve\nthe computational cost of MCTT by constraining the future and previous\ncontext in attention computations.\n"
      ],
      "doi": "10.21437/Interspeech.2021-655",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "tsunoo21_interspeech": {
      "authors": [
        [
          "Emiru",
          "Tsunoo"
        ],
        [
          "Kentaro",
          "Shibata"
        ],
        [
          "Chaitanya",
          "Narisetty"
        ],
        [
          "Yosuke",
          "Kashiwagi"
        ],
        [
          "Shinji",
          "Watanabe"
        ]
      ],
      "title": "Data Augmentation Methods for End-to-End Speech Recognition on Distant-Talk Scenarios",
      "original": "0958",
      "page_count": 5,
      "order": 61,
      "p1": "301",
      "pn": "305",
      "abstract": [
        "Although end-to-end automatic speech recognition (E2E ASR) has achieved\ngreat performance in tasks that have numerous paired data, it is still\nchallenging to make E2E ASR robust against noisy and low-resource conditions.\nIn this study, we investigated data augmentation methods for E2E ASR\nin distant-talk scenarios. E2E ASR models are trained on the series\nof CHiME challenge datasets, which are suitable tasks for studying\nrobustness against noisy and spontaneous speech. We propose to use\nthree augmentation methods and their combinations: 1) data augmentation\nusing text-to-speech (TTS) data, 2) cycle-consistent generative adversarial\nnetwork (Cycle-GAN) augmentation trained to map two different audio\ncharacteristics, the one of clean speech and of noisy recordings, to\nmatch the testing condition, and 3) pseudo-label augmentation provided\nby the pretrained ASR module for smoothing label distributions. Experimental\nresults using the CHiME-6/CHiME-4 datasets show that each augmentation\nmethod individually improves the accuracy on top of the conventional\nSpecAugment; further improvements are obtained by combining these approaches.\nWe achieved 4.3% word error rate (WER) reduction, which was more significant\nthan that of the SpecAugment, when we combine all three augmentations\nfor the CHiME-6 task.\n"
      ],
      "doi": "10.21437/Interspeech.2021-958",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "ma21_interspeech": {
      "authors": [
        [
          "Guodong",
          "Ma"
        ],
        [
          "Pengfei",
          "Hu"
        ],
        [
          "Jian",
          "Kang"
        ],
        [
          "Shen",
          "Huang"
        ],
        [
          "Hao",
          "Huang"
        ]
      ],
      "title": "Leveraging Phone Mask Training for Phonetic-Reduction-Robust E2E Uyghur Speech Recognition",
      "original": "0964",
      "page_count": 5,
      "order": 62,
      "p1": "306",
      "pn": "310",
      "abstract": [
        "In Uyghur speech, consonant and vowel reduction are often encountered,\nespecially in spontaneous speech with high speech rate, which will\ncause a degradation of speech recognition performance. To solve this\nproblem, we propose an effective phone mask training method for Conformer-based\nUyghur end-to-end (E2E) speech recognition. The idea is to randomly\nmask off a certain percentage features of phones during model training,\nwhich simulates the above verbal phenomena and facilitates E2E model\nto learn more contextual information. According to experiments, the\nabove issues can be greatly alleviated. In addition, deep investigations\nare carried out into different units in masking, which shows the effectiveness\nof our proposed masking unit. We also further study the masking method\nand optimize filling strategy of phone mask. Finally, compared with\nConformer-based E2E baseline without mask training, our model demonstrates\nabout 5.51% relative Word Error Rate (WER) reduction on reading speech\nand 12.92% on spontaneous speech, respectively. The above approach\nhas also been verified on test-set of open-source data THUYG-20, which\nshows 20% relative improvements.\n"
      ],
      "doi": "10.21437/Interspeech.2021-964",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "likhomanenko21_interspeech": {
      "authors": [
        [
          "Tatiana",
          "Likhomanenko"
        ],
        [
          "Qiantong",
          "Xu"
        ],
        [
          "Vineel",
          "Pratap"
        ],
        [
          "Paden",
          "Tomasello"
        ],
        [
          "Jacob",
          "Kahn"
        ],
        [
          "Gilad",
          "Avidov"
        ],
        [
          "Ronan",
          "Collobert"
        ],
        [
          "Gabriel",
          "Synnaeve"
        ]
      ],
      "title": "Rethinking Evaluation in ASR: Are Our Models Robust Enough?",
      "original": "1758",
      "page_count": 5,
      "order": 63,
      "p1": "311",
      "pn": "315",
      "abstract": [
        "Is pushing numbers on a single benchmark valuable in automatic speech\nrecognition? Research results in acoustic modeling are typically evaluated\nbased on performance on a single dataset. While the research community\nhas coalesced around various benchmarks, we set out to understand generalization\nperformance in acoustic modeling across datasets &#8212; in particular,\nif models trained on a single dataset transfer to other (possibly out-of-domain)\ndatasets. Further, we demonstrate that when a large enough set of benchmarks\nis used, average word error rate (WER) performance over them provides\na good proxy for performance on real-world data. Finally, we show that\ntraining a single acoustic model on the most widely-used datasets &#8212;\ncombined &#8212; reaches competitive performance on both research and\nreal-world benchmarks.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1758",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "lam21_interspeech": {
      "authors": [
        [
          "Max W.Y.",
          "Lam"
        ],
        [
          "Jun",
          "Wang"
        ],
        [
          "Chao",
          "Weng"
        ],
        [
          "Dan",
          "Su"
        ],
        [
          "Dong",
          "Yu"
        ]
      ],
      "title": "Raw Waveform Encoder with Multi-Scale Globally Attentive Locally Recurrent Networks for End-to-End Speech Recognition",
      "original": "2084",
      "page_count": 5,
      "order": 64,
      "p1": "316",
      "pn": "320",
      "abstract": [
        "End-to-end speech recognition generally uses hand-engineered acoustic\nfeatures as input and excludes the feature extraction module from its\njoint optimization. To extract learnable and adaptive features and\nmitigate information loss, we propose a new encoder that adopts globally\nattentive locally recurrent (GALR) networks and directly takes raw\nwaveform as input. We observe improved ASR performance and robustness\nby applying GALR on different window lengths to aggregate fine-grain\ntemporal information into multi-scale acoustic features. Experiments\nare conducted on a benchmark dataset <i>AISHELL-2</i> and two large-scale\nMandarin speech corpus of 5,000 hours and 21,000 hours. With faster\nspeed and comparable model size, our proposed multi-scale GALR waveform\nencoder achieved consistent character error rate reductions (CERRs)\nfrom 7.9% to 28.1% relative over strong baselines, including Conformer\nand TDNN-Conformer. In particular, our approach demonstrated notable\nrobustness than the traditional handcrafted features and outperformed\nthe baseline MFCC-based TDNN-Conformer model by a 15.2% CERR on a music-mixed\nreal-world speech test set.\n"
      ],
      "doi": "10.21437/Interspeech.2021-2084",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "hou21_interspeech": {
      "authors": [
        [
          "Yuanbo",
          "Hou"
        ],
        [
          "Zhesong",
          "Yu"
        ],
        [
          "Xia",
          "Liang"
        ],
        [
          "Xingjian",
          "Du"
        ],
        [
          "Bilei",
          "Zhu"
        ],
        [
          "Zejun",
          "Ma"
        ],
        [
          "Dick",
          "Botteldooren"
        ]
      ],
      "title": "Attention-Based Cross-Modal Fusion for Audio-Visual Voice Activity Detection in Musical Video Streams",
      "original": "0037",
      "page_count": 5,
      "order": 65,
      "p1": "321",
      "pn": "325",
      "abstract": [
        "Many previous audio-visual voice-related works focus on speech, ignoring\nthe singing voice in the growing number of musical video streams on\nthe Internet. For processing diverse musical video data, voice activity\ndetection is a necessary step. This paper attempts to detect the speech\nand singing voices of target performers in musical video streams using\naudio-visual information. To integrate information of audio and visual\nmodalities, a multi-branch network is proposed to learn audio and image\nrepresentations, and the representations are fused by attention based\non semantic similarity to shape the acoustic representations through\nthe probability of anchor vocalization. Experiments show the proposed\naudio-visual multi-branch network far outperforms the audio-only model\nin challenging acoustic environments, indicating the cross-modal information\nfusion based on semantic correlation is sensible and successful.\n"
      ],
      "doi": "10.21437/Interspeech.2021-37",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "kim21b_interspeech": {
      "authors": [
        [
          "Ui-Hyun",
          "Kim"
        ]
      ],
      "title": "Noise-Tolerant Self-Supervised Learning for Audio-Visual Voice Activity Detection",
      "original": "0043",
      "page_count": 5,
      "order": 66,
      "p1": "326",
      "pn": "330",
      "abstract": [
        "Recent audio-visual voice activity detectors based on supervised learning\nrequire large amounts of labeled training data with manual mouth-region\ncropping in videos, and the performance is sensitive to a mismatch\nbetween the training and testing noise conditions. This paper introduces\ncontrastive self-supervised learning for audio-visual voice activity\ndetection as a possible solution to such problems. In addition, a novel\nself-supervised learning framework is proposed to improve overall training\nefficiency and testing performance on noise-corrupted datasets, as\nin real-world scenarios. This framework includes a branched audio encoder\nand a noise-tolerant loss function to cope with the uncertainty of\nspeech and noise feature separation in a self-supervised manner. Experimental\nresults, particularly under mismatched noise conditions, demonstrate\nthe improved performance compared with a self-supervised learning baseline\nand a supervised learning framework.\n"
      ],
      "doi": "10.21437/Interspeech.2021-43",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "park21_interspeech": {
      "authors": [
        [
          "Hyun-Jin",
          "Park"
        ],
        [
          "Pai",
          "Zhu"
        ],
        [
          "Ignacio Lopez",
          "Moreno"
        ],
        [
          "Niranjan",
          "Subrahmanya"
        ]
      ],
      "title": "Noisy Student-Teacher Training for Robust Keyword Spotting",
      "original": "0072",
      "page_count": 5,
      "order": 67,
      "p1": "331",
      "pn": "335",
      "abstract": [
        "We propose self-training with noisy student-teacher approach for streaming\nkeyword spotting, that can utilize large-scale unlabeled data and aggressive\ndata augmentation. The proposed method applies aggressive data augmentation\n(spectral augmentation) on the input of both student and teacher and\nutilize unlabeled data at scale, which significantly boosts the accuracy\nof student against challenging conditions. Such aggressive augmentation\nusually degrades model performance when used with supervised training\nwith hard-labeled data. Experiments show that aggressive spec augmentation\non baseline supervised training method degrades accuracy, while the\nproposed self-training with noisy student-teacher training improves\naccuracy of some difficult-conditioned test sets by as much as 60%.\n"
      ],
      "doi": "10.21437/Interspeech.2021-72",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "ichikawa21_interspeech": {
      "authors": [
        [
          "Osamu",
          "Ichikawa"
        ],
        [
          "Kaito",
          "Nakano"
        ],
        [
          "Takahiro",
          "Nakayama"
        ],
        [
          "Hajime",
          "Shirouzu"
        ]
      ],
      "title": "Multi-Channel VAD for Transcription of Group Discussion",
      "original": "0200",
      "page_count": 5,
      "order": 68,
      "p1": "336",
      "pn": "340",
      "abstract": [
        "Attempts are being made to visualize the learning process by attaching\nmicrophones to students participating in group works conducted in classrooms,\nand subsequently, their speech using an automatic speech recognition\n(ASR) system. However, the voices of nearby students frequently become\nmixed with the output speech data, even when using close-talk microphones\nwith noise robustness. To resolve this challenge, in this paper, we\npropose using multi-channel voice activity detection (VAD) to determine\nthe speech segments of a target speaker while also referencing the\noutput speech from the microphones attached to the other speakers in\nthe group. The conducted evaluation experiments using the actual speech\nof middle school students during group work lessons showed that our\nproposed method significantly improves the frame error rate (38.7%)\ncompared to that of the conventional technology, single-channel VAD\n(49.5%). In our view, conventional approaches, such as distributed\nmicrophone arrays and deep learning, are somewhat dependent on the\ntemporal stationarity of the speakers&#8217; positions. However, the\nproposed method is essentially a VAD process and thus works robustly.\nIt is the practical and proven solution in a real classroom environment.\n"
      ],
      "doi": "10.21437/Interspeech.2021-200",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "zhou21_interspeech": {
      "authors": [
        [
          "Hengshun",
          "Zhou"
        ],
        [
          "Jun",
          "Du"
        ],
        [
          "Hang",
          "Chen"
        ],
        [
          "Zijun",
          "Jing"
        ],
        [
          "Shifu",
          "Xiong"
        ],
        [
          "Chin-Hui",
          "Lee"
        ]
      ],
      "title": "Audio-Visual Information Fusion Using Cross-Modal Teacher-Student Learning for Voice Activity Detection in Realistic Environments",
      "original": "0592",
      "page_count": 5,
      "order": 69,
      "p1": "341",
      "pn": "345",
      "abstract": [
        "We propose an information fusion approach to audio-visual voice activity\ndetection (AV-VAD) based on cross-modal teacher-student learning leveraging\non factorized bilinear pooling (FBP) and Kullback-Leibler (KL) regularization.\nFirst, we design an audio-visual network by using FBP fusion to fully\nutilize the interaction between audio and video modalities. Next, to\ntransfer the rich information in audio-based VAD (A-VAD) model trained\nwith a massive audio-only dataset to AV-VAD model built with relatively\nlimited multi-modal data, a cross-modal teacher-student learning framework\nis then proposed based on cross entropy with regulated KL-divergence.\nFinally, evaluated on an in-house dataset recorded in realistic conditions\nusing standard VAD metrics, the proposed approach yields consistent\nand significant improvements over other state-of-the-art techniques.\nMoreover, by applying our AV-VAD technique to an audio-visual Chinese\nspeech recognition task, the character error rate is reduced by 24.15%\nand 8.66% from A-VAD and the baseline AV-VAD systems, respectively.\n"
      ],
      "doi": "10.21437/Interspeech.2021-592",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "makishima21_interspeech": {
      "authors": [
        [
          "Naoki",
          "Makishima"
        ],
        [
          "Mana",
          "Ihori"
        ],
        [
          "Tomohiro",
          "Tanaka"
        ],
        [
          "Akihiko",
          "Takashima"
        ],
        [
          "Shota",
          "Orihashi"
        ],
        [
          "Ryo",
          "Masumura"
        ]
      ],
      "title": "Enrollment-Less Training for Personalized Voice Activity Detection",
      "original": "0731",
      "page_count": 5,
      "order": 70,
      "p1": "346",
      "pn": "350",
      "abstract": [
        "We present a novel personalized voice activity detection (PVAD) learning\nmethod that does not require enrollment data during training. PVAD\nis a task to detect the speech segments of a specific target speaker\nat the frame level using enrollment speech of the target speaker. Since\nPVAD must learn speakers&#8217; speech variations to clarify the boundary\nbetween speakers, studies on PVAD used large-scale datasets that contain\nmany utterances for each speaker. However, the datasets to train a\nPVAD model are often limited because substantial cost is needed to\nprepare such a dataset. In addition, we cannot utilize the datasets\nused to train the standard VAD because they often lack speaker labels.\nTo solve these problems, our key idea is to use one utterance as both\na kind of enrollment speech and an input to the PVAD during training,\nwhich enables PVAD training without enrollment speech. In our proposed\nmethod, called enrollment-less training, we augment one utterance so\nas to create variability between the input and the enrollment speech\nwhile keeping the speaker identity, which avoids the mismatch between\ntraining and inference. Our experimental results demonstrate the efficacy\nof the method.\n"
      ],
      "doi": "10.21437/Interspeech.2021-731",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "nonaka21_interspeech": {
      "authors": [
        [
          "Yuto",
          "Nonaka"
        ],
        [
          "Chee Siang",
          "Leow"
        ],
        [
          "Akio",
          "Kobayashi"
        ],
        [
          "Takehito",
          "Utsuro"
        ],
        [
          "Hiromitsu",
          "Nishizaki"
        ]
      ],
      "title": "Voice Activity Detection for Live Speech of Baseball Game Based on Tandem Connection with Speech/Noise Separation Model",
      "original": "0792",
      "page_count": 5,
      "order": 71,
      "p1": "351",
      "pn": "355",
      "abstract": [
        "When applying voice activity detection (VAD) to a noisy sound, in general,\nnoise reduction (speech separation) and VAD are performed separately.\nIn this case, the noise reduction may suppress the speech, and the\nVAD may not work well for the speech after the noise reduction. This\nstudy proposes a VAD model through the tandem connection of neural\nnetwork-based noise separation and a VAD model. By training the two\nmodels simultaneously, the noise separation model is expected to be\ntrained to consider the VAD results, and thus effective noise separation\ncan be achieved. Moreover, the improved speech/noise separation model\nwill improve the accuracy of the VAD model. In this research, we deal\nwith real-live speeches from baseball games, which have a very poor\nsignal-to-noise ratio. The VAD experiments showed that the VAD performance\nat the frame level achieved 4.2 points improvement in F1-score by tandemly\nconnecting the speech/noise separation model and the VAD model.\n"
      ],
      "doi": "10.21437/Interspeech.2021-792"
    },
    "kwon21_interspeech": {
      "authors": [
        [
          "Young D.",
          "Kwon"
        ],
        [
          "Jagmohan",
          "Chauhan"
        ],
        [
          "Cecilia",
          "Mascolo"
        ]
      ],
      "title": "FastICARL: Fast Incremental Classifier and Representation Learning with Efficient Budget Allocation in Audio Sensing Applications",
      "original": "1091",
      "page_count": 5,
      "order": 72,
      "p1": "356",
      "pn": "360",
      "abstract": [
        "Various incremental learning (IL) approaches have been proposed to\nhelp deep learning models learn new tasks/classes continuously without\nforgetting what was learned previously (i.e., avoid catastrophic forgetting).\nWith the growing number of deployed audio sensing applications that\nneed to dynamically incorporate new tasks and changing input distribution\nfrom users, the ability of IL on-device becomes essential for both\nefficiency and user privacy.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  However, prior works\nsuffer from high computational costs and storage demands which hinders\nthe deployment of IL on-device. In this work, to overcome these limitations,\nwe develop an end-to-end and on-device IL framework, FastICARL, that\nincorporates an exemplar-based IL and quantization in the context of\naudio-based applications. We first employ k-nearest-neighbor to reduce\nthe latency of IL. Then, we jointly utilize a quantization technique\nto decrease the storage requirements of IL. We implement FastICARL\non two types of mobile devices and demonstrate that FastICARL remarkably\ndecreases the IL time up to 78&#8211;92% and the storage requirements\nby 2&#8211;4 times without sacrificing its performance. FastICARL enables\ncomplete on-device IL, ensuring user privacy as the user data does\nnot need to leave the device.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1091",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "wei21_interspeech": {
      "authors": [
        [
          "Bo",
          "Wei"
        ],
        [
          "Meirong",
          "Yang"
        ],
        [
          "Tao",
          "Zhang"
        ],
        [
          "Xiao",
          "Tang"
        ],
        [
          "Xing",
          "Huang"
        ],
        [
          "Kyuhong",
          "Kim"
        ],
        [
          "Jaeyun",
          "Lee"
        ],
        [
          "Kiho",
          "Cho"
        ],
        [
          "Sung-Un",
          "Park"
        ]
      ],
      "title": "End-to-End Transformer-Based Open-Vocabulary Keyword Spotting with Location-Guided Local Attention",
      "original": "1335",
      "page_count": 5,
      "order": 73,
      "p1": "361",
      "pn": "365",
      "abstract": [
        "Open-vocabulary keyword spotting (KWS) aims to detect arbitrary keywords\nfrom continuous speech, which allows users to define their personal\nkeywords. In this paper, we propose a novel location guided end-to-end\n(E2E) keyword spotting system. Firstly, we predict endpoints of keyword\nin the entire speech based on attention mechanism. Secondly, we calculate\nthe existence probability of keyword by fusing the located keyword\nspeech segment and text with local attention. The results on Librispeech\ndataset and Google speech commands dataset show our proposed method\nsignificantly outperforms the baseline method and the latest small-footprint\nE2E KWS method.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1335",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "bhati21_interspeech": {
      "authors": [
        [
          "Saurabhchand",
          "Bhati"
        ],
        [
          "Jes\u00fas",
          "Villalba"
        ],
        [
          "Piotr",
          "\u017belasko"
        ],
        [
          "Laureano",
          "Moro-Vel\u00e1zquez"
        ],
        [
          "Najim",
          "Dehak"
        ]
      ],
      "title": "Segmental Contrastive Predictive Coding for Unsupervised Word Segmentation",
      "original": "1874",
      "page_count": 5,
      "order": 74,
      "p1": "366",
      "pn": "370",
      "abstract": [
        "Automatic detection of phoneme or word-like units is one of the core\nobjectives in zero-resource speech processing. Recent attempts employ\nself-supervised training methods, such as contrastive predictive coding\n(CPC), where the next frame is predicted given past context. However,\nCPC only looks at the audio signal&#8217;s frame-level structure. We\novercome this limitation with a segmental contrastive predictive coding\n(SCPC) framework that can model the signal structure at a higher level\ne.g. at the phoneme level. In this framework, a convolutional neural\nnetwork learns frame-level representation from the raw waveform via\nnoise-contrastive estimation (NCE). A differentiable boundary detector\nfinds variable-length segments, which are then used to optimize a segment\nencoder via NCE to learn segment representations. The differentiable\nboundary detector allows us to train frame-level and segment-level\nencoders jointly. Typically, phoneme and word segmentation are treated\nas separate tasks. We unify them and experimentally show that our single\nmodel outperforms existing phoneme and word segmentation methods on\nTIMIT and Buckeye datasets. We analyze the impact of boundary threshold\nand when is the right time to include the segmental loss in the learning\nprocess.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1874",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "xu21b_interspeech": {
      "authors": [
        [
          "Xuenan",
          "Xu"
        ],
        [
          "Heinrich",
          "Dinkel"
        ],
        [
          "Mengyue",
          "Wu"
        ],
        [
          "Kai",
          "Yu"
        ]
      ],
      "title": "A Lightweight Framework for Online Voice Activity Detection in the Wild",
      "original": "1977",
      "page_count": 5,
      "order": 75,
      "p1": "371",
      "pn": "375",
      "abstract": [
        "Voice activity detection (VAD) is an essential pre-processing component\nfor speech-related tasks such as automatic speech recognition (ASR).\nTraditional VAD systems require strong frame-level supervision for\ntraining, inhibiting their performance in real-world test scenarios.\nPreviously, the general-purpose VAD (GPVAD) framework has been proposed\nto enhance noise robustness significantly. However, GPVAD models are\ncomparatively large and only work for offline evaluation. This work\nproposes the use of a knowledge distillation framework, where a (large,\noffline) teacher model provides frame-level supervision to a (light,\nonline) student model. Our experiments verify that our proposed lightweight\nstudent models outperform GPVAD on all test sets, including clean,\nsynthetic and real-world scenarios. Our smallest student model only\nuses 2.2% of the parameters and 15.9% duration cost of our teacher\nmodel for inference when evaluated on a Raspberry Pi.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1977",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "chlebowski21_interspeech": {
      "authors": [
        [
          "Aur\u00e9lie",
          "Chl\u00e9bowski"
        ],
        [
          "Nicolas",
          "Ballier"
        ]
      ],
      "title": "&#8220;See what I mean, huh?&#8221; Evaluating Visual Inspection of F<SUB>0</SUB> Tracking in Nasal Grunts",
      "original": "0129",
      "page_count": 5,
      "order": 76,
      "p1": "376",
      "pn": "380",
      "abstract": [
        "This paper proposes to evaluate the method used in Chl&#233;bowski\nand Ballier [1] for the annotation of F<SUB>0</SUB> variations in nasal\ngrunts. We discuss and test issues raised by this kind of approach\nexclusively based on visual inspection of the F<SUB>0</SUB> tracking\nin <i>Praat</i> [2]. Results tend to show that consistency in the annotation\ndepends on acoustic features intrinsic to the grunts such as F<SUB>0</SUB>\nslope and duration that are sensitive to display settings. We nonetheless\nacknowledge the potential benefits of such a method for automation\nand implementation in IA and in this respect, we introduce <i>Prosogram</i>\n[3] as an alternative material-maker.\n"
      ],
      "doi": "10.21437/Interspeech.2021-129"
    },
    "wang21c_interspeech": {
      "authors": [
        [
          "Bruce Xiao",
          "Wang"
        ],
        [
          "Vincent",
          "Hughes"
        ]
      ],
      "title": "System Performance as a Function of Calibration Methods, Sample Size and Sampling Variability in Likelihood Ratio-Based Forensic Voice Comparison",
      "original": "0267",
      "page_count": 5,
      "order": 77,
      "p1": "381",
      "pn": "385",
      "abstract": [
        "In data-driven forensic voice comparison, sample size is an issue which\ncan have substantial effects on system output. Numerous calibration\nmethods have been developed and some have been proposed as solutions\nto sample size issues. In this paper, we test four calibration methods\n(i.e. logistic regression, regularised logistic regression, Bayesian\nmodel, ELUB) under different conditions of sampling variability and\nsample size. Training and test scores were simulated from skewed distributions\nderived from real experiments, increasing sample sizes from 20 to 100\nspeakers for both the training and test sets. For each sample size,\nthe experiments were replicated 100 times to test the susceptibility\nof different calibration methods to sampling variability. The C<SUB>llr</SUB>\nmean and range across replications were used for evaluation. The Bayesian\nmodel and regularized logistic regression produced the most stable\nC<SUB>llr</SUB> values when the sample size is small (i.e. 20 speakers),\nalthough mean C<SUB>llr</SUB> is consistently lowest using logistic\nregression. The ELUB calibration method generally is the least preferred\nas it is the most sensitive to sample size and sampling variability\n(mean = 0.66, range = 0.21&#8211;0.59).\n"
      ],
      "doi": "10.21437/Interspeech.2021-267",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "bonneau21_interspeech": {
      "authors": [
        [
          "Anne",
          "Bonneau"
        ]
      ],
      "title": "Voicing Assimilations by French Speakers of German in Stop-Fricative Sequences",
      "original": "0601",
      "page_count": 5,
      "order": 78,
      "p1": "386",
      "pn": "390",
      "abstract": [
        "Voicing assimilations inside groups of obstruents occur in opposite\ndirections in French and German, where they are respectively regressive\nand progressive. The aim of the study is to investigate (1) whether\nnon native speakers (here French learners of German) are apt to acquire\nsubtle L2 specificities like assimilation direction, although they\nare not aware of their very existence, or (2) whether their productions\ndepend essentially upon other factors, in particular consonant place\nof articulation. To that purpose, a corpus made up of groups of obstruents\n(/t/ followed by /z/, /v/ or /f/) embedded into sentences has been\nrecorded by 16 French learners of German (beginners and advanced speakers).\nThe consonants are separated by a word or a syllable boundary. Results,\nderived from the analysis of consonant periodicity and duration, do\nnot stand for an acquisition of progressive assimilation, even by advanced\nspeakers, and do not show differences between the productions of advanced\nspeakers and beginners. On the contrary the boundary type and the consonant\nplace of articulation play an important role in the presence or absence\nof voicing inside obstruent groups. The role of phonetic, universal\nmechanisms against linguistic specific rules is discussed to interpret\nthe data.\n"
      ],
      "doi": "10.21437/Interspeech.2021-601",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "chakraborty21_interspeech": {
      "authors": [
        [
          "Titas",
          "Chakraborty"
        ],
        [
          "Vaishali",
          "Patil"
        ],
        [
          "Preeti",
          "Rao"
        ]
      ],
      "title": "The Four-Way Classification of Stops with Voicing and Aspiration for Non-Native Speech Evaluation",
      "original": "0635",
      "page_count": 5,
      "order": 79,
      "p1": "391",
      "pn": "395",
      "abstract": [
        "The four-way distinction of plosives in terms of voicing and aspiration\nis rare in the world&#8217;s languages, but is an important characteristic\nof the Indo-Aryan language family. Both perception and production pose\nchallenges to the language learner whose native tongue does not afford\nthe specific distinctions. A study of the acoustic-phonetics of the\nsounds and their possible dependence on speaker characteristics, such\nas gender or native tongue, can inform methods for accurate feedback\non the quality of the phones produced by a non-native learner. We present\na system for the four-way classification of stops building on features\npreviously proposed for aspiration detection in unvoiced and voiced\nplosives. Trained on an available dataset of Hindi speech by native\nspeakers, the system works reliably on production data comprising Bangla\nwords uttered by native Bangla and non-native (American English L1)\nspeakers. The latter display a variety of articulation patterns for\nthe given target contrasts, providing useful insights related to L1\ninfluence on the voicing-aspiration production in word-initial CV contexts.\n"
      ],
      "doi": "10.21437/Interspeech.2021-635",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "urooj21_interspeech": {
      "authors": [
        [
          "Saba",
          "Urooj"
        ],
        [
          "Benazir",
          "Mumtaz"
        ],
        [
          "Sarmad",
          "Hussain"
        ],
        [
          "Ehsan ul",
          "Haq"
        ]
      ],
      "title": "Acoustic and Prosodic Correlates of Emotions in Urdu Speech",
      "original": "0910",
      "page_count": 5,
      "order": 80,
      "p1": "396",
      "pn": "400",
      "abstract": [
        "Emotional speech corpora exhibit differences in duration, intensity\nand fundamental frequency. We investigated acoustic as well as prosodic\ncorrelates of emotional speech in Urdu. We recorded a corpus of 23\nsentences from four speakers of Urdu covering four emotional states.\nMain results show that: a) sadness exhibits lowest utterance rate,\nlowest intensity and narrow pitch range, b) anger exhibits highest\nutterance rate, highest intensity and wider pitch range, and c) happiness\nexhibits higher utterance rate and wider pitch range as compared to\nneutral and sadness; but no significant differences are found between\nthe intensity and pitch range of anger and happiness. The analysis\nalso shows differences in terms of pitch or phrase accents and boundary\ntones.\n"
      ],
      "doi": "10.21437/Interspeech.2021-910",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "tamim21_interspeech": {
      "authors": [
        [
          "Nour",
          "Tamim"
        ],
        [
          "Silke",
          "Hamann"
        ]
      ],
      "title": "Voicing Contrasts in the Singleton Stops of Palestinian Arabic: Production and Perception",
      "original": "1079",
      "page_count": 5,
      "order": 81,
      "p1": "401",
      "pn": "405",
      "abstract": [
        "This study investigates the stop voicing contrast in Palestinian Arabic\n(PA) by examining Voice Onset Time (VOT) in both production and perception.\nAn acoustic analysis of the recordings of 8 speakers showed that word-initial\nvoiced stops in sentence context have an average VOT of -93 msec, and\nword-initial voiceless stops one of 29 msec. PA thus belongs, like\nmost dialects of Arabic, to true voicing languages, i.e., languages\nwith a contrast between voicing lead and short lag VOT.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  We furthermore tested\nwhether the phoneme /b/, without voiceless counterpart /p/ in PA, has\nsimilar VOT values to /d, d<sup>&#x295;</sup>/, which have voiceless\ncounterparts /t, t<sup>&#x295;</sup>/. Similarly, we compared /k/,\nwithout counterpart /g/ in the PA dialect we investigated, to /t, t<sup>&#x295;</sup>/.\nFor /b/ we found very similar VOT values to /d, d<sup>&#x295;</sup>/,\nwhile for /k/ we found a difference to /t, t<sup>&#x295;</sup>/, attributable\nto a general tendency of velars to have longer VOT than denti-alveolars.\nWe thus found no evidence for a less contrastive realization of unpaired\nplosives in PA.<br style=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  In a categorization experiment of the denti-alveolar phoneme pairs\nwith the same 8 speakers, VOT proved sufficient as a perceptual cue,\nthough f0 of the following vowel also influenced the categorization.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1079",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "coy21_interspeech": {
      "authors": [
        [
          "Thomas",
          "Coy"
        ],
        [
          "Vincent",
          "Hughes"
        ],
        [
          "Philip",
          "Harrison"
        ],
        [
          "Amelia J.",
          "Gully"
        ]
      ],
      "title": "A Comparison of the Accuracy of Dissen and Keshet&#8217;s (2016) DeepFormants and Traditional LPC Methods for Semi-Automatic Speaker Recognition",
      "original": "1487",
      "page_count": 5,
      "order": 82,
      "p1": "406",
      "pn": "410",
      "abstract": [
        "There is a growing trend in the field of forensic speech science towards\nintegrating the vanguard of speech technology with traditional linguistic\nmethods in pursuit of both scalable (i.e. automatable) and accurate\nevidential methods. To this end, this paper investigates DeepFormants,\na DNN formant estimator which its creators, Dissen and Keshet [1],\nclaim constitutes an accurate tool ready for use by linguists. In the\npresent paper, DeepFormants is integrated into semi-automatic speaker\nrecognition systems using long-term formant distributions and compared\nagainst systems using traditional linear predictive coding. The readiness\nof the tool is assessed on overall speaker recognition performance,\nmeasured using equal error rates (EER) and the log LR cost functions\n(C<SUB>llr</SUB>). In high-quality conditions, DeepFormants outperforms\nthe best performing LPC systems. Much poorer overall performance is\nfound in channel mismatch conditions for DeepFormants, suggesting it\nis not adaptable to conditions it was not originally trained on. However,\nthis is also true of LPC methods, raising questions over the validity\nof using formant analysis at all in such cases. A major benefit of\nDeepFormants over LPC is that the analyst does not need to specify\nsettings. We discuss the implications of this with regard to results\nfor individual speakers.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1487"
    },
    "jessen21_interspeech": {
      "authors": [
        [
          "Michael",
          "Jessen"
        ]
      ],
      "title": "MAP Adaptation Characteristics in Forensic Long-Term Formant Analysis",
      "original": "1697",
      "page_count": 5,
      "order": 83,
      "p1": "411",
      "pn": "415",
      "abstract": [
        "Forensic data from long-term formant analysis were used as input to\nthe GMM-UBM approach, which is a way of deriving Likelihood Ratios.\nTests were performed running 22 same-speaker comparisons and 462 different-speaker\ncomparisons from a corpus of anonymized casework data involving telephone-intercepted\nspeech. In a first series of tests, the number of Gaussian modules\nfor GMM-modeling was increased from 1 to 32. In a second series of\ntests the duration of formant input in the compared files was reduced\nfrom 10 seconds to 5 and then to 2.5. All tests were performed both\nwithout and with the use of MAP adaptation. Results were evaluated\nin terms of overall performance characteristics EER and Cllr and in\nterms of score distributions visualized as Tippett plots. The main\ngoal of the study was to compare the use and non-use of MAP and to\nlook at the practical forensic implications of the difference. Results\nshow that in terms of overall performance characteristics there is\nlittle difference between the selection and de-selection of MAP. Tippett\nplot patterns however reveal strong differences. Application of MAP\nallows for more symmetric same- and different-speaker distributions\nand shows more robustness against duration reductions, both of which\nare forensically important.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1697",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "lo21_interspeech": {
      "authors": [
        [
          "Justin J.H.",
          "Lo"
        ]
      ],
      "title": "Cross-Linguistic Speaker Individuality of Long-Term Formant Distributions: Phonetic and Forensic Perspectives",
      "original": "1699",
      "page_count": 5,
      "order": 84,
      "p1": "416",
      "pn": "420",
      "abstract": [
        "This study considers issues of language- and speaker-specificity in\nlong-term formant distributions (LTFDs) from phonetic and forensic\nperspectives and examines their potential value in cases of cross-language\nforensic voice comparison. Acoustic analysis of 60 male English&#8211;French\nbilinguals revealed systematic differences in LTFDs between the two\nlanguages, with higher LTF2&#8211;4 in French than in English. Cross-linguistic\ndifferences in the shapes of LTFDs were also found. These differences\nare argued to reflect not only vowel inventories of each language but\nalso language-specific phonetic settings. At the same time, a high\ndegree of within-speaker consistency was found across languages. Likelihood\nratio based testing was carried out to examine the effect of language\nmismatch on the utility of LTFDs as speaker discriminants. Results\nshowed that while the performance of LTFDs was worse in cross-language\ncomparisons than in same-language comparisons, they were still capable\nof providing speaker-specific information. These findings demonstrate\nthat, in spite of deteriorated performance, LTFDs are still potentially\nuseful speaker discriminants in cases of language mismatch. These findings\nthus call for further empirical investigation into the use of linguistic-phonetic\nfeatures in cross-language comparisons.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1699",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "soo21_interspeech": {
      "authors": [
        [
          "Rachel",
          "Soo"
        ],
        [
          "Khia A.",
          "Johnson"
        ],
        [
          "Molly",
          "Babel"
        ]
      ],
      "title": "Sound Change in Spontaneous Bilingual Speech: A Corpus Study on the Cantonese n-l Merger in Cantonese-English Bilinguals",
      "original": "1754",
      "page_count": 5,
      "order": 85,
      "p1": "421",
      "pn": "425",
      "abstract": [
        "In Cantonese and several other Chinese languages, /n/ is merging with\n/l/. The Cantonese merger appears categorical, with /n/ becoming /l/\nword-initially. This project aims to describe the status of /n/ and\n/l/ in bilingual Cantonese and English speech to better understand\nindividual differences at the interface of crosslinguistic influence\nand sound change. We examine bilingual speech using the SpiCE corpus,\ncomposed of speech from 34 early Cantonese-English bilinguals. Acoustic\nmeasures were collected on pre-vocalic nasal and lateral onsets in\nboth languages. If bilinguals maintain separate representations for\ncorresponding segments across languages, smaller differences between\n/n/ and /l/ are predicted in Cantonese compared to English. Measures\nof mid-frequency spectral tilt suggest that the /n/ and /l/ contrast\nis robustly maintained in English, but not Cantonese. The spacing of\nF2-F1 suggests small differences between Cantonese /n/ and /l/, and\nrobust differences in English. While cross-language categories appear\nindependent, substantial individual differences exist in the data.\nThese data contribute to the understanding of the /n/ and /l/ merger\nin Cantonese and other Chinese languages, in addition to providing\nempirical and theoretical insights into crosslinguistic influence in\nearly bilinguals.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1754",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "lalhminghlui21_interspeech": {
      "authors": [
        [
          "Wendy",
          "Lalhminghlui"
        ],
        [
          "Priyankoo",
          "Sarmah"
        ]
      ],
      "title": "Characterizing Voiced and Voiceless Nasals in Mizo",
      "original": "2104",
      "page_count": 5,
      "order": 86,
      "p1": "426",
      "pn": "430",
      "abstract": [
        "Mizo has voicing contrasts in nasals. This study investigates the acoustic\nproperties of Mizo voiced and voiceless nasals using nasometric measurements.\nThe dual channel data obtained for Mizo nasals is separated into oral\nand nasal channels and nasalance is calculated at every 10% of the\nduration of the nasals. Apart from that, the amount of voicing and\nduration of the nasals are also measured. The results show that nasalance\nis affected by the place of articulation of the nasals. Additionally,\nthe voiceless nasals are found to be significantly longer than the\nvoiced nasals.\n"
      ],
      "doi": "10.21437/Interspeech.2021-2104",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "schuller21_interspeech": {
      "authors": [
        [
          "Bj\u00f6rn W.",
          "Schuller"
        ],
        [
          "Anton",
          "Batliner"
        ],
        [
          "Christian",
          "Bergler"
        ],
        [
          "Cecilia",
          "Mascolo"
        ],
        [
          "Jing",
          "Han"
        ],
        [
          "Iulia",
          "Lefter"
        ],
        [
          "Heysem",
          "Kaya"
        ],
        [
          "Shahin",
          "Amiriparian"
        ],
        [
          "Alice",
          "Baird"
        ],
        [
          "Lukas",
          "Stappen"
        ],
        [
          "Sandra",
          "Ottl"
        ],
        [
          "Maurice",
          "Gerczuk"
        ],
        [
          "Panagiotis",
          "Tzirakis"
        ],
        [
          "Chlo\u00eb",
          "Brown"
        ],
        [
          "Jagmohan",
          "Chauhan"
        ],
        [
          "Andreas",
          "Grammenos"
        ],
        [
          "Apinan",
          "Hasthanasombat"
        ],
        [
          "Dimitris",
          "Spathis"
        ],
        [
          "Tong",
          "Xia"
        ],
        [
          "Pietro",
          "Cicuta"
        ],
        [
          "Leon J.M.",
          "Rothkrantz"
        ],
        [
          "Joeri A.",
          "Zwerts"
        ],
        [
          "Jelle",
          "Treep"
        ],
        [
          "Casper S.",
          "Kaandorp"
        ]
      ],
      "title": "The INTERSPEECH 2021 Computational Paralinguistics Challenge: COVID-19 Cough, COVID-19 Speech, Escalation &amp; Primates",
      "original": "0019",
      "page_count": 5,
      "order": 87,
      "p1": "431",
      "pn": "435",
      "abstract": [
        "The INTERSPEECH 2021 Computational Paralinguistics Challenge addresses\nfour different problems for the first time in a research competition\nunder well-defined conditions: In the <i>COVID-19 Cough</i> and <i>COVID-19\nSpeech</i> Sub-Challenges, a binary classification on COVID-19 infection\nhas to be made based on coughing sounds and speech; in the <i>Escalation</i>\nSub-Challenge, a three-way assessment of the level of escalation in\na dialogue is featured; and in the <i>Primates</i> Sub-Challenge, four\nspecies vs background need to be classified. We describe the Sub-Challenges,\nbaseline feature extraction, and classifiers based on the &#8216;usual&#8217;\n ComParE and BoAW features as well as deep unsupervised representation\nlearning using the  auDeep toolkit, and deep feature extraction from\npre-trained CNNs using the  Deep Spectrum toolkit; in addition, we\nadd deep end-to-end sequential modelling, and partially linguistic\nanalysis.\n"
      ],
      "doi": "10.21437/Interspeech.2021-19"
    },
    "soleraurena21_interspeech": {
      "authors": [
        [
          "Rub\u00e9n",
          "Solera-Ure\u00f1a"
        ],
        [
          "Catarina",
          "Botelho"
        ],
        [
          "Francisco",
          "Teixeira"
        ],
        [
          "Thomas",
          "Rolland"
        ],
        [
          "Alberto",
          "Abad"
        ],
        [
          "Isabel",
          "Trancoso"
        ]
      ],
      "title": "Transfer Learning-Based Cough Representations for Automatic Detection of COVID-19",
      "original": "1702",
      "page_count": 5,
      "order": 88,
      "p1": "436",
      "pn": "440",
      "abstract": [
        "In the last months, there has been an increasing interest in developing\nreliable, cost-effective, immediate and easy to use machine learning\nbased tools that can help health care operators, institutions, companies,\netc. to optimize their screening campaigns. In this line, several initiatives\nemerged aimed at the automatic detection of COVID-19 from speech, breathing\nand coughs, with inconclusive preliminary results. The ComParE 2021\nCOVID-19 Cough Sub-challenge provides researchers from all over the\nworld a suitable test-bed for the evaluation and comparison of their\nwork. In this paper, we present the INESC-ID contribution to the ComParE\n2021 COVID-19 Cough Sub-challenge. We leverage transfer learning to\ndevelop a set of three expert classifiers based on deep cough representation\nextractors. A calibrated decision-level fusion system provides the\nfinal classification of coughs recordings as either COVID-19 positive\nor negative. Results show unweighted average recalls of 72.3% and 69.3%\nin the development and test sets, respectively. Overall, the experimental\nassessment shows the potential of this approach although much more\nresearch on extended respiratory sounds datasets is needed.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1702",
      "author_area_id": "3",
      "author_area_label": "Paralinguistics in Speech and Language: Human and Automatic Analysis and Processing:14 Special Sessions"
    },
    "klumpp21_interspeech": {
      "authors": [
        [
          "P.",
          "Klumpp"
        ],
        [
          "T.",
          "Bocklet"
        ],
        [
          "T.",
          "Arias-Vergara"
        ],
        [
          "J.C.",
          "V\u00e1squez-Correa"
        ],
        [
          "P.A.",
          "P\u00e9rez-Toro"
        ],
        [
          "S.P.",
          "Bayerl"
        ],
        [
          "J.R.",
          "Orozco-Arroyave"
        ],
        [
          "Elmar",
          "N\u00f6th"
        ]
      ],
      "title": "The Phonetic Footprint of Covid-19?",
      "original": "1488",
      "page_count": 5,
      "order": 89,
      "p1": "441",
      "pn": "445",
      "abstract": [
        "Against the background of the ongoing pandemic, this year&#8217;s Computational\nParalinguistics Challenge featured a classification problem to detect\nCovid-19 from speech recordings. The presented approach is based on\na phonetic analysis of speech samples, thus it enabled us not only\nto discriminate between Covid and non-Covid samples, but also to better\nunderstand how the condition influenced an individual&#8217;s speech\nsignal.<br style=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  Our deep acoustic model was trained with datasets collected exclusively\nfrom healthy speakers. It served as a tool for segmentation and feature\nextraction on the samples from the challenge dataset. Distinct patterns\nwere found in the embeddings of phonetic classes that have their place\nof articulation deep inside the vocal tract. We observed profound differences\nin classification results for development and test splits, similar\nto the baseline method.<br style=\"mso-data-placement:same-cell;\" /><br\nstyle=\"mso-data-placement:same-cell;\" />\n  We concluded that, based\non our phonetic findings, it was safe to assume that our classifier\nwas able to reliably detect a pathological condition located in the\nrespiratory tract. However, we found no evidence to claim that the\nsystem was able to discriminate between Covid-19 and other respiratory\ndiseases.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1488",
      "author_area_id": "3",
      "author_area_label": "Paralinguistics in Speech and Language: Human and Automatic Analysis and Processing:14 Special Sessions"
    },
    "casanova21_interspeech": {
      "authors": [
        [
          "Edresson",
          "Casanova"
        ],
        [
          "Arnaldo",
          "Candido Jr."
        ],
        [
          "Ricardo Corso",
          "Fernandes Jr."
        ],
        [
          "Marcelo",
          "Finger"
        ],
        [
          "Lucas Rafael Stefanel",
          "Gris"
        ],
        [
          "Moacir Antonelli",
          "Ponti"
        ],
        [
          "Daniel Peixoto",
          "Pinto da Silva"
        ]
      ],
      "title": "Transfer Learning and Data Augmentation Techniques to the COVID-19 Identification Tasks in ComParE 2021",
      "original": "1798",
      "page_count": 5,
      "order": 90,
      "p1": "446",
      "pn": "450",
      "abstract": [
        "In this work, we propose several techniques to address data scarceness\nin ComParE 2021 COVID-19 identification tasks for the application of\ndeep models such as Convolutional Neural Networks. Data is initially\npreprocessed into spectrogram or MFCC-gram formats. After preprocessing,\nwe combine three different data augmentation techniques to be applied\nin model training. Then we employ transfer learning techniques from\npretrained audio neural networks. Those techniques are applied to several\ndistinct neural architectures. For COVID-19 identification in speech\nsegments, we obtained competitive results. On the other hand, in the\nidentification task based on cough data, we succeeded in producing\na noticeable improvement on existing baselines, reaching 75.9% unweighted\naverage recall (UAR).\n"
      ],
      "doi": "10.21437/Interspeech.2021-1798",
      "author_area_id": "3",
      "author_area_label": "Paralinguistics in Speech and Language: Human and Automatic Analysis and Processing:14 Special Sessions"
    },
    "illium21_interspeech": {
      "authors": [
        [
          "Steffen",
          "Illium"
        ],
        [
          "Robert",
          "M\u00fcller"
        ],
        [
          "Andreas",
          "Sedlmeier"
        ],
        [
          "Claudia-Linnhoff",
          "Popien"
        ]
      ],
      "title": "Visual Transformers for Primates Classification and Covid Detection",
      "original": "0273",
      "page_count": 5,
      "order": 91,
      "p1": "451",
      "pn": "455",
      "abstract": [
        "We apply the vision transformer, a deep machine learning model build\naround the attention mechanism, on mel-spectrogram representations\nof raw audio recordings. When adding mel-based data augmentation techniques\nand sample-weighting, we achieve comparable performance on both (PRS\nand CCS challenge) tasks of ComParE21, outperforming most single model\nbaselines. We further introduce overlapping vertical patching and evaluate\nthe influence of parameter configurations.\n"
      ],
      "doi": "10.21437/Interspeech.2021-273"
    },
    "pellegrini21_interspeech": {
      "authors": [
        [
          "Thomas",
          "Pellegrini"
        ]
      ],
      "title": "Deep-Learning-Based Central African Primate Species Classification with MixUp and SpecAugment",
      "original": "1911",
      "page_count": 5,
      "order": 92,
      "p1": "456",
      "pn": "460",
      "abstract": [
        "In this paper, we report experiments in which we aim to automatically\nclassify primate vocalizations according to four primate species of\ninterest, plus a background category with forest sound events. We compare\nseveral standard deep neural networks architectures: standard deep\nconvolutional neural networks (CNNs), MobileNets and ResNets. To tackle\nthe small size of the training dataset, less than seven thousand audio\nfiles, the data augmentation techniques SpecAugment and MixUp proved\nto be very useful. Against the very unbalanced classes of the dataset,\nwe used a balanced data sampler that showed to be efficient. An exponential\nmoving average of the model weights allowed to get slight further gains.\nThe best model was a standard 10-layer CNN, comprised of about five\nmillion parameters. It achieved a 93.6% Unweighted Average Recall (UAR)\non the development set, and generalized well on the test set with a\n92.5% UAR, outperforming an official baseline of 86.6%. We quantify\nthe performance gains brought by the augmentations and training tricks,\nand report fusion and classification experiments based on embeddings\nthat did not bring better results.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1911",
      "author_area_id": "3",
      "author_area_label": "Paralinguistics in Speech and Language: Human and Automatic Analysis and Processing:14 Special Sessions"
    },
    "muller21_interspeech": {
      "authors": [
        [
          "Robert",
          "M\u00fcller"
        ],
        [
          "Steffen",
          "Illium"
        ],
        [
          "Claudia",
          "Linnhoff-Popien"
        ]
      ],
      "title": "A Deep and Recurrent Architecture for Primate Vocalization Classification",
      "original": "1274",
      "page_count": 5,
      "order": 93,
      "p1": "461",
      "pn": "465",
      "abstract": [
        "Wildlife monitoring is an essential part of most conservation efforts\nwhere one of the many building blocks is acoustic monitoring. Acoustic\nmonitoring has the advantage of being non-invasive and applicable in\nareas of high vegetation. In this work, we present a deep and recurrent\narchitecture for the classification of primate vocalizations that is\nbased upon well proven modules such as bidirectional Long Short-Term\nMemory neural networks, pooling, normalized softmax and focal loss.\nAdditionally, we apply Bayesian optimization to obtain a suitable set\nof hyperparameters. We test our approach on a recently published dataset\nof primate vocalizations that were recorded in an African wildlife\nsanctuary. Using an ensemble of the best five models found during hyperparameter\noptimization on the development set, we achieve a Unweighted Average\nRecall of 89.3% on the test set. Our approach outperforms the best\nbaseline, an ensemble of various deep and shallow classifiers, which\nachieves a UAR of 87.5%.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1274",
      "author_area_id": "3",
      "author_area_label": "Paralinguistics in Speech and Language: Human and Automatic Analysis and Processing:14 Special Sessions"
    },
    "zwerts21_interspeech": {
      "authors": [
        [
          "Joeri A.",
          "Zwerts"
        ],
        [
          "Jelle",
          "Treep"
        ],
        [
          "Casper S.",
          "Kaandorp"
        ],
        [
          "Floor",
          "Meewis"
        ],
        [
          "Amparo C.",
          "Koot"
        ],
        [
          "Heysem",
          "Kaya"
        ]
      ],
      "title": "Introducing a Central African Primate Vocalisation Dataset for Automated Species Classification",
      "original": "0154",
      "page_count": 5,
      "order": 94,
      "p1": "466",
      "pn": "470",
      "abstract": [
        "Automated classification of animal vocalisations is a potentially powerful\nwildlife monitoring tool. Training robust classifiers requires sizable\nannotated datasets, which are not easily recorded in the wild. To circumvent\nthis problem, we recorded four primate species under semi-natural conditions\nin a wildlife sanctuary in Cameroon with the objective to train a classifier\ncapable of detecting species in the wild. Here, we introduce the collected\ndataset, describe our approach and initial results of classifier development.\nTo increase the efficiency of the annotation process, we condensed\nthe recordings with an energy/change based automatic vocalisation detection.\nSegmenting the annotated chunks into training, validation and test\nsets, initial results reveal up to 82% unweighted average recall test\nset performance in four-class primate species classification.\n"
      ],
      "doi": "10.21437/Interspeech.2021-154",
      "author_area_id": "3",
      "author_area_label": "Paralinguistics in Speech and Language: Human and Automatic Analysis and Processing"
    },
    "rizos21_interspeech": {
      "authors": [
        [
          "Georgios",
          "Rizos"
        ],
        [
          "Jenna",
          "Lawson"
        ],
        [
          "Zhuoda",
          "Han"
        ],
        [
          "Duncan",
          "Butler"
        ],
        [
          "James",
          "Rosindell"
        ],
        [
          "Krystian",
          "Mikolajczyk"
        ],
        [
          "Cristina",
          "Banks-Leite"
        ],
        [
          "Bj\u00f6rn W.",
          "Schuller"
        ]
      ],
      "title": "Multi-Attentive Detection of the Spider Monkey Whinny in the (Actual) Wild",
      "original": "1969",
      "page_count": 5,
      "order": 95,
      "p1": "471",
      "pn": "475",
      "abstract": [
        "We study deep bioacoustic event detection through multi-head attention\nbased pooling, exemplified by wildlife monitoring. In the multiple\ninstance learning framework, a core deep neural network learns a projection\nof the input acoustic signal into a sequence of embeddings, each representing\na segment of the input. Sequence pooling is then required to aggregate\nthe information present in the sequence such that we have a single\nclip-wise representation. We propose an improvement based on Squeeze-and-Excitation\nmechanisms upon a recently proposed audio tagging ResNet, and show\nthat it performs significantly better than the baseline, as well as\na collection of other recent audio models. We then further enhance\nour model, by performing an extensive comparative study of recent sequence\npooling mechanisms, and achieve our best result using multi-head self-attention\nfollowed by concatenation of the head-specific pooled embeddings &#8212;\nbetter than prediction pooling methods, as well as compared to other\nrecent sequence pooling tricks. We perform these experiments on a novel\ndataset of spider monkey whinny calls we introduce here, recorded in\na rainforest in the South-Pacific coast of Costa Rica, with a promising\noutlook pertaining to minimally invasive wildlife monitoring.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1969",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "egaslopez21_interspeech": {
      "authors": [
        [
          "Jos\u00e9 Vicente",
          "Egas-L\u00f3pez"
        ],
        [
          "Mercedes",
          "Vetr\u00e1b"
        ],
        [
          "L\u00e1szl\u00f3",
          "T\u00f3th"
        ],
        [
          "G\u00e1bor",
          "Gosztolya"
        ]
      ],
      "title": "Identifying Conflict Escalation and Primates by Using Ensemble X-Vectors and Fisher Vector Features",
      "original": "1173",
      "page_count": 5,
      "order": 96,
      "p1": "476",
      "pn": "480",
      "abstract": [
        "Computational paralinguistics is concerned with the automatic identification\nof non-verbal information in human speech. The Interspeech ComParE\nchallenge features new paralinguistic tasks each year; this time, among\nothers, a cross-corpus conflict escalation task and the identification\nof primates based solely on audio are the actual problems set. In our\nentry to ComParE 2021, we utilize x-vectors and Fisher vectors as features.\nTo improve the robustness of the predictions, we also experiment with\nbuilding an ensemble of classifiers from the x-vectors. Lastly, we\nexploit the fact that the Escalation Sub-Challenge is a conflict detection\ntask, and incorporate the SSPNet Conflict Corpus in our training workflow.\nUsing these approaches, at the time of writing, we had already surpassed\nthe official Challenge baselines on both tasks, which demonstrates\nthe efficiency of the employed techniques.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1173",
      "author_area_id": "3",
      "author_area_label": "Paralinguistics in Speech and Language: Human and Automatic Analysis and Processing:14 Special Sessions"
    },
    "verkholyak21_interspeech": {
      "authors": [
        [
          "Oxana",
          "Verkholyak"
        ],
        [
          "Denis",
          "Dresvyanskiy"
        ],
        [
          "Anastasia",
          "Dvoynikova"
        ],
        [
          "Denis",
          "Kotov"
        ],
        [
          "Elena",
          "Ryumina"
        ],
        [
          "Alena",
          "Velichko"
        ],
        [
          "Danila",
          "Mamontov"
        ],
        [
          "Wolfgang",
          "Minker"
        ],
        [
          "Alexey",
          "Karpov"
        ]
      ],
      "title": "Ensemble-Within-Ensemble Classification for Escalation Prediction from Speech",
      "original": "1821",
      "page_count": 5,
      "order": 97,
      "p1": "481",
      "pn": "485",
      "abstract": [
        "Conflict situations arise frequently in our daily life and often require\ntimely response to resolve the issues. In order to automatically classify\nconflict (also referred to as escalation) speech utterances we propose\nensemble learning as it improves prediction performance by combining\nseveral heterogeneous models that compensate for each other&#8217;s\nweaknesses. However, the effectiveness of the classification ensemble\ngreatly depends on its constituents and their fusion strategy. This\npaper provides experimental evidence for effectiveness of different\nprediction-level fusion strategies and demonstrates the performance\nof each proposed ensemble on the Escalation Sub-Challenge (ESS) in\nthe framework of the Computational Paralinguistics Challenge (ComParE-2021).\nThe ensembles comprise various machine learning approaches based on\nacoustic and linguistic characteristics of speech. The training strategy\nis specifically designed to increase the generalization performance\non the unseen data, while the diverse nature of ensemble candidates\nensures high prediction power and accurate classification.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1821",
      "author_area_id": "3",
      "author_area_label": "Paralinguistics in Speech and Language: Human and Automatic Analysis and Processing:14 Special Sessions"
    },
    "schiller21_interspeech": {
      "authors": [
        [
          "Dominik",
          "Schiller"
        ],
        [
          "Silvan",
          "Mertes"
        ],
        [
          "Pol van",
          "Rijn"
        ],
        [
          "Elisabeth",
          "Andr\u00e9"
        ]
      ],
      "title": "Analysis by Synthesis: Using an Expressive TTS Model as Feature Extractor for Paralinguistic Speech Classification",
      "original": "1587",
      "page_count": 5,
      "order": 98,
      "p1": "486",
      "pn": "490",
      "abstract": [
        "Modeling adequate features of speech prosody is one key factor to good\nperformance in affective speech classification. However, the distinction\nbetween the prosody that is induced by &#8216;how&#8217; something\nis said (i.e., affective prosody) and the prosody that is induced by\n&#8216;what&#8217; is being said (i.e., linguistic prosody) is neglected\nin state-of-the-art feature extraction systems. This results in high\nvariability of the calculated feature values for different sentences\nthat are spoken with the same affective intent, which might negatively\nimpact the performance of the classification. While this distinction\nbetween different prosody types is mostly neglected in affective speech\nrecognition, it is explicitly modeled in expressive speech synthesis\nto create controlled prosodic variation. In this work, we use the expressive\nText-To-Speech model Global Style Token Tacotron to extract features\nfor a speech analysis task. We show that the learned prosodic representations\noutperform state-of-the-art feature extraction systems in the exemplary\nuse case of Escalation Level Classification.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1587",
      "author_area_id": "3",
      "author_area_label": "Paralinguistics in Speech and Language: Human and Automatic Analysis and Processing:14 Special Sessions"
    },
    "christensen21_interspeech": {
      "authors": [
        [
          "Heidi",
          "Christensen"
        ]
      ],
      "title": "Towards Automatic Speech Recognition for People with Atypical Speech",
      "original": "abs2",
      "page_count": 0,
      "order": 99,
      "p1": "0",
      "pn": "",
      "abstract": [
        "In the last decade we have seen how speech technologies for typical\nspeech have matured and thus enabled the advancement of a multitude\nof services and technologies including voice-enabled conversational\ninterfaces, dictation and successfully underpinning the use of state-of-the-art\nNLP techniques. This ever more pervasive offering allows for an often\nfar more convenient and natural way of interacting with machines and\nsystems. However it also represents an ever-growing gap experienced\nby people with atypical (dysarthric) voices: people with even just\nmild-to-moderate speech disorders cannot achieve satisfactory performance\nwith current automatic speech recognition (ASR) systems and hence they\nare falling further and further behind in terms of their ability to\nuse modern devices and interfaces. This talk will present the major\nchallenges in porting mainstream ASR methodologies to work for atypical\nspeech, discuss recent advances and present thoughts on where the research\neffort should be focusing to have real impact in this community of\npotential users. Being able to speak a query or dictate an email offers\na lot of convenience to most of us but for this group of people can\nhave significant implications on ability to fully take part in society\nand life quality.\n"
      ]
    },
    "luu21_interspeech": {
      "authors": [
        [
          "Chau",
          "Luu"
        ],
        [
          "Peter",
          "Bell"
        ],
        [
          "Steve",
          "Renals"
        ]
      ],
      "title": "Leveraging Speaker Attribute Information Using Multi Task Learning for Speaker Verification and Diarization",
      "original": "0622",
      "page_count": 5,
      "order": 100,
      "p1": "491",
      "pn": "495",
      "abstract": [
        "Deep speaker embeddings have become the leading method for encoding\nspeaker identity in speaker recognition tasks. The embedding space\nshould ideally capture the variations between all possible speakers,\nencoding the multiple acoustic aspects that make up a speaker&#8217;s\nidentity, whilst being robust to non-speaker acoustic variation. Deep\nspeaker embeddings are normally trained discriminatively, predicting\nspeaker identity labels on the training data. We hypothesise that additionally\npredicting speaker-related auxiliary variables &#8212; such as age\nand nationality &#8212; may yield representations that are better able\nto generalise to unseen speakers. We propose a framework for making\nuse of auxiliary label information, even when it is only available\nfor speech corpora mismatched to the target application. On a test\nset of US Supreme Court recordings, we show that by leveraging two\nadditional forms of speaker attribute information derived respectively\nfrom the matched training data, and VoxCeleb corpus, we improve the\nperformance of our deep speaker embeddings for both verification and\ndiarization tasks, achieving a relative improvement of 26.2% in DER\nand 6.7% in EER compared to baselines using speaker labels only. This\nimprovement is obtained despite the auxiliary labels having been scraped\nfrom the web and being potentially noisy.\n"
      ],
      "doi": "10.21437/Interspeech.2021-622",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "rybicka21_interspeech": {
      "authors": [
        [
          "Magdalena",
          "Rybicka"
        ],
        [
          "Jes\u00fas",
          "Villalba"
        ],
        [
          "Piotr",
          "\u017belasko"
        ],
        [
          "Najim",
          "Dehak"
        ],
        [
          "Konrad",
          "Kowalczyk"
        ]
      ],
      "title": "Spine2Net: SpineNet with Res2Net and Time-Squeeze-and-Excitation Blocks for Speaker Recognition",
      "original": "1163",
      "page_count": 5,
      "order": 101,
      "p1": "496",
      "pn": "500",
      "abstract": [
        "Modeling speaker embeddings using deep neural networks is currently\nstate-of-the-art in speaker recognition. Recently, ResNet-based structures\nhave gained a broader interest, slowly becoming the baseline along\nwith the deep-rooted Time Delay Neural Network based models. However,\nthe scale-decreased design of the ResNet models may not preserve all\nof the speaker information. In this paper, we investigate the SpineNet\nstructure with scale-permuted design to tackle this problem, in which\nfeature size either increases or decreases depending on the processing\nstage in the network. Apart from the presented adjustments of the SpineNet\nmodel for the speaker recognition task, we also incorporate popular\nmodules dedicated to the residual-like structures, namely the Res2Net\nand Squeeze-and-Excitation blocks, and modify them to work effectively\nin the presented neural network architectures. The final proposed model,\ni.e., the SpineNet architecture with Res2Net and Time-Squeeze-and-Excitation\nblocks, achieves remarkable Equal Error Rates of 0.99 and 0.92 for\nthe Extended and Original trial lists of the well-known VoxCeleb1 dataset.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1163",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "stafylakis21_interspeech": {
      "authors": [
        [
          "Themos",
          "Stafylakis"
        ],
        [
          "Johan",
          "Rohdin"
        ],
        [
          "Luk\u00e1\u0161",
          "Burget"
        ]
      ],
      "title": "Speaker Embeddings by Modeling Channel-Wise Correlations",
      "original": "1442",
      "page_count": 5,
      "order": 102,
      "p1": "501",
      "pn": "505",
      "abstract": [
        "Speaker embeddings extracted with deep 2D convolutional neural networks\nare typically modeled as projections of first and second order statistics\nof channel-frequency pairs onto a linear layer, using either average\nor attentive pooling along the time axis. In this paper we examine\nan alternative pooling method, where pairwise correlations between\nchannels for given frequencies are used as statistics. The method is\ninspired by style-transfer methods in computer vision, where the style\nof an image, modeled by the matrix of channel-wise correlations, is\ntransferred to another image, in order to produce a new image having\nthe style of the first and the content of the second. By drawing analogies\nbetween image style and speaker characteristics, and between image\ncontent and phonetic sequence, we explore the use of such channel-wise\ncorrelations features to train a ResNet architecture in an end-to-end\nfashion. Our experiments on VoxCeleb demonstrate the effectiveness\nof the proposed pooling method in speaker recognition.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1442",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "he21_interspeech": {
      "authors": [
        [
          "Weipeng",
          "He"
        ],
        [
          "Petr",
          "Motlicek"
        ],
        [
          "Jean-Marc",
          "Odobez"
        ]
      ],
      "title": "Multi-Task Neural Network for Robust Multiple Speaker Embedding Extraction",
      "original": "1769",
      "page_count": 5,
      "order": 103,
      "p1": "506",
      "pn": "510",
      "abstract": [
        "This paper introduces a novel approach for extracting speaker embeddings\nfrom audio mixtures of multiple overlapping voices. This approach is\nbased on a multi-task neural network. The network first extracts a\nlatent feature for each direction. This feature is used for detecting\nsound sources as well as identifying speakers. In contrast to traditional\napproaches, the proposed method does not rely on explicit sound source\nseparation. The neural network model learns from data to extract the\nmost suitable features of the sounds at different directions. The experiments\nusing audio recordings of overlapping sound sources show that the proposed\napproach outperforms a beamforming-based traditional method.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1769",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "peng21_interspeech": {
      "authors": [
        [
          "Junyi",
          "Peng"
        ],
        [
          "Xiaoyang",
          "Qu"
        ],
        [
          "Jianzong",
          "Wang"
        ],
        [
          "Rongzhi",
          "Gu"
        ],
        [
          "Jing",
          "Xiao"
        ],
        [
          "Luk\u00e1\u0161",
          "Burget"
        ],
        [
          "Jan",
          "\u010cernock\u00fd"
        ]
      ],
      "title": "ICSpk: Interpretable Complex Speaker Embedding Extractor from Raw Waveform",
      "original": "2016",
      "page_count": 5,
      "order": 104,
      "p1": "511",
      "pn": "515",
      "abstract": [
        "Recently, extracting speaker embedding directly from raw waveform has\ndrawn increasing attention in the field of speaker verification. Parametric\nreal-valued filters in the first convolutional layer are learned to\ntransform the waveform into time-frequency representations. However,\nthese methods only focus on the magnitude spectrum and the poor interpretability\nof the learned filters limits the performance. In this paper, we propose\na complex speaker embedding extractor, named ICSpk, with higher interpretability\nand fewer parameters. Specifically, at first, to quantify the speaker-related\nfrequency response of waveform, we modify the original short-term Fourier\ntransform filters into a family of complex exponential filters, named\ninterpretable complex (IC) filters. Each IC filter is confined by a\ncomplex exponential filter parameterized by frequency. Then, a deep\ncomplex-valued speaker embedding extractor is designed to operate on\nthe complex-valued output of IC filters. The proposed ICSpk is evaluated\non VoxCeleb and CNCeleb databases. Experimental results demonstrate\nthe IC filters-based system exhibits a significant improvement over\nthe complex spectrogram based systems. Furthermore, the proposed ICSpk\noutperforms existing raw waveform based systems by a large margin.\n"
      ],
      "doi": "10.21437/Interspeech.2021-2016",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "xiao21_interspeech": {
      "authors": [
        [
          "Xiao",
          "Xiao"
        ],
        [
          "Nicolas",
          "Audibert"
        ],
        [
          "Gr\u00e9goire",
          "Locqueville"
        ],
        [
          "Christophe",
          "d'Alessandro"
        ],
        [
          "Barbara",
          "Kuhnert"
        ],
        [
          "Claire",
          "Pillot-Loiseau"
        ]
      ],
      "title": "Prosodic Disambiguation Using Chironomic Stylization of Intonation with Native and Non-Native Speakers",
      "original": "0182",
      "page_count": 5,
      "order": 105,
      "p1": "516",
      "pn": "520",
      "abstract": [
        "This paper introduces an interface that enables the real-time gestural\ncontrol of intonation in phrases produced by a vocal synthesizer. The\nmelody and timing of a target phrase can be modified by tracing melodic\ncontours on the touch-screen of a mobile tablet. Envisioning this interface\nas a means for non-native speakers to practice the intonation of a\nforeign language, we present a pilot study where native and non-native\nspeakers imitated the pronunciation of French phrases using their voice\nand the interface, with a visual guide and without. Comparison of resulting\nF0 curves against the reference contour and a preliminary perceptual\nassessment of synthesized utterances suggest that for both non-native\nand native speakers, imitation with the help of a visual guide is comparable\nin accuracy to vocal imitation, and that timing control was a source\nof difficulty.\n"
      ],
      "doi": "10.21437/Interspeech.2021-182"
    },
    "block21_interspeech": {
      "authors": [
        [
          "Aleese",
          "Block"
        ],
        [
          "Michelle",
          "Cohn"
        ],
        [
          "Georgia",
          "Zellou"
        ]
      ],
      "title": "Variation in Perceptual Sensitivity and Compensation for Coarticulation Across Adult and Child Naturally-Produced and TTS Voices",
      "original": "0228",
      "page_count": 5,
      "order": 106,
      "p1": "521",
      "pn": "525",
      "abstract": [
        "The current study explores whether perception of coarticulatory vowel\nnasalization differs by speaker age (adult vs. child) and type of voice\n(naturally produced vs. synthetic speech). Listeners completed a 4IAX\ndiscrimination task between pairs containing acoustically identical\n(both nasal or oral) vowels and acoustically distinct (one oral, one\nnasal) vowels. Vowels occurred in either the same consonant contexts\nor different contexts across pairs. Listeners completed the experiment\nwith either naturally produced speech or text-to-speech (TTS). For\nsame-context trials, listeners were better at discriminating between\noral and nasal vowels for child speech in the synthetic voices but\nadult speech in the natural voices. Meanwhile, in different-context\ntrials, listeners were less able to discriminate, indicating more perceptual\ncompensation for synthetic voices. There was no difference in different-context\ndiscrimination across talker ages, indicating that listeners did not\ncompensate differently if the speaker was a child or adult. Findings\nare relevant for models of compensation, computer personification theories,\nand speaker-indexical perception accounts.\n"
      ],
      "doi": "10.21437/Interspeech.2021-228",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production and Acquisition  by Human Listeners"
    },
    "monesi21_interspeech": {
      "authors": [
        [
          "Mohammad Jalilpour",
          "Monesi"
        ],
        [
          "Bernd",
          "Accou"
        ],
        [
          "Tom",
          "Francart"
        ],
        [
          "Hugo",
          "Van hamme"
        ]
      ],
      "title": "Extracting Different Levels of Speech Information from EEG Using an LSTM-Based Model",
      "original": "0336",
      "page_count": 5,
      "order": 107,
      "p1": "526",
      "pn": "530",
      "abstract": [
        "Decoding the speech signal that a person is listening to from the human\nbrain via electroencephalography (EEG) can help us understand how our\nauditory system works. Linear models have been used to reconstruct\nthe EEG from speech or vice versa. Recently, Artificial Neural Networks\n(ANNs) such as Convolutional Neural Network (CNN) and Long Short-Term\nMemory (LSTM) based architectures have outperformed linear models in\nmodeling the relation between EEG and speech. Before attempting to\nuse these models in real-world applications such as hearing tests or\n(second) language comprehension assessment we need to know what level\nof speech information is being utilized by these models. In this study,\nwe aim to analyze the performance of an LSTM-based model using different\nlevels of speech features. The task of the model is to determine which\nof two given speech segments is matched with the recorded EEG. We used\nlow- and high-level speech features including: envelope, mel spectrogram,\nvoice activity, phoneme identity, and word embedding. Our results suggest\nthat the model exploits information about silences, intensity, and\nbroad phonetic classes from the EEG. Furthermore, the mel spectrogram,\nwhich contains all this information, yields the highest accuracy (84%)\namong all the features.\n"
      ],
      "doi": "10.21437/Interspeech.2021-336",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production and Acquisition  by Human Listeners"
    },
    "bosch21_interspeech": {
      "authors": [
        [
          "Louis ten",
          "Bosch"
        ],
        [
          "Lou",
          "Boves"
        ]
      ],
      "title": "Word Competition: An Entropy-Based Approach in the DIANA Model of Human Word Comprehension",
      "original": "1394",
      "page_count": 5,
      "order": 108,
      "p1": "531",
      "pn": "535",
      "abstract": [
        "We discuss the role of entropy of the set of unfolding word candidates\nin the context of DIANA, a computational model of human auditory speech\ncomprehension. DIANA consists of three major interacting components:\nActivation, Decision and Execution. The Activation component computes\nactivations of word candidates that change over time as a function\nof the unfolding audio input. The resulting set of word candidate activations\ncan be associated with an entropy that is related to difficulty of\nthe decision when one of these candidates must be selected at time\nT. The paper presents the close relation between entropy measures and\nthe between-word competition during the unfolding of the auditory stimuli,\nand at the end of the stimuli if no decision could be made before stimulus\noffset. We present a way for computing the entropy that takes into\naccount linguistic-phonetic constraints that play a role in speech\ncomprehension and in lexical decision experiments. Using the BALDEY\ndata set and linear mixed effects regression models for RT, we show\nthat entropy measures explain differences between RTs of words with\ndifferent morphological structure.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1394",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production and Acquisition  by Human Listeners"
    },
    "bosch21b_interspeech": {
      "authors": [
        [
          "Louis ten",
          "Bosch"
        ],
        [
          "Lou",
          "Boves"
        ]
      ],
      "title": "Time-to-Event Models for Analyzing Reaction Time Sequences",
      "original": "1408",
      "page_count": 5,
      "order": 109,
      "p1": "536",
      "pn": "540",
      "abstract": [
        "We investigate reaction time (RT) sequences obtained from lexical decision\nexperiments by applying Time-to-Event modelling (Survival Analysis).\nThis is a branch of statistics for analyzing the expected duration\nuntil one or more events happen, associated with a set of potential\n&#8216;causes&#8217; (in our case the decision for a &#8216;word&#8217;\njudgment as a function of conventional predictors such as lexical frequency,\nstimulus duration, reduction, etc.). In this analysis, RTs are considered\na by-product of an (unobservable) cumulative incidence function that\nresults in a decision when it exceeds a certain threshold.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  We show that Survival\nAnalysis can be effectively used to narrow the gap between data-oriented\nmodels and process-oriented models for RT data from lexical decision\nexperiments. Results of this analysis technique are presented for two\ndifferent RT data sets. The analysis reveals time-varying patterns\nof predictors that reflect the differences in cognitive processes during\nthe presentation of auditory stimuli.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1408",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production and Acquisition  by Human Listeners"
    },
    "brand21_interspeech": {
      "authors": [
        [
          "Sophie",
          "Brand"
        ],
        [
          "Kimberley",
          "Mulder"
        ],
        [
          "Louis ten",
          "Bosch"
        ],
        [
          "Lou",
          "Boves"
        ]
      ],
      "title": "Models of Reaction Times in Auditory Lexical Decision: RTonset versus RToffset",
      "original": "1700",
      "page_count": 5,
      "order": 110,
      "p1": "541",
      "pn": "545",
      "abstract": [
        "We investigate how the role of predictors in models of reaction times\nin auditory lexical decision experiments depends on the operational\ndefinition of RT: whether the time is measured from stimulus onset\nor from stimulus offset. In a large body of literature, RTs are measured\nfrom the onset of the stimulus to the start of the response (often\na button press or an oral response). The rationale behind this choice\nis that information about the stimulus becomes available to the listener\nstarting at onset. Alternatively, the RT from offset is less dependent\non stimulus duration and is assumed to focus on those cognitive processes\nthat play a role late(r) in the word and after word offset, when all\ninformation is available.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  The paper presents\nRT-onset and RT-offset-based linear mixed effects models for three\ndifferent lexical decision-based data sets and explains the significant\ndifferences between these models, showing to what extent both definitions\nof reaction time reveal different roles for predictors and how early\nand later contributions to the overall RT can be differentiated.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1700",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production and Acquisition  by Human Listeners"
    },
    "kim21c_interspeech": {
      "authors": [
        [
          "Gwantae",
          "Kim"
        ],
        [
          "David K.",
          "Han"
        ],
        [
          "Hanseok",
          "Ko"
        ]
      ],
      "title": "SpecMix : A Mixed Sample Data Augmentation Method for Training with Time-Frequency Domain Features",
      "original": "0103",
      "page_count": 5,
      "order": 111,
      "p1": "546",
      "pn": "550",
      "abstract": [
        "A mixed sample data augmentation strategy is proposed to enhance the\nperformance of models on audio scene classification, sound event classification,\nand speech enhancement tasks. While there have been several augmentation\nmethods shown to be effective in improving image classification performance,\ntheir efficacy toward time-frequency domain features of audio is not\nassured. We propose a novel audio data augmentation approach named\n&#8220;Specmix&#8221; specifically designed for dealing with time-frequency\ndomain features. The augmentation method consists of mixing two different\ndata samples by applying time-frequency masks effective in preserving\nthe spectral correlation of each audio sample. Our experiments on acoustic\nscene classification, sound event classification, and speech enhancement\ntasks show that the proposed Specmix improves the performance of various\nneural network architectures by a maximum of 2.7%.\n"
      ],
      "doi": "10.21437/Interspeech.2021-103",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "wang21d_interspeech": {
      "authors": [
        [
          "Helin",
          "Wang"
        ],
        [
          "Yuexian",
          "Zou"
        ],
        [
          "Wenwu",
          "Wang"
        ]
      ],
      "title": "SpecAugment++: A Hidden Space Data Augmentation Method for Acoustic Scene Classification",
      "original": "0140",
      "page_count": 5,
      "order": 112,
      "p1": "551",
      "pn": "555",
      "abstract": [
        "In this paper, we present SpecAugment++, a novel data augmentation\nmethod for deep neural networks based acoustic scene classification\n(ASC). Different from other popular data augmentation methods such\nas SpecAugment and mixup that only work on the input space, SpecAugment++\nis applied to both the input space and the hidden space of the deep\nneural networks to enhance the input and the intermediate feature representations.\nFor an intermediate hidden state, the augmentation techniques consist\nof masking blocks of frequency channels and masking blocks of time\nframes, which improve generalization by enabling a model to attend\nnot only to the most discriminative parts of the feature, but also\nthe entire parts. Apart from using zeros for masking, we also examine\ntwo approaches for masking based on the use of other samples within\nthe mini-batch, which helps introduce noises to the networks to make\nthem more discriminative for classification. The experimental results\non the DCASE 2018 Task1 dataset and DCASE 2019 Task1 dataset show that\nour proposed method can obtain 3.6% and 4.7% accuracy gains over a\nstrong baseline without augmentation (i.e. CP-ResNet) respectively,\nand outperforms other previous data augmentation methods.\n"
      ],
      "doi": "10.21437/Interspeech.2021-140",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "zheng21_interspeech": {
      "authors": [
        [
          "Xu",
          "Zheng"
        ],
        [
          "Yan",
          "Song"
        ],
        [
          "Li-Rong",
          "Dai"
        ],
        [
          "Ian",
          "McLoughlin"
        ],
        [
          "Lin",
          "Liu"
        ]
      ],
      "title": "An Effective Mutual Mean Teaching Based Domain Adaptation Method for Sound Event Detection",
      "original": "0281",
      "page_count": 5,
      "order": 113,
      "p1": "556",
      "pn": "560",
      "abstract": [
        "In this paper, we present a novel mutual mean teaching based domain\nadaptation (MMT-DA) method for sound event detection (SED) task, which\ncan effectively exploit synthetic data to improve the SED performance.\nExisting methods simply treat the synthetic data as strongly-labeled\ndata in semi-supervised learning (SSL) framework. Benefiting from the\nstrong labels of synthetic data, superior SED performance can be achieved.\nHowever, a distribution mismatch between synthetic and real data raises\nan evident challenge for domain adaptation (DA). In MMT-DA, convolutional\nrecurrent neural networks (CRNN) learned from different datasets (i.e.\n<i>total data</i>:real+synthetic, and  <i>real data</i>) are exploited\nfor DA. Specifically, mean teacher method using CRNN is employed for\nutilizing the unlabeled real data. To compensate the domain diversity,\nan additional domain classifier with gradient reverse layer(GRL) is\nused for training a mean teacher for  <i>total data</i>. The student\nCRNNs are mutually taught using the soft predictions of unlabeled data\nobtained from different teachers. Furthermore, a strip pooling based\nattention module is exploited to model the inter-dependencies between\nchannels and time-frequency dimensions to exploit the structure information.\nExperimental results on Task4 of DCASE2020 demonstrate the ability\nof the proposed method, achieving 52.0% F1-score on the validation\ndataset, which outperforms the winning system&#8217;s 50.6%.\n"
      ],
      "doi": "10.21437/Interspeech.2021-281",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "nandi21_interspeech": {
      "authors": [
        [
          "Ritika",
          "Nandi"
        ],
        [
          "Shashank",
          "Shekhar"
        ],
        [
          "Manjunath",
          "Mulimani"
        ]
      ],
      "title": "Acoustic Scene Classification Using Kervolution-Based SubSpectralNet",
      "original": "0656",
      "page_count": 5,
      "order": 114,
      "p1": "561",
      "pn": "565",
      "abstract": [
        "In this paper, a Kervolution-based SubSpectralNet model is proposed\nfor Acoustic Scene Classification (ASC). SubSpectralNet is a competitive\nmodel which divides the mel spectrogram into horizontal slices termed\nas sub-spectrograms that are considered as input to the Convolutional\nNeural Network (CNN). In this work, the linear convolutional operation\nof SubSpectralNet is replaced with a non-linear operation using the\nkernel trick. This is also known as kervolution (kernel convolution)-based\nSubSpectralNet. The performance of the proposed methodology is evaluated\non the DCASE (Detection and Classification of Acoustic Scenes and Events)\n2018 development dataset. The proposed method achieves 73.52% and 75.76%\naccuracy with Polynomial and Gaussian Kernels respectively.\n"
      ],
      "doi": "10.21437/Interspeech.2021-656",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "sundar21_interspeech": {
      "authors": [
        [
          "Harshavardhan",
          "Sundar"
        ],
        [
          "Ming",
          "Sun"
        ],
        [
          "Chao",
          "Wang"
        ]
      ],
      "title": "Event Specific Attention for Polyphonic Sound Event Detection",
      "original": "0684",
      "page_count": 5,
      "order": 115,
      "p1": "566",
      "pn": "570",
      "abstract": [
        "The concept of multi-headed self attention (MHSA) introduced as a critical\nbuilding block of a Transformer Encoder/Decoder Module has made a significant\nimpact in the areas of natural language processing (NLP), automatic\nspeech recognition (ASR) and recently in the area of sound event detection\n(SED). The current state-of-the-art approaches to SED employ a shared\nattention mechanism achieved through a stack of MHSA blocks to detect\nmultiple sound events. Consequently, in a multi-label SED task, a common\nattention mechanism would be responsible for generating relevant feature\nrepresentations for each of the events to be detected. In this paper,\nwe show through empirical evaluation that having more MHSA blocks dedicated\nspecifically for individual events, rather than having a stack of shared\nMHSA blocks, improves the overall detection performance. Interestingly,\nthis improvement in performance comes about because the event-specific\nattention blocks help in resolving confusions in the case of co-occurring\nevents. The proposed &#8220;Event-specific Attention Network&#8221;\n(ESA-Net) can be trained in an end-to-end manner. On the DCASE 2020\nTask 4 data set, we show that with ESA-Net, the best single model achieves\nan event-based F1 score of 52.1% on the public validation data set\nimproving over the existing state of the art result.\n"
      ],
      "doi": "10.21437/Interspeech.2021-684",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "gong21b_interspeech": {
      "authors": [
        [
          "Yuan",
          "Gong"
        ],
        [
          "Yu-An",
          "Chung"
        ],
        [
          "James",
          "Glass"
        ]
      ],
      "title": "AST: Audio Spectrogram Transformer",
      "original": "0698",
      "page_count": 5,
      "order": 116,
      "p1": "571",
      "pn": "575",
      "abstract": [
        "In the past decade, convolutional neural networks (CNNs) have been\nwidely adopted as the main building block for end-to-end audio classification\nmodels, which aim to learn a direct mapping from audio spectrograms\nto corresponding labels. To better capture long-range global context,\na recent trend is to add a self-attention mechanism on top of the CNN,\nforming a CNN-attention hybrid model. However, it is unclear whether\nthe reliance on a CNN is necessary, and if neural networks purely based\non attention are sufficient to obtain good performance in audio classification.\nIn this paper, we answer the question by introducing the <i>Audio Spectrogram\nTransformer</i> (AST), the first convolution-free, purely attention-based\nmodel for audio classification. We evaluate AST on various audio classification\nbenchmarks, where it achieves new state-of-the-art results of 0.485\nmAP on AudioSet, 95.6% accuracy on ESC-50, and 98.1% accuracy on Speech\nCommands V2.\n"
      ],
      "doi": "10.21437/Interspeech.2021-698",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "seo21_interspeech": {
      "authors": [
        [
          "Soonshin",
          "Seo"
        ],
        [
          "Donghyun",
          "Lee"
        ],
        [
          "Ji-Hwan",
          "Kim"
        ]
      ],
      "title": "Shallow Convolution-Augmented Transformer with Differentiable Neural Computer for Low-Complexity Classification of Variable-Length Acoustic Scene",
      "original": "1308",
      "page_count": 5,
      "order": 117,
      "p1": "576",
      "pn": "580",
      "abstract": [
        "Convolutional neural networks (CNNs) exhibit good performance in low-complexity\nclassification with fixed-length acoustic scenes. However, previous\nstudies have not considered variable-length acoustic scenes in which\nperformance degradation is prevalent. In this regard, we investigate\ntwo novel architectures &#8212; convolution-augmented transformer (Conformer)\nand differentiable neural computer (DNC). Both the models show desirable\nperformance for variable-length data but require a large amount of\ndata. In other words, small amounts of data, such as those from acoustic\nscenes, lead to overfitting in these models. In this paper, we propose\na shallow convolution-augmented Transformer with a differentiable neural\ncomputer (shallow Conformer-DNC) for the low-complexity classification\nof variable-length acoustic scenes. The shallow Conformer-DNC is enabled\nto converge with small amounts of data. Short-term and long-term contexts\nof variable-length acoustic scenes are trained by using the shallow\nConformer and shallow DNC, respectively. The experiments were conducted\nfor variable-length conditions using the TAU Urban Acoustic Scenes\n2020 Mobile dataset. As a result, a peak accuracy of 61.25% was confirmed\nfor shallow Conformer-DNC with a model parameter of 34 K. It is comparable\nperformance to state-of-the-art CNNs.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1308",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "bear21_interspeech": {
      "authors": [
        [
          "Helen L.",
          "Bear"
        ],
        [
          "Veronica",
          "Morfi"
        ],
        [
          "Emmanouil",
          "Benetos"
        ]
      ],
      "title": "An Evaluation of Data Augmentation Methods for Sound Scene Geotagging",
      "original": "1837",
      "page_count": 5,
      "order": 118,
      "p1": "581",
      "pn": "585",
      "abstract": [
        "Sound scene geotagging is a new topic of research which has evolved\nfrom acoustic scene classification. It is motivated by the idea of\naudio surveillance. Not content with only describing a scene in a recording,\na machine which can locate where the recording was captured would be\nof use to many. In this paper we explore a series of common audio data\naugmentation methods to evaluate which best improves the accuracy of\naudio geotagging classifiers.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  Our work improves\non the state-of-the-art city geotagging method by 23% in terms of classification\naccuracy.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1837",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "hori21_interspeech": {
      "authors": [
        [
          "Chiori",
          "Hori"
        ],
        [
          "Takaaki",
          "Hori"
        ],
        [
          "Jonathan Le",
          "Roux"
        ]
      ],
      "title": "Optimizing Latency for Online Video Captioning Using Audio-Visual Transformers",
      "original": "1975",
      "page_count": 5,
      "order": 119,
      "p1": "586",
      "pn": "590",
      "abstract": [
        "Video captioning is an essential technology to understand scenes and\ndescribe events in natural language. To apply it to real-time monitoring,\na system needs not only to describe events accurately but also to produce\nthe captions as soon as possible. Low-latency captioning is needed\nto realize such functionality, but this research area for online video\ncaptioning has not been pursued yet. This paper proposes a novel approach\nto optimize each caption&#8217;s output timing based on a trade-off\nbetween latency and caption quality. An audio-visual Transformer is\ntrained to generate ground-truth captions using only a small portion\nof all video frames, and to mimic outputs of a pre-trained Transformer\nto which all the frames are given. A CNN-based timing detector is also\ntrained to detect a proper output timing, where the captions generated\nby the two Transformers become sufficiently close to each other. With\nthe jointly trained Transformer and timing detector, a caption can\nbe generated in the early stages of an event-triggered video clip,\nas soon as an event happens or when it can be forecasted. Experiments\nwith the ActivityNet Captions dataset show that our approach achieves\n94% of the caption quality of the upper bound given by the pre-trained\nTransformer using the entire video clips, using only 28% of frames\nfrom the beginning.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1975"
    },
    "si21_interspeech": {
      "authors": [
        [
          "Shijing",
          "Si"
        ],
        [
          "Jianzong",
          "Wang"
        ],
        [
          "Huiming",
          "Sun"
        ],
        [
          "Jianhan",
          "Wu"
        ],
        [
          "Chuanyao",
          "Zhang"
        ],
        [
          "Xiaoyang",
          "Qu"
        ],
        [
          "Ning",
          "Cheng"
        ],
        [
          "Lei",
          "Chen"
        ],
        [
          "Jing",
          "Xiao"
        ]
      ],
      "title": "Variational Information Bottleneck for Effective Low-Resource Audio Classification",
      "original": "2028",
      "page_count": 5,
      "order": 120,
      "p1": "591",
      "pn": "595",
      "abstract": [
        "Large-scale deep neural networks (DNNs) such as convolutional neural\nnetworks (CNNs) have achieved impressive performance in audio classification\nfor their powerful capacity and strong generalization ability. However,\nwhen training a DNN model on low-resource tasks, it is usually prone\nto overfitting the small data and learning too much redundant information.\nTo address this issue, we propose to use variational information bottleneck\n(VIB) to mitigate overfitting and suppress irrelevant information.\nIn this work, we conduct experiments on a 4-layer CNN. However, the\nVIB framework is ready-to-use and could be easily utilized with many\nother state-of-the-art network architectures. Evaluation on a few audio\ndatasets shows that our approach significantly outperforms baseline\nmethods, yielding &#8805; 5.0% improvement in terms of classification\naccuracy in some low-source settings.\n"
      ],
      "doi": "10.21437/Interspeech.2021-2028",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "deshmukh21_interspeech": {
      "authors": [
        [
          "Soham",
          "Deshmukh"
        ],
        [
          "Bhiksha",
          "Raj"
        ],
        [
          "Rita",
          "Singh"
        ]
      ],
      "title": "Improving Weakly Supervised Sound Event Detection with Self-Supervised Auxiliary Tasks",
      "original": "2079",
      "page_count": 5,
      "order": 121,
      "p1": "596",
      "pn": "600",
      "abstract": [
        "While multitask and transfer learning has shown to improve the performance\nof neural networks in limited data settings, they require pretraining\nof the model on large datasets beforehand. In this paper, we focus\non improving the performance of weakly supervised sound event detection\nin low data and noisy settings simultaneously without requiring any\npretraining task. To that extent, we propose a shared encoder architecture\nwith sound event detection as a primary task and an additional secondary\ndecoder for a self-supervised auxiliary task. We empirically evaluate\nthe proposed framework for weakly supervised sound event detection\non a remix dataset of the DCASE 2019 task 1 acoustic scene data with\nDCASE 2018 Task 2 sounds event data under 0, 10 and 20 dB SNR. To ensure\nwe retain the localisation information of multiple sound events, we\npropose a two-step attention pooling mechanism that provides a time-frequency\nlocalisation of multiple audio events in the clip. The proposed framework\nwith two-step attention outperforms existing benchmark models by 22.3%,\n12.8%, 5.9% on 0, 10 and 20 dB SNR respectively. We carry out an ablation\nstudy to determine the contribution of the auxiliary task and two-step\nattention pooling to the SED performance improvement.\n"
      ],
      "doi": "10.21437/Interspeech.2021-2079",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "komatsu21_interspeech": {
      "authors": [
        [
          "Tatsuya",
          "Komatsu"
        ],
        [
          "Shinji",
          "Watanabe"
        ],
        [
          "Koichi",
          "Miyazaki"
        ],
        [
          "Tomoki",
          "Hayashi"
        ]
      ],
      "title": "Acoustic Event Detection with Classifier Chains",
      "original": "2218",
      "page_count": 5,
      "order": 122,
      "p1": "601",
      "pn": "605",
      "abstract": [
        "This paper proposes acoustic event detection (AED) with classifier\nchains, a new classifier based on the probabilistic chain rule. The\nproposed AED with classifier chains consists of a gated recurrent unit\nand performs iterative binary detection of each event one by one. In\neach iteration, the event&#8217;s activity is estimated and used to\ncondition the next output based on the probabilistic chain rule to\nform classifier chains. Therefore, the proposed method can handle the\ninterdependence among events upon classification, while the conventional\nAED methods with multiple binary classifiers with a linear layer and\nsigmoid function have placed an assumption of conditional independence.\nIn the experiments with a real-recording dataset, the proposed method\ndemonstrates its superior AED performance to a relative 14.80% improvement\ncompared to a convolutional recurrent neural network baseline system\nwith the multiple binary classifiers.\n"
      ],
      "doi": "10.21437/Interspeech.2021-2218",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "tseng21_interspeech": {
      "authors": [
        [
          "Shu-Chuan",
          "Tseng"
        ],
        [
          "Yi-Fen",
          "Liu"
        ]
      ],
      "title": "Segment and Tone Production in Continuous Speech of Hearing and Hearing-Impaired Children",
      "original": "0757",
      "page_count": 5,
      "order": 123,
      "p1": "606",
      "pn": "610",
      "abstract": [
        "Verbal communication in daily use is conducted in the form of continuous\nspeech that theoretically is the ideal data format for assessing oral\nlanguage ability in educational and clinical domains. But as phonetic\nreduction and particularly lexical tones in Chinese are greatly affected\nby discourse context, it is a challenging task for automatic systems\nto evaluate continuous speech only by acoustic features. This study\nanalyzed repetitive and storytelling speech produced by selected Chinese-speaking\nhearing and hearing-impaired children with distinctively high and low\nspeech intelligibility levels. Word-based reduction types are derived\nby phonological properties that characterize contraction degrees of\nautomatically generated surface forms of disyllabic words. F0-based\ntonal contours are visualized using the centroid-nearest data points\nin the major clusters computed for tonal syllables. Our results show\nthat primary speech characteristics across different groups of children\ncan be differentiated by means of reduction type and tone production.\n"
      ],
      "doi": "10.21437/Interspeech.2021-757",
      "author_area_id": "13",
      "author_area_label": " Speech, Voice, and Hearing Disorders"
    },
    "wang21e_interspeech": {
      "authors": [
        [
          "Feng",
          "Wang"
        ],
        [
          "Jing",
          "Chen"
        ],
        [
          "Fei",
          "Chen"
        ]
      ],
      "title": "Effect of Carrier Bandwidth on Understanding Mandarin Sentences in Simulated Electric-Acoustic Hearing",
      "original": "0024",
      "page_count": 5,
      "order": 124,
      "p1": "611",
      "pn": "615",
      "abstract": [
        "For patients suffering with high-frequency hearing loss and preserving\nlow-frequency hearing, combined electric-acoustic stimulation (EAS)\nmay significantly improve their speech perception compared with cochlear\nimplants (CIs). In combined EAS, a hearing aid provides low-frequency\ninformation via acoustic (A) stimulation and a CI evokes high-frequency\nsound sensation via electrical (E) stimulation. The present work investigated\nthe EAS advantage when only a small number (i.e., 1 or 2) of channels\nwere provided for electrical stimulation in a CI, and the effect of\ncarrier bandwidth on understanding Mandarin sentences in a simulation\nof combined EAS experiment. The A-portion was extracted via low-pass\nfiltering processing and the E-portion was generated with a vocoder\nmodel preserving multi-channel temporal envelope waveforms, whereas\na noise-vocoder and a tone-vocoder were used to simulate the effect\nof carrier bandwidth. The synthesized stimuli were presented to normal-hearing\nlisteners to recognize. Experimental results showed that while low-pass\nfiltered Mandarin speech was not very intelligible, adding one or two\nE channels could significantly improve the intelligibility score to\nabove 86.0%. Under the condition with one E channel, using a large\ncarrier bandwidth in noise-vocoder processing provided a better intelligibility\nperformance than using a narrow carrier bandwidth in tone-vocoder processing.\n"
      ],
      "doi": "10.21437/Interspeech.2021-24",
      "author_area_id": "13",
      "author_area_label": " Speech, Voice, and Hearing Disorders"
    },
    "sharma21_interspeech": {
      "authors": [
        [
          "Manthan",
          "Sharma"
        ],
        [
          "Navaneetha",
          "Gaddam"
        ],
        [
          "Tejas",
          "Umesh"
        ],
        [
          "Aditya",
          "Murthy"
        ],
        [
          "Prasanta Kumar",
          "Ghosh"
        ]
      ],
      "title": "A Comparative Study of Different EMG Features for Acoustics-to-EMG Mapping",
      "original": "2240",
      "page_count": 5,
      "order": 125,
      "p1": "616",
      "pn": "620",
      "abstract": [
        "Electromyography (EMG) signals have been extensively used to capture\nfacial muscle movements while speaking since they are one of the most\nclosely related bio-signals generated during speech production. In\nthis work, we focus on speech acoustics to EMG prediction. We present\na comparative study of ten different EMG signal-based features including\nTime Domain (TD) features existing in the literature to examine their\neffectiveness in speech acoustics to EMG inverse (AEI) mapping. We\npropose a novel feature based on the Hilbert envelope of the filtered\nEMG signal. The raw EMG signal is reconstructed from these features\nas well. For the AEI mapping, we use a bi-directional long short-term\nmemory (BLSTM) network in a session-dependent manner. To estimate the\nraw EMG signal from the EMG features, we use a CNN-BLSTM model comprising\nof a convolution neural network (CNN) followed by BLSTM layers. AEI\nmapping performance using the BLSTM network reveals that the Hilbert\nenvelope based feature is predicted from speech with the highest accuracy,\namong all the features. Therefore, it could be the most representative\nfeature of the underlying muscle activation during speech production.\nThe proposed Hilbert envelope feature, when used together with the\nexisting TD features, improves the raw EMG signal reconstruction performance\ncompared to using the TD features alone.\n"
      ],
      "doi": "10.21437/Interspeech.2021-2240"
    },
    "abraham21_interspeech": {
      "authors": [
        [
          "Ajish K.",
          "Abraham"
        ],
        [
          "V.",
          "Sivaramakrishnan"
        ],
        [
          "N.",
          "Swapna"
        ],
        [
          "N.",
          "Manohar"
        ]
      ],
      "title": "Image-Based Assessment of Jaw Parameters and Jaw Kinematics for Articulatory Simulation: Preliminary Results",
      "original": "1155",
      "page_count": 5,
      "order": 126,
      "p1": "621",
      "pn": "625",
      "abstract": [
        "Correcting the deficits in jaw movements have often been ignored in\nassessment and treatment of speech disorders. A robotic simulation\nis being developed to facilitate Speech Language Pathologists to demonstrate\nthe movement of jaw, tongue and teeth during production of speech sounds,\nas a part of a larger study. Profiling of jaw movement is an important\naspect of articulatory simulation. The present study attempts to develop\na simple and efficient technique for deriving the jaw parameters and\nusing them to simulate jaw movements through inverse kinematics.<br\nstyle=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  Three Kannada speaking male participants in the age range of 26\nto 33 years were instructed to produce selected speech sounds. The\nimage of the final position of the jaw during production of each speech\nsound was recorded through CT scan and video camera. Angle of ramus\nand angle of body of mandible were simulated through inverse kinematics\nusing RoboAnalyzer software. The variables for inverse kinematics were\nderived through kinematic analysis. The Denavit-Hartenberg (D-H) parameters\nrequired for kinematic analysis were obtained from still image. Angles\nsimulated were compared with the angles obtained from CT scan images.\nNo significant difference was observed.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1155",
      "author_area_id": "13",
      "author_area_label": " Speech, Voice, and Hearing Disorders"
    },
    "wang21f_interspeech": {
      "authors": [
        [
          "Jianrong",
          "Wang"
        ],
        [
          "Nan",
          "Gu"
        ],
        [
          "Mei",
          "Yu"
        ],
        [
          "Xuewei",
          "Li"
        ],
        [
          "Qiang",
          "Fang"
        ],
        [
          "Li",
          "Liu"
        ]
      ],
      "title": "An Attention Self-Supervised Contrastive Learning Based Three-Stage Model for Hand Shape Feature Representation in Cued Speech",
      "original": "0440",
      "page_count": 5,
      "order": 127,
      "p1": "626",
      "pn": "630",
      "abstract": [
        "Cued Speech (CS) is a communication system for deaf people or hearing\nimpaired people, in which a speaker uses it to aid a lipreader in phonetic\nlevel by clarifying potentially ambiguous mouth movements with hand\nshape and positions. Feature extraction of multi-modal CS is a key\nstep in CS recognition. Recent supervised deep learning based methods\nsuffer from noisy CS data annotations especially for hand shape modality.\nIn this work, we first propose a self-supervised contrastive learning\nmethod to learn the feature representation of image without using labels.\nSecondly, a small amount of manually annotated CS data are used to\nfine-tune the first module. Thirdly, we present a module, which combines\nBi-LSTM and self-attention networks to further learn sequential features\nwith temporal and contextual information. Besides, to enlarge the volume\nand the diversity of the current limited CS datasets, we build a new\nBritish English dataset containing 5 native CS speakers. Evaluation\nresults on both French and British English datasets show that our model\nachieves over 90% accuracy in hand shape recognition. Significant improvements\nof 8.75% (for French) and 10.09% (for British English) are achieved\nin CS phoneme recognition correctness compared with the state-of-the-art.\n"
      ],
      "doi": "10.21437/Interspeech.2021-440",
      "author_area_id": "13",
      "author_area_label": " Speech, Voice, and Hearing Disorders"
    },
    "dineley21_interspeech": {
      "authors": [
        [
          "Judith",
          "Dineley"
        ],
        [
          "Grace",
          "Lavelle"
        ],
        [
          "Daniel",
          "Leightley"
        ],
        [
          "Faith",
          "Matcham"
        ],
        [
          "Sara",
          "Siddi"
        ],
        [
          "Maria Teresa",
          "Pe\u00f1arrubia-Mar\u00eda"
        ],
        [
          "Katie M.",
          "White"
        ],
        [
          "Alina",
          "Ivan"
        ],
        [
          "Carolin",
          "Oetzmann"
        ],
        [
          "Sara",
          "Simblett"
        ],
        [
          "Erin",
          "Dawe-Lane"
        ],
        [
          "Stuart",
          "Bruce"
        ],
        [
          "Daniel",
          "Stahl"
        ],
        [
          "Yatharth",
          "Ranjan"
        ],
        [
          "Zulqarnain",
          "Rashid"
        ],
        [
          "Pauline",
          "Conde"
        ],
        [
          "Amos A.",
          "Folarin"
        ],
        [
          "Josep Maria",
          "Haro"
        ],
        [
          "Til",
          "Wykes"
        ],
        [
          "Richard J.B.",
          "Dobson"
        ],
        [
          "Vaibhav A.",
          "Narayan"
        ],
        [
          "Matthew",
          "Hotopf"
        ],
        [
          "Bj\u00f6rn W.",
          "Schuller"
        ],
        [
          "Nicholas",
          "Cummins"
        ],
        [
          "",
          "The RADAR-CNS Consortium"
        ]
      ],
      "title": "Remote Smartphone-Based Speech Collection: Acceptance and Barriers in Individuals with Major Depressive Disorder",
      "original": "1240",
      "page_count": 5,
      "order": 128,
      "p1": "631",
      "pn": "635",
      "abstract": [
        "The ease of in-the-wild speech recording using smartphones has sparked\nconsiderable interest in the combined application of speech, remote\nmeasurement technology (RMT) and advanced analytics as a research and\nhealthcare tool. For this to be realised, the acceptability of remote\nspeech collection to the user must be established, in addition to feasibility\nfrom an analytical perspective. To understand the acceptance, facilitators,\nand barriers of smartphone-based speech recording, we invited 384 individuals\nwith major depressive disorder (MDD) from the Remote Assessment of\nDisease and Relapse &#8212; Central Nervous System (RADAR-CNS) research\nprogramme in Spain and the UK to complete a survey on their experiences\nrecording their speech. In this analysis, we demonstrate that study\nparticipants were more comfortable completing a scripted speech task\nthan a free speech task. For both speech tasks, we found depression\nseverity and country to be significant predictors of comfort. Not seeing\nsmartphone notifications of the scheduled speech tasks, low mood and\nforgetfulness were the most commonly reported obstacles to providing\nspeech recordings.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1240",
      "author_area_id": "13",
      "author_area_label": " Speech, Voice, and Hearing Disorders"
    },
    "li21_interspeech": {
      "authors": [
        [
          "Sarah R.",
          "Li"
        ],
        [
          "Colin T.",
          "Annand"
        ],
        [
          "Sarah",
          "Dugan"
        ],
        [
          "Sarah M.",
          "Schwab"
        ],
        [
          "Kathryn J.",
          "Eary"
        ],
        [
          "Michael",
          "Swearengen"
        ],
        [
          "Sarah",
          "Stack"
        ],
        [
          "Suzanne",
          "Boyce"
        ],
        [
          "Michael A.",
          "Riley"
        ],
        [
          "T. Douglas",
          "Mast"
        ]
      ],
      "title": "An Automatic, Simple Ultrasound Biofeedback Parameter for Distinguishing Accurate and Misarticulated Rhotic Syllables",
      "original": "1749",
      "page_count": 5,
      "order": 129,
      "p1": "636",
      "pn": "640",
      "abstract": [
        "Characterizing accurate vs. misarticulated patterns of tongue movement\nusing ultrasound can be challenging in real time because of the fast,\nindependent movement of tongue regions. The usefulness of ultrasound\nfor biofeedback speech therapy is limited because speakers must mentally\ntrack and compare differences between their tongue movement and available\nmodels. It is desirable to automate this interpretive task using a\nsingle parameter representing deviation from known accurate tongue\nmovements. In this study, displacements recorded automatically by ultrasound\nimage tracking were transformed into a single biofeedback parameter\n(time-dependent difference between blade and dorsum displacements).\nReceiver operating characteristic (ROC) curve analysis was used to\nevaluate this parameter as a predictor of production accuracy over\na range of different vowel contexts with initial and final /r/ in American\nEnglish. Areas under ROC curves were 0.8 or above, indicating that\nthis simple parameter may provide useful real-time biofeedback on /r/\naccuracy within a range of rhotic contexts.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1749",
      "author_area_id": "13",
      "author_area_label": " Speech, Voice, and Hearing Disorders"
    },
    "ribeiro21_interspeech": {
      "authors": [
        [
          "Manuel Sam",
          "Ribeiro"
        ],
        [
          "Aciel",
          "Eshky"
        ],
        [
          "Korin",
          "Richmond"
        ],
        [
          "Steve",
          "Renals"
        ]
      ],
      "title": "Silent versus Modal Multi-Speaker Speech Recognition from Ultrasound and Video",
      "original": "0023",
      "page_count": 5,
      "order": 130,
      "p1": "641",
      "pn": "645",
      "abstract": [
        "We investigate multi-speaker speech recognition from ultrasound images\nof the tongue and video images of the lips. We train our systems on\nimaging data from modal speech, and evaluate on matched test sets of\ntwo speaking modes: silent and modal speech. We observe that silent\nspeech recognition from imaging data underperforms compared to modal\nspeech recognition, likely due to a speaking-mode mismatch between\ntraining and testing. We improve silent speech recognition performance\nusing techniques that address the domain mismatch, such as fMLLR and\nunsupervised model adaptation. We also analyse the properties of silent\nand modal speech in terms of utterance duration and the size of the\narticulatory space. To estimate the articulatory space, we compute\nthe convex hull of tongue splines, extracted from ultrasound tongue\nimages. Overall, we observe that the duration of silent speech is longer\nthan that of modal speech, and that silent speech covers a smaller\narticulatory space than modal speech. Although these two properties\nare statistically significant across speaking modes, they do not directly\ncorrelate with word error rates from speech recognition.\n"
      ],
      "doi": "10.21437/Interspeech.2021-23",
      "author_area_id": "13",
      "author_area_label": " Speech, Voice, and Hearing Disorders"
    },
    "ferreira21_interspeech": {
      "authors": [
        [
          "David",
          "Ferreira"
        ],
        [
          "Samuel",
          "Silva"
        ],
        [
          "Francisco",
          "Curado"
        ],
        [
          "Ant\u00f3nio",
          "Teixeira"
        ]
      ],
      "title": "RaSSpeR: Radar-Based Silent Speech Recognition",
      "original": "1413",
      "page_count": 5,
      "order": 131,
      "p1": "646",
      "pn": "650",
      "abstract": [
        "Speech is our most natural and efficient way of communication and offers\na strong potential to improve how we interact with machines. However,\nspeech communication can sometimes be limited by environmental (e.g.,\nambient noise), contextual (e.g., need for privacy in a public place),\nor health conditions (e.g., laryngectomy), hindering the consideration\nof audible speech. In this regard, silent speech interfaces (SSI) have\nbeen proposed (e.g., considering video, electromyography), however,\nmany technologies still face limitations regarding their everyday use,\ne.g., the need to place equipment in contact with the speaker (e.g.,\nelectrodes/ultrasound probe), and raise technical (e.g., lighting conditions\nfor video) or privacy concerns. In this context, the consideration\nof technologies that can help tackle these issues, e.g, by being contactless\nand/or placed in the environment, can foster the widespread use of\nSSI. In this article, continuous-wave radar is explored to assess its\npotential for SSI. To this end, a corpus of 13 words was acquired,\nfor 3 speakers, and different classifiers were tested on the resulting\ndata. The best results, obtained using Bagging classifier, trained\nfor each speaker, with 5-fold cross-validation, yielded an average\naccuracy of 0.826, an encouraging result that establishes promising\ngrounds for further exploration of this technology for silent speech\nrecognition.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1413",
      "author_area_id": "13",
      "author_area_label": " Speech, Voice, and Hearing Disorders"
    },
    "cao21_interspeech": {
      "authors": [
        [
          "Beiming",
          "Cao"
        ],
        [
          "Nordine",
          "Sebkhi"
        ],
        [
          "Arpan",
          "Bhavsar"
        ],
        [
          "Omer T.",
          "Inan"
        ],
        [
          "Robin",
          "Samlan"
        ],
        [
          "Ted",
          "Mau"
        ],
        [
          "Jun",
          "Wang"
        ]
      ],
      "title": "Investigating Speech Reconstruction for Laryngectomees for Silent Speech Interfaces",
      "original": "1842",
      "page_count": 5,
      "order": 132,
      "p1": "651",
      "pn": "655",
      "abstract": [
        "Silent speech interfaces (SSIs) are devices that convert non-audio\nbio-signals to speech, which hold the potential of recovering quality\nspeech for laryngectomees (people who have undergone laryngectomy).\nAlthough significant progress has been made, most of the recent SSI\nworks focused on data collected from healthy speakers. SSIs for laryngectomees\nhave rarely been investigated. In this study, we investigated the reconstruction\nof speech for two laryngectomees who either use tracheoesophageal puncture\n(TEP) or electro-larynx (EL) speech as their post-surgery communication\nmode. We reconstructed their speech using two SSI designs (1) real-time\nrecognition-and-synthesis and (2) directly articulation-to-speech synthesis\n(ATS). The reconstructed speech samples were measured in subjective\nevaluation by 20 listeners in terms of naturalness and intelligibility.\nThe results indicated that both designs increased the naturalness of\nalaryngeal speech. The real-time recognition-and-synthesis design obtained\nhigher intelligibility in electrolarynx speech as well, while the ATS\ndid not. These preliminary results suggest the real-time recognition-and-synthesis\ndesign may have a better potential for clinical applications (for laryngectomees)\nthan ATS.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1842",
      "author_area_id": "13",
      "author_area_label": " Speech, Voice, and Hearing Disorders"
    },
    "schroter21_interspeech": {
      "authors": [
        [
          "Hendrik",
          "Schr\u00f6ter"
        ],
        [
          "Tobias",
          "Rosenkranz"
        ],
        [
          "Alberto N.",
          "Escalante-B"
        ],
        [
          "Andreas",
          "Maier"
        ]
      ],
      "title": "LACOPE: Latency-Constrained Pitch Estimation for Speech Enhancement",
      "original": "0633",
      "page_count": 5,
      "order": 133,
      "p1": "656",
      "pn": "660",
      "abstract": [
        "Fundamental frequency (f<SUB>0</SUB>) estimation, also known as pitch\ntracking, has been a long-standing research topic in the speech and\nsignal processing community. Many pitch estimation algorithms, however,\nfail in noisy conditions or introduce large delays due to their frame\nsize or Viterbi decoding.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  In this study, we\npropose a deep learning-based pitch estimation algorithm, LACOPE, which\nwas trained in a joint pitch estimation and speech enhancement framework.\nIn contrast to previous work, this algorithm allows for a configurable\nlatency down to an algorithmic delay of 0. This is achieved by exploiting\nthe smoothness properties of the pitch trajectory. That is, a recurrent\nneural network compensates delay introduced by the feature computation\nby predicting the pitch for a desired point, allowing a trade-off between\npitch accuracy and latency.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  We integrate the pitch\nestimation in a speech enhancement framework for hearing aids. For\nthis application, we allow a delay on the analysis side of approx.\n5ms. The pitch estimate is then used for constructing a comb filter\nin frequency domain as post-processing step to remove intra-harmonic\nnoise.<br style=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  Our pitch estimation performance is on par with SOTA algorithms\nlike PYIN or CREPE for spoken speech in all noise conditions while\nintroducing minimal latency.\n"
      ],
      "doi": "10.21437/Interspeech.2021-633",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "fontaine21_interspeech": {
      "authors": [
        [
          "Mathieu",
          "Fontaine"
        ],
        [
          "Kouhei",
          "Sekiguchi"
        ],
        [
          "Aditya Arie",
          "Nugraha"
        ],
        [
          "Yoshiaki",
          "Bando"
        ],
        [
          "Kazuyoshi",
          "Yoshii"
        ]
      ],
      "title": "Alpha-Stable Autoregressive Fast Multichannel Nonnegative Matrix Factorization for Joint Speech Enhancement and Dereverberation",
      "original": "0742",
      "page_count": 5,
      "order": 134,
      "p1": "661",
      "pn": "665",
      "abstract": [
        "This paper proposes &#945;-stable autoregressive fast multichannel\nnonnegative matrix factorization (&#945;-AR-FastMNMF), a robust joint\nblind speech enhancement and dereverberation method for improved automatic\nspeech recognition in a realistic adverse environment. The state-of-the-art\nversatile blind source separation method called FastMNMF that assumes\nthe short-time Fourier transform (STFT) coefficients of a direct sound\nto follow a circular complex Gaussian distribution with jointly-diagonalizable\nfull-rank spatial covariance matrices was extended to AR-FastMNMF with\nan autoregressive reverberation model. Instead of the light-tailed\nGaussian distribution, we use the heavy-tailed &#945;-stable distribution,\nwhich also has the reproductive property useful for the additive source\nmodeling, to better deal with the large dynamic range of the direct\nsound. The experimental results demonstrate that the proposed &#945;-AR-FastMNMF\nworks well as a front-end of an automatic speech recognition system.\nIt outperforms &#945;-AR-ILRMA, which is a special case of &#945;-AR-FastMNMF,\nand their Gaussian counterparts, i.e., AR-FastMNMF and AR-ILRMA, in\nterms of the speech signal quality metrics and word error rate.\n"
      ],
      "doi": "10.21437/Interspeech.2021-742",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "zhang21e_interspeech": {
      "authors": [
        [
          "Siyuan",
          "Zhang"
        ],
        [
          "Xiaofei",
          "Li"
        ]
      ],
      "title": "Microphone Array Generalization for Multichannel Narrowband Deep Speech Enhancement",
      "original": "0944",
      "page_count": 5,
      "order": 135,
      "p1": "666",
      "pn": "670",
      "abstract": [
        "This paper addresses the problem of microphone array generalization\nfor deep-learning-based end-to-end multichannel speech enhancement.\nWe aim to train a unique deep neural network (DNN) potentially performing\nwell on unseen microphone arrays. The microphone array geometry shapes\nthe network&#8217;s parameters when training on a fixed microphone\narray, and thus restricts the generalization of the trained network\nto another microphone array. To resolve this problem, a single network\nis trained using data recorded by various microphone arrays of different\ngeometries. We design three variants of our recently proposed narrowband\nnetwork to cope with the agnostic number of microphones. Overall, the\ngoal is to make the network learn the universal information for speech\nenhancement that is available for any array geometry, rather than learn\nthe one-array-dedicated characteristics. The experiments on both simulated\nand real room impulse responses (RIR) demonstrate the excellent across-array\ngeneralization capability of the proposed networks, in the sense that\ntheir performance measures are very close to, or even exceed the network\ntrained with test arrays. Moreover, they notably outperform various\nbeamforming methods and other advanced deep-learning-based methods.\n"
      ],
      "doi": "10.21437/Interspeech.2021-944",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "song21_interspeech": {
      "authors": [
        [
          "Hyungchan",
          "Song"
        ],
        [
          "Jong Won",
          "Shin"
        ]
      ],
      "title": "Multiple Sound Source Localization Based on Interchannel Phase Differences in All Frequencies with Spectral Masks",
      "original": "1178",
      "page_count": 5,
      "order": 136,
      "p1": "671",
      "pn": "675",
      "abstract": [
        "One of the most widely used cues for sound source localization is the\ninterchannel phase differences (IPDs) in the frequency domain. However,\nthe spatial aliasing makes the utilization of the IPDs in the high\nfrequencies difficult, especially when the distance between the microphones\nis high. Recently, the phase replication method which considers the\ndirection-of-arrival (DoA) candidates corresponding to all the possible\nunwrapped phase differences in all frequency bins was proposed. However,\nhigh frequency bins with possible spatial aliasing contribute more\nwhen constructing initial DoA histograms compared with low frequency\nbins, which may not be desirable for source localization. In this paper,\nwe propose to utilize the IPDs in all the frequency bins with equal\nweights regardless of maximum number of phase wrapping in that frequency\nfor dual microphone sound source localization. We applied spectral\nmasks based on local signal-to-noise ratios and coherences between\nmicrophone signals to exclude time-frequency bins without directional\naudio signal from the DoA histogram construction. Experimental results\nshow that the proposed method results in more distinct peaks in the\nDoA histogram and outperforms the conventional method in various noisy\nand reverberant environments.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1178",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "zarazaga21_interspeech": {
      "authors": [
        [
          "Pablo P\u00e9rez",
          "Zarazaga"
        ],
        [
          "Mariem Bouafif",
          "Mansali"
        ],
        [
          "Tom",
          "B\u00e4ckstr\u00f6m"
        ],
        [
          "Zied",
          "Lachiri"
        ]
      ],
      "title": "Cancellation of Local Competing Speaker with Near-Field Localization for Distributed ad-hoc Sensor Network",
      "original": "1329",
      "page_count": 5,
      "order": 137,
      "p1": "676",
      "pn": "680",
      "abstract": [
        "In scenarios such as remote work, open offices and call centers, multiple\npeople may simultaneously have independent spoken interactions with\ntheir devices in the same room. The speech of competing speakers will\nhowever be picked up by all microphones, both reducing the quality\nof audio and exposing speakers to breaches in privacy. We propose a\ncooperative cross-talk cancellation solution breaking the single active\nspeaker assumption employed by most telecommunication systems. The\nproposed method applies source separation on the microphone signals\nof independent devices, to extract the dominant speaker in each device.\nIt is realized using a localization estimator based on a deep neural\nnetwork, followed by a time-frequency mask to separate the target speech\nfrom the interfering one at each time-frequency unit referring to its\norientation. By experimental evaluation, we confirm that the proposed\nmethod effectively reduces crosstalk and exceeds the baseline expectation\nmaximization method by 10 dB in terms of interference rejection. This\nperformance makes the proposed method a viable solution for cross-talk\ncancellation in near-field conditions, thus protecting the privacy\nof external speakers in the same acoustic space.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1329",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "zhang21f_interspeech": {
      "authors": [
        [
          "Hao",
          "Zhang"
        ],
        [
          "DeLiang",
          "Wang"
        ]
      ],
      "title": "A Deep Learning Method to Multi-Channel Active Noise Control",
      "original": "1512",
      "page_count": 5,
      "order": 138,
      "p1": "681",
      "pn": "685",
      "abstract": [
        "This paper addresses multi-channel active noise control (MCANC) on\nthe basis of deep ANC, which performs active noise control by employing\ndeep learning to encode the optimal control parameters corresponding\nto different noises and environments. The proposed method trains a\nconvolutional recurrent network (CRN) to estimate the real and imaginary\nspectrograms of all the canceling signals simultaneously from the reference\nsignals so that the corresponding anti-noises cancel or attenuate the\nprimary noises in an MCANC system. We evaluate the proposed method\nunder multiple MCANC setups and investigate the impact of the number\nof canceling loudspeakers and error microphones on the overall performance.\nExperimental results show that deep ANC is effective for MCANC in various\nscenarios. Moreover, the proposed method is robust against untrained\nnoises and works well in the presence of loudspeaker nonlinearity.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1512",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "graetzer21_interspeech": {
      "authors": [
        [
          "Simone",
          "Graetzer"
        ],
        [
          "Jon",
          "Barker"
        ],
        [
          "Trevor J.",
          "Cox"
        ],
        [
          "Michael",
          "Akeroyd"
        ],
        [
          "John F.",
          "Culling"
        ],
        [
          "Graham",
          "Naylor"
        ],
        [
          "Eszter",
          "Porter"
        ],
        [
          "Rhoddy Viveros",
          "Mu\u00f1oz"
        ]
      ],
      "title": "Clarity-2021 Challenges: Machine Learning Challenges for Advancing Hearing Aid Processing",
      "original": "1574",
      "page_count": 5,
      "order": 139,
      "p1": "686",
      "pn": "690",
      "abstract": [
        "In recent years, rapid advances in speech technology have been made\npossible by machine learning challenges such as CHiME, REVERB, Blizzard,\nand Hurricane. In the Clarity project, the machine learning approach\nis applied to the problem of hearing aid processing of speech-in-noise,\nwhere current technology in enhancing the speech signal for the hearing\naid wearer is often ineffective. The scenario is a (simulated) cuboid-shaped\nliving room in which there is a single listener, a single target speaker\nand a single interferer, which is either a competing talker or domestic\nnoise. All sources are static, the target is always within &#177;30&#176;\nazimuth of the listener and at the same elevation, and the interferer\nis an omnidirectional point source at the same elevation. The target\nspeech comes from an open source 40-speaker British English speech\ndatabase collected for this purpose. This paper provides a baseline\ndescription of the round one Clarity challenges for both enhancement\n(CEC1) and prediction (CPC1). To the authors&#8217; knowledge, these\nare the first machine learning challenges to consider the problem of\nhearing aid speech signal processing.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1574",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "tu21b_interspeech": {
      "authors": [
        [
          "Zehai",
          "Tu"
        ],
        [
          "Ning",
          "Ma"
        ],
        [
          "Jon",
          "Barker"
        ]
      ],
      "title": "Optimising Hearing Aid Fittings for Speech in Noise with a Differentiable Hearing Loss Model",
      "original": "1613",
      "page_count": 5,
      "order": 140,
      "p1": "691",
      "pn": "695",
      "abstract": [
        "Current hearing aids normally provide amplification based on a general\nprescriptive fitting, and the benefits provided by the hearing aids\nvary among different listening environments despite the inclusion of\nnoise suppression feature. Motivated by this fact, this paper proposes\na data-driven machine learning technique to develop hearing aid fittings\nthat are customised to speech in different noisy environments. A differentiable\nhearing loss model is proposed and used to optimise fittings with back-propagation.\nThe customisation is reflected on the data of speech in different noise\nwith also the consideration of noise suppression. The objective evaluation\nshows the advantages of optimised custom fittings over general prescriptive\nfittings.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1613",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "sivasankaran21_interspeech": {
      "authors": [
        [
          "Sunit",
          "Sivasankaran"
        ],
        [
          "Emmanuel",
          "Vincent"
        ],
        [
          "Dominique",
          "Fohr"
        ]
      ],
      "title": "Explaining Deep Learning Models for Speech Enhancement",
      "original": "1764",
      "page_count": 5,
      "order": 141,
      "p1": "696",
      "pn": "700",
      "abstract": [
        "We consider the problem of explaining the robustness of neural networks\nused to compute time-frequency masks for speech enhancement to mismatched\nnoise conditions. We employ the Deep SHapley Additive exPlanations\n(DeepSHAP) feature attribution method to quantify the contribution\nof every time-frequency bin in the input noisy speech signal to every\ntime-frequency bin in the output time-frequency mask. We define an\nobjective metric &#8212; referred to as the speech relevance score\n&#8212; that summarizes the obtained SHAP values and show that it correlates\nwith the enhancement performance, as measured by the word error rate\non the CHiME-4 real evaluation dataset. We use the speech relevance\nscore to explain the generalization ability of three speech enhancement\nmodels trained using synthetically generated speech-shaped noise, noise\nfrom a professional sound effects library, or real CHiME-4 noise. To\nthe best of our knowledge, this is the first study on neural network\nexplainability in the context of speech enhancement.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1764",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "huang21_interspeech": {
      "authors": [
        [
          "Weilong",
          "Huang"
        ],
        [
          "Jinwei",
          "Feng"
        ]
      ],
      "title": "Minimum-Norm Differential Beamforming for Linear Array with Directional Microphones",
      "original": "1989",
      "page_count": 5,
      "order": 142,
      "p1": "701",
      "pn": "705",
      "abstract": [
        "Among different differential beamforming approaches, the minimum-norm\none has received much attention as it maximizes the white noise gain(WNG).\nWNG measures the robustness of beamformer. But in practice, the conventional\nminimum-norm differential beamforming with omnidirectional elements\nstill suffers in low white-noise-gain at the low frequencies. The major\ncontributions of this paper are as follows: First, we extend the existing\nwork by presenting a new solution with the use of the directional microphone\nelements, and show clearly the connection between the conventional\nbeamforming and the proposed beamforming. Second, through the derivation\nas well as simulations, we show the proposed solution brings noticeable\nimprovement in WNG at the low frequencies when the null positions of\nthe directional elements coincide with the null-constraints of minimum\nnorm solution.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1989",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "cao21b_interspeech": {
      "authors": [
        [
          "Songjun",
          "Cao"
        ],
        [
          "Yueteng",
          "Kang"
        ],
        [
          "Yanzhe",
          "Fu"
        ],
        [
          "Xiaoshuo",
          "Xu"
        ],
        [
          "Sining",
          "Sun"
        ],
        [
          "Yike",
          "Zhang"
        ],
        [
          "Long",
          "Ma"
        ]
      ],
      "title": "Improving Streaming Transformer Based ASR Under a Framework of Self-Supervised Learning",
      "original": "1454",
      "page_count": 5,
      "order": 143,
      "p1": "706",
      "pn": "710",
      "abstract": [
        "Recently self-supervised learning has emerged as an effective approach\nto improve the performance of automatic speech recognition (ASR). Under\nsuch a framework, the neural network is usually pre-trained with massive\nunlabeled data and then fine-tuned with limited labeled data. However,\nthe non-streaming architecture like bidirectional transformer is usually\nadopted by the neural network to achieve competitive results, which\ncannot be used in streaming scenarios. In this paper, we mainly focus\non improving the performance of streaming transformer under the self-supervised\nlearning framework. Specifically, we propose a novel two-stage training\nmethod during fine-tuning, which combines knowledge distilling and\nself-training. The proposed training method achieves 16.3% relative\nword error rate (WER) reduction on Librispeech noisy test set. Finally,\nby only using the 100h clean subset of Librispeech as the labeled data\nand the rest (860h) as the unlabeled data, our streaming transformer\nbased model obtains competitive WERs 3.5/8.7 on Librispeech clean/noisy\ntest sets.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1454",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "sadhu21_interspeech": {
      "authors": [
        [
          "Samik",
          "Sadhu"
        ],
        [
          "Di",
          "He"
        ],
        [
          "Che-Wei",
          "Huang"
        ],
        [
          "Sri Harish",
          "Mallidi"
        ],
        [
          "Minhua",
          "Wu"
        ],
        [
          "Ariya",
          "Rastrow"
        ],
        [
          "Andreas",
          "Stolcke"
        ],
        [
          "Jasha",
          "Droppo"
        ],
        [
          "Roland",
          "Maas"
        ]
      ],
      "title": "wav2vec-C: A Self-Supervised Model for Speech Representation Learning",
      "original": "0717",
      "page_count": 5,
      "order": 144,
      "p1": "711",
      "pn": "715",
      "abstract": [
        "wav2vec-C introduces a novel representation learning technique combining\nelements from wav2vec 2.0 and VQ-VAE. Our model learns to reproduce\nquantized representations from partially masked speech encoding using\na contrastive loss in a way similar to wav2vec 2.0. However, the quantization\nprocess is regularized by an additional consistency network that learns\nto reconstruct the input features to the wav2vec 2.0 network from the\nquantized representations in a way similar to a VQ-VAE model. The proposed\nself-supervised model is trained on 10k hours of unlabeled data and\nsubsequently used as the speech encoder in a RNN-T ASR model and fine-tuned\nwith 1k hours of labeled data. This work is one of the very few studies\nof self-supervised learning on speech tasks with a large volume of\nreal far-field labeled data. The wav2vec-C encoded representations\nachieve, on average, twice the error reduction over baseline and a\nhigher codebook utilization in comparison to wav2vec 2.0.\n"
      ],
      "doi": "10.21437/Interspeech.2021-717",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "wallington21_interspeech": {
      "authors": [
        [
          "Electra",
          "Wallington"
        ],
        [
          "Benji",
          "Kershenbaum"
        ],
        [
          "Ond\u0159ej",
          "Klejch"
        ],
        [
          "Peter",
          "Bell"
        ]
      ],
      "title": "On the Learning Dynamics of Semi-Supervised Training for ASR",
      "original": "1777",
      "page_count": 5,
      "order": 145,
      "p1": "716",
      "pn": "720",
      "abstract": [
        "The use of semi-supervised training (SST) has become an increasingly\npopular way of increasing the performance of ASR acoustic models without\nthe need for further transcribed speech data. However, the performance\nof the technique can be very sensitive to the quality of the initial\nASR system. This paper undertakes a comprehensive study of the improvements\ngained with respect to variation in the initial systems, the quantity\nof untranscribed data used, and the learning schedules. We postulate\nthat the reason SST can be effective even when the initial model is\npoor is because it enables utterance-level information to be propagated\nto the frame level, and hence hypothesise that the quality of the language\nmodel plays a much larger role than the quality of the acoustic model.\nIn experiments on Tagalog data from the IARPA MATERIAL programme, we\nfind that indeed this is the case, and show that with an appropriately\nchosen recipe it is possible to achieve over 50% relative WER reductions\nfrom SST, even when the WER of the initial system is more than 80%.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1777",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "hsu21_interspeech": {
      "authors": [
        [
          "Wei-Ning",
          "Hsu"
        ],
        [
          "Anuroop",
          "Sriram"
        ],
        [
          "Alexei",
          "Baevski"
        ],
        [
          "Tatiana",
          "Likhomanenko"
        ],
        [
          "Qiantong",
          "Xu"
        ],
        [
          "Vineel",
          "Pratap"
        ],
        [
          "Jacob",
          "Kahn"
        ],
        [
          "Ann",
          "Lee"
        ],
        [
          "Ronan",
          "Collobert"
        ],
        [
          "Gabriel",
          "Synnaeve"
        ],
        [
          "Michael",
          "Auli"
        ]
      ],
      "title": "Robust wav2vec 2.0: Analyzing Domain Shift in Self-Supervised Pre-Training",
      "original": "0236",
      "page_count": 5,
      "order": 146,
      "p1": "721",
      "pn": "725",
      "abstract": [
        "Self-supervised learning of speech representations has been a very\nactive research area but most work is focused on a single domain such\nas read audio books for which there exist large quantities of labeled\nand unlabeled data. In this paper, we explore more general setups where\nthe domain of the unlabeled data for pre-training data differs from\nthe domain of the labeled data for fine-tuning, which in turn may differ\nfrom the test data domain. Our experiments show that using target domain\ndata during pre-training leads to large performance improvements across\na variety of setups. With no access to in-domain labeled data, pre-training\non unlabeled in-domain data closes 66&#8211;73% of the performance\ngap between the ideal setting of in-domain labeled data and a competitive\nsupervised out-of-domain model. This has obvious practical implications\nsince it is much easier to obtain unlabeled target domain data than\nlabeled data. Moreover, we find that pre-training on multiple domains\nimproves generalization performance on domains not seen during training.\nWe will release pre-trained models.\n"
      ],
      "doi": "10.21437/Interspeech.2021-236",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "higuchi21_interspeech": {
      "authors": [
        [
          "Yosuke",
          "Higuchi"
        ],
        [
          "Niko",
          "Moritz"
        ],
        [
          "Jonathan Le",
          "Roux"
        ],
        [
          "Takaaki",
          "Hori"
        ]
      ],
      "title": "Momentum Pseudo-Labeling for Semi-Supervised Speech Recognition",
      "original": "0571",
      "page_count": 5,
      "order": 147,
      "p1": "726",
      "pn": "730",
      "abstract": [
        "Pseudo-labeling (PL) has been shown to be effective in semi-supervised\nautomatic speech recognition (ASR), where a base model is self-trained\nwith pseudo-labels generated from unlabeled data. While PL can be further\nimproved by iteratively updating pseudo-labels as the model evolves,\nmost of the previous approaches involve inefficient retraining of the\nmodel or intricate control of the label update. We present <i>momentum\npseudo-labeling</i> (MPL), a simple yet effective strategy for semi-supervised\nASR. MPL consists of a pair of <i>online</i> and <i>offline</i> models\nthat interact and learn from each other, inspired by the mean teacher\nmethod. The online model is trained to predict pseudo-labels generated\non the fly by the offline model. The offline model maintains a momentum-based\nmoving average of the online model. MPL is performed in a single training\nprocess and the interaction between the two models effectively helps\nthem reinforce each other to improve the ASR performance. We apply\nMPL to an end-to-end ASR model based on the connectionist temporal\nclassification. The experimental results demonstrate that MPL effectively\nimproves over the base model and is scalable to different semi-supervised\nscenarios with varying amounts of data or domain mismatch.\n"
      ],
      "doi": "10.21437/Interspeech.2021-571",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "misra21_interspeech": {
      "authors": [
        [
          "Ananya",
          "Misra"
        ],
        [
          "Dongseong",
          "Hwang"
        ],
        [
          "Zhouyuan",
          "Huo"
        ],
        [
          "Shefali",
          "Garg"
        ],
        [
          "Nikhil",
          "Siddhartha"
        ],
        [
          "Arun",
          "Narayanan"
        ],
        [
          "Khe Chai",
          "Sim"
        ]
      ],
      "title": "A Comparison of Supervised and Unsupervised Pre-Training of End-to-End Models",
      "original": "0654",
      "page_count": 5,
      "order": 148,
      "p1": "731",
      "pn": "735",
      "abstract": [
        "In the absence of large-scale in-domain supervised training data, ASR\nmodels can achieve reasonable performance through pre-training on additional\ndata that is unlabeled, mismatched or both. Given such data constraints,\nwe compare pre-training end-to-end models on matched but unlabeled\ndata (unsupervised) and on labeled but mismatched data (supervised),\nwhere the labeled data is mismatched in either domain or language.\nAcross encoder architectures, pre-training methods and languages, our\nexperiments indicate that both types of pre-training improve performance,\nwith relative WER reductions of 15&#8211;30% in the domain mismatch\ncase and up to 15% in the language mismatch condition. We further find\nthat the advantage from unsupervised pre-training is most prominent\nwhen there is no matched and labeled fine-tuning data, provided that\na sufficient amount of mismatched data is still available for supervised\nfine-tuning.\n"
      ],
      "doi": "10.21437/Interspeech.2021-654",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "chen21c_interspeech": {
      "authors": [
        [
          "Zhehuai",
          "Chen"
        ],
        [
          "Andrew",
          "Rosenberg"
        ],
        [
          "Yu",
          "Zhang"
        ],
        [
          "Heiga",
          "Zen"
        ],
        [
          "Mohammadreza",
          "Ghodsi"
        ],
        [
          "Yinghui",
          "Huang"
        ],
        [
          "Jesse",
          "Emond"
        ],
        [
          "Gary",
          "Wang"
        ],
        [
          "Bhuvana",
          "Ramabhadran"
        ],
        [
          "Pedro J.",
          "Moreno"
        ]
      ],
      "title": "Semi-Supervision in ASR: Sequential MixMatch and Factorized TTS-Based Augmentation",
      "original": "0677",
      "page_count": 5,
      "order": 149,
      "p1": "736",
      "pn": "740",
      "abstract": [
        "Semi and self-supervised training techniques have the potential to\nimprove performance of speech recognition systems without additional\ntranscribed speech data. In this work, we demonstrate the efficacy\nof two approaches to semi-supervision for automated speech recognition.\nThe two approaches leverage vast amounts of available unspoken text\nand untranscribed audio. First, we present <i>factorized multilingual\nspeech synthesis</i> to improve data augmentation on unspoken text.\nNext, we propose the <i>Sequential MixMatch</i> algorithm with <i>iterative\nlearning</i> to learn from untranscribed speech. The algorithm is built\non top of our online implementation of Noisy Student Training. We demonstrate\nthe compatibility of these techniques yielding an overall relative\nreduction of word error rate of up to 14.4% on the voice search tasks\non 4 Indic languages.\n"
      ],
      "doi": "10.21437/Interspeech.2021-677",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "likhomanenko21b_interspeech": {
      "authors": [
        [
          "Tatiana",
          "Likhomanenko"
        ],
        [
          "Qiantong",
          "Xu"
        ],
        [
          "Jacob",
          "Kahn"
        ],
        [
          "Gabriel",
          "Synnaeve"
        ],
        [
          "Ronan",
          "Collobert"
        ]
      ],
      "title": "slimIPL: Language-Model-Free Iterative Pseudo-Labeling",
      "original": "0740",
      "page_count": 5,
      "order": 150,
      "p1": "741",
      "pn": "745",
      "abstract": [
        "Recent results in end-to-end automatic speech recognition have demonstrated\nthe efficacy of pseudo-labeling for semi-supervised models trained\nboth with Connectionist Temporal Classification (CTC) and Sequence-to-Sequence\n(seq2seq) losses. Iterative Pseudo-Labeling (IPL), which continuously\ntrains a single model using pseudo-labels iteratively re-generated\nas the model learns, has been shown to further improve performance\nin ASR. We improve upon the IPL algorithm: as the model learns, we\npropose to iteratively re-generate transcriptions with hard labels\n(the most probable tokens), that is, <i>without</i> a language model.\nWe call this approach Language-Model-Free IPL (slimIPL) and give a\nresultant training setup for low-resource settings with CTC-based models.\nslimIPL features a dynamic cache for pseudo-labels which reduces sensitivity\nto changes in relabeling hyperparameters and results in improved training\nstability. slimIPL is also highly-efficient and requires 3.5&#8211;4&#215;\nfewer computational resources to converge than other state-of-the-art\nsemi/self-supervised approaches. With only 10 hours of labeled audio,\nslimIPL is competitive with self-supervised approaches, and is state-of-the-art\nwith 100 hours of labeled audio without the use of a language model\nboth at test time and during pseudo-label generation.\n"
      ],
      "doi": "10.21437/Interspeech.2021-740",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "yue21_interspeech": {
      "authors": [
        [
          "Xianghu",
          "Yue"
        ],
        [
          "Haizhou",
          "Li"
        ]
      ],
      "title": "Phonetically Motivated Self-Supervised Speech Representation Learning",
      "original": "0905",
      "page_count": 5,
      "order": 151,
      "p1": "746",
      "pn": "750",
      "abstract": [
        "Self-supervised representation learning has seen remarkable success\nin encoding high-level semantic information from unlabelled speech\ndata. The studies have been focused on exploring new pretext tasks\nto improve the learned speech representation and various masking schemes\nwith reference to speech frames. We consider effective latent speech\nrepresentation should be phonetically informed. In this work, we propose\na novel phonetically motivated masking scheme. Specifically, we select\nthe masked speech frames according to the phonetic segmentation in\nan utterance. The phonetically motivated self-supervised representation\nlearns the speech representation that benefits downstream speech processing\ntasks. We evaluate the proposed learning algorithm on phoneme classification,\nspeech recognition, and speaker recognition, and show that it consistently\noutperforms competitive baselines.\n"
      ],
      "doi": "10.21437/Interspeech.2021-905",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "deng21_interspeech": {
      "authors": [
        [
          "Yan",
          "Deng"
        ],
        [
          "Rui",
          "Zhao"
        ],
        [
          "Zhong",
          "Meng"
        ],
        [
          "Xie",
          "Chen"
        ],
        [
          "Bing",
          "Liu"
        ],
        [
          "Jinyu",
          "Li"
        ],
        [
          "Yifan",
          "Gong"
        ],
        [
          "Lei",
          "He"
        ]
      ],
      "title": "Improving RNN-T for Domain Scaling Using Semi-Supervised Training with Neural TTS",
      "original": "1017",
      "page_count": 5,
      "order": 152,
      "p1": "751",
      "pn": "755",
      "abstract": [
        "Recurrent neural network transducer (RNN-T) has shown to be comparable\nwith conventional hybrid model for speech recognition. However, there\nis still a challenge in out-of-domain scenarios with context or words\ndifferent from training data. In this paper, we explore the semi-supervised\ntraining which optimizes RNN-T jointly with neural text-to-speech (TTS)\nto better generalize to new domains using domain-specific text data.\nWe apply the method to two tasks: one with out-of-domain context and\nthe other with significant out-of-vocabulary (OOV) words. The results\nshow that the proposed method significantly improves the recognition\naccuracy in both tasks, resulting in 61.4% and 53.8% relative word\nerror rate (WER) reductions respectively, from a well-trained RNN-T\nwith 65 thousand hours of training data. We do further study on the\nsemi-supervised training methodology: 1) which modules of RNN-T model\nto be updated; 2) the impact of using different neural TTS models;\n3) the performance of using text with different relevancy to target\ndomain. Finally, we compare several RNN-T customization methods, and\nconclude that semi-supervised training with neural TTS is comparable\nand complementary with Internal Language Model Estimation (ILME) or\nbiasing.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1017",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "seyfarth21_interspeech": {
      "authors": [
        [
          "Scott",
          "Seyfarth"
        ],
        [
          "Sundararajan",
          "Srinivasan"
        ],
        [
          "Katrin",
          "Kirchhoff"
        ]
      ],
      "title": "Speaker-Conversation Factorial Designs for Diarization Error Analysis",
      "original": "1864",
      "page_count": 5,
      "order": 153,
      "p1": "756",
      "pn": "760",
      "abstract": [
        "Speaker diarization accuracy can be affected by both acoustics and\nconversation characteristics. Determining the cause of diarization\nerrors is difficult because speaker voice acoustics and conversation\nstructure co-vary, and the interactions between acoustics, conversational\nstructure, and diarization accuracy are complex. This paper proposes\na methodology that can distinguish independent marginal effects of\nacoustic and conversation characteristics on diarization accuracy by\nremixing conversations in a factorial design. As an illustration, this\napproach is used to investigate gender-related and language-related\naccuracy differences with three diarization systems: a baseline system\nusing subsegment x-vector clustering, a variant of it with shorter\nsubsegments, and a third system based on a Bayesian hidden Markov model.\nOur analysis shows large accuracy disparities for the baseline system\nprimarily due to conversational structure, which are partially mitigated\nin the other two systems. The illustration thus demonstrates how the\nmethodology can be used to identify and guide diarization model improvements.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1864",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "mcgowan21_interspeech": {
      "authors": [
        [
          "Ross",
          "McGowan"
        ],
        [
          "Jinru",
          "Su"
        ],
        [
          "Vince",
          "DiCocco"
        ],
        [
          "Thejaswi",
          "Muniyappa"
        ],
        [
          "Grant P.",
          "Strimel"
        ]
      ],
      "title": "SmallER: Scaling Neural Entity Resolution for Edge Devices",
      "original": "0098",
      "page_count": 5,
      "order": 154,
      "p1": "761",
      "pn": "765",
      "abstract": [
        "In this paper we introduce SmallER, a scalable neural entity resolution\nsystem capable of running directly on edge devices. SmallER addresses\nconstraints imposed by the on-device setting such as bounded memory\nconsumption for both model and catalog storage, limited compute resources,\nand related latency challenges introduced by those restrictions. Our\nmodel includes distinct modules to learn syntactic and semantic information\nand is trained to handle multiple domains within one compact architecture.\nWe use compressed tries to reduce the space required to store catalogs\nand a novel implementation of spatial partitioning trees to strike\na balance between reducing runtime latency and preserving recall relative\nto full catalog search. Our final model consumes only 3MB of memory\nat inference time with classification accuracy surpassing that of previously\nestablished, domain-specific baseline models on live customer utterances.\nFor the largest catalogs we consider (300 or more entries), our proxy\nmetric for runtime latency is reduced by more than 90%.\n"
      ],
      "doi": "10.21437/Interspeech.2021-98",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "rocholl21_interspeech": {
      "authors": [
        [
          "Johann C.",
          "Rocholl"
        ],
        [
          "Vicky",
          "Zayats"
        ],
        [
          "Daniel D.",
          "Walker"
        ],
        [
          "Noah B.",
          "Murad"
        ],
        [
          "Aaron",
          "Schneider"
        ],
        [
          "Daniel J.",
          "Liebling"
        ]
      ],
      "title": "Disfluency Detection with Unlabeled Data and Small BERT Models",
      "original": "0351",
      "page_count": 5,
      "order": 155,
      "p1": "766",
      "pn": "770",
      "abstract": [
        "Disfluency detection models now approach high accuracy on English text.\nHowever, little exploration has been done in improving the size and\ninference time of the model. At the same time, Automatic Speech Recognition\n(ASR) models are moving from server-side inference to local, on-device\ninference. Supporting models in the transcription pipeline (like disfluency\ndetection) must follow suit. In this work we concentrate on the disfluency\ndetection task, focusing on small, fast, on-device models based on\nthe BERT architecture. We demonstrate it is possible to train disfluency\ndetection models as small as 1.3 MiB, while retaining high performance.\nWe build on previous work that showed the benefit of data augmentation\napproaches such as self-training. Then, we evaluate the effect of domain\nmismatch between conversational and written text on model performance.\nWe find that domain adaptation and data augmentation strategies have\na more pronounced effect on these smaller models, as compared to conventional\nBERT models.\n"
      ],
      "doi": "10.21437/Interspeech.2021-351",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "chen21d_interspeech": {
      "authors": [
        [
          "Qian",
          "Chen"
        ],
        [
          "Wen",
          "Wang"
        ],
        [
          "Mengzhe",
          "Chen"
        ],
        [
          "Qinglin",
          "Zhang"
        ]
      ],
      "title": "Discriminative Self-Training for Punctuation Prediction",
      "original": "0246",
      "page_count": 5,
      "order": 156,
      "p1": "771",
      "pn": "775",
      "abstract": [
        "Punctuation prediction for automatic speech recognition (ASR) output\ntranscripts plays a crucial role for improving the readability of the\nASR transcripts and for improving the performance of downstream natural\nlanguage processing applications. However, achieving good performance\non punctuation prediction often requires large amounts of labeled speech\ntranscripts, which is expensive and laborious. In this paper, we propose\na Discriminative Self-Training approach with weighted loss and discriminative\nlabel smoothing to exploit unlabeled speech transcripts. Experimental\nresults on the English IWSLT2011 benchmark test set and an internal\nChinese spoken language dataset demonstrate that the proposed approach\nachieves significant improvement on punctuation prediction accuracy\nover strong baselines including BERT, RoBERTa, and ELECTRA models.\nThe proposed Discriminative Self-Training approach outperforms the\nvanilla self-training approach. We establish a new state-of-the-art\n(SOTA) on the IWSLT2011 test set, outperforming the current SOTA model\nby 1.3% absolute gain on F<SUB>1</SUB>.\n"
      ],
      "doi": "10.21437/Interspeech.2021-246",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "ihori21_interspeech": {
      "authors": [
        [
          "Mana",
          "Ihori"
        ],
        [
          "Naoki",
          "Makishima"
        ],
        [
          "Tomohiro",
          "Tanaka"
        ],
        [
          "Akihiko",
          "Takashima"
        ],
        [
          "Shota",
          "Orihashi"
        ],
        [
          "Ryo",
          "Masumura"
        ]
      ],
      "title": "Zero-Shot Joint Modeling of Multiple Spoken-Text-Style Conversion Tasks Using Switching Tokens",
      "original": "1607",
      "page_count": 5,
      "order": 157,
      "p1": "776",
      "pn": "780",
      "abstract": [
        "In this paper, we propose a novel spoken-text-style conversion method\nthat can simultaneously execute multiple style conversion modules such\nas punctuation restoration and disfluency deletion without preparing\nmatched datasets. In practice, transcriptions generated by automatic\nspeech recognition systems are not highly readable because they often\ninclude many disfluencies and do not include punctuation marks. To\nimprove their readability, multiple spoken-text-style conversion modules\nthat individually model a single conversion task are cascaded because\nmatched datasets that simultaneously handle multiple conversion tasks\nare often unavailable. However, the cascading is unstable against the\norder of tasks because of the chain of conversion errors. Besides,\nthe computation cost of the cascading must be higher than the single\nconversion. To execute multiple conversion tasks simultaneously without\npreparing matched datasets, our key idea is to distinguish individual\nconversion tasks using the <i>on-off switch</i>. In our proposed zero-shot\njoint modeling, we switch the individual tasks using multiple switching\ntokens, enabling us to utilize a zero-shot learning approach to executing\nsimultaneous conversions. Our experiments on joint modeling of disfluency\ndeletion and punctuation restoration demonstrate the effectiveness\nof our method.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1607",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "lin21_interspeech": {
      "authors": [
        [
          "Binghuai",
          "Lin"
        ],
        [
          "Liyuan",
          "Wang"
        ]
      ],
      "title": "A Noise Robust Method for Word-Level Pronunciation Assessment",
      "original": "1005",
      "page_count": 5,
      "order": 158,
      "p1": "781",
      "pn": "785",
      "abstract": [
        "The common approach for pronunciation evaluation is based on Goodness\nof pronunciation (GOP). It has been found that GOP may perform worse\nunder noise conditions. Traditional methods compensate pronunciation\nfeatures to improve the performance of pronunciation assessment in\nnoise situations. This paper proposed a noise robust model for word-level\npronunciation assessment based on a domain adversarial training (DAT)\nmethod. We treat the pronunciation assessment in the clean and noise\nsituations as the source and target domains. The network is optimized\nby incorporating both the pronunciation assessment and noise domain\ndiscrimination. The domain labels are generated from unsupervised methods\nto adapt to various noise situations. We evaluate the model performance\nbased on English words recorded by Chinese English learners and labeled\nby three experts. Experimental results show on average the proposed\nmodel outperforms the baseline by 3% in Pearson correlation coefficients\n(PCC) and 4% in accuracy under different noise conditions.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1005",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "wintrode21_interspeech": {
      "authors": [
        [
          "Jonathan",
          "Wintrode"
        ]
      ],
      "title": "Targeted Keyword Filtering for Accelerated Spoken Topic Identification",
      "original": "1670",
      "page_count": 5,
      "order": 159,
      "p1": "786",
      "pn": "790",
      "abstract": [
        "We present a novel framework for spoken topic identification that simultaneously\nlearns both topic-specific keywords and acoustic keyword filters from\nonly document-level topic labels. At inference time, only audio segments\nlikely to contain topic-salient keywords are fully decoded, reducing\nthe system&#8217;s overall computation cost. We show that this filtering\nallows for effective topic classification while decoding only 50% of\nASR output word lattices, and achieves error rates within 1.2% and\nprecision within 2.6% of an unfiltered baseline system.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1670",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "palaskar21_interspeech": {
      "authors": [
        [
          "Shruti",
          "Palaskar"
        ],
        [
          "Ruslan",
          "Salakhutdinov"
        ],
        [
          "Alan W.",
          "Black"
        ],
        [
          "Florian",
          "Metze"
        ]
      ],
      "title": "Multimodal Speech Summarization Through Semantic Concept Learning",
      "original": "1923",
      "page_count": 5,
      "order": 160,
      "p1": "791",
      "pn": "795",
      "abstract": [
        "We propose a cascaded multimodal abstractive speech summarization model\nthat generates semantic concepts as an intermediate step towards summarization.\nWe describe a method to leverage existing multimodal dataset annotations\nto curate groundtruth labels for such intermediate concept modeling.\nIn addition to cascaded training, the concept labels also provide an\ninterpretable intermediate output level that helps improve performance\non the downstream summarization task. On the open-domain How2 data,\nwe conduct utterance-level and video-level experiments for two granularities\nof concepts: Specific and Abstract. We compare various multimodal fusion\nmodels for concept generation based on the respective input modalities.\nWe observe consistent improvements in concept modeling by using multimodal\nadaptation models over unimodal models. Using the cascaded multimodal\nspeech summarization model, we see a significant improvement of 7.5\nMETEOR points and 5.1 ROUGE-L points compared to previous methods of\nspeech summarization. Finally, we show the benefits of scalability\nof the proposed approaches on 2000 h of video data.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1923",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "lee21_interspeech": {
      "authors": [
        [
          "Hyunjae",
          "Lee"
        ],
        [
          "Jaewoong",
          "Yun"
        ],
        [
          "Hyunjin",
          "Choi"
        ],
        [
          "Seongho",
          "Joe"
        ],
        [
          "Youngjune L.",
          "Gwon"
        ]
      ],
      "title": "Enhancing Semantic Understanding with Self-Supervised Methods for Abstractive Dialogue Summarization",
      "original": "1270",
      "page_count": 5,
      "order": 161,
      "p1": "796",
      "pn": "800",
      "abstract": [
        "Contextualized word embeddings can lead to state-of-the-art performances\nin natural language understanding. Recently, a pre-trained deep contextualized\ntext encoder such as BERT has shown its potential in improving natural\nlanguage tasks including abstractive summarization. Existing approaches\nin dialogue summarization focus on incorporating a large language model\ninto summarization task trained on large-scale corpora consisting of\nnews articles rather than dialogues of multiple speakers. In this paper,\nwe introduce self-supervised methods to compensate shortcomings to\ntrain a dialogue summarization model. Our principle is to detect incoherent\ninformation flows using pretext dialogue text to enhance BERT&#8217;s\nability to contextualize the dialogue text representations. We build\nand fine-tune an abstractive dialogue summarization model on a shared\nencoder-decoder architecture using the enhanced BERT. We empirically\nevaluate our abstractive dialogue summarizer with the SAMSum corpus,\na recently introduced dataset with abstractive dialogue summaries.\nAll of our methods have contributed improvements to abstractive summary\nmeasured in ROUGE scores. Through an extensive ablation study, we also\npresent a sensitivity analysis to critical model hyperparameters, probabilities\nof switching utterances and masking interlocutors.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1270",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "wodarczak21_interspeech": {
      "authors": [
        [
          "Marcin",
          "W\u0142odarczak"
        ],
        [
          "Emer",
          "Gilmartin"
        ]
      ],
      "title": "Speaker Transition Patterns in Three-Party Conversation: Evidence from English, Estonian and Swedish",
      "original": "0199",
      "page_count": 5,
      "order": 162,
      "p1": "801",
      "pn": "805",
      "abstract": [
        "During conversation, speakers hold and relinquish the floor, resulting\nin turn yield and retention. We examine these phenomena in three-party\nconversations in English, Swedish, and Estonian. We define within-\nand between-speaker transitions in terms of shorter intervals of speech,\nsilence and overlap bounded by stretches of one-party speech longer\nthan 1 second by the same or different speakers. This method gives\nus insights into how turn change and retention proceed, revealing that\nthe majority of speaker transitions are more complex and involve more\nintermediate activity than a single silence or overlap. We examine\nthe composition of within and between transitions in terms of number\nof speakers involved, incidence and proportion of solo speech, silence\nand overlap. We derive the most common within- and between-speaker\ntransitions in the three languages, finding evidence of striking commonalities\nin how the floor is managed. Our findings suggest that current models\nof turn-taking used in dialogue technology could be extended using\nthese results to more accurately reflect the realities of human-human\ndialogue.\n"
      ],
      "doi": "10.21437/Interspeech.2021-199",
      "author_area_id": "11",
      "author_area_label": "Spoken Dialog Systems and Conversational Analysis"
    },
    "broughton21_interspeech": {
      "authors": [
        [
          "Samuel J.",
          "Broughton"
        ],
        [
          "Md. Asif",
          "Jalal"
        ],
        [
          "Roger K.",
          "Moore"
        ]
      ],
      "title": "Investigating Deep Neural Structures and their Interpretability in the Domain of Voice Conversion",
      "original": "1730",
      "page_count": 5,
      "order": 163,
      "p1": "806",
      "pn": "810",
      "abstract": [
        "Generative Adversarial Networks (GANs) are machine learning networks\nbased around creating synthetic data. Voice Conversion (VC) is a subset\nof voice translation that involves translating the paralinguistic features\nof a source speaker to a target speaker while preserving the linguistic\ninformation. The aim of non-parallel conditional GANs for VC is to\ntranslate an acoustic speech feature sequence from one domain to another\nwithout the use of paired data. In the study reported here, we investigated\nthe interpretability of state-of-the-art implementations of non-parallel\nGANs in the domain of VC. We show that the learned representations\nin the repeating layers of a particular GAN architecture remain close\nto their original random initialised parameters, demonstrating that\nit is the number of repeating layers that is more responsible for the\nquality of the output. We also analysed the learned representations\nof a model trained on one particular dataset when used during transfer\nlearning on another dataset. This also showed high levels of similarity\nin the repeating layers. Together, these results provide new insight\ninto how the learned representations of deep generative networks change\nduring learning and the importance of the number of layers, which would\nhelp build better GAN-based speech conversion models.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1730",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "zhou21b_interspeech": {
      "authors": [
        [
          "Kun",
          "Zhou"
        ],
        [
          "Berrak",
          "Sisman"
        ],
        [
          "Haizhou",
          "Li"
        ]
      ],
      "title": "Limited Data Emotional Voice Conversion Leveraging Text-to-Speech: Two-Stage Sequence-to-Sequence Training",
      "original": "0781",
      "page_count": 5,
      "order": 164,
      "p1": "811",
      "pn": "815",
      "abstract": [
        "Emotional voice conversion (EVC) aims to change the emotional state\nof an utterance while preserving the linguistic content and speaker\nidentity. In this paper, we propose a novel 2-stage training strategy\nfor sequence-to-sequence emotional voice conversion with a limited\namount of emotional speech data. We note that the proposed EVC framework\nleverages text-to-speech (TTS) as they share a common goal that is\nto generate high-quality expressive voice. In stage 1, we perform style\ninitialization with a multi-speaker TTS corpus, to disentangle speaking\nstyle and linguistic content. In stage 2, we perform emotion training\nwith a limited amount of emotional speech data, to learn how to disentangle\nemotional style and linguistic information from the speech. The proposed\nframework can perform both spectrum and prosody conversion and achieves\nsignificant improvement over the state-of-the-art baselines in both\nobjective and subjective evaluation.\n"
      ],
      "doi": "10.21437/Interspeech.2021-781",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "ding21_interspeech": {
      "authors": [
        [
          "Yi-Yang",
          "Ding"
        ],
        [
          "Li-Juan",
          "Liu"
        ],
        [
          "Yu",
          "Hu"
        ],
        [
          "Zhen-Hua",
          "Ling"
        ]
      ],
      "title": "Adversarial Voice Conversion Against Neural Spoofing Detectors",
      "original": "0948",
      "page_count": 5,
      "order": 165,
      "p1": "816",
      "pn": "820",
      "abstract": [
        "The naturalness and similarity of voice conversion have been significantly\nimproved in recent years with the development of deep-learning-based\nconversion models and neural vocoders. Accordingly, the task of detecting\nspoofing speech also attracts research attention. In the latest ASVspoof\n2019 challenge, the best spoofing detection model can distinguish most\nartificial utterances from natural ones. Inspired by recent progress\nof adversarial example generation, this paper proposes an adversarial\npost-processing network (APN) which generates adversarial examples\nagainst a neural-network-based spoofing detector by white-box attack.\nThe APN model post-processes the speech waveforms generated by a baseline\nvoice conversion system. An adversarial loss derived from the spoofing\ndetector together with two regularization losses are applied to optimize\nthe parameters of APN. In our experiments, using the logical access\n(LA) dataset of ASVspoof 2019, results show that our proposed method\ncan improve the adversarial ability of converted speech against the\nspoofing detectors based on light convolution neural networks (LCNNs)\neffectively without degrading its subjective quality.\n"
      ],
      "doi": "10.21437/Interspeech.2021-948",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "he21b_interspeech": {
      "authors": [
        [
          "Xiangheng",
          "He"
        ],
        [
          "Junjie",
          "Chen"
        ],
        [
          "Georgios",
          "Rizos"
        ],
        [
          "Bj\u00f6rn W.",
          "Schuller"
        ]
      ],
      "title": "An Improved StarGAN for Emotional Voice Conversion: Enhancing Voice Quality and Data Augmentation",
      "original": "1253",
      "page_count": 5,
      "order": 166,
      "p1": "821",
      "pn": "825",
      "abstract": [
        "Emotional Voice Conversion (EVC) aims to convert the emotional style\nof a source speech signal to a target style while preserving its content\nand speaker identity information. Previous emotional conversion studies\ndo not disentangle emotional information from emotion-independent information\nthat should be preserved, thus transforming it all in a monolithic\nmanner and generating audio of low quality, with linguistic distortions.\nTo address this distortion problem, we propose a novel StarGAN framework\nalong with a two-stage training process that separates emotional features\nfrom those independent of emotion by using an autoencoder with two\nencoders as the generator of the Generative Adversarial Network (GAN).\nThe proposed model achieves favourable results in both the objective\nevaluation and the subjective evaluation in terms of distortion, which\nreveals that the proposed model can effectively reduce distortion.\nFurthermore, in data augmentation experiments for end-to-end speech\nemotion recognition, the proposed StarGAN model achieves an increase\nof 2% in Micro-F1 and 5% in Macro-F1 compared to the baseline StarGAN\nmodel, which indicates that the proposed model is more valuable for\ndata augmentation.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1253",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "chen21e_interspeech": {
      "authors": [
        [
          "Ziyi",
          "Chen"
        ],
        [
          "Pengyuan",
          "Zhang"
        ]
      ],
      "title": "TVQVC: Transformer Based Vector Quantized Variational Autoencoder with CTC Loss for Voice Conversion",
      "original": "1301",
      "page_count": 5,
      "order": 167,
      "p1": "826",
      "pn": "830",
      "abstract": [
        "Techniques of voice conversion (VC) aim to modify the speaker identity\nand style of an utterance while preserving the linguistic content.\nAlthough there are lots of VC methods, the state of the art of VC is\nstill cascading automatic speech recognition (ASR) and text-to-speech\n(TTS). This paper presents a new structure of vector-quantized autoencoder\nbased on transformer with CTC loss for non-parallel VC, which inspired\nby cascading ASR and TTS VC method. Our proposed method combines CTC\nloss and vector quantization to get high-level linguistic information\nwithout speaker information. Objective and subjective evaluations on\nthe mandarin datasets show that the converted speech of our proposed\nmodel is better than baselines on naturalness, rhythm and speaker similarity.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1301",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "wang21g_interspeech": {
      "authors": [
        [
          "Zhichao",
          "Wang"
        ],
        [
          "Xinyong",
          "Zhou"
        ],
        [
          "Fengyu",
          "Yang"
        ],
        [
          "Tao",
          "Li"
        ],
        [
          "Hongqiang",
          "Du"
        ],
        [
          "Lei",
          "Xie"
        ],
        [
          "Wendong",
          "Gan"
        ],
        [
          "Haitao",
          "Chen"
        ],
        [
          "Hai",
          "Li"
        ]
      ],
      "title": "Enriching Source Style Transfer in Recognition-Synthesis Based Non-Parallel Voice Conversion",
      "original": "1351",
      "page_count": 5,
      "order": 168,
      "p1": "831",
      "pn": "835",
      "abstract": [
        "Current voice conversion (VC) methods can successfully convert timbre\nof the audio. As modeling source audio&#8217;s prosody effectively\nis a challenging task, there are still limitations of transferring\nsource style to the converted speech. This study proposes a source\nstyle transfer method based on recognition-synthesis framework. Previously\nin speech generation task, prosody can be modeled explicitly with prosodic\nfeatures or implicitly with a latent prosody extractor. In this paper,\ntaking advantages of both, we model the prosody in a hybrid manner,\nwhich effectively combines explicit and implicit methods in a proposed\nprosody module. Specifically, prosodic features are used to explicit\nmodel prosody, while VAE and reference encoder are used to implicitly\nmodel prosody, which take Mel spectrum and bottleneck feature as input\nrespectively. Furthermore, adversarial training is introduced to remove\nspeaker-related information from the VAE outputs, avoiding leaking\nsource speaker information while transferring style. Finally, we use\na modified self-attention based encoder to extract sentential context\nfrom bottleneck features, which also implicitly aggregates the prosodic\naspects of source speech from the layered representations. Experiments\nshow that our approach is superior to the baseline and a competitive\nsystem in terms of style transfer; meanwhile, the speech quality and\nspeaker similarity are well maintained.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1351",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "lin21b_interspeech": {
      "authors": [
        [
          "Jheng-hao",
          "Lin"
        ],
        [
          "Yist Y.",
          "Lin"
        ],
        [
          "Chung-Ming",
          "Chien"
        ],
        [
          "Hung-yi",
          "Lee"
        ]
      ],
      "title": "S2VC: A Framework for Any-to-Any Voice Conversion with Self-Supervised Pretrained Representations",
      "original": "1356",
      "page_count": 5,
      "order": 169,
      "p1": "836",
      "pn": "840",
      "abstract": [
        "Any-to-any voice conversion (VC) aims to convert the timbre of utterances\nfrom and to any speakers seen or unseen during training. Various any-to-any\nVC approaches have been proposed like  AutoVC, AdaINVC, and FragmentVC.\n AutoVC, and AdaINVC utilize source and target encoders to disentangle\nthe content and speaker information of the features. FragmentVC utilizes\ntwo encoders to encode source and target information and adopts cross\nattention to align the source and target features with similar phonetic\ncontent. Moreover, pretrained features are adopted.  AutoVC used d-vector\nto extract speaker information, and self-supervised learning (SSL)\nfeatures like wav2vec 2.0 is used in FragmentVC to extract the phonetic\ncontent information. Different from previous works, we proposed S2VC\nthat utilizes Self-Supervised features as both source and target features\nfor the VC model. Supervised phoneme posteriorgram (PPG), which is\nbelieved to be speaker-independent and widely used in VC to extract\ncontent information, is chosen as a strong baseline for SSL features.\nThe objective evaluation and subjective evaluation both show models\ntaking SSL feature CPC as both source and target features outperforms\nthat taking PPG as source feature, suggesting that SSL features have\ngreat potential in improving VC.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1356",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "liberatore21_interspeech": {
      "authors": [
        [
          "Christopher",
          "Liberatore"
        ],
        [
          "Ricardo",
          "Gutierrez-Osuna"
        ]
      ],
      "title": "An Exemplar Selection Algorithm for Native-Nonnative Voice Conversion",
      "original": "1740",
      "page_count": 5,
      "order": 170,
      "p1": "841",
      "pn": "845",
      "abstract": [
        "We present an algorithm for selecting exemplars for native-to-nonnative\nvoice conversion (VC) using a Sparse, Anchor-Based Representation of\nspeech (SABR). The algorithm uses phoneme labels and clustering to\nlearn optimal exemplars when source and target speakers are affected\nby poor time alignment, as is common in in native-to-nonnative voice\nconversion. We evaluate the method on speech from the ARCTIC and L2-ARCTIC\ncorpora and compare it to a baseline exemplar-based VC algorithm. The\nproposed algorithm significantly improves synthesis quality and more\nthan doubles that of a baseline exemplar-based VC system while using\ntwo orders of magnitude fewer atoms. Additionally, the proposed algorithm\nsignificantly reduces the VC error and improves the synthesis quality\nas compared to unoptimized SABR models. We discuss the implications\nof both optimization algorithms for SABR and broader exemplar-based\nVC systems.Index terms should be included as shown below.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1740",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "wang21h_interspeech": {
      "authors": [
        [
          "Jie",
          "Wang"
        ],
        [
          "Jingbei",
          "Li"
        ],
        [
          "Xintao",
          "Zhao"
        ],
        [
          "Zhiyong",
          "Wu"
        ],
        [
          "Shiyin",
          "Kang"
        ],
        [
          "Helen",
          "Meng"
        ]
      ],
      "title": "Adversarially Learning Disentangled Speech Representations for Robust Multi-Factor Voice Conversion",
      "original": "1990",
      "page_count": 5,
      "order": 171,
      "p1": "846",
      "pn": "850",
      "abstract": [
        "Factorizing speech as disentangled speech representations is vital\nto achieve highly controllable style transfer in voice conversion (VC).\nConventional speech representation learning methods in VC only factorize\nspeech as speaker and content, lacking controllability on other prosody-related\nfactors. State-of-the-art speech representation learning methods for\nmore speech factors are using primary disentangle algorithms such as\nrandom resampling and ad-hoc bottleneck layer size adjustment, which\nhowever is hard to ensure robust speech representation disentanglement.\nTo increase the robustness of highly controllable style transfer on\nmultiple factors in VC, we propose a disentangled speech representation\nlearning framework based on adversarial learning. Four speech representations\ncharacterizing content, timbre, rhythm and pitch are extracted, and\nfurther disentangled by an adversarial Mask-And-Predict (MAP) network\ninspired by BERT. The adversarial network is used to minimize the correlations\nbetween the speech representations, by randomly masking and predicting\none of the representations from the others. Experimental results show\nthat the proposed framework significantly improves the robustness of\nVC on multiple factors by increasing the speech quality MOS from 2.79\nto 3.30 and decreasing the MCD from 3.89 to 3.58.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1990",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "luong21_interspeech": {
      "authors": [
        [
          "Manh",
          "Luong"
        ],
        [
          "Viet Anh",
          "Tran"
        ]
      ],
      "title": "Many-to-Many Voice Conversion Based Feature Disentanglement Using Variational Autoencoder",
      "original": "2086",
      "page_count": 5,
      "order": 172,
      "p1": "851",
      "pn": "855",
      "abstract": [
        "Voice conversion is a challenging task which transforms the voice characteristics\nof a source speaker to a target speaker without changing linguistic\ncontent. Recently, there have been many works on many-to-many Voice\nConversion (VC) based on Variational Autoencoder (VAEs) achieving good\nresults, however, these methods lack the ability to disentangle speaker\nidentity and linguistic content to achieve good performance on unseen\nspeaker&#8217;s scenarios. In this paper, we propose a new method based\non feature disentanglement to tackle many-to-many voice conversion.\nThe method has the capability to disentangle speaker identity and linguistic\ncontent from utterances, it can convert from many source speakers to\nmany target speakers with a single autoencoder network. Moreover, it\nnaturally deals with the unseen target speaker&#8217;s scenarios. We\nperform both objective and subjective evaluations to show the competitive\nperformance of our proposed method compared with other state-of-the-art\nmodels in terms of naturalness and target speaker similarity.\n"
      ],
      "doi": "10.21437/Interspeech.2021-2086",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "chouchane21_interspeech": {
      "authors": [
        [
          "Ouba\u00efda",
          "Chouchane"
        ],
        [
          "Baptiste",
          "Brossier"
        ],
        [
          "Jorge Esteban Gamboa",
          "Gamboa"
        ],
        [
          "Thomas",
          "Lardy"
        ],
        [
          "Hemlata",
          "Tak"
        ],
        [
          "Orhan",
          "Ermis"
        ],
        [
          "Madhu R.",
          "Kamble"
        ],
        [
          "Jose",
          "Patino"
        ],
        [
          "Nicholas",
          "Evans"
        ],
        [
          "Melek",
          "\u00d6nen"
        ],
        [
          "Massimiliano",
          "Todisco"
        ]
      ],
      "title": "Privacy-Preserving Voice Anti-Spoofing Using Secure Multi-Party Computation",
      "original": "0983",
      "page_count": 5,
      "order": 173,
      "p1": "856",
      "pn": "860",
      "abstract": [
        "In recent years the automatic speaker verification (ASV) community\nhas grappled with vulnerabilities to spoofing attacks whereby fraudsters\nmasquerade as enrolled subjects to provoke illegitimate accepts. Countermeasures\nhave hence been developed to protect ASV systems from such attacks.\nGiven that recordings of speech contain potentially sensitive information,\nany system operating upon them, including spoofing countermeasures,\nmust have provisions for privacy preservation. While privacy enhancing\ntechnologies such as Homomorphic Encryption or Secure Multi-Party Computation\n(MPC) are effective in preserving privacy, these tend to impact upon\ncomputational capacity and computational precision, while no available\nspoofing countermeasures preserve privacy. This paper reports the first\nsolution based upon the combination of shallow neural networks with\nsecure MPC. Experiments performed using the ASVspoof 2019 logical access\ndatabase show that the proposed solution is not only computationally\nefficient, but that it also improves upon the performance of the ASVspoof\nbaseline countermeasure, all while preserving privacy.\n"
      ],
      "doi": "10.21437/Interspeech.2021-983",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement:14 Special Sessions"
    },
    "aloufi21_interspeech": {
      "authors": [
        [
          "Ranya",
          "Aloufi"
        ],
        [
          "Hamed",
          "Haddadi"
        ],
        [
          "David",
          "Boyle"
        ]
      ],
      "title": "Configurable Privacy-Preserving Automatic Speech Recognition",
      "original": "1783",
      "page_count": 5,
      "order": 174,
      "p1": "861",
      "pn": "865",
      "abstract": [
        "Voice assistive technologies have given rise to far-reaching privacy\nand security concerns. In this paper we investigate whether modular\nautomatic speech recognition (ASR) can improve privacy in voice assistive\nsystems by combining independently trained separation, recognition,\nand discretization modules to design configurable privacy-preserving\nASR systems. We evaluate privacy concerns and the effects of applying\nvarious state-of-the-art techniques at each stage of the system, and\nreport results using task-specific metrics (i.e., WER, ABX, and accuracy).\nWe show that overlapping speech inputs to ASR systems present further\nprivacy concerns, and how these may be mitigated using speech separation\nand optimization techniques. Our discretization module is shown to\nminimize paralinguistics privacy leakage from ASR acoustic models to\nlevels commensurate with random guessing. We show that voice privacy\ncan be <i>configurable</i>, and argue this presents new opportunities\nfor privacy-preserving applications incorporating ASR.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1783",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement:14 Special Sessions"
    },
    "novotney21_interspeech": {
      "authors": [
        [
          "Scott",
          "Novotney"
        ],
        [
          "Yile",
          "Gu"
        ],
        [
          "Ivan",
          "Bulyko"
        ]
      ],
      "title": "Adjunct-Emeritus Distillation for Semi-Supervised Language Model Adaptation",
      "original": "0027",
      "page_count": 5,
      "order": 175,
      "p1": "866",
      "pn": "870",
      "abstract": [
        "To improve customer privacy, commercial speech applications are reducing\nhuman transcription of customer data. This has a negative impact on\nlanguage model training due to a smaller amount of in-domain transcripts.\nPrior work demonstrated that training on automated transcripts alone\nprovides modest gains due to reinforcement of recognition errors. We\nconsider a new condition, where a model trained on historical human\ntranscripts, but not the transcripts themselves, are available to us.\nTo overcome temporal drift in vocabulary and topics, we propose a novel\nextension of knowledge distillation, <i>adjunct-emeritus distillation</i>\nwhere two imperfect teachers jointly train a student model. We conduct\nexperiments on an English voice assistant domain and simulate a one\nyear gap in human transcription. Unlike fine-tuning, our approach is\narchitecture agnostic and achieves a 14% relative reduction in perplexity\nover the baseline approach of freezing model development and improves\nover the baseline of knowledge distillation.\n"
      ],
      "doi": "10.21437/Interspeech.2021-27",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement:14 Special Sessions"
    },
    "ro21_interspeech": {
      "authors": [
        [
          "Jae",
          "Ro"
        ],
        [
          "Mingqing",
          "Chen"
        ],
        [
          "Rajiv",
          "Mathews"
        ],
        [
          "Mehryar",
          "Mohri"
        ],
        [
          "Ananda Theertha",
          "Suresh"
        ]
      ],
      "title": "Communication-Efficient Agnostic Federated Averaging",
      "original": "0153",
      "page_count": 5,
      "order": 176,
      "p1": "871",
      "pn": "875",
      "abstract": [
        "In distributed learning settings such as federated learning, the training\nalgorithm can be potentially biased towards different clients. [1]\nproposed a domain-agnostic learning algorithm, where the model is optimized\nfor any target distribution formed by a mixture of the client distributions\nin order to overcome this bias. They further proposed an algorithm\nfor the cross-silo federated learning setting, where the number of\nclients is small. We consider this problem in the cross-device setting,\nwhere the number of clients is much larger. We propose a communication-efficient\ndistributed algorithm called  Agnostic Federated Averaging (or  AgnosticFedAvg)\nto minimize the domain-agnostic objective proposed in [1], which is\namenable to other private mechanisms such as secure aggregation. We\nhighlight two types of naturally occurring domains in federated learning\nand argue that  AgnosticFedAvg performs well on both. To demonstrate\nthe practical effectiveness of  AgnosticFedAvg, we report positive\nresults for large-scale language modeling tasks in both simulation\nand live experiments, where the latter involves training language models\nfor Spanish virtual keyboard for millions of user devices.\n"
      ],
      "doi": "10.21437/Interspeech.2021-153",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement:14 Special Sessions"
    },
    "koppelmann21_interspeech": {
      "authors": [
        [
          "Timm",
          "Koppelmann"
        ],
        [
          "Alexandru",
          "Nelus"
        ],
        [
          "Lea",
          "Sch\u00f6nherr"
        ],
        [
          "Dorothea",
          "Kolossa"
        ],
        [
          "Rainer",
          "Martin"
        ]
      ],
      "title": "Privacy-Preserving Feature Extraction for Cloud-Based Wake Word Verification",
      "original": "0262",
      "page_count": 5,
      "order": 177,
      "p1": "876",
      "pn": "880",
      "abstract": [
        "Wake word detection and verification systems often involve a local,\non-device wake word detector and a cloud-based verification node. In\nsuch systems, the audio representation sent to the cloud-based server\nmay exhibit sensitive information that might be intercepted by an eavesdropper.\nTo improve privacy of cloud-based wake word verification (WWV) systems,\nwe propose to use a privacy-preserving feature representation that\nminimizes the automatic speech recognition (ASR) capability of a potential\nattacker. The proposed approach employs an adversarial training schedule\nthat aims to minimize an attacker&#8217;s word error rate (WER) while\nmaintaining a high WWV performance. To this end, we apply an adaptive\nweighting factor in the combined loss function to control the balance\nbetween minimizing the WWV loss and maximizing the ASR loss. We show\nthat the proposed training method significantly reduces possible privacy\nrisks while maintaining a strong WWV performance.\n"
      ],
      "doi": "10.21437/Interspeech.2021-262",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement:14 Special Sessions"
    },
    "yang21_interspeech": {
      "authors": [
        [
          "Chao-Han Huck",
          "Yang"
        ],
        [
          "Sabato Marco",
          "Siniscalchi"
        ],
        [
          "Chin-Hui",
          "Lee"
        ]
      ],
      "title": "PATE-AAE: Incorporating Adversarial Autoencoder into Private Aggregation of Teacher Ensembles for Spoken Command Classification",
      "original": "0640",
      "page_count": 5,
      "order": 178,
      "p1": "881",
      "pn": "885",
      "abstract": [
        "We propose using an adversarial autoencoder (AAE) to replace generative\nadversarial network (GAN) in private aggregation of teacher ensembles\n(PATE), a solution for ensuring differential privacy in speech applications.\nThe AAE architecture allows us to obtain good synthetic speech leveraging\nupon a discriminative training of latent vectors. Such synthetic speech\nis used to build a privacy-preserving classifier when non-sensitive\ndata is not sufficiently available in the public domain. This classifier\nfollows the PATE scheme that uses an ensemble of noisy outputs to label\nthe synthetic samples and guarantee &#949;-differential privacy (DP)\non its derived classifiers. Our proposed framework thus consists of\nan AAE-based generator and a PATE-based classifier (PATE-AAE). Evaluated\non the Google Speech Commands Dataset Version II, the proposed PATE-AAE\nimproves the average classification accuracy by +2.11% and +6.60%,\nrespectively, when compared with alternative privacy-preserving solutions,\nnamely PATE-GAN and DP-GAN, while maintaining a strong level of privacy\ntarget at &#949;=0.01 with a fixed &#948;=10<SUP>-5</SUP>.\n"
      ],
      "doi": "10.21437/Interspeech.2021-640",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement:14 Special Sessions"
    },
    "ma21b_interspeech": {
      "authors": [
        [
          "Haoxin",
          "Ma"
        ],
        [
          "Jiangyan",
          "Yi"
        ],
        [
          "Jianhua",
          "Tao"
        ],
        [
          "Ye",
          "Bai"
        ],
        [
          "Zhengkun",
          "Tian"
        ],
        [
          "Chenglong",
          "Wang"
        ]
      ],
      "title": "Continual Learning for Fake Audio Detection",
      "original": "0794",
      "page_count": 5,
      "order": 179,
      "p1": "886",
      "pn": "890",
      "abstract": [
        "Fake audio attack becomes a major threat to the speaker verification\nsystem. Although current detection approaches have achieved promising\nresults on dataset-specific scenarios, they encounter difficulties\non unseen spoofing data. Fine-tuning and retraining from scratch have\nbeen applied to incorporate new data. However, fine-tuning leads to\nperformance degradation on previous data. Retraining takes a lot of\ntime and computation resources. Besides, previous data are unavailable\ndue to privacy in some situations. To solve the above problems, this\npaper proposes detecting fake without forgetting, a continual-learning-based\nmethod, to make the model learn new spoofing attacks incrementally.\nA knowledge distillation loss is introduced to loss function to preserve\nthe memory of original model. Supposing the distribution of genuine\nvoice is consistent among different scenarios, an extra embedding similarity\nloss is used as another constraint to further do a positive sample\nalignment. Experiments are conducted on the ASVspoof2019 dataset. The\nresults show that our proposed method outperforms fine-tuning by the\nrelative reduction of average equal error rate up to 81.62%.\n"
      ],
      "doi": "10.21437/Interspeech.2021-794",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement:14 Special Sessions"
    },
    "shah21_interspeech": {
      "authors": [
        [
          "Muhammad A.",
          "Shah"
        ],
        [
          "Joseph",
          "Szurley"
        ],
        [
          "Markus",
          "Mueller"
        ],
        [
          "Athanasios",
          "Mouchtaris"
        ],
        [
          "Jasha",
          "Droppo"
        ]
      ],
      "title": "Evaluating the Vulnerability of End-to-End Automatic Speech Recognition Models to Membership Inference Attacks",
      "original": "1188",
      "page_count": 5,
      "order": 180,
      "p1": "891",
      "pn": "895",
      "abstract": [
        "Recent studies have shown that it may be possible to determine if a\nmachine learning model was trained on a given data sample, using Membership\nInference Attacks (MIA). In this paper we evaluate the vulnerability\nof state-of-the-art speech recognition models to MIA under black-box\naccess. Using models trained with standard methods and public datasets,\nwe demonstrate that without any knowledge of the target model&#8217;s\nparameters or training data a MIA can successfully infer membership\nwith precision and recall more than 60%. Furthermore, for utterances\nfrom about 39% of the speakers the precision is more than 75%, indicating\nthat training data membership can be inferred more precisely for some\nspeakers than others. While strong regularization reduces the overall\naccuracy of MIA to almost 50%, the attacker can still infer membership\nfor utterances from 25% of the speakers with high precision. These\nresults indicate that (1) speaker-level MIA success should be reported,\nalong with overall accuracy, to provide a holistic view of the model&#8217;s\nvulnerability and (2) conventional regularization is an inadequate\ndefense against MIA.We believe that the insights gleaned from this\nstudy can direct future work towards more effective defenses.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1188",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement:14 Special Sessions"
    },
    "fazel21_interspeech": {
      "authors": [
        [
          "Amin",
          "Fazel"
        ],
        [
          "Wei",
          "Yang"
        ],
        [
          "Yulan",
          "Liu"
        ],
        [
          "Roberto",
          "Barra-Chicote"
        ],
        [
          "Yixiong",
          "Meng"
        ],
        [
          "Roland",
          "Maas"
        ],
        [
          "Jasha",
          "Droppo"
        ]
      ],
      "title": "SynthASR: Unlocking Synthetic Data for Speech Recognition",
      "original": "1882",
      "page_count": 5,
      "order": 181,
      "p1": "896",
      "pn": "900",
      "abstract": [
        "End-to-end (E2E) automatic speech recognition (ASR) models have recently\ndemonstrated superior performance over the traditional hybrid ASR models.\nTraining an E2E ASR model requires a large amount of data which is\nnot only expensive but may also raise dependency on production data.\nAt the same time, synthetic speech generated by the state-of-the-art\ntext-to-speech (TTS) engines has advanced to near-human naturalness.\nIn this work, we propose to utilize synthetic speech for ASR training\n(SynthASR) in applications where data is sparse or hard to get for\nASR model training. In addition, we apply continual learning with a\nnovel multi-stage training strategy to address catastrophic forgetting,\nachieved by a mix of weighted multi-style training, data augmentation,\nencoder freezing, and parameter regularization. In our experiments\nconducted on in-house datasets for a new application of recognizing\nmedication names, training ASR RNN-T models with synthetic audio via\nthe proposed multi-stage training improved the recognition performance\non new application by more than 65% relative, without degradation on\nexisting general applications. Our observations show that SynthASR\nholds great promise in training the state-of-the-art large-scale E2E\nASR models for new applications while reducing the costs and dependency\non production data.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1882",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement:14 Special Sessions"
    },
    "muguli21_interspeech": {
      "authors": [
        [
          "Ananya",
          "Muguli"
        ],
        [
          "Lancelot",
          "Pinto"
        ],
        [
          "Nirmala",
          "R"
        ],
        [
          "Neeraj",
          "Sharma"
        ],
        [
          "Prashant",
          "Krishnan"
        ],
        [
          "Prasanta Kumar",
          "Ghosh"
        ],
        [
          "Rohit",
          "Kumar"
        ],
        [
          "Shrirama",
          "Bhat"
        ],
        [
          "Srikanth Raj",
          "Chetupalli"
        ],
        [
          "Sriram",
          "Ganapathy"
        ],
        [
          "Shreyas",
          "Ramoji"
        ],
        [
          "Viral",
          "Nanda"
        ]
      ],
      "title": "DiCOVA Challenge: Dataset, Task, and Baseline System for COVID-19 Diagnosis Using Acoustics",
      "original": "0074",
      "page_count": 5,
      "order": 182,
      "p1": "901",
      "pn": "905",
      "abstract": [
        "The DiCOVA challenge aims at accelerating research in diagnosing COVID-19\nusing acoustics (DiCOVA), a topic at the intersection of speech and\naudio processing, respiratory health diagnosis, and machine learning.\nThis challenge is an open call for researchers to analyze a dataset\nof sound recordings, collected from COVID-19 infected and non-COVID-19\nindividuals, for a two-class classification. These recordings were\ncollected via crowdsourcing from multiple countries, through a website\napplication. The challenge features two tracks, one focusing on cough\nsounds, and the other on using a collection of breath, sustained vowel\nphonation, and number counting speech recordings. In this paper, we\nintroduce the challenge and provide a detailed description of the task,\nand present a baseline system for the task.\n"
      ],
      "doi": "10.21437/Interspeech.2021-74",
      "author_area_id": "13",
      "author_area_label": " Speech, Voice, and Hearing Disorders"
    },
    "kamble21_interspeech": {
      "authors": [
        [
          "Madhu R.",
          "Kamble"
        ],
        [
          "Jose A.",
          "Gonzalez-Lopez"
        ],
        [
          "Teresa",
          "Grau"
        ],
        [
          "Juan M.",
          "Espin"
        ],
        [
          "Lorenzo",
          "Cascioli"
        ],
        [
          "Yiqing",
          "Huang"
        ],
        [
          "Alejandro",
          "Gomez-Alanis"
        ],
        [
          "Jose",
          "Patino"
        ],
        [
          "Roberto",
          "Font"
        ],
        [
          "Antonio M.",
          "Peinado"
        ],
        [
          "Angel M.",
          "Gomez"
        ],
        [
          "Nicholas",
          "Evans"
        ],
        [
          "Maria A.",
          "Zuluaga"
        ],
        [
          "Massimiliano",
          "Todisco"
        ]
      ],
      "title": "PANACEA Cough Sound-Based Diagnosis of COVID-19 for the DiCOVA 2021 Challenge",
      "original": "1062",
      "page_count": 5,
      "order": 183,
      "p1": "906",
      "pn": "910",
      "abstract": [
        "The COVID-19 pandemic has led to the saturation of public health services\nworldwide. In this scenario, the early diagnosis of SARS-Cov-2 infections\ncan help to stop or slow the spread of the virus and to manage the\ndemand upon health services. This is especially important when resources\nare also being stretched by heightened demand linked to other seasonal\ndiseases, such as the flu. In this context, the organisers of the DiCOVA\n2021 challenge have collected a database with the aim of diagnosing\nCOVID-19 through the use of coughing audio samples. This work presents\nthe details of the automatic system for COVID-19 detection from cough\nrecordings presented by team PANACEA. This team consists of researchers\nfrom two European academic institutions and one company: EURECOM (France),\nUniversity of Granada (Spain), and Biometric Vox S.L. (Spain). We developed\nseveral systems based on established signal processing and machine\nlearning methods. Our best system employs a Teager energy operator\ncepstral coefficients (TECCs) based front-end and Light gradient boosting\nmachine (LightGBM) back-end. The AUC obtained by this system on the\ntest set is 76.31% which corresponds to a 10% improvement over the\nofficial baseline.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1062",
      "author_area_id": "3",
      "author_area_label": "Paralinguistics in Speech and Language: Human and Automatic Analysis and Processing:14 Special Sessions"
    },
    "karas21_interspeech": {
      "authors": [
        [
          "Vincent",
          "Karas"
        ],
        [
          "Bj\u00f6rn W.",
          "Schuller"
        ]
      ],
      "title": "Recognising Covid-19 from Coughing Using Ensembles of SVMs and LSTMs with Handcrafted and Deep Audio Features",
      "original": "1267",
      "page_count": 5,
      "order": 184,
      "p1": "911",
      "pn": "915",
      "abstract": [
        "As the Covid-19 pandemic continues, digital health solutions can provide\nvaluable insights and assist in diagnosis and prevention. Since the\ndisease affects the respiratory system, it is hypothesised that sound\nformation is changed, and thus, an infection can be automatically recognised\nthrough audio analysis. We present an ensemble learning approach used\nin our entry to Track 1 of the DiCOVA 2021 Challenge, which aims at\nbinary classification of Covid-19 infection on a crowd-sourced dataset\nof 1 040 cough sounds. Our system is based on a combination of handcrafted\nfeatures for paralinguistics with deep feature extraction from spectrograms\nusing pre-trained CNNs. We extract features both at segment level and\nwith a sliding window approach, and process them with SVMs and LSTMs,\nrespectively. We then perform least-squares weighted late fusion of\nour classifiers. Our system surpasses the challenge baseline, with\na ROC-AUC on the test set of 78.18%.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1267",
      "author_area_id": "3",
      "author_area_label": "Paralinguistics in Speech and Language: Human and Automatic Analysis and Processing:14 Special Sessions"
    },
    "sodergren21_interspeech": {
      "authors": [
        [
          "Isabella",
          "S\u00f6dergren"
        ],
        [
          "Maryam Pahlavan",
          "Nodeh"
        ],
        [
          "Prakash Chandra",
          "Chhipa"
        ],
        [
          "Konstantina",
          "Nikolaidou"
        ],
        [
          "Gy\u00f6rgy",
          "Kov\u00e1cs"
        ]
      ],
      "title": "Detecting COVID-19 from Audio Recording of Coughs Using Random Forests and Support Vector Machines",
      "original": "2191",
      "page_count": 5,
      "order": 185,
      "p1": "916",
      "pn": "920",
      "abstract": [
        "The detection of COVID-19 is and will remain in the foreseeable future\na crucial challenge, making the development of tools for the task important.\nOne possible approach, on the confines of speech and audio processing,\nis detecting potential COVID-19 cases based on cough sounds. We propose\na simple, yet robust method based on the well-known ComParE 2016 feature\nset, and two classical machine learning models, namely Random Forests,\nand Support Vector Machines (SVMs). Furthermore, we combine the two\nmethods, by calculating the weighted average of their predictions.\nOur results in the DiCOVA challenge show that this simple approach\nleads to a robust solution while producing competitive results. Based\non the Area Under the Receiver Operating Characteristic Curve (AUC\nROC) score, both classical machine learning methods we applied markedly\noutperform the baseline provided by the challenge organisers. Moreover,\ntheir combination attains an AUC ROC score of 85.21, positioning us\nat fourth place on the leaderboard (where the second team attained\na similar, 85.43 score). Here, we would describe this system in more\ndetail, and analyse the resulting models, drawing conclusions, and\ndetermining future work directions.\n"
      ],
      "doi": "10.21437/Interspeech.2021-2191",
      "author_area_id": "3",
      "author_area_label": "Paralinguistics in Speech and Language: Human and Automatic Analysis and Processing:14 Special Sessions"
    },
    "das21_interspeech": {
      "authors": [
        [
          "Rohan Kumar",
          "Das"
        ],
        [
          "Maulik",
          "Madhavi"
        ],
        [
          "Haizhou",
          "Li"
        ]
      ],
      "title": "Diagnosis of COVID-19 Using Auditory Acoustic Cues",
      "original": "0497",
      "page_count": 5,
      "order": 186,
      "p1": "921",
      "pn": "925",
      "abstract": [
        "COVID-19 can be pre-screened based on symptoms and confirmed using\nother laboratory tests. The cough or speech from patients are also\nstudied in the recent time for detection of COVID-19 as they are indicators\nof change in anatomy and physiology of the respiratory system. Along\nthis direction, the diagnosis of COVID-19 using acoustics (DiCOVA)\nchallenge aims to promote such research by releasing publicly available\ncough/speech corpus. We participated in the Track-1 of the challenge,\nwhich deals with COVID-19 detection using cough sounds from individuals.\nIn this challenge, we use a few novel auditory acoustic cues based\non long-term transform, equivalent rectangular bandwidth spectrum and\ngammatone filterbank. We evaluate these representations using logistic\nregression, random forest and multilayer perceptron classifiers for\ndetection of COVID-19. On the blind test set, we obtain an area under\nthe ROC curve (AUC) of 83.49% for the best system submitted to the\nchallenge. It is worth noting that the submitted system ranked among\nthe top few systems on the leaderboard and outperformed the challenge\nbaseline by a large margin.\n"
      ],
      "doi": "10.21437/Interspeech.2021-497",
      "author_area_id": "3",
      "author_area_label": "Paralinguistics in Speech and Language: Human and Automatic Analysis and Processing:14 Special Sessions"
    },
    "harvill21_interspeech": {
      "authors": [
        [
          "John",
          "Harvill"
        ],
        [
          "Yash R.",
          "Wani"
        ],
        [
          "Mark",
          "Hasegawa-Johnson"
        ],
        [
          "Narendra",
          "Ahuja"
        ],
        [
          "David",
          "Beiser"
        ],
        [
          "David",
          "Chestek"
        ]
      ],
      "title": "Classification of COVID-19 from Cough Using Autoregressive Predictive Coding Pretraining and Spectral Data Augmentation",
      "original": "0799",
      "page_count": 5,
      "order": 187,
      "p1": "926",
      "pn": "930",
      "abstract": [
        "Serum and saliva-based testing methods have been crucial to slowing\nthe COVID-19 pandemic, yet have been limited by slow throughput and\ncost. A system able to determine COVID-19 status from cough sounds\nalone would provide a low cost, rapid, and remote alternative to current\ntesting methods. We explore the applicability of recent techniques\nsuch as pre-training and spectral augmentation in improving the performance\nof a neural cough classification system. We use Autoregressive Predictive\nCoding (APC) to pre-train a unidirectional LSTM on the COUGHVID dataset.\nWe then generate our final model by fine-tuning added BLSTM layers\non the DiCOVA challenge dataset. We perform various ablation studies\nto see how each component impacts performance and improves generalization\nwith a small dataset. Our final system achieves an AUC of 85.35 and\nplaces third out of 29 entries in the DiCOVA challenge.\n"
      ],
      "doi": "10.21437/Interspeech.2021-799",
      "author_area_id": "3",
      "author_area_label": "Paralinguistics in Speech and Language: Human and Automatic Analysis and Processing:14 Special Sessions"
    },
    "deshpande21_interspeech": {
      "authors": [
        [
          "Gauri",
          "Deshpande"
        ],
        [
          "Bj\u00f6rn W.",
          "Schuller"
        ]
      ],
      "title": "The DiCOVA 2021 Challenge &#8212; An Encoder-Decoder Approach for COVID-19 Recognition from Coughing Audio",
      "original": "0811",
      "page_count": 5,
      "order": 188,
      "p1": "931",
      "pn": "935",
      "abstract": [
        "This paper presents the automatic recognition of COVID-19 from coughing.\nIn particular, it describes our contribution to the DiCOVA challenge\n&#8212; Track 1, which addresses such cough sound analysis for COVID-19\ndetection. Pathologically, the effects of a COVID-19 infection on the\nrespiratory system and on breathing patterns are known. We demonstrate\nthe use of breathing patterns of the cough audio signal in identifying\nthe COVID-19 status. Breathing patterns of the cough audio signal are\nderived using a model trained with the subset of the UCL Speech Breath\nMonitoring (UCL-SBM) database. This database provides speech recordings\nof the participants while their breathing values are captured by a\nrespiratory belt. We use an encoder-decoder architecture. The encoder\nencodes the audio signal into breathing patterns and the decoder decodes\nthe COVID-19 status for the corresponding breathing patterns using\nan attention mechanism. The encoder uses a pre-trained model which\npredicts breathing patterns from the speech signal, and transfers the\nlearned patterns to cough audio signals.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  With this architecture,\nwe achieve an AUC of 64.42% on the evaluation set of Track 1.\n"
      ],
      "doi": "10.21437/Interspeech.2021-811",
      "author_area_id": "3",
      "author_area_label": "Paralinguistics in Speech and Language: Human and Automatic Analysis and Processing:14 Special Sessions"
    },
    "ritwik21_interspeech": {
      "authors": [
        [
          "Kotra Venkata Sai",
          "Ritwik"
        ],
        [
          "Shareef Babu",
          "Kalluri"
        ],
        [
          "Deepu",
          "Vijayasenan"
        ]
      ],
      "title": "COVID-19 Detection from Spectral Features on the DiCOVA Dataset",
      "original": "1031",
      "page_count": 5,
      "order": 189,
      "p1": "936",
      "pn": "940",
      "abstract": [
        "In this paper we investigate the cues of COVID-19 on sustained phonation\nof Vowel-/i/, deep breathing and number counting data of the DiCOVA\ndataset. We use an ensemble of classifiers trained on different features,\nnamely, super-vectors, formants, harmonics and MFCC features. We fit\na two-class Weighted SVM classifier to separate the COVID-19 audio\nfrom Non-COVID-19 audio. Weighted penalties help mitigate the challenge\nof class imbalance in the dataset. The results are reported on the\nstationary (breathing, Vowel-/i/) and non-stationary (counting data)\ndata using individual and combination of features on each type of utterance.\nWe find that the Formant information plays a crucial role in classification.\nThe proposed system resulted in an AUC score of 0.734 for cross validation,\nand 0.717 for evaluation dataset.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1031",
      "author_area_id": "3",
      "author_area_label": "Paralinguistics in Speech and Language: Human and Automatic Analysis and Processing:14 Special Sessions"
    },
    "mallolragolta21_interspeech": {
      "authors": [
        [
          "Adria",
          "Mallol-Ragolta"
        ],
        [
          "Helena",
          "Cuesta"
        ],
        [
          "Emilia",
          "G\u00f3mez"
        ],
        [
          "Bj\u00f6rn W.",
          "Schuller"
        ]
      ],
      "title": "Cough-Based COVID-19 Detection with Contextual Attention Convolutional Neural Networks and Gender Information",
      "original": "1052",
      "page_count": 5,
      "order": 190,
      "p1": "941",
      "pn": "945",
      "abstract": [
        "The aim of this contribution is to automatically detect COVID-19 patients\nby analysing the acoustic information embedded in coughs. COVID-19\naffects the respiratory system, and, consequently, respiratory-related\nsignals have the potential to contain salient information for the task\nat hand. We focus on analysing the spectrogram representations of cough\nsamples with the aim to investigate whether COVID-19 alters the frequency\ncontent of these signals. Furthermore, this work also assesses the\nimpact of gender in the automatic detection of COVID-19. To extract\ndeep-learnt representations of the spectrograms, we compare the performance\nof a cough-specific, and a Resnet18 pre-trained Convolutional Neural\nNetwork (CNN). Additionally, our approach explores the use of contextual\nattention, so the model can learn to highlight the most relevant deep-learnt\nfeatures extracted by the CNN. We conduct our experiments on the dataset\nreleased for the Cough Sound Track of the DICOVA 2021 Challenge. The\nbest performance on the test set is obtained using the Resnet18 pre-trained\nCNN with contextual attention, which scored an Area Under the Curve\n(AUC) of 70.91% at 80% sensitivity.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1052",
      "author_area_id": "3",
      "author_area_label": "Paralinguistics in Speech and Language: Human and Automatic Analysis and Processing:14 Special Sessions"
    },
    "bhosale21_interspeech": {
      "authors": [
        [
          "Swapnil",
          "Bhosale"
        ],
        [
          "Upasana",
          "Tiwari"
        ],
        [
          "Rupayan",
          "Chakraborty"
        ],
        [
          "Sunil Kumar",
          "Kopparapu"
        ]
      ],
      "title": "Contrastive Learning of Cough Descriptors for Automatic COVID-19 Preliminary Diagnosis",
      "original": "1249",
      "page_count": 5,
      "order": 191,
      "p1": "946",
      "pn": "950",
      "abstract": [
        "Cough sounds as a descriptor have been used for detecting various respiratory\nailments based on its intensity, duration of intermediate phase between\ntwo cough sounds, repetitions, dryness etc. However, COVID-19 diagnosis\nusing only cough sounds is challenging because of cough being a common\nsymptom among many non COVID-19 health diseases and inherent data imbalance\nwithin the available datasets. As one of the approach in this direction,\nwe explore the robustness of multi-domain representation by performing\nthe early fusion over a wide set of temporal, spectral and tempo-spectral\nhandcrafted features, followed by training a Support Vector Machine\n(SVM) classifier. In our second approach, using a contrastive loss\nfunction we learn a latent space from Mel Filter Cepstral Coefficients\n(MFCCs) where representations belonging to samples having similar cough\ncharacteristics are closer. This helps learn representations for the\nhighly varied COVID-negative class (healthy and symptomatic COVID-negative),\nby learning multiple smaller clusters. Using only the DiCOVA data,\nmulti-domain features yields an absolute improvement of 0.74% and 1.07%,\nwhereas our second approach shows an improvement of 2.09% and 3.98%,\nover the blind test and validation set, respectively, when compared\nwith challenge baseline.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1249",
      "author_area_id": "3",
      "author_area_label": "Paralinguistics in Speech and Language: Human and Automatic Analysis and Processing:14 Special Sessions"
    },
    "avila21_interspeech": {
      "authors": [
        [
          "Flavio",
          "Avila"
        ],
        [
          "Amir H.",
          "Poorjam"
        ],
        [
          "Deepak",
          "Mittal"
        ],
        [
          "Charles",
          "Dognin"
        ],
        [
          "Ananya",
          "Muguli"
        ],
        [
          "Rohit",
          "Kumar"
        ],
        [
          "Srikanth Raj",
          "Chetupalli"
        ],
        [
          "Sriram",
          "Ganapathy"
        ],
        [
          "Maneesh",
          "Singh"
        ]
      ],
      "title": "Investigating Feature Selection and Explainability for COVID-19 Diagnostics from Cough Sounds",
      "original": "2197",
      "page_count": 5,
      "order": 192,
      "p1": "951",
      "pn": "955",
      "abstract": [
        "In this paper, we propose an approach to automatically classify COVID-19\nand non-COVID-19 cough samples based on the combination of both feature\nengineering and deep learning models. In the feature engineering approach,\nwe develop a support vector machine classifier over high dimensional\n(6373D) space of acoustic features. In the deep learning-based approach,\non the other hand, we apply a convolutional neural network trained\non the log-mel spectrograms. These two methodologically diverse models\nare then combined by fusing the probability scores of the models. The\nproposed system, which ranked 9<SUP>th</SUP> on the 2021 Diagnosing\nCOVID-19 using Acoustics (DiCOVA) challenge leaderboard, obtained an\narea under the receiver operating characteristic curve (AUC) of 0.81\non the blind test data set, which is a 10.9% absolute improvement compared\nto the baseline. Moreover, we analyze the explainability of the deep\nlearning-based model when detecting COVID-19 from cough signals.\n"
      ],
      "doi": "10.21437/Interspeech.2021-2197"
    },
    "kiss21_interspeech": {
      "authors": [
        [
          "G\u00e1bor",
          "Kiss"
        ],
        [
          "D\u00e1vid",
          "Sztah\u00f3"
        ],
        [
          "Mikl\u00f3s G\u00e1briel",
          "Tulics"
        ]
      ],
      "title": "Application for Detecting Depression, Parkinson&#8217;s Disease and Dysphonic Speech",
      "original": "8001",
      "page_count": 2,
      "order": 193,
      "p1": "956",
      "pn": "957",
      "abstract": [
        "In this Show&amp;Tell presentation we demonstrate an application that\nis able to assess a voice sample according to three different voice\ndisorders: depression, Parkinson&#8217;s disease and dysphonic speech.\nAffection probability of each disorder is analyzed along with their\nseverity estimation. Although the acoustic models (support vector machine\nand regression models) are trained on Hungarian voice samples, English\nsamples can also be utilized for assessment. The results are displayed\nby as pie chart for probabilities and separate severity scores. The\ninput of the application is a read text with a fixed linguistic content.\nIt is possible to load a pre-recorded voice sample or create a live\nrecording. The developed system could evaluate a speaker&#8217;s voice\nsample, assisting medical staff.\n"
      ]
    },
    "weingartova21_interspeech": {
      "authors": [
        [
          "Lenka",
          "Weingartov\u00e1"
        ],
        [
          "Veronika",
          "Voln\u00e1"
        ],
        [
          "Ewa",
          "Balejov\u00e1"
        ]
      ],
      "title": "Beey: More Than a Speech-to-Text Editor",
      "original": "8002",
      "page_count": 2,
      "order": 194,
      "p1": "958",
      "pn": "959",
      "abstract": [
        "We present Beey, a newly developed web-based multimedia platform for\nproducing Automatic Speech Recognition (ASR) and editing its output.\nIn addition to ASR, Beey employs modules for speaker diarization and\nidentification, text formatting, automatic punctuation insertion, subtitling,\nautomatic translation, transcription of stream and more.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  The platform and its\ndevelopment are focused on user experience and fast document creation.\nOur aim is to transfer research results in the field of speech recognition\nand signal processing into practice and enable Beey&#8217;s users to\nmake their production processes faster and cheaper by minimizing human\neffort and costs.\n"
      ]
    },
    "arai21_interspeech": {
      "authors": [
        [
          "Takayuki",
          "Arai"
        ]
      ],
      "title": "Downsizing of Vocal-Tract Models to Line up Variations and Reduce Manufacturing Costs",
      "original": "8003",
      "page_count": 2,
      "order": 195,
      "p1": "960",
      "pn": "961",
      "abstract": [
        "Demonstrating vowel production with physical models of the human vocal\ntract is a part of intuitive education in speech science. The adult\nmale vocal tract was most often used as a model in the past because\nof the limited availability of physical models, but discussions on\ndifferent vocal tract sizes were ongoing. Therefore, we focused on\ndownsizing the vocal-tract models in this study, especially the straight\nmodels. We reduced the cross-sectional area function for the sliding\nthree-tube model (including the total length) to female adult and child\nsizes. Furthermore, we created fixed straight models of similar dimensions\nfor the five Japanese vowels. We found that the intelligibility of\neach model was preserved as long as the ratios of the cross-sectional\nareas were maintained even if the cross-sections were less than the\naverage human sizes. This indicates that we can reduce the cost of\nmanufacturing the models, as cost is typically a barrier when the models\nare used for pedagogical purposes.\n"
      ]
    },
    "fabien21_interspeech": {
      "authors": [
        [
          "Ma\u00ebl",
          "Fabien"
        ],
        [
          "Shantipriya",
          "Parida"
        ],
        [
          "Petr",
          "Motlicek"
        ],
        [
          "Dawei",
          "Zhu"
        ],
        [
          "Aravind",
          "Krishnan"
        ],
        [
          "Hoang H.",
          "Nguyen"
        ]
      ],
      "title": "ROXANNE Research Platform: Automate Criminal Investigations",
      "original": "8004",
      "page_count": 3,
      "order": 196,
      "p1": "962",
      "pn": "964",
      "no_doi": true,
      "abstract": [
        "Criminal investigations require manual intervention of several investigators\nand translators. However, the amount and the diversity of the data\ncollected raises many challenges, and cross-border investigations against\norganized crime can quickly impossible to handle. We developed ROXANNE\nResearch platform, an all-in-one platform which processes intercepted\nphone calls, runs state-of-the-art components such as speaker identification,\nautomatic speech recognition or named entity detection, and builds\na knowledge graph of the extracted information. Our aim for this work\nis to do a first step in the direction of an open research platform\ncombining speech, text, and video processing algorithms with criminal\nnetwork analysis for combating organized crime.\n"
      ]
    },
    "flucha21_interspeech": {
      "authors": [
        [
          "Alexandre",
          "Flucha"
        ],
        [
          "Anthony",
          "Larcher"
        ],
        [
          "Ambuj",
          "Mehrish"
        ],
        [
          "Sylvain",
          "Meignier"
        ],
        [
          "Florian",
          "Plaut"
        ],
        [
          "Nicolas",
          "Poupon"
        ],
        [
          "Yevhenii",
          "Prokopalo"
        ],
        [
          "Adrien",
          "Puertolas"
        ],
        [
          "Meysam",
          "Shamsi"
        ],
        [
          "Marie",
          "Tahon"
        ]
      ],
      "title": "The LIUM Human Active Correction Platform for Speaker Diarization",
      "original": "8005",
      "page_count": 2,
      "order": 197,
      "p1": "965",
      "pn": "966",
      "abstract": [
        "We developed a human assisted speaker diarization platform that enables\na human annotator to correct the output of any speaker diarization\nsystem by providing a graphical view of the diarization segmentation\nand clustering steps while guiding the human annotator to optimize\nthe correction process and easily improve the resulting diarization.\n"
      ]
    },
    "oh21_interspeech": {
      "authors": [
        [
          "Yoo Rhee",
          "Oh"
        ],
        [
          "Kiyoung",
          "Park"
        ]
      ],
      "title": "On-Device Streaming Transformer-Based End-to-End Speech Recognition",
      "original": "8006",
      "page_count": 2,
      "order": 198,
      "p1": "967",
      "pn": "968",
      "abstract": [
        "This work is the first attempt to run streaming Transformer-based end-to-end\nspeech recognition on embedded scale IoT systems. Recently there are\nmany researches on online Transformer-based speech recognition such\nas a contextual block encoder [1] and a block-wise synchronous beam\nsearch [2]. Based on them we designed a novel fully-streaming end-to-end\nspeech recognition method using Transformer. By efficiently utilizing\na connectionist temporal classification network to detect symbol and\nsentence boundaries, we make decoder in streaming manner. Moreover,\nby using the optimized model structure, the proposed method could be\ndeployed on a low-power edge device such as Raspberry Pi 4B with the\nhigh accuracy and the small latency. With the experiments with Librispeech\ncorpus, the methods achieved word error rates of 3.76% and 9.25% respectively.\nAlso the recognition speed is measured in two aspects; the real-time\nfactor and the user perceived latency. The system is evaluated to have\n0.84 xRT and the average latency of 0.75&#177;0.62 seconds on Raspberry\nPi 4B.\n"
      ]
    },
    "cmejla21_interspeech": {
      "authors": [
        [
          "J.",
          "\u010cmejla"
        ],
        [
          "T.",
          "Kounovsk\u00fd"
        ],
        [
          "J.",
          "Jansk\u00fd"
        ],
        [
          "Jiri",
          "Malek"
        ],
        [
          "M.",
          "Rozkovec"
        ],
        [
          "Z.",
          "Koldovsk\u00fd"
        ]
      ],
      "title": "Advanced Semi-Blind Speaker Extraction and Tracking Implemented in Experimental Device with Revolving Dense Microphone Array",
      "original": "8007",
      "page_count": 2,
      "order": 199,
      "p1": "969",
      "pn": "970",
      "abstract": [
        "We present a new device for speaker extraction and physical tracking\nand demonstrate its use in real conditions. The device is equipped\nwith a dense planar array consisting of 64 microphones mounted on a\nrotating platform. State-of-the-art blind source extraction algorithms\ncontrolled by x-vector piloting are used to extract the desired speaker,\nwhich is being tracked by the rotating microphone array. The audience\nwill experience the functionality of the device and the potential of\nthe blind algorithms to extract the speaker from multi-source noisy\nrecordings in a live situation.\n"
      ]
    },
    "ney21_interspeech": {
      "authors": [
        [
          "Hermann",
          "Ney"
        ]
      ],
      "title": "Forty Years of Speech and Language Processing: From Bayes Decision Rule to Deep Learning",
      "original": "abs5",
      "page_count": 0,
      "order": 200,
      "p1": "0",
      "pn": "",
      "abstract": [
        "When research on automatic speech recognition started, the statistical\n(or data-driven) approach was associated with methods like Bayes decision\nrule, hidden Markov models, Gaussian models and expectation-maximization\nalgorithm. Later extensions included discriminative training and hybrid\nhidden Markov models using multi-layer perceptrons and recurrent neural\nnetworks. Some of the methods originally developed for speech recognition\nturned out to be seminal for other language processing tasks like machine\ntranslation, handwritten character recognition and sign language processing.\nToday&#8217;s research on speech and language processing is dominated\nby deep learning, which is typically identified with methods like attention\nmodelling, sequence-to-sequence processing and end-to-end processing.<br\nstyle=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  In this talk, I will present my personal view of the historical\ndevelopments of research on speech and language processing. I will\nput particular emphasis on the framework of Bayes decision rule and\non the question of how the various approaches developed fit into this\nframework.\n"
      ]
    },
    "chorowski21_interspeech": {
      "authors": [
        [
          "Jan",
          "Chorowski"
        ],
        [
          "Grzegorz",
          "Ciesielski"
        ],
        [
          "Jaros\u0142aw",
          "Dzikowski"
        ],
        [
          "Adrian",
          "\u0141a\u0144cucki"
        ],
        [
          "Ricard",
          "Marxer"
        ],
        [
          "Mateusz",
          "Opala"
        ],
        [
          "Piotr",
          "Pusz"
        ],
        [
          "Pawe\u0142",
          "Rychlikowski"
        ],
        [
          "Micha\u0142",
          "Stypu\u0142kowski"
        ]
      ],
      "title": "Information Retrieval for ZeroSpeech 2021: The Submission by University of Wroclaw",
      "original": "1465",
      "page_count": 5,
      "order": 201,
      "p1": "971",
      "pn": "975",
      "abstract": [
        "We present a number of low-resource approaches to the tasks of the\nZero Resource Speech Challenge 2021. We build on the unsupervised representations\nof speech proposed by the organizers as a baseline, derived from CPC\nand clustered with the k-means algorithm. We demonstrate that simple\nmethods of refining those representations can narrow the gap, or even\nimprove upon the solutions which use a high computational budget. The\nresults lead to the conclusion that the CPC-derived representations\nare still too noisy for training language models, but stable enough\nfor simpler forms of pattern matching and retrieval.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1465",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications:14 Special Sessions"
    },
    "chorowski21b_interspeech": {
      "authors": [
        [
          "Jan",
          "Chorowski"
        ],
        [
          "Grzegorz",
          "Ciesielski"
        ],
        [
          "Jaros\u0142aw",
          "Dzikowski"
        ],
        [
          "Adrian",
          "\u0141a\u0144cucki"
        ],
        [
          "Ricard",
          "Marxer"
        ],
        [
          "Mateusz",
          "Opala"
        ],
        [
          "Piotr",
          "Pusz"
        ],
        [
          "Pawe\u0142",
          "Rychlikowski"
        ],
        [
          "Micha\u0142",
          "Stypu\u0142kowski"
        ]
      ],
      "title": "Aligned Contrastive Predictive Coding",
      "original": "1544",
      "page_count": 5,
      "order": 202,
      "p1": "976",
      "pn": "980",
      "abstract": [
        "We investigate the possibility of forcing a self-supervised model trained\nusing a contrastive predictive loss, to extract slowly varying latent\nrepresentations. Rather than producing individual predictions for each\nof the future representations, the model emits a sequence of predictions\nshorter than the sequence of upcoming representations to which they\nwill be aligned. In this way, the prediction network solves a simpler\ntask of predicting the next symbols, but not their exact timing, while\nthe encoding network is trained to produce piece-wise constant latent\ncodes. We evaluate the model on a speech coding task and demonstrate\nthat the proposed Aligned Contrastive Predictive Coding (ACPC) leads\nto higher linear phone prediction accuracy and lower ABX error rates,\nwhile being slightly faster to train due to the reduced number of prediction\nheads.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1544",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "suter21_interspeech": {
      "authors": [
        [
          "Benjamin",
          "Suter"
        ],
        [
          "Josef",
          "Novak"
        ]
      ],
      "title": "Neural Text Denormalization for Speech Transcripts",
      "original": "1814",
      "page_count": 5,
      "order": 203,
      "p1": "981",
      "pn": "985",
      "abstract": [
        "This paper presents a simple sequence-to-sequence approach to restore\nstandard orthography in raw, normalized speech transcripts, including\ninsertion of punctuation marks, prediction of capitalization, restoration\nof numeric forms, formatting of dates and times, and other, fully data-driven\nadjustments. We further describe our method to generate synthetic parallel\ntraining data, and explore suitable performance metrics, which we align\nwith human judgment through subjective MOS-like evaluations.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  Our models for English,\nRussian, and German have a word error rate of 6.36%, 4.88%, and 5.23%,\nrespectively. We focus on simplicity and reproducibility, make our\nframework available under a BSD license, and share our base models\nfor English and Russian.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1814",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "joglekar21_interspeech": {
      "authors": [
        [
          "Aditya",
          "Joglekar"
        ],
        [
          "Seyed Omid",
          "Sadjadi"
        ],
        [
          "Meena",
          "Chandra-Shekar"
        ],
        [
          "Christopher",
          "Cieri"
        ],
        [
          "John H.L.",
          "Hansen"
        ]
      ],
      "title": "Fearless Steps Challenge Phase-3 (FSC P3): Advancing SLT for Unseen Channel and Mission Data Across NASA Apollo Audio",
      "original": "2011",
      "page_count": 5,
      "order": 204,
      "p1": "986",
      "pn": "990",
      "abstract": [
        "The Fearless Steps Challenge (FSC) initiative was designed to host\na series of progressively complex tasks to promote advanced speech\nresearch across naturalistic &#8220;Big Data&#8221; corpora. The Center\nfor Robust Speech Systems at UT-Dallas in collaboration with the National\nInstitute of Standards and Technology (NIST) and Linguistic Data Consortium\n(LDC) conducted Phase-3 of the FSC series (FSC P3), with a focus on\nmotivating speech and language technology (SLT) system generalizability\nacross channel and mission diversity under the same training conditions\nas in Phase-2. The FSC P3 introduced 10 hours of previously unseen\nchannel audio from Apollo-11 and 5 hours of novel audio from Apollo-13\nto be evaluated over both previously established and newly introduced\nSLT tasks with streamlined tracks. This paper presents an overview\nof the newly introduced conversational analysis tracks, Apollo-13 data,\nand analysis of system performance for matched and mismatched challenge\nconditions. We also discuss the Phase-3 challenge results, evolution\nof system performance across the three Phases, and next steps in the\nChallenge Series.\n"
      ],
      "doi": "10.21437/Interspeech.2021-2011",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications:14 Special Sessions"
    },
    "leykum21_interspeech": {
      "authors": [
        [
          "Hannah",
          "Leykum"
        ]
      ],
      "title": "Voice Quality in Verbal Irony: Electroglottographic Analyses of Ironic Utterances in Standard Austrian German",
      "original": "0452",
      "page_count": 5,
      "order": 205,
      "p1": "991",
      "pn": "995",
      "abstract": [
        "When using verbal irony in interpersonal communication, paraverbal\ncues can reduce the risk of misunderstandings. Besides fundamental\nfrequency, intensity and duration, speakers could use voice quality\nparameters to disambiguate between ironic and literal utterances. How\nthese paraverbal cues are used to mark irony appears to be language-\nand/or culture-specific. Since the role of voice quality in ironic\nutterances has not yet been investigated in Austrian German, the present\nstudy addresses this issue. In addition to the acoustic signal, the\nvocal fold vibration is recorded via electroglottography (EGG). The\ndetailed analysis of the EGG data as well as the acoustic data, provides\ninsight into voice quality characteristics of ironic and literal realisations\nof short utterances. The analyses reveal that, in Standard Austrian\nGerman, some differences in voice quality exist between ironic and\nliteral realisations of utterances: When being ironic, speakers&#8217;\nvoices tend to be breathier, creakier or rougher. Differences are more\npronounced in the older age group and in male speakers.\n"
      ],
      "doi": "10.21437/Interspeech.2021-452",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "hutin21_interspeech": {
      "authors": [
        [
          "Mathilde",
          "Hutin"
        ],
        [
          "Yaru",
          "Wu"
        ],
        [
          "Ad\u00e8le",
          "Jatteau"
        ],
        [
          "Ioana",
          "Vasilescu"
        ],
        [
          "Lori",
          "Lamel"
        ],
        [
          "Martine",
          "Adda-Decker"
        ]
      ],
      "title": "Synchronic Fortition in Five Romance Languages? A Large Corpus-Based Study of Word-Initial Devoicing",
      "original": "0939",
      "page_count": 5,
      "order": 206,
      "p1": "996",
      "pn": "1000",
      "abstract": [
        "Devoicing is a process whereby a voiced consonant such as /bdg/ is\nrealized as voiceless [ptk]. Some theorists [1,2] propose that this\nphenomenon is an instance of fortition, or consonant strengthening,\nespecially when it occurs word-initially. This study proposes an in-depth\nexploration of voicing alternations in word-initial position in five\nRomance languages (Portuguese, Spanish, French, Italian, Romanian)\nusing large corpora (ca. 1000h of speech) and automatic alignment.\nOur results show that (i) there is initial devoicing in all languages,\nand (ii) this devoicing is conditioned by the preceding context. This\nallows the languages to be divided into those displaying (a) only phrase-initial\nfortition (Spanish), (b) phrase-initial and post-obstruent fortition\n(French, Romanian and possibly Italian) and (c) generalized word-initial\nfortition (Portuguese).\n"
      ],
      "doi": "10.21437/Interspeech.2021-939",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "kraljevski21_interspeech": {
      "authors": [
        [
          "Ivan",
          "Kraljevski"
        ],
        [
          "Maria Paola",
          "Bissiri"
        ],
        [
          "Frank",
          "Duckhorn"
        ],
        [
          "Constanze",
          "Tschoepe"
        ],
        [
          "Matthias",
          "Wolff"
        ]
      ],
      "title": "Glottal Stops in Upper Sorbian: A Data-Driven Approach",
      "original": "1101",
      "page_count": 5,
      "order": 207,
      "p1": "1001",
      "pn": "1005",
      "abstract": [
        "We present a data-driven approach for the quantitative analysis of\nglottal stops before word-initial vowels in Upper Sorbian, a West Slavic\nminority language spoken in Germany. Glottal stops are word-boundary\nmarkers and their detection can improve the performance of automatic\nspeech recognition and speech synthesis systems.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  We employed cross-language\ntransfer using an acoustic model in German to develop a forced-alignment\nmethod for the phonetic segmentation of a read-speech corpus in Upper\nSorbian. The missing phonemic units were created by combining the existing\nphoneme models. In the forced-alignment procedure, the glottal stops\nwere considered optional in front of word-initial vowels.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  To investigate the\ninfluence of speaker type (males, females, and children) and vowel\non the occurrence of glottal stops, binomial regression analysis with\na generalized linear mixed model was performed. Results show that children\nglottalize word-initial vowels more frequently than adults, and that\nglottal stop occurrences are influenced by vowel quality.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1101",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "ludusan21_interspeech": {
      "authors": [
        [
          "Bogdan",
          "Ludusan"
        ],
        [
          "Petra",
          "Wagner"
        ],
        [
          "Marcin",
          "W\u0142odarczak"
        ]
      ],
      "title": "Cue Interaction in the Perception of Prosodic Prominence: The Role of Voice Quality",
      "original": "1357",
      "page_count": 5,
      "order": 208,
      "p1": "1006",
      "pn": "1010",
      "abstract": [
        "Voice quality is an important dimension in human communication, used\nto mark a variety of phenomena in speech, including prosodic prominence.\nEven though numerous studies have shown that speakers modify their\nvoice quality parameters for marking prosodic prominence, the impact\nof these modifications on perceived prominence is less studied. Our\ninvestigation looks at the effect of a well-known measure of voice\nquality, cepstral peak prominence (CPP), on syllabic prominence ratings\ngiven by both naive and expert listeners. Employing read speech materials\nin German, we quantify the role of CPP alone and in combination with\nother acoustic cues marking prominence, namely intensity, duration\nand fundamental frequency. While CPP, by itself, had a significant\neffect on the perceived prominence for most of the listeners, when\nused in conjunction with the other cues, its impact was reduced. Moreover,\nwhen assessing the importance of each of these four cues for determining\nthe perceived prominence score we found important individual variation,\nas well as differences between naive and expert listeners.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1357",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "rodriguez21_interspeech": {
      "authors": [
        [
          "Jenifer Vega",
          "Rodriguez"
        ],
        [
          "Nathalie",
          "Vall\u00e9e"
        ]
      ],
      "title": "Glottal Sounds in Korebaju",
      "original": "1417",
      "page_count": 4,
      "order": 209,
      "p1": "1011",
      "pn": "1014",
      "abstract": [
        "Korebaju (ISO639-3: coe) [&#x0301;k&#242;r&#232;&#x3B2;&#224;h&#237;&#x335;]\nis a tonal language spoken in the foothills of the Colombian Amazon.\nThree field surveys carried out between 2017 and 2019 with six native\nspeakers (3 females and 3 males) from the same village provide a set\nof glottal productions at both phonetic and phonological levels. This\nstudy focuses on the four types of glottal units we have found in this\nlanguage: A set of vowels /a<sup>&#660;</sup>/, /e<sup>&#660;</sup>/,\n/o<sup>&#660;</sup>/, [i<sup>&#660;</sup>] and [&#616;<sup>&#660;</sup>]\nincluding 3 phonemes; the glottal stop [&#660;] and the consonant [*]\ntranscribed and described as a <i>creaky voiced glottal approximant</i>\nby [1]. Both consonants occurred in intervocalic contexts and can be\nanalyzed as a suprasegmental feature [constricted glottis] which marks\nthe syllable onset. Finally, we have also found a clear and systematic\nburst which accompanies the release of the nasal consonants [m<sup>&#660;</sup>,\nn<sup>&#660;</sup>, &#x272;<sup>&#660;</sup>]. No change was found\nin the EGG signal for these consonants suggesting an abrupt release\nof the aeroacoustic pressure.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1417",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "chanclu21_interspeech": {
      "authors": [
        [
          "Ana\u00efs",
          "Chanclu"
        ],
        [
          "Imen Ben",
          "Amor"
        ],
        [
          "C\u00e9dric",
          "Gendrot"
        ],
        [
          "Emmanuel",
          "Ferragne"
        ],
        [
          "Jean-Fran\u00e7ois",
          "Bonastre"
        ]
      ],
      "title": "Automatic Classification of Phonation Types in Spontaneous Speech: Towards a New Workflow for the Characterization of Speakers&#8217; Voice Quality",
      "original": "1765",
      "page_count": 4,
      "order": 210,
      "p1": "1015",
      "pn": "1018",
      "abstract": [
        "Voice quality is known to be an important factor for the characterization\nof a speaker&#8217;s voice, both in terms of physiological features\n(mainly laryngeal and supralaryngeal) and of the speaker&#8217;s habits\n(sociolinguistic factors). This paper is devoted to one of the main\ncomponents of voice quality: phonation type. It proposes neural representations\nof speech followed by a cascade of two binary neural network-based\nclassifiers, one dedicated to the detection of modal and nonmodal vowels,\nand one for the classification of nonmodal vowels into creaky and breathy\ntypes. This approach is evaluated on the spontaneous part of the PTSVOX\ndatabase, following an expert manual labelling of the data by phonation\ntype. The results of the proposed classifiers reaches on average 85%accuracy\nat the frame-level and up to 95% accuracy at the segment-level. Further\nresearch is planned to generalize the classifiers on more contexts\nand speakers, and thus pave the way for a new workflow aimed at characterizing\nphonation types.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1765",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "son21_interspeech": {
      "authors": [
        [
          "Rob J.J.H. van",
          "Son"
        ]
      ],
      "title": "Measuring Voice Quality Parameters After Speaker Pseudonymization",
      "original": "0026",
      "page_count": 5,
      "order": 211,
      "p1": "1019",
      "pn": "1023",
      "abstract": [
        "Collecting and sharing speech resources is important for progress in\nspeech science and technology. Often, speech resources cannot be shared\nbecause of concerns over the privacy of the speakers, e.g., minors\nor people with medical conditions. Current technologies for pseudonymizing\nspeech have only been tested on &#8220;standard&#8221; speech for which\npseudonymization methods are evaluated on speaker identification risk,\nintelligibility, and naturalness. For many applications, the important\ncharacteristics are para-linguistic aspects of the speech, e.g., voice\nquality, emotion, or disease progression. Little information is available\nabout the extent to which speaker pseudonymization methods preserve\nsuch paralinguistic information. The current study investigates how\nwell voice quality parameters are preserved by an example speech pseudonymization\napplication. Correlations prove to be high between original and pseudonymized\nrecordings for seven acoustic parameters and a composite measure of\ndysphonia, the <i>AVQI</i>. Root mean square errors for these parameters\nwere reasonably small. A linear mixed effect model shows a link between\nthe difference between source and target speaker and the size of the\nabsolute difference in the <i>AVQI</i>. It is argued that new measures\nof quality are needed for pseudonymized non-standard speech before\nwide-spread application of pseudonymized speech can be considered in\nresearch and clinical practise.\n"
      ],
      "doi": "10.21437/Interspeech.2021-26",
      "author_area_id": "3",
      "author_area_label": "Paralinguistics in Speech and Language: Human and Automatic Analysis and Processing"
    },
    "steinert21_interspeech": {
      "authors": [
        [
          "Lars",
          "Steinert"
        ],
        [
          "Felix",
          "Putze"
        ],
        [
          "Dennis",
          "K\u00fcster"
        ],
        [
          "Tanja",
          "Schultz"
        ]
      ],
      "title": "Audio-Visual Recognition of Emotional Engagement of People with Dementia",
      "original": "0567",
      "page_count": 5,
      "order": 212,
      "p1": "1024",
      "pn": "1028",
      "abstract": [
        "Dementia places an immeasurable burden on affected individuals and\ncaregivers. In addition to general cognitive decline, dementia has\na negative impact on communication. Technical activation systems are\nthus in high demand, as cognitive activation may help to moderate the\ndecline. However, effective activation requires sustained engagement\n&#8212; which, in turn, first needs to be reliably recognized. In this\nstudy, we examine emotional engagement recognition for People with\nDementia (PwD) using non-intrusive biosignals resulting from speech\ncommunication and facial expressions. PwD suffering from mild to severe\ndementia used a tablet-based activation system over multiple sessions.\nWe demonstrate that they retained their ability to verbally express\nemotional engagement even at severe stages of the disease. For recognition\nof emotional engagement, we propose an architecture of Bidirectional\nLong-Short-Term-Memory Networks that combines video information with\nup to three speech-based feature sets (eGeMAPS, ComParE&#8217;13, DeepSpectrum).\nUsing data of 24 PwD, we show that adding speech improves recognition\nperformance significantly compared to a video-only model. Interestingly,\ndisease-progression did not appear to have a substantial impact on\nrecognition performance in this sample. We further discuss the opportunities\nand challenges of detecting emotional engagement from speech in PwD.\n"
      ],
      "doi": "10.21437/Interspeech.2021-567",
      "author_area_id": "3",
      "author_area_label": "Paralinguistics in Speech and Language: Human and Automatic Analysis and Processing"
    },
    "hecker21_interspeech": {
      "authors": [
        [
          "Pascal",
          "Hecker"
        ],
        [
          "Florian B.",
          "Pokorny"
        ],
        [
          "Katrin D.",
          "Bartl-Pokorny"
        ],
        [
          "Uwe",
          "Reichel"
        ],
        [
          "Zhao",
          "Ren"
        ],
        [
          "Simone",
          "Hantke"
        ],
        [
          "Florian",
          "Eyben"
        ],
        [
          "Dagmar M.",
          "Schuller"
        ],
        [
          "Bert",
          "Arnrich"
        ],
        [
          "Bj\u00f6rn W.",
          "Schuller"
        ]
      ],
      "title": "Speaking Corona? Human and Machine Recognition of COVID-19 from Voice",
      "original": "1771",
      "page_count": 5,
      "order": 213,
      "p1": "1029",
      "pn": "1033",
      "abstract": [
        "With the COVID-19 pandemic, several research teams have reported successful\nadvances in automated recognition of COVID-19 by voice. Resulting voice-based\nscreening tools for COVID-19 could support large-scale testing efforts.\nWhile capabilities of machines on this task are progressing, we approach\nthe so far unexplored aspect whether human raters can distinguish COVID-19\npositive and negative tested speakers from voice samples, and compare\ntheir performance to a machine learning baseline. To account for the\nchallenging symptom similarity between COVID-19 and other respiratory\ndiseases, we use a carefully balanced dataset of voice samples, in\nwhich COVID-19 positive and negative tested speakers are matched by\ntheir symptoms alongside COVID-19 negative speakers without symptoms.\nBoth human raters and the machine struggle to reliably identify COVID-19\npositive speakers in our dataset. These results indicate that particular\nattention should be paid to the distribution of symptoms across all\nspeakers of a dataset when assessing the capabilities of existing systems.\nThe identification of acoustic aspects of COVID-19-related symptom\nmanifestations might be the key for a reliable voice-based COVID-19\ndetection in the future by both trained human raters and machine learning\nmodels.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1771",
      "author_area_id": "3",
      "author_area_label": "Paralinguistics in Speech and Language: Human and Automatic Analysis and Processing"
    },
    "nguyen21b_interspeech": {
      "authors": [
        [
          "Huyen",
          "Nguyen"
        ],
        [
          "Ralph",
          "Vente"
        ],
        [
          "David",
          "Lupea"
        ],
        [
          "Sarah Ita",
          "Levitan"
        ],
        [
          "Julia",
          "Hirschberg"
        ]
      ],
      "title": "Acoustic-Prosodic, Lexical and Demographic Cues to Persuasiveness in Competitive Debate Speeches",
      "original": "1891",
      "page_count": 5,
      "order": 214,
      "p1": "1034",
      "pn": "1038",
      "abstract": [
        "We analyze the acoustic-prosodic and lexical correlates of persuasiveness,\ntaking into account speaker, judge and debate characteristics in a\nnovel data set of 674 audio profiles, transcripts, evaluation scores\nand demographic data from professional debate tournament speeches.\nBy conducting 10-fold cross validation experiments with linear, LASSO\nand random forest regression, we predict how different feature combinations\ncontribute toward speech scores (i.e. persuasiveness) between men and\nwomen. Overall, lexical features, i.e. word complexity, nouns, fillers\nand hedges, are the most predictive features of speech evaluation scores;\nin addition to the gender composition of judge panels and opponents.\nIn a combined lexical and demographic feature model, we achieve an\nR<SUP>2</SUP> of 0.40. Different lexical features predict speech evaluation\nscores for male vs. female speakers, and further investigation is necessary\nto understand whether differential evaluation standards applied across\ngenders. This work contributes a larger-scale debate data set in a\ndemocratically relevant, competitive format with high external relevance\nto persuasive speech education in other competitive settings.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1891",
      "author_area_id": "3",
      "author_area_label": "Paralinguistics in Speech and Language: Human and Automatic Analysis and Processing"
    },
    "borgstrom21_interspeech": {
      "authors": [
        [
          "Bengt J.",
          "Borgstr\u00f6m"
        ]
      ],
      "title": "Unsupervised Bayesian Adaptation of PLDA for Speaker Verification",
      "original": "0033",
      "page_count": 5,
      "order": 215,
      "p1": "1039",
      "pn": "1043",
      "abstract": [
        "This paper presents a Bayesian framework for unsupervised domain adaptation\nof Probabilistic Linear Discriminant Analysis (PLDA). By interpreting\nclass labels as latent random variables, Variational Bayes (VB) is\nused to derive a maximum <i>a posterior</i> (MAP) solution of the adapted\nPLDA model when labels are missing, referred to as VB-MAP. The VB solution\niteratively infers class labels and updates PLDA hyperparameters, offering\na systematic framework for dealing with unlabeled data. While presented\nas a general solution, this paper includes experimental results for\ndomain adaptation in speaker verification. VB-MAP estimation is applied\nto the 2016 and 2018 NIST Speaker Recognition Evaluations (SREs), both\nof which included small and unlabeled in-domain data sets, and is shown\nto provide performance improvements over a variety of state-of-the-art\ndomain adaptation methods. Additionally, VB-MAP estimation is used\nto train a fully unsupervised PLDA model, suffering only minor performance\ndegradation relative to conventional supervised training, offering\npromise for training PLDA models when no relevant labeled data exists.\n"
      ],
      "doi": "10.21437/Interspeech.2021-33",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "wang21i_interspeech": {
      "authors": [
        [
          "Weiqing",
          "Wang"
        ],
        [
          "Danwei",
          "Cai"
        ],
        [
          "Jin",
          "Wang"
        ],
        [
          "Qingjian",
          "Lin"
        ],
        [
          "Xuyang",
          "Wang"
        ],
        [
          "Mi",
          "Hong"
        ],
        [
          "Ming",
          "Li"
        ]
      ],
      "title": "The DKU-Duke-Lenovo System Description for the Fearless Steps Challenge Phase III",
      "original": "0235",
      "page_count": 5,
      "order": 216,
      "p1": "1044",
      "pn": "1048",
      "abstract": [
        "This paper describes the systems developed by the DKU-Duke-Lenovo team\nfor the Fearless Steps Challenge Phase III. For the speech activity\ndetection (SAD) task, we employ the U-Net-based model which has not\nbeen used for SAD before, observing a DCF of 1.915% on the eval set.\nFor the speaker identification (SID) task, we adopt the ResNet-SE and\nECAPA-TDNN model, and we obtain a Top-5 accuracy of 86.21%. For the\nspeaker diarization (SD) task, we employ several different clustering\nmethods. Besides, domain adaptation, system fusion, and Target-Speaker\nVoice Activity Detection (TS-VAD) significantly improve the SD performance.\nWe obtain a DER of 12.32% on track 2, and the major contribution is\nfrom our ResNet-based TS-VAD model. We finally achieve a first-place\nranking for SD and SID and a second-place for SAD in the challenge.\n"
      ],
      "doi": "10.21437/Interspeech.2021-235",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification:14 Special Sessions"
    },
    "chen21f_interspeech": {
      "authors": [
        [
          "Yafeng",
          "Chen"
        ],
        [
          "Wu",
          "Guo"
        ],
        [
          "Bin",
          "Gu"
        ]
      ],
      "title": "Improved Meta-Learning Training for Speaker Verification",
      "original": "0405",
      "page_count": 5,
      "order": 217,
      "p1": "1049",
      "pn": "1053",
      "abstract": [
        "Meta-learning (ML) has recently become a research hotspot in speaker\nverification (SV). We introduce two methods to improve the meta-learning\ntraining for SV in this paper. For the first method, a backbone embedding\nnetwork is first jointly trained with the conventional cross entropy\nloss and prototypical networks (PN) loss. Then, inspired by speaker\nadaptive training in speech recognition, additional transformation\ncoefficients are trained with only the PN loss. The transformation\ncoefficients are used to modify the original backbone embedding network\nin the x-vector extraction process. Furthermore, the random erasing\n(RE) data augmentation technique is applied to all support samples\nin each episode to construct positive pairs, and a contrastive loss\nbetween the augmented and the original support samples is added to\nthe objective in model training. Experiments are carried out on the\nSpeaker in the Wild (SITW) and VOiCES databases. Both of the methods\ncan obtain consistent improvements over existing meta-learning training\nframeworks. By combining these two methods, we can observe further\nimprovements on these two databases.\n"
      ],
      "doi": "10.21437/Interspeech.2021-405",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "wang21j_interspeech": {
      "authors": [
        [
          "Dan",
          "Wang"
        ],
        [
          "Yuanjie",
          "Dong"
        ],
        [
          "Yaxing",
          "Li"
        ],
        [
          "Yunfei",
          "Zi"
        ],
        [
          "Zhihui",
          "Zhang"
        ],
        [
          "Xiaoqi",
          "Li"
        ],
        [
          "Shengwu",
          "Xiong"
        ]
      ],
      "title": "Variational Information Bottleneck Based Regularization for Speaker Recognition",
      "original": "0482",
      "page_count": 5,
      "order": 218,
      "p1": "1054",
      "pn": "1058",
      "abstract": [
        "Speaker recognition (SR) is inevitably affected by noise in real-life\nscenarios, resulting in decreased recognition accuracy. In this paper,\nwe introduce a novel regularization method, variable information bottleneck\n(VIB), in speaker recognition to extract robust speaker embeddings.\nVIB prompts the neural network to ignore as much speaker-identity irrelevant\ninformation as possible. We also propose a more effective network,\nVovNet with an ultra-lightweight subspace attention module (ULSAM),\nas a feature extractor. ULSAM infers different attention maps for each\nfeature map subspace, enabling efficient learning of cross-channel\ninformation along with multi-scale and multi-frequency feature representation.\nThe experimental results demonstrate that our proposed framework outperforms\nthe ResNet-based baseline by 11.4% in terms of equal error rate (EER).\nThe VIB regularization method gives a further performance boost with\nan 18.9% EER decrease.\n"
      ],
      "doi": "10.21437/Interspeech.2021-482",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "brummer21_interspeech": {
      "authors": [
        [
          "Niko",
          "Br\u00fcmmer"
        ],
        [
          "Luciana",
          "Ferrer"
        ],
        [
          "Albert",
          "Swart"
        ]
      ],
      "title": "Out of a Hundred Trials, How Many Errors Does Your Speaker Verifier Make?",
      "original": "0541",
      "page_count": 5,
      "order": 219,
      "p1": "1059",
      "pn": "1063",
      "abstract": [
        "Out of a hundred trials, how many errors does your speaker verifier\nmake? For the user this is an important, practical question, but researchers\nand vendors typically sidestep it and supply instead the conditional\nerror-rates that are given by the ROC/DET curve. We posit that the\nuser&#8217;s question is answered by the Bayes error-rate. We present\na tutorial to show how to compute the error-rate that results when\nmaking Bayes decisions with calibrated likelihood ratios, supplied\nby the verifier, and an hypothesis prior, supplied by the user. For\nperfect calibration, the Bayes error-rate is upper bounded by min(EER,P,1-P),\nwhere EER is the equal-error-rate and P, 1-P are the prior probabilities\nof the competing hypotheses. The EER represents the accuracy of the\nverifier, while min(P,1-P) represents the hardness of the classification\nproblem. We further show how the Bayes error-rate can be computed also\nfor non-perfect calibration and how to generalize from error-rate to\nexpected cost. We offer some criticism of decisions made by direct\nscore thresholding. Finally, we demonstrate by analyzing error-rates\nof the recently published DCA-PLDA speaker verifier.\n"
      ],
      "doi": "10.21437/Interspeech.2021-541",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "chojnacka21_interspeech": {
      "authors": [
        [
          "Roza",
          "Chojnacka"
        ],
        [
          "Jason",
          "Pelecanos"
        ],
        [
          "Quan",
          "Wang"
        ],
        [
          "Ignacio Lopez",
          "Moreno"
        ]
      ],
      "title": "SpeakerStew: Scaling to Many Languages with a Triaged Multilingual Text-Dependent and Text-Independent Speaker Verification System",
      "original": "0646",
      "page_count": 5,
      "order": 220,
      "p1": "1064",
      "pn": "1068",
      "abstract": [
        "In this paper, we describe <i>SpeakerStew</i> &#8212; a hybrid system\nto perform speaker verification on 46 languages. Two core ideas were\nexplored in this system: (1) Pooling training data of different languages\ntogether for multilingual generalization and reducing development cycles;\n(2) A novel triage mechanism between text-dependent and text-independent\nmodels to reduce runtime cost and expected latency. To the best of\nour knowledge, this is the first study of speaker verification systems\nat the scale of 46 languages. The problem is framed from the perspective\nof using a smart speaker device with interactions consisting of a wake-up\nkeyword (text-dependent) followed by a speech query (text-independent).\nExperimental evidence suggests that training on multiple languages\ncan generalize to unseen varieties while maintaining performance on\nseen varieties. We also found that it can reduce computational requirements\nfor training models by an order of magnitude. Furthermore, during model\ninference on English data, we observe that leveraging a triage framework\ncan reduce the number of calls to the more computationally expensive\ntext-independent system by 73% (and reduce latency by 59%) while maintaining\nan EER no worse than the text-independent setup.\n"
      ],
      "doi": "10.21437/Interspeech.2021-646"
    },
    "wang21k_interspeech": {
      "authors": [
        [
          "Zhiming",
          "Wang"
        ],
        [
          "Furong",
          "Xu"
        ],
        [
          "Kaisheng",
          "Yao"
        ],
        [
          "Yuan",
          "Cheng"
        ],
        [
          "Tao",
          "Xiong"
        ],
        [
          "Huijia",
          "Zhu"
        ]
      ],
      "title": "AntVoice Neural Speaker Embedding System for FFSVC 2020",
      "original": "0966",
      "page_count": 5,
      "order": 221,
      "p1": "1069",
      "pn": "1073",
      "abstract": [
        "This paper presents a comprehensive description of the AntVoice system\nfor the first two tracks of far-field speaker verification from single\nmicrophone array in FFSVC 2020 [1]. The system is based on neural speaker\nembeddings from deep neural network-based encoder networks. These encoder\nnetworks for acoustic modeling include 2D convolutional residual-like\nnetworks that are shown to be effective on the tasks. Specifically,\nwe apply the Squeeze-and-Excitation residual network (SE-ResNet) [2]\nto model cross-channel inter-dependency information. On short utterances,\nwe observe that SE-ResNet outperforms alternative methods in the text-dependent\nverification task. The system adopts a joint loss function that combines\nthe additive cosine margin softmax loss [3] with the equidistant triplet-based\nloss[4]. This loss function results in performance gains with more\ndiscriminative speaker embeddings from enhanced intra-class similarity\nand increased inter-class variances. We also apply speech enhancement\nand data augmentation to improve data quality and diversity. Even without\nusing model ensembles, the proposed system significantly outperforms\nthe baselines [1] in both tracks of the speaker verification challenge.\nWith fusion of several encoder neural networks, this system is able\nto achieve further performance improvements consistently. In the end,\nthe AntVoice system achieves the third place in the text-dependent\nverification task.\n"
      ],
      "doi": "10.21437/Interspeech.2021-966",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "li21b_interspeech": {
      "authors": [
        [
          "Jianchen",
          "Li"
        ],
        [
          "Jiqing",
          "Han"
        ],
        [
          "Hongwei",
          "Song"
        ]
      ],
      "title": "Gradient Regularization for Noise-Robust Speaker Verification",
      "original": "1216",
      "page_count": 5,
      "order": 222,
      "p1": "1074",
      "pn": "1078",
      "abstract": [
        "Noise robustness is a challenge for speaker recognition systems. To\nsolve this problem, one of the most common approaches is to joint-train\na model by using both clean and noisy utterances. However, the gradients\ncalculated on noisy utterances generally contain speaker-irrelevant\nnoisy components, resulting in overfitting for the seen noisy data\nand poor generalization for the unseen noisy environments. To alleviate\nthis problem, we propose the gradient regularization method to reduce\nthe speaker-irrelevant noisy components by aligning the gradients among\nthe noisy utterances and their clean counterparts. Specifically, the\ngradients on noisy utterances are forced to follow the directions of\nthe gradients calculated on their clean counterparts, and the gradients\nacross different types of noisy utterances are also aligned to point\nin similar directions. Since the noise-related components of the gradients\ncan be reduced by the above alignment, the speaker model can be prevented\nfrom encoding irrelevant noisy information. To achieve the gradient\nregularization goals, a novel sequential inner training strategy is\nalso proposed. Experiments on the VoxCeleb1 dataset indicate that our\nmethod achieves the best performance in seen and unseen noisy environments.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1216",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "kataria21_interspeech": {
      "authors": [
        [
          "Saurabh",
          "Kataria"
        ],
        [
          "Jes\u00fas",
          "Villalba"
        ],
        [
          "Piotr",
          "\u017belasko"
        ],
        [
          "Laureano",
          "Moro-Vel\u00e1zquez"
        ],
        [
          "Najim",
          "Dehak"
        ]
      ],
      "title": "Deep Feature CycleGANs: Speaker Identity Preserving Non-Parallel Microphone-Telephone Domain Adaptation for Speaker Verification",
      "original": "1502",
      "page_count": 5,
      "order": 223,
      "p1": "1079",
      "pn": "1083",
      "abstract": [
        "With the increase in the availability of speech from varied domains,\nit is imperative to use such out-of-domain data to improve existing\nspeech systems. Domain adaptation is a prominent pre-processing approach\nfor this. We investigate it to adapt microphone speech to the telephone\ndomain. Specifically, we explore CycleGAN-based unpaired translation\nof microphone data to improve the x-vector/speaker embedding network\nfor Telephony Speaker Verification. We first demonstrate the efficacy\nof this on real challenging data and then, to improve further, we modify\nthe CycleGAN formulation to make the adaptation <i>task-specific</i>.\nWe modify CycleGAN&#8217;s identity loss, cycle-consistency loss, and\nadversarial loss to operate in the <i>deep feature</i> space. <i>Deep\nfeatures</i> of a signal are extracted from an auxiliary (speaker embedding)\nnetwork and, hence, preserves speaker identity. Our 3D convolution-based\nDeep Feature Discriminators (DFD) show relative improvements of 5&#8211;10%\nin terms of equal error rate. To dive deeper, we study a challenging\nscenario of pooling (adapted) microphone and telephone data with data\naugmentations and telephone codecs. Finally, we highlight the sensitivity\nof CycleGAN hyper-parameters and introduce a parameter called <i>probability\nof adaptation</i>.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1502",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "pu21_interspeech": {
      "authors": [
        [
          "Jie",
          "Pu"
        ],
        [
          "Yuguang",
          "Yang"
        ],
        [
          "Ruirui",
          "Li"
        ],
        [
          "Oguz",
          "Elibol"
        ],
        [
          "Jasha",
          "Droppo"
        ]
      ],
      "title": "Scaling Effect of Self-Supervised Speech Models",
      "original": "1935",
      "page_count": 5,
      "order": 224,
      "p1": "1084",
      "pn": "1088",
      "abstract": [
        "The success of modern deep learning systems is built on two cornerstones,\nmassive amount of annotated training data and advanced computational\ninfrastructure to support large-scale computation. In recent years,\nthe model size of state-of-the-art deep learning systems has rapidly\nincreased and sometimes reached to billions of parameters. Herein we\ntake a close look into this phenomenon and present an empirical study\non the scaling effect of model size for self-supervised speech models.\nIn particular, we investigate the quantitative relationship between\nthe model size and the loss/accuracy performance on speech tasks. First,\nthe power-law scaling property between the number of parameters and\nthe L<SUB>1</SUB> self-supervised loss is verified for speech models.\nThen the advantage of large speech models in learning effective speech\nrepresentations is demonstrated in two downstream tasks: i) speaker\nrecognition and ii) phoneme classification. Moreover, it has been shown\nthat the model size of self-supervised speech networks is able to compensate\nthe lack of annotation when there is insufficient training data.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1935",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "wu21c_interspeech": {
      "authors": [
        [
          "Yibo",
          "Wu"
        ],
        [
          "Longbiao",
          "Wang"
        ],
        [
          "Kong Aik",
          "Lee"
        ],
        [
          "Meng",
          "Liu"
        ],
        [
          "Jianwu",
          "Dang"
        ]
      ],
      "title": "Joint Feature Enhancement and Speaker Recognition with Multi-Objective Task-Oriented Network",
      "original": "1978",
      "page_count": 5,
      "order": 225,
      "p1": "1089",
      "pn": "1093",
      "abstract": [
        "Recently, increasing attention has been paid to the joint training\nof upstream and downstream tasks, and to address the challenge of how\nto synchronize various loss functions in a multi-objective scenario.\nIn this paper, to address the competing gradient directions between\nthe speaker classification loss and the feature enhancement loss, we\npropose an asynchronous subregion optimization approach for the joint\ntraining of feature enhancement and speaker embedding neural networks.\nFor the asynchronous subregion optimization, the squeeze and excitation\n(SE) method is introduced in the enhancement network to adaptively\nselect important channels for speaker embedding. Furthermore, channel-wise\nfeature concatenation is applied between the input feature and the\nenhanced feature to address the distortion of speaker information that\nis caused by enhancement loss. By using the proposed joint training\nnetwork with asynchronous subregion optimization and channel-wise feature\nconcatenation, we obtained relative gains of 11.95% and 6.43% in equal\nerror rate on a noisy version of Voxceleb1 and VOiCES corpus, respectively.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1978",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "zhang21g_interspeech": {
      "authors": [
        [
          "Li",
          "Zhang"
        ],
        [
          "Qing",
          "Wang"
        ],
        [
          "Kong Aik",
          "Lee"
        ],
        [
          "Lei",
          "Xie"
        ],
        [
          "Haizhou",
          "Li"
        ]
      ],
      "title": "Multi-Level Transfer Learning from Near-Field to Far-Field Speaker Verification",
      "original": "1980",
      "page_count": 5,
      "order": 226,
      "p1": "1094",
      "pn": "1098",
      "abstract": [
        "In far-field speaker verification, the performance of speaker embeddings\nis susceptible to degradation when there is a mismatch between the\nconditions of enrollment and test speech. To solve this problem, we\npropose the feature-level and instance-level transfer learning in the\nteacher-student framework to learn a domain-invariant embedding space.\nFor the feature-level knowledge transfer, we develop the contrastive\nloss to transfer knowledge from teacher model to student model, which\nnot only decrease the intra-class distance, but also enlarge the inter-class\ndistance. Moreover, we propose the instance-level pairwise distance\ntransfer method to force the student model to preserve pairwise instances\ndistance from the well optimized embedding space of the teacher model.\nOn FFSVC 2020 evaluation set, our EER on Full-eval trials is relatively\nreduced by 13.9% compared with the fusion system result on Partial-eval\ntrials of Task2. On Task1, compared with the winner&#8217;s DenseNet\nresult on Partial-eval trials, our minDCF on Full-eval trials is relatively\nreduced by 6.3%. On Task3, the EER and minDCF of our proposed method\non Full-eval trials are very close to the result of the fusion system\non Partial-eval trials. Our results also outperform other competitive\ndomain adaptation methods.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1980",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "patino21_interspeech": {
      "authors": [
        [
          "Jose",
          "Patino"
        ],
        [
          "Natalia",
          "Tomashenko"
        ],
        [
          "Massimiliano",
          "Todisco"
        ],
        [
          "Andreas",
          "Nautsch"
        ],
        [
          "Nicholas",
          "Evans"
        ]
      ],
      "title": "Speaker Anonymisation Using the McAdams Coefficient",
      "original": "1070",
      "page_count": 5,
      "order": 227,
      "p1": "1099",
      "pn": "1103",
      "abstract": [
        "Anonymisation has the goal of manipulating speech signals in order\nto degrade the reliability of automatic approaches to speaker recognition,\nwhile preserving other aspects of speech, such as those relating to\nintelligibility and naturalness. This paper reports an approach to\nanonymisation that, unlike other current approaches, requires no training\ndata, is based upon well-known signal processing techniques and is\nboth efficient and effective. The proposed solution uses the McAdams\ncoefficient to transform the spectral envelope of speech signals. Results\nderived using common VoicePrivacy 2020 databases and protocols show\nthat random, optimised transformations can outperform competing solutions\nin terms of anonymisation while causing only modest, additional degradations\nto intelligibility, even in the case of a semi-informed privacy adversary.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1070",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "luo21_interspeech": {
      "authors": [
        [
          "Yiyu",
          "Luo"
        ],
        [
          "Jing",
          "Wang"
        ],
        [
          "Liang",
          "Xu"
        ],
        [
          "Lidong",
          "Yang"
        ]
      ],
      "title": "Multi-Stream Gated and Pyramidal Temporal Convolutional Neural Networks for Audio-Visual Speech Separation in Multi-Talker Environments",
      "original": "0366",
      "page_count": 5,
      "order": 228,
      "p1": "1104",
      "pn": "1108",
      "abstract": [
        "Speech separation is the task of extracting target speech from noisy\nmixture. In applications like video telephones or video conferencing,\nlip movements of the target speaker are accessible, which can be leveraged\nfor speech separation. This paper proposes a time-domain audio-visual\nspeech separation model under multi-talker environments. The model\nreceives audio-visual inputs including noisy mixture and speaker lip\nembedding, and reconstructs clean speech waveform for the target speaker.\nOnce trained, the model can be flexibly applied to unknown number of\ntotal speakers. This paper introduces and investigates the multi-stream\ngating mechanism and pyramidal convolution in temporal convolutional\nneural networks for audio-visual speech separation task. Speaker- and\nnoise-independent multi-talker separation experiments are conducted\non GRID benchmark dataset. The experimental results demonstrate the\nproposed method achieves 3.9 dB and 1.0 dB SI-SNRi improvement when\ncompared with audio-only and audio-visual baselines respectively, showing\neffectiveness of the proposed method.\n"
      ],
      "doi": "10.21437/Interspeech.2021-366",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "wang21l_interspeech": {
      "authors": [
        [
          "Helin",
          "Wang"
        ],
        [
          "Bo",
          "Wu"
        ],
        [
          "Lianwu",
          "Chen"
        ],
        [
          "Meng",
          "Yu"
        ],
        [
          "Jianwei",
          "Yu"
        ],
        [
          "Yong",
          "Xu"
        ],
        [
          "Shi-Xiong",
          "Zhang"
        ],
        [
          "Chao",
          "Weng"
        ],
        [
          "Dan",
          "Su"
        ],
        [
          "Dong",
          "Yu"
        ]
      ],
      "title": "TeCANet: Temporal-Contextual Attention Network for Environment-Aware Speech Dereverberation",
      "original": "0481",
      "page_count": 5,
      "order": 229,
      "p1": "1109",
      "pn": "1113",
      "abstract": [
        "In this paper, we exploit the effective way to leverage contextual\ninformation to improve the speech dereverberation performance in real-world\nreverberant environments. We propose a temporal-contextual attention\napproach on the deep neural network (DNN) for environment-aware speech\ndereverberation, which can adaptively attend to the contextual information.\nMore specifically, a FullBand based Temporal Attention approach (FTA)\nis proposed, which models the correlations between the fullband information\nof the context frames. In addition, considering the difference between\nthe attenuation of high frequency bands and low frequency bands (high\nfrequency bands attenuate faster than low frequency bands) in the room\nimpulse response (RIR), we also propose a SubBand based Temporal Attention\napproach (STA). In order to guide the network to be more aware of the\nreverberant environments, we jointly optimize the dereverberation network\nand the reverberation time (RT60) estimator in a multi-task manner.\nOur experimental results indicate that the proposed method outperforms\nour previously proposed reverberation-time-aware DNN and the learned\nattention weights are fully physical consistent. We also report a preliminary\nyet promising dereverberation and recognition experiment on real test\ndata.\n"
      ],
      "doi": "10.21437/Interspeech.2021-481",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "gu21_interspeech": {
      "authors": [
        [
          "Jianjun",
          "Gu"
        ],
        [
          "Longbiao",
          "Cheng"
        ],
        [
          "Xingwei",
          "Sun"
        ],
        [
          "Junfeng",
          "Li"
        ],
        [
          "Yonghong",
          "Yan"
        ]
      ],
      "title": "Residual Echo and Noise Cancellation with Feature Attention Module and Multi-Domain Loss Function",
      "original": "0538",
      "page_count": 5,
      "order": 230,
      "p1": "1114",
      "pn": "1118",
      "abstract": [
        "For real-time acoustic echo cancellation in noisy environments, the\nclassical linear adaptive filters (LAFs) can only remove the linear\ncomponents of acoustic echo. To further attenuate the non-linear echo\ncomponents and background noise, this paper proposes a deep learning-based\nresidual echo and noise cancellation (RENC) model, where multiple inputs\nare utilized and weighted by a feature attention module. More specifically,\ninput features extracted from the far-end reference and the echo estimated\nby the LAF are scaled with time-frequency attention weights, depending\non their correlation with the residual interference in LAF&#8217;s\noutput. Moreover, a scale-independent mean square error and perceptual\nloss function are further suggested for training the RENC model. Experimental\nresults validate the efficacy of the proposed feature attention module\nand multi-domain loss function, which achieve an 8.4%, 14.9% and 29.5%\nimprovement in perceptual evaluation of speech quality (PESQ), scale-invariant\nsignal-to-distortion ratio (SI-SDR) and echo return loss enhancement\n(ERLE), respectively.\n"
      ],
      "doi": "10.21437/Interspeech.2021-538",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "li21c_interspeech": {
      "authors": [
        [
          "Xiyun",
          "Li"
        ],
        [
          "Yong",
          "Xu"
        ],
        [
          "Meng",
          "Yu"
        ],
        [
          "Shi-Xiong",
          "Zhang"
        ],
        [
          "Jiaming",
          "Xu"
        ],
        [
          "Bo",
          "Xu"
        ],
        [
          "Dong",
          "Yu"
        ]
      ],
      "title": "MIMO Self-Attentive RNN Beamformer for Multi-Speaker Speech Separation",
      "original": "0570",
      "page_count": 5,
      "order": 231,
      "p1": "1119",
      "pn": "1123",
      "abstract": [
        "Recently, our proposed recurrent neural network (RNN) based all deep\nlearning minimum variance distortionless response (ADL-MVDR) beamformer\nmethod yielded superior performance over the conventional MVDR by replacing\nthe matrix inversion and eigenvalue decomposition with two RNNs. In\nthis work, we present a self-attentive RNN beamformer to further improve\nour previous RNN-based beamformer by leveraging on the powerful modeling\ncapability of self-attention. Temporal-spatial self-attention module\nis proposed to better learn the beamforming weights from the speech\nand noise spatial covariance matrices. The temporal self-attention\nmodule could help RNN to learn global statistics of covariance matrices.\nThe spatial self-attention module is designed to attend on the cross-channel\ncorrelation in the covariance matrices. Furthermore, a multi-channel\ninput with multi-speaker directional features and multi-speaker speech\nseparation outputs (MIMO) model is developed to improve the inference\nefficiency. The evaluations demonstrate that our proposed MIMO self-attentive\nRNN beamformer improves both the automatic speech recognition (ASR)\naccuracy and the perceptual estimation of speech quality (PESQ) against\nprior arts.\n"
      ],
      "doi": "10.21437/Interspeech.2021-570",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "giri21_interspeech": {
      "authors": [
        [
          "Ritwik",
          "Giri"
        ],
        [
          "Shrikant",
          "Venkataramani"
        ],
        [
          "Jean-Marc",
          "Valin"
        ],
        [
          "Umut",
          "Isik"
        ],
        [
          "Arvindh",
          "Krishnaswamy"
        ]
      ],
      "title": "Personalized PercepNet: Real-Time, Low-Complexity Target Voice Separation and Enhancement",
      "original": "0694",
      "page_count": 5,
      "order": 232,
      "p1": "1124",
      "pn": "1128",
      "abstract": [
        "The presence of multiple talkers in the surrounding environment poses\na difficult challenge for real-time speech communication systems considering\nthe constraints on network size and complexity. In this paper, we present\nPersonalized PercepNet, a real-time speech enhancement model that separates\na target speaker from a noisy multi-talker mixture without compromising\non complexity of the recently proposed PercepNet. To enable speaker-dependent\nspeech enhancement, we first show how we can train a perceptually motivated\nspeaker embedder network to produce a representative embedding vector\nfor the given speaker. Personalized PercepNet uses the target speaker\nembedding as additional information to pick out and enhance only the\ntarget speaker while suppressing all other competing sounds. Our experiments\nshow that the proposed model significantly outperforms PercepNet and\nother baselines, both in terms of objective speech enhancement metrics\nand human opinion scores.\n"
      ],
      "doi": "10.21437/Interspeech.2021-694",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "yemini21_interspeech": {
      "authors": [
        [
          "Yochai",
          "Yemini"
        ],
        [
          "Ethan",
          "Fetaya"
        ],
        [
          "Haggai",
          "Maron"
        ],
        [
          "Sharon",
          "Gannot"
        ]
      ],
      "title": "Scene-Agnostic Multi-Microphone Speech Dereverberation",
      "original": "0889",
      "page_count": 5,
      "order": 233,
      "p1": "1129",
      "pn": "1133",
      "abstract": [
        "Neural networks (NNs) have been widely applied in speech processing\ntasks, and, in particular, those employing microphone arrays. Nevertheless,\nmost existing NN architectures can only deal with fixed and position-specific\nmicrophone arrays. In this paper, we present an NN architecture that\ncan cope with microphone arrays whose number and positions of the microphones\nare unknown, and demonstrate its applicability in the speech dereverberation\ntask. To this end, our approach harnesses recent advances in deep learning\non set-structured data to design an architecture that enhances the\nreverberant log-spectrum. We use noisy and noiseless versions of a\nsimulated reverberant dataset to test the proposed architecture. Our\nexperiments on the noisy data show that the proposed scene-agnostic\nsetup outperforms a powerful scene-aware framework, sometimes even\nwith fewer microphones. With the noiseless dataset we show that, in\nmost cases, our method outperforms the position-aware network as well\nas the state-of-the-art weighted linear prediction error (WPE) algorithm.\n"
      ],
      "doi": "10.21437/Interspeech.2021-889",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "tanaka21_interspeech": {
      "authors": [
        [
          "Keitaro",
          "Tanaka"
        ],
        [
          "Ryosuke",
          "Sawata"
        ],
        [
          "Shusuke",
          "Takahashi"
        ]
      ],
      "title": "Manifold-Aware Deep Clustering: Maximizing Angles Between Embedding Vectors Based on Regular Simplex",
      "original": "1029",
      "page_count": 5,
      "order": 234,
      "p1": "1134",
      "pn": "1138",
      "abstract": [
        "This paper presents a new deep clustering (DC) method called manifold-aware\nDC (M-DC) that can enhance hyperspace utilization more effectively\nthan the original DC. The original DC has a limitation in that a pair\nof two speakers has to be embedded having an orthogonal relationship\ndue to its use of the one-hot vector-based loss function, while our\nmethod derives a unique loss function aimed at maximizing the target\nangle in the hyperspace based on the nature of a regular simplex. Our\nproposed loss imposes a higher penalty than the original DC when the\nspeaker is assigned incorrectly. The change from DC to M-DC can be\neasily achieved by rewriting just one term in the loss function of\nDC, without any other modifications to the network architecture or\nmodel parameters. As such, our method has high practicability because\nit does not affect the original inference part. The experimental results\nshow that the proposed method improves the performances of the original\nDC and its expansion method.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1029",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "zhang21h_interspeech": {
      "authors": [
        [
          "Hao",
          "Zhang"
        ],
        [
          "DeLiang",
          "Wang"
        ]
      ],
      "title": "A Deep Learning Approach to Multi-Channel and Multi-Microphone Acoustic Echo Cancellation",
      "original": "1508",
      "page_count": 5,
      "order": 235,
      "p1": "1139",
      "pn": "1143",
      "abstract": [
        "Building on deep learning based acoustic echo cancellation (AEC) in\nthe single-loudspeaker (single-channel) and single-microphone setup,\nthis paper investigates multi-channel (multi-loudspeaker) AEC (MCAEC)\nand multi-microphone AEC (MMAEC). A convolutional recurrent network\n(CRN) is trained to predict the near-end speech from microphone signals\nwith far-end signals used as additional information. We find that the\ndeep learning based MCAEC approach avoids the non-uniqueness problem\nin traditional MCAEC algorithms. For the AEC setup with multiple microphones,\nrather than employing AEC for each microphone, we propose to train\na single network to achieve echo removal for all microphones. Combining\ndeep learning based AEC with supervised beamforming further improves\nthe system performance. Experimental results show the effectiveness\nof deep learning approach to MCAEC and MMAEC. Furthermore, deep learning\nbased methods are capable of removing echo and noise simultaneously\nand work well in the presence of nonlinear distortions.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1508",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "na21_interspeech": {
      "authors": [
        [
          "Yueyue",
          "Na"
        ],
        [
          "Ziteng",
          "Wang"
        ],
        [
          "Zhang",
          "Liu"
        ],
        [
          "Biao",
          "Tian"
        ],
        [
          "Qiang",
          "Fu"
        ]
      ],
      "title": "Joint Online Multichannel Acoustic Echo Cancellation, Speech Dereverberation and Source Separation",
      "original": "1950",
      "page_count": 5,
      "order": 236,
      "p1": "1144",
      "pn": "1148",
      "abstract": [
        "This paper presents a joint source separation algorithm that simultaneously\nreduces acoustic echo, reverberation and interfering sources. Target\nspeeches are separated from the mixture by maximizing independence\nwith respect to the other sources. It is shown that the separation\nprocess can be decomposed into cascading sub-processes that separately\nrelate to acoustic echo cancellation, speech dereverberation and source\nseparation, all of which are solved using the auxiliary function based\nindependent component/vector analysis techniques, and their solving\norders are exchangeable. The cascaded solution not only leads to lower\ncomputational complexity but also better separation performance than\nthe vanilla joint algorithm.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1950",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "sato21_interspeech": {
      "authors": [
        [
          "Hiroshi",
          "Sato"
        ],
        [
          "Tsubasa",
          "Ochiai"
        ],
        [
          "Marc",
          "Delcroix"
        ],
        [
          "Keisuke",
          "Kinoshita"
        ],
        [
          "Takafumi",
          "Moriya"
        ],
        [
          "Naoyuki",
          "Kamo"
        ]
      ],
      "title": "Should We Always Separate?: Switching Between Enhanced and Observed Signals for Overlapping Speech Recognition",
      "original": "2253",
      "page_count": 5,
      "order": 237,
      "p1": "1149",
      "pn": "1153",
      "abstract": [
        "Although recent advances in deep learning technology improved automatic\nspeech recognition (ASR), it remains difficult to recognize speech\nwhen it overlaps other people&#8217;s voices. Speech separation or\nextraction is often used as a front-end to ASR to handle such overlapping\nspeech. However, deep neural network-based speech enhancement can generate\n&#8216;processing artifacts&#8217; as a side effect of the enhancement,\nwhich degrades ASR performance. For example, it is well known that\nsingle-channel noise reduction for non-speech noise (non-overlapping\nspeech) often does not improve ASR. Likewise, the processing artifacts\nmay also be detrimental to ASR in some conditions when processing overlapping\nspeech with a separation/extraction method, although it is usually\nbelieved that separation/extraction improves ASR. In order to answer\nthe question &#8216;Do we always have to separate/extract speech from\nmixtures?&#8217;, we analyze ASR performance on observed and enhanced\nspeech at various noise and interference conditions, and show that\nspeech enhancement degrades ASR under some conditions even for overlapping\nspeech. Based on these findings, we propose a simple switching algorithm\nbetween observed and enhanced speech based on the estimated signal-to-interference\nratio and signal-to-noise ratio. We demonstrated experimentally that\nsuch a simple switching mechanism can improve recognition performance\nwhen processing artifacts are detrimental to ASR.\n"
      ],
      "doi": "10.21437/Interspeech.2021-2253",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "udupa21_interspeech": {
      "authors": [
        [
          "Sathvik",
          "Udupa"
        ],
        [
          "Anwesha",
          "Roy"
        ],
        [
          "Abhayjeet",
          "Singh"
        ],
        [
          "Aravind",
          "Illa"
        ],
        [
          "Prasanta Kumar",
          "Ghosh"
        ]
      ],
      "title": "Estimating Articulatory Movements in Speech Production with Transformer Networks",
      "original": "1375",
      "page_count": 5,
      "order": 238,
      "p1": "1154",
      "pn": "1158",
      "abstract": [
        "We estimate articulatory movements in speech production from different\nmodalities - acoustics and phonemes. Acoustic-to-articulatory inversion\n(AAI) is a sequence-to-sequence task. On the other hand, phoneme to\narticulatory (PTA) motion estimation faces a key challenge in reliably\naligning the text and the articulatory movements. To address this challenge,\nwe explore the use of a transformer architecture &#8212; FastSpeech,\nwith explicit duration modelling to learn hard alignments between the\nphonemes and articulatory movements. We also train a transformer model\non AAI. We use correlation coefficient (CC) and root mean squared error\n(rMSE) to assess the estimation performance in comparison to existing\nmethods on both tasks. We observe 154%, 11.8% &amp; 4.8% relative improvement\nin CC with subject-dependent, pooled and fine-tuning strategies, respectively,\nfor PTA estimation. Additionally, on the AAI task, we obtain 1.5%,\n3% and 3.1% relative gain in CC on the same setups compared to the\nstate-of-the-art baseline. We further present the computational benefits\nof having transformer architecture as representation blocks.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1375",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "yang21b_interspeech": {
      "authors": [
        [
          "Dongchao",
          "Yang"
        ],
        [
          "Helin",
          "Wang"
        ],
        [
          "Yuexian",
          "Zou"
        ]
      ],
      "title": "Unsupervised Multi-Target Domain Adaptation for Acoustic Scene Classification",
      "original": "0300",
      "page_count": 5,
      "order": 239,
      "p1": "1159",
      "pn": "1163",
      "abstract": [
        "It is well known that the mismatch between training (source) and test\n(target) data distribution will significantly decrease the performance\nof acoustic scene classification (ASC) systems. To address this issue,\ndomain adaptation (DA) is one solution and many unsupervised DA methods\nhave been proposed. These methods focus on a scenario of single source\ndomain to single target domain. However, we will face such problem\nthat test data comes from multiple target domains. This problem can\nbe addressed by producing one model per target domain, but this solution\nis too costly. In this paper, we propose a novel unsupervised multi-target\ndomain adaption (MTDA) method for ASC, which can adapt to multiple\ntarget domains simultaneously and make use of the underlying relation\namong multiple domains. Specifically, our approach combines traditional\nadversarial adaptation with two novel discriminator tasks that learns\na common subspace shared by all domains. Furthermore, we propose to\ndivide the target domain into the easy-to-adapt and hard-to-adapt domain,\nwhich enables the system to pay more attention to hard-to-adapt domain\nin training. The experimental results on the DCASE 2020 Task 1-A dataset\nand the DCASE 2019 Task 1-B dataset show that our proposed method significantly\noutperforms the previous unsupervised DA methods.\n"
      ],
      "doi": "10.21437/Interspeech.2021-300",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "jaramillo21_interspeech": {
      "authors": [
        [
          "Alfredo Esquivel",
          "Jaramillo"
        ],
        [
          "Jesper Kj\u00e6r",
          "Nielsen"
        ],
        [
          "Mads Gr\u00e6sb\u00f8ll",
          "Christensen"
        ]
      ],
      "title": "Speech Decomposition Based on a Hybrid Speech Model and Optimal Segmentation",
      "original": "0047",
      "page_count": 5,
      "order": 240,
      "p1": "1164",
      "pn": "1168",
      "abstract": [
        "In a hybrid speech model, both voiced and unvoiced components can coexist\nin a segment. Often, the voiced speech is regarded as the deterministic\ncomponent, and the unvoiced speech and additive noise are the stochastic\ncomponents. Typically, the speech signal is considered stationary within\nfixed segments of 20&#8211;40 ms, but the degree of stationarity varies\nover time. For decomposing noisy speech into its voiced and unvoiced\ncomponents, a fixed segmentation may be too crude, and we here propose\nto adapt the segment length according to the signal local characteristics.\nThe segmentation relies on parameter estimates of a hybrid speech model\nand the maximum a posteriori (MAP) and log-likelihood criteria as rules\nfor model selection among the possible segment lengths, for voiced\nand unvoiced speech, respectively. Given the optimal segmentation markers\nand the estimated statistics, both components are estimated using linear\nfiltering. A codebook-based approach differentiates between unvoiced\nspeech and noise. A better extraction of the components is possible\nby taking into account the adaptive segmentation, compared to a fixed\none. Also, a lower distortion for voiced speech and higher segSNR for\nboth components is possible, as compared to other decomposition methods.\n"
      ],
      "doi": "10.21437/Interspeech.2021-47",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "luo21b_interspeech": {
      "authors": [
        [
          "Jian",
          "Luo"
        ],
        [
          "Jianzong",
          "Wang"
        ],
        [
          "Ning",
          "Cheng"
        ],
        [
          "Jing",
          "Xiao"
        ]
      ],
      "title": "Dropout Regularization for Self-Supervised Learning of Transformer Encoder Speech Representation",
      "original": "1066",
      "page_count": 5,
      "order": 241,
      "p1": "1169",
      "pn": "1173",
      "abstract": [
        "Predicting the altered acoustic frames is an effective way of self-supervised\nlearning for speech representation. However, it is challenging to prevent\nthe pretrained model from overfitting. In this paper, we proposed to\nintroduce two dropout regularization methods into the pretraining of\ntransformer encoder: (1) attention dropout, (2) layer dropout. Both\nof the two dropout methods encourage the model to utilize global speech\ninformation, and avoid just copying local spectrum features when reconstructing\nthe masked frames. We evaluated the proposed methods on phoneme classification\nand speaker recognition tasks. The experiments demonstrate that our\ndropout approaches achieve competitive results, and improve the performance\nof classification accuracy on downstream tasks.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1066",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "yarra21_interspeech": {
      "authors": [
        [
          "Chiranjeevi",
          "Yarra"
        ],
        [
          "Prasanta Kumar",
          "Ghosh"
        ]
      ],
      "title": "Noise Robust Pitch Stylization Using Minimum Mean Absolute Error Criterion",
      "original": "1307",
      "page_count": 5,
      "order": 242,
      "p1": "1174",
      "pn": "1178",
      "abstract": [
        "We propose a pitch stylization technique in the presence of pitch halving\nand doubling errors. The technique uses an optimization criterion based\non a minimum mean absolute error to make the stylization robust to\nsuch pitch estimation errors, particularly under noisy conditions.\nWe obtain segments for the stylization automatically using dynamic\nprogramming. Experiments are performed at the frame level and the syllable\nlevel. At the frame level, the closeness of stylized pitch is analyzed\nwith the ground truth pitch, which is obtained using a laryngograph\nsignal, considering root mean square error (RMSE) measure. At the syllable\nlevel, the effectiveness of perceptual relevant embeddings in the stylized\npitch is analyzed by estimating syllabic tones and comparing those\nwith manual tone markings using the Levenshtein distance measure. The\nproposed approach performs better than a minimum mean squared error\ncriterion based pitch stylization scheme at the frame level and a knowledge-based\ntone estimation scheme at the syllable level under clean and 20dB,\n10dB and 0dB SNR conditions with five noises and four pitch estimation\ntechniques. Among all the combinations of SNR, noise and pitch estimation\ntechniques, the highest absolute RMSE and mean distance improvements\nare found to be 6.49Hz and 0.23, respectively.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1307",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "huang21b_interspeech": {
      "authors": [
        [
          "Yu-Lin",
          "Huang"
        ],
        [
          "Bo-Hao",
          "Su"
        ],
        [
          "Y.-W. Peter",
          "Hong"
        ],
        [
          "Chi-Chun",
          "Lee"
        ]
      ],
      "title": "An Attribute-Aligned Strategy for Learning Speech Representation",
      "original": "1341",
      "page_count": 5,
      "order": 243,
      "p1": "1179",
      "pn": "1183",
      "abstract": [
        "Advancement in speech technology has brought convenience to our life.\nHowever, the concern is on the rise as speech signal contains multiple\npersonal attributes, which would lead to either sensitive information\nleakage or bias toward decision. In this work, we propose an attribute-aligned\nlearning strategy to derive speech representation that can flexibly\naddress these issues by attribute-selection mechanism. Specifically,\nwe propose a layered-representation variational autoencoder (LR-VAE),\nwhich factorizes speech representation into attribute-sensitive nodes,\nto derive an identity-free representation for speech emotion recognition\n(SER), and an emotionless representation for speaker verification (SV).\nOur proposed method achieves competitive performances on identity-free\nSER and a better performance on emotionless SV, comparing to the current\nstate-of-the-art method of using adversarial learning applied on a\nlarge emotion corpora, the MSP-Podcast. Also, our proposed learning\nstrategy reduces the model and training process needed to achieve multiple\nprivacy-preserving tasks.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1341",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "shahrebabaki21_interspeech": {
      "authors": [
        [
          "Abdolreza Sabzi",
          "Shahrebabaki"
        ],
        [
          "Sabato Marco",
          "Siniscalchi"
        ],
        [
          "Torbj\u00f8rn",
          "Svendsen"
        ]
      ],
      "title": "Raw Speech-to-Articulatory Inversion by Temporal Filtering and Decimation",
      "original": "1429",
      "page_count": 5,
      "order": 244,
      "p1": "1184",
      "pn": "1188",
      "abstract": [
        "We propose a novel sequence-to-sequence acoustic-to-articulatory inversion\n(AAI) neural architecture in the temporal waveform domain. In contrast\nto traditional AAI approaches that leverage hand-crafted short-time\nspectral features obtained from the windowed signal, such as LSFs,\nor MFCCs, our solution directly process the input speech signal in\nthe time domain, avoiding any intermediate signal transformation, using\na cascade of 1D convolutional filters in a deep model. The time-rate\nsynchronization between raw speech signal and the articulatory signal\nis obtained through a decimation process that acts upon each convolution\nstep. Decimation in time thus avoids degradation phenomena observed\nin the conventional AAI procedure, caused by the need of framing the\nspeech signal to produce a feature sequence that perfectly matches\nthe articulatory data rate. Experimental evidence on the &#8220;Haskins\nProduction Rate Comparison&#8221; corpus demonstrates the effectiveness\nof the proposed solution, which outperforms a conventional state-of-the-art\nAAI system leveraging MFCCs with an 20% relative improvement in terms\nof Pearson correlation coefficient (PCC) in mismatched speaking rate\nconditions. Finally, the proposed approach attains the same accuracy\nas the conventional AAI solution in the typical matched speaking rate\ncondition.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1429",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "lilley21_interspeech": {
      "authors": [
        [
          "Jason",
          "Lilley"
        ],
        [
          "H. Timothy",
          "Bunnell"
        ]
      ],
      "title": "Unsupervised Training of a DNN-Based Formant Tracker",
      "original": "1690",
      "page_count": 5,
      "order": 245,
      "p1": "1189",
      "pn": "1193",
      "abstract": [
        "Phonetic analysis often requires reliable estimation of formants, but\nestimates provided by popular programs can be unreliable. Recently,\nDissen et al. [1] described DNN-based formant trackers that produced\nmore accurate frequency estimates than several others, but require\nmanually-corrected formant data for training. Here we describe a novel\nunsupervised training method for corpus-based DNN formant parameter\nestimation and tracking with accuracy similar to [1]. Frame-wise spectral\nenvelopes serve as the input. The output is estimates of the frequencies\nand bandwidths plus amplitude adjustments for a prespecified number\nof poles and zeros, hereafter referred to as &#8220;formant parameters.&#8221;\nA custom loss measure based on the difference between the input envelope\nand one generated from the estimated formant parameters is calculated\nand back-propagated through the network to establish the gradients\nwith respect to the formant parameters. The approach is similar to\nthat of autoencoders, in that the model is trained to reproduce its\ninput in order to discover latent features, in this case, the formant\nparameters. Our results demonstrate that a reliable formant tracker\ncan be constructed for a speech corpus without the need for hand-corrected\ntraining data.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1690",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "yang21c_interspeech": {
      "authors": [
        [
          "Shu-wen",
          "Yang"
        ],
        [
          "Po-Han",
          "Chi"
        ],
        [
          "Yung-Sung",
          "Chuang"
        ],
        [
          "Cheng-I Jeff",
          "Lai"
        ],
        [
          "Kushal",
          "Lakhotia"
        ],
        [
          "Yist Y.",
          "Lin"
        ],
        [
          "Andy T.",
          "Liu"
        ],
        [
          "Jiatong",
          "Shi"
        ],
        [
          "Xuankai",
          "Chang"
        ],
        [
          "Guan-Ting",
          "Lin"
        ],
        [
          "Tzu-Hsien",
          "Huang"
        ],
        [
          "Wei-Cheng",
          "Tseng"
        ],
        [
          "Ko-tik",
          "Lee"
        ],
        [
          "Da-Rong",
          "Liu"
        ],
        [
          "Zili",
          "Huang"
        ],
        [
          "Shuyan",
          "Dong"
        ],
        [
          "Shang-Wen",
          "Li"
        ],
        [
          "Shinji",
          "Watanabe"
        ],
        [
          "Abdelrahman",
          "Mohamed"
        ],
        [
          "Hung-yi",
          "Lee"
        ]
      ],
      "title": "SUPERB: Speech Processing Universal PERformance Benchmark",
      "original": "1775",
      "page_count": 5,
      "order": 246,
      "p1": "1194",
      "pn": "1198",
      "abstract": [
        "Self-supervised learning (SSL) has proven vital for advancing research\nin natural language processing (NLP) and computer vision (CV). The\nparadigm pretrains a <i>shared model</i> on large volumes of unlabeled\ndata and achieves state-of-the-art (SOTA) <i>for various tasks with\nminimal adaptation</i>. However, the speech processing community lacks\na similar setup to systematically explore the paradigm. To bridge this\ngap, we introduce Speech processing Universal PERformance Benchmark\n(SUPERB). SUPERB is a leaderboard to benchmark the performance of a\nshared model across a wide range of speech processing tasks with minimal\narchitecture changes and labeled data. Among multiple usages of the\nshared model, we especially focus on extracting the representation\nlearned from SSL for its preferable re-usability. We present a simple\nframework to solve SUPERB tasks by learning task-specialized <i>lightweight</i>\nprediction heads on top of the <i>frozen shared</i> model. Our results\ndemonstrate that the framework is promising as SSL representations\nshow competitive generalizability and accessibility across SUPERB tasks.\nWe release SUPERB as a challenge with a leaderboard and a benchmark\ntoolkit to fuel the research in representation learning and general\nspeech processing.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1775",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "zhang21i_interspeech": {
      "authors": [
        [
          "Cong",
          "Zhang"
        ],
        [
          "Jian",
          "Zhu"
        ]
      ],
      "title": "Synchronising Speech Segments with Musical Beats in Mandarin and English Singing",
      "original": "1841",
      "page_count": 5,
      "order": 247,
      "p1": "1199",
      "pn": "1203",
      "abstract": [
        "Generating synthesised singing voice with models trained on speech\ndata has many advantages due to the models&#8217; flexibility and controllability.\nHowever, since the information about the temporal relationship between\nsegments and beats are lacking in speech training data, the synthesised\nsinging may sound off-beat at times. Therefore, the availability of\nthe information on the temporal relationship between speech segments\nand music beats is crucial. The current study investigated the segment-beat\nsynchronisation in singing data, with hypotheses formed based on the\nlinguistics theories of P-centre and sonority hierarchy. A Mandarin\ncorpus and an English corpus of professional singing data were manually\nannotated and analysed. The results showed that the presence of musical\nbeats was more dependent on segment duration than sonority. However,\nthe sonority hierarchy and the P-centre theory were highly related\nto the location of beats. Mandarin and English demonstrated cross-linguistic\nvariations despite exhibiting common patterns.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1841",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "peplinski21_interspeech": {
      "authors": [
        [
          "Jacob",
          "Peplinski"
        ],
        [
          "Joel",
          "Shor"
        ],
        [
          "Sachin",
          "Joglekar"
        ],
        [
          "Jake",
          "Garrison"
        ],
        [
          "Shwetak",
          "Patel"
        ]
      ],
      "title": "FRILL: A Non-Semantic Speech Embedding for Mobile Devices",
      "original": "2070",
      "page_count": 5,
      "order": 248,
      "p1": "1204",
      "pn": "1208",
      "abstract": [
        "Learned speech representations can drastically improve performance\non tasks with limited labeled data. However, due to their size and\ncomplexity, learned representations have limited utility in mobile\nsettings where run-time performance can be a significant bottleneck.\nIn this work, we propose a class of lightweight non-semantic speech\nembedding models that run efficiently on mobile devices based on the\nrecently proposed TRILL speech embedding. We combine novel architectural\nmodifications with existing speed-up techniques to create embedding\nmodels that are fast enough to run in real-time on a mobile device\nand exhibit minimal performance degradation on a benchmark of non-semantic\nspeech tasks. One such model (FRILL) is 32&#215; faster on a Pixel\n1 smartphone and 40% the size of TRILL, with an average decrease in\naccuracy of only 2%. To our knowledge, FRILL is the highest-quality\nnon-semantic embedding designed for use on mobile devices. Furthermore,\nwe demonstrate that these representations are useful for mobile health\ntasks such as non-speech human sounds detection and face-masked speech\ndetection. Our models and code are publicly available.\n"
      ],
      "doi": "10.21437/Interspeech.2021-2070",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "mori21_interspeech": {
      "authors": [
        [
          "Hiroki",
          "Mori"
        ]
      ],
      "title": "Pitch Contour Separation from Overlapping Speech",
      "original": "2164",
      "page_count": 5,
      "order": 249,
      "p1": "1209",
      "pn": "1213",
      "abstract": [
        "In everyday conversation, speakers&#8217; utterances often overlap.\nFor conversation corpora that are recorded in diverse environments,\nresults of pitch extraction in the overlapping parts may be incorrect.\nThe goal of this study is to establish the technique of separating\neach speaker&#8217;s pitch contour from an overlapping speech in conversation.\nThe proposed method estimates statistically most plausible f<SUB>o</SUB>\ncontour from the spectrogram of overlapping speech, along with the\ninformation of the speaker to extract. Visual inspection of the separation\nresults showed that the proposed model was able to extract accurate\nf<SUB>o</SUB> contours from overlapping speeches of specified speakers.\nBy applying this method, voicing decision errors and gross pitch errors\nwere reduced by 63% compared to simple pitch extraction for overlapping\nspeech.\n"
      ],
      "doi": "10.21437/Interspeech.2021-2164",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "kumar21_interspeech": {
      "authors": [
        [
          "Anurag",
          "Kumar"
        ],
        [
          "Yun",
          "Wang"
        ],
        [
          "Vamsi Krishna",
          "Ithapu"
        ],
        [
          "Christian",
          "Fuegen"
        ]
      ],
      "title": "Do Sound Event Representations Generalize to Other Audio Tasks? A Case Study in Audio Transfer Learning",
      "original": "0347",
      "page_count": 5,
      "order": 250,
      "p1": "1214",
      "pn": "1218",
      "abstract": [
        "Transfer learning is critical for efficient information transfer across\nmultiple related learning problems. A simple, yet effective transfer\nlearning approach utilizes deep neural networks trained on a large-scale\ntask for feature extraction. Such representations are then used to\nlearn related downstream tasks. In this paper, we investigate transfer\nlearning capacity of audio representations obtained from neural networks\ntrained on a large-scale sound event detection dataset. We build and\nevaluate these representations across a wide range of other audio tasks,\nvia a simple linear classifier transfer mechanism. We show that such\nsimple linear transfer is already powerful enough to achieve high performance\non the downstream tasks. We also provide insights into the attributes\nof sound event representations that enable such efficient information\ntransfer.\n"
      ],
      "doi": "10.21437/Interspeech.2021-347",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "peng21b_interspeech": {
      "authors": [
        [
          "Baolin",
          "Peng"
        ],
        [
          "Chenguang",
          "Zhu"
        ],
        [
          "Michael",
          "Zeng"
        ],
        [
          "Jianfeng",
          "Gao"
        ]
      ],
      "title": "Data Augmentation for Spoken Language Understanding via Pretrained Language Models",
      "original": "0117",
      "page_count": 5,
      "order": 251,
      "p1": "1219",
      "pn": "1223",
      "abstract": [
        "The training of spoken language understanding (SLU) models often faces\nthe problem of data scarcity. In this paper, we put forward a data\naugmentation method using pretrained language models to boost the variability\nand accuracy of generated utterances. Furthermore, we investigate and\npropose solutions to two previously overlooked semi-supervised learning\nscenarios of data scarcity in SLU: i) <i>Rich-in-Ontology</i>: ontology\ninformation with numerous valid dialogue acts is given; ii) <i>Rich-in-Utterance</i>:\na large number of unlabelled utterances are available. Empirical results\nshow that our method can produce synthetic training data that boosts\nthe performance of language understanding models in various scenarios.\n"
      ],
      "doi": "10.21437/Interspeech.2021-117",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "radfar21_interspeech": {
      "authors": [
        [
          "Martin",
          "Radfar"
        ],
        [
          "Athanasios",
          "Mouchtaris"
        ],
        [
          "Siegfried",
          "Kunzmann"
        ],
        [
          "Ariya",
          "Rastrow"
        ]
      ],
      "title": "FANS: Fusing ASR and NLU for On-Device SLU",
      "original": "0793",
      "page_count": 5,
      "order": 252,
      "p1": "1224",
      "pn": "1228",
      "abstract": [
        "Spoken language understanding (SLU) systems translate voice input commands\nto semantics which are encoded as an intent and pairs of slot tags\nand values. Most current SLU systems deploy a cascade of two neural\nmodels where the first one maps the input audio to a transcript (ASR)\nand the second predicts the intent and slots from the transcript (NLU).\nIn this paper, we introduce FANS, a new end-to-end SLU model that fuses\nan ASR audio encoder to a multi-task NLU decoder to infer the intent,\nslot tags, and slot values directly from a given input audio, obviating\nthe need for transcription. FANS consists of a shared audio encoder\nand three decoders, two of which are seq-to-seq decoders that predict\nnon null slot tags and slot values in parallel and in an auto-regressive\nmanner. FANS neural encoder and decoders architectures are flexible\nwhich allows us to leverage different combinations of LSTM, self-attention,\nand attenders. Our experiments show compared to the state-of-the-art\nend-to-end SLU models, FANS reduces ICER and IRER errors relatively\nby 30% and 7%, respectively, when tested on an in-house SLU dataset\nand by 0.86% and 2% absolute when tested on a public SLU dataset.\n"
      ],
      "doi": "10.21437/Interspeech.2021-793",
      "author_area_id": "11",
      "author_area_label": "Spoken Dialog Systems and Conversational Analysis"
    },
    "cao21c_interspeech": {
      "authors": [
        [
          "Yiran",
          "Cao"
        ],
        [
          "Nihal",
          "Potdar"
        ],
        [
          "Anderson R.",
          "Avila"
        ]
      ],
      "title": "Sequential End-to-End Intent and Slot Label Classification and Localization",
      "original": "1569",
      "page_count": 5,
      "order": 253,
      "p1": "1229",
      "pn": "1233",
      "abstract": [
        "Human-computer interaction (HCI) is significantly impacted by delayed\nresponses from a spoken dialogue system. Hence, end-to-end (e2e) spoken\nlanguage understanding (SLU) solutions have recently been proposed\nto decrease latency. Such approaches allow for the extraction of semantic\ninformation directly from the speech signal, thus bypassing the need\nfor a transcript from an automatic speech recognition (ASR) system.\nIn this paper, we propose a compact e2e SLU architecture for streaming\nscenarios, where chunks of the speech signal are processed continuously\nto predict intent and slot values. Our model is based on a 3D convolutional\nneural network (3D-CNN) and a unidirectional long short-term memory\n(LSTM). We compare the performance of two alignment-free losses: the\nconnectionist temporal classification (CTC) method and its adapted\nversion, namely connectionist temporal localization (CTL). The latter\nperforms not only the classification but also localization of sequential\naudio events. The proposed solution is evaluated on the Fluent Speech\nCommand dataset and results show our model ability to process incoming\nspeech signal, reaching accuracy as high as 98.97% for CTC and 98.78%\nfor CTL on single-label classification, and as high as 95.69% for CTC\nand 95.28% for CTL on two-label prediction.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1569",
      "author_area_id": "11",
      "author_area_label": "Spoken Dialog Systems and Conversational Analysis"
    },
    "muralidharan21_interspeech": {
      "authors": [
        [
          "Deepak",
          "Muralidharan"
        ],
        [
          "Joel Ruben Antony",
          "Moniz"
        ],
        [
          "Weicheng",
          "Zhang"
        ],
        [
          "Stephen",
          "Pulman"
        ],
        [
          "Lin",
          "Li"
        ],
        [
          "Megan",
          "Barnes"
        ],
        [
          "Jingjing",
          "Pan"
        ],
        [
          "Jason",
          "Williams"
        ],
        [
          "Alex",
          "Acero"
        ]
      ],
      "title": "DEXTER: Deep Encoding of External Knowledge for Named Entity Recognition in Virtual Assistants",
      "original": "1877",
      "page_count": 5,
      "order": 254,
      "p1": "1234",
      "pn": "1238",
      "abstract": [
        "Named entity recognition (NER) is usually developed and tested on text\nfrom well-written sources. However, in intelligent voice assistants,\nwhere NER is an important component, input to NER may be noisy because\nof user or speech recognition error. In applications, entity labels\nmay change frequently, and non-textual properties like topicality or\npopularity may be needed to choose among alternatives.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  We describe a NER\nsystem intended to address these problems. We test and train this system\non a proprietary user-derived dataset. We compare with a baseline text-only\nNER system; the baseline enhanced with external gazetteers; and the\nbaseline enhanced with the search and indirect labelling techniques\nwe describe below. The final configuration gives around 6% reduction\nin NER error rate. We also show that this technique improves related\ntasks, such as semantic parsing, with an improvement of up to 5% in\nerror rate.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1877",
      "author_area_id": "11",
      "author_area_label": "Spoken Dialog Systems and Conversational Analysis"
    },
    "wu21d_interspeech": {
      "authors": [
        [
          "Ting-Wei",
          "Wu"
        ],
        [
          "Ruolin",
          "Su"
        ],
        [
          "Biing-Hwang",
          "Juang"
        ]
      ],
      "title": "A Context-Aware Hierarchical BERT Fusion Network for Multi-Turn Dialog Act Detection",
      "original": "0095",
      "page_count": 5,
      "order": 255,
      "p1": "1239",
      "pn": "1243",
      "abstract": [
        "The success of interactive dialog systems is usually associated with\nthe quality of the spoken language understanding (SLU) task, which\nmainly identifies the corresponding dialog acts and slot values in\neach turn. By treating utterances in isolation, most SLU systems often\noverlook the semantic context in which a dialog act is expected. The\nact dependency between turns is nontrivial and yet critical to the\nidentification of the correct semantic representations. Previous works\nwith limited context awareness have exposed the inadequacy of dealing\nwith complexity in multiproned user intents, which are subject to spontaneous\nchange during turn transitions. In this work, we propose to enhance\nSLU in multi-turn dialogs, employing a context-aware hierarchical BERT\nfusion Network (CaBERT-SLU) to not only discern context information\nwithin a dialog but also jointly identify multiple dialog acts and\nslots in each utterance. Experimental results show that our approach\nreaches new state-of-the-art (SOTA) performances in two complicated\nmulti-turn dialogue datasets with considerable improvements compared\nwith previous methods, which only consider single utterances for multiple\nintents and slot filling.\n"
      ],
      "doi": "10.21437/Interspeech.2021-95",
      "author_area_id": "11",
      "author_area_label": "Spoken Dialog Systems and Conversational Analysis"
    },
    "chen21g_interspeech": {
      "authors": [
        [
          "Qian",
          "Chen"
        ],
        [
          "Wen",
          "Wang"
        ],
        [
          "Qinglin",
          "Zhang"
        ]
      ],
      "title": "Pre-Training for Spoken Language Understanding with Joint Textual and Phonetic Representation Learning",
      "original": "0234",
      "page_count": 5,
      "order": 256,
      "p1": "1244",
      "pn": "1248",
      "abstract": [
        "In the traditional cascading architecture for spoken language understanding\n(SLU), it has been observed that automatic speech recognition errors\ncould be detrimental to the performance of natural language understanding.\nEnd-to-end (E2E) SLU models have been proposed to directly map speech\ninput to desired semantic frame with a single model, hence mitigating\nASR error propagation. Recently, pre-training technologies have been\nexplored for these E2E models. In this paper, we propose a novel joint\ntextual-phonetic pre-training approach for learning spoken language\nrepresentations, aiming at exploring the full potentials of phonetic\ninformation to improve SLU robustness to ASR errors. We explore phoneme\nlabels as high-level speech features, and design and compare pre-training\ntasks based on conditional masked language model objectives and inter-sentence\nrelation objectives. We also investigate the efficacy of combining\ntextual and phonetic information during fine-tuning. Experimental results\non spoken language understanding benchmarks, Fluent Speech Commands\nand SNIPS, show that the proposed approach significantly outperforms\nstrong baseline models and improves robustness of spoken language understanding\nto ASR errors.\n"
      ],
      "doi": "10.21437/Interspeech.2021-234",
      "author_area_id": "11",
      "author_area_label": "Spoken Dialog Systems and Conversational Analysis"
    },
    "do21b_interspeech": {
      "authors": [
        [
          "Quynh",
          "Do"
        ],
        [
          "Judith",
          "Gaspers"
        ],
        [
          "Daniil",
          "Sorokin"
        ],
        [
          "Patrick",
          "Lehnen"
        ]
      ],
      "title": "Predicting Temporal Performance Drop of Deployed Production Spoken Language Understanding Models",
      "original": "0580",
      "page_count": 5,
      "order": 257,
      "p1": "1249",
      "pn": "1253",
      "abstract": [
        "In deployed real-world spoken language understanding (SLU) applications,\ndata continuously flows into the system. This leads to distributional\ndifferences between training and application data that can deteriorate\nmodel performance. While regularly retraining the deployed model with\nnew data helps mitigating this problem, it implies significant computational\nand human costs. In this paper, we develop a method, which can help\nguiding decisions on whether a model is safe to keep in production\nwithout notable performance loss or needs to be retrained. Towards\nthis goal, we build a performance drop regression model for an SLU\nmodel that was trained offline to detect a potential model drift in\nthe production phase. We present a wide range of experiments on multiple\nreal-world datasets, indicating that our method is useful for guiding\ndecisions in the SLU model development cycle and to reduce costs for\nmodel retraining.\n"
      ],
      "doi": "10.21437/Interspeech.2021-580",
      "author_area_id": "11",
      "author_area_label": "Spoken Dialog Systems and Conversational Analysis"
    },
    "ganhotra21_interspeech": {
      "authors": [
        [
          "Jatin",
          "Ganhotra"
        ],
        [
          "Samuel",
          "Thomas"
        ],
        [
          "Hong-Kwang J.",
          "Kuo"
        ],
        [
          "Sachindra",
          "Joshi"
        ],
        [
          "George",
          "Saon"
        ],
        [
          "Zolt\u00e1n",
          "T\u00fcske"
        ],
        [
          "Brian",
          "Kingsbury"
        ]
      ],
      "title": "Integrating Dialog History into End-to-End Spoken Language Understanding Systems",
      "original": "1460",
      "page_count": 5,
      "order": 258,
      "p1": "1254",
      "pn": "1258",
      "abstract": [
        "End-to-end spoken language understanding (SLU) systems that process\nhuman-human or human-computer interactions are often context independent\nand process each turn of a conversation independently. Spoken conversations\non the other hand, are very much context dependent, and dialog history\ncontains useful information that can improve the processing of each\nconversational turn. In this paper, we investigate the importance of\ndialog history and how it can be effectively integrated into end-to-end\nSLU systems. While processing a spoken utterance, our proposed RNN\ntransducer (RNN-T) based SLU model has access to its dialog history\nin the form of decoded transcripts and SLU labels of previous turns.\nWe encode the dialog history as BERT embeddings, and use them as an\nadditional input to the SLU model along with the speech features for\nthe current utterance. We evaluate our approach on a recently released\nspoken dialog data set, the  HarperValleyBank corpus. We observe significant\nimprovements: 8% for dialog action and 30% for caller intent recognition\ntasks, in comparison to a competitive context independent end-to-end\nbaseline system.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1460",
      "author_area_id": "11",
      "author_area_label": "Spoken Dialog Systems and Conversational Analysis"
    },
    "han21_interspeech": {
      "authors": [
        [
          "Ting",
          "Han"
        ],
        [
          "Chongxuan",
          "Huang"
        ],
        [
          "Wei",
          "Peng"
        ]
      ],
      "title": "Coreference Augmentation for Multi-Domain Task-Oriented Dialogue State Tracking",
      "original": "1463",
      "page_count": 5,
      "order": 259,
      "p1": "1259",
      "pn": "1263",
      "abstract": [
        "Dialogue State Tracking (DST), which is the process of inferring user\ngoals by estimating belief states given the dialogue history, plays\na critical role in task-oriented dialogue systems. A coreference phenomenon\nobserved in multi-turn conversations is not addressed by existing DST\nmodels, leading to suboptimal performances. In this paper, we propose\nCoreference Dialogue State Tracker (CDST) that explicitly models the\ncoreference feature. In particular, at each turn, the proposed model\njointly predicts the coreferred domain-slot pair and extracts the coreference\nvalues from the dialogue context. Experimental results on MultiWOZ\n2.1 dataset show that the proposed model achieves the state-of-the-art\njoint goal accuracy of 56.47%.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1463",
      "author_area_id": "11",
      "author_area_label": "Spoken Dialog Systems and Conversational Analysis"
    },
    "arora21_interspeech": {
      "authors": [
        [
          "Siddhant",
          "Arora"
        ],
        [
          "Alissa",
          "Ostapenko"
        ],
        [
          "Vijay",
          "Viswanathan"
        ],
        [
          "Siddharth",
          "Dalmia"
        ],
        [
          "Florian",
          "Metze"
        ],
        [
          "Shinji",
          "Watanabe"
        ],
        [
          "Alan W.",
          "Black"
        ]
      ],
      "title": "Rethinking End-to-End Evaluation of Decomposable Tasks: A Case Study on Spoken Language Understanding",
      "original": "1537",
      "page_count": 5,
      "order": 260,
      "p1": "1264",
      "pn": "1268",
      "abstract": [
        "Decomposable tasks are complex and comprise of a hierarchy of sub-tasks.\nSpoken intent prediction, for example, combines automatic speech recognition\nand natural language understanding. Existing benchmarks, however, typically\nhold out examples for only the surface-level sub-task. As a result,\nmodels with similar performance on these benchmarks may have unobserved\nperformance differences on the other sub-tasks. To allow insightful\ncomparisons between competitive end-to-end architectures, we propose\na framework to construct robust test sets using coordinate ascent over\nsub-task specific utility functions. Given a dataset for a decomposable\ntask, our method optimally creates a test set for each sub-task to\nindividually assess sub-components of the end-to-end model. Using spoken\nlanguage understanding as a case study, we generate new splits for\nthe Fluent Speech Commands and Snips SmartLights datasets. Each split\nhas two test sets: one with held-out utterances assessing natural language\nunderstanding abilities, and one with held-out speakers to test speech\nprocessing skills. Our splits identify performance gaps up to 10% between\nend-to-end systems that were within 1% of each other on the original\ntest sets. These performance gaps allow more realistic and actionable\ncomparisons between different architectures, driving future model development.\nWe release our splits and tools for the community.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1537",
      "author_area_id": "11",
      "author_area_label": "Spoken Dialog Systems and Conversational Analysis"
    },
    "sun21b_interspeech": {
      "authors": [
        [
          "Jianwei",
          "Sun"
        ],
        [
          "Zhiyuan",
          "Tang"
        ],
        [
          "Hengxin",
          "Yin"
        ],
        [
          "Wei",
          "Wang"
        ],
        [
          "Xi",
          "Zhao"
        ],
        [
          "Shuaijiang",
          "Zhao"
        ],
        [
          "Xiaoning",
          "Lei"
        ],
        [
          "Wei",
          "Zou"
        ],
        [
          "Xiangang",
          "Li"
        ]
      ],
      "title": "Semantic Data Augmentation for End-to-End Mandarin Speech Recognition",
      "original": "1162",
      "page_count": 5,
      "order": 261,
      "p1": "1269",
      "pn": "1273",
      "abstract": [
        "End-to-end models have gradually become the preferred option for automatic\nspeech recognition (ASR) applications. During the training of end-to-end\nASR, data augmentation is a quite effective technique for regularizing\nthe neural networks. This paper proposes a novel data augmentation\ntechnique based on semantic transposition of the transcriptions via\nsyntax rules for end-to-end Mandarin ASR. Specifically, we first segment\nthe transcriptions based on part-of-speech tags. Then transposition\nstrategies, such as placing the object in front of the subject or swapping\nthe subject and the object, are applied on the segmented sentences.\nFinally, the acoustic features corresponding to the transposed transcription\nare reassembled based on the audio-to-text forced-alignment produced\nby a pre-trained ASR system. The combination of original data and augmented\none is used for training a new ASR system. The experiments are conducted\non the Transformer[2] and Conformer[3] based ASR. The results show\nthat the proposed method can give consistent performance gain to the\nsystem. Augmentation related issues, such as comparison of different\nstrategies and ratios for data combination are also investigated.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1162",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "gong21c_interspeech": {
      "authors": [
        [
          "Xun",
          "Gong"
        ],
        [
          "Yizhou",
          "Lu"
        ],
        [
          "Zhikai",
          "Zhou"
        ],
        [
          "Yanmin",
          "Qian"
        ]
      ],
      "title": "Layer-Wise Fast Adaptation for End-to-End Multi-Accent Speech Recognition",
      "original": "1075",
      "page_count": 5,
      "order": 262,
      "p1": "1274",
      "pn": "1278",
      "abstract": [
        "Accent variability has posed a huge challenge to automatic speech recognition\n(ASR) modeling. Although one-hot accent vector based adaptation systems\nare commonly used, they require prior knowledge about the target accent\nand cannot handle unseen accents. Furthermore, simply concatenating\naccent embeddings does not make good use of accent knowledge, which\nhas limited improvements. In this work, we aim to tackle these problems\nwith a novel layer-wise adaptation structure injected into the E2E\nASR model encoder. The adapter layer encodes an arbitrary accent in\nthe accent space and assists the ASR model in recognizing accented\nspeech. Given an utterance, the adaptation structure extracts the corresponding\naccent information and transforms the input acoustic feature into an\naccent-related feature through the linear combination of all accent\nbases. We further explore the injection position of the adaptation\nlayer, the number of accent bases, and different types of accent bases\nto achieve better accent adaptation. Experimental results show that\nthe proposed adaptation structure brings 12% and 10% relative word\nerror rate (WER) reduction on the AESRC2020 accent dataset and the\nLibrispeech dataset, respectively, compared to the baseline.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1075",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "wang21m_interspeech": {
      "authors": [
        [
          "Jinhan",
          "Wang"
        ],
        [
          "Yunzheng",
          "Zhu"
        ],
        [
          "Ruchao",
          "Fan"
        ],
        [
          "Wei",
          "Chu"
        ],
        [
          "Abeer",
          "Alwan"
        ]
      ],
      "title": "Low Resource German ASR with Untranscribed Data Spoken by Non-Native Children &#8212; INTERSPEECH 2021 Shared Task SPAPL System",
      "original": "1974",
      "page_count": 5,
      "order": 263,
      "p1": "1279",
      "pn": "1283",
      "abstract": [
        "This paper describes the SPAPL system for the INTERSPEECH 2021 Challenge:\nShared Task on Automatic Speech Recognition for Non-Native Children&#8217;s\nSpeech in German. &#126;5 hours of transcribed data and &#126;60 hours\nof untranscribed data are provided to develop a German ASR system for\nchildren. For the training of the transcribed data, we propose a non-speech\nstate discriminative loss (NSDL) to mitigate the influence of long-duration\nnon-speech segments within speech utterances. In order to explore the\nuse of the untranscribed data, various approaches are implemented and\ncombined together to incrementally improve the system performance.\nFirst, bidirectional autoregressive predictive coding (Bi-APC) is used\nto learn initial parameters for acoustic modelling using the provided\nuntranscribed data. Second, incremental semi-supervised learning is\nfurther used to iteratively generate pseudo-transcribed data. Third,\ndifferent data augmentation schemes are used at different training\nstages to increase the variability and size of the training data. Finally,\na recurrent neural network language model (RNNLM) is used for rescoring.\nOur system achieves a word error rate (WER) of 39.68% on the evaluation\ndata, an approximately 12% relative improvement over the official baseline\n(45.21%).\n"
      ],
      "doi": "10.21437/Interspeech.2021-1974",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation:14 Special Sessions"
    },
    "sim21_interspeech": {
      "authors": [
        [
          "Khe Chai",
          "Sim"
        ],
        [
          "Angad",
          "Chandorkar"
        ],
        [
          "Fan",
          "Gao"
        ],
        [
          "Mason",
          "Chua"
        ],
        [
          "Tsendsuren",
          "Munkhdalai"
        ],
        [
          "Fran\u00e7oise",
          "Beaufays"
        ]
      ],
      "title": "Robust Continuous On-Device Personalization for Automatic Speech Recognition",
      "original": "0318",
      "page_count": 5,
      "order": 264,
      "p1": "1284",
      "pn": "1288",
      "abstract": [
        "On-device personalization of an all-neural automatic speech recognition\n(ASR) model can be achieved efficiently by fine-tuning the last few\nlayers of the model. This approach has been shown to be effective for\nadapting the model to recognize rare named entities using only a small\namount of data. To reliably perform continuous on-device learning,\nit is important for the training process to be completely autonomous\nwithout manual intervention. Our simulation studies show that training\nover many rounds may eventually lead to a significant model drift if\nthe personalized model is indiscriminately accepted at the end of each\ntraining round. It is important to have appropriate acceptance criteria\nin place to guard the model against drifting. Moreover, for storage\nefficiency, it is desirable to persist the model weights in quantized\nform. We found that quantizing and dequantizing the model weights in\nbetween training rounds can prevent the model from learning effectively.\nThis issue can be circumvented by adding noise to the quantized weights\nat the start of each training round.\n"
      ],
      "doi": "10.21437/Interspeech.2021-318",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "kumar21b_interspeech": {
      "authors": [
        [
          "Shashi",
          "Kumar"
        ],
        [
          "Shakti P.",
          "Rath"
        ],
        [
          "Abhishek",
          "Pandey"
        ]
      ],
      "title": "Speaker Normalization Using Joint Variational Autoencoder",
      "original": "0467",
      "page_count": 5,
      "order": 265,
      "p1": "1289",
      "pn": "1293",
      "abstract": [
        "Speaker adaptation is known to provide significant improvement in speech\nrecognition accuracy. However, in practical scenario, only a few seconds\nof audio is available due to which it may be infeasible to apply speaker\nadaptation methods such as i-vector and fMLLR robustly. Also, decoding\nwith fMLLR transformation happens in two-passes which is impractical\nfor real-time applications. In recent past, mapping speech features\nfrom speaker independent (SI) space to fMLLR normalized space using\ndenoising autoencoder (DA) has been explored. To the best of our knowledge,\nsuch mapping generally does not yield consistent improvement. In this\npaper, we show that our proposed joint VAE based mapping achieves a\nlarge improvements over ASR models trained using filterbank SI features.\nWe also show that joint VAE outperforms DA by a large margin. We observe\na relative improvement of 17% in word error rate (WER) compared to\nASR model trained using filterbank features with i-vectors and 23%\nwithout i-vectors.\n"
      ],
      "doi": "10.21437/Interspeech.2021-467",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "xu21c_interspeech": {
      "authors": [
        [
          "Gaopeng",
          "Xu"
        ],
        [
          "Song",
          "Yang"
        ],
        [
          "Lu",
          "Ma"
        ],
        [
          "Chengfei",
          "Li"
        ],
        [
          "Zhongqin",
          "Wu"
        ]
      ],
      "title": "The TAL System for the INTERSPEECH2021 Shared Task on Automatic Speech Recognition for Non-Native Childrens Speech",
      "original": "1104",
      "page_count": 5,
      "order": 266,
      "p1": "1294",
      "pn": "1298",
      "abstract": [
        "This paper describes TAL&#8217;s system for the INTERSPEECH 2021 shared\ntask on Automatic Speech Recognition (ASR) for non-native children&#8217;s\nspeech. In this work, we attempt to apply the self-supervised approach\nto non-native German children&#8217;s ASR. First, we conduct some baseline\nexperiments to indicate that self-supervised learning can capture more\nacoustic information on non-native children&#8217;s speech. Then, we\napply the 11-fold data augmentation and combine it with data clean-up\nto supplement to the limited training data. Moreover, an in-domain\nsemi-supervised VAD model is utilized to segment untranscribed audio.\nThese strategies can significantly improve the system performance.\nFurthermore, we use two types of language models to further improve\nperformance, i.e., a 4-gram LM with CTC beam-search and a Transformer\nLM for 2-pass rescoring. Our ASR system reduces the Word Error Rate\n(WER) by about 48% relatively in comparison with the baseline, achieving\n1st in the evaluation period with the WER of 23.5%.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1104",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation:14 Special Sessions"
    },
    "lam21b_interspeech": {
      "authors": [
        [
          "Tsz Kin",
          "Lam"
        ],
        [
          "Mayumi",
          "Ohta"
        ],
        [
          "Shigehiko",
          "Schamoni"
        ],
        [
          "Stefan",
          "Riezler"
        ]
      ],
      "title": "On-the-Fly Aligned Data Augmentation for Sequence-to-Sequence ASR",
      "original": "1679",
      "page_count": 5,
      "order": 267,
      "p1": "1299",
      "pn": "1303",
      "abstract": [
        "We propose an on-the-fly data augmentation method for automatic speech\nrecognition (ASR) that uses alignment information to generate effective\ntraining samples. Our method, called Aligned Data Augmentation (ADA)\nfor ASR, replaces transcribed tokens and the speech representations\nin an aligned manner to generate previously unseen training pairs.\nThe speech representations are sampled from an audio dictionary that\nhas been extracted from the training corpus and inject speaker variations\ninto the training examples. The transcribed tokens are either predicted\nby a language model such that the augmented data pairs are semantically\nclose to the original data, or randomly sampled. Both strategies result\nin training pairs that improve robustness in ASR training. Our experiments\non a Seq-to-Seq architecture show that ADA can be applied on top of\nSpecAugment, and achieves about 9&#8211;23% and 4&#8211;15% relative\nimprovements in WER over SpecAugment alone on LibriSpeech 100h and\nLibriSpeech 960h test datasets, respectively.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1679",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "gao21_interspeech": {
      "authors": [
        [
          "Heting",
          "Gao"
        ],
        [
          "Junrui",
          "Ni"
        ],
        [
          "Yang",
          "Zhang"
        ],
        [
          "Kaizhi",
          "Qian"
        ],
        [
          "Shiyu",
          "Chang"
        ],
        [
          "Mark",
          "Hasegawa-Johnson"
        ]
      ],
      "title": "Zero-Shot Cross-Lingual Phonetic Recognition with External Language Embedding",
      "original": "1843",
      "page_count": 5,
      "order": 268,
      "p1": "1304",
      "pn": "1308",
      "abstract": [
        "Many existing languages are too sparsely resourced for monolingual\ndeep learning networks to achieve high accuracy. Multilingual phonetic\nrecognition systems mitigate data sparsity issues by training models\non data from multiple languages and learning a speech-to-phone or speech-to-text\nmodel universal to all languages. However, despite their good performance\non the seen training languages, multilingual systems have poor performance\non unseen languages. This paper argues that in the real world, even\nan unseen language has metadata: linguists can tell us the language\nname, its language family and, usually, its phoneme inventory. Even\nwith no transcribed speech, it is possible to train a language embedding\nusing only data from language typologies (phylogenetic node and phoneme\ninventory) that reduces ASR error rates. Experiments on a 20-language\ncorpus show that our methods achieve phonetic token error rate (PTER)\nreduction on all the unseen test languages. An ablation study shows\nthat using the wrong language embedding usually harms PTER if the two\nlanguages are from different language families. However, even the wrong\nlanguage embedding often improves PTER if the language embedding belongs\nto another member of the same language family.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1843",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "huang21c_interspeech": {
      "authors": [
        [
          "Yan",
          "Huang"
        ],
        [
          "Guoli",
          "Ye"
        ],
        [
          "Jinyu",
          "Li"
        ],
        [
          "Yifan",
          "Gong"
        ]
      ],
      "title": "Rapid Speaker Adaptation for Conformer Transducer: Attention and Bias Are All You Need",
      "original": "1884",
      "page_count": 5,
      "order": 269,
      "p1": "1309",
      "pn": "1313",
      "abstract": [
        "Conformer transducer achieves new state-of-the-art end-to-end (E2E)\nsystem performance and has become increasingly appealing for production.\nIn this paper, we study how to effectively perform rapid speaker adaptation\nin a conformer transducer and how it compares with the RNN transducer.\nWe hierarchically decompose the conformer transducer and compare adapting\neach component through fine-tuning. Among various interesting observations,\nthere are three distinct findings: First, adapting the self-attention\ncan achieve more than 80% gain of the full network adaptation. When\nthe adaptation data is extremely scarce, attention is all you need\nto adapt. Second, within the self-attention, adapting the value projection\noutperforms adapting the key or the query projection. Lastly, bias\nadaptation, despite of its compact parameter space, is surprisingly\neffective. We conduct experiments on a state-of-the-art conformer transducer\nfor an email dictation task. With 3 to 5 min source speech and 200\nminute personalized TTS speech, the best performing encoder and joint\nnetwork adaptation yields 38.37% and 19.90% relative word error rate\n(WER) reduction. Combining the attention and bias adaptation can achieve\n90% of the gain with significantly smaller footprint. Further comparison\nwith the RNN-T suggests the new state-of-the-art conformer transducer\ncan benefit as much as if not more from personalization.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1884",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "das21b_interspeech": {
      "authors": [
        [
          "Nilaksh",
          "Das"
        ],
        [
          "Sravan",
          "Bodapati"
        ],
        [
          "Monica",
          "Sunkara"
        ],
        [
          "Sundararajan",
          "Srinivasan"
        ],
        [
          "Duen Horng",
          "Chau"
        ]
      ],
      "title": "Best of Both Worlds: Robust Accented Speech Recognition with Adversarial Transfer Learning",
      "original": "1888",
      "page_count": 5,
      "order": 270,
      "p1": "1314",
      "pn": "1318",
      "abstract": [
        "Training deep neural networks for automatic speech recognition (ASR)\nrequires large amounts of transcribed speech. This becomes a bottleneck\nfor training robust models for <i>accented</i> speech which typically\ncontains high variability in pronunciation and other semantics, since\nobtaining large amounts of annotated accented data is both tedious\nand costly. Often, we only have access to large amounts of <i>unannotated</i>\nspeech from different accents. In this work, we leverage this unannotated\ndata to provide semantic regularization to an ASR model that has been\ntrained only on one accent, to improve its performance for multiple\naccents. We propose Accent Pre-Training (Acc-PT), a semi-supervised\ntraining strategy that combines transfer learning and adversarial training.\nOur approach improves the performance of a state-of-the-art ASR model\nby 33% on average over the baseline across multiple accents, training\nonly on annotated samples from one standard accent, and as little as\n105 minutes of <i>unannotated</i> speech from a target accent.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1888",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "chu21_interspeech": {
      "authors": [
        [
          "Wei",
          "Chu"
        ],
        [
          "Peng",
          "Chang"
        ],
        [
          "Jing",
          "Xiao"
        ]
      ],
      "title": "Extending Pronunciation Dictionary with Automatically Detected Word Mispronunciations to Improve PAII&#8217;s System for Interspeech 2021 Non-Native Child English Close Track ASR Challenge",
      "original": "2053",
      "page_count": 5,
      "order": 271,
      "p1": "1319",
      "pn": "1323",
      "abstract": [
        "This paper proposed to automatically detect mispronounced words over\nthe regions that have low Goodness-of-Pronunciation scores through\na constrained phone decoder, then add these word mispronunciations\ninto the orthodox lexicon without colliding with existing pronunciations,\nfinally use the expanded lexicon for decoding non-native speech. The\nconstrained phone decoder is compiled by using a phone-level automatically\ngenerated one-edit-distance network to eliminate the need of extended\nrecognition networks designed by phonologists. Results and analysis\nhave shown that the pronunciation dictionary extension is effective\nin improving WER performance for non-native speech recognition. This\npaper also described the details of PAII&#8217;s single-pass fusion-free\nhybrid system for this Interspeech 2021 non-native children English\nclose track ASR challenge, especially showed the effective use of non-speech\nsegments in the training set as noise sources to perform noise augmentation\non the training data, and also conducted a comparison of acoustic models\nwith different neural network architectures with analysis. Final WERs\nof 12.10%/28.25% are obtained compared to a well-optimized baseline\nwith WERs of 13.37%/33.51% on development/evaluation set, respectively.\n"
      ],
      "doi": "10.21437/Interspeech.2021-2053",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation:14 Special Sessions"
    },
    "li21d_interspeech": {
      "authors": [
        [
          "Tingle",
          "Li"
        ],
        [
          "Yichen",
          "Liu"
        ],
        [
          "Chenxu",
          "Hu"
        ],
        [
          "Hang",
          "Zhao"
        ]
      ],
      "title": "CVC: Contrastive Learning for Non-Parallel Voice Conversion",
      "original": "0137",
      "page_count": 5,
      "order": 272,
      "p1": "1324",
      "pn": "1328",
      "abstract": [
        "Cycle consistent generative adversarial network (CycleGAN) and variational\nautoencoder (VAE) based models have gained popularity in non-parallel\nvoice conversion recently. However, they often suffer from difficult\ntraining process and unsatisfactory results. In this paper, we propose\na contrastive learning-based adversarial approach for voice conversion,\nnamely contrastive voice conversion (CVC). Compared to previous CycleGAN-based\nmethods, CVC only requires an efficient one-way GAN training by taking\nthe advantage of contrastive learning. When it comes to non-parallel\none-to-one voice conversion, CVC is on par or better than CycleGAN\nand VAE while effectively reducing training time. CVC further demonstrates\nsuperior performance in many-to-one voice conversion, enabling the\nconversion from unseen speakers.\n"
      ],
      "doi": "10.21437/Interspeech.2021-137",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "huang21d_interspeech": {
      "authors": [
        [
          "Wen-Chin",
          "Huang"
        ],
        [
          "Kazuhiro",
          "Kobayashi"
        ],
        [
          "Yu-Huai",
          "Peng"
        ],
        [
          "Ching-Feng",
          "Liu"
        ],
        [
          "Yu",
          "Tsao"
        ],
        [
          "Hsin-Min",
          "Wang"
        ],
        [
          "Tomoki",
          "Toda"
        ]
      ],
      "title": "A Preliminary Study of a Two-Stage Paradigm for Preserving Speaker Identity in Dysarthric Voice Conversion",
      "original": "0208",
      "page_count": 5,
      "order": 273,
      "p1": "1329",
      "pn": "1333",
      "abstract": [
        "We propose a new paradigm for maintaining speaker identity in dysarthric\nvoice conversion (DVC). The poor quality of dysarthric speech can be\ngreatly improved by statistical VC, but as the normal speech utterances\nof a dysarthria patient are nearly impossible to collect, previous\nwork failed to recover the individuality of the patient. In light of\nthis, we suggest a novel, two-stage approach for DVC, which is highly\nflexible in that no normal speech of the patient is required. First,\na powerful parallel sequence-to-sequence model converts the input dysarthric\nspeech into a normal speech of a reference speaker as an intermediate\nproduct, and a nonparallel, frame-wise VC model realized with a variational\nautoencoder then converts the speaker identity of the reference speech\nback to that of the patient while assumed to be capable of preserving\nthe enhanced quality. We investigate several design options. Experimental\nevaluation results demonstrate the potential of our approach to improving\nthe quality of the dysarthric speech while maintaining the speaker\nidentity.\n"
      ],
      "doi": "10.21437/Interspeech.2021-208",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "eskimez21_interspeech": {
      "authors": [
        [
          "Sefik Emre",
          "Eskimez"
        ],
        [
          "Dimitrios",
          "Dimitriadis"
        ],
        [
          "Kenichi",
          "Kumatani"
        ],
        [
          "Robert",
          "Gmyr"
        ]
      ],
      "title": "One-Shot Voice Conversion with Speaker-Agnostic StarGAN",
      "original": "0221",
      "page_count": 5,
      "order": 274,
      "p1": "1334",
      "pn": "1338",
      "abstract": [
        "In this work, we propose a variant of STARGAN for many-to-many voice\nconversion (VC) conditioned on the d-vectors for short-duration (2&#8211;15\nseconds) speech. We make several modifications to the STARGAN training\nand employ new network architectures. We employ a transformer encoder\nin the discriminator network, and we apply the discriminator loss to\nthe cycle consistency and identity samples in addition to the generated\n(fake) samples. Instead of classifying the samples as either real or\nfake, our discriminator tries to predict the categorical speaker class,\nwhere a fake class is added for the generated samples. Furthermore,\nwe employ a reverse gradient layer after the generator&#8217;s encoder\nand use an auxiliary classifier to remove the speaker&#8217;s information\nfrom the encoded representation. We show that our method yields better\nresults than the baseline method in objective and subjective evaluations\nin terms of voice conversion quality. Moreover, we provide an ablation\nstudy and show each component&#8217;s influence on speaker similarity.\n"
      ],
      "doi": "10.21437/Interspeech.2021-221",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "koshizuka21_interspeech": {
      "authors": [
        [
          "Takeshi",
          "Koshizuka"
        ],
        [
          "Hidefumi",
          "Ohmura"
        ],
        [
          "Kouichi",
          "Katsurada"
        ]
      ],
      "title": "Fine-Tuning Pre-Trained Voice Conversion Model for Adding New Target Speakers with Limited Data",
      "original": "0244",
      "page_count": 5,
      "order": 275,
      "p1": "1339",
      "pn": "1343",
      "abstract": [
        "Voice conversion (VC) is a technique that converts speaker-dependent\nnon-linguistic information into that of another speaker, while retaining\nthe linguistic information of the input speech. A typical VC system\ncomprises two modules: an encoder module that removes speaker individuality\nfrom the input speech and a decoder module that incorporates another\nspeaker&#8217;s individuality in synthesized speech. This paper proposes\na training method for a vocoder-free any-to-many encoder-decoder VC\nmodel with limited data. Various pre-training techniques have been\nproposed to solve problems training to limited training data; some\nof these techniques employ the text-to-speech (TTS) task for pre-training.\nWe pre-train the decoder module in the voice conversion task for growing\nour pre-training technique into continuously adding target speakers\nto the VC system. The experimental results show that good conversion\nperformance can be achieved by conducting VC-based pre-training. We\nalso confirmed that the rehearsal and pseudo-rehearsal methods can\neffectively fine-tune the model without degrading the conversion performance\nof the pre-trained target speakers.\n"
      ],
      "doi": "10.21437/Interspeech.2021-244",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "wang21n_interspeech": {
      "authors": [
        [
          "Disong",
          "Wang"
        ],
        [
          "Liqun",
          "Deng"
        ],
        [
          "Yu Ting",
          "Yeung"
        ],
        [
          "Xiao",
          "Chen"
        ],
        [
          "Xunying",
          "Liu"
        ],
        [
          "Helen",
          "Meng"
        ]
      ],
      "title": "VQMIVC: Vector Quantization and Mutual Information-Based Unsupervised Speech Representation Disentanglement for One-Shot Voice Conversion",
      "original": "0283",
      "page_count": 5,
      "order": 276,
      "p1": "1344",
      "pn": "1348",
      "abstract": [
        "One-shot voice conversion (VC), which performs conversion across arbitrary\nspeakers with only a single target-speaker utterance for reference,\ncan be effectively achieved by speech representation disentanglement.\nExisting work generally ignores the correlation between different speech\nrepresentations during training, which causes leakage of content information\ninto the speaker representation and thus degrades VC performance. To\nalleviate this issue, we employ vector quantization (VQ) for content\nencoding and introduce mutual information (MI) as the correlation metric\nduring training, to achieve proper disentanglement of content, speaker\nand pitch representations, by reducing their inter-dependencies in\nan unsupervised manner. Experimental results reflect the superiority\nof the proposed method in learning effective disentangled speech representations\nfor retaining source linguistic content and intonation variations,\nwhile capturing target speaker characteristics. In doing so, the proposed\napproach achieves higher speech naturalness and speaker similarity\nthan current state-of-the-art one-shot VC systems. Our code, pre-trained\nmodels and demo are publicly available.\n"
      ],
      "doi": "10.21437/Interspeech.2021-283",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "li21e_interspeech": {
      "authors": [
        [
          "Yinghao Aaron",
          "Li"
        ],
        [
          "Ali",
          "Zare"
        ],
        [
          "Nima",
          "Mesgarani"
        ]
      ],
      "title": "StarGANv2-VC: A Diverse, Unsupervised, Non-Parallel Framework for Natural-Sounding Voice Conversion",
      "original": "0319",
      "page_count": 5,
      "order": 277,
      "p1": "1349",
      "pn": "1353",
      "abstract": [
        "We present an unsupervised non-parallel many-to-many voice conversion\n(VC) method using a generative adversarial network (GAN) called StarGAN\nv2. Using a combination of adversarial source classifier loss and perceptual\nloss, our model significantly outperforms previous VC models. Although\nour model is trained only with 20 English speakers, it generalizes\nto a variety of voice conversion tasks, such as any-to-many, cross-lingual,\nand singing conversion. Using a style encoder, our framework can also\nconvert plain reading speech into stylistic speech, such as emotional\nand falsetto speech. Subjective and objective evaluation experiments\non a non-parallel many-to-many voice conversion task revealed that\nour model produces natural sounding voices, close to the sound quality\nof state-of-the-art text-to-speech (TTS) based voice conversion methods\nwithout the need for text labels. Moreover, our model is completely\nconvolutional and with a faster-than-real-time vocoder such as Parallel\nWaveGAN can perform real-time voice conversion.\n"
      ],
      "doi": "10.21437/Interspeech.2021-319",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "kumar21c_interspeech": {
      "authors": [
        [
          "Neeraj",
          "Kumar"
        ],
        [
          "Srishti",
          "Goel"
        ],
        [
          "Ankur",
          "Narang"
        ],
        [
          "Brejesh",
          "Lall"
        ]
      ],
      "title": "Normalization Driven Zero-Shot Multi-Speaker Speech Synthesis",
      "original": "0441",
      "page_count": 5,
      "order": 278,
      "p1": "1354",
      "pn": "1358",
      "abstract": [
        "In this paper, we present a novel zero-shot multi-speaker speech synthesis\napproach (ZSM-SS) that leverages the normalization architecture and\nspeaker encoder with non-autoregressive multi-head attention driven\nencoder-decoder architecture. Given an input text and a reference speech\nsample of an unseen person, ZSM-SS can generate speech in that person&#8217;s\nstyle in a zero-shot manner. Additionally, we demonstrate how the affine\nparameters of normalization help in capturing the prosodic features\nsuch as energy and fundamental frequency in a disentangled fashion\nand can be used to generate morphed speech output. We demonstrate the\nefficacy of our proposed architecture on multi-speaker VCTK[1] and\nLibriTTS [2] datasets, using multiple quantitative metrics that measure\ngenerated speech distortion and MOS, along with speaker embedding analysis\nof the proposed speaker encoder model.\n"
      ],
      "doi": "10.21437/Interspeech.2021-441",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "sakamoto21_interspeech": {
      "authors": [
        [
          "Shoki",
          "Sakamoto"
        ],
        [
          "Akira",
          "Taniguchi"
        ],
        [
          "Tadahiro",
          "Taniguchi"
        ],
        [
          "Hirokazu",
          "Kameoka"
        ]
      ],
      "title": "StarGAN-VC+ASR: StarGAN-Based Non-Parallel Voice Conversion Regularized by Automatic Speech Recognition",
      "original": "0492",
      "page_count": 5,
      "order": 279,
      "p1": "1359",
      "pn": "1363",
      "abstract": [
        "Preserving the linguistic content of input speech is essential during\nvoice conversion (VC). The star generative adversarial network-based\nVC method (StarGAN-VC) is a recently developed method that allows non-parallel\nmany-to-many VC. Although this method is powerful, it can fail to preserve\nthe linguistic content of input speech when the number of available\ntraining samples is extremely small. To overcome this problem, we propose\nthe use of automatic speech recognition to assist model training, to\nimprove StarGAN-VC, especially in low-resource scenarios. Experimental\nresults show that using our proposed method, StarGAN-VC can retain\nmore linguistic information than vanilla StarGAN-VC.\n"
      ],
      "doi": "10.21437/Interspeech.2021-492",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "xu21d_interspeech": {
      "authors": [
        [
          "Xuexin",
          "Xu"
        ],
        [
          "Liang",
          "Shi"
        ],
        [
          "Jinhui",
          "Chen"
        ],
        [
          "Xunquan",
          "Chen"
        ],
        [
          "Jie",
          "Lian"
        ],
        [
          "Pingyuan",
          "Lin"
        ],
        [
          "Zhihong",
          "Zhang"
        ],
        [
          "Edwin R.",
          "Hancock"
        ]
      ],
      "title": "Two-Pathway Style Embedding for Arbitrary Voice Conversion",
      "original": "0506",
      "page_count": 5,
      "order": 280,
      "p1": "1364",
      "pn": "1368",
      "abstract": [
        "Arbitrary voice conversion, also referred to as zero-shot voice conversion,\nhas recently attracted increased attention in the literature. Although\ndisentangling the linguistic and style representations for acoustic\nfeatures is an effective way to achieve zero-shot voice conversion,\nthe problem of how to convert to a natural speaker style is challenging\nbecause of the intrinsic variabilities of speech and the difficulties\nof completely decoupling them. For this reason, in this paper, we propose\na Two-Pathway Style Embedding Voice Conversion framework (TPSE-VC)\nfor realistic and natural speech conversion. The novel feature of this\nmethod is to simultaneously embed sentence-level and phoneme-level\nstyle information. A novel attention mechanism is proposed to implement\nthe implicit alignment for timbre style and phoneme content, further\nembedding a phoneme-level style representation. In addition, we consider\nembedding the complete set of time steps of audio style into a fixed-length\nvector to obtain the sentence-level style representation. Moreover,\nTPSEVC does not require any pre-trained models, and is only trained\nwith non-parallel speech data. Experimental results demonstrate that\nthe proposed TPSE-VC outperforms the state-of-the-art results on zero-shot\nvoice conversion.\n"
      ],
      "doi": "10.21437/Interspeech.2021-506",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "liu21c_interspeech": {
      "authors": [
        [
          "Yufei",
          "Liu"
        ],
        [
          "Chengzhu",
          "Yu"
        ],
        [
          "Wang",
          "Shuai"
        ],
        [
          "Zhenchuan",
          "Yang"
        ],
        [
          "Yang",
          "Chao"
        ],
        [
          "Weibin",
          "Zhang"
        ]
      ],
      "title": "Non-Parallel Any-to-Many Voice Conversion by Replacing Speaker Statistics",
      "original": "0557",
      "page_count": 5,
      "order": 281,
      "p1": "1369",
      "pn": "1373",
      "abstract": [
        "This paper proposes a non-parallel any-to-many voice conversion (VC)\napproach with a novel statistics replacement layer. Non-parallel VC\nis usually achieved by firstly disentangling linguistic and speaker\nrepresentations, and then concatenating the linguistic content with\nthe learned target speaker&#8217;s embedding at the conversion stage.\nWhile such a concatenation-based approach could introduce speaker-specific\ncharacteristics into the network, it is not very effective as it entirely\nrelies on the network to learn to combine the linguistic content and\nthe speaker characteristics. Inspired by X-vectors, where the statistics\nof hidden representation such as means and standard deviations are\nused for speaker differentiation, we propose a statistics replacement\nlayer in VC systems to directly modify the hidden states to have the\ntarget speaker&#8217;s statistics. The speaker-specific statistics\nof hidden states are learned for each target speaker during training\nand are used as guidance for the statistics replacement layer during\ninference. Moreover, to better concentrate the speaker information\ninto the statistics of hidden representation, a multitask training\nwith X-vector based speaker classification is also performed. Experimental\nresults with Librispeech and VCTK datasets show that the proposed method\ncan effectively improve the converted speech&#8217;s naturalness and\nsimilarity.\n"
      ],
      "doi": "10.21437/Interspeech.2021-557",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "zhou21c_interspeech": {
      "authors": [
        [
          "Yi",
          "Zhou"
        ],
        [
          "Xiaohai",
          "Tian"
        ],
        [
          "Zhizheng",
          "Wu"
        ],
        [
          "Haizhou",
          "Li"
        ]
      ],
      "title": "Cross-Lingual Voice Conversion with a Cycle Consistency Loss on Linguistic Representation",
      "original": "0687",
      "page_count": 5,
      "order": 282,
      "p1": "1374",
      "pn": "1378",
      "abstract": [
        "Cross-Lingual Voice Conversion (XVC) aims to modify a source speaker\nidentity towards a target while preserving the source linguistic content.\nThis paper introduces a cycle consistency loss on linguistic representation\nto ensure the speech content unchanged after conversion. The proposed\nXVC model consists of two loss functions during optimization: a spectral\nreconstruction loss and a linguistic cycle consistency loss. The cycle\nconsistency loss seeks to maintain the source speech&#8217;s linguistic\ncontent. Specifically, we utilize Phonetic PosteriorGram (PPG) to represent\nthe linguistic content. XVC experiments were conducted between English\nand Mandarin. Both objective and subjective evaluations demonstrated\nthat with the proposed cycle consistency loss, converted speech is\nmore intelligible.\n"
      ],
      "doi": "10.21437/Interspeech.2021-687",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "du21_interspeech": {
      "authors": [
        [
          "Hongqiang",
          "Du"
        ],
        [
          "Lei",
          "Xie"
        ]
      ],
      "title": "Improving Robustness of One-Shot Voice Conversion with Deep Discriminative Speaker Encoder",
      "original": "2132",
      "page_count": 5,
      "order": 283,
      "p1": "1379",
      "pn": "1383",
      "abstract": [
        "One-shot voice conversion has received significant attention since\nonly one utterance from source speaker and target speaker respectively\nis required. Moreover, source speaker and target speaker do not need\nto be seen during training. However, available one-shot voice conversion\napproaches are not stable for unseen speakers as the speaker embedding\nextracted from one utterance of an unseen speaker is not reliable.\nIn this paper, we propose a deep discriminative speaker encoder to\nextract speaker embedding from one utterance more effectively. Specifically,\nthe speaker encoder first integrates residual network and squeeze-and-excitation\nnetwork to extract discriminative speaker information in frame level\nby modeling frame-wise and channel-wise interdependence in features.\nThen attention mechanism is introduced to further emphasize speaker\nrelated information via assigning different weights to frame level\nspeaker information. Finally a statistic pooling layer is used to aggregate\nweighted frame level speaker information to form utterance level speaker\nembedding. The experimental results demonstrate that our proposed speaker\nencoder can improve the robustness of one-shot voice conversion for\nunseen speakers and outperforms baseline systems in terms of speech\nquality and speaker similarity.\n"
      ],
      "doi": "10.21437/Interspeech.2021-2132",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "white21_interspeech": {
      "authors": [
        [
          "Hannah",
          "White"
        ],
        [
          "Joshua",
          "Penney"
        ],
        [
          "Andy",
          "Gibson"
        ],
        [
          "Anita",
          "Szakay"
        ],
        [
          "Felicity",
          "Cox"
        ]
      ],
      "title": "Optimizing an Automatic Creaky Voice Detection Method for Australian English Speaking Females",
      "original": "0711",
      "page_count": 5,
      "order": 284,
      "p1": "1384",
      "pn": "1388",
      "abstract": [
        "Creaky voice is a nonmodal phonation type that has various linguistic\nand sociolinguistic functions. Manually annotating creaky voice for\nphonetic analysis is time-consuming and labor-intensive. In recent\nyears, automatic tools for detecting creaky voice have been proposed,\nwhich present the possibility for easier, faster and more consistent\ncreak identification. One of these proposed tools is a Creak Detector\nalgorithm that uses an automatic neural network taking its input from\nseveral acoustic cues to identify creaky voice. Previous work has suggested\nthat the creak probability threshold at which this tool determines\nan instance to be creaky may vary depending on the speaker population.\nThe present study investigates the optimal creak detection threshold\nfor female Australian English speakers.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  Results show further\nsupport for the practice of first finding the optimal threshold when\nusing the Creak Detection algorithm on new data sets. Additionally,\nresults show that accuracy of creaky voice detection using the Creak\nDetection algorithm can be significantly improved by excluding non-sonorant\ndata.\n"
      ],
      "doi": "10.21437/Interspeech.2021-711",
      "author_area_id": "13",
      "author_area_label": " Speech, Voice, and Hearing Disorders:14 Special Sessions"
    },
    "penney21_interspeech": {
      "authors": [
        [
          "Joshua",
          "Penney"
        ],
        [
          "Andy",
          "Gibson"
        ],
        [
          "Felicity",
          "Cox"
        ],
        [
          "Michael",
          "Proctor"
        ],
        [
          "Anita",
          "Szakay"
        ]
      ],
      "title": "A Comparison of Acoustic Correlates of Voice Quality Across Different Recording Devices: A Cautionary Tale",
      "original": "0729",
      "page_count": 5,
      "order": 285,
      "p1": "1389",
      "pn": "1393",
      "abstract": [
        "There has been a recent increase in speech research utilizing data\nrecorded with participants&#8217; personal devices, particularly in\nlight of the COVID-19 pandemic and restrictions on face-to-face interactions.\nThis raises important questions about whether these recordings are\ncomparable to those made in traditional lab-based settings. Some previous\nstudies have compared the viability of recordings made with personal\ndevices for the clinical evaluation of voice quality. However, these\nstudies rely on simple statistical analyses and do not examine acoustic\ncorrelates of voice quality typically examined in the (socio-) phonetic\nliterature (e.g. H1-H2). In this study, we compare recordings from\na set of smartphones/laptops and a solid-state recorder to assess the\nreliability of a range of acoustic correlates of voice quality. The\nresults show significant differences for many acoustic measures of\nvoice quality across devices. Further exploratory analyses demonstrate\nthat these differences are not simple offsets, but rather that their\nmagnitude depends on the value of the measurement of interest. We therefore\nurge researchers to exercise caution when examining voice quality based\non recordings made with participants&#8217; devices, particularly when\ninterested in small effect sizes. We also call on the speech research\ncommunity to investigate these issues more thoroughly.\n"
      ],
      "doi": "10.21437/Interspeech.2021-729",
      "author_area_id": "13",
      "author_area_label": " Speech, Voice, and Hearing Disorders:14 Special Sessions"
    },
    "sfakianaki21_interspeech": {
      "authors": [
        [
          "Anna",
          "Sfakianaki"
        ],
        [
          "George P.",
          "Kafentzis"
        ]
      ],
      "title": "Investigating Voice Function Characteristics of Greek Speakers with Hearing Loss Using Automatic Glottal Source Feature Extraction",
      "original": "0870",
      "page_count": 5,
      "order": 286,
      "p1": "1394",
      "pn": "1398",
      "abstract": [
        "The current study investigates voice quality characteristics of Greek\nadults with normal hearing and hearing loss, automatically obtained\nfrom glottal inverse filtering analysis using the Aalto Aparat toolkit.\nAalto Aparat has been employed in glottal flow analysis of disordered\nspeech, but to the best of the authors&#8217; knowledge, not as yet\nin hearing impaired voice analysis and assessment. Five speakers, three\nwomen and two men, with normal hearing (NH) and five speakers with\nprelingual profound hearing impairment (HI), matched for age and sex,\nproduced symmetrical /&#x2C8;pVpV/ disyllables, where V=/i, a, u/.\nA state-of-the-art method named quasi-closed phase analysis (QCP) is\noffered in Aparat and it is used to estimate the glottal source signal.\nGlottal source features were obtained using time- and frequency-domain\nparametrization methods and analysed statistically. The interpretation\nof the results attempts to shed light on potential differences between\nHI and NH phonation strategies, while advantages and limitations of\ninverse filtering methods in HI voice assessment are discussed.\n"
      ],
      "doi": "10.21437/Interspeech.2021-870",
      "author_area_id": "13",
      "author_area_label": " Speech, Voice, and Hearing Disorders:14 Special Sessions"
    },
    "huckvale21_interspeech": {
      "authors": [
        [
          "Mark",
          "Huckvale"
        ],
        [
          "Catinca",
          "Buciuleac"
        ]
      ],
      "title": "Automated Detection of Voice Disorder in the Saarbr&#252;cken Voice Database: Effects of Pathology Subset and Audio Materials",
      "original": "1507",
      "page_count": 5,
      "order": 287,
      "p1": "1399",
      "pn": "1403",
      "abstract": [
        "The Saarbr&#252;cken Voice Database contains speech and simultaneous\nelectroglottography recordings of 1002 speakers exhibiting a wide range\nof voice disorders, together with recordings of 851 controls. Previous\nstudies have used this database to build systems for automated detection\nof voice disorders and for differential diagnosis. These studies have\nvaried considerably in the subset of pathologies tested, the audio\nmaterials analyzed, the cross-validation method used and the performance\nmetric reported. This variation has made it hard to determine the most\npromising approaches to the problem of detecting voice disorders. In\nthis study we re-implement three recently published systems that have\nbeen trained to detect pathology using the SVD and compare their performance\non the same pathologies with the same audio materials using a common\ncross-validation protocol and performance metric. We show that under\nthis approach, there is much less difference in performance across\nsystems than in their original publication. We also show that voice\ndisorder detection on the basis of a short phrase gives similar performance\nto that based on a sequence of vowels of different pitch. Our evaluation\nprotocol may be useful for future studies on voice disorder detection\nwith the SVD.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1507"
    },
    "lulich21_interspeech": {
      "authors": [
        [
          "Steven M.",
          "Lulich"
        ],
        [
          "Rita R.",
          "Patel"
        ]
      ],
      "title": "Accelerometer-Based Measurements of Voice Quality in Children During Semi-Occluded Vocal Tract Exercise with a Narrow Straw in Air",
      "original": "1918",
      "page_count": 5,
      "order": 288,
      "p1": "1404",
      "pn": "1408",
      "abstract": [
        "Non-invasive measures of voice quality, such as H1-H2, rely on oral\nflow signals, inverse filtered speech signals, or corrections for the\neffects of formants. Voice quality measures play especially important\nroles in the assessment of voice disorders and the evaluation of treatment\nefficacy. One type of treatment that is increasingly common in voice\ntherapy, as well as in voice training for singers and actors, is semi-occluded\nvocal tract exercises (SOVTEs). The goal of SOVTEs is to change patterns\nof vocal fold vibration and thereby improve voice quality and vocal\nefficiency. Accelerometers applied to the skin of the neck have been\nused to investigate subglottal acoustics, to inverse-filter speech\nsignals, and to obtain voice quality metrics. This paper explores the\napplication of neck-skin accelerometers to measure voice quality without\noral flow, inverse filtering, or formant correction. Accelerometer-based\nmeasures (uncorrected K1-K2 and corrected K1*-K2*, analogous to microphone-based\nH1-H2 and H1*-H2*) were obtained from typically developing children\nwith healthy voice, before and during SOVTEs. Traditional microphone-based\nH1-H2 measures (corrected and uncorrected) were also obtained. Results\nshowed that K1-K2 and K1*-K2* were not substantially affected by vocal\ntract acoustic changes in formant frequencies.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1918",
      "author_area_id": "13",
      "author_area_label": " Speech, Voice, and Hearing Disorders:14 Special Sessions"
    },
    "perez21_interspeech": {
      "authors": [
        [
          "Matthew",
          "Perez"
        ],
        [
          "Amrit",
          "Romana"
        ],
        [
          "Angela",
          "Roberts"
        ],
        [
          "Noelle",
          "Carlozzi"
        ],
        [
          "Jennifer Ann",
          "Miner"
        ],
        [
          "Praveen",
          "Dayalu"
        ],
        [
          "Emily Mower",
          "Provost"
        ]
      ],
      "title": "Articulatory Coordination for Speech Motor Tracking in Huntington Disease",
      "original": "0688",
      "page_count": 5,
      "order": 289,
      "p1": "1409",
      "pn": "1413",
      "abstract": [
        "Huntington Disease (HD) is a progressive disorder which often manifests\nin motor impairment. Motor severity (captured via motor score) is a\nkey component in assessing overall HD severity. However, motor score\nevaluation involves in-clinic visits with a trained medical professional,\nwhich are expensive and not always accessible. Speech analysis provides\nan attractive avenue for tracking HD severity because speech is easy\nto collect remotely and provides insight into motor changes. HD speech\nis typically characterized as having irregular articulation. With this\nin mind, acoustic features that can capture vocal tract movement and\narticulatory coordination are particularly promising for characterizing\nmotor symptom progression in HD. In this paper, we present an experiment\nthat uses Vocal Tract Coordination (VTC) features extracted from read\nspeech to estimate a motor score. When using an elastic-net regression\nmodel, we find that VTC features significantly outperform other acoustic\nfeatures across varied-length audio segments, which highlights the\neffectiveness of these features for both short- and long-form reading\ntasks. Lastly, we analyze the F-value scores of VTC features to visualize\nwhich channels are most related to motor score. This work enables future\nresearch efforts to consider VTC features for acoustic analyses which\ntarget HD motor symptomatology tracking.\n"
      ],
      "doi": "10.21437/Interspeech.2021-688",
      "author_area_id": "13",
      "author_area_label": " Speech, Voice, and Hearing Disorders"
    },
    "ferrer21_interspeech": {
      "authors": [
        [
          "Carlos A.",
          "Ferrer"
        ],
        [
          "Efren",
          "Arag\u00f3n"
        ],
        [
          "Mar\u00eda E.",
          "Hdez-D\u00edaz"
        ],
        [
          "Marc S. de",
          "Bodt"
        ],
        [
          "Roman",
          "Cmejla"
        ],
        [
          "Marina",
          "Englert"
        ],
        [
          "Mara",
          "Behlau"
        ],
        [
          "Elmar",
          "N\u00f6th"
        ]
      ],
      "title": "Modeling Dysphonia Severity as a Function of Roughness and Breathiness Ratings in the GRBAS Scale",
      "original": "1540",
      "page_count": 5,
      "order": 290,
      "p1": "1414",
      "pn": "1418",
      "abstract": [
        "Dysphonia comprises many perceptually deviating aspects of voice, and\nits overall severity perception is made by the listener according to\nmethods of aggregating the single dimensions which are personally conceived\nand not well studied. Roughness and breathiness are constituent dimensions\nin most devised rating scales in clinical use. In this paper, we evaluate\nseveral ways to model the mapping of the overall severity as a function\nof the particular ratings of roughness and breathiness. The models\ninclude the simple linear averaging as well as several non-linear variants\nsuggested elsewhere, and some minor adjustments. The models are evaluated\non four datasets from different countries, allowing a more global evaluation\nof how the mapping is conceived.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  Results show the limitations\nof the most widely assumed linear approach, while also hinting at a\nneed for a more uniform coverage of the sample space in voice pathology\ndatasets. The models explored in this paper can be expanded to higher-dimensional\nscales.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1540",
      "author_area_id": "13",
      "author_area_label": " Speech, Voice, and Hearing Disorders:14 Special Sessions"
    },
    "karpov21_interspeech": {
      "authors": [
        [
          "Nikolay",
          "Karpov"
        ],
        [
          "Alexander",
          "Denisenko"
        ],
        [
          "Fedor",
          "Minkin"
        ]
      ],
      "title": "Golos: Russian Dataset for Speech Research",
      "original": "0462",
      "page_count": 5,
      "order": 291,
      "p1": "1419",
      "pn": "1423",
      "abstract": [
        "This paper introduces a novel Russian speech dataset called Golos,\na large corpus suitable for speech research. The dataset mainly consists\nof recorded audio files manually annotated on the crowd-sourcing platform.\nThe total duration of the audio is about 1240 hours. We have made the\ncorpus freely available to download, along with the acoustic model\nwith CTC loss prepared on this corpus. Additionally, transfer learning\nwas applied to improve the performance of the acoustic model. In order\nto evaluate the quality of the dataset with the beam-search algorithm,\nwe have built a 3-gram language model on the open Common Crawl dataset.\nThe total word error rate (WER) metrics turned out to be about 3.3%\nand 11.5%.\n"
      ],
      "doi": "10.21437/Interspeech.2021-462",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "sadhu21b_interspeech": {
      "authors": [
        [
          "Samik",
          "Sadhu"
        ],
        [
          "Hynek",
          "Hermansky"
        ]
      ],
      "title": "Radically Old Way of Computing Spectra: Applications in End-to-End ASR",
      "original": "0643",
      "page_count": 5,
      "order": 292,
      "p1": "1424",
      "pn": "1428",
      "abstract": [
        "We propose a technique to compute spectrograms using Frequency Domain\nLinear Prediction (FDLP) that uses all-pole models to fit the squared\nHilbert envelope of speech in different frequency sub-bands. The spectrogram\nof a complete speech utterance is computed by overlap-add of contiguous\nall-pole model responses. A long context window of 1.5 seconds allows\nus to capture the low frequency temporal modulations of speech in the\nspectrogram. For an end-to-end automatic speech recognition task, the\nFDLP spectrogram performs on par with the standard mel spectrogram\nfeatures for clean read speech training and test data. For more realistic\nspeech data with train-test domain mismatches or reverberations, FDLP\nspectrogram shows up to 25% and 22% relative WER improvements over\nmel spectrogram respectively.\n"
      ],
      "doi": "10.21437/Interspeech.2021-643",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "alghezi21_interspeech": {
      "authors": [
        [
          "Ragheb",
          "Al-Ghezi"
        ],
        [
          "Yaroslav",
          "Getman"
        ],
        [
          "Aku",
          "Rouhe"
        ],
        [
          "Raili",
          "Hild\u00e9n"
        ],
        [
          "Mikko",
          "Kurimo"
        ]
      ],
      "title": "Self-Supervised End-to-End ASR for Low Resource L2 Swedish",
      "original": "1710",
      "page_count": 5,
      "order": 293,
      "p1": "1429",
      "pn": "1433",
      "abstract": [
        "Unlike traditional (hybrid) Automatic Speech Recognition (ASR), end-to-end\nASR systems simplify the training procedure by directly mapping acoustic\nfeatures to sequences of graphemes or characters, thereby eliminating\nthe need for specialized acoustic, language, or pronunciation models.\nHowever, one drawback of end-to-end ASR systems is that they require\nmore training data than conventional ASR systems to achieve similar\nword error rate (WER). This makes it difficult to develop ASR systems\nfor tasks where transcribed target data is limited such as developing\nASR for Second Language (L2) speakers of Swedish. Nonetheless, recent\nadvancements in self-supervised acoustic learning, manifested in wav2vec\nmodels [1, 2, 3], leverage the available untranscribed speech data\nto provide compact acoustic representation that can achieve low WER\nwhen incorporated in end-to-end systems. To this end, we experiment\nwith several monolingual and cross-lingual self-supervised acoustic\nmodels to develop end-to-end ASR system for L2 Swedish. Even though\nour test is very small, it indicates that these systems are competitive\nin performance with traditional ASR pipeline. Our best model seems\nto reduce the WER by 7% relative to our traditional ASR baseline trained\non the same target data.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1710",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "oneill21_interspeech": {
      "authors": [
        [
          "Patrick K.",
          "O\u2019Neill"
        ],
        [
          "Vitaly",
          "Lavrukhin"
        ],
        [
          "Somshubra",
          "Majumdar"
        ],
        [
          "Vahid",
          "Noroozi"
        ],
        [
          "Yuekai",
          "Zhang"
        ],
        [
          "Oleksii",
          "Kuchaiev"
        ],
        [
          "Jagadeesh",
          "Balam"
        ],
        [
          "Yuliya",
          "Dovzhenko"
        ],
        [
          "Keenan",
          "Freyberg"
        ],
        [
          "Michael D.",
          "Shulman"
        ],
        [
          "Boris",
          "Ginsburg"
        ],
        [
          "Shinji",
          "Watanabe"
        ],
        [
          "Georg",
          "Kucsko"
        ]
      ],
      "title": "SPGISpeech: 5,000 Hours of Transcribed Financial Audio for Fully Formatted End-to-End Speech Recognition",
      "original": "1860",
      "page_count": 5,
      "order": 294,
      "p1": "1434",
      "pn": "1438",
      "abstract": [
        "In the English speech-to-text (STT) machine learning task, acoustic\nmodels are conventionally trained on uncased Latin characters, and\nany necessary orthography (such as capitalization, punctuation, and\ndenormalization of non-standard words) is imputed by separate post-processing\nmodels. This adds complexity and limits performance, as many formatting\ntasks benefit from semantic information present in the acoustic signal\nbut absent in transcription. Here we propose a new STT task: end-to-end\nneural transcription with fully formatted text for target labels. We\npresent baseline Conformer-based models trained on a corpus of 5,000\nhours of professionally transcribed earnings calls, achieving a CER\nof 1.7. As a contribution to the STT research community, we release\nthe corpus free for non-commercial use.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1860",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "evain21_interspeech": {
      "authors": [
        [
          "Sol\u00e8ne",
          "Evain"
        ],
        [
          "Ha",
          "Nguyen"
        ],
        [
          "Hang",
          "Le"
        ],
        [
          "Marcely Zanon",
          "Boito"
        ],
        [
          "Salima",
          "Mdhaffar"
        ],
        [
          "Sina",
          "Alisamir"
        ],
        [
          "Ziyi",
          "Tong"
        ],
        [
          "Natalia",
          "Tomashenko"
        ],
        [
          "Marco",
          "Dinarelli"
        ],
        [
          "Titouan",
          "Parcollet"
        ],
        [
          "Alexandre",
          "Allauzen"
        ],
        [
          "Yannick",
          "Est\u00e8ve"
        ],
        [
          "Benjamin",
          "Lecouteux"
        ],
        [
          "Fran\u00e7ois",
          "Portet"
        ],
        [
          "Solange",
          "Rossato"
        ],
        [
          "Fabien",
          "Ringeval"
        ],
        [
          "Didier",
          "Schwab"
        ],
        [
          "Laurent",
          "Besacier"
        ]
      ],
      "title": " <i>LeBenchmark</i>: A Reproducible Framework for Assessing Self-Supervised Representation Learning from Speech",
      "original": "0556",
      "page_count": 5,
      "order": 295,
      "p1": "1439",
      "pn": "1443",
      "abstract": [
        "Self-Supervised Learning (SSL) using huge unlabeled data has been successfully\nexplored for image and natural language processing. Recent works also\ninvestigated SSL from speech. They were notably successful to improve\nperformance on downstream tasks such as automatic speech recognition\n(ASR). While these works suggest it is possible to reduce dependence\non labeled data for building efficient speech systems, their evaluation\nwas mostly made on ASR and using multiple and heterogeneous experimental\nsettings (most of them for English). This questions the objective comparison\nof SSL approaches and the evaluation of their impact on building speech\nsystems. In this paper, we propose <i>LeBenchmark</i>: a reproducible\nframework for assessing SSL from speech. It not only includes ASR (high\nand low resource) tasks but also spoken language understanding, speech\ntranslation and emotion recognition. We also focus on speech technologies\nin a language different than English: French. SSL models of different\nsizes are trained from carefully sourced and documented datasets. Experiments\nshow that SSL is beneficial for most but not all tasks which confirms\nthe need for exhaustive and reliable benchmarks to evaluate its real\nimpact. <i>LeBenchmark</i> is shared with the scientific community\nfor reproducible research in SSL from speech.\n"
      ],
      "doi": "10.21437/Interspeech.2021-556"
    },
    "sturm21_interspeech": {
      "authors": [
        [
          "Pavel",
          "\u0160turm"
        ],
        [
          "Radek",
          "Skarnitzl"
        ],
        [
          "Tom\u00e1\u0161",
          "Nechansk\u00fd"
        ]
      ],
      "title": "Prosodic Accommodation in Face-to-Face and Telephone Dialogues",
      "original": "0130",
      "page_count": 5,
      "order": 296,
      "p1": "1444",
      "pn": "1448",
      "abstract": [
        "The study of phonetic accommodation in various communicative situations\nis still relatively limited. This paper examines accommodation in spontaneous\nconversations of eight pairs of Czech young male speakers in two communicative\nconditions: unconstrained face-to-face conversation and goal-oriented\ninteraction via mobile telephone. Articulation rate and measures of\nf0 level, range and variability were measured in 40 prosodic phrases\nper speaker in each condition. Analyses of LME models did not reveal\na significant global effect of time throughout the interaction on the\ndistance between speakers (convergence) in any of the examined parameters,\nor that of preceding phrase value on the subsequent turn-initial value\n(synchrony). However, more consistent patterns were observed when speaker\npairs were examined separately, revealing substantial individual variation\non the one hand and non-linear effects on the other. This shows that\naggregate analyses can be misleading in the study of phonetic accommodation\nand that speakers dynamically employ different strategies throughout\nnatural conversations.\n"
      ],
      "doi": "10.21437/Interspeech.2021-130",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "riverincoutlee21_interspeech": {
      "authors": [
        [
          "Josiane",
          "Riverin-Coutl\u00e9e"
        ],
        [
          "Concei\u00e7\u00e3o",
          "Cunha"
        ],
        [
          "Enkeleida",
          "Kapia"
        ],
        [
          "Jonathan",
          "Harrington"
        ]
      ],
      "title": "Dialect Features in Heterogeneous and Homogeneous Gheg Speaking Communities",
      "original": "1090",
      "page_count": 5,
      "order": 297,
      "p1": "1449",
      "pn": "1453",
      "abstract": [
        "This apparent and real time study analyses how dialect features in\nthe speech of children and adults are differently affected depending\non whether they live in homogeneous or heterogeneous speech communities.\nThe general hypotheses are that speakers in such high contact settings\nas heterogeneous urban centers are more prone to innovation than speakers\nin homogeneous tightly-knit communities, and that children accelerate\nleveling, especially through schooling and socialization. This study\nis of Gheg Albanian, a dialect spoken in and around the capital Tirana.\nTwo features were investigated: rounding of /a/ and vowel length contrasts.\nTwo groups of adults and children were compared: one from Tirana and\none from a nearby village. Additionally, the children were recorded\ntwice over a period of 12 months and were compared longitudinally.\nThe results showed that length contrasts were still present in both\ncommunities and age groups. Rounding of /a/ was lost in the city, but\nundergoing change in the village, with differences measured in apparent\ntime, but also in child speech within the 12-month span. Our study\nfurther raises the issue of combining both apparent and real time data\nwithin the same design.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1090",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "zellers21_interspeech": {
      "authors": [
        [
          "Margaret",
          "Zellers"
        ],
        [
          "Alena",
          "Witzlack-Makarevich"
        ],
        [
          "Lilja",
          "Saeboe"
        ],
        [
          "Saudah",
          "Namyalo"
        ]
      ],
      "title": "An Exploration of the Acoustic Space of Rhotics and Laterals in Ruruuli",
      "original": "1328",
      "page_count": 5,
      "order": 298,
      "p1": "1454",
      "pn": "1458",
      "abstract": [
        "Liquid consonants &#8212; rhotics and laterals &#8212; have been shown\nto demonstrate unique distributional patterns cross-linguistically.\nIt is also claimed that rhotics are more difficult to distinguish from\none another phonetically than laterals, and that rhotics are less flexible\nthan laterals when it comes to participation in consonant clusters\nand coarticulatory patterns. We investigate the phonetic realization\nof the rhotic and lateral phonemes in a Bantu language, Ruruuli. The\nacoustic space used for rhotics and laterals in this language is extremely\nsimilar, although the density peaks in terms of formant values are\ndifferent. Formant values as well as formant ratios can be reliably\nused to distinguish between rhotics and laterals. In common with many\nother languages, an asymmetry between laterals and rhotics is found\nin Ruruuli, with laterals being more positionally constrained than\nrhotics. The overlap in acoustic space between rhotics and laterals\nmay cast doubt on the status or stability of the phonological contrast\nbetween rhotics and laterals in this language.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1328",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "bodur21_interspeech": {
      "authors": [
        [
          "Kubra",
          "Bodur"
        ],
        [
          "Sweeney",
          "Branje"
        ],
        [
          "Morgane",
          "Peirolo"
        ],
        [
          "Ingrid",
          "Tiscareno"
        ],
        [
          "James S.",
          "German"
        ]
      ],
      "title": "Domain-Initial Strengthening in Turkish: Acoustic Cues to Prosodic Hierarchy in Stop Consonants",
      "original": "2230",
      "page_count": 5,
      "order": 299,
      "p1": "1459",
      "pn": "1463",
      "abstract": [
        "Studies have shown that cross-linguistically, consonants at the left\nedge of higher-level prosodic boundaries tend to be more forcefully\narticulated than those at lower-level boundaries, a phenomenon known\nas <i>domain-initial strengthening</i>. This study tests whether similar\neffects occur in Turkish, using the Autosegmental-Metrical model proposed\nby Ipek &amp; Jun [1, 2] as the basis for assessing boundary strength.\nProductions of /t/ and /d/ were elicited in four domain-initial prosodic\npositions corresponding to progressively higher-level boundaries: syllable,\nword, intermediate phrase, and Intonational Phrase. A fifth position,\nnuclear word, was included in order to better situate it within the\nprosodic hierarchy. Acoustic correlates of articulatory strength were\nmeasured, including closure duration for /d/ and /t/, as well as voice\nonset time and burst energy for /t/. Our results show that closure\nduration increases cumulatively from syllable to intermediate phrase,\nwhile voice onset time and burst energy are not influenced by boundary\nstrength. These findings provide corroborating evidence for Ipek &amp;\nJun&#8217;s model, particularly for the distinction between word and\nintermediate phrase boundaries. Additionally, articulatory strength\nat the left edge of the nuclear word patterned closely with word-initial\nposition, supporting the view that the nuclear word is not associated\nwith a distinct phrasing domain.\n"
      ],
      "doi": "10.21437/Interspeech.2021-2230",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "zmolikova21_interspeech": {
      "authors": [
        [
          "Katerina",
          "Zmolikova"
        ],
        [
          "Marc",
          "Delcroix"
        ],
        [
          "Desh",
          "Raj"
        ],
        [
          "Shinji",
          "Watanabe"
        ],
        [
          "Jan",
          "\u010cernock\u00fd"
        ]
      ],
      "title": "Auxiliary Loss Function for Target Speech Extraction and Recognition with Weak Supervision Based on Speaker Characteristics",
      "original": "0986",
      "page_count": 5,
      "order": 300,
      "p1": "1464",
      "pn": "1468",
      "abstract": [
        "Automatic speech recognition systems deteriorate in presence of overlapped\nspeech. A popular approach to alleviate this is target speech extraction.\nThe extraction system is usually trained with a loss function measuring\nthe discrepancy between the estimated and the reference target speech.\nThis often leads to distortions to the target signal which is detrimental\nto the recognition accuracy. Additionally, it is necessary to have\nthe strong supervision provided by parallel data consisting of speech\nmixtures and single-speaker signals. We propose an auxiliary loss function\nfor retraining the target speech extraction. It is composed of two\nparts: first, a speaker identity loss, forcing the estimated speech\nto have correct speaker characteristics, and second, a mixture consistency\nloss, making the extracted sources sum back to the original mixture.\nThe only supervision required for the proposed loss is speaker characteristics\nobtained from several segments spoken by the target speaker. Such weak\nsupervision makes the loss suitable for adapting the system directly\non real recordings. We show that the proposed loss yields signals more\nsuitable for speech recognition and further, we can gain additional\nimprovements by adaptation to target data. Overall, we can reduce the\nword error rate on LibriCSS dataset from 27.4% to 24.0%.\n"
      ],
      "doi": "10.21437/Interspeech.2021-986",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "borsdorf21_interspeech": {
      "authors": [
        [
          "Marvin",
          "Borsdorf"
        ],
        [
          "Chenglin",
          "Xu"
        ],
        [
          "Haizhou",
          "Li"
        ],
        [
          "Tanja",
          "Schultz"
        ]
      ],
      "title": "Universal Speaker Extraction in the Presence and Absence of Target Speakers for Speech of One and Two Talkers",
      "original": "1939",
      "page_count": 5,
      "order": 301,
      "p1": "1469",
      "pn": "1473",
      "abstract": [
        "Speaker extraction has been studied mostly for the scenarios where\na target speaker is present in a two or more talkers mixture. Such\nscenarios do not adequately reflect everyday conversations. For example,\na target speaker can be the only active talker, be quiet for a while,\nor leave the conversation, that means the target speaker is absent\nfrom the mixture. Traditional speaker extraction models fail in these\nscenarios. We propose a novel speaker extraction approach to handle\nspeech mixtures with one or two talkers in which the target speaker\ncan either be present or absent. First, we formulate four speaker extraction\nconditions to cover the typical scenarios of everyday conversations\nwith one and two talkers. Second, we introduce a joint training scheme\nwith one unified loss function that works for all four conditions.\nWe show that only a small amount of data is required to adapt the model\nto work well in the four conditions.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1939",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "mateju21_interspeech": {
      "authors": [
        [
          "Lukas",
          "Mateju"
        ],
        [
          "Frantisek",
          "Kynych"
        ],
        [
          "Petr",
          "Cerva"
        ],
        [
          "Jindrich",
          "Zdansky"
        ],
        [
          "Jiri",
          "Malek"
        ]
      ],
      "title": "Using X-Vectors for Speech Activity Detection in Broadcast Streams",
      "original": "0192",
      "page_count": 5,
      "order": 302,
      "p1": "1474",
      "pn": "1478",
      "abstract": [
        "A new approach to speech activity detection (SAD) is presented in this\nwork. It allows us to reduce the complexity and computation demands,\nnamely in services that process streaming speech, where a SAD module\nusually forms the first block of the data pipeline (e.g., in a platform\nfor 24/7 broadcast transcription). Our approach utilizes x-vectors\nas input features so that, within the subsequent pipeline stages, these\nembedding instances can also directly be employed for speaker diarization\nand recognition. The x-vectors are extracted by feed-forward sequential\nmemory network (FSMN), allowing for modeling long-time dependencies;\nthey thus form an input into a computationally undemanding binary classifier,\nwhose output is smoothed by a decoder. Evaluation is performed on the\nstandardized QUT-NOISE-TIMIT dataset as well as on broadcast data with\nlarge portions of music and background noise. The former data allows\nfor comparison with other existing approaches. The latter shows the\nperformance in terms of word error rate (WER) and reduction in real-time\nfactor (RTF) of the transcription process.\n"
      ],
      "doi": "10.21437/Interspeech.2021-192",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "salvati21_interspeech": {
      "authors": [
        [
          "Daniele",
          "Salvati"
        ],
        [
          "Carlo",
          "Drioli"
        ],
        [
          "Gian Luca",
          "Foresti"
        ]
      ],
      "title": "Time Delay Estimation for Speaker Localization Using CNN-Based Parametrized GCC-PHAT Features",
      "original": "0988",
      "page_count": 5,
      "order": 303,
      "p1": "1479",
      "pn": "1483",
      "abstract": [
        "We propose a time delay estimation (TDE) method for speaker localization\nbased on parametrized generalized cross-correlation phase transform\n(PGCC-PHAT) functions and convolutional neural networks (CNNs). The\nPGCC-PHAT is used to build a feature matrix, which gives TDE information\nof two microphone signals with different normalization levels in the\ncross-correlation functions. The feature matrix is processed by a CNN,\ncomposed by several convolutional layers and fully connected layers\nand by a regression output for the directly estimation of the time\ndifference of arrival (TDOA). Simulations in noisy and reverberant\nadverse conditions show that the proposed method improves the TDOA\nestimation performance if compared to the GCC-PHAT.\n"
      ],
      "doi": "10.21437/Interspeech.2021-988",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "yousefi21_interspeech": {
      "authors": [
        [
          "Midia",
          "Yousefi"
        ],
        [
          "John H.L.",
          "Hansen"
        ]
      ],
      "title": "Real-Time Speaker Counting in a Cocktail Party Scenario Using Attention-Guided Convolutional Neural Network",
      "original": "0331",
      "page_count": 5,
      "order": 304,
      "p1": "1484",
      "pn": "1488",
      "abstract": [
        "Most current speech technology systems are designed to operate well\neven in the presence of multiple active speakers. However, most solutions\nassume that the number of co-current speakers is known. Unfortunately,\nthis information might not always be available in real-world applications.\nIn this study, we propose a real-time, single-channel attention-guided\nConvolutional Neural Network (CNN) to estimate the number of active\nspeakers in overlapping speech. The proposed system extracts higher-level\ninformation from the speech spectral content using a CNN model. Next,\nthe attention mechanism summarizes the extracted information into a\ncompact feature vector without losing critical information. Finally,\nthe active speakers are classified using a fully connected network.\nExperiments on simulated overlapping speech using WSJ corpus show that\nthe attention solution is shown to improve the performance by almost\n3% absolute over conventional temporal average pooling. The proposed\nAttention-guided CNN achieves 76.15% for both Weighted Accuracy and\naverage Recall, and 75.80% Precision on speech segments as short as\n20 frames (i.e., 200 ms). All the classification metrics exceed 92%\nfor the attention-guided model in offline scenarios where the input\nsignal is more than 100 frames long (i.e., 1s).\n"
      ],
      "doi": "10.21437/Interspeech.2021-331",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "liu21d_interspeech": {
      "authors": [
        [
          "Hexin",
          "Liu"
        ],
        [
          "Leibny Paola Garc\u00eda",
          "Perera"
        ],
        [
          "Xinyi",
          "Zhang"
        ],
        [
          "Justin",
          "Dauwels"
        ],
        [
          "Andy W.H.",
          "Khong"
        ],
        [
          "Sanjeev",
          "Khudanpur"
        ],
        [
          "Suzy J.",
          "Styles"
        ]
      ],
      "title": "End-to-End Language Diarization for Bilingual Code-Switching Speech",
      "original": "0082",
      "page_count": 5,
      "order": 305,
      "p1": "1489",
      "pn": "1493",
      "abstract": [
        "We propose two end-to-end neural configurations for language diarization\non bilingual code-switching speech. The first, a BLSTM-E2E architecture,\nincludes a set of stacked bidirectional LSTMs to compute embeddings\nand incorporates the deep clustering loss to enforce grouping of languages\nbelonging to the same class. The second, an XSA-E2E architecture, is\nbased on an x-vector model followed by a self-attention encoder. The\nformer encodes frame-level features into segment-level embeddings while\nthe latter considers all those embeddings to generate a sequence of\nsegment-level language labels. We evaluated the proposed methods on\nthe dataset obtained from the shared task B in WSTCSMC 2020 and our\nhandcrafted simulated data from the SEAME dataset. Experimental results\nshow that our proposed XSA-E2E architecture achieved a relative improvement\nof 12.1% in equal error rate and a 7.4% relative improvement on accuracy\ncompared with the baseline algorithm in the WSTCSMC 2020 dataset. Our\nproposed XSA-E2E architecture achieved an accuracy of 89.84% with a\nbaseline of 85.60% on the simulated data derived from the SEAME dataset.\n"
      ],
      "doi": "10.21437/Interspeech.2021-82",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "duroselle21_interspeech": {
      "authors": [
        [
          "Rapha\u00ebl",
          "Duroselle"
        ],
        [
          "Md.",
          "Sahidullah"
        ],
        [
          "Denis",
          "Jouvet"
        ],
        [
          "Irina",
          "Illina"
        ]
      ],
      "title": "Modeling and Training Strategies for Language Recognition Systems",
      "original": "0277",
      "page_count": 5,
      "order": 306,
      "p1": "1494",
      "pn": "1498",
      "abstract": [
        "Automatic speech recognition is complementary to language recognition.\nThe language recognition systems exploit this complementarity by using\nframe-level bottleneck features extracted from neural networks trained\nwith a phone recognition task. Recent methods apply frame-level bottleneck\nfeatures extracted from an end-to-end sequence-to-sequence speech recognition\nmodel. In this work, we study an integrated approach of the training\nof the speech recognition feature extractor and language recognition\nmodules. We show that for both classical phone recognition and end-to-end\nsequence-to-sequence features, sequential training of the two modules\nis not the optimal strategy. The feature extractor can be improved\nby supervision with the language identification loss, either in a fine-tuning\nstep or in a multi-task training framework. Besides, we notice that\nend-to-end sequence-to-sequence bottleneck features are on par with\nclassical phone recognition bottleneck features without requiring a\nforced alignment of the signal with target tokens. However, for sequence-to-sequence,\nthe architecture of the model seems to play an important role; the\nConformer architectures leads to much better results than the conventional\nstacked DNNs approach; and can even be trained directly with the LID\nmodule in an end-to-end approach.\n"
      ],
      "doi": "10.21437/Interspeech.2021-277",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "wang21o_interspeech": {
      "authors": [
        [
          "Hui",
          "Wang"
        ],
        [
          "Lin",
          "Liu"
        ],
        [
          "Yan",
          "Song"
        ],
        [
          "Lei",
          "Fang"
        ],
        [
          "Ian",
          "McLoughlin"
        ],
        [
          "Li-Rong",
          "Dai"
        ]
      ],
      "title": "A Weight Moving Average Based Alternate Decoupled Learning Algorithm for Long-Tailed Language Identification",
      "original": "0776",
      "page_count": 5,
      "order": 307,
      "p1": "1499",
      "pn": "1503",
      "abstract": [
        "Language identification (LID) research has made tremendous progress\nin recent years, especially with the introduction of deep learning\ntechniques. However, for real-world applications where the distribution\nof different language data is highly imbalanced, the performance of\nexisting LID systems is still far from satisfactory. This raises the\nchallenge of <i>long-tailed LID</i>. In this paper, we propose an effective\nweight moving average (WMA) based alternate decoupled learning algorithm,\ntermed WADCL, for long-tailed LID. The system is divided into two components,\na frontend feature extractor and a backend classifier. These are then\nalternately learned in an end-to-end manner using different sampling\nschemes to alleviate the distribution mismatch between training and\ntest datasets. Furthermore, our WMA method aims to mitigate the side-effects\nof re-sampling schemes, by fusing the model parameters learned along\nthe trajectory of stochastic gradient descent (SGD) optimization. To\nvalidate the effectiveness of the proposed WADCL algorithm, we evaluate\nand compare several systems over a language dataset constructed to\nmatch a long-tailed distribution based on real world application [1].\nThe experimental results from the long-tailed language dataset demonstrate\nthat the proposed algorithm is able to achieve significant performance\ngains over existing state-of-the-art x-vector based LID methods.\n"
      ],
      "doi": "10.21437/Interspeech.2021-776",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "deng21b_interspeech": {
      "authors": [
        [
          "Keqi",
          "Deng"
        ],
        [
          "Songjun",
          "Cao"
        ],
        [
          "Long",
          "Ma"
        ]
      ],
      "title": "Improving Accent Identification and Accented Speech Recognition Under a Framework of Self-Supervised Learning",
      "original": "1186",
      "page_count": 5,
      "order": 308,
      "p1": "1504",
      "pn": "1508",
      "abstract": [
        "Recently, self-supervised pre-training has gained success in automatic\nspeech recognition (ASR). However, considering the difference between\nspeech accents in real scenarios, how to identify accents and use accent\nfeatures to improve ASR is still challenging. In this paper, we employ\nthe self-supervised pre-training method for both accent identification\nand accented speech recognition tasks. For the former task, a standard\ndeviation constraint loss (SDC-loss) based end-to-end (E2E) architecture\nis proposed to identify accents under the same language. As for accented\nspeech recognition task, we design an accent-dependent ASR system,\nwhich can utilize additional accent input features. Furthermore, we\npropose a frame-level accent feature, which is extracted based on the\nproposed accent identification model and can be dynamically adjusted.\nWe pre-train our models using 960 hours unlabeled LibriSpeech dataset\nand fine-tune them on AESRC2020 speech dataset. The experimental results\nshow that our proposed accent-dependent ASR system is significantly\nahead of the AESRC2020 baseline and achieves 6.5% relative word error\nrate (WER) reduction compared with our accent-independent ASR system.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1186",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "fan21_interspeech": {
      "authors": [
        [
          "Zhiyun",
          "Fan"
        ],
        [
          "Meng",
          "Li"
        ],
        [
          "Shiyu",
          "Zhou"
        ],
        [
          "Bo",
          "Xu"
        ]
      ],
      "title": "Exploring wav2vec 2.0 on Speaker Verification and Language Identification",
      "original": "1280",
      "page_count": 5,
      "order": 309,
      "p1": "1509",
      "pn": "1513",
      "abstract": [
        "wav2vec 2.0 is a recently proposed self-supervised framework for speech\nrepresentation learning. It follows a two-stage training process of\npre-training and fine-tuning, and performs well in speech recognition\ntasks especially ultra-low resource cases. In this work, we attempt\nto extend the self-supervised framework to speaker verification and\nlanguage identification. First, we use some preliminary experiments\nto indicate that wav2vec 2.0 can capture the information about the\nspeaker and language. Then we demonstrate the effectiveness of wav2vec\n2.0 on the two tasks respectively. For speaker verification, we obtain\na competitive result with the Equal Error Rate (EER) of 3.61% on the\nVoxCeleb1 dataset. For language identification, we obtain an EER of\n12.02% on the 1 second condition and an EER of 3.47% on the full-length\ncondition of the AP17-OLR dataset. Finally, we utilize one model to\nachieve the unified modeling by the multi-task learning for the two\ntasks.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1280",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "ramesh21_interspeech": {
      "authors": [
        [
          "G.",
          "Ramesh"
        ],
        [
          "C. Shiva",
          "Kumar"
        ],
        [
          "K. Sri Rama",
          "Murty"
        ]
      ],
      "title": "Self-Supervised Phonotactic Representations for Language Identification",
      "original": "1310",
      "page_count": 5,
      "order": 310,
      "p1": "1514",
      "pn": "1518",
      "abstract": [
        "Phonotactic constraints characterize the sequence of permissible phoneme\nstructures in a language and hence form an important cue for language\nidentification (LID) task. As phonotactic constraints span across multiple\nphonemes, the short-term spectral analysis (20&#8211;30 ms) alone is\nnot sufficient to capture them. The speech signal has to be analyzed\nover longer contexts (100s of milliseconds) in order to extract features\nrepresenting the phonotactic constraints. The supervised senone classifiers,\naimed at modeling triphone context, have been used for extracting language-specific\nfeatures for the LID task. However, it is difficult to get large amounts\nof manually labeled data to train the supervised models. In this work,\nwe explore a self-supervised approach to extract long-term contextual\nfeatures for the LID task. We have used wav2vec architecture to extract\ncontextualized representations from multiple frames of the speech signal.\nThe contextualized representations extracted from the pre-trained wav2vec\nmodel are used for the LID task. The performance of the proposed features\nis evaluated on a dataset containing 7 Indian languages. The proposed\nself-supervised embeddings achieved 23% absolute improvement over the\nacoustic features and 3% absolute improvement over their supervised\ncounterparts.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1310",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "zhang21j_interspeech": {
      "authors": [
        [
          "Jicheng",
          "Zhang"
        ],
        [
          "Yizhou",
          "Peng"
        ],
        [
          "Van Tung",
          "Pham"
        ],
        [
          "Haihua",
          "Xu"
        ],
        [
          "Hao",
          "Huang"
        ],
        [
          "Eng Siong",
          "Chng"
        ]
      ],
      "title": "E2E-Based Multi-Task Learning Approach to Joint Speech and Accent Recognition",
      "original": "1495",
      "page_count": 5,
      "order": 311,
      "p1": "1519",
      "pn": "1523",
      "abstract": [
        "In this paper, we propose a single multi-task learning framework to\nperform End-to-End (E2E) speech recognition (ASR) and accent recognition\n(AR) simultaneously. The proposed framework is not only more compact\nbut can also yield comparable or even better results than standalone\nsystems. Specifically, we found that the overall performance is predominantly\ndetermined by the ASR task, and the E2E-based ASR pretraining is essential\nto achieve improved performance, particularly for the AR task. Additionally,\nwe conduct several analyses of the proposed method. First, though the\nobjective loss for the AR task is much smaller compared with its counterpart\nof ASR task, a smaller weighting factor with the AR task in the joint\nobjective function is necessary to yield better results for each task.\nSecond, we found that sharing only a few layers of the encoder yields\nbetter AR results than sharing the overall encoder. Experimentally,\nthe proposed method produces WER results close to the best standalone\nE2E ASR ones, while it achieves 7.7% and 4.2% relative improvement\nover standalone and single-task-based joint recognition methods on\ntest set for accent recognition respectively.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1495",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "tzudir21_interspeech": {
      "authors": [
        [
          "Moakala",
          "Tzudir"
        ],
        [
          "Shikha",
          "Baghel"
        ],
        [
          "Priyankoo",
          "Sarmah"
        ],
        [
          "S.R. Mahadeva",
          "Prasanna"
        ]
      ],
      "title": "Excitation Source Feature Based Dialect Identification in Ao &#8212; A Low Resource Language",
      "original": "1672",
      "page_count": 5,
      "order": 312,
      "p1": "1524",
      "pn": "1528",
      "abstract": [
        "Ao is an under-resourced Tibeto-Burman tonal language spoken in Nagaland,\nIndia. There are three distinct dialects of the language, namely, Chungli,\nMongsen and Changki. The objective of dialect identification is to\nidentify one dialect from the other within the same language family.\nThe goal of this study is to ascertain the potential of excitation\nsource features for automatic dialect identification in Ao. In this\ndirection, Integrated Linear Prediction Residual (ILPR), an approximate\nrepresentation of source signal, is explored. The log Mel spectrogram\nof ILPR (<i>S<SUB>Ext</SUB></i>) signal is used to exploit the time-frequency\ncharacteristics of the excitation source. This work proposes attention\nbased CNN-BiGRU architecture for automatic dialect identification tasks.\nAdditionally, log Mel spectrogram (<i>S<SUB>VT</SUB></i>), extracted\nfrom the pre-emphasized speech signal, is used as a baseline method.\nThe (<i>S<SUB>VT</SUB></i>) contains the vocal-tract characteristics\nof the speech signal. A significant performance improvement of (nearly)\n6% accuracy is observed when the excitation source feature (<i>S<SUB>Ext</SUB></i>)\nis combined with the vocal tract representation (<i>S<SUB>VT</SUB></i>).\nTo analyse the effect of segment duration, dialect identification performance\nis reported for three different durations, viz., 1 sec, 3 sec and 6\nsec. The effect of gender in dialect identification task for Ao is\nalso studied in this work.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1672",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "khare21_interspeech": {
      "authors": [
        [
          "Shreya",
          "Khare"
        ],
        [
          "Ashish",
          "Mittal"
        ],
        [
          "Anuj",
          "Diwan"
        ],
        [
          "Sunita",
          "Sarawagi"
        ],
        [
          "Preethi",
          "Jyothi"
        ],
        [
          "Samarth",
          "Bharadwaj"
        ]
      ],
      "title": "Low Resource ASR: The Surprising Effectiveness of High Resource Transliteration",
      "original": "2062",
      "page_count": 5,
      "order": 313,
      "p1": "1529",
      "pn": "1533",
      "abstract": [
        "Cross-lingual transfer of knowledge from high-resource languages to\nlow-resource languages is an important research problem in automatic\nspeech recognition (ASR). We propose a new strategy of transfer learning\nby pretraining using large amounts of speech in the high-resource language\nbut with its text transliterated to the target low-resource language.\nThis simple mapping of scripts explicitly encourages increased sharing\nbetween the output spaces of both languages and is surprisingly effective\neven when the high-resource and low-resource languages are from unrelated\nlanguage families. The utility of our proposed technique is more evident\nin very low-resource scenarios, where better initializations are more\nbeneficial. We evaluate our technique on a transformer ASR architecture\nand the state-of-the-art wav2vec2.0 ASR architecture, with English\nas the high-resource language and six languages as low-resource targets.\nWith access to 1 hour of target speech, we obtain relative WER reductions\nof up to 8.2% compared to existing transfer-learning approaches.\n"
      ],
      "doi": "10.21437/Interspeech.2021-2062",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "feng21_interspeech": {
      "authors": [
        [
          "Siyuan",
          "Feng"
        ],
        [
          "Piotr",
          "\u017belasko"
        ],
        [
          "Laureano",
          "Moro-Vel\u00e1zquez"
        ],
        [
          "Odette",
          "Scharenborg"
        ]
      ],
      "title": "Unsupervised Acoustic Unit Discovery by Leveraging a Language-Independent Subword Discriminative Feature Representation",
      "original": "1664",
      "page_count": 5,
      "order": 314,
      "p1": "1534",
      "pn": "1538",
      "abstract": [
        "This paper tackles automatically discovering phone-like acoustic units\n(AUD) from unlabeled speech data. Past studies usually proposed single-step\napproaches. We propose a two-stage approach: the first stage learns\na subword-discriminative feature representation, and the second stage\napplies clustering to the learned representation and obtains phone-like\nclusters as the discovered acoustic units. In the first stage, a recently\nproposed method in the task of unsupervised subword modeling is improved\nby replacing a monolingual out-of-domain (OOD) ASR system with a multilingual\none to create a subword-discriminative representation that is more\nlanguage-independent. In the second stage, segment-level k-means is\nadopted, and two methods to represent the variable-length speech segments\nas fixed-dimension feature vectors are compared. Experiments on a very\nlow-resource Mboshi language corpus show that our approach outperforms\nstate-of-the-art AUD in both normalized mutual information (NMI) and\nF-score. The multilingual ASR improved upon the monolingual ASR in\nproviding OOD phone labels and in estimating the phone boundaries.\nA comparison of our systems with and without knowing the ground-truth\nphone boundaries showed a 16% NMI performance gap, suggesting that\nthe current approach can significantly benefit from improved phone\nboundary estimation.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1664",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "kamper21_interspeech": {
      "authors": [
        [
          "Herman",
          "Kamper"
        ],
        [
          "Benjamin van",
          "Niekerk"
        ]
      ],
      "title": "Towards Unsupervised Phone and Word Segmentation Using Self-Supervised Vector-Quantized Neural Networks",
      "original": "0050",
      "page_count": 5,
      "order": 315,
      "p1": "1539",
      "pn": "1543",
      "abstract": [
        "We investigate segmenting and clustering speech into low-bitrate phone-like\nsequences without supervision. We specifically constrain pretrained\nself-supervised vector-quantized (VQ) neural networks so that blocks\nof contiguous feature vectors are assigned to the same code, thereby\ngiving a variable-rate segmentation of the speech into discrete units.\nTwo segmentation methods are considered. In the first, features are\ngreedily merged until a prespecified number of segments are reached.\nThe second uses dynamic programming to optimize a squared error with\na penalty term to encourage fewer but longer segments. We show that\nthese VQ segmentation methods can be used without alteration across\na wide range of tasks: unsupervised phone segmentation, ABX phone discrimination,\nsame-different word discrimination, and as inputs to a symbolic word\nsegmentation algorithm. The penalized dynamic programming method generally\nperforms best. While performance on individual tasks is only comparable\nto the state-of-the-art in some cases, in all tasks a reasonable competing\napproach is outperformed at a substantially lower bitrate.\n"
      ],
      "doi": "10.21437/Interspeech.2021-50",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "jiang21_interspeech": {
      "authors": [
        [
          "Dongwei",
          "Jiang"
        ],
        [
          "Wubo",
          "Li"
        ],
        [
          "Miao",
          "Cao"
        ],
        [
          "Wei",
          "Zou"
        ],
        [
          "Xiangang",
          "Li"
        ]
      ],
      "title": "Speech SimCLR: Combining Contrastive and Reconstruction Objective for Self-Supervised Speech Representation Learning",
      "original": "0391",
      "page_count": 5,
      "order": 316,
      "p1": "1544",
      "pn": "1548",
      "abstract": [
        "Self-supervised visual pretraining has shown significant progress recently.\nAmong those methods, SimCLR greatly advanced the state of the art in\nself-supervised and semi-supervised learning on ImageNet. The input\nfeature representations for speech and visual tasks are both continuous,\nso it is natural to consider applying similar objective on speech representation\nlearning. In this paper, we propose Speech SimCLR, a new self-supervised\nobjective for speech representation learning. During training, Speech\nSimCLR applies augmentation on raw speech and its spectrogram. Its\nobjective is the combination of contrastive loss that maximizes agreement\nbetween differently augmented samples in the latent space and reconstruction\nloss of input representation. The proposed method achieved competitive\nresults on speech emotion recognition and speech recognition.\n"
      ],
      "doi": "10.21437/Interspeech.2021-391",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "jacobs21_interspeech": {
      "authors": [
        [
          "Christiaan",
          "Jacobs"
        ],
        [
          "Herman",
          "Kamper"
        ]
      ],
      "title": "Multilingual Transfer of Acoustic Word Embeddings Improves When Training on Languages Related to the Target Zero-Resource Language",
      "original": "0461",
      "page_count": 5,
      "order": 317,
      "p1": "1549",
      "pn": "1553",
      "abstract": [
        "Acoustic word embedding models map variable duration speech segments\nto fixed dimensional vectors, enabling efficient speech search and\ndiscovery. Previous work explored how embeddings can be obtained in\nzero-resource settings where no labelled data is available in the target\nlanguage. The current best approach uses transfer learning: a single\nsupervised multilingual model is trained using labelled data from multiple\nwell-resourced languages and then applied to a target zero-resource\nlanguage (without fine-tuning). However, it is still unclear how the\nspecific choice of training languages affect downstream performance.\nConcretely, here we ask whether it is beneficial to use training languages\nrelated to the target. Using data from eleven languages spoken in Southern\nAfrica, we experiment with adding data from different language families\nwhile controlling for the amount of data per language. In word discrimination\nand query-by-example search evaluations, we show that training on languages\nfrom the same family gives large improvements. Through finer-grained\nanalysis, we show that training on even just a single related language\ngives the largest gain. We also find that adding data from unrelated\nlanguages generally doesn&#8217;t hurt performance.\n"
      ],
      "doi": "10.21437/Interspeech.2021-461",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "niekerk21_interspeech": {
      "authors": [
        [
          "Benjamin van",
          "Niekerk"
        ],
        [
          "Leanne",
          "Nortje"
        ],
        [
          "Matthew",
          "Baas"
        ],
        [
          "Herman",
          "Kamper"
        ]
      ],
      "title": "Analyzing Speaker Information in Self-Supervised Models to Improve Zero-Resource Speech Processing",
      "original": "1182",
      "page_count": 5,
      "order": 318,
      "p1": "1554",
      "pn": "1558",
      "abstract": [
        "Contrastive predictive coding (CPC) aims to learn representations of\nspeech by distinguishing future observations from a set of negative\nexamples. Previous work has shown that linear classifiers trained on\nCPC features can accurately predict speaker and phone labels. However,\nit is unclear how the features actually capture speaker and phonetic\ninformation, and whether it is possible to normalize out the irrelevant\ndetails (depending on the downstream task). In this paper, we first\nshow that the per-utterance mean of CPC features captures speaker information\nto a large extent. Concretely, we find that comparing means performs\nwell on a speaker verification task. Next, probing experiments show\nthat standardizing the features effectively removes speaker information.\nBased on this observation, we propose a speaker normalization step\nto improve acoustic unit discovery using K-means clustering of CPC\nfeatures. Finally, we show that a language model trained on the resulting\nunits achieves some of the best results in the ZeroSpeech2021 Challenge.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1182",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications:14 Special Sessions"
    },
    "takahashi21_interspeech": {
      "authors": [
        [
          "Shun",
          "Takahashi"
        ],
        [
          "Sakriani",
          "Sakti"
        ],
        [
          "Satoshi",
          "Nakamura"
        ]
      ],
      "title": "Unsupervised Neural-Based Graph Clustering for Variable-Length Speech Representation Discovery of Zero-Resource Languages",
      "original": "1340",
      "page_count": 5,
      "order": 319,
      "p1": "1559",
      "pn": "1563",
      "abstract": [
        "Discovering symbolic units from unannotated speech data is fundamental\nin zero resource speech technology. Previous studies focused on learning\nfixed-length frame units based on acoustic features. Although they\nachieve high quality, they also suffer from a high bit-rate due to\ntime-frame encoding. In this work, to discover variable-length, low\nbit-rate speech representation from a limited amount of unannotated\nspeech data, we propose an approach based on graph neural networks\n(GNNs), and we study the temporal closeness of salient speech features.\nOur approach is built upon vector-quantized neural networks (VQNNs),\nwhich learn discrete encoding by contrastive predictive coding (CPC).\nWe exploit the predetermined finite set of embeddings (a codebook)\nused by VQNNs to encode input data. We consider a codebook a set of\nnodes in a directed graph, where each arc represents the transition\nfrom one feature to another. Subsequently, we extract and encode the\ntopological features of nodes in the graph to cluster them using graph\nconvolution. By this process, we can obtain coarsened speech representation.\nWe evaluated our model on the English data set of the ZeroSpeech 2020\nchallenge on Track 2019. Our model successfully drops the bit rate\nwhile achieving high unit quality.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1340",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "maekaku21_interspeech": {
      "authors": [
        [
          "Takashi",
          "Maekaku"
        ],
        [
          "Xuankai",
          "Chang"
        ],
        [
          "Yuya",
          "Fujita"
        ],
        [
          "Li-Wei",
          "Chen"
        ],
        [
          "Shinji",
          "Watanabe"
        ],
        [
          "Alexander",
          "Rudnicky"
        ]
      ],
      "title": "Speech Representation Learning Combining Conformer CPC with Deep Cluster for the ZeroSpeech Challenge 2021",
      "original": "1503",
      "page_count": 5,
      "order": 320,
      "p1": "1564",
      "pn": "1568",
      "abstract": [
        "We present a system for the Zero Resource Speech Challenge 2021, which\ncombines a Contrastive Predictive Coding (CPC) with deep cluster. In\ndeep cluster, we first prepare pseudo-labels obtained by clustering\nthe outputs of a CPC network with k-means. Then, we train an additional\nautoregressive model to classify the previously obtained pseudo-labels\nin a supervised manner. Phoneme discriminative representation is achieved\nby executing the second-round clustering with the outputs of the final\nlayer of the autoregressive model. We show that replacing a Transformer\nlayer with a Conformer layer leads to a further gain in a lexical metric.\nExperimental results show that a relative improvement of 35% in a phonetic\nmetric, 1.5% in the lexical metric, and 2.3% in a syntactic metric\nare achieved compared to a baseline method of CPC-small which is trained\non LibriSpeech 460h data. We achieve top results in this challenge\nwith the syntactic metric.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1503",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications:14 Special Sessions"
    },
    "cui21_interspeech": {
      "authors": [
        [
          "Xia",
          "Cui"
        ],
        [
          "Amila",
          "Gamage"
        ],
        [
          "Terry",
          "Hanley"
        ],
        [
          "Tingting",
          "Mu"
        ]
      ],
      "title": "Identifying Indicators of Vulnerability from Short Speech Segments Using Acoustic and Textual Features",
      "original": "1525",
      "page_count": 5,
      "order": 321,
      "p1": "1569",
      "pn": "1573",
      "abstract": [
        "In order to protect vulnerable people in telemarketing, organisations\nhave to investigate the speech recordings to identify them first. Typically,\nthe investigation is manually conducted. As such, the procedure is\ncostly and time-consuming. With an automatic vulnerability detection\nsystem, more vulnerable people can be identified and protected. A standard\ntelephone conversation lasts around 5 minutes, the detection system\nis expected to be able to identify such a potential vulnerable speaker\nfrom speech segments. Due to the complexity of the vulnerability definition\nand the unavailable annotated vulnerability examples, this paper attempts\nto address the detection problem as three classification tasks: age\nclassification, accent classification and patient/non-patient classification\nutilising publicly available datasets. In the proposed system, we trained\nthree sub models using acoustic and textual features for each sub task.\nEach trained model was evaluated on multiple datasets and achieved\ncompetitive results compared to a strong baseline (i.e. in-dataset\naccuracy).\n"
      ],
      "doi": "10.21437/Interspeech.2021-1525",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "dunbar21_interspeech": {
      "authors": [
        [
          "Ewan",
          "Dunbar"
        ],
        [
          "Mathieu",
          "Bernard"
        ],
        [
          "Nicolas",
          "Hamilakis"
        ],
        [
          "Tu Anh",
          "Nguyen"
        ],
        [
          "Maureen de",
          "Seyssel"
        ],
        [
          "Patricia",
          "Roz\u00e9"
        ],
        [
          "Morgane",
          "Rivi\u00e8re"
        ],
        [
          "Eugene",
          "Kharitonov"
        ],
        [
          "Emmanuel",
          "Dupoux"
        ]
      ],
      "title": "The Zero Resource Speech Challenge 2021: Spoken Language Modelling",
      "original": "1755",
      "page_count": 5,
      "order": 322,
      "p1": "1574",
      "pn": "1578",
      "abstract": [
        "We present the Zero Resource Speech Challenge 2021, which asks participants\nto learn a language model directly from audio, without any text or\nlabels. The challenge is based on the Libri-light dataset, which provides\nup to 60k hours of audio from English audio books without any associated\ntext. We provide a pipeline baseline system consisting on an encoder\nbased on contrastive predictive coding (CPC), a quantizer (k-means)\nand a standard language model (BERT or LSTM). The metrics evaluate\nthe learned representations at the acoustic (ABX discrimination), lexical\n(spot-the-word), syntactic (acceptability judgment) and semantic levels\n(similarity judgment). We present an overview of the eight submitted\nsystems from four groups and discuss the main results.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1755"
    },
    "gudur21_interspeech": {
      "authors": [
        [
          "Gautham Krishna",
          "Gudur"
        ],
        [
          "Satheesh Kumar",
          "Perepu"
        ]
      ],
      "title": "Zero-Shot Federated Learning with New Classes for Audio Classification",
      "original": "2264",
      "page_count": 5,
      "order": 323,
      "p1": "1579",
      "pn": "1583",
      "abstract": [
        "Federated learning is an effective way of extracting insights from\ndifferent user devices while preserving the privacy of users. However,\nnew classes with completely unseen data distributions can stream across\nany device in a federated learning setting, whose data cannot be accessed\nby the global server or other users. To this end, we propose a unified\nzero-shot framework to handle these aforementioned challenges during\nfederated learning. We simulate two scenarios here &#8212; 1) when\nthe new class labels are not reported by the user, the traditional\nFL setting is used; 2) when new class labels are reported by the user,\nwe synthesize <i>Anonymized Data Impressions</i> by calculating class\nsimilarity matrices corresponding to each device&#8217;s new classes\nfollowed by unsupervised clustering to distinguish between new classes\nacross different users. Moreover, our proposed framework can also handle\nstatistical heterogeneities in both labels and models across the participating\nusers. We empirically evaluate our framework on-device across different\ncommunication rounds (FL iterations) with new classes in both local\nand global updates, along with heterogeneous labels and models, on\ntwo widely used audio classification applications &#8212; keyword spotting\nand urban sound classification, and observe an average deterministic\naccuracy increase of &#126;4.041% and &#126;4.258% respectively.\n"
      ],
      "doi": "10.21437/Interspeech.2021-2264",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "rouditchenko21_interspeech": {
      "authors": [
        [
          "Andrew",
          "Rouditchenko"
        ],
        [
          "Angie",
          "Boggust"
        ],
        [
          "David",
          "Harwath"
        ],
        [
          "Brian",
          "Chen"
        ],
        [
          "Dhiraj",
          "Joshi"
        ],
        [
          "Samuel",
          "Thomas"
        ],
        [
          "Kartik",
          "Audhkhasi"
        ],
        [
          "Hilde",
          "Kuehne"
        ],
        [
          "Rameswar",
          "Panda"
        ],
        [
          "Rogerio",
          "Feris"
        ],
        [
          "Brian",
          "Kingsbury"
        ],
        [
          "Michael",
          "Picheny"
        ],
        [
          "Antonio",
          "Torralba"
        ],
        [
          "James",
          "Glass"
        ]
      ],
      "title": "AVLnet: Learning Audio-Visual Language Representations from Instructional Videos",
      "original": "1312",
      "page_count": 5,
      "order": 324,
      "p1": "1584",
      "pn": "1588",
      "abstract": [
        "Current methods for learning visually grounded language from videos\noften rely on text annotation, such as human generated captions or\nmachine generated automatic speech recognition (ASR) transcripts. In\nthis work, we introduce the Audio-Video Language Network (AVLnet),\na self-supervised network that learns a shared audio-visual embedding\nspace directly from raw video inputs. To circumvent the need for text\nannotation, we learn audio-visual representations from randomly segmented\nvideo clips and their raw audio waveforms. We train AVLnet on HowTo100M,\na large corpus of publicly available instructional videos, and evaluate\non image retrieval and video retrieval tasks, achieving state-of-the-art\nperformance. Finally, we perform analysis of AVLnet&#8217;s learned\nrepresentations, showing our model utilizes speech and natural sounds\nto learn audio-visual concepts.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1312",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "lee21b_interspeech": {
      "authors": [
        [
          "Gyeong-Hoon",
          "Lee"
        ],
        [
          "Tae-Woo",
          "Kim"
        ],
        [
          "Hanbin",
          "Bae"
        ],
        [
          "Min-Ji",
          "Lee"
        ],
        [
          "Young-Ik",
          "Kim"
        ],
        [
          "Hoon-Young",
          "Cho"
        ]
      ],
      "title": "N-Singer: A Non-Autoregressive Korean Singing Voice Synthesis System for Pronunciation Enhancement",
      "original": "0239",
      "page_count": 5,
      "order": 325,
      "p1": "1589",
      "pn": "1593",
      "abstract": [
        "Recently, end-to-end Korean singing voice systems have been designed\nto generate realistic singing voices. However, these systems still\nsuffer from a lack of robustness in terms of pronunciation accuracy.\nIn this paper, we propose N-Singer, a non-autoregressive Korean singing\nvoice system, to synthesize accurate and pronounced Korean singing\nvoices in parallel. N-Singer consists of a Transformer-based mel-generator,\na convolutional network-based postnet, and voicing-aware discriminators.\nIt can contribute in the following ways. First, for accurate pronunciation,\nN-Singer separately models linguistic and pitch information without\nother acoustic features. Second, to achieve improved mel-spectrograms,\nN-Singer uses a combination of Transformer-based modules and convolutional\nnetwork-based modules. Third, in adversarial training, voicing-aware\nconditional discriminators are used to capture the harmonic features\nof voiced segments and noise components of unvoiced segments. The experimental\nresults prove that N-Singer can synthesize a natural singing voice\nin parallel with a more accurate pronunciation than the baseline model.\n"
      ],
      "doi": "10.21437/Interspeech.2021-239"
    },
    "maniati21_interspeech": {
      "authors": [
        [
          "Georgia",
          "Maniati"
        ],
        [
          "Nikolaos",
          "Ellinas"
        ],
        [
          "Konstantinos",
          "Markopoulos"
        ],
        [
          "Georgios",
          "Vamvoukakis"
        ],
        [
          "June Sig",
          "Sung"
        ],
        [
          "Hyoungmin",
          "Park"
        ],
        [
          "Aimilios",
          "Chalamandaris"
        ],
        [
          "Pirros",
          "Tsiakoulis"
        ]
      ],
      "title": "Cross-Lingual Low Resource Speaker Adaptation Using Phonological Features",
      "original": "0327",
      "page_count": 5,
      "order": 326,
      "p1": "1594",
      "pn": "1598",
      "abstract": [
        "The idea of using phonological features instead of phonemes as input\nto sequence-to-sequence TTS has been recently proposed for zero-shot\nmultilingual speech synthesis. This approach is useful for code-switching,\nas it facilitates the seamless uttering of foreign text embedded in\na stream of native text. In our work, we train a language-agnostic\nmultispeaker model conditioned on a set of phonologically derived features\ncommon across different languages, with the goal of achieving cross-lingual\nspeaker adaptation. We first experiment with the effect of language\nphonological similarity on cross-lingual TTS of several source-target\nlanguage combinations. Subsequently, we fine-tune the model with very\nlimited data of a new speaker&#8217;s voice in either a seen or an\nunseen language, and achieve synthetic speech of equal quality, while\npreserving the target speaker&#8217;s identity. With as few as 32 and\n8 utterances of target speaker data, we obtain high speaker similarity\nscores and naturalness comparable to the corresponding literature.\nIn the extreme case of only 2 available adaptation utterances, we find\nthat our model behaves as a few-shot learner, as the performance is\nsimilar in both the seen and unseen adaptation language scenarios.\n"
      ],
      "doi": "10.21437/Interspeech.2021-327",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "zhan21_interspeech": {
      "authors": [
        [
          "Haoyue",
          "Zhan"
        ],
        [
          "Haitong",
          "Zhang"
        ],
        [
          "Wenjie",
          "Ou"
        ],
        [
          "Yue",
          "Lin"
        ]
      ],
      "title": "Improve Cross-Lingual Text-To-Speech Synthesis on Monolingual Corpora with Pitch Contour Information",
      "original": "0474",
      "page_count": 5,
      "order": 327,
      "p1": "1599",
      "pn": "1603",
      "abstract": [
        "Cross-lingual text-to-speech (TTS) synthesis on monolingual corpora\nis still a challenging task, especially when many kinds of languages\nare involved. In this paper, we improve the cross-lingual TTS model\non monolingual corpora with pitch contour information. We propose a\nmethod to obtain pitch contour sequences for different languages without\nmanual annotation, and extend the Tacotron-based TTS model with the\nproposed Pitch Contour Extraction (PCE) module. Our experimental results\nshow that the proposed approach can effectively improve the naturalness\nand consistency of synthesized mixed-lingual utterances.\n"
      ],
      "doi": "10.21437/Interspeech.2021-474",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "yang21d_interspeech": {
      "authors": [
        [
          "Zhenchuan",
          "Yang"
        ],
        [
          "Weibin",
          "Zhang"
        ],
        [
          "Yufei",
          "Liu"
        ],
        [
          "Xiaofen",
          "Xing"
        ]
      ],
      "title": "Cross-Lingual Voice Conversion with Disentangled Universal Linguistic Representations",
      "original": "0552",
      "page_count": 5,
      "order": 328,
      "p1": "1604",
      "pn": "1608",
      "abstract": [
        "Intra-lingual voice conversion has achieved great progress recently\nin terms of naturalness and similarity. However, in cross-lingual voice\nconversion, there is still an urgent need to improve the quality of\nthe converted speech, especially with nonparallel training data. Previous\nworks usually use Phonetic Posteriorgrams (PPGs) as the linguistic\nrepresentations. In the case of cross-lingual voice conversion, the\nlinguistic information is therefore represented as PPGs. It is well-known\nthat PPGs may suffer from word dropping and mispronunciation, especially\nwhen the input speech is noisy. In addition, systems using PPGs can\nonly convert the input into a known target language that is seen during\ntraining. This paper proposes an any-to-many voice conversion system\nbased on disentangled universal linguistic representations (ULRs),\nwhich are extracted from a mix-lingual phoneme recognition system.\nTwo methods are proposed to remove speaker information from ULRs. Experimental\nresults show that the proposed method can effectively improve the converted\nspeech objectively and subjectively. The system can also convert speech\nutterances naturally even if the language is not seen during training.\n"
      ],
      "doi": "10.21437/Interspeech.2021-552",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "liu21e_interspeech": {
      "authors": [
        [
          "Zhengchen",
          "Liu"
        ],
        [
          "Chenfeng",
          "Miao"
        ],
        [
          "Qingying",
          "Zhu"
        ],
        [
          "Minchuan",
          "Chen"
        ],
        [
          "Jun",
          "Ma"
        ],
        [
          "Shaojun",
          "Wang"
        ],
        [
          "Jing",
          "Xiao"
        ]
      ],
      "title": "EfficientSing: A Chinese Singing Voice Synthesis System Using Duration-Free Acoustic Model and HiFi-GAN Vocoder",
      "original": "0771",
      "page_count": 5,
      "order": 329,
      "p1": "1609",
      "pn": "1613",
      "abstract": [
        "In this paper, we present EfficientSing, a Chinese singing voice synthesis\n(SVS) system based on a non-autoregressive duration-free acoustic model\nand HiFi-GAN neural vocoder. Different from many existing SVS methods,\nno auxiliary duration prediction module is needed in this work, since\na newly proposed monotonic alignment modeling mechanism is adopted.\nMoreover, we follow the non-autoregressive architecture of EfficientTTS\nwith some singing-specific adaption, making training and inference\nfully parallel and efficient. HiFi-GAN vocoder is adopted to improve\nthe voice quality of synthesized songs and inference efficiency. Both\nobjective and subjective experimental results show that the proposed\nsystem can produce quite natural and high-fidelity songs and outperform\nthe Tacotron-based baseline in terms of pronunciation, pitch and rhythm.\n"
      ],
      "doi": "10.21437/Interspeech.2021-771",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "xin21_interspeech": {
      "authors": [
        [
          "Detai",
          "Xin"
        ],
        [
          "Yuki",
          "Saito"
        ],
        [
          "Shinnosuke",
          "Takamichi"
        ],
        [
          "Tomoki",
          "Koriyama"
        ],
        [
          "Hiroshi",
          "Saruwatari"
        ]
      ],
      "title": "Cross-Lingual Speaker Adaptation Using Domain Adaptation and Speaker Consistency Loss for Text-To-Speech Synthesis",
      "original": "0897",
      "page_count": 5,
      "order": 330,
      "p1": "1614",
      "pn": "1618",
      "abstract": [
        "We present a cross-lingual speaker adaptation method based on domain\nadaptation and a speaker consistency loss for text-to-speech (TTS)\nsynthesis. Existing monolingual speaker adaptation methods based on\ndirect fine-tuning are not applicable for cross-lingual data. The proposed\nmethod first trains a language-independent speaker encoder by speaker\nverification using domain adaption on multilingual data, including\nthe source and the target languages. Then the proposed method trains\na monolingual multi-speaker TTS model on the source language&#8217;s\ndata using the speaker embeddings generated by the speaker encoder.\nTo adapt the TTS model of the source language to new speakers the proposed\nmethod uses a speaker consistency loss to maximize the cosine similarity\nbetween speaker embeddings generated from the natural speech and the\nsame speaker&#8217;s synthesized speech. This makes fine-tuning the\nTTS model of source language on speech data of target language become\npossible. We conduct experiments on multi-speaker English and Japanese\ndatasets with 207 speakers in total. Results of comprehensive experiments\ndemonstrate that the proposed method can significantly improve speech\nnaturalness compared to the baseline method.\n"
      ],
      "doi": "10.21437/Interspeech.2021-897",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "shang21_interspeech": {
      "authors": [
        [
          "Zengqiang",
          "Shang"
        ],
        [
          "Zhihua",
          "Huang"
        ],
        [
          "Haozhe",
          "Zhang"
        ],
        [
          "Pengyuan",
          "Zhang"
        ],
        [
          "Yonghong",
          "Yan"
        ]
      ],
      "title": "Incorporating Cross-Speaker Style Transfer for Multi-Language Text-to-Speech",
      "original": "1265",
      "page_count": 5,
      "order": 331,
      "p1": "1619",
      "pn": "1623",
      "abstract": [
        "Recently multilingual TTS systems using only monolingual datasets have\nobtained significant improvement. However, the quality of cross-language\nspeech synthesis is not comparable to the speaker&#8217;s own language\nand often comes with a heavy foreign accent. This paper proposed a\nmulti-speaker multi-style multi-language speech synthesis system (M3),\nwhich improves the speech quality by introducing a fine-grained style\nencoder and overcomes the non-authentic accent problem through cross-speaker\nstyle transfer. To avoid leaking timbre information into style encoder,\nwe utilized a speaker conditional variational encoder and conducted\nadversarial speaker training using the gradient reversal layer. Then,\nwe built a Mixture Density Network (MDN) for mapping text to extracted\nstyle vectors for each speaker. At the inference stage, cross-language\nstyle transfer could be achieved by assigning any speaker&#8217;s style\ntype in the target language. Our system uses existing speaker style\nand genuinely avoids foreign accents. In the MOS-speech-naturalness,\nthe proposed method generally achieves 4.0 and significantly outperform\nthe baseline system.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1265",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "kesim21_interspeech": {
      "authors": [
        [
          "Ege",
          "Kesim"
        ],
        [
          "Engin",
          "Erzin"
        ]
      ],
      "title": "Investigating Contributions of Speech and Facial Landmarks for Talking Head Generation",
      "original": "1585",
      "page_count": 5,
      "order": 332,
      "p1": "1624",
      "pn": "1628",
      "abstract": [
        "Talking head generation is an active research problem. It has been\nwidely studied as a direct speech-to-video or two stage speech-to-landmarks-to-video\nmapping problem. In this study, our main motivation is to assess individual\nand joint contributions of the speech and facial landmarks to the talking\nhead generation quality through a state-of-the-art generative adversarial\nnetwork (GAN) architecture. Incorporating frame and sequence discriminators\nand a feature matching loss, we investigate performances of speech\nonly, landmark only and joint speech and landmark driven talking head\ngeneration on the CREMA-D dataset. Objective evaluations using the\npeak signal-to-noise ratio (PSNR), structural similarity index (SSIM)\nand landmark distance (LMD) indicate that while landmarks bring PSNR\nand SSIM improvements to the speech driven system, speech brings LMD\nimprovement to the landmark driven system. Furthermore, feature matching\nis observed to improve the speech driven talking head generation models\nsignificantly.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1585",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "si21b_interspeech": {
      "authors": [
        [
          "Shijing",
          "Si"
        ],
        [
          "Jianzong",
          "Wang"
        ],
        [
          "Xiaoyang",
          "Qu"
        ],
        [
          "Ning",
          "Cheng"
        ],
        [
          "Wenqi",
          "Wei"
        ],
        [
          "Xinghua",
          "Zhu"
        ],
        [
          "Jing",
          "Xiao"
        ]
      ],
      "title": "Speech2Video: Cross-Modal Distillation for Speech to Video Generation",
      "original": "1996",
      "page_count": 5,
      "order": 333,
      "p1": "1629",
      "pn": "1633",
      "abstract": [
        "This paper investigates a novel task of talking face video generation\nsolely from speeches. The speech-to-video generation technique can\nspark interesting applications in entertainment, customer service,\nand human-computer-interaction industries. Indeed, the timbre, accent\nand speed in speeches could contain rich information relevant to speakers&#8217;\nappearance. The challenge mainly lies in disentangling the distinct\nvisual attributes from audio signals. In this article, we propose a\nlight-weight, cross-modal distillation method to extract disentangled\nemotional and identity information from unlabelled video inputs. The\nextracted features are then integrated by a generative adversarial\nnetwork into talking face video clips. With carefully crafted discriminators,\nthe proposed framework achieves realistic generation results. Experiments\nwith observed individuals demonstrated that the proposed framework\ncaptures the emotional expressions solely from speeches, and produces\nspontaneous facial motion in the video output. Compared to the baseline\nmethod where speeches are combined with a static image of the speaker,\nthe results of the proposed framework is almost indistinguishable.\nUser studies also show that the proposed method outperforms the existing\nalgorithms in terms of emotion expression in the generated videos.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1996",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "lee21c_interspeech": {
      "authors": [
        [
          "Junhyeok",
          "Lee"
        ],
        [
          "Seungu",
          "Han"
        ]
      ],
      "title": "NU-Wave: A Diffusion Probabilistic Model for Neural Audio Upsampling",
      "original": "0036",
      "page_count": 5,
      "order": 334,
      "p1": "1634",
      "pn": "1638",
      "abstract": [
        "In this work, we introduce <i>NU-Wave</i>, the first neural audio upsampling\nmodel to produce waveforms of sampling rate 48kHz from coarse 16kHz\nor 24kHz inputs, while prior works could generate only up to 16kHz.\nNU-Wave is the first diffusion probabilistic model for audio super-resolution\nwhich is engineered based on neural vocoders. NU-Wave generates high-quality\naudio that achieves high performance in terms of signal-to-noise ratio\n(SNR), log-spectral distance (LSD), and accuracy of the ABX test. In\nall cases, NU-Wave outperforms the baseline models despite the substantially\nsmaller model capacity (3.0M parameters) than baselines (5.4&#8211;21%).\nThe audio samples of our model are publicly available, and the code\nwill be made available soon.\n"
      ],
      "doi": "10.21437/Interspeech.2021-36",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "lin21c_interspeech": {
      "authors": [
        [
          "Gang-Xuan",
          "Lin"
        ],
        [
          "Shih-Wei",
          "Hu"
        ],
        [
          "Yen-Ju",
          "Lu"
        ],
        [
          "Yu",
          "Tsao"
        ],
        [
          "Chun-Shien",
          "Lu"
        ]
      ],
      "title": "QISTA-Net-Audio: Audio Super-Resolution via Non-Convex &#8467;_q-Norm Minimization",
      "original": "0670",
      "page_count": 5,
      "order": 335,
      "p1": "1639",
      "pn": "1643",
      "abstract": [
        "Audio super-resolution (ASR) aims to reconstruct the high-resolution\nsignal from its corresponding low-resolution one, which is hard while\nthe correlation between them is low.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  In this paper, we\npropose a learning model, QISTA-Net-Audio, to solve ASR in a paradigm\nof linear inverse problem. QISTA-Net-Audio is composed of two components.\nFirst, an audio waveform can be presented as a complex-valued spectrum,\nwhich is composed of a real and an imaginary part, in the frequency\ndomain. We treat the real and imaginary parts as an image, and predict\na high-resolution spectrum but only keep the phase information from\nthe viewpoint of image reconstruction. Second, we predict the magnitude\ninformation by solving the sparse signal reconstruction problem. By\ncombining the predicted magnitude and the phase together, we can recover\nthe high-resolution waveform. Comparison with the state-of-the-art\nmethod MfNet [1], in terms of measure metrics SNR, PESQ, and STOI,\ndemonstrates the superior performance of our method.\n"
      ],
      "doi": "10.21437/Interspeech.2021-670"
    },
    "wen21_interspeech": {
      "authors": [
        [
          "Liang",
          "Wen"
        ],
        [
          "Lizhong",
          "Wang"
        ],
        [
          "Xue",
          "Wen"
        ],
        [
          "Yuxing",
          "Zheng"
        ],
        [
          "Youngo",
          "Park"
        ],
        [
          "Kwang Pyo",
          "Choi"
        ]
      ],
      "title": "X-net: A Joint Scale Down and Scale Up Method for Voice Call",
      "original": "0812",
      "page_count": 5,
      "order": 336,
      "p1": "1644",
      "pn": "1648",
      "abstract": [
        "This paper proposes X-net, a jointly learned scale-down and scale-up\narchitecture for data pre- and post-processing in voice calls, as a\nmeans to bandwidth extension over band-limited channels. Scale-down\nand scale-up are deployed separately on transmitter and receiver to\nperform down- and upsampling. Separate supervisions are used on the\nsubmodules so that X-net can work properly even if one submodule is\nmissing. A two-stage training method is used to learn X-net for improved\nperceptual quality. Results show that jointly learned X-net achieves\npromising improvement over blind audio super-resolution by both objective\nand subjective metrics, even in a lightweight implementation with only\n1k parameters.\n"
      ],
      "doi": "10.21437/Interspeech.2021-812",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "zhang21k_interspeech": {
      "authors": [
        [
          "Kexun",
          "Zhang"
        ],
        [
          "Yi",
          "Ren"
        ],
        [
          "Changliang",
          "Xu"
        ],
        [
          "Zhou",
          "Zhao"
        ]
      ],
      "title": "WSRGlow: A Glow-Based Waveform Generative Model for Audio Super-Resolution",
      "original": "0892",
      "page_count": 5,
      "order": 337,
      "p1": "1649",
      "pn": "1653",
      "abstract": [
        "Audio super-resolution is the task of constructing a high-resolution\n(HR) audio from a low-resolution (LR) audio by adding the missing band.\nPrevious methods based on convolutional neural networks and mean squared\nerror training objective have relatively low performance, while adversarial\ngenerative models are difficult to train and tune. Recently, normalizing\nflow has attracted a lot of attention for its high performance, simple\ntraining and fast inference. In this paper, we propose WSRGlow, a Glow-based\nwaveform generative model to perform audio super-resolution. Specifically,\n1) we integrate WaveNet and Glow to directly maximize the exact likelihood\nof the target HR audio conditioned on LR information; and 2) to exploit\nthe audio information from low-resolution audio, we propose an LR audio\nencoder and an STFT encoder, which encode the LR information from the\ntime domain and frequency domain respectively. The experimental results\nshow that the proposed model is easier to train and outperforms the\nprevious works in terms of both objective and perceptual quality. WSRGlow\nis also the first model to produce 48kHz waveforms from 12kHz LR audio.\nAudio samples are publicly available.\n"
      ],
      "doi": "10.21437/Interspeech.2021-892",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "yi21_interspeech": {
      "authors": [
        [
          "Jiangyan",
          "Yi"
        ],
        [
          "Ye",
          "Bai"
        ],
        [
          "Jianhua",
          "Tao"
        ],
        [
          "Haoxin",
          "Ma"
        ],
        [
          "Zhengkun",
          "Tian"
        ],
        [
          "Chenglong",
          "Wang"
        ],
        [
          "Tao",
          "Wang"
        ],
        [
          "Ruibo",
          "Fu"
        ]
      ],
      "title": "Half-Truth: A Partially Fake Audio Detection Dataset",
      "original": "0930",
      "page_count": 5,
      "order": 338,
      "p1": "1654",
      "pn": "1658",
      "abstract": [
        "Diverse promising datasets have been designed to further the development\nof fake audio detection, such as ASVspoof databases. However, previous\ndatasets ignore an attacking situation, in which the hacker hides some\nsmall fake clips in real speech audio. This poses a serious threat\nsince that it is difficult to distinguish the small fake clip from\nthe whole speech utterance. Therefore, this paper develops such a dataset\nfor half-truth audio detection (HAD). Partially fake audio in the HAD\ndataset involves only changing a few words in an utterance. The audio\nof the words is generated with the very latest state-of-the-art speech\nsynthesis technology. We can not only detect fake utterances but also\nlocalize manipulated regions in a speech using this dataset. Some benchmark\nresults are presented on this dataset. The results show that partially\nfake audio presents much more challenging than fully fake audio for\nfake audio detection.\n"
      ],
      "doi": "10.21437/Interspeech.2021-930",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement:14 Special Sessions"
    },
    "chettri21_interspeech": {
      "authors": [
        [
          "Bhusan",
          "Chettri"
        ],
        [
          "Rosa Gonz\u00e1lez",
          "Hautam\u00e4ki"
        ],
        [
          "Md.",
          "Sahidullah"
        ],
        [
          "Tomi",
          "Kinnunen"
        ]
      ],
      "title": "Data Quality as Predictor of Voice Anti-Spoofing Generalization",
      "original": "1180",
      "page_count": 5,
      "order": 339,
      "p1": "1659",
      "pn": "1663",
      "abstract": [
        "Voice anti-spoofing aims at classifying a given utterance either as\na bonafide human sample, or a spoofing attack (e.g. synthetic or replayed\nsample). Many anti-spoofing methods have been proposed but most of\nthem fail to generalize across domains (corpora) &#8212; and we do\nnot know <i>why</i>. We outline a novel interpretative framework for\ngauging the impact of data quality upon anti-spoofing performance.\nOur within- and between-domain experiments pool data from seven public\ncorpora and three anti-spoofing methods based on Gaussian mixture and\nconvolutive neural network models. We assess the impacts of long-term\nspectral information, speaker population (through x-vector speaker\nembeddings), signal-to-noise ratio, and selected voice quality features.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1180",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "cheon21_interspeech": {
      "authors": [
        [
          "Youngju",
          "Cheon"
        ],
        [
          "Soojoong",
          "Hwang"
        ],
        [
          "Sangwook",
          "Han"
        ],
        [
          "Inseon",
          "Jang"
        ],
        [
          "Jong Won",
          "Shin"
        ]
      ],
      "title": "Coded Speech Enhancement Using Neural Network-Based Vector-Quantized Residual Features",
      "original": "1204",
      "page_count": 5,
      "order": 340,
      "p1": "1664",
      "pn": "1668",
      "abstract": [
        "Various approaches have been proposed to improve the quality of the\nspeech coded at low bitrates. Recently, deep neural networks have also\nbeen used for speech coding, providing a high quality of speech with\nlow bitrates. Although designing an entire codec with neural networks\nmay be more effective, backward compatibility with the existing codecs\ncan be desirable so that the systems with the legacy codec can still\ndecode the coded bitstream. In this paper, we propose to generate side\ninformation based on neural networks for an existing codec and enhance\nthe decoded speech with another neural networks using the side information.\nThe vector-quantization variational autoencoder (VQ-VAE) is applied\nto generate vector-quantized side information and reconstruct the residual\nfeatures, which are the difference between the features extracted from\nthe original and decoded signals. The post-processor in the decoder\nside, which is another neural network, takes the decoded signal of\nthe main codec and the reconstructed residual features to estimate\nthe features for the original signal. Experimental results show that\nthe proposed method can significantly improve the quality of the enhanced\nsignals with additional bitrate of 0.6 kbps for two of the implementations\nof the high-efficiency advanced audio coding (HE-AAC) v1.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1204",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "drude21_interspeech": {
      "authors": [
        [
          "Lukas",
          "Drude"
        ],
        [
          "Jahn",
          "Heymann"
        ],
        [
          "Andreas",
          "Schwarz"
        ],
        [
          "Jean-Marc",
          "Valin"
        ]
      ],
      "title": "Multi-Channel Opus Compression for Far-Field Automatic Speech Recognition with a Fixed Bitrate Budget",
      "original": "1214",
      "page_count": 5,
      "order": 341,
      "p1": "1669",
      "pn": "1673",
      "abstract": [
        "Automatic speech recognition (ASR) in the cloud allows the use of larger\nmodels and more powerful multi-channel signal processing front-ends\ncompared to on-device processing. However, it also adds an inherent\nlatency due to the transmission of the audio signal, especially when\ntransmitting multiple channels of a microphone array. One way to reduce\nthe network bandwidth requirements is client-side compression with\na lossy codec such as Opus. However, this compression can have a detrimental\neffect especially on multi-channel ASR front-ends, due to the distortion\nand loss of spatial information introduced by the codec. In this publication,\nwe propose an improved approach for the compression of microphone array\nsignals based on Opus, using a modified joint channel coding approach\nand additionally introducing a multi-channel spatial decorrelating\ntransform to reduce redundancy in the transmission. We illustrate the\neffect of the proposed approach on the spatial information retained\nin multi-channel signals after compression, and evaluate the performance\non far-field ASR with a multi-channel beamforming front-end. We demonstrate\nthat our approach can lead to a 37.5% bitrate reduction or a 5.1% relative\nword error rate (WER) reduction for a fixed bitrate budget in a seven\nchannel setup.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1214",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "siegert21_interspeech": {
      "authors": [
        [
          "Ingo",
          "Siegert"
        ]
      ],
      "title": "Effects of Prosodic Variations on Accidental Triggers of a Commercial Voice Assistant",
      "original": "1354",
      "page_count": 5,
      "order": 342,
      "p1": "1674",
      "pn": "1678",
      "abstract": [
        "The use of modern voice assistants has rapidly grown and they can be\nfound in more and more households. By design, these systems have to\nscan every sound in their surroundings waiting for their respective\nwake-word before being able to react to the users&#8217; commands.\nThe drawback of this method is that phonetic similar expressions can\nactivate the voice assistant and thus speech utterances or whole private\nconversations will be recorded and streamed to the cloud back-end for\nfurther processing. Many news articles and scientific work reported\non inaccurate wake-word detection. Resulting in at least a user&#8217;s\nconfusion or at worst security breaches. The current paper is based\non a broader analysis of phonetic similar accidental triggers conducted\nby Sch&#246;nherr et al., they presented a systematic analysis to detect\naccidental triggers, using a pronouncing dictionary and a weighted,\nphone-based Levenshtein distance. In this work, the previously identified\naccidental triggers are recorded by several speakers under various\nconditions to investigate the influence of phonetic variances (i.e.\nintonation and speaking/articulation rate) on the robustness of accidental\ntriggers in a real-world environment.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1354",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "gabrys21_interspeech": {
      "authors": [
        [
          "Adam",
          "Gabry\u015b"
        ],
        [
          "Yunlong",
          "Jiao"
        ],
        [
          "Viacheslav",
          "Klimkov"
        ],
        [
          "Daniel",
          "Korzekwa"
        ],
        [
          "Roberto",
          "Barra-Chicote"
        ]
      ],
      "title": "Improving the Expressiveness of Neural Vocoding with Non-Affine Normalizing Flows",
      "original": "1555",
      "page_count": 5,
      "order": 343,
      "p1": "1679",
      "pn": "1683",
      "abstract": [
        "This paper proposes a general enhancement to the Normalizing Flows\n(NF) used in neural vocoding. As a case study, we improve expressive\nspeech vocoding with a revamped Parallel Wavenet (PW). Specifically,\nwe propose to extend the affine transformation of PW to the more expressive\ninvertible non-affine function. The greater expressiveness of the improved\nPW leads to better-perceived signal quality and naturalness in the\nwaveform reconstruction and text-to-speech (TTS) tasks. We evaluate\nthe model across different speaking styles on a multi-speaker, multi-lingual\ndataset. In the waveform reconstruction task, the proposed model closes\nthe naturalness and signal quality gap from the original PW to recordings\nby 10%, and from other state-of-the-art neural vocoding systems by\nmore than 60%. We also demonstrate improvements in objective metrics\non the evaluation test set with L2 Spectral Distance and Cross-Entropy\nreduced by 3% and 6&#x2030; comparing to the affine PW. Furthermore,\nwe extend the probability density distillation procedure proposed by\nthe original PW paper, so that it works with any non-affine invertible\nand differentiable function.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1555",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "prajapati21_interspeech": {
      "authors": [
        [
          "Gauri P.",
          "Prajapati"
        ],
        [
          "Dipesh K.",
          "Singh"
        ],
        [
          "Preet P.",
          "Amin"
        ],
        [
          "Hemant A.",
          "Patil"
        ]
      ],
      "title": "Voice Privacy Through x-Vector and CycleGAN-Based Anonymization",
      "original": "1573",
      "page_count": 5,
      "order": 344,
      "p1": "1684",
      "pn": "1688",
      "abstract": [
        "With the rise in usage of voice assistants and spoken language interfaces,\nimportant concerns regarding voice data privacy have been prompted.\nIn an attempt to reduce the threat of attacks on voice data, in this\npaper, we propose a speaker anonymization system based on CycleGAN.\nThis method modifies the speaker&#8217;s gender and accent information\nfrom the original speech signal. The proposed method gives a more natural-sounding\nanonymized voice in addition to a de-identified speaker. We have chosen\nbaseline-1 of The Voice Privacy Challenge-2020 as our baseline system.\nTraining of CycleGAN, ASR, and ASV experiments are performed on the\nsubset of Librispeech corpus. In this paper, the double anonymization\ntechnique is also explored in which the CycleGAN-based anonymization\ntechnique is adopted on top of the baseline system. Experimental results\nshow that combining the proposed method with the x-vector and neural\nsource-filter (NSF) model-based method (baseline system) gives up to\n5.61% relative improvement in EER of original-anonymized, enroll-trial\npairs. However, it gives up to 19.30% relative improvement in EER for\nanonymized-anonymized enroll-trial pairs. We observed that along with\nthe good speaker de-identification, the anonymized utterances have\nadequate speech intelligibility and naturalness.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1573",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "lin21d_interspeech": {
      "authors": [
        [
          "Ju",
          "Lin"
        ],
        [
          "Yun",
          "Wang"
        ],
        [
          "Kaustubh",
          "Kalgaonkar"
        ],
        [
          "Gil",
          "Keren"
        ],
        [
          "Didi",
          "Zhang"
        ],
        [
          "Christian",
          "Fuegen"
        ]
      ],
      "title": "A Two-Stage Approach to Speech Bandwidth Extension",
      "original": "1941",
      "page_count": 5,
      "order": 345,
      "p1": "1689",
      "pn": "1693",
      "abstract": [
        "Algorithms for speech bandwidth extension (BWE) may work in either\nthe time domain or the frequency domain. Time-domain methods often\ndo not sufficiently recover the high-frequency content of speech signals;\nfrequency-domain methods are better at recovering the spectral envelope,\nbut have difficulty reconstructing the details of the waveform. In\nthis paper, we propose a two-stage approach for BWE, which enjoys the\nadvantages of both time- and frequency-domain methods. The first stage\nis a frequency-domain neural network, which predicts the high-frequency\npart of the wide-band spectrogram from the narrow-band input spectrogram.\nThe wide-band spectrogram is then converted into a time-domain waveform,\nand passed through the second stage to refine the temporal details.\nFor the first stage, we compare a convolutional recurrent network (CRN)\nwith a temporal convolutional network (TCN), and find that the latter\nis able to capture long-span dependencies equally well as the former\nwhile using a lot fewer parameters. For the second stage, we enhance\nthe Wave-U-Net architecture with a multi-resolution short-time Fourier\ntransform (MSTFT) loss function. A series of comprehensive experiments\nshow that the proposed system achieves superior performance in speech\nenhancement (measured by both time- and frequency-domain metrics) as\nwell as speech recognition.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1941",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "byun21_interspeech": {
      "authors": [
        [
          "Joon",
          "Byun"
        ],
        [
          "Seungmin",
          "Shin"
        ],
        [
          "Youngcheol",
          "Park"
        ],
        [
          "Jongmo",
          "Sung"
        ],
        [
          "Seungkwon",
          "Beack"
        ]
      ],
      "title": "Development of a Psychoacoustic Loss Function for the Deep Neural Network\t(DNN)-Based Speech Coder",
      "original": "2151",
      "page_count": 5,
      "order": 346,
      "p1": "1694",
      "pn": "1698",
      "abstract": [
        "This paper presents a loss function to compensate for the perceptual\nloss of the deep neural network (DNN)-based speech coder. By utilizing\nthe psychoacoustic model (PAM), we design a loss function to maximize\nthe mask-to-noise ratio (MNR) in multi-resolution Mel-frequency scales.\nAlso, a perceptual entropy (PE)-based weighting scheme is incorporated\nonto the MNR loss so that the DNN model focuses more on perceptually\nimportant Mel-frequency bands. The proposed loss function was tested\non a CNN-based autoencoder implementing the softmax quantization and\nentropy-based bitrate control. Objective and subjective tests conducted\nwith speech signals showed that the proposed loss function produced\nhigher perceptual quality than the previous perceptual loss functions.\n"
      ],
      "doi": "10.21437/Interspeech.2021-2151",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "stoidis21_interspeech": {
      "authors": [
        [
          "Dimitrios",
          "Stoidis"
        ],
        [
          "Andrea",
          "Cavallaro"
        ]
      ],
      "title": "Protecting Gender and Identity with Disentangled Speech Representations",
      "original": "2163",
      "page_count": 5,
      "order": 347,
      "p1": "1699",
      "pn": "1703",
      "abstract": [
        "Besides its linguistic content, our speech is rich in biometric information\nthat can be inferred by classifiers. Learning privacy-preserving representations\nfor speech signals enables downstream tasks without sharing unnecessary,\nprivate information about an individual. In this paper, we show that\nprotecting gender information in speech is more effective than modelling\nspeaker-identity information only when generating a non-sensitive representation\nof speech. Our method relies on reconstructing speech by decoding linguistic\ncontent along with gender information using a variational autoencoder.\nSpecifically, we exploit disentangled representation learning to encode\ninformation about different attributes into separate subspaces that\ncan be factorised independently. We present a novel way to encode gender\ninformation and disentangle two sensitive biometric identifiers, namely\ngender and identity, in a privacy-protecting setting. Experiments on\nthe LibriSpeech dataset show that gender recognition and speaker verification\ncan be reduced to a random guess, protecting against classification-based\nattacks.\n"
      ],
      "doi": "10.21437/Interspeech.2021-2163",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "aldholmi21_interspeech": {
      "authors": [
        [
          "Yahya",
          "Aldholmi"
        ],
        [
          "Rawan",
          "Aldhafyan"
        ],
        [
          "Asma",
          "Alqahtani"
        ]
      ],
      "title": "Perception of Standard Arabic Synthetic Speech Rate",
      "original": "0039",
      "page_count": 4,
      "order": 348,
      "p1": "1704",
      "pn": "1707",
      "abstract": [
        "This experiment investigated how Arabic speakers perceive synthetic\nStandard Arabic speech rate produced by Google TTS, at normal vs. accelerated\nrates. Twenty syntactically identical Standard Arabic sentences with\na similar length (<i>M</i>= 22 syllables per sentence, <i>SD</i>= 1)\nwere auditorily presented in a female voice to thirty female participants\nwho were instructed to rate the tempo of the normal (<i>M</i>&#8776;\n4.5 syllable per second) and accelerated (by 10%, 20%, and 30%) stimuli\non a 1&#8211;7 Likert scale (1= extremely slow, 4= normal, 7= extremely\nfast). The results show that differences in the four-condition synthetic\nspeech rates were reflected in the ratings provided by the participants:\nthe more the speech was accelerated, the higher rating it received.\nMore importantly, the findings support the observation that the current\nnormal speech rate of Google TTS synthetic speech is not perceived\nas normal by Arabic speakers, but rather is perceived as slow. This\nmay negatively affect the likelihood that users are comfortable using\nthis technology. Hence, the outcome of this study does not only call\nfor further investigation into Standard Arabic synthetic speech rates,\nbut also reveals the need to define a baseline for a natural speech\nrate in Arabic.\n"
      ],
      "doi": "10.21437/Interspeech.2021-39",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production and Acquisition  by Human Listeners"
    },
    "kishiyama21_interspeech": {
      "authors": [
        [
          "Takeshi",
          "Kishiyama"
        ]
      ],
      "title": "The Influence of Parallel Processing on Illusory Vowels",
      "original": "0089",
      "page_count": 5,
      "order": 349,
      "p1": "1708",
      "pn": "1712",
      "abstract": [
        "Research has shown that listeners perceive illusory vowels inside consonant\nclusters that are not allowed in their L1. This phenomenon has been\nexamined using several psycholinguistic and computational models, including\nhidden Markov models (HMMs), applied to human phoneme perception. However,\nthe inference algorithm of HMMs assumes that parallel processing, which\nhas not been proven to have psychological reality, is a valid cognitive\nprocess. This study tested the psychological reality of parallel processing\nby attempting to duplicate two results from previous studies: First,\nlisteners perceive an illusory vowel in consonant clusters that are\nnot permissible in their L1. Second, the illusory vowel is based on\nthe characteristics of the preceding consonant, indicating that listeners\nintegrate phonotactics and acoustic information. The experiment manipulated\nthe number of candidates that the model can refer to, and the algorithm\ncan be considered parallel when it allows models to use more than two\ncandidates that are stored in memory. In addition, the transition probabilities\nbetween consonants were manipulated to represent the different phonotactics.\nThe results showed that only the parallel processing condition reproduced\nthe two observations above, supporting the psychological reality of\nparallel processing.\n"
      ],
      "doi": "10.21437/Interspeech.2021-89",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production and Acquisition  by Human Listeners"
    },
    "chingacham21_interspeech": {
      "authors": [
        [
          "Anupama",
          "Chingacham"
        ],
        [
          "Vera",
          "Demberg"
        ],
        [
          "Dietrich",
          "Klakow"
        ]
      ],
      "title": "Exploring the Potential of Lexical Paraphrases for Mitigating Noise-Induced Comprehension Errors",
      "original": "0306",
      "page_count": 5,
      "order": 350,
      "p1": "1713",
      "pn": "1717",
      "abstract": [
        "Listening in noisy environments can be difficult even for individuals\nwith a normal hearing thresholds. The speech signal can be masked by\nnoise, which may lead to word misperceptions on the side of the listener,\nand overall difficulty to understand the message. To mitigate hearing\ndifficulties on listeners, a co-operative speaker utilizes voice modulation\nstrategies like Lombard speech to generate noise-robust utterances,\nand similar solutions have been developed for speech synthesis systems.\nIn this work, we propose an alternate solution of choosing noise-robust\nlexical paraphrases to represent an intended meaning. Our results show\nthat lexical paraphrases differ in their intelligibility in noise.\nWe evaluate the intelligibility of synonyms in context and find that\nchoosing a lexical unit that is less risky to be misheard than its\nsynonym introduced an average gain in comprehension of 37% at SNR -5\ndB and 21% at SNR 0 dB for babble noise.\n"
      ],
      "doi": "10.21437/Interspeech.2021-306",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production and Acquisition  by Human Listeners"
    },
    "simantiraki21_interspeech": {
      "authors": [
        [
          "Olympia",
          "Simantiraki"
        ],
        [
          "Martin",
          "Cooke"
        ]
      ],
      "title": " SpeechAdjuster: A Tool for Investigating Listener Preferences and Speech Intelligibility",
      "original": "0324",
      "page_count": 5,
      "order": 351,
      "p1": "1718",
      "pn": "1722",
      "abstract": [
        "Most of what we know about speech perception has been gleaned from\ntests in which listeners respond to stimuli chosen by an experimenter.\nThis paper presents  SpeechAdjuster, an open source tool that reverses\nthe roles of listener and experimenter by allowing listeners direct\ncontrol of speech characteristics in real-time. This change of paradigm\nenables listener preferences &#8212; reflecting factors such as cognitive\neffort, naturalness or distortion &#8212; to be measured directly,\nwithout recourse to rating scales. Incorporation of a test phase in\nwhich listener preferences are frozen also enables intelligibility\nto be estimated within the same trial. Offline computation and smooth\nonline interpolation within the tool permits the impact of changes\nin practically any target speech feature (e.g. fundamental frequency\nor spectral slope) or background characteristic (e.g. noise spectrum),\nregardless of complexity, to be measured. The paper describes the tool&#8217;s\ncapabilities, presents a range of visualisations, and notes some potential\napplications and limitations.\n"
      ],
      "doi": "10.21437/Interspeech.2021-324"
    },
    "saito21_interspeech": {
      "authors": [
        [
          "Susumu",
          "Saito"
        ],
        [
          "Yuta",
          "Ide"
        ],
        [
          "Teppei",
          "Nakano"
        ],
        [
          "Tetsuji",
          "Ogawa"
        ]
      ],
      "title": "VocalTurk: Exploring Feasibility of Crowdsourced Speaker Identification",
      "original": "0464",
      "page_count": 5,
      "order": 352,
      "p1": "1723",
      "pn": "1727",
      "abstract": [
        "This paper presents VocalTurk, a feasibility study of crowdsourced\nspeaker identification based on our worker dataset collected in Amazon\nMechanical Turk. Crowdsourced data labeling has already been acknowledged\nin speech data processing nowadays, but empirical analysis that answer\nto common questions such as &#8220;<i>how accurate are workers capable\nof labeling speech data?</i>&#8221; and &#8220;<i>what does a good\nspeech-labeling microtask interface look like?</i>&#8221; still remain\nunderexplored, which would limit the quality and scale of the dataset\ncollection. Focusing on the speaker identification task in particular,\nwe thus conducted two studies in Amazon Mechanical Turk: i) hired 3,800+\nunique workers to test their performances and confidences in giving\nanswers to voice pair comparison tasks, and ii) additionally assigned\nmore-difficult tasks of <i>1-vs-N</i> voice set comparisons to 350+\ntop-scoring workers to test their accuracy-speed performances across\npatterns of N = 1, 3, 5. The results revealed some positive findings\nthat would motivate speech researchers toward crowdsourced data labeling,\nsuch as that the top-scoring workers were capable of giving labels\nto our voice comparison pairs with 99% accuracy after majority voting,\nas well as they were even capable of batch-labeling which significantly\nshortened up to 34% of their completion time but still with no statistically-significant\ndegradation in accuracy.\n"
      ],
      "doi": "10.21437/Interspeech.2021-464",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production and Acquisition  by Human Listeners"
    },
    "xu21e_interspeech": {
      "authors": [
        [
          "Min",
          "Xu"
        ],
        [
          "Jing",
          "Shao"
        ],
        [
          "Lan",
          "Wang"
        ]
      ],
      "title": "Effects of Aging and Age-Related Hearing Loss on Talker Discrimination",
      "original": "0682",
      "page_count": 5,
      "order": 353,
      "p1": "1728",
      "pn": "1732",
      "abstract": [
        "Paralinguistic information is as important as linguistic information.\nBeing familiar with talker&#8217;s voice may facilitate speech perception,\nespecially in challenging conditions. Previous studies have suggested\nthat aging and age-related hearing loss lead to the deterioration of\nthe phonetic and phonological processing ability. The current study\naims to explore whether these two factors exert effects on the talker&#8217;\nvoice discrimination. Three groups of participants, including young\nadults (YA) and older adults (OA) with and without hearing loss, were\ntested on talker discrimination in four types of stimuli varying in\nlanguage familiarity: Mandarin real words, pseudowords, Arabic words\nand reversed Mandarin words. The results showed that OA with and without\nhearing loss performed worse than YA in both nonnative and native conditions.\nOA with hearing loss further performed worse than OA with normal hearing\nin Mandarin real word condition. These findings indicated that aging\nand hearing loss affected both low-level phonetic and high-level phonological\nprocessing, but hearing loss had extra effect on phonological processing.\nAltogether, these results implied that OA could not utilize phonetic\nand phonological cues as effectively as YA, and OA with hearing loss\nencountered more difficulties in utilizing phonological cues in talker\ndiscrimination.\n"
      ],
      "doi": "10.21437/Interspeech.2021-682",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production and Acquisition  by Human Listeners"
    },
    "zhang21l_interspeech": {
      "authors": [
        [
          "Yuqing",
          "Zhang"
        ],
        [
          "Zhu",
          "Li"
        ],
        [
          "Bin",
          "Wu"
        ],
        [
          "Yanlu",
          "Xie"
        ],
        [
          "Binghuai",
          "Lin"
        ],
        [
          "Jinsong",
          "Zhang"
        ]
      ],
      "title": "Relationships Between Perceptual Distinctiveness, Articulatory Complexity and Functional Load in Speech Communication",
      "original": "0721",
      "page_count": 5,
      "order": 354,
      "p1": "1733",
      "pn": "1737",
      "abstract": [
        "Work on communicative efficiency has hypothesized that phonological\ncontrasts signaling more meaning distinctions (i.e., of high functional\nload (FL)) tend to have the least articulatory complexity and the highest\nperceptual salience. However, only a few studies have examined the\npreference for perceptual distinctiveness based on the traditional\nmeasures of FL (e.g., the number of minimal pairs, the change in entropy\nof the lexicon), which are weak in modeling contexts of individual\nwords. And little attention has been devoted to investigating the need\nto minimize effort. This study explores whether and how the communicative\npressures to minimize the likelihood of confusion and minimize articulatory\neffort influence phonemic contrasts&#8217; functional contributions\nto speech communication. We used a revised definition of FL capable\nof modeling contextual information (i.e., the change in mutual information\nbetween phoneme sequences and spoken texts after the contrast in question\nis neutralized) and quantified information contributions of phonemic\ncontrasts in English. The results indicated that FL of each phoneme\npair increased significantly with its perceptual distinctiveness, and\ndecreased significantly with articulatory complexity of the phoneme\nrequiring less articulatory effort in the contrast. Altogether, these\nfindings suggest that communicative pressures modulate the work a phonemic\ncontrast does in distinguishing words.\n"
      ],
      "doi": "10.21437/Interspeech.2021-721",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production and Acquisition  by Human Listeners"
    },
    "terblanche21_interspeech": {
      "authors": [
        [
          "Camryn",
          "Terblanche"
        ],
        [
          "Philip",
          "Harrison"
        ],
        [
          "Amelia J.",
          "Gully"
        ]
      ],
      "title": "Human Spoofing Detection Performance on Degraded Speech",
      "original": "1225",
      "page_count": 5,
      "order": 355,
      "p1": "1738",
      "pn": "1742",
      "abstract": [
        "Over the past few years attention has been focused on the automatic\ndetection of spoofing in the context of automatic speaker verification\n(ASV) systems. However, little is known about how well humans perform\nat detecting spoofed speech, particularly under degraded conditions.\nUsing the latest synthesis technologies from ASVspoof 2019, this paper\nexplores human judgements of speech authenticity by considering three\ncommon channel degradations &#8212; a GSM network, a VoIP network,\nand background noise &#8212; in conjunction with varying synthesis\nquality. The results reveal that channel degradation reduces the size\nof the perceptual difference between genuine and spoofed speech, and\noverall participants correctly identified human and spoofed speech\nonly 56% of the time. In background noise and GSM transmission, lower-quality\nsynthetic speech was judged as more human, and in VoIP transmission\nall speech, including genuine recordings, was judged as less human.\nUnder all conditions, state-of-the-art synthetic speech was judged\nas human, or more human than, genuine recorded speech. The paper also\nconsiders the listener factors which may contribute to an individual&#8217;s\nspoofing detection performance, and finds that a listener&#8217;s familiarity\nwith the accents involved, their age, and the audio equipment used\nfor playback, have an effect on their spoofing detection performance.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1225",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production and Acquisition  by Human Listeners"
    },
    "einfeldt21_interspeech": {
      "authors": [
        [
          "Marieke",
          "Einfeldt"
        ],
        [
          "Rita",
          "Sevastjanova"
        ],
        [
          "Katharina",
          "Zahner-Ritter"
        ],
        [
          "Ekaterina",
          "Kazak"
        ],
        [
          "Bettina",
          "Braun"
        ]
      ],
      "title": "Reliable Estimates of Interpretable Cue Effects with Active Learning in Psycholinguistic Research",
      "original": "1524",
      "page_count": 5,
      "order": 356,
      "p1": "1743",
      "pn": "1747",
      "abstract": [
        "Studying the relative weighting of different cues for the interpretation\nof a linguistic phenomenon is a core element in psycholinguistic research.\nThis research needs to strike a balance between two things: generalisability\nto diverse lexical settings, which requires a high number of different\nlexicalisations and the investigation of a large number of different\ncues, which requires a high number of different test conditions. Optimizing\nboth is impossible with classical psycholinguistic designs as this\nwould leave the participants with too many experimental trials. Previously\nwe showed that Active Learning (AL) systems allow to test numerous\nconditions (eight) and items (32) within the same experiment. As stimulus\nselection was informed by the system&#8217;s learning mechanism, AL\nsped-up the labelling process. In the present study, we extend the\nuse case to an experiment with 16 conditions, manipulated through four\nbinary factors (the experimental setting and three prosodic cues; two\nlevels each). Our findings show that the AL system correctly predicted\nthe intended result pattern after twelve trials only. Hence, AL further\nconfirmed previous findings and proved to be an efficient tool, which\noffers a promising solution to complex study designs in psycholinguistic\nresearch.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1524",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production and Acquisition  by Human Listeners"
    },
    "kumar21d_interspeech": {
      "authors": [
        [
          "Puneet",
          "Kumar"
        ],
        [
          "Vishesh",
          "Kaushik"
        ],
        [
          "Balasubramanian",
          "Raman"
        ]
      ],
      "title": "Towards the Explainability of Multimodal Speech Emotion Recognition",
      "original": "1718",
      "page_count": 5,
      "order": 357,
      "p1": "1748",
      "pn": "1752",
      "abstract": [
        "In this paper, a multimodal speech emotion recognition system has been\ndeveloped, and a novel technique to explain its predictions has been\nproposed. The audio and textual features are extracted separately using\nattention-based Gated Recurrent Unit (GRU) and pre-trained Bidirectional\nEncoder Representations from Transformers (BERT), respectively. Then\nthey are concatenated and used to predict the final emotion class.\nThe weighted and unweighted emotion recognition accuracy of 71.7% and\n75.0% has been achieved on Emotional Dyadic Motion Capture (IEMOCAP)\ndataset containing speech utterances and corresponding text transcripts.\nThe training and predictions of network layers have been analyzed qualitatively\nthrough emotion embedding plots and quantitatively by analyzing the\nintersection matrices for various emotion classes&#8217; embeddings.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1718",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production and Acquisition  by Human Listeners"
    },
    "zeng21_interspeech": {
      "authors": [
        [
          "Biao",
          "Zeng"
        ],
        [
          "Rui",
          "Wang"
        ],
        [
          "Guoxing",
          "Yu"
        ],
        [
          "Christian",
          "Dobel"
        ]
      ],
      "title": "Primacy of Mouth over Eyes: Eye Movement Evidence from Audiovisual Mandarin Lexical Tones and Vowels",
      "original": "1741",
      "page_count": 4,
      "order": 358,
      "p1": "1753",
      "pn": "1756",
      "abstract": [
        "This study investigated Chinese speakers&#8217; eye movements when\nthey were asked to identify audiovisual Mandarin lexical tones and\nvowels. In the lexical tone identification task, Chinese speakers were\npresented with an audiovisual clip of Mandarin monosyllables (/&#259;/,\n/&#224;/, /&#x12D;/, /&#236;/) and asked to identify whether the syllables\nwere presented in a dipping (/&#259;/, /&#x12D;/) or falling tone (/&#224;/,\n/&#236;/). In the vowel identification task, they were asked to identify\nwhether the vowels were /a/ or /i/ regardless of lexical tone. These\naudiovisual syllables were presented in clear, noisy, and silent conditions.\nAn eye-tracker recorded the participants&#8217; eye movements.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  Results showed participants\ngazed more at the mouth than the eyes in both lexical tones and vowels.\nAdditionally, when acoustic conditions degraded from clear to noisy\nand eventually silent, Chinese speakers increased their gaze towards\nthe mouth rather than the eyes. These findings suggest the mouth to\nbe the primary area that is utilised during audiovisual speech perception.\nThe similar patterns of eye movements between vowels and lexical tones\nindicate that the mouth acts as a perceptual cue that provides articulatory\ninformation.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1741",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production and Acquisition  by Human Listeners"
    },
    "ashihara21_interspeech": {
      "authors": [
        [
          "Takanori",
          "Ashihara"
        ],
        [
          "Takafumi",
          "Moriya"
        ],
        [
          "Makio",
          "Kashino"
        ]
      ],
      "title": "Investigating the Impact of Spectral and Temporal Degradation on End-to-End Automatic Speech Recognition Performance",
      "original": "2091",
      "page_count": 5,
      "order": 359,
      "p1": "1757",
      "pn": "1761",
      "abstract": [
        "Humans have a sophisticated capability to robustly handle incomplete\nsensory input, as often happens in real environments. In earlier studies,\nthe robustness of human speech perception was observed qualitatively\nby spectrally and temporally degraded stimuli. The current study investigates\nhow machine speech recognition, especially end-to-end automatic speech\nrecognition (E2E-ASR), can yield similar robustness against distorted\nacoustic cues. To evaluate the performance of E2E-ASR, we employ four\ntypes of distorted speech based on previous studies: locally time-reversed\nspeech, noise-vocoded speech, phonemic restoration, and modulation-filtered\nspeech. Those stimuli are synthesized by spectral and/or temporal manipulation\nfrom original speech samples whose human speech intelligibility scores\nhave been well-reported. An experiment was conducted on the TED-LIUM2\nfor English and the Corpus of Spontaneous Japanese (CSJ) for Japanese.\nWe found that while there is a tendency to exhibit similar robustness\nin some experiments, full recovery from the harmful effect of the severe\nspectral degradation is not achieved.\n"
      ],
      "doi": "10.21437/Interspeech.2021-2091",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production and Acquisition  by Human Listeners"
    },
    "nguyen21c_interspeech": {
      "authors": [
        [
          "Thai-Son",
          "Nguyen"
        ],
        [
          "Sebastian",
          "St\u00fcker"
        ],
        [
          "Alex",
          "Waibel"
        ]
      ],
      "title": "Super-Human Performance in Online Low-Latency Recognition of Conversational Speech",
      "original": "1114",
      "page_count": 5,
      "order": 360,
      "p1": "1762",
      "pn": "1766",
      "abstract": [
        "Achieving super-human performance in recognizing human speech has been\na goal for several decades as researchers have worked on increasingly\nchallenging tasks. In the 1990&#8217;s it was discovered, that conversational\nspeech between two humans turns out to be considerably more difficult\nthan read speech as hesitations, disfluencies, false starts and sloppy\narticulation complicate acoustic processing and require robust joint\nhandling of acoustic, lexical and language context. Early attempts\nwith statistical models could only reach word error rates (WER) of\nover 50% which is far from human performance with shows a WER of around\n5.5%. Neural hybrid models and recent attention-based encoder-decoder\nmodels have considerably improved performance as such contexts can\nnow be learned in an integral fashion. However, processing such contexts\nrequires an entire utterance presentation and thus introduces unwanted\ndelays before a recognition result can be output. In this paper, we\naddress performance <i>as well as</i> latency. We present results for\na system that can achieve super-human performance, i.e. a WER of 5.0%\non the Switchboard conversational benchmark, at a word based latency\nof only 1 second behind a speaker&#8217;s speech. The system uses multiple\nattention-based encoder-decoder networks integrated within a novel\nlow latency incremental inference approach.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1114",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "joshi21_interspeech": {
      "authors": [
        [
          "Vikas",
          "Joshi"
        ],
        [
          "Amit",
          "Das"
        ],
        [
          "Eric",
          "Sun"
        ],
        [
          "Rupesh R.",
          "Mehta"
        ],
        [
          "Jinyu",
          "Li"
        ],
        [
          "Yifan",
          "Gong"
        ]
      ],
      "title": "Multiple Softmax Architecture for Streaming Multilingual End-to-End ASR Systems",
      "original": "1298",
      "page_count": 5,
      "order": 361,
      "p1": "1767",
      "pn": "1771",
      "abstract": [
        "Improving multilingual end-to-end (E2E) automatic speech recognition\n(ASR) systems have manifold advantages. They simplify the training\nstrategy, are easier to scale and exhibit better performance over monolingual\nmodels. However, it is still challenging to use a single multilingual\nmodel to recognize multiple languages without knowing the input language,\nas most multilingual models assume the availability of the input language.\nIn this paper, we introduce multi-softmax model to improve the multilingual\nrecurrent neural network transducer (RNN-T) models, by having language\nspecific softmax, joint and embedding layers, while sharing rest of\nthe parameters. We extend the multi-softmax model to work without knowing\nthe input language, by integrating a language identification (LID)\nmodel, that estimates the LID on-the-fly and also does the recognition\nat the same time. The multi-softmax model outperforms monolingual models\nwith an average word error rate relative (WERR) reduction of 4.65%\non Indian languages. Finetuning further improves the WERR reduction\nto 12.2%. The multi-softmax model with on-the-fly LID estimation, shows\nWERR reduction of 13.86% compared to the multilingual baseline.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1298",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "le21_interspeech": {
      "authors": [
        [
          "Duc",
          "Le"
        ],
        [
          "Mahaveer",
          "Jain"
        ],
        [
          "Gil",
          "Keren"
        ],
        [
          "Suyoun",
          "Kim"
        ],
        [
          "Yangyang",
          "Shi"
        ],
        [
          "Jay",
          "Mahadeokar"
        ],
        [
          "Julian",
          "Chan"
        ],
        [
          "Yuan",
          "Shangguan"
        ],
        [
          "Christian",
          "Fuegen"
        ],
        [
          "Ozlem",
          "Kalinli"
        ],
        [
          "Yatharth",
          "Saraf"
        ],
        [
          "Michael L.",
          "Seltzer"
        ]
      ],
      "title": "Contextualized Streaming End-to-End Speech Recognition with Trie-Based Deep Biasing and Shallow Fusion",
      "original": "1566",
      "page_count": 5,
      "order": 362,
      "p1": "1772",
      "pn": "1776",
      "abstract": [
        "How to leverage dynamic contextual information in end-to-end speech\nrecognition has remained an active research area. Previous solutions\nto this problem were either designed for specialized use cases that\ndid not generalize well to open-domain scenarios, did not scale to\nlarge biasing lists, or underperformed on rare long-tail words. We\naddress these limitations by proposing a novel solution that combines\nshallow fusion, trie-based deep biasing, and neural network language\nmodel contextualization. These techniques result in significant 19.5%\nrelative Word Error Rate improvement over existing contextual biasing\napproaches and 5.4%&#8211;9.3% improvement compared to a strong hybrid\nbaseline on both open-domain and constrained contextualization tasks,\nwhere the targets consist of mostly rare long-tail words. Our final\nsystem remains lightweight and modular, allowing for quick modification\nwithout model re-training.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1566",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "sainath21_interspeech": {
      "authors": [
        [
          "Tara N.",
          "Sainath"
        ],
        [
          "Yanzhang",
          "He"
        ],
        [
          "Arun",
          "Narayanan"
        ],
        [
          "Rami",
          "Botros"
        ],
        [
          "Ruoming",
          "Pang"
        ],
        [
          "David",
          "Rybach"
        ],
        [
          "Cyril",
          "Allauzen"
        ],
        [
          "Ehsan",
          "Variani"
        ],
        [
          "James",
          "Qin"
        ],
        [
          "Quoc-Nam",
          "Le-The"
        ],
        [
          "Shuo-Yiin",
          "Chang"
        ],
        [
          "Bo",
          "Li"
        ],
        [
          "Anmol",
          "Gulati"
        ],
        [
          "Jiahui",
          "Yu"
        ],
        [
          "Chung-Cheng",
          "Chiu"
        ],
        [
          "Diamantino",
          "Caseiro"
        ],
        [
          "Wei",
          "Li"
        ],
        [
          "Qiao",
          "Liang"
        ],
        [
          "Pat",
          "Rondon"
        ]
      ],
      "title": "An Efficient Streaming Non-Recurrent On-Device End-to-End Model with Improvements to Rare-Word Modeling",
      "original": "0206",
      "page_count": 5,
      "order": 363,
      "p1": "1777",
      "pn": "1781",
      "abstract": [
        "On-device end-to-end (E2E) models have shown improvements over a conventional\nmodel on Search test sets in both quality, as measured by Word Error\nRate (WER) [1], and latency [2], measured by the time the result is\nfinalized after the user stops speaking. However, the E2E model is\ntrained on a small fraction of audio-text pairs compared to the 100\nbillion text utterances that a conventional language model (LM) is\ntrained with. Thus E2E models perform poorly on rare words and phrases.\nIn this paper, building upon the two-pass streaming Cascaded Encoder\nE2E model [3], we explore using a Hybrid Autoregressive Transducer\n(HAT) [4] factorization to better integrate an on-device neural LM\ntrained on text-only data. Furthermore, to further improve decoder\nlatency we introduce a non-recurrent embedding decoder, in place of\nthe typical LSTM decoder, into the Cascaded Encoder model. Overall,\nwe present a streaming on-device model that incorporates an external\nneural LM and outperforms the conventional model in both search and\nrare-word quality, as well as latency, and is 318&#215; smaller.\n"
      ],
      "doi": "10.21437/Interspeech.2021-206",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "lu21_interspeech": {
      "authors": [
        [
          "Liang",
          "Lu"
        ],
        [
          "Naoyuki",
          "Kanda"
        ],
        [
          "Jinyu",
          "Li"
        ],
        [
          "Yifan",
          "Gong"
        ]
      ],
      "title": "Streaming Multi-Talker Speech Recognition with Joint Speaker Identification",
      "original": "0207",
      "page_count": 5,
      "order": 364,
      "p1": "1782",
      "pn": "1786",
      "abstract": [
        "In multi-talker scenarios such as meetings and conversations, speech\nprocessing systems are usually required to transcribe the audio as\nwell as identify the speakers for downstream applications. Since overlapped\nspeech is common in this case, conventional approaches usually address\nthis problem in a cascaded fashion that involves speech separation,\nspeech recognition and speaker identification that are trained independently.\nIn this paper, we propose Streaming Unmixing, Recognition and Identification\nTransducer (SURIT) &#8212; a new framework that deals with this problem\nin an end-to-end streaming fashion. SURIT employs the recurrent neural\nnetwork transducer (RNN-T) as the backbone for both speech recognition\nand speaker identification. We validate our idea on the LibrispeechMix\ndataset &#8212; a multi-talker dataset derived from Librispeech, and\npresent encouraging results.\n"
      ],
      "doi": "10.21437/Interspeech.2021-207",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "moriya21_interspeech": {
      "authors": [
        [
          "Takafumi",
          "Moriya"
        ],
        [
          "Tomohiro",
          "Tanaka"
        ],
        [
          "Takanori",
          "Ashihara"
        ],
        [
          "Tsubasa",
          "Ochiai"
        ],
        [
          "Hiroshi",
          "Sato"
        ],
        [
          "Atsushi",
          "Ando"
        ],
        [
          "Ryo",
          "Masumura"
        ],
        [
          "Marc",
          "Delcroix"
        ],
        [
          "Taichi",
          "Asami"
        ]
      ],
      "title": "Streaming End-to-End Speech Recognition for Hybrid RNN-T/Attention Architecture",
      "original": "0437",
      "page_count": 5,
      "order": 365,
      "p1": "1787",
      "pn": "1791",
      "abstract": [
        "We present a novel architecture with its decoding approach for improving\nrecurrent neural network-transducer (RNN-T) performance. RNN-T is promising\nfor building time-synchronous automatic speech recognition (ASR) systems\nand thus enhancing streaming ASR applications. We note that encoder-decoder-based\nsequence-to-sequence models (S2S) have been also used successfully\nby the ASR community. In this paper, we integrate these popular models\nin the RNN-T+S2S approach; higher recognition performance than either\nis achieved due to their integration. However, it is generally deemed\nto be complicated to use S2S in streaming systems, because the attention\nmechanism can use arbitrarily long past and future contexts during\ndecoding. Our RNN-T+S2S is composed of the shared encoder, an RNN-T\ndecoder and a triggered attention-based decoder which uses time restricted\nencoder outputs for attention weight computation. By using the trigger\npoints generated from RNN-T outputs, the S2S branch of RNN-T+S2S activates\nonly when the triggers are detected, which makes streaming ASR practical.\nExperiments on public and private datasets created to research various\ntasks demonstrate that our proposal can yield superior recognition\nperformance.\n"
      ],
      "doi": "10.21437/Interspeech.2021-437",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "schwarz21_interspeech": {
      "authors": [
        [
          "Andreas",
          "Schwarz"
        ],
        [
          "Ilya",
          "Sklyar"
        ],
        [
          "Simon",
          "Wiesler"
        ]
      ],
      "title": "Improving RNN-T ASR Accuracy Using Context Audio",
      "original": "0542",
      "page_count": 5,
      "order": 366,
      "p1": "1792",
      "pn": "1796",
      "abstract": [
        "We present a training scheme for streaming automatic speech recognition\n(ASR) based on recurrent neural network transducers (RNN-T) which allows\nthe encoder network to learn to exploit context audio from a stream,\nusing segmented or partially labeled sequences of the stream during\ntraining. We show that the use of context audio during training and\ninference can lead to word error rate reductions of more than 6% in\na realistic production setting for a voice assistant ASR system. We\ninvestigate the effect of the proposed training approach on acoustically\nchallenging data containing background speech and present data points\nwhich indicate that this approach helps the network learn both speaker\nand environment adaptation. To gain further insight into the ability\nof a long short-term memory (LSTM) based ASR encoder to exploit long-term\ncontext, we also visualize RNN-T loss gradients with respect to the\ninput.\n"
      ],
      "doi": "10.21437/Interspeech.2021-542",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "huang21e_interspeech": {
      "authors": [
        [
          "Lu",
          "Huang"
        ],
        [
          "Jingyu",
          "Sun"
        ],
        [
          "Yufeng",
          "Tang"
        ],
        [
          "Junfeng",
          "Hou"
        ],
        [
          "Jinkun",
          "Chen"
        ],
        [
          "Jun",
          "Zhang"
        ],
        [
          "Zejun",
          "Ma"
        ]
      ],
      "title": "HMM-Free Encoder Pre-Training for Streaming RNN Transducer",
      "original": "0586",
      "page_count": 5,
      "order": 367,
      "p1": "1797",
      "pn": "1801",
      "abstract": [
        "This work describes an encoder pre-training procedure using frame-wise\nlabel to improve the training of streaming recurrent neural network\ntransducer (RNN-T) model. Streaming RNN-T trained from scratch usually\nperforms worse than non-streaming RNN-T. Although it is common to address\nthis issue through pre-training components of RNN-T with other criteria\nor frame-wise alignment guidance, the alignment is not easily available\nin end-to-end manner. In this work, frame-wise alignment, used to pre-train\nstreaming RNN-T&#8217;s encoder, is generated without using a HMM-based\nsystem. Therefore an all-neural framework equipping HMM-free encoder\npre-training is constructed. This is achieved by expanding the spikes\nof CTC model to their left/right blank frames, and two expanding strategies\nare proposed. To our best knowledge, this is the first work to simulate\nHMM-based frame-wise label using CTC model for pre-training. Experiments\nconducted on LibriSpeech and MLS English tasks show the proposed pre-training\nprocedure, compared with random initialization, reduces the WER by\nrelatively 5%&#126;11% and the emission latency by 60 ms. Besides,\nthe method is lexicon-free, so it is friendly to new languages without\nmanually designed lexicon.\n"
      ],
      "doi": "10.21437/Interspeech.2021-586",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "cui21b_interspeech": {
      "authors": [
        [
          "Xiaodong",
          "Cui"
        ],
        [
          "Brian",
          "Kingsbury"
        ],
        [
          "George",
          "Saon"
        ],
        [
          "David",
          "Haws"
        ],
        [
          "Zolt\u00e1n",
          "T\u00fcske"
        ]
      ],
      "title": "Reducing Exposure Bias in Training Recurrent Neural Network Transducers",
      "original": "0587",
      "page_count": 5,
      "order": 368,
      "p1": "1802",
      "pn": "1806",
      "abstract": [
        "When recurrent neural network transducers (RNNTs) are trained using\nthe typical maximum likelihood criterion, the prediction network is\ntrained only on ground truth label sequences. This leads to a mismatch\nduring inference, known as exposure bias, when the model must deal\nwith label sequences containing errors. In this paper we investigate\napproaches to reducing exposure bias in training to improve the generalization\nof RNNT models for automatic speech recognition (ASR). A label-preserving\ninput perturbation to the prediction network is introduced. The input\ntoken sequences are perturbed using SwitchOut and scheduled sampling\nbased on an additional token language model. Experiments conducted\non the 300-hour Switchboard dataset demonstrate their effectiveness.\nBy reducing the exposure bias, we show that we can further improve\nthe accuracy of a high-performance RNNT ASR model and obtain state-of-the-art\nresults on the 300-hour Switchboard dataset.\n"
      ],
      "doi": "10.21437/Interspeech.2021-587",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "doutre21_interspeech": {
      "authors": [
        [
          "Thibault",
          "Doutre"
        ],
        [
          "Wei",
          "Han"
        ],
        [
          "Chung-Cheng",
          "Chiu"
        ],
        [
          "Ruoming",
          "Pang"
        ],
        [
          "Olivier",
          "Siohan"
        ],
        [
          "Liangliang",
          "Cao"
        ]
      ],
      "title": "Bridging the Gap Between Streaming and Non-Streaming ASR Systems by Distilling Ensembles of CTC and RNN-T Models",
      "original": "0637",
      "page_count": 5,
      "order": 369,
      "p1": "1807",
      "pn": "1811",
      "abstract": [
        "Streaming end-to-end automatic speech recognition (ASR) systems are\nwidely used in everyday applications that require transcribing speech\nto text in real-time. Their minimal latency makes them suitable for\nsuch tasks. Unlike their non-streaming counterparts, streaming models\nare constrained to be causal with no future context and suffer from\nhigher word error rates (WER). To improve streaming models, a recent\nstudy [1] proposed to distill a non-streaming teacher model on unsupervised\nutterances, and then train a streaming student using the teachers&#8217;\npredictions. However, the performance gap between teacher and student\nWERs remains high. In this paper, we aim to close this gap by using\na diversified set of non-streaming teacher models and combining them\nusing Recognizer Output Voting Error Reduction (ROVER). In particular,\nwe show that, despite being weaker than RNN-T models, CTC models are\nremarkable teachers. Further, by fusing RNN-T and CTC models together,\nwe build the strongest teachers. The resulting student models drastically\nimprove upon streaming models of previous work [1]: the WER decreases\nby 41% on Spanish, 27% on Portuguese, and 13% on French.\n"
      ],
      "doi": "10.21437/Interspeech.2021-637",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "audhkhasi21_interspeech": {
      "authors": [
        [
          "Kartik",
          "Audhkhasi"
        ],
        [
          "Tongzhou",
          "Chen"
        ],
        [
          "Bhuvana",
          "Ramabhadran"
        ],
        [
          "Pedro J.",
          "Moreno"
        ]
      ],
      "title": "Mixture Model Attention: Flexible Streaming and Non-Streaming Automatic Speech Recognition",
      "original": "0720",
      "page_count": 5,
      "order": 370,
      "p1": "1812",
      "pn": "1816",
      "abstract": [
        "Streaming automatic speech recognition (ASR) hypothesizes words as\nsoon as the input audio arrives, whereas non-streaming ASR can potentially\nwait for the completion of the entire utterance to hypothesize words.\nStreaming and non-streaming ASR systems have typically used different\nacoustic encoders. Recent work has attempted to unify them by either\njointly training a fixed stack of streaming and non-streaming layers\nor using knowledge distillation during training to ensure consistency\nbetween the streaming and non-streaming predictions. We propose mixture\nmodel (MiMo) attention as a simpler and theoretically-motivated alternative\nthat replaces only the attention mechanism, requires no change to the\ntraining loss, and allows greater flexibility of switching between\nstreaming and non-streaming mode during inference. Our experiments\non the public Librispeech data set and a few Indic language data sets\nshow that MiMo attention endows a single ASR model with the ability\nto operate in both streaming and non-streaming modes without any overhead\nand without significant loss in accuracy compared to separately-trained\nstreaming and non-streaming models. We also illustrate this benefit\nof MiMo attention in a second-pass rescoring setting.\n"
      ],
      "doi": "10.21437/Interspeech.2021-720",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "inaguma21_interspeech": {
      "authors": [
        [
          "Hirofumi",
          "Inaguma"
        ],
        [
          "Tatsuya",
          "Kawahara"
        ]
      ],
      "title": "StableEmit: Selection Probability Discount for Reducing Emission Latency of Streaming Monotonic Attention ASR",
      "original": "1110",
      "page_count": 5,
      "order": 371,
      "p1": "1817",
      "pn": "1821",
      "abstract": [
        "While attention-based encoder-decoder (AED) models have been successfully\nextended to the online variants for streaming automatic speech recognition\n(ASR), such as monotonic chunkwise attention (MoChA), the models still\nhave a large label emission latency because of the unconstrained end-to-end\ntraining objective. Previous works tackled this problem by leveraging\nalignment information to control the timing to emit tokens during training.\nIn this work, we propose a simple <i>alignment-free</i> regularization\nmethod, <i>StableEmit</i>, to encourage MoChA to emit tokens earlier.\nStableEmit discounts the selection probabilities in hard monotonic\nattention for token boundary detection by a constant factor and regularizes\nthem to recover the total attention mass during training. As a result,\nthe scale of the selection probabilities is increased, and the values\ncan reach a threshold for token emission earlier, leading to a reduction\nof emission latency and deletion errors. Moreover, StableEmit can be\ncombined with methods that constraint alignments to further improve\nthe accuracy and latency. Experimental evaluations with LSTM and Conformer\nencoders demonstrate that StableEmit significantly reduces the recognition\nerrors and the emission latency simultaneously. We also show that the\nuse of alignment information is complementary in both metrics.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1110",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "moritz21_interspeech": {
      "authors": [
        [
          "Niko",
          "Moritz"
        ],
        [
          "Takaaki",
          "Hori"
        ],
        [
          "Jonathan Le",
          "Roux"
        ]
      ],
      "title": "Dual Causal/Non-Causal Self-Attention for Streaming End-to-End Speech Recognition",
      "original": "1693",
      "page_count": 5,
      "order": 372,
      "p1": "1822",
      "pn": "1826",
      "abstract": [
        "Attention-based end-to-end automatic speech recognition (ASR) systems\nhave recently demonstrated state-of-the-art results for numerous tasks.\nHowever, the application of self-attention and attention-based encoder-decoder\nmodels remains challenging for streaming ASR, where each word must\nbe recognized shortly after it was spoken. In this work, we present\nthe dual causal/non-causal self-attention (DCN) architecture, which\nin contrast to restricted self-attention prevents the overall context\nto grow beyond the look-ahead of a single layer when used in a deep\narchitecture. DCN is compared to chunk-based and restricted self-attention\nusing streaming transformer and conformer architectures, showing improved\nASR performance over restricted self-attention and competitive ASR\nresults compared to chunk-based self-attention, while providing the\nadvantage of frame-synchronous processing. Combined with triggered\nattention, the proposed streaming end-to-end ASR systems obtained state-of-the-art\nresults on the LibriSpeech, HKUST, and Switchboard ASR tasks.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1693",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "kim21d_interspeech": {
      "authors": [
        [
          "Kwangyoun",
          "Kim"
        ],
        [
          "Felix",
          "Wu"
        ],
        [
          "Prashant",
          "Sridhar"
        ],
        [
          "Kyu J.",
          "Han"
        ],
        [
          "Shinji",
          "Watanabe"
        ]
      ],
      "title": "Multi-Mode Transformer Transducer with Stochastic Future Context",
      "original": "1953",
      "page_count": 5,
      "order": 373,
      "p1": "1827",
      "pn": "1831",
      "abstract": [
        "Automatic speech recognition (ASR) models make fewer errors when more\nsurrounding speech information is presented as context. Unfortunately,\nacquiring a larger future context leads to higher latency. There exists\nan inevitable trade-off between speed and accuracy. Na&#239;vely, to\nfit different latency requirements, people have to store multiple models\nand pick the best one under the constraints. Instead, a more desirable\napproach is to have a single model that can dynamically adjust its\nlatency based on different constraints, which we refer to as <i>Multi-mode\nASR</i>. A Multi-mode ASR model can fulfill various latency requirements\nduring inference &#8212; when a larger latency becomes acceptable,\nthe model can process longer future context to achieve higher accuracy\nand when a latency budget is not flexible, the model can be less dependent\non future context but still achieve reliable accuracy. In pursuit of\nMulti-mode ASR, we propose <i>Stochastic Future Context</i>, a simple\ntraining procedure that samples one streaming configuration in each\niteration. Through extensive experiments on AISHELL-1 and LibriSpeech\ndatasets, we show that a Multi-mode ASR model rivals, if not surpasses,\na set of competitive streaming baselines trained with different latency\nbudgets.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1953",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "ren21_interspeech": {
      "authors": [
        [
          "Xinlei",
          "Ren"
        ],
        [
          "Xu",
          "Zhang"
        ],
        [
          "Lianwu",
          "Chen"
        ],
        [
          "Xiguang",
          "Zheng"
        ],
        [
          "Chen",
          "Zhang"
        ],
        [
          "Liang",
          "Guo"
        ],
        [
          "Bing",
          "Yu"
        ]
      ],
      "title": "A Causal U-Net Based Neural Beamforming Network for Real-Time Multi-Channel Speech Enhancement",
      "original": "1457",
      "page_count": 5,
      "order": 374,
      "p1": "1832",
      "pn": "1836",
      "abstract": [
        "People are meeting through video conferencing more often. While single\nchannel speech enhancement techniques are useful for the individual\nparticipants, the speech quality will be significantly degraded in\nlarge meeting rooms where the far-field and reverberate conditions\nare introduced. Approaches based on microphone array signal processing\nare proposed to explore the inter-channel correlation among the individual\nmicrophone channels. In this work, a new causal U-net based multiple-in-multiple-out\nstructure is proposed for real-time multi-channel speech enhancement.\nThe proposed method incorporates the traditional beamforming structure\nwith the multi-channel causal U-net by explicitly adding a beamforming\noperation at the end of the neural beamformer. The proposed method\nhas entered the INTERSPEECH Far-field Multi-Channel Speech Enhancement\nChallenge for Video Conferencing. With 1.97M model parameters and 0.25\nreal-time factor on Intel Core i7 (2.6GHz) CPU, the proposed method\nhas outperforms the baseline system of this challenge on PESQ, Si-SNR\nand STOI metrics.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1457",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement:14 Special Sessions"
    },
    "zhu21d_interspeech": {
      "authors": [
        [
          "Rui",
          "Zhu"
        ],
        [
          "Feiran",
          "Yang"
        ],
        [
          "Yuepeng",
          "Li"
        ],
        [
          "Shidong",
          "Shang"
        ]
      ],
      "title": "A Partitioned-Block Frequency-Domain Adaptive Kalman Filter for Stereophonic Acoustic Echo Cancellation",
      "original": "0135",
      "page_count": 5,
      "order": 375,
      "p1": "1837",
      "pn": "1841",
      "abstract": [
        "The rapid development of online video conferencing systems has caused\nrenewed attention to the multi-channel recording and playback systems.\nStereophonic acoustic echo cancellation (SAEC) is the key issue of\nthis systems. This paper proposes an optimally designed partitioned-block\nfrequency-domain Kalman filter (PBFDKF) algorithm for SAEC. We establish\nthe frequency-domain observation equation using the overlap-and-save\nmethod and we use the first-order Markov model to describe the state\nequation. The exact PBFDKF algorithm is derived under the umbrella\nof Kalman filter theory and two fast implementations are then presented\nto reduce the complexity. The proposed algorithm is equivalent to the\ndual-channel partitioned-block frequency-domain gradient-based algorithm\nwith optimum step-size control, and hence it exhibits very good convergence\nperformance and is found to be robust to near-end interference without\na double-talk detector. Extensive experiments in different SAEC conditions\nconfirm the effectiveness of the proposed algorithm.\n"
      ],
      "doi": "10.21437/Interspeech.2021-135",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement:14 Special Sessions"
    },
    "wang21p_interspeech": {
      "authors": [
        [
          "Taihui",
          "Wang"
        ],
        [
          "Feiran",
          "Yang"
        ],
        [
          "Rui",
          "Zhu"
        ],
        [
          "Jun",
          "Yang"
        ]
      ],
      "title": "Real-Time Independent Vector Analysis Using Semi-Supervised Nonnegative Matrix Factorization as a Source Model",
      "original": "0146",
      "page_count": 5,
      "order": 376,
      "p1": "1842",
      "pn": "1846",
      "abstract": [
        "Online independent vector analysis (IVA) based on auxiliary technology\nis effective to separate audio source in real time. However, the separated\nsignal may contain residual interference noise because the source model\nof IVA lacks flexibility and cannot treat the specific harmonic structures\nof sources. This paper presents a real-time IVA method where the amplitude\nspectrum of separated signal is modeled by semi-supervised nonnegative\nmatrix factorization (SSNMF). Using the pre-trained basis matrix which\ncontains source structures, we can extract the target source from the\nseparated signal in real time. The advantage of the proposed method\nis that the extracted source can provide a more accurate variance than\nthe separated signal and hence the proposed method can obtain a better\nseparation performance than the oracle IVA. Experimental results in\nspeech denoising task show the effectiveness and the robustness of\nthe proposed method with different types of noise.\n"
      ],
      "doi": "10.21437/Interspeech.2021-146",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement:14 Special Sessions"
    },
    "han21b_interspeech": {
      "authors": [
        [
          "Jiangyu",
          "Han"
        ],
        [
          "Wei",
          "Rao"
        ],
        [
          "Yannan",
          "Wang"
        ],
        [
          "Yanhua",
          "Long"
        ]
      ],
      "title": "Improving Channel Decorrelation for Multi-Channel Target Speech Extraction",
      "original": "0298",
      "page_count": 5,
      "order": 377,
      "p1": "1847",
      "pn": "1851",
      "abstract": [
        "Target speech extraction has attracted widespread attention. When microphone\narrays are available, the additional spatial information can be helpful\nin extracting the target speech. We have recently proposed a channel\ndecorrelation (CD) mechanism to extract the inter-channel differential\ninformation to enhance the reference channel encoder representation.\nAlthough the proposed mechanism has shown promising results for extracting\nthe target speech from mixtures, the extraction performance is still\nlimited by the nature of the original decorrelation theory. In this\npaper, we propose two methods to broaden the horizon of the original\nchannel decorrelation, by replacing the original softmax-based inter-channel\nsimilarity between encoder representations, using an unrolled probability\nand a normalized cosine-based similarity at the dimensional-level.\nMoreover, new combination strategies of the CD-based spatial information\nand target speaker adaptation of parallel encoder outputs are also\ninvestigated. Experiments on the reverberant WSJ0 2-mix show that the\nimproved CD can result in more discriminative differential information\nand the new adaptation strategy is also very effective to improve the\ntarget speech extraction.\n"
      ],
      "doi": "10.21437/Interspeech.2021-298",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement:14 Special Sessions"
    },
    "liu21f_interspeech": {
      "authors": [
        [
          "Jinjiang",
          "Liu"
        ],
        [
          "Xueliang",
          "Zhang"
        ]
      ],
      "title": "Inplace Gated Convolutional Recurrent Neural Network for Dual-Channel Speech Enhancement",
      "original": "0899",
      "page_count": 5,
      "order": 378,
      "p1": "1852",
      "pn": "1856",
      "abstract": [
        "For dual-channel speech enhancement, it is a promising idea to design\nan end-to-end model based on the traditional array signal processing\nguideline and the manifold space of multi-channel signals. We found\nthat the idea above can be effectively implemented by the classical\nconvolutional recurrent neural networks (CRN) architecture. We propose\na very compact inplace gated convolutional recurrent neural network\n(inplace GCRN) for end-to-end multi-channel speech enhancement, which\nutilizes inplace-convolution for frequency pattern extraction and reconstruction.\nThe inplace characteristics efficiently preserve spatial cues in each\nfrequency bin for channel-wise long short-term memory neural networks\n(LSTM) tracing the spatial source. In addition, we come up with a new\nspectrum recovery method by predict amplitude mask, mapping, and phase,\nwhich effectively improves the speech quality.\n"
      ],
      "doi": "10.21437/Interspeech.2021-899",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement:14 Special Sessions"
    },
    "raj21_interspeech": {
      "authors": [
        [
          "R.G. Prithvi",
          "Raj"
        ],
        [
          "Rohit",
          "Kumar"
        ],
        [
          "M.K.",
          "Jayesh"
        ],
        [
          "Anurenjan",
          "Purushothaman"
        ],
        [
          "Sriram",
          "Ganapathy"
        ],
        [
          "M.A. Basha",
          "Shaik"
        ]
      ],
      "title": "SRIB-LEAP Submission to Far-Field Multi-Channel Speech Enhancement Challenge for Video Conferencing",
      "original": "1111",
      "page_count": 5,
      "order": 379,
      "p1": "1857",
      "pn": "1861",
      "abstract": [
        "This paper presents the details of the SRIB-LEAP submission to the\nConferencingSpeech challenge 2021. The challenge involved the task\nof multi-channel speech enhancement to improve the quality of far field\nspeech from microphone arrays in a video conferencing room. We propose\na two stage method involving a beamformer followed by single channel\nenhancement. For the beamformer, we incorporated self-attention mechanism\nas inter-channel processing layer in the filter-and-sum network (FaSNet),\nan end-to-end time-domain beamforming system. The single channel speech\nenhancement is done in log spectral domain using convolution neural\nnetwork (CNN)-long short term memory (LSTM) based architecture. We\nachieved improvements in objective quality metrics &#8212; perceptual\nevaluation of speech quality (PESQ) of 0.5 on the noisy data. On subjective\nquality evaluation, the proposed approach improved the mean opinion\nscore (MOS) by an absolute measure of 0.9 over the noisy audio.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1111"
    },
    "xue21_interspeech": {
      "authors": [
        [
          "Cheng",
          "Xue"
        ],
        [
          "Weilong",
          "Huang"
        ],
        [
          "Weiguang",
          "Chen"
        ],
        [
          "Jinwei",
          "Feng"
        ]
      ],
      "title": "Real-Time Multi-Channel Speech Enhancement Based on Neural Network Masking with Attention Model",
      "original": "2266",
      "page_count": 5,
      "order": 380,
      "p1": "1862",
      "pn": "1866",
      "abstract": [
        "In this paper, we propose a real-time multi-channel speech enhancement\nmethod for noise reduction and dereverberation in far-field environments.\nThe proposed method consists of two components: differential beamforming\nand mask estimation network. The differential beamforming is employed\nto suppress the interference signals from non-target directions such\nthat a relatively clean speech can be obtained. The mask estimation\nnetwork with an attention model is developed to capture the signal\ncorrelation among different channels in the feature extraction stage\nand enhance the feature representation that needs to be reconstructed\ninto the target speech in the estimation mask stage. In the inference\nphase, the spectrum after differential beamforming is filtered by the\nestimated mask to obtain the final output. The spectrum after differential\nbeamforming can provide a higher signal-to-noise ratio (SNR) than the\noriginal spectrum, so the estimated mask can more easily filter out\nthe noise. We conducted experiments on the ConferencingSpeech2021 challenge\n(INTERSPEECH 2021) dataset to evaluate the proposed method. With only\n2.9M parameters, the proposed method achieved competitive performance.\n"
      ],
      "doi": "10.21437/Interspeech.2021-2266",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement:14 Special Sessions"
    },
    "ganapathy21_interspeech": {
      "authors": [
        [
          "Sriram",
          "Ganapathy"
        ]
      ],
      "title": "Uncovering the Acoustic Cues of COVID-19 Infection",
      "original": "abs12",
      "page_count": 0,
      "order": 381,
      "p1": "0",
      "pn": "",
      "abstract": [
        "The investigation of acoustic biomarkers of respiratory diseases has\nsocietal and public health impact following the onset of COVID-19 pandemic.\nThe efforts in the pre-pandemic period focused on developing smartphone\nfriendly diagnostic tools for the detection of chronic pulmonary diseases,\nTuberculosis and asthmatic conditions using cough sounds. During the\npast two years, several research works of varying scales have been\nundertaken by the speech and signal processing community for analyzing\nthe acoustic symptoms of COVID. The motivation for the development\nof acoustic-based tools for COVID diagnostics arises from the key limitations\nof cost, time, and safety of the current gold standard in COVID testing,\nnamely the reverse transcription polymerase chain reaction (RT-PCR)\ntesting.<br style=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  In this talk, I will survey the major efforts undertaken by groups\nacross the world in i) developing data resources of acoustic signals\nfor COVID-19 diagnostics, and ii) designing models and learning algorithms\nfor tool development. The landscape of data resources ranges from controlled\nhospital recordings to crowdsourced smartphone-based data. While the\nprimary signal modality recorded is the cough data, the impact of COVID\non other modalities like breathing, speech and symptom data are also\nstudied. In the talk, I will also discuss the considerations in designing\ndata representations and machine learning models for COVID detection\nfrom acoustic data. The pointers to open-source data resources and\ntools will be highlighted with the aim of encouraging budding researchers\nto pursue this important direction.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  The talk will conclude\nby remarking about the progress made by our group, Coswara, where a\nmulti-modal combination of information from several modalities shows\nthe potential to surpass regulatory requirements needed for a rapid\nacoustic-based point of care testing (POCT) tool. \n"
      ]
    },
    "fung21_interspeech": {
      "authors": [
        [
          "Pascale",
          "Fung"
        ]
      ],
      "title": "Ethical and Technological Challenges of Conversational AI",
      "original": "abs13",
      "page_count": 0,
      "order": 382,
      "p1": "0",
      "pn": "",
      "abstract": [
        "Conversational AI (ConvAI) systems have applications ranging from personal\nassistance, health assistance to customer services. They have been\nin place since the first call centre agent went live in the late 1990s.\nMore recently, smart speakers and smartphones are powered with conversational\nAI with similar architecture as those from the 90s. On the other hand,\nresearch on ConvAI systems has made leaps and bounds in recent years\nwith sequence-to-sequence, generation-based models. Thanks to the advent\nof large scale pre-trained language models, state-of-the-art ConvAI\nsystems can generate surprisingly human-like responses to user queries\nin open domain conversations, known as chit-chat. However, these generation\nbased ConvAI systems are difficult to control and can lead to inappropriate,\nbiased and sometimes even toxic responses. In addition, unlike previous\nmodular conversational AI systems, it is also challenging to incorporate\nexternal knowledge into these models for task-oriented dialog scenarios\nsuch as personal assistance and customer services, and to maintain\nconsistency.<br style=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  With great power comes great responsibility. We must address the\nmany ethical and technical challenges of generation based conversational\nAI systems to control for bias and safety, consistency, style, knowledge\nincorporation, etc. In this talk, I will introduce state-of-the-art\ngeneration based conversational AI approaches, and will point out remaining\nchallenges of conversational AI and possible directions for future\nresearch, including how to mitigate inappropriate responses. I will\nalso present some ethical guidelines that conversational AI systems\ncan follow.\n"
      ]
    },
    "fohr21_interspeech": {
      "authors": [
        [
          "Dominique",
          "Fohr"
        ],
        [
          "Irina",
          "Illina"
        ]
      ],
      "title": "BERT-Based Semantic Model for Rescoring N-Best Speech Recognition List",
      "original": "0313",
      "page_count": 5,
      "order": 383,
      "p1": "1867",
      "pn": "1871",
      "abstract": [
        "This work aims to improve automatic speech recognition (ASR) by modeling\nlong-term semantic relations. We propose to perform this through rescoring\nthe ASR N-best hypotheses list. To achieve this, we propose two deep\nneural network (DNN) models and combine semantic, acoustic, and linguistic\ninformation. Our DNN rescoring models are aimed at selecting hypotheses\nthat have better semantic consistency and therefore lower WER. We investigate\na powerful representation as part of input features to our DNN model:\ndynamic contextual embeddings from Transformer-based BERT. Acoustic\nand linguistic features are also included. We perform experiments on\nthe publicly available dataset TED-LIUM. We evaluate in clean and in\nnoisy conditions, with n-gram and Recurrent Neural Network Language\nModel (RNNLM), more precisely Long Short-Term Memory (LSTM) model.\nThe proposed rescoring approaches give significant WER improvements\nover the ASR system without rescoring models. Furthermore, the combination\nof rescoring methods based on BERT and GPT-2 scores achieves the best\nresults.\n"
      ],
      "doi": "10.21437/Interspeech.2021-313",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "benes21_interspeech": {
      "authors": [
        [
          "Karel",
          "Bene\u0161"
        ],
        [
          "Luk\u00e1\u0161",
          "Burget"
        ]
      ],
      "title": "Text Augmentation for Language Models in High Error Recognition Scenario",
      "original": "0627",
      "page_count": 5,
      "order": 384,
      "p1": "1872",
      "pn": "1876",
      "abstract": [
        "In this paper, we explore several data augmentation strategies for\ntraining of language models for speech recognition. We compare augmentation\nbased on global error statistics with one based on unigram statistics\nof ASR errors and with label-smoothing and its sampled variant. Additionally,\nwe investigate the stability and the predictive power of perplexity\nestimated on augmented data. Despite being trivial, augmentation driven\nby global substitution, deletion and insertion rates achieves the best\nrescoring results. On the other hand, even though the associated perplexity\nmeasure is stable, it gives no better prediction of the final error\nrate than the vanilla one. Our best augmentation scheme increases the\nWER improvement from second-pass rescoring from 1.1% to 1.9% absolute\non the CHiMe-6 challenge.\n"
      ],
      "doi": "10.21437/Interspeech.2021-627",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "gao21b_interspeech": {
      "authors": [
        [
          "Yingbo",
          "Gao"
        ],
        [
          "David",
          "Thulke"
        ],
        [
          "Alexander",
          "Gerstenberger"
        ],
        [
          "Khoa Viet",
          "Tran"
        ],
        [
          "Ralf",
          "Schl\u00fcter"
        ],
        [
          "Hermann",
          "Ney"
        ]
      ],
      "title": "On Sampling-Based Training Criteria for Neural Language Modeling",
      "original": "1067",
      "page_count": 5,
      "order": 385,
      "p1": "1877",
      "pn": "1881",
      "abstract": [
        "As the vocabulary size of modern word-based language models becomes\never larger, many sampling-based training criteria are proposed and\ninvestigated. The essence of these sampling methods is that the softmax-related\ntraversal over the entire vocabulary can be simplified, giving speedups\ncompared to the baseline. A problem we notice about the current landscape\nof such sampling methods is the lack of a systematic comparison and\nsome myths about preferring one over another. In this work, we consider\nMonte Carlo sampling, importance sampling, a novel method we call compensated\npartial summation, and noise contrastive estimation. Linking back to\nthe three traditional criteria, namely mean squared error, binary cross-entropy,\nand cross-entropy, we derive the theoretical solutions to the training\nproblems. Contrary to some common belief, we show that all these sampling\nmethods can perform equally well, as long as we correct for the intended\nclass posterior probabilities. Experimental results in language modeling\nand automatic speech recognition on Switchboard and LibriSpeech support\nour claim, with all sampling-based methods showing similar perplexities\nand word error rates while giving the expected speedups.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1067",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "pylkkonen21_interspeech": {
      "authors": [
        [
          "Janne",
          "Pylkk\u00f6nen"
        ],
        [
          "Antti",
          "Ukkonen"
        ],
        [
          "Juho",
          "Kilpikoski"
        ],
        [
          "Samu",
          "Tamminen"
        ],
        [
          "Hannes",
          "Heikinheimo"
        ]
      ],
      "title": "Fast Text-Only Domain Adaptation of RNN-Transducer Prediction Network",
      "original": "1191",
      "page_count": 5,
      "order": 386,
      "p1": "1882",
      "pn": "1886",
      "abstract": [
        "Adaption of end-to-end speech recognition systems to new tasks is known\nto be challenging. A number of solutions have been proposed which apply\nexternal language models with various fusion methods, possibly with\na combination of two-pass decoding. Also TTS systems have been used\nto generate adaptation data for the end-to-end models. In this paper\nwe show that RNN-transducer models can be effectively adapted to new\ndomains using only small amounts of textual data. By taking advantage\nof model&#8217;s inherent structure, where the prediction network is\ninterpreted as a language model, we can apply fast adaptation to the\nmodel. Adapting the model avoids the need for complicated decoding\ntime fusions and external language models. Using appropriate regularization,\nthe prediction network can be adapted to new domains while still retaining\ngood generalization capabilities. We show with multiple ASR evaluation\ntasks how this method can provide relative gains of 10&#8211;45% in\ntarget task WER. We also share insights how RNN-transducer prediction\nnetwork performs as a language model.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1191",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "cieri21_interspeech": {
      "authors": [
        [
          "Christopher",
          "Cieri"
        ],
        [
          "James",
          "Fiumara"
        ],
        [
          "Jonathan",
          "Wright"
        ]
      ],
      "title": "Using Games to Augment Corpora for Language Recognition and Confusability",
      "original": "1611",
      "page_count": 5,
      "order": 387,
      "p1": "1887",
      "pn": "1891",
      "abstract": [
        "We present a Game with a Purpose to elicit judgements of the language\nspoken in short audio clips of broadcast and conversational telephone\nspeech, the resulting corpus and their potential use in research on\nlanguage recognition and confusability.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1611",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "fenu21_interspeech": {
      "authors": [
        [
          "Gianni",
          "Fenu"
        ],
        [
          "Mirko",
          "Marras"
        ],
        [
          "Giacomo",
          "Medda"
        ],
        [
          "Giacomo",
          "Meloni"
        ]
      ],
      "title": "Fair Voice Biometrics: Impact of Demographic Imbalance on Group Fairness in Speaker Recognition",
      "original": "1857",
      "page_count": 5,
      "order": 388,
      "p1": "1892",
      "pn": "1896",
      "abstract": [
        "Speaker recognition systems are playing a key role in modern online\napplications. Though the susceptibility of these systems to discrimination\naccording to group fairness metrics has been recently studied, their\nassessment has been mainly focused on the difference in equal error\nrate across groups, not accounting for other fairness criteria important\nin anti-discrimination policies, defined for demographic groups characterized\nby sensitive attributes. In this paper, we therefore study how existing\ngroup fairness metrics relate with the balancing settings of the training\ndata set in speaker recognition. We conduct this analysis by operationalizing\nseveral definitions of fairness and monitoring them under varied data\nbalancing settings. Experiments performed on three deep neural architectures,\nevaluated on a data set including gender/age-based groups, show that\nbalancing group representation positively impacts on fairness and that\nthe friction across security, usability, and fairness depends on the\nfairness metric and the recognition threshold.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1857",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "zhang21m_interspeech": {
      "authors": [
        [
          "Leying",
          "Zhang"
        ],
        [
          "Zhengyang",
          "Chen"
        ],
        [
          "Yanmin",
          "Qian"
        ]
      ],
      "title": "Knowledge Distillation from Multi-Modality to Single-Modality for Person Verification",
      "original": "2119",
      "page_count": 5,
      "order": 389,
      "p1": "1897",
      "pn": "1901",
      "abstract": [
        "Voice and face are two important biometric characteristics that can\nbe used for person identity verification. Previous works have proved\nthe strong complementarity between audio and visual modalities in person\nverification tasks that multi-modality system can achieve significant\nperformance improvement compared to single-modality system. However,\ndue to the limitations in the real world, it is hard to access both\naudio and visual data at the same time. In this paper, we investigate\nseveral strategies to distill the knowledge from a multi-modality system\nand transfer it to the single-modality system in a teacher-student\nmode. We applied the knowledge distillation at three different levels:\nlabel level, embedding level, and distribution level. All the experiments\nare based on the VoxCeleb dataset. The results show that the visual\nsingle-modality system achieves 10% EER (equal error rate) improvement\non the VoxCeleb1 evaluation set using our proposed knowledge distillation\nmethod. Besides, the improvement on the audio system is only reflected\non part of the evaluation trials, and we give a detailed analysis for\nthis phenomenon.\n"
      ],
      "doi": "10.21437/Interspeech.2021-2119",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "noe21_interspeech": {
      "authors": [
        [
          "Paul-Gauthier",
          "No\u00e9"
        ],
        [
          "Mohammad",
          "Mohammadamini"
        ],
        [
          "Driss",
          "Matrouf"
        ],
        [
          "Titouan",
          "Parcollet"
        ],
        [
          "Andreas",
          "Nautsch"
        ],
        [
          "Jean-Fran\u00e7ois",
          "Bonastre"
        ]
      ],
      "title": "Adversarial Disentanglement of Speaker Representation for Attribute-Driven Privacy Preservation",
      "original": "1712",
      "page_count": 5,
      "order": 390,
      "p1": "1902",
      "pn": "1906",
      "abstract": [
        "In speech technologies, speaker&#8217;s voice representation is used\nin many applications such as speech recognition, voice conversion,\nspeech synthesis and, obviously, user authentication. Modern vocal\nrepresentations of the speaker are based on neural embeddings. In addition\nto the targeted information, these representations usually contain\nsensitive information about the speaker, like the age, sex, physical\nstate, education level or ethnicity. In order to allow the user to\nchoose which information to protect, we introduce in this paper the\nconcept of <i>attribute-driven privacy preservation</i> in speaker\nvoice representation. It allows a person to hide one or more personal\naspects to a potential malicious interceptor and to the application\nprovider. As a first solution to this concept, we propose to use an\nadversarial autoencoding method that disentangles in the voice representation\na given speaker attribute thus allowing its concealment. We focus here\non the sex attribute for an Automatic Speaker Verification (ASV) task.\nExperiments carried out using the VoxCeleb datasets have shown that\nthe proposed method enables the concealment of this attribute while\npreserving ASV ability.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1712",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "romana21_interspeech": {
      "authors": [
        [
          "Amrit",
          "Romana"
        ],
        [
          "John",
          "Bandon"
        ],
        [
          "Matthew",
          "Perez"
        ],
        [
          "Stephanie",
          "Gutierrez"
        ],
        [
          "Richard",
          "Richter"
        ],
        [
          "Angela",
          "Roberts"
        ],
        [
          "Emily Mower",
          "Provost"
        ]
      ],
      "title": "Automatically Detecting Errors and Disfluencies in Read Speech to Predict Cognitive Impairment in People with Parkinson&#8217;s Disease",
      "original": "1694",
      "page_count": 5,
      "order": 391,
      "p1": "1907",
      "pn": "1911",
      "abstract": [
        "Parkinson&#8217;s disease (PD) is a central nervous system disorder\nthat causes motor impairment. Recent studies have found that people\nwith PD also often suffer from cognitive impairment (CI). While a large\nbody of work has shown that speech can be used to predict motor symptom\nseverity in people with PD, much less has focused on cognitive symptom\nseverity. Existing work has investigated if acoustic features, derived\nfrom speech, can be used to detect CI in people with PD. However, these\nacoustic features are general and are not targeted toward capturing\nCI. Speech errors and disfluencies provide additional insight into\nCI. In this study, we focus on read speech, which offers a controlled\ntemplate from which we can detect errors and disfluencies, and we analyze\nhow errors and disfluencies vary with CI. The novelty of this work\nis an automated pipeline, including transcription and error and disfluency\ndetection, capable of predicting CI in people with PD. This will enable\nefficient analyses of how cognition modulates speech for people with\nPD, leading to scalable speech assessments of CI.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1694",
      "author_area_id": "13",
      "author_area_label": " Speech, Voice, and Hearing Disorders"
    },
    "vaysse21_interspeech": {
      "authors": [
        [
          "Robin",
          "Vaysse"
        ],
        [
          "J\u00e9r\u00f4me",
          "Farinas"
        ],
        [
          "Corine",
          "Ast\u00e9sano"
        ],
        [
          "R\u00e9gine",
          "Andr\u00e9-Obrecht"
        ]
      ],
      "title": "Automatic Extraction of Speech Rhythm Descriptors for Speech Intelligibility Assessment in the Context of Head and Neck Cancers",
      "original": "1736",
      "page_count": 5,
      "order": 392,
      "p1": "1912",
      "pn": "1916",
      "abstract": [
        "The temporal dimension of speech acoustics is rarely taken into account\nin automatic models for Speech Intelligibility evaluation, although\nthe rhythmic recurrence of phonemes, syllables and prosodic groups\nare allegedly good predictors of speech intelligibility. The present\nstudy aims at unravelling those automatic parameters that best account\nfor the different levels of the speech signal&#8217;s rhythmic structure,\nand to evaluate their correlation with a perceptual intelligibility\nmeasure. The parameters are extracted from the Fourier Transform of\nthe amplitude modulation of the signal (Envelope Modulation Spectrum)\n[1, 2]. A Lasso linear model for feature selection is first implemented\nto select the most relevant parameters, and a SVR regression analysis\nis run to reveal the best parameters&#8217; combination. Our analyses\nof EMS, using data from the French corpora of cancer speech C2SI [3],\nshow strong performances of the automatic prediction, with a correlation\nof 0.70 between our model and an intelligibility evaluation score by\nspeech-pathologists. In particular, the highest correlation with speech\nintelligibility lies in the ratio between the energy in the low frequency\nband (0.5&#8211;4 Hz that represents slow rhythmic modulations indicative\nof prosodic groups) and in the higher one (4&#8211;10 Hz that represents\nfast rhythmic modulations like phonemes).\n"
      ],
      "doi": "10.21437/Interspeech.2021-1736",
      "author_area_id": "13",
      "author_area_label": " Speech, Voice, and Hearing Disorders"
    },
    "qi21b_interspeech": {
      "authors": [
        [
          "Jinzi",
          "Qi"
        ],
        [
          "Hugo",
          "Van hamme"
        ]
      ],
      "title": "Speech Disorder Classification Using Extended Factorized Hierarchical Variational Auto-Encoders",
      "original": "2180",
      "page_count": 5,
      "order": 393,
      "p1": "1917",
      "pn": "1921",
      "abstract": [
        "Objective speech disorder classification for speakers with communication\ndifficulty is desirable for diagnosis and administering therapy. With\nthe current state of speech technology, it is evident to propose neural\nnetworks for this application. But neural network model training is\nhampered by a lack of labeled disordered speech data. In this research,\nwe apply an extended version of Factorized Hierarchical Variational\nAuto-encoders (FHVAE) for representation learning on disordered speech.\nThe FHVAE model extracts both content-related and sequence-related\nlatent variables from speech data, and we utilize the extracted variables\nto explore how disorder type information is represented in the latent\nvariables. For better classification performance, the latent variables\nare aggregated at the word and sentence level. We show that an extension\nof the FHVAE model succeeds in the better disentanglement of the content-related\nand sequence-related related representations, but both representations\nare still required for best results on disorder type classification.\n"
      ],
      "doi": "10.21437/Interspeech.2021-2180",
      "author_area_id": "13",
      "author_area_label": " Speech, Voice, and Hearing Disorders"
    },
    "mathad21_interspeech": {
      "authors": [
        [
          "Vikram C.",
          "Mathad"
        ],
        [
          "Tristan J.",
          "Mahr"
        ],
        [
          "Nancy",
          "Scherer"
        ],
        [
          "Kathy",
          "Chapman"
        ],
        [
          "Katherine C.",
          "Hustad"
        ],
        [
          "Julie",
          "Liss"
        ],
        [
          "Visar",
          "Berisha"
        ]
      ],
      "title": "The Impact of Forced-Alignment Errors on Automatic Pronunciation Evaluation",
      "original": "1403",
      "page_count": 5,
      "order": 394,
      "p1": "1922",
      "pn": "1926",
      "abstract": [
        "Automatic evaluation of phone-level pronunciation scores typically\ninvolves two stages: (1) automatic phonetic segmentation via text-constrained\nphoneme alignment and (2) quantification of acoustic deviation for\neach phoneme-level relative to a database of correctly-pronounced speech.\nIt&#8217;s clear that the second stage depends on the first. That is,\nif there is misalignment, the acoustic deviation will also be impacted.\nIn this paper, we analyzed the impact of alignment error on a measure\nof goodness of pronunciation. We computed (1) automatic pronunciation\nscores using force-aligned samples, (2) the forced-alignment error\nrate, and (3) acoustic deviation using manually-aligned samples. We\nused a bivariate linear regression model to characterize the contributions\nof forced alignment errors and acoustic deviation on the automatic\npronunciation scores. This was done across two different children speech\ndatabases, namely children with cleft lip/palate and typically developing\nchildren between the ages of 3&#8211;6 years. The analysis shows that,\nfor speech from typically-developing children, most of the variation\nin the automatic pronunciation scores is explained by acoustic deviation,\nwith the errors in forced alignment playing a relatively minor role.\nThe forced alignment errors have a small but significant downstream\nimpact on pronunciation assessment for children with cleft lip/palate.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1403",
      "author_area_id": "13",
      "author_area_label": " Speech, Voice, and Hearing Disorders"
    },
    "villatorotello21_interspeech": {
      "authors": [
        [
          "Esa\u00fa",
          "Villatoro-Tello"
        ],
        [
          "S. Pavankumar",
          "Dubagunta"
        ],
        [
          "Julian",
          "Fritsch"
        ],
        [
          "Gabriela",
          "Ram\u00edrez-de-la-Rosa"
        ],
        [
          "Petr",
          "Motlicek"
        ],
        [
          "Mathew",
          "Magimai-Doss"
        ]
      ],
      "title": "Late Fusion of the Available Lexicon and Raw Waveform-Based Acoustic Modeling for Depression and Dementia Recognition",
      "original": "1288",
      "page_count": 5,
      "order": 395,
      "p1": "1927",
      "pn": "1931",
      "abstract": [
        "Mental disorders, e.g. depression and dementia, are categorized as\npriority conditions according to the World Health Organization (WHO).\nWhen diagnosing, psychologists employ structured questionnaires/interviews,\nand different cognitive tests. Although accurate, there is an increasing\nnecessity of developing digital mental health support technologies\nto alleviate the burden faced by professionals. In this paper, we propose\na multi-modal approach for modeling the communication process employed\nby patients being part of a clinical interview or a cognitive test.\nThe language-based modality, inspired by the Lexical Availability (LA)\ntheory from psycho-linguistics, identifies the most <i>accessible</i>\nvocabulary of the interviewed subject and use it as features in a classification\nprocess. The acoustic-based modality is processed by a Convolutional\nNeural Network (CNN) trained on signals of speech that predominantly\ncontained voice source characteristics. In the end, a late fusion technique,\nbased on majority voting, assigns the final classification. Results\nshow the complementarity of both modalities, reaching an overall Macro-F1\nof 84% and 90% for Depression and Alzheimer&#8217;s dementia respectively.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1288",
      "author_area_id": "13",
      "author_area_label": " Speech, Voice, and Hearing Disorders"
    },
    "shandiz21_interspeech": {
      "authors": [
        [
          "Amin Honarmandi",
          "Shandiz"
        ],
        [
          "L\u00e1szl\u00f3",
          "T\u00f3th"
        ],
        [
          "G\u00e1bor",
          "Gosztolya"
        ],
        [
          "Alexandra",
          "Mark\u00f3"
        ],
        [
          "Tam\u00e1s G\u00e1bor",
          "Csap\u00f3"
        ]
      ],
      "title": "Neural Speaker Embeddings for Ultrasound-Based Silent Speech Interfaces",
      "original": "1466",
      "page_count": 5,
      "order": 396,
      "p1": "1932",
      "pn": "1936",
      "abstract": [
        "Articulatory-to-acoustic mapping seeks to reconstruct speech from a\nrecording of the articulatory movements, for example, an ultrasound\nvideo. Just like speech signals, these recordings represent not only\nthe linguistic content, but are also highly specific to the actual\nspeaker. Hence, due to the lack of multi-speaker data sets, researchers\nhave so far concentrated on speaker-dependent modeling. Here, we present\nmulti-speaker experiments using the recently published TaL80 corpus.\nTo model speaker characteristics, we adjusted the x-vector framework\npopular in speech processing to operate with ultrasound tongue videos.\nNext, we performed speaker recognition experiments using 50 speakers\nfrom the corpus. Then, we created speaker embedding vectors and evaluated\nthem on the remaining speakers. Finally, we examined how the embedding\nvector influences the accuracy of our ultrasound-to-speech conversion\nnetwork in a multi-speaker scenario. In the experiments we attained\nspeaker recognition error rates below 3%, and we also found that the\nembedding vectors generalize nicely to unseen speakers. Our first attempt\nto apply them in a multi-speaker silent speech framework brought about\na marginal reduction in the error rate of the spectral estimation step.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1466",
      "author_area_id": "13",
      "author_area_label": " Speech, Voice, and Hearing Disorders"
    },
    "lamba21_interspeech": {
      "authors": [
        [
          "Jatin",
          "Lamba"
        ],
        [
          "",
          "Abhishek"
        ],
        [
          "Jayaprakash",
          "Akula"
        ],
        [
          "Rishabh",
          "Dabral"
        ],
        [
          "Preethi",
          "Jyothi"
        ],
        [
          "Ganesh",
          "Ramakrishnan"
        ]
      ],
      "title": "Cross-Modal Learning for Audio-Visual Video Parsing",
      "original": "2135",
      "page_count": 5,
      "order": 397,
      "p1": "1937",
      "pn": "1941",
      "abstract": [
        "In this paper, we present a novel approach to the audio-visual video\nparsing (AVVP) task that demarcates events from a video separately\nfor audio and visual modalities. The proposed parsing approach simultaneously\ndetects the temporal boundaries in terms of start and end times of\nsuch events. We show how AVVP can benefit from the following techniques\ngeared towards effective cross-modal learning: (i) adversarial training\nand skip connections (ii) global context aware attention and, (iii)\nself-supervised pretraining using an audio-video grounding objective\nto obtain cross-modal audio-video representations. We present extensive\nexperimental evaluations on the Look, Listen, and Parse (LLP) dataset\nand show that we outperform the state-of-the-art Hybrid Attention Network\n(HAN) on all five metrics proposed for AVVP. We also present several\nablations to validate the effect of pretraining, global attention and\nadversarial training.\n"
      ],
      "doi": "10.21437/Interspeech.2021-2135",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "cook21_interspeech": {
      "authors": [
        [
          "Darren",
          "Cook"
        ],
        [
          "Miri",
          "Zilka"
        ],
        [
          "Simon",
          "Maskell"
        ],
        [
          "Laurence",
          "Alison"
        ]
      ],
      "title": "A Psychology-Driven Computational Analysis of Political Interviews",
      "original": "2249",
      "page_count": 5,
      "order": 398,
      "p1": "1942",
      "pn": "1946",
      "abstract": [
        "Can an interviewer influence the cooperativeness of an interviewee?\nThe role of an interviewer in actualising a successful interview is\nan active field of social psychological research. A large-scale analysis\nof interviews, however, typically involves time-exorbitant manual tasks\nand considerable human effort. Despite recent advances in computational\nfields, many automated methods continue to rely on manually labelled\ntraining data to establish ground-truth. This reliance obscures explainability\nand hinders the mobility of analysis between applications. In this\nwork, we introduce a cross-disciplinary approach to analysing interviewer\nefficacy. We suggest computational success measures as a transparent,\nautomated, and reproducible alternative for pre-labelled data. We validate\nthese measures with a small-scale study with human-responders. To study\nthe interviewer&#8217;s influence on the interviewee we utilise features\ninformed by social psychological theory to predict interview quality\nbased on the interviewer&#8217;s linguistic behaviour. Our psychologically\ninformed model significantly outperforms a bag-of-words model, demonstrating\nthe strength of a cross-disciplinary approach toward the analysis of\nconversational data at scale.\n"
      ],
      "doi": "10.21437/Interspeech.2021-2249",
      "author_area_id": "11",
      "author_area_label": "Spoken Dialog Systems and Conversational Analysis"
    },
    "santoso21_interspeech": {
      "authors": [
        [
          "Jennifer",
          "Santoso"
        ],
        [
          "Takeshi",
          "Yamada"
        ],
        [
          "Shoji",
          "Makino"
        ],
        [
          "Kenkichi",
          "Ishizuka"
        ],
        [
          "Takekatsu",
          "Hiramura"
        ]
      ],
      "title": "Speech Emotion Recognition Based on Attention Weight Correction Using Word-Level Confidence Measure",
      "original": "0411",
      "page_count": 5,
      "order": 399,
      "p1": "1947",
      "pn": "1951",
      "abstract": [
        "Emotion recognition is essential for human behavior analysis and possible\nthrough various inputs such as speech and images. However, in practical\nsituations, such as in call center analysis, the available information\nis limited to speech. This leads to the study of speech emotion recognition\n(SER). Considering the complexity of emotions, SER is a challenging\ntask. Recently, automatic speech recognition (ASR) has played a role\nin obtaining text information from speech. The combination of speech\nand ASR results has improved the SER performance. However, ASR results\nare highly affected by speech recognition errors. Although there is\na method to improve ASR performance on emotional speech, it requires\nthe fine-tuning of ASR, which is costly. To mitigate the errors in\nSER using ASR systems, we propose the use of the combination of a self-attention\nmechanism and a word-level confidence measure (CM), which indicates\nthe reliability of ASR results, to reduce the importance of words with\na high chance of error. Experimental results confirmed that the combination\nof self-attention mechanism and CM reduced the effects of incorrectly\nrecognized words in ASR results, providing a better focus on words\nthat determine emotion recognition. Our proposed method outperformed\nthe state-of-the-art methods on the IEMOCAP dataset.\n"
      ],
      "doi": "10.21437/Interspeech.2021-411",
      "author_area_id": "11",
      "author_area_label": "Spoken Dialog Systems and Conversational Analysis"
    },
    "silpachai21_interspeech": {
      "authors": [
        [
          "Alif",
          "Silpachai"
        ],
        [
          "Ivana",
          "Rehman"
        ],
        [
          "Taylor Anne",
          "Barriuso"
        ],
        [
          "John",
          "Levis"
        ],
        [
          "Evgeny",
          "Chukharev-Hudilainen"
        ],
        [
          "Guanlong",
          "Zhao"
        ],
        [
          "Ricardo",
          "Gutierrez-Osuna"
        ]
      ],
      "title": "Effects of Voice Type and Task on L2 Learners&#8217; Awareness of Pronunciation Errors",
      "original": "0701",
      "page_count": 5,
      "order": 400,
      "p1": "1952",
      "pn": "1956",
      "abstract": [
        "Research suggests learners may improve their second language (L2) pronunciation\nby imitating voices with similar acoustic profiles. However, previously\nreported improvements have been in suprasegmentals (prosodic features\nsuch as intonation). It remains unclear if voice similarity applies\nto L2 segmentals (consonants and vowels). To address this issue, this\nstudy investigates how voice similarity facilitates awareness of pronunciation\nerrors, a necessary step in pronunciation improvement. In two experiments,\nadvanced L2 learners identified their pronunciation errors by comparing\ntheir production to the production of a resynthesized model voice using\nlearners&#8217; voices as the base (Golden Speaker voice), or to an\nunfamiliar resynthesized voice with the same gender as the learner\n(Silver Speaker voice). In Experiment 1, L2 learners identified all\nsyllables with vowel and consonant errors when comparing their production\nto the model voice. Their choices were compared to identifications\nby expert judges. In Experiment 2, learners were told how many errors\nthe expert judges had identified before identifying the same number\nof errors. Results did not support facilitative effects of Golden Speaker\nvoices in either experiment, but Experiment 2 resulted in higher identification\npercentages. Discussion of the challenges in self-identification of\nerrors in relation to voice similarity are offered.\n"
      ],
      "doi": "10.21437/Interspeech.2021-701",
      "author_area_id": "11",
      "author_area_label": "Spoken Dialog Systems and Conversational Analysis"
    },
    "menshikova21_interspeech": {
      "authors": [
        [
          "Alla",
          "Menshikova"
        ],
        [
          "Daniil",
          "Kocharov"
        ],
        [
          "Tatiana",
          "Kachkovskaia"
        ]
      ],
      "title": "Lexical Entrainment and Intra-Speaker Variability in Cooperative Dialogues",
      "original": "1441",
      "page_count": 5,
      "order": 401,
      "p1": "1957",
      "pn": "1961",
      "abstract": [
        "In dialogues, intra-speaker variability is often explained by the relationship\nbetween interlocutors. A person may speak differently with a friend\nand a stranger or depending on the interlocutor&#8217;s gender or age\n&#8212; in all these cases we expect speech entrainment, but the degree\nof entrainment may vary. In this research, we measured lexical entrainment\nin a series of dialogues, where each one of 20 &#8220;core&#8221; speakers\ntalked to five different interlocutors: a sibling, a close friend,\nan unfamiliar person of the same gender and similar age, an unfamiliar\nperson of the other gender and similar age, and an unfamiliar person\nof the same gender, greater age and higher job position. We hypothesized\nthat the degree of speech entrainment systematically varies according\nto the type of interlocutor, across all the &#8220;core&#8221; speakers.\nThe following measures of entrainment were used: parts of speech statistics,\nverb forms statistics, language style matching, and lexical density.\nOur data have shown that a person speaks very similarly to his/her\nsibling; dialogues with a friend or a same-gender stranger of similar\nage show fewer similarities; the least &#8220;common language&#8221;\nis observed in dialogues with a stranger of the opposite gender and\nwith a stranger of greater age and higher job position.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1441",
      "author_area_id": "11",
      "author_area_label": "Spoken Dialog Systems and Conversational Analysis"
    },
    "nasreen21_interspeech": {
      "authors": [
        [
          "Shamila",
          "Nasreen"
        ],
        [
          "Julian",
          "Hough"
        ],
        [
          "Matthew",
          "Purver"
        ]
      ],
      "title": "Detecting Alzheimer&#8217;s Disease Using Interactional and Acoustic Features from Spontaneous Speech",
      "original": "1526",
      "page_count": 5,
      "order": 402,
      "p1": "1962",
      "pn": "1966",
      "abstract": [
        "Alzheimer&#8217;s Disease (AD) is a form of Dementia that manifests\nin cognitive decline including memory, language, and changes in behavior.\nSpeech data has proven valuable for inferring cognitive status, used\nin many health assessment tasks, and can be easily elicited in natural\nsettings. Much work focuses on analysis using linguistic features;\nhere, we focus on non-linguistic features and their use in distinguishing\nAD patients from similar-age Non-AD patients with other health conditions\nin the Carolinas Conversation Collection (CCC) dataset. We used two\ntypes of features: patterns of <i>interaction</i> including pausing\nbehaviour and floor control, and <i>acoustic</i> features including\npitch, amplitude, energy, and cepstral coefficients. Fusion of the\ntwo kinds of features, combined with feature selection, obtains very\npromising classification results: classification accuracy of 90% using\nstandard models such as support vector machines and logistic regression.\nWe also obtain promising results using interactional features alone\n(87% accuracy), which can be easily extracted from natural conversations\nin daily life and thus have the potential for future implementation\nas a non-invasive method for AD diagnosis and monitoring.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1526",
      "author_area_id": "11",
      "author_area_label": "Spoken Dialog Systems and Conversational Analysis"
    },
    "kothare21_interspeech": {
      "authors": [
        [
          "Hardik",
          "Kothare"
        ],
        [
          "Vikram",
          "Ramanarayanan"
        ],
        [
          "Oliver",
          "Roesler"
        ],
        [
          "Michael",
          "Neumann"
        ],
        [
          "Jackson",
          "Liscombe"
        ],
        [
          "William",
          "Burke"
        ],
        [
          "Andrew",
          "Cornish"
        ],
        [
          "Doug",
          "Habberstad"
        ],
        [
          "Alaa",
          "Sakallah"
        ],
        [
          "Sara",
          "Markuson"
        ],
        [
          "Seemran",
          "Kansara"
        ],
        [
          "Afik",
          "Faerman"
        ],
        [
          "Yasmine",
          "Bensidi-Slimane"
        ],
        [
          "Laura",
          "Fry"
        ],
        [
          "Saige",
          "Portera"
        ],
        [
          "David",
          "Suendermann-Oeft"
        ],
        [
          "David",
          "Pautler"
        ],
        [
          "Carly",
          "Demopoulos"
        ]
      ],
      "title": "Investigating the Interplay Between Affective, Phonatory and Motoric Subsystems in Autism Spectrum Disorder Using a Multimodal Dialogue Agent",
      "original": "1796",
      "page_count": 5,
      "order": 403,
      "p1": "1967",
      "pn": "1971",
      "abstract": [
        "We explore the utility of an on-demand multimodal conversational platform\nin extracting speech and facial metrics in children with Autism Spectrum\nDisorder (ASD). We investigate the extent to which these metrics correlate\nwith objective clinical measures, particularly as they pertain to the\ninterplay between the affective, phonatory and motoric subsystems.\n22 participants diagnosed with ASD engaged with a virtual agent in\nconversational affect production tasks designed to elicit facial and\nvocal affect. We found significant correlations between vocal pitch\nand loudness extracted by our platform during these tasks and accuracy\nin recognition of facial and vocal affect, assessed via the Diagnostic\nAnalysis of Nonverbal Accuracy-2 (DANVA-2) neuropsychological task.\nWe also found significant correlations between jaw kinematic metrics\nextracted using our platform and motor speed of the dominant hand assessed\nvia a standardised neuropsychological finger tapping task. These findings\noffer preliminary evidence for the usefulness of these audiovisual\nanalytic metrics and could help us better model the interplay between\ndifferent physiological subsystems in individuals with ASD.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1796"
    },
    "ishi21_interspeech": {
      "authors": [
        [
          "Carlos Toshinori",
          "Ishi"
        ],
        [
          "Taiken",
          "Shintani"
        ]
      ],
      "title": "Analysis of Eye Gaze Reasons and Gaze Aversions During Three-Party Conversations",
      "original": "2134",
      "page_count": 5,
      "order": 404,
      "p1": "1972",
      "pn": "1976",
      "abstract": [
        "The background of this study is the generation of natural gaze behaviors\nin human-robot multimodal interaction. For that purpose, in this study\nwe analyzed gaze behaviors of multiple speakers in a dataset containing\nthree-party conversations, in terms of the reasons/intentions of their\ngaze events.<br style=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  Analyses of the gaze reasons were conducted separately for the\ngaze behaviors towards a dialogue partner, and for gaze aversions (i.e.,\ngazing away from a person&#8217;s face). Analysis on the eyeball movements\nduring gaze aversions was also conducted. Different distributions for\naverage durations and gaze direction patterns were observed depending\non the gaze reasons (e.g., in listening mode, speaking mode, towards\ndialogue partner&#8217;s reactions, in gaze aversions during thinking\nand remembering, and during the speaker&#8217;s own behaviors like\nnodding and laughing).\n"
      ],
      "doi": "10.21437/Interspeech.2021-2134",
      "author_area_id": "11",
      "author_area_label": "Spoken Dialog Systems and Conversational Analysis"
    },
    "kim21e_interspeech": {
      "authors": [
        [
          "Suyoun",
          "Kim"
        ],
        [
          "Abhinav",
          "Arora"
        ],
        [
          "Duc",
          "Le"
        ],
        [
          "Ching-Feng",
          "Yeh"
        ],
        [
          "Christian",
          "Fuegen"
        ],
        [
          "Ozlem",
          "Kalinli"
        ],
        [
          "Michael L.",
          "Seltzer"
        ]
      ],
      "title": "Semantic Distance: A New Metric for ASR Performance Analysis Towards Spoken Language Understanding",
      "original": "1929",
      "page_count": 5,
      "order": 405,
      "p1": "1977",
      "pn": "1981",
      "abstract": [
        "Word Error Rate (WER) has been the predominant metric used to evaluate\nthe performance of automatic speech recognition (ASR) systems. However,\nWER is sometimes not a good indicator for downstream Natural Language\nUnderstanding (NLU) tasks, such as intent recognition, slot filling,\nand semantic parsing in task-oriented dialog systems. This is because\nWER takes into consideration only literal correctness instead of semantic\ncorrectness, the latter of which is typically more important for these\ndownstream tasks. In this study, we propose a novel Semantic Distance\n(SemDist) measure as an alternative evaluation metric for ASR systems\nto address this issue. We define SemDist as the distance between a\nreference and hypothesis pair in a sentence-level embedding space.\nTo represent the reference and hypothesis as a sentence embedding,\nwe exploit RoBERTa, a state-of-the-art pre-trained deep contextualized\nlanguage model based on the transformer architecture. We demonstrate\nthe effectiveness of our proposed metric on various downstream tasks,\nincluding intent recognition, semantic parsing, and named entity recognition.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1929",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "wang21q_interspeech": {
      "authors": [
        [
          "Xiaoqiang",
          "Wang"
        ],
        [
          "Yanqing",
          "Liu"
        ],
        [
          "Sheng",
          "Zhao"
        ],
        [
          "Jinyu",
          "Li"
        ]
      ],
      "title": "A Light-Weight Contextual Spelling Correction Model for Customizing Transducer-Based Speech Recognition Systems",
      "original": "0379",
      "page_count": 5,
      "order": 406,
      "p1": "1982",
      "pn": "1986",
      "abstract": [
        "It&#8217;s challenging to customize transducer-based automatic speech\nrecognition (ASR) system with context information which is dynamic\nand unavailable during model training. In this work, we introduce a\nlight-weight contextual spelling correction model to correct context-related\nrecognition errors in transducer-based ASR systems. We incorporate\nthe context information into the spelling correction model with a shared\ncontext encoder and use a filtering algorithm to handle large-size\ncontext lists. Experiments show that the model improves baseline ASR\nmodel performance with about 50% relative word error rate reduction,\nwhich also significantly outperforms the baseline method such as contextual\nLM biasing. The model also shows excellent performance for out-of-vocabulary\nterms not seen during training.\n"
      ],
      "doi": "10.21437/Interspeech.2021-379",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "shi21_interspeech": {
      "authors": [
        [
          "Ning",
          "Shi"
        ],
        [
          "Wei",
          "Wang"
        ],
        [
          "Boxin",
          "Wang"
        ],
        [
          "Jinfeng",
          "Li"
        ],
        [
          "Xiangyu",
          "Liu"
        ],
        [
          "Zhouhan",
          "Lin"
        ]
      ],
      "title": "Incorporating External POS Tagger for Punctuation Restoration",
      "original": "1708",
      "page_count": 5,
      "order": 407,
      "p1": "1987",
      "pn": "1991",
      "abstract": [
        "Punctuation restoration is an important post-processing step in automatic\nspeech recognition. Among other kinds of external information, part-of-speech\n(POS) taggers provide informative tags, suggesting each input token&#8217;s\nsyntactic role, which has been shown to be beneficial for the punctuation\nrestoration task. In this work, we incorporate an external POS tagger\nand fuse its predicted labels into the existing language model to provide\nsyntactic information. Besides, we propose sequence boundary sampling\n(SBS) to learn punctuation positions more efficiently as a sequence\ntagging task. Experimental results show that our methods can consistently\nobtain performance gains and achieve a new state-of-the-art on the\ncommon IWSLT benchmark. Further ablation studies illustrate that both\nlarge pre-trained language models and the external POS tagger take\nessential parts to improve the model&#8217;s performance.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1708",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "papadourakis21_interspeech": {
      "authors": [
        [
          "Vasileios",
          "Papadourakis"
        ],
        [
          "Markus",
          "M\u00fcller"
        ],
        [
          "Jing",
          "Liu"
        ],
        [
          "Athanasios",
          "Mouchtaris"
        ],
        [
          "Maurizio",
          "Omologo"
        ]
      ],
      "title": "Phonetically Induced Subwords for End-to-End Speech Recognition",
      "original": "1787",
      "page_count": 5,
      "order": 408,
      "p1": "1992",
      "pn": "1996",
      "abstract": [
        "End-to-end automatic speech recognition systems map a sequence of acoustic\nfeatures to text. In modern systems, text is encoded to grapheme subwords\nwhich are generated by methods designed for text processing tasks and\ntherefore don&#8217;t model or take advantage of the statistics of\nthe acoustic features. Here, we present a novel method for generating\ngrapheme subwords that are derived from phoneme sequences, therefore\ncapturing phonetical statistics. The phonetically induced subwords\ncan be used for training and inference in any system that benefits\nfrom subwords, regardless of architecture and without the need of a\npronunciation lexicon. We compare our method to other commonly used\nmethods, which are based on text statistics or on text-phoneme correspondence\nand present experiments on CTC and RNN-T architectures, evaluating\nsubword sets of different sizes. We find that our phonetically induced\nsubwords can improve performance of RNN-T models with relative improvements\nof up to 15.21% compared to other subword methods.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1787",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "mansfield21_interspeech": {
      "authors": [
        [
          "Courtney",
          "Mansfield"
        ],
        [
          "Sara",
          "Ng"
        ],
        [
          "Gina-Anne",
          "Levow"
        ],
        [
          "Richard A.",
          "Wright"
        ],
        [
          "Mari",
          "Ostendorf"
        ]
      ],
      "title": "Revisiting Parity of Human vs. Machine Conversational Speech Transcription",
      "original": "1908",
      "page_count": 5,
      "order": 409,
      "p1": "1997",
      "pn": "2001",
      "abstract": [
        "A number of studies have compared human and machine transcription,\nshowing that automatic speech recognition (ASR) is approaching human\nperformance in some contexts. Most studies look at differences as measured\nby the standard speech recognition scoring criterion: word error rate\n(WER). This study looks at more fine-grained analysis of differences\nfor conversational speech data where systems have reached human parity\nin terms of average WER, specifically insertions vs. deletions, word\ncategory, and word context characterized by linguistic surprisal. In\ncontrast to ASR systems, humans are more likely to miss words than\nto misrecognize them, and they are much more likely to make errors\nin transcribing words associated primarily with conversational contexts\n(fillers, backchannels and discourse cue words). The differences are\nmore pronounced for more informal contexts, i.e. conversations between\nfamily members. Although human transcribers may miss these words, conversational\npartners seem to use them in turntaking and processing disfluencies.\nThus, ASR systems may need superhuman transcription performance for\nspoken language technology to achieve human-level conversation skills.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1908",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "huang21f_interspeech": {
      "authors": [
        [
          "W. Ronny",
          "Huang"
        ],
        [
          "Tara N.",
          "Sainath"
        ],
        [
          "Cal",
          "Peyser"
        ],
        [
          "Shankar",
          "Kumar"
        ],
        [
          "David",
          "Rybach"
        ],
        [
          "Trevor",
          "Strohman"
        ]
      ],
      "title": "Lookup-Table Recurrent Language Models for Long Tail Speech Recognition",
      "original": "0340",
      "page_count": 5,
      "order": 410,
      "p1": "2002",
      "pn": "2006",
      "abstract": [
        "We introduce Lookup-Table Language Models (LookupLM), a method for\nscaling up the size of RNN language models with only a constant increase\nin the floating point operations, by increasing the expressivity of\nthe embedding table. In particular, we instantiate an (additional)\nembedding table which embeds the previous n-gram token sequence, rather\nthan a single token. This allows the embedding table to be scaled up\narbitrarily &#8212; with a commensurate increase in performance &#8212;\nwithout changing the token vocabulary. Since embeddings are sparsely\nretrieved from the table via a lookup; increasing the size of the table\nadds neither extra operations to each forward pass nor extra parameters\nthat need to be stored on limited GPU/TPU memory. We explore scaling\nn-gram embedding tables up to nearly a billion parameters. When trained\non a 3-billion sentence corpus, we find that LookupLM improves long\ntail log perplexity by 2.44 and long tail WER by 23.4% on a downstream\nspeech recognition task over a standard RNN language model baseline,\nan improvement comparable to a scaling up the baseline by 6.2&#215;\nthe number of floating point operations.\n"
      ],
      "doi": "10.21437/Interspeech.2021-340",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "andresferrer21_interspeech": {
      "authors": [
        [
          "Jes\u00fas",
          "Andr\u00e9s-Ferrer"
        ],
        [
          "Dario",
          "Albesano"
        ],
        [
          "Puming",
          "Zhan"
        ],
        [
          "Paul",
          "Vozila"
        ]
      ],
      "title": "Contextual Density Ratio for Language Model Biasing of Sequence to Sequence ASR Systems",
      "original": "0443",
      "page_count": 5,
      "order": 411,
      "p1": "2007",
      "pn": "2011",
      "abstract": [
        "End-2-end (E2E) models have become increasingly popular in some ASR\ntasks because of their performance and advantages. These E2E models\ndirectly approximate the posterior distribution of tokens given the\nacoustic inputs. Consequently, the E2E systems implicitly define a\nlanguage model (LM) over the output tokens, which makes the exploitation\nof independently trained language models less straightforward than\nin conventional ASR systems. This makes it difficult to dynamically\nadapt E2E ASR system to contextual profiles for better recognizing\nspecial words such as named entities. In this work, we propose a contextual\ndensity ratio approach for both training a contextual aware E2E model\nand adapting the language model to named entities. We apply the aforementioned\ntechnique to an E2E ASR system, which transcribes doctor and patient\nconversations, for better adapting the E2E system to the names in the\nconversations. Our proposed technique achieves a relative improvement\nof up to 46.5% on the names over an E2E baseline without degrading\nthe overall recognition accuracy of the whole test set. Moreover, it\nalso surpasses a contextual shallow fusion baseline by 22.1% relative.\n"
      ],
      "doi": "10.21437/Interspeech.2021-443",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "huang21g_interspeech": {
      "authors": [
        [
          "Qiushi",
          "Huang"
        ],
        [
          "Tom",
          "Ko"
        ],
        [
          "H. Lilian",
          "Tang"
        ],
        [
          "Xubo",
          "Liu"
        ],
        [
          "Bo",
          "Wu"
        ]
      ],
      "title": "Token-Level Supervised Contrastive Learning for Punctuation Restoration",
      "original": "0661",
      "page_count": 5,
      "order": 412,
      "p1": "2012",
      "pn": "2016",
      "abstract": [
        "Punctuation is critical in understanding natural language text. Currently,\nmost automatic speech recognition (ASR) systems do not generate punctuation,\nwhich affects the performance of downstream tasks, such as intent detection\nand slot filling. This gives rise to the need for punctuation restoration.\nRecent work in punctuation restoration heavily utilizes pre-trained\nlanguage models without considering data imbalance when predicting\npunctuation classes. In this work, we address this problem by proposing\na token-level supervised contrastive learning method that aims at maximizing\nthe distance of representation of different punctuation marks in the\nembedding space. The result shows that training with token-level supervised\ncontrastive learning obtains up to 3.2% absolute F<SUB>1</SUB> improvement\non the test set.\n"
      ],
      "doi": "10.21437/Interspeech.2021-661",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "zhao21_interspeech": {
      "authors": [
        [
          "Yun",
          "Zhao"
        ],
        [
          "Xuerui",
          "Yang"
        ],
        [
          "Jinchao",
          "Wang"
        ],
        [
          "Yongyu",
          "Gao"
        ],
        [
          "Chao",
          "Yan"
        ],
        [
          "Yuanfu",
          "Zhou"
        ]
      ],
      "title": "BART Based Semantic Correction for Mandarin Automatic Speech Recognition System",
      "original": "0739",
      "page_count": 5,
      "order": 413,
      "p1": "2017",
      "pn": "2021",
      "abstract": [
        "Although automatic speech recognition (ASR) systems achieved significantly\nimprovements in recent years, spoken language recognition error occurs\nwhich can be easily spotted by human beings. Various language modeling\ntechniques have been developed on post recognition tasks like semantic\ncorrection. In this paper, we propose a Transformer based semantic\ncorrection method with pretrained BART initialization, Experiments\non 10000 hours Mandarin speech dataset show that character error rate\n(CER) can be effectively reduced by 21.7% relatively compared to our\nbaseline ASR system. Expert evaluation demonstrates that actual improvement\nof our model surpasses what CER indicates.\n"
      ],
      "doi": "10.21437/Interspeech.2021-739",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "dai21b_interspeech": {
      "authors": [
        [
          "Lingfeng",
          "Dai"
        ],
        [
          "Qi",
          "Liu"
        ],
        [
          "Kai",
          "Yu"
        ]
      ],
      "title": "Class-Based Neural Network Language Model for Second-Pass Rescoring in ASR",
      "original": "1080",
      "page_count": 5,
      "order": 414,
      "p1": "2022",
      "pn": "2026",
      "abstract": [
        "Language model rescoring, especially neural network language model\n(NNLM) rescoring, is widely used to achieve improved performance in\na second-pass automatic speech recognition (ASR) system. The rescoring\nNNLM is usually trained separately from the ASR system. Typically,\nthe two&#8217;s training corpora are different, leading to the vocabulary\nmismatch problem, consequently degrading ASR performance. Previous\nresearch focuses more on the language domain mismatch problem, while\nthe vocabulary mismatch problem, which may also cause significant performance\ndegradation, has not been well studied. This paper proposes a novel\nclass-based NNLM framework to address the vocabulary mismatch problem\nfor language model rescoring. Here, OOV words (unknown words to the\nrescoring NNLM are called OOV words for short) are assigned to well-trained\nclasses of NNLM and inherit the class probability. Experiments show\nthat class-based NNLM rescoring can significantly reduce performance\ndegradation due to vocabulary mismatch.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1080",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "kurata21_interspeech": {
      "authors": [
        [
          "Gakuto",
          "Kurata"
        ],
        [
          "George",
          "Saon"
        ],
        [
          "Brian",
          "Kingsbury"
        ],
        [
          "David",
          "Haws"
        ],
        [
          "Zolt\u00e1n",
          "T\u00fcske"
        ]
      ],
      "title": "Improving Customization of Neural Transducers by Mitigating Acoustic Mismatch of Synthesized Audio",
      "original": "1656",
      "page_count": 5,
      "order": 415,
      "p1": "2027",
      "pn": "2031",
      "abstract": [
        "Customization of automatic speech recognition (ASR) models using text\ndata from a target domain is essential to deploying ASR in various\ndomains. End-to-end (E2E) modeling for ASR has made remarkable progress,\nbut the advantage of E2E modeling, where all neural network parameters\nare jointly optimized, is offset by the challenge of customizing such\nmodels. In conventional hybrid models, it is easy to directly modify\na language model or a lexicon using text data, but this is not true\nfor E2E models. One popular approach for customizing E2E models uses\naudio synthesized from the target domain text, but the acoustic mismatch\nbetween the synthesized and real audio can be problematic. We propose\na method that avoids the negative effect of synthesized audio by (1)\nadding a mapping network before the encoder network to map the acoustic\nfeatures of the synthesized audio to those of the source domain, (2)\ntraining the added mapping network using text and synthesized audio\nfrom the source domain while freezing all layers in the E2E model,\n(3) training the E2E model with text and synthesized audio from the\ntarget domain, and (4) removing the added mapping network when decoding\nreal audio from the target domain. Experiments on customizing RNN Transducer\nand Conformer Transducer models demonstrate the advantage of the proposed\nmethod over encoder freezing, a popular customization method for E2E\nmodels.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1656",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "saebi21_interspeech": {
      "authors": [
        [
          "Mandana",
          "Saebi"
        ],
        [
          "Ernest",
          "Pusateri"
        ],
        [
          "Aaksha",
          "Meghawat"
        ],
        [
          "Christophe Van",
          "Gysel"
        ]
      ],
      "title": "A Discriminative Entity-Aware Language Model for Virtual Assistants",
      "original": "1767",
      "page_count": 5,
      "order": 416,
      "p1": "2032",
      "pn": "2036",
      "abstract": [
        "High-quality automatic speech recognition (ASR) is essential for virtual\nassistants (VAs) to work well. However, ASR often performs poorly on\nVA requests containing named entities. In this work, we start from\nthe observation that many ASR errors on named entities are inconsistent\nwith real-world knowledge. We extend previous discriminative n-gram\nlanguage modeling approaches to incorporate real-world knowledge from\na Knowledge Graph (KG), using features that capture entity type-entity\nand entity-entity relationships. We apply our model through an efficient\nlattice rescoring process, achieving relative sentence error rate reductions\nof more than 25% on some synthesized test sets covering less popular\nentities, with minimal degradation on a uniformly sampled VA test set.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1767",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "namazifar21_interspeech": {
      "authors": [
        [
          "Mahdi",
          "Namazifar"
        ],
        [
          "John",
          "Malik"
        ],
        [
          "Li Erran",
          "Li"
        ],
        [
          "Gokhan",
          "Tur"
        ],
        [
          "Dilek Hakkani",
          "T\u00fcr"
        ]
      ],
      "title": "Correcting Automated and Manual Speech Transcription Errors Using Warped Language Models",
      "original": "0591",
      "page_count": 5,
      "order": 417,
      "p1": "2037",
      "pn": "2041",
      "abstract": [
        "Masked language models have revolutionized natural language processing\nsystems in the past few years. A recently introduced generalization\nof masked language models called warped language models are trained\nto be more robust to the types of errors that appear in automatic or\nmanual transcriptions of spoken language by exposing the language model\nto the same types of errors during the training of language models.\nIn this work we propose a novel approach that takes advantage of the\nrobustness of warped language models to transcription noise for correcting\ntranscriptions of spoken language. We show that our proposed approach\nis able to achieve up to 10% reduction in word error rates of both\nautomatic and manual transcriptions of spoken language.\n"
      ],
      "doi": "10.21437/Interspeech.2021-591",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "shi21b_interspeech": {
      "authors": [
        [
          "Yangyang",
          "Shi"
        ],
        [
          "Varun",
          "Nagaraja"
        ],
        [
          "Chunyang",
          "Wu"
        ],
        [
          "Jay",
          "Mahadeokar"
        ],
        [
          "Duc",
          "Le"
        ],
        [
          "Rohit",
          "Prabhavalkar"
        ],
        [
          "Alex",
          "Xiao"
        ],
        [
          "Ching-Feng",
          "Yeh"
        ],
        [
          "Julian",
          "Chan"
        ],
        [
          "Christian",
          "Fuegen"
        ],
        [
          "Ozlem",
          "Kalinli"
        ],
        [
          "Michael L.",
          "Seltzer"
        ]
      ],
      "title": "Dynamic Encoder Transducer: A Flexible Solution for Trading Off Accuracy for Latency",
      "original": "1272",
      "page_count": 5,
      "order": 418,
      "p1": "2042",
      "pn": "2046",
      "abstract": [
        "We propose a dynamic encoder transducer (DET) for on-device speech\nrecognition. One DET model scales to multiple devices with different\ncomputation capacities without retraining or finetuning. To trading\noff accuracy and latency, DET assigns different encoders to decode\ndifferent parts of an utterance. We apply and compare the layer dropout\nand the collaborative learning for DET training. The layer dropout\nmethod that randomly drops out encoder layers in the training phase,\ncan do on-demand layer dropout in decoding. Collaborative learning\njointly trains multiple encoders with different depths in one single\nmodel. Experiment results on Librispeech and in-house data show that\nDET provides a flexible accuracy and latency trade-off. Results on\nLibrispeech show that the full-size encoder in DET relatively reduces\nthe word error rate of the same size baseline by over 8%. The lightweight\nencoder in DET trained with collaborative learning reduces the model\nsize by 25% but still gets similar WER as the full-size baseline. DET\ngets similar accuracy as a baseline model with better latency on a\nlarge in-house data set by assigning a lightweight encoder for the\nbeginning part of one utterance and a full-size encoder for the rest.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1272",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "zhang21n_interspeech": {
      "authors": [
        [
          "Shiqi",
          "Zhang"
        ],
        [
          "Yan",
          "Liu"
        ],
        [
          "Deyi",
          "Xiong"
        ],
        [
          "Pei",
          "Zhang"
        ],
        [
          "Boxing",
          "Chen"
        ]
      ],
      "title": "Domain-Aware Self-Attention for Multi-Domain Neural Machine Translation",
      "original": "1477",
      "page_count": 5,
      "order": 419,
      "p1": "2047",
      "pn": "2051",
      "abstract": [
        "In this paper, we investigate multi-domain neural machine translation\n(NMT) that translates sentences of different domains in a single model.\nTo this end, we propose a domain-aware self-attention mechanism that\njointly learns domain representations with the single NMT model. The\nlearned domain representations are integrated into both the encoder\nand decoder. We further propose two different domain representation\nlearning approaches: 1) word-level unsupervised learning via a domain\nattention network and 2) guided learning with an auxiliary loss. The\ntwo learning approaches allow our multi-domain NMT to work in different\nsettings as to whether the domain information is available or not.\nExperiments on both Chinese-English and English-French demonstrate\nthat our multi-domain model outperforms a strong baseline built on\nthe Transformer and other previous multi-domain NMT approaches. Further\nanalyses show that our model is able to learn domain clusters even\nwithout prior knowledge about the domain structure.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1477",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "zeyer21_interspeech": {
      "authors": [
        [
          "Albert",
          "Zeyer"
        ],
        [
          "Andr\u00e9",
          "Merboldt"
        ],
        [
          "Wilfried",
          "Michel"
        ],
        [
          "Ralf",
          "Schl\u00fcter"
        ],
        [
          "Hermann",
          "Ney"
        ]
      ],
      "title": "Librispeech Transducer Model with Internal Language Model Prior Correction",
      "original": "1510",
      "page_count": 5,
      "order": 420,
      "p1": "2052",
      "pn": "2056",
      "abstract": [
        "We present our transducer model on Librispeech. We study variants to\ninclude an external language model (LM) with shallow fusion and subtract\nan estimated internal LM. This is justified by a Bayesian interpretation\nwhere the transducer model prior is given by the estimated internal\nLM. The subtraction of the internal LM gives us over 14% relative improvement\nover normal shallow fusion. Our transducer has a separate probability\ndistribution for the non-blank labels which allows for easier combination\nwith the external LM, and easier estimation of the internal LM. We\nadditionally take care of including the end-of-sentence (EOS) probability\nof the external LM in the last blank probability which further improves\nthe performance. All our code and setups are published.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1510",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "mavandadi21_interspeech": {
      "authors": [
        [
          "Sepand",
          "Mavandadi"
        ],
        [
          "Tara N.",
          "Sainath"
        ],
        [
          "Ke",
          "Hu"
        ],
        [
          "Zelin",
          "Wu"
        ]
      ],
      "title": "A Deliberation-Based Joint Acoustic and Text Decoder",
      "original": "0165",
      "page_count": 5,
      "order": 421,
      "p1": "2057",
      "pn": "2061",
      "abstract": [
        "We propose a new two-pass E2E speech recognition model that improves\nASR performance by training on a combination of paired data and unpaired\ntext data. Previously, the joint acoustic and text decoder (JATD) has\nshown promising results through the use of text data during model training\nand the recently introduced deliberation architecture has reduced recognition\nerrors by leveraging first-pass decoding results. Our method, dubbed\nDeliberation-JATD, combines the spelling correcting abilities of deliberation\nwith JATD&#8217;s use of unpaired text data to further improve performance.\nThe proposed model produces substantial gains across multiple test\nsets, especially those focused on rare words, where it reduces word\nerror rate (WER) by between 12% and 22.5% relative. This is done without\nincreasing model size or requiring multi-stage training, making Deliberation-JATD\nan efficient candidate for on-device applications.\n"
      ],
      "doi": "10.21437/Interspeech.2021-165",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "tuske21_interspeech": {
      "authors": [
        [
          "Zolt\u00e1n",
          "T\u00fcske"
        ],
        [
          "George",
          "Saon"
        ],
        [
          "Brian",
          "Kingsbury"
        ]
      ],
      "title": "On the Limit of English Conversational Speech Recognition",
      "original": "0211",
      "page_count": 5,
      "order": 422,
      "p1": "2062",
      "pn": "2066",
      "abstract": [
        "In our previous work we demonstrated that a single headed attention\nencoder-decoder model is able to reach state-of-the-art results in\nconversational speech recognition. In this paper, we further improve\nthe results for both Switchboard 300 and 2000. Through use of an improved\noptimizer, speaker vector embeddings, and alternative speech representations\nwe reduce the recognition errors of our LSTM system on Switchboard-300\nby 4% relative. Compensation of the decoder model with the probability\nratio approach allows more efficient integration of an external language\nmodel, and we report 5.9% and 11.5% WER on the SWB and CHM parts of\nHub5&#8217;00 with very simple LSTM models. Our study also considers\nthe recently proposed conformer, and more advanced self-attention based\nlanguage models. Overall, the conformer shows similar performance to\nthe LSTM; nevertheless, their combination and decoding with an improved\nLM reaches a new record on Switchboard-300, 5.0% and 10.0% WER on SWB\nand CHM. Our findings are also confirmed on Switchboard-2000, and a\nnew state of the art is reported, practically reaching the limit of\nthe benchmark.\n"
      ],
      "doi": "10.21437/Interspeech.2021-211",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "an21_interspeech": {
      "authors": [
        [
          "Keyu",
          "An"
        ],
        [
          "Yi",
          "Zhang"
        ],
        [
          "Zhijian",
          "Ou"
        ]
      ],
      "title": "Deformable TDNN with Adaptive Receptive Fields for Speech Recognition",
      "original": "0387",
      "page_count": 5,
      "order": 423,
      "p1": "2067",
      "pn": "2071",
      "abstract": [
        "Time Delay Neural Networks (TDNNs) are widely used in both DNN-HMM\nbased hybrid speech recognition systems and recent end-to-end systems.\nNevertheless, the receptive fields of TDNNs are limited and fixed,\nwhich is not desirable for tasks like speech recognition, where the\ntemporal dynamics of speech are varied and affected by many factors.\nIn this paper, we propose to use deformable TDNNs for adaptive temporal\ndynamics modeling in end-to-end speech recognition. Inspired by deformable\nConvNets, deformable TDNNs augment the temporal sampling locations\nwith additional offsets and learn the offsets automatically based on\nthe ASR criterion, without additional supervision. Experiments show\nthat deformable TDNNs obtain state-of-the-art results on WSJ benchmarks\n(1.42%/3.45% WER on WSJ eval92/dev93 respectively), outperforming standard\nTDNNs significantly. Furthermore, we propose the latency control mechanism\nfor deformable TDNNs, which enables deformable TDNNs to do streaming\nASR without accuracy degradation.\n"
      ],
      "doi": "10.21437/Interspeech.2021-387",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "you21_interspeech": {
      "authors": [
        [
          "Zhao",
          "You"
        ],
        [
          "Shulin",
          "Feng"
        ],
        [
          "Dan",
          "Su"
        ],
        [
          "Dong",
          "Yu"
        ]
      ],
      "title": "SpeechMoE: Scaling to Large Acoustic Models with Dynamic Routing Mixture of Experts",
      "original": "0478",
      "page_count": 5,
      "order": 424,
      "p1": "2077",
      "pn": "2081",
      "abstract": [
        "Recently, Mixture of Experts (MoE) based Transformer has shown promising\nresults in many domains. This is largely due to the following advantages\nof this architecture: firstly, MoE based Transformer can increase model\ncapacity without computational cost increasing both at training and\ninference time. Besides, MoE based Transformer is a dynamic network\nwhich can adapt to the varying complexity of input instances in real-world\napplications. In this work, we explore the MoE based model for speech\nrecognition, named SpeechMoE. To further control the sparsity of router\nactivation and improve the diversity of gate values, we propose a sparsity\nL1 loss and a mean importance loss respectively. In addition, a new\nrouter architecture is used in SpeechMoE which can simultaneously utilize\nthe information from a shared embedding network and the hierarchical\nrepresentation of different MoE layers. Experimental results show that\nSpeechMoE can achieve lower character error rate (CER) with comparable\ncomputation cost than traditional static networks, providing 7.0%&#126;23.0%\nrelative CER improvements on four evaluation datasets.\n"
      ],
      "doi": "10.21437/Interspeech.2021-478",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "leong21_interspeech": {
      "authors": [
        [
          "Chi-Hang",
          "Leong"
        ],
        [
          "Yu-Han",
          "Huang"
        ],
        [
          "Jen-Tzung",
          "Chien"
        ]
      ],
      "title": "Online Compressive Transformer for End-to-End Speech Recognition",
      "original": "0545",
      "page_count": 5,
      "order": 425,
      "p1": "2082",
      "pn": "2086",
      "abstract": [
        "Traditionally, transformer with connectionist temporal classification\n(CTC) was developed for offline speech recognition where the transcription\nwas generated after the whole utterance has been spoken. However, it\nis crucial to carry out online transcription of speech signal for many\napplications including live broadcasting and meeting. This paper presents\nan online transformer for real-time speech recognition where online\ntranscription is generated chunk by chuck. In particular, an online\ncompressive transformer (OCT) is proposed for end-to-end speech recognition.\nThis OCT aims to generate immediate transcription for each audio chunk\nwhile the comparable performance with offline speech recognition can\nbe still achieved. In the implementation, OCT tightly combines with\nboth CTC and recurrent neural network transducer by minimizing their\nlosses for training. In addition, this OCT systematically merges with\ncompressive memory to reduce potential performance degradation due\nto online processing. This degradation is caused by online transcription\nwhich is generated by the chunks without history information. The experiments\non speech recognition show that OCT does not only obtain comparable\nperformance with offline transformer, but also work faster than the\nbaseline model.\n"
      ],
      "doi": "10.21437/Interspeech.2021-545",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "lin21e_interspeech": {
      "authors": [
        [
          "Binghuai",
          "Lin"
        ],
        [
          "Liyuan",
          "Wang"
        ]
      ],
      "title": "End to End Transformer-Based Contextual Speech Recognition Based on Pointer Network",
      "original": "0774",
      "page_count": 5,
      "order": 426,
      "p1": "2087",
      "pn": "2091",
      "abstract": [
        "Most spoken language assessment systems rely on the text features extracted\nfrom the automatic speech recognition (ASR) transcripts and thus depend\nheavily on the accuracy of the ASR systems. Automatic speech scoring\ntasks such as reading aloud and spontaneous speech are commonly provided\nwith the prompts in advance to guide test takers&#8217; answers, which\ncontain information that should be included in the answers (e.g., listening\npassage, and sample response). Utilizing these texts to improve ASR\nperformance is of great importance for these tasks. In this paper,\nwe develop an end-to-end (E2E) ASR system incorporating contextual\ninformation provided by prompts. Specifically, we add an extra prompt\nencoder to a transformer-based E2E ASR system. To fuse the probabilities\nof the ASR output and the prompts dynamically, we train a soft gate\nbased on the pointer network with carefully constructed prompt training\ncorpus. We experiment the proposed method with data collected from\nEnglish speaking proficiency tests recorded by Chinese teenagers from\n16 to 18 years old. The results show the improved performance of speech\nrecognition with a nearly 50% drop in word error rate (WER) utilizing\nprompts. Furthermore, the proposed network performs well in rare word\nrecognition such as locations and personal names.\n"
      ],
      "doi": "10.21437/Interspeech.2021-774",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "karita21_interspeech": {
      "authors": [
        [
          "Shigeki",
          "Karita"
        ],
        [
          "Yotaro",
          "Kubo"
        ],
        [
          "Michiel Adriaan Unico",
          "Bacchiani"
        ],
        [
          "Llion",
          "Jones"
        ]
      ],
      "title": "A Comparative Study on Neural Architectures and Training Methods for Japanese Speech Recognition",
      "original": "0775",
      "page_count": 5,
      "order": 427,
      "p1": "2092",
      "pn": "2096",
      "abstract": [
        "End-to-end (E2E) modeling is advantageous for automatic speech recognition\n(ASR) especially for Japanese since word-based tokenization of Japanese\nis not trivial, and E2E modeling is able to model character sequences\ndirectly. This paper focuses on the latest E2E modeling techniques,\nand investigates their performances on character-based Japanese ASR\nby conducting comparative experiments. The results are analyzed and\ndiscussed in order to understand the relative advantages of long short-term\nmemory (LSTM), and Conformer models in combination with connectionist\ntemporal classification, transducer, and attention-based loss functions.\nFurthermore, the paper investigates on effectivity of the recent training\ntechniques such as data augmentation (SpecAugment), variational noise\ninjection, and exponential moving average. The best configuration found\nin the paper achieved the state-of-the-art character error rates of\n4.1%, 3.2%, and 3.5% for Corpus of Spontaneous Japanese (CSJ) eval1,\neval2, and eval3 tasks, respectively. The system is also shown to be\ncomputationally efficient thanks to the efficiency of Conformer transducers.\n"
      ],
      "doi": "10.21437/Interspeech.2021-775",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "hori21b_interspeech": {
      "authors": [
        [
          "Takaaki",
          "Hori"
        ],
        [
          "Niko",
          "Moritz"
        ],
        [
          "Chiori",
          "Hori"
        ],
        [
          "Jonathan Le",
          "Roux"
        ]
      ],
      "title": "Advanced Long-Context End-to-End Speech Recognition Using Context-Expanded Transformers",
      "original": "1643",
      "page_count": 5,
      "order": 428,
      "p1": "2097",
      "pn": "2101",
      "abstract": [
        "This paper addresses end-to-end automatic speech recognition (ASR)\nfor long audio recordings such as lecture and conversational speeches.\nMost end-to-end ASR models are designed to recognize independent utterances,\nbut contextual information (e.g., speaker or topic) over multiple utterances\nis known to be useful for ASR. In our prior work, we proposed a context-expanded\nTransformer that accepts multiple consecutive utterances at the same\ntime and predicts an output sequence for the last utterance, achieving\n5&#8211;15% relative error reduction from utterance-based baselines\nin lecture and conversational ASR benchmarks. Although the results\nhave shown remarkable performance gain, there is still potential to\nfurther improve the model architecture and the decoding process. In\nthis paper, we extend our prior work by (1) introducing the Conformer\narchitecture to further improve the accuracy, (2) accelerating the\ndecoding process with a novel activation recycling technique, and (3)\nenabling streaming decoding with triggered attention. We demonstrate\nthat the extended Transformer provides state-of-the-art end-to-end\nASR performance, obtaining a 17.3% character error rate for the HKUST\ndataset and 12.0%/6.3% word error rates for the Switchboard-300 Eval2000\nCallHome/Switchboard test sets. The new decoding method reduces decoding\ntime by more than 50% and further enables streaming ASR with limited\naccuracy degradation. \n"
      ],
      "doi": "10.21437/Interspeech.2021-1643",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "haidar21_interspeech": {
      "authors": [
        [
          "Md. Akmal",
          "Haidar"
        ],
        [
          "Chao",
          "Xing"
        ],
        [
          "Mehdi",
          "Rezagholizadeh"
        ]
      ],
      "title": "Transformer-Based ASR Incorporating Time-Reduction Layer and Fine-Tuning with Self-Knowledge Distillation",
      "original": "1743",
      "page_count": 5,
      "order": 429,
      "p1": "2102",
      "pn": "2106",
      "abstract": [
        "Reducing the input sequence length of speech features to alleviate\nthe complexity of alignment between speech features and text transcript\nby sub-sampling approaches is an important way to get better results\nin end-to-end (E2E) automatic speech recognition (ASR) systems. This\nissue is more important in Transformer-based ASR, because the self-attention\nmechanism in Transformers has O(n<SUP>2</SUP>) order of complexity\nin both training and inference. In this paper, we propose a Transformer-based\nASR model with the time-reduction layer, in which we incorporate time-reduction\nlayer inside transformer encoder layers in addition to traditional\nsub-sampling methods to input features that further reduce the frame-rate.\nThis can help in reducing the computational cost of the self-attention\nprocess for training and inference with performance improvement. Moreover,\nwe introduce a fine-tuning approach for pre-trained ASR models using\nself-knowledge distillation (S-KD) which further improves the performance\nof our ASR model. Experiments on LibriSpeech datasets show that our\nproposed methods outperform all other Transformer-based ASR systems.\nFurthermore, with language model (LM) fusion, we achieve new state-of-the-art\nword error rate (WER) results for Transformer-based ASR models with\njust 30 million parameters trained without any external data.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1743",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "mahadeokar21_interspeech": {
      "authors": [
        [
          "Jay",
          "Mahadeokar"
        ],
        [
          "Yangyang",
          "Shi"
        ],
        [
          "Yuan",
          "Shangguan"
        ],
        [
          "Chunyang",
          "Wu"
        ],
        [
          "Alex",
          "Xiao"
        ],
        [
          "Hang",
          "Su"
        ],
        [
          "Duc",
          "Le"
        ],
        [
          "Ozlem",
          "Kalinli"
        ],
        [
          "Christian",
          "Fuegen"
        ],
        [
          "Michael L.",
          "Seltzer"
        ]
      ],
      "title": "Flexi-Transducer: Optimizing Latency, Accuracy and Compute for Multi-Domain On-Device Scenarios",
      "original": "1921",
      "page_count": 5,
      "order": 430,
      "p1": "2107",
      "pn": "2111",
      "abstract": [
        "Often, the storage and computational constraints of embedded devices\ndemand that a single on-device ASR model serve multiple use-cases /\ndomains. In this paper, we propose a <i>Flexible Transducer</i> (FlexiT)\nfor on-device automatic speech recognition to flexibly deal with multiple\nuse-cases / domains with different accuracy and latency requirements.\nSpecifically, using a single compact model, FlexiT provides a fast\nresponse for <i>voice commands</i>, and accurate transcription but\nwith more latency for <i>dictation</i>. In order to achieve flexible\nand better accuracy and latency trade-offs, the following techniques\nare used. Firstly, we propose using domain-specific altering of segment\nsize for Emformer encoder that enables FlexiT to achieve flexible decoding.\nSecondly, we use Alignment Restricted RNNT loss to achieve flexible\nfine-grained control on token emission latency for different domains.\nFinally, we add a domain indicator vector as an additional input to\nthe FlexiT model. Using the combination of techniques, we show that\na single model can be used to improve WERs and real time factor for\ndictation scenarios while maintaining optimal latency for voice commands\nuse-cases.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1921",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "falkowskigilski21_interspeech": {
      "authors": [
        [
          "Przemyslaw",
          "Falkowski-Gilski"
        ]
      ],
      "title": "Difference in Perceived Speech Signal Quality Assessment Among Monolingual and Bilingual Teenage Students",
      "original": "0016",
      "page_count": 5,
      "order": 431,
      "p1": "2112",
      "pn": "2116",
      "abstract": [
        "The user perceived quality is a mixture of factors, including the background\nof an individual. The process of auditory perception is discussed in\na wide variety of fields, ranging from engineering to medicine. Many\nstudies examine the difference between musicians and non-musicians.\nSince musical training develops musical hearing and other various auditory\ncapabilities, similar enhancements should be observable in case of\nbilingual people. This paper examines the difference in perceived speech\nsignal quality between students from monolingual and bilingual classes.\nThe subjective study was carried out on a group of 30 people, with\n15 individuals in each class, aged 16&#8211;18 years old, considering\nthree languages: English, German, and Polish. Results of this study\nmay aid researchers as well as professionals active in the field of\nauditory perception, hearing loss related with ageing, and of course\nevaluation of networks and services.\n"
      ],
      "doi": "10.21437/Interspeech.2021-16",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "schymura21_interspeech": {
      "authors": [
        [
          "Christopher",
          "Schymura"
        ],
        [
          "Benedikt",
          "B\u00f6nninghoff"
        ],
        [
          "Tsubasa",
          "Ochiai"
        ],
        [
          "Marc",
          "Delcroix"
        ],
        [
          "Keisuke",
          "Kinoshita"
        ],
        [
          "Tomohiro",
          "Nakatani"
        ],
        [
          "Shoko",
          "Araki"
        ],
        [
          "Dorothea",
          "Kolossa"
        ]
      ],
      "title": "PILOT: Introducing Transformers for Probabilistic Sound Event Localization",
      "original": "0124",
      "page_count": 5,
      "order": 432,
      "p1": "2117",
      "pn": "2121",
      "abstract": [
        "Sound event localization aims at estimating the positions of sound\nsources in the environment with respect to an acoustic receiver (e.g.\na microphone array). Recent advances in this domain most prominently\nfocused on utilizing deep recurrent neural networks. Inspired by the\nsuccess of transformer architectures as a suitable alternative to classical\nrecurrent neural networks, this paper introduces a novel transformer-based\nsound event localization framework, where temporal dependencies in\nthe received multi-channel audio signals are captured via self-attention\nmechanisms. Additionally, the estimated sound event positions are represented\nas multivariate Gaussian variables, yielding an additional notion of\nuncertainty, which many previously proposed deep learning-based systems\ndesigned for this application do not provide. The framework is evaluated\non three publicly available multi-source sound event localization datasets\nand compared against state-of-the-art methods in terms of localization\nerror and event detection accuracy. It outperforms all competing systems\non all datasets with statistical significant differences in performance.\n"
      ],
      "doi": "10.21437/Interspeech.2021-124",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "togami21_interspeech": {
      "authors": [
        [
          "Masahito",
          "Togami"
        ],
        [
          "Robin",
          "Scheibler"
        ]
      ],
      "title": "Sound Source Localization with Majorization Minimization",
      "original": "0126",
      "page_count": 5,
      "order": 433,
      "p1": "2122",
      "pn": "2126",
      "abstract": [
        "We propose a sound source localization technique that estimates a speech\nsource location without precise grid searching. The source location\nis estimated in a parameter optimization manner to minimize the steered-response\npower (SRP) function with the near-field assumption. Because there\nis no closed-form solution for the SRP function, we introduce an auxiliary\nfunction of the SRP function based on the majorization-minimization\n(MM) algorithm. Parameters are updated iteratively to minimize the\nauxiliary function with alternate execution of time-difference-of-arrival\n(TDOA) estimation and range-difference (RD) based localization. When\nTDOA estimation and RD-based localization are performed in a cascade\nmanner, the estimation accuracy of the source location is strongly\naffected by the estimation accuracy of the TDOA. On contrary, the proposed\nmethod corrects the estimated TDOA by referring to the estimated source\nlocation in the previous iteration. Thus, it is expected for the proposed\nmethod to be robust against TDOA estimation error which occurs under\nreverberant environments. Experimental results show that the proposed\nmethod outperforms conventional techniques under a reverberant environment.\n"
      ],
      "doi": "10.21437/Interspeech.2021-126",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "mittag21_interspeech": {
      "authors": [
        [
          "Gabriel",
          "Mittag"
        ],
        [
          "Babak",
          "Naderi"
        ],
        [
          "Assmaa",
          "Chehadi"
        ],
        [
          "Sebastian",
          "M\u00f6ller"
        ]
      ],
      "title": "NISQA: A Deep CNN-Self-Attention Model for Multidimensional Speech Quality Prediction with Crowdsourced Datasets",
      "original": "0299",
      "page_count": 5,
      "order": 434,
      "p1": "2127",
      "pn": "2131",
      "abstract": [
        "In this paper, we present an update to the NISQA speech quality prediction\nmodel that is focused on distortions that occur in communication networks.\nIn contrast to the previous version, the model is trained end-to-end\nand the time-dependency modelling and time-pooling is achieved through\na Self-Attention mechanism. Besides overall speech quality, the model\nalso predicts the four speech quality dimensions <i>Noisiness</i>,\n<i>Coloration</i>, <i>Discontinuity</i>, and <i>Loudness</i>, and in\nthis way gives more insight into the cause of a quality degradation.\nFurthermore, new datasets with over 13,000 speech files were created\nfor training and validation of the model. The model was finally tested\non a new, live-talking test dataset that contains recordings of real\ntelephone calls. Overall, NISQA was trained and evaluated on 81 datasets\nfrom different sources and showed to provide reliable predictions also\nfor unknown speech samples. The code, model weights, and datasets are\nopen-sourced.\n"
      ],
      "doi": "10.21437/Interspeech.2021-299",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "naderi21_interspeech": {
      "authors": [
        [
          "Babak",
          "Naderi"
        ],
        [
          "Ross",
          "Cutler"
        ]
      ],
      "title": "Subjective Evaluation of Noise Suppression Algorithms in Crowdsourcing",
      "original": "0343",
      "page_count": 5,
      "order": 435,
      "p1": "2132",
      "pn": "2136",
      "abstract": [
        "The quality of the speech communication systems, which include noise\nsuppression algorithms, are typically evaluated in laboratory experiments\naccording to the ITU-T Rec. P.835, in which participants rate background\nnoise, speech signal, and overall quality separately. This paper introduces\nan open-source toolkit for conducting subjective quality evaluation\nof noise suppressed speech in crowdsourcing. We followed the ITU-T\nRec. P.835, and P.808 and highly automate the process to prevent moderator&#8217;s\nerror. To assess the validity of our evaluation method, we compared\nthe Mean Opinion Scores (MOS), calculated using ratings collected with\nour implementation and the MOS values from a standard laboratory experiment\nconducted according to the ITU-T Rec P.835. Results show a high validity\nin all three scales, namely background noise, speech signal and overall\nquality (average Pearson Correlation Coefficient (PCC) = 0.961). Results\nof a round-robin test (N=5) showed that our implementation is also\na highly reproducible evaluation method (PCC=0.99). Finally, we used\nour implementation in the INTERSPEECH 2021 Deep Noise Suppression Challenge\n[1] as the primary evaluation metric, which demonstrates it is practical\nto use at scale. The results are analyzed to determine why the overall\nperformance was the best in terms of background noise and speech quality.\n"
      ],
      "doi": "10.21437/Interspeech.2021-343",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "geng21_interspeech": {
      "authors": [
        [
          "Jianhua",
          "Geng"
        ],
        [
          "Sifan",
          "Wang"
        ],
        [
          "Juan",
          "Li"
        ],
        [
          "JingWei",
          "Li"
        ],
        [
          "Xin",
          "Lou"
        ]
      ],
      "title": "Reliable Intensity Vector Selection for Multi-Source Direction-of-Arrival Estimation Using a Single Acoustic Vector Sensor",
      "original": "0375",
      "page_count": 5,
      "order": 436,
      "p1": "2137",
      "pn": "2141",
      "abstract": [
        "In the context of multi-source direction of arrival (DOA) estimation\nusing a single acoustic vector sensor (AVS), the received signal is\nusually a mixture of noise, reverberation and source signals. The identification\nof the time-frequency (TF) bins that are dominated by the source signals\ncan significantly improve the robustness of the DOA estimation. In\nthis paper, a TF bin selection based DOA estimation pipeline is proposed.\nThe proposed pipeline mainly involves three key steps: key frame identification,\nTF bin selection and DOA extraction. We identify the key frames by\nframe-wisely examining the effective rank. Subsequently, the geometric\nmedians of the selected key frames are extracted to alleviate the impact\nof extreme outliers. The simulation results show that the accuracy\nand the robustness of the proposed pipeline outperform the state-of-the-art\n(SOTA) techniques.\n"
      ],
      "doi": "10.21437/Interspeech.2021-375",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "yu21_interspeech": {
      "authors": [
        [
          "Meng",
          "Yu"
        ],
        [
          "Chunlei",
          "Zhang"
        ],
        [
          "Yong",
          "Xu"
        ],
        [
          "Shi-Xiong",
          "Zhang"
        ],
        [
          "Dong",
          "Yu"
        ]
      ],
      "title": "MetricNet: Towards Improved Modeling For Non-Intrusive Speech Quality Assessment",
      "original": "0659",
      "page_count": 5,
      "order": 437,
      "p1": "2142",
      "pn": "2146",
      "abstract": [
        "The objective speech quality assessment is usually conducted by comparing\nreceived speech signal with its clean reference, while human beings\nare capable of evaluating the speech quality without any reference,\nsuch as in the mean opinion score (MOS) tests. Non-intrusive speech\nquality assessment has attracted much attention recently due to the\nlack of access to clean reference signals for objective evaluations\nin real scenarios. In this paper, we propose a novel non-intrusive\nspeech quality measurement model, MetricNet, which leverages label\ndistribution learning and joint speech reconstruction learning to achieve\nsignificantly improved performance compared to the existing non-intrusive\nspeech quality measurement models. We demonstrate that the proposed\napproach yields promisingly high correlation to the intrusive objective\nevaluation of speech quality on clean, noisy and processed speech data.\n"
      ],
      "doi": "10.21437/Interspeech.2021-659",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "toma21_interspeech": {
      "authors": [
        [
          "Andrea",
          "Toma"
        ],
        [
          "Daniele",
          "Salvati"
        ],
        [
          "Carlo",
          "Drioli"
        ],
        [
          "Gian Luca",
          "Foresti"
        ]
      ],
      "title": "CNN-Based Processing of Acoustic and Radio Frequency Signals for Speaker Localization from MAVs",
      "original": "0886",
      "page_count": 5,
      "order": 438,
      "p1": "2147",
      "pn": "2151",
      "abstract": [
        "A novel speaker localization algorithm from micro aerial vehicles (MAVs)\nis investigated. It introduces a joint direction of arrival (DOA) and\ndistance prediction method based on processing and fusion of the multi-channel\nspeech data with radio frequency (RF) measurements of the received\nsignal strength. Possible applications include unmanned aerial vehicles\n(UAVs)-based reconnaissance and surveillance against intrusions and\nsearch and rescue in hostile environments. A 3-stages convolutional\nneural network (CNN) with a fusion layer is proposed to perform this\ntask with the objective of augmenting the source localization from\nmulti-channel speech signals. Two parallel CNNs process the speech\nand RF data, and the regression network produces predictions of the\nangle and distance from the source after the fusion layer. To show\nthe performance and effectiveness of this RF-assisted method, the experimental\nscenario and datasets are presented and experiments are then discussed\nalong with the results that have been obtained.\n"
      ],
      "doi": "10.21437/Interspeech.2021-886",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "itoyama21_interspeech": {
      "authors": [
        [
          "Katsutoshi",
          "Itoyama"
        ],
        [
          "Yoshiya",
          "Morimoto"
        ],
        [
          "Shungo",
          "Masaki"
        ],
        [
          "Ryosuke",
          "Kojima"
        ],
        [
          "Kenji",
          "Nishida"
        ],
        [
          "Kazuhiro",
          "Nakadai"
        ]
      ],
      "title": "Assessment of von Mises-Bernoulli Deep Neural Network in Sound Source Localization",
      "original": "1050",
      "page_count": 5,
      "order": 439,
      "p1": "2152",
      "pn": "2156",
      "abstract": [
        "This paper addresses the properties and effectiveness of the von Mises-Bernoulli\ndeep neural network (vM-B DNN), a neural network capable of learning\nperiodic information, in sound source localization. The phase, which\nis periodic information, is an important cue in sound source localization,\nbut typical neural network cannot handle periodic input values properly.\nThe vM-B DNN has been theoretically revealed to be able to handle periodic\ninput values and its effectiveness has been shown in a simple case\nstudy of sound source localization using artificial sinusoids, but\nit was not in the case of speech signals. We conducted both numerical\nsimulation and actual environment experiments. We compared a sound\nsource localization method using vM-B DNN with those using ordinary\nneural networks, and showed that the vM-B DNN outperforms other methods\nunder various conditions.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1050",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "liu21g_interspeech": {
      "authors": [
        [
          "Rongliang",
          "Liu"
        ],
        [
          "Nengheng",
          "Zheng"
        ],
        [
          "Xi",
          "Chen"
        ]
      ],
      "title": "Feature Fusion by Attention Networks for Robust DOA Estimation",
      "original": "1051",
      "page_count": 5,
      "order": 440,
      "p1": "2157",
      "pn": "2161",
      "abstract": [
        "Direction of arrival (DOA) estimation is a key front-end technology\nfor many speech-based intelligent systems. Deep neural networks-based\nDOA systems have recently demonstrated better performances than conventional\nones. However, most of the existing networks use only one specific\nacoustical feature as input, limiting their noise-robustness. This\npaper proposes an attention-based feature fusion approach for DOA estimation.\nTwo classical DOA estimation approaches, i.e., the least mean square-based\nadaptive filtering and the generalized cross-correlation, are adopted,\nand the respective features are served as input to the networks. Network\nwith attention mechanism is built to learn the optimal weighting scheme,\nwhich can take advantage of the two features&#8217; complementary contributions\nin DOA estimation. Simulation and real test results show that the proposed\nmethod could use the complementary DOA information in different features\nand improve estimation accuracy under acoustic conditions with both\nnoise and reverberation.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1051",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "lin21f_interspeech": {
      "authors": [
        [
          "Shoufeng",
          "Lin"
        ],
        [
          "Zhaojie",
          "Luo"
        ]
      ],
      "title": "Far-Field Speaker Localization and Adaptive GLMB Tracking",
      "original": "1160",
      "page_count": 5,
      "order": 441,
      "p1": "2162",
      "pn": "2166",
      "abstract": [
        "In the speech signal processing area, far-field speaker localization\nusing only the audio modality has been a fundamental but challenging\nproblem, especially in presence of reverberation and a varying number\nof moving speakers. Many existing methods use speech onsets as reliable\ndirectional cues against reverberation and interference. However, signal\nprocessing can be computationally costly especially in time domain.\nIn this paper, we present a computationally efficient implementation\nof the recently proposed Onset-Multichannel Cross Correlation Coefficient\n(MCCC) method. Instead of scanning the entire spatial grid, reverse\nmapping and linear interpolation are used. The proposed algorithm with\nbetter efficiency is referred to as the Onset-MCC in this paper. Performance\nof the Onset-MCC is studied over various reverberant and noisy scenarios.\nTo further suppress outliers and address miss-detections, as well as\nfor the adaptive tracking of a varying number of moving speakers, we\npresent an adaptive implementation of the generalized labeled multi-Bernoulli\n(GLMB) filter. As shown in studied cases, the proposed system demonstrates\nreliable and accurate location estimates in far-field (T<SUB>60</SUB>\n= 1s), and is applicable to tracking an unknown and time-varying number\nof moving speakers.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1160",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "narayanaswamy21_interspeech": {
      "authors": [
        [
          "Vivek Sivaraman",
          "Narayanaswamy"
        ],
        [
          "Jayaraman J.",
          "Thiagarajan"
        ],
        [
          "Andreas",
          "Spanias"
        ]
      ],
      "title": "On the Design of Deep Priors for Unsupervised Audio Restoration",
      "original": "1890",
      "page_count": 5,
      "order": 442,
      "p1": "2167",
      "pn": "2171",
      "abstract": [
        "Unsupervised deep learning methods for solving audio restoration problems\nextensively rely on carefully tailored neural architectures that carry\nstrong inductive biases for defining priors in the time or spectral\ndomain. In this context, lot of recent success has been achieved with\nsophisticated convolutional network constructions that recover audio\nsignals in the spectral domain. However, in practice, audio priors\nrequire careful engineering of the convolutional kernels to be effective\nat solving ill-posed restoration tasks, while also being easy to train.\nTo this end, in this paper, we propose a new U-Net based prior that\ndoes not impact either the network complexity or convergence behavior\nof existing convolutional architectures, yet leads to significantly\nimproved restoration. In particular, we advocate the use of carefully\ndesigned dilation schedules and dense connections in the U-Net architecture\nto obtain powerful audio priors. Using empirical studies on standard\nbenchmarks and a variety of ill-posed restoration tasks, such as audio\ndenoising, in-painting and source separation, we demonstrate that our\nproposed approach consistently outperforms widely adopted audio prior\narchitectures.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1890",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "chen21h_interspeech": {
      "authors": [
        [
          "Weiguang",
          "Chen"
        ],
        [
          "Cheng",
          "Xue"
        ],
        [
          "Xionghu",
          "Zhong"
        ]
      ],
      "title": "Cram&#233;r-Rao Lower Bound for DOA Estimation with an Array of Directional Microphones in Reverberant Environments",
      "original": "2267",
      "page_count": 5,
      "order": 443,
      "p1": "2172",
      "pn": "2176",
      "abstract": [
        "Existing direction-of-arrival (DOA) estimation methods usually assume\nthat signals are received by an array of omnidirectional microphones.\nThe performance can be seriously degraded due to heavy reverberation\nand noise. In this paper, DOA estimation using an array with directional\nmicrophones is considered. As the signal response varies over different\nDOAs, the magnitude information as well as the phase information can\nbe employed to estimate the DOA. We first introduce the spherically\nisotropic noise field using directional microphones. The Cram&#233;r-Rao\nLower Bound (CRLB) for DOA estimation is then derived and compared\nwith that using omnidirectional microphones under different signal-to-reverberation\nratio (SRR) environments. In addition, we extend existing steered response\npower (SRP), minimum variance distortionless response (MVDR) and multiple\nsignal classification (MUSIC) estimators for the DOA estimation using\ndirectional microphone arrays. Both CRLB Analysis and DOA estimation\nshow that better DOA estimation performance can be achieved by using\na directional microphone array.\n"
      ],
      "doi": "10.21437/Interspeech.2021-2267"
    },
    "you21b_interspeech": {
      "authors": [
        [
          "Jaeseong",
          "You"
        ],
        [
          "Dalhyun",
          "Kim"
        ],
        [
          "Gyuhyeon",
          "Nam"
        ],
        [
          "Geumbyeol",
          "Hwang"
        ],
        [
          "Gyeongsu",
          "Chae"
        ]
      ],
      "title": "GAN Vocoder: Multi-Resolution Discriminator Is All You Need",
      "original": "0041",
      "page_count": 5,
      "order": 444,
      "p1": "2177",
      "pn": "2181",
      "abstract": [
        "Several of the latest GAN-based vocoders show remarkable achievements,\noutperforming autoregressive and flow-based competitors in both qualitative\nand quantitative measures while synthesizing orders of magnitude faster.\nIn this work, we hypothesize that the common factor underlying their\nsuccess is the multi-resolution discriminating framework, not the minute\ndetails in architecture, loss function, or training strategy. We experimentally\ntest the hypothesis by evaluating six different generators paired with\none shared multi-resolution discriminating framework. For all evaluative\nmeasures with respect to text-to-speech syntheses and for all perceptual\nmetrics, their performances are not distinguishable from one another,\nwhich supports our hypothesis.\n"
      ],
      "doi": "10.21437/Interspeech.2021-41",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "cong21_interspeech": {
      "authors": [
        [
          "Jian",
          "Cong"
        ],
        [
          "Shan",
          "Yang"
        ],
        [
          "Lei",
          "Xie"
        ],
        [
          "Dan",
          "Su"
        ]
      ],
      "title": "Glow-WaveGAN: Learning Speech Representations from GAN-Based Variational Auto-Encoder for High Fidelity Flow-Based Speech Synthesis",
      "original": "0414",
      "page_count": 5,
      "order": 445,
      "p1": "2182",
      "pn": "2186",
      "abstract": [
        "Current two-stage TTS framework typically integrates an acoustic model\nwith a vocoder &#8212; the acoustic model predicts a low resolution\nintermediate representation such as Mel-spectrum while the vocoder\ngenerates waveform from the intermediate representation. Although the\nintermediate representation is served as a bridge, there still exists\ncritical mismatch between the acoustic model and the vocoder as they\nare commonly separately learned and work on different distributions\nof representation, leading to inevitable artifacts in the synthesized\nspeech. In this work, different from using pre-designed intermediate\nrepresentation in most previous studies, we propose to use VAE combining\nwith GAN to learn a latent representation directly from speech and\nthen utilize a flow-based acoustic model to model the distribution\nof the latent representation from text. In this way, the mismatch problem\nis migrated as the two stages work on the same distribution. Results\ndemonstrate that the flow-based acoustic model can exactly model the\ndistribution of our learned speech representation and the proposed\nTTS framework, namely Glow-WaveGAN, can produce high fidelity speech\noutperforming the state-of-the-art GAN-based model.\n"
      ],
      "doi": "10.21437/Interspeech.2021-414",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "yoneyama21_interspeech": {
      "authors": [
        [
          "Reo",
          "Yoneyama"
        ],
        [
          "Yi-Chiao",
          "Wu"
        ],
        [
          "Tomoki",
          "Toda"
        ]
      ],
      "title": "Unified Source-Filter GAN: Unified Source-Filter Network Based On Factorization of Quasi-Periodic Parallel WaveGAN",
      "original": "0517",
      "page_count": 5,
      "order": 446,
      "p1": "2187",
      "pn": "2191",
      "abstract": [
        "We propose a unified approach to data-driven source-filter modeling\nusing a single neural network for developing a neural vocoder capable\nof generating high-quality synthetic speech waveforms while retaining\nflexibility of the source-filter model to control their voice characteristics.\nOur proposed network called unified source-filter generative adversarial\nnetworks (uSFGAN) is developed by factorizing quasi-periodic parallel\nWaveGAN (QPPWG), one of the neural vocoders based on a single neural\nnetwork, into a source excitation generation network and a vocal tract\nresonance filtering network by additionally implementing a regularization\nloss. Moreover, inspired by neural source filter (NSF), only a sinusoidal\nwaveform is additionally used as the simplest clue to generate a periodic\nsource excitation waveform while minimizing the effect of approximations\nin the source filter model. The experimental results demonstrate that\nuSFGAN outperforms conventional neural vocoders, such as QPPWG and\nNSF in both speech quality and pitch controllability.\n"
      ],
      "doi": "10.21437/Interspeech.2021-517",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "mizuta21_interspeech": {
      "authors": [
        [
          "Kazuki",
          "Mizuta"
        ],
        [
          "Tomoki",
          "Koriyama"
        ],
        [
          "Hiroshi",
          "Saruwatari"
        ]
      ],
      "title": "Harmonic WaveGAN: GAN-Based Speech Waveform Generation Model with Harmonic Structure Discriminator",
      "original": "0583",
      "page_count": 5,
      "order": 447,
      "p1": "2192",
      "pn": "2196",
      "abstract": [
        "This paper proposes Harmonic WaveGAN, a GAN-based waveform generation\nmodel that focuses on the harmonic structure of a speech waveform.\nOur proposed model uses two discriminators to capture characteristics\nof a speech waveform in a time domain and in a frequency domain, respectively.\nIn one of them, a harmonic structure discriminator, a 2-D convolution\nlayer called &#8220;harmonic convolution&#8221; is inserted to model\na harmonic structure of a speech waveform. Although harmonic convolution\nhas been shown to perform well in audio restoration tasks, this convolution\nlayer has not yet been fully explored in the field of speech synthesis.\nTherefore, we seek to improve the perceptual quality of speech samples\nsynthesized by the waveform generation model and investigate the usefulness\nof harmonic convolution in the field of speech synthesis. Mean opinion\nscore tests showed that the Harmonic WaveGAN can synthesize more natural\nspeech than conventional Parallel WaveGAN. We also showed that a spectrogram\nof a speech waveform showed a clearer harmonic structure when synthesized\nby our model than a speech waveform synthesized by the original Parallel\nWaveGAN.\n"
      ],
      "doi": "10.21437/Interspeech.2021-583",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "kim21f_interspeech": {
      "authors": [
        [
          "Ji-Hoon",
          "Kim"
        ],
        [
          "Sang-Hoon",
          "Lee"
        ],
        [
          "Ji-Hyun",
          "Lee"
        ],
        [
          "Seong-Whan",
          "Lee"
        ]
      ],
      "title": "Fre-GAN: Adversarial Frequency-Consistent Audio Synthesis",
      "original": "0845",
      "page_count": 5,
      "order": 448,
      "p1": "2197",
      "pn": "2201",
      "abstract": [
        "Although recent works on neural vocoder have improved the quality of\nsynthesized audio, there still exists a gap between generated and ground-truth\naudio in frequency space. This difference leads to spectral artifacts\nsuch as hissing noise or reverberation, and thus degrades the sample\nquality. In this paper, we propose Fre-GAN which achieves frequency-consistent\naudio synthesis with highly improved generation quality. Specifically,\nwe first present resolution-connected generator and resolution-wise\ndiscriminators, which help learn various scales of spectral distributions\nover multiple frequency bands. Additionally, to reproduce high-frequency\ncomponents accurately, we leverage discrete wavelet transform in the\ndiscriminators. From our experiments, Fre-GAN achieves high-fidelity\nwaveform generation with a gap of only 0.03 MOS compared to ground-truth\naudio while outperforming standard models in quality.\n"
      ],
      "doi": "10.21437/Interspeech.2021-845",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "yang21e_interspeech": {
      "authors": [
        [
          "Jinhyeok",
          "Yang"
        ],
        [
          "Jae-Sung",
          "Bae"
        ],
        [
          "Taejun",
          "Bak"
        ],
        [
          "Young-Ik",
          "Kim"
        ],
        [
          "Hoon-Young",
          "Cho"
        ]
      ],
      "title": "GANSpeech: Adversarial Training for High-Fidelity Multi-Speaker Speech Synthesis",
      "original": "0971",
      "page_count": 5,
      "order": 449,
      "p1": "2202",
      "pn": "2206",
      "abstract": [
        "Recent advances in neural multi-speaker text-to-speech (TTS) models\nhave enabled the generation of reasonably good speech quality with\na single model and made it possible to synthesize the speech of a speaker\nwith limited training data. Fine-tuning to the target speaker data\nwith the multi-speaker model can achieve better quality, however, there\nstill exists a gap compared to the real speech sample and the model\ndepends on the speaker. In this work, we propose GANSpeech, which is\na high-fidelity multi-speaker TTS model that adopts the adversarial\ntraining method to a non-autoregressive multi-speaker TTS model. In\naddition, we propose simple but efficient automatic scaling methods\nfor feature matching loss used in adversarial training. In the subjective\nlistening tests, GANSpeech significantly outperformed the baseline\nmulti-speaker FastSpeech and FastSpeech2 models, and showed a better\nMOS score than the speaker-specific fine-tuned FastSpeech2.\n"
      ],
      "doi": "10.21437/Interspeech.2021-971",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "jang21_interspeech": {
      "authors": [
        [
          "Won",
          "Jang"
        ],
        [
          "Dan",
          "Lim"
        ],
        [
          "Jaesam",
          "Yoon"
        ],
        [
          "Bongwan",
          "Kim"
        ],
        [
          "Juntae",
          "Kim"
        ]
      ],
      "title": "UnivNet: A Neural Vocoder with Multi-Resolution Spectrogram Discriminators for High-Fidelity Waveform Generation",
      "original": "1016",
      "page_count": 5,
      "order": 450,
      "p1": "2207",
      "pn": "2211",
      "abstract": [
        "Most neural vocoders employ band-limited mel-spectrograms to generate\nwaveforms. If full-band spectral features are used as the input, the\nvocoder can be provided with as much acoustic information as possible.\nHowever, in some models employing full-band mel-spectrograms, an over-smoothing\nproblem occurs as part of which non-sharp spectrograms are generated.\nTo address this problem, we propose UnivNet, a neural vocoder that\nsynthesizes high-fidelity waveforms in real time. Inspired by works\nin the field of voice activity detection, we added a multi-resolution\nspectrogram discriminator that employs multiple linear spectrogram\nmagnitudes computed using various parameter sets. Using full-band mel-spectrograms\nas input, we expect to generate high-resolution signals by adding a\ndiscriminator that employs spectrograms of multiple resolutions as\nthe input. In an evaluation on a dataset containing information on\nhundreds of speakers, UnivNet obtained the best objective and subjective\nresults among competing models for both seen and unseen speakers. These\nresults, including the best subjective score for text-to-speech, demonstrate\nthe potential for fast adaptation to new speakers without a need for\ntraining from scratch.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1016",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "alradhi21_interspeech": {
      "authors": [
        [
          "Mohammed Salah",
          "Al-Radhi"
        ],
        [
          "Tam\u00e1s G\u00e1bor",
          "Csap\u00f3"
        ],
        [
          "Csaba",
          "Zaink\u00f3"
        ],
        [
          "G\u00e9za",
          "N\u00e9meth"
        ]
      ],
      "title": "Continuous Wavelet Vocoder-Based Decomposition of Parametric Speech Waveform Synthesis",
      "original": "1600",
      "page_count": 5,
      "order": 451,
      "p1": "2212",
      "pn": "2216",
      "abstract": [
        "To date, various speech technology systems have adopted the vocoder\napproach, a method for synthesizing speech waveform that shows a major\nrole in the performance of statistical parametric speech synthesis.\nHowever, conventional source-filter systems (i.e., STRAIGHT) and sinusoidal\nmodels (i.e., MagPhase) tend to produce over-smoothed spectra, which\noften result in muffled and buzzy synthesized text-to-speech (TTS).\nWaveNet, one of the best models that nearly resembles the human voice,\nhas to generate a waveform in a time-consuming sequential manner with\nan extremely complex structure of its neural networks. WaveNet needs\nlarge quantities of voice data before accurate predictions can be obtained.\nIn order to motivate a new, alternative approach to these issues, we\npresent an updated synthesizer, which is a simple signal model to train\nand easy to generate waveforms, using Continuous Wavelet Transform\n(CWT) to characterize and decompose speech features. CWT provides time\nand frequency resolutions different from those of the short-time Fourier\ntransform. It can also retain the fine spectral envelope and achieve\nhigh controllability of the structure closer to human auditory scales.\nWe confirmed through experiments that our speech synthesis system was\nable to provide natural-sounding synthetic speech and outperformed\nthe state-of-the-art WaveNet vocoder.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1600",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "tobing21_interspeech": {
      "authors": [
        [
          "Patrick Lumban",
          "Tobing"
        ],
        [
          "Tomoki",
          "Toda"
        ]
      ],
      "title": "High-Fidelity and Low-Latency Universal Neural Vocoder Based on Multiband WaveRNN with Data-Driven Linear Prediction for Discrete Waveform Modeling",
      "original": "1984",
      "page_count": 5,
      "order": 452,
      "p1": "2217",
      "pn": "2221",
      "abstract": [
        "This paper presents a novel high-fidelity and low-latency universal\nneural vocoder framework based on multiband WaveRNN with data-driven\nlinear prediction for discrete waveform modeling (MWDLP). MWDLP employs\na coarse-fine bit WaveRNN architecture for 10-bit mu-law waveform modeling.\nA sparse gated recurrent unit with a relatively large size of hidden\nunits is utilized, while the multiband modeling is deployed to achieve\nreal-time low-latency usage. A novel technique for data-driven linear\nprediction (LP) with discrete waveform modeling is proposed, where\nthe LP coefficients are estimated in a data-driven manner. Moreover,\na novel loss function using short-time Fourier transform (STFT) for\ndiscrete waveform modeling with Gumbel approximation is also proposed.\nThe experimental results demonstrate that the proposed MWDLP framework\ngenerates high-fidelity synthetic speech for seen and unseen speakers\nand/or language on 300 speakers training data including clean and noisy/reverberant\nconditions, where the number of training utterances is limited to 60\nper speaker, while allowing for real-time low-latency processing using\na single core of &#126;2.1&#8211;2.7 GHz CPU with &#126;0.57&#8211;0.64\nreal-time factor including input/output and feature extraction.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1984",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "liu21h_interspeech": {
      "authors": [
        [
          "Zhengxi",
          "Liu"
        ],
        [
          "Yanmin",
          "Qian"
        ]
      ],
      "title": "Basis-MelGAN: Efficient Neural Vocoder Based on Audio Decomposition",
      "original": "2173",
      "page_count": 5,
      "order": 453,
      "p1": "2222",
      "pn": "2226",
      "abstract": [
        "Recent studies have shown that neural vocoders based on generative\nadversarial network (GAN) can generate audios with high quality. While\nGAN based neural vocoders have shown to be computationally much more\nefficient than those based on autoregressive predictions, the real-time\ngeneration of the highest quality audio on CPU is still a very challenging\ntask. One major computation of all GAN-based neural vocoders comes\nfrom the stacked upsampling layers, which were designed to match the\nlength of the waveform&#8217;s length of output and temporal resolution.\nMeanwhile, the computational complexity of upsampling networks is closely\ncorrelated with the numbers of samples generated for each window. To\nreduce the computation of upsampling layers, we propose a new GAN based\nneural vocoder called Basis-MelGAN where the raw audio samples are\ndecomposed with a learned basis and their associated weights. As the\nprediction targets of Basis-MelGAN are the weight values associated\nwith each learned basis instead of the raw audio samples, the upsampling\nlayers in Basis-MelGAN can be designed with much simpler networks.\nCompared with other GAN based neural vocoders, the proposed Basis-MelGAN\ncould produce comparable high-quality audio but significantly reduced\ncomputational complexity from HiFi-GAN V1&#8217;s 17.74 GFLOPs to 7.95\nGFLOPs.\n"
      ],
      "doi": "10.21437/Interspeech.2021-2173",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "hwang21_interspeech": {
      "authors": [
        [
          "Min-Jae",
          "Hwang"
        ],
        [
          "Ryuichi",
          "Yamamoto"
        ],
        [
          "Eunwoo",
          "Song"
        ],
        [
          "Jae-Min",
          "Kim"
        ]
      ],
      "title": "High-Fidelity Parallel WaveGAN with Multi-Band Harmonic-Plus-Noise Model",
      "original": "0976",
      "page_count": 5,
      "order": 454,
      "p1": "2227",
      "pn": "2231",
      "abstract": [
        "This paper proposes a multi-band harmonic-plus-noise (HN) Parallel\nWaveGAN (PWG) vocoder. To generate a high-fidelity speech signal, it\nis important to well-reflect the harmonic-noise characteristics of\nthe speech waveform in the time-frequency domain. However, it is difficult\nfor the conventional PWG model to accurately match this condition,\nas its single generator inefficiently represents the complicated nature\nof harmonic-noise structures. In the proposed method, the HN WaveNet\nmodels are employed to overcome this limitation, which enable the separate\ngeneration of the harmonic and noise components of speech signals from\nthe pitch-dependent sine wave and Gaussian noise sources, respectively.\nThen, the energy ratios between harmonic and noise components in multiple\nfrequency bands (i.e., subband harmonicities) are predicted by an additional\nharmonicity estimator. Weighted by the estimated harmonicities, the\ngain of harmonic and noise components in each subband is adjusted,\nand finally mixed together to compose the full-band speech signal.\nSubjective evaluation results showed that the proposed method significantly\nimproved the perceptual quality of the synthesized speech.\n"
      ],
      "doi": "10.21437/Interspeech.2021-976",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "chen21i_interspeech": {
      "authors": [
        [
          "Junkun",
          "Chen"
        ],
        [
          "Mingbo",
          "Ma"
        ],
        [
          "Renjie",
          "Zheng"
        ],
        [
          "Liang",
          "Huang"
        ]
      ],
      "title": "SpecRec: An Alternative Solution for Improving End-to-End Speech-to-Text Translation via Spectrogram Reconstruction",
      "original": "0733",
      "page_count": 5,
      "order": 455,
      "p1": "2232",
      "pn": "2236",
      "abstract": [
        "End-to-end Speech-to-text Translation (E2E-ST), which directly translates\nsource language speech to target language text, is widely useful in\npractice, but traditional cascaded approaches (ASR+MT) often suffer\nfrom error propagation in the pipeline. On the other hand, existing\nend-to-end solutions heavily depend on the source language transcriptions\nfor pre-training or multi-task training with Automatic Speech Recognition\n(ASR). We instead propose a simple technique to learn a robust speech\nencoder in a self-supervised fashion only on the speech side, which\ncan utilize speech data without transcription. This technique termed\nSpectrogram Reconstruction (SpecRec), learns better speech representation\nvia recovering the missing speech frames and provides an alternative\nsolution to improving E2E-ST. We conduct our experiments over 8 different\ntranslation directions. In the setting without using any transcriptions,\nour technique achieves an average improvement of +1.1 BLEU. SpecRec\nalso improves the translation accuracy with +0.7 BLEU over the baseline\nin speech translation with ASR multitask training setting.\n"
      ],
      "doi": "10.21437/Interspeech.2021-733",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "cherry21_interspeech": {
      "authors": [
        [
          "Colin",
          "Cherry"
        ],
        [
          "Naveen",
          "Arivazhagan"
        ],
        [
          "Dirk",
          "Padfield"
        ],
        [
          "Maxim",
          "Krikun"
        ]
      ],
      "title": "Subtitle Translation as Markup Translation",
      "original": "0744",
      "page_count": 5,
      "order": 456,
      "p1": "2237",
      "pn": "2241",
      "abstract": [
        "Automatic subtitle translation is an important technology to make video\ncontent available across language barriers. Subtitle translation complicates\nthe normal translation problem by adding the challenge of how to format\nthe system output into subtitles. We propose a simple technique that\ntreats subtitle translation as standard sentence translation plus alignment\ndriven markup transfer, which enables us to reliably maintain timing\nand formatting information from the source subtitles. We also introduce\ntwo metrics to measure the quality of subtitle boundaries: a Timed\nBLEU that penalizes mistimed tokens with respect to a reference subtitle\nsequence, and a measure of how much Timed BLEU is lost due to suboptimal\nsubtitle boundary placement. In experiments on TED and YouTube subtitles,\nwe show that we are able to achieve much better translation quality\nthan a baseline that translates each subtitle independently, while\ncoming very close to optimal subtitle boundary placement.\n"
      ],
      "doi": "10.21437/Interspeech.2021-744",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "wang21r_interspeech": {
      "authors": [
        [
          "Changhan",
          "Wang"
        ],
        [
          "Anne",
          "Wu"
        ],
        [
          "Juan",
          "Pino"
        ],
        [
          "Alexei",
          "Baevski"
        ],
        [
          "Michael",
          "Auli"
        ],
        [
          "Alexis",
          "Conneau"
        ]
      ],
      "title": "Large-Scale Self- and Semi-Supervised Learning for Speech Translation",
      "original": "1912",
      "page_count": 5,
      "order": 457,
      "p1": "2242",
      "pn": "2246",
      "abstract": [
        "In this paper, we improve speech translation (ST) through effectively\nleveraging large quantities of unlabeled speech and text data in different\nand complementary ways. We explore both pretraining and self-training\nby using the large Libri-Light speech audio corpus and language modeling\nwith CommonCrawl. Our experiments improve over the previous state of\nthe art by 2.8 BLEU on average on all four considered CoVoST 2 language\npairs via a simple recipe of combining wav2vec 2.0 pretraining, a single\niteration of self-training and decoding with a language model. Different\nfrom existing work, our approach does not leverage any other supervision\nthan ST data. Code and models are publicly released.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1912",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "wang21s_interspeech": {
      "authors": [
        [
          "Changhan",
          "Wang"
        ],
        [
          "Anne",
          "Wu"
        ],
        [
          "Jiatao",
          "Gu"
        ],
        [
          "Juan",
          "Pino"
        ]
      ],
      "title": "CoVoST 2 and Massively Multilingual Speech Translation",
      "original": "2027",
      "page_count": 5,
      "order": 458,
      "p1": "2247",
      "pn": "2251",
      "abstract": [
        "Speech translation (ST) is an increasingly popular topic of research,\npartly due to the development of benchmark datasets. Nevertheless,\ncurrent datasets cover a limited number of languages. With the aim\nto foster research into massive multilingual ST and ST for low resource\nlanguages, we release CoVoST 2, a large-scale multilingual ST corpus\ncovering translations from 21 languages into English and from English\ninto 15 languages. This represents the largest open dataset available\nto date for volume and language coverage. Data checks provide evidence\nabout the data quality. We provide extensive speech recognition (ASR),\nmachine translation (MT) and ST baselines. We demonstrate the value\nof CoVoST 2 for multilingual ST research by leveraging it in 4 investigations:\nsimplify multilingual training by removing ASR pretraining, study multilingual\nmodel scaling properties and investigate zero-shot and transfer learning\ncapabilities of models trained on CoVoST 2.\n"
      ],
      "doi": "10.21437/Interspeech.2021-2027",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "cheng21_interspeech": {
      "authors": [
        [
          "Yao-Fei",
          "Cheng"
        ],
        [
          "Hung-Shin",
          "Lee"
        ],
        [
          "Hsin-Min",
          "Wang"
        ]
      ],
      "title": "AlloST: Low-Resource Speech Translation Without Source Transcription",
      "original": "0526",
      "page_count": 5,
      "order": 459,
      "p1": "2252",
      "pn": "2256",
      "abstract": [
        "The end-to-end architecture has made promising progress in speech translation\n(ST). However, the ST task is still challenging under low-resource\nconditions. Most ST models have shown unsatisfactory results, especially\nin the absence of word information from the source speech utterance.\nIn this study, we survey methods to improve ST performance without\nusing source transcription, and propose a learning framework that utilizes\na language-independent universal phone recognizer. The framework is\nbased on an attention-based sequence-to-sequence model, where the encoder\ngenerates the phonetic embeddings and phone-aware acoustic representations,\nand the decoder controls the fusion of the two embedding streams to\nproduce the target token sequence. In addition to investigating different\nfusion strategies, we explore the specific usage of byte pair encoding\n(BPE), which compresses a phone sequence into a syllable-like segmented\nsequence. Due to the conversion of symbols, a segmented sequence represents\nnot only pronunciation but also language-dependent information lacking\nin phones. Experiments conducted on the Fisher Spanish-English and\nTaigi-Mandarin drama corpora show that our method outperforms the conformer-based\nbaseline, and the performance is close to that of the existing best\nmethod using source transcription.\n"
      ],
      "doi": "10.21437/Interspeech.2021-526",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "effendi21_interspeech": {
      "authors": [
        [
          "Johanes",
          "Effendi"
        ],
        [
          "Sakriani",
          "Sakti"
        ],
        [
          "Satoshi",
          "Nakamura"
        ]
      ],
      "title": "Weakly-Supervised Speech-to-Text Mapping with Visually Connected Non-Parallel Speech-Text Data Using Cyclic Partially-Aligned Transformer",
      "original": "0970",
      "page_count": 5,
      "order": 460,
      "p1": "2257",
      "pn": "2261",
      "abstract": [
        "Despite the successful development of automatic speech recognition\n(ASR) systems for several of the world&#8217;s major languages, they\nrequire a tremendous amount of parallel speech-text data. Unfortunately,\nfor many other languages, such resources are usually unavailable. This\nstudy addresses the speech-to-text mapping problem given only a collection\nof visually connected non-parallel speech-text data. We call this &#8220;mapping&#8221;\nsince the system attempts to learn the semantic association between\nspeech and text instead of recognizing the speech with the exact word-by-word\ntranscription. Here, we propose utilizing our novel cyclic partially-aligned\nTransformer with two-fold mechanisms. First, we train a Transformer-based\nvector-quantized variational autoencoder (VQ-VAE) to produce a discrete\nspeech representation in a self-supervised manner. Then, we use a Transformer-based\nsequence-to-sequence model inside a chain mechanism to map from unknown\nuntranscribed speech utterances into a semantically equivalent text.\nBecause this is not strictly recognizing speech, we focus on evaluating\nthe semantic equivalence of the generated text hypothesis. Our evaluation\nshows that our proposed method is also effective for a multispeaker\nnatural speech dataset and can also be applied for a cross-lingual\napplication.\n"
      ],
      "doi": "10.21437/Interspeech.2021-970",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "tokuyama21_interspeech": {
      "authors": [
        [
          "Hirotaka",
          "Tokuyama"
        ],
        [
          "Sakriani",
          "Sakti"
        ],
        [
          "Katsuhito",
          "Sudoh"
        ],
        [
          "Satoshi",
          "Nakamura"
        ]
      ],
      "title": "Transcribing Paralinguistic Acoustic Cues to Target Language Text in Transformer-Based Speech-to-Text Translation",
      "original": "1020",
      "page_count": 5,
      "order": 461,
      "p1": "2262",
      "pn": "2266",
      "abstract": [
        "In spoken communication, a speaker may convey their message in words\n(linguistic cues) with supplemental information (paralinguistic cues)\nsuch as emotion and emphasis. Transforming all spoken information into\na written or verbal form is not trivial, especially if the transformation\nhas to be done across languages. Most existing speech-to-text translation\nsystems focus only on translating linguistic information while ignoring\nparalinguistic information. A few recent studies that proposed paralinguistic\ntranslation used a machine translation with hidden Markov model (HMM)-based\nautomatic speech recognition (ASR) and text-to-speech (TTS) that were\ncomplicated and suboptimal. Furthermore, paralinguistic information\nwas kept in the acoustic form. Here, we focused on transcribing paralinguistic\nacoustic cues of emphasis in the target language text. Specifically,\nwe constructed cascade and direct neural Transformer-based speech-to-text\ntranslation, and we investigated various methods of expressing emphasis\ninformation in the written form of the target language. We performed\nour experiments on a Japanese-to-English linguistic and paralinguistic\nspeech-to-text translation framework. The results revealed that our\nproposed method can translate both linguistic and paralinguistic information\nwhile keeping the performance as in standard linguistic translation.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1020",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "ye21_interspeech": {
      "authors": [
        [
          "Rong",
          "Ye"
        ],
        [
          "Mingxuan",
          "Wang"
        ],
        [
          "Lei",
          "Li"
        ]
      ],
      "title": "End-to-End Speech Translation via Cross-Modal Progressive Training",
      "original": "1065",
      "page_count": 5,
      "order": 462,
      "p1": "2267",
      "pn": "2271",
      "abstract": [
        "End-to-end speech translation models have become a new trend in research\ndue to their potential of reducing error propagation. However, these\nmodels still suffer from the challenge of data scarcity. How to effectively\nuse unlabeled or other parallel corpora from machine translation is\npromising but still an open problem. In this paper, we propose Cross\nSpeech-Text Network (XSTNet), an end-to-end model for speech-to-text\ntranslation. XSTNet takes both speech and text as input and outputs\nboth transcription and translation text. The model benefits from its\nthree key design aspects: a self-supervised pre-trained sub-network\nas the audio encoder, a multi-task training objective to exploit additional\nparallel bilingual text, and a progressive training procedure. We evaluate\nthe performance of XSTNet and baselines on the MuST-C En-X and LibriSpeech\nEn-Fr datasets. In particular, XSTNet achieves state-of-the-art results\non all language directions with an average BLEU of 28.8, outperforming\nthe previous best method by 3.2 BLEU. Code, models, cases, and more\ndetailed analysis are publicly available.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1065",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "ko21_interspeech": {
      "authors": [
        [
          "Yuka",
          "Ko"
        ],
        [
          "Katsuhito",
          "Sudoh"
        ],
        [
          "Sakriani",
          "Sakti"
        ],
        [
          "Satoshi",
          "Nakamura"
        ]
      ],
      "title": "ASR Posterior-Based Loss for Multi-Task End-to-End Speech Translation",
      "original": "1105",
      "page_count": 5,
      "order": 463,
      "p1": "2272",
      "pn": "2276",
      "abstract": [
        "End-to-end speech translation (ST) translates source language speech\ndirectly into target language without an intermediate automatic speech\nrecognition (ASR) output, as in a cascading approach. End-to-end ST\nhas the advantage of avoiding error propagation from the intermediate\nASR results, but its performance still lags behind the cascading approach.\nA recent effort to increase performance is multi-task learning using\nan auxiliary task of ASR. However, previous multi-task learning for\nend-to-end ST using cross entropy (CE) loss in ASR-task targets one-hot\nreferences and does not consider ASR confusion. In this study, we propose\na novel end-to-end ST training method using ASR loss against ASR posterior\ndistributions given by a pre-trained model, which we call ASR posterior-based\nloss. The proposed method is expected to consider possible ASR confusion\ndue to competing hypotheses with similar pronunciations. The proposed\nmethod demonstrated better BLEU results in our Fisher Spanish-to-English\ntranslation experiments than the baseline with standard CE loss with\nlabel smoothing.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1105",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "perezgonzalezdemartos21_interspeech": {
      "authors": [
        [
          "Alejandro",
          "P\u00e9rez-Gonz\u00e1lez-de-Martos"
        ],
        [
          "Javier",
          "Iranzo-S\u00e1nchez"
        ],
        [
          "Adri\u00e0 Gim\u00e9nez",
          "Pastor"
        ],
        [
          "Javier",
          "Jorge"
        ],
        [
          "Joan-Albert",
          "Silvestre-Cerd\u00e0"
        ],
        [
          "Jorge",
          "Civera"
        ],
        [
          "Albert",
          "Sanchis"
        ],
        [
          "Alfons",
          "Juan"
        ]
      ],
      "title": "Towards Simultaneous Machine Interpretation",
      "original": "0201",
      "page_count": 5,
      "order": 464,
      "p1": "2277",
      "pn": "2281",
      "abstract": [
        "Automatic speech-to-speech translation (S2S) is one of the most challenging\nspeech and language processing tasks, especially when considering its\napplication to real-time settings. Recent advances on streaming Automatic\nSpeech Recognition (ASR), simultaneous Machine Translation (MT) and\nincremental neural Text-To-Speech (TTS) make it possible to develop\nreal-time cascade S2S systems with greatly improved accuracy. On the\nway to simultaneous machine interpretation, a state-of-the-art cascade\nstreaming S2S system is described and empirically assessed in the simultaneous\ninterpretation of European Parliament debates. We pay particular attention\nto the TTS component, particularly in terms of speech naturalness under\na variety of response-time settings, as well as in terms of speaker\nsimilarity for its cross-lingual voice cloning capabilities.\n"
      ],
      "doi": "10.21437/Interspeech.2021-201",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "martucci21_interspeech": {
      "authors": [
        [
          "Giuseppe",
          "Martucci"
        ],
        [
          "Mauro",
          "Cettolo"
        ],
        [
          "Matteo",
          "Negri"
        ],
        [
          "Marco",
          "Turchi"
        ]
      ],
      "title": "Lexical Modeling of ASR Errors for Robust Speech Translation",
      "original": "0265",
      "page_count": 5,
      "order": 465,
      "p1": "2282",
      "pn": "2286",
      "abstract": [
        "Error propagation from automatic speech recognition (ASR) to machine\ntranslation (MT) is a critical issue for the (still) dominant <i>cascade</i>\napproach to speech translation. To robustify MT to ill-formed inputs,\nwe propose a technique to artificially corrupt clean transcripts so\nas to emulate noisy automatic transcripts. Our <i>Lexical Noise</i>\nmodel relies on estimating from ASR data: i) the probability distribution\nof the possible edit operations applicable to each word, and ii) the\nprobability distribution of possible lexical substitutes for that word.\nCorrupted data generated from these probabilities are paired with their\noriginal clean counterpart for MT adaptation via fine-tuning. Contrastive\nexperiments on three language pairs led to three main findings. First,\non noisy transcripts, the adapted models outperform MT systems fine-tuned\non synthetic data corrupted with previous noising techniques, approaching\nthe upper bound performance obtained by fine-tuning on real ASR data.\nSecond, the increased robustness does not come at the cost of performance\ndrops on clean test data. Third, and crucial from the application standpoint,\nour approach is domain/ASR-independent: noising patterns learned from\na given ASR system in a certain domain can be successfully applied\nto robustify MT to errors made by other ASR systems in a different\ndomain.\n"
      ],
      "doi": "10.21437/Interspeech.2021-265",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "vyas21_interspeech": {
      "authors": [
        [
          "Piyush",
          "Vyas"
        ],
        [
          "Anastasia",
          "Kuznetsova"
        ],
        [
          "Donald S.",
          "Williamson"
        ]
      ],
      "title": "Optimally Encoding Inductive Biases into the Transformer Improves End-to-End Speech Translation",
      "original": "2007",
      "page_count": 5,
      "order": 466,
      "p1": "2287",
      "pn": "2291",
      "abstract": [
        "Transformer-based encoder-decoder architectures have recently shown\npromising results in end-to-end speech translation. However, the content-based\nattention mechanism employed by the Transformer was designed for text\nsequences and can only encode global inductive bias, that alone is\nnot sufficient for learning good representations from speech signals.\nIn this work, we address this by putting architectural constraints\non the Transformer to allow encoding of both local and global inductive\nbiases. This is accomplished by replacing the Transformer encoder with\na Conformer encoder that, in contrast to the Transformer encoder, employs\nconvolution in addition to self-attention and feed-forward. As a result,\nthe new model named Conformer-Transformer has an encoder that captures\nboth local feature correlations and long-range dependencies from speech\nsignals. Experiments on seven non-English to English language directions\nshow that the Conformer-Transformer, compared to strong Transformer-based\nbaselines, achieves up to 3.54 BLEU score improvements with a pre-trained\nencoder and up to 10.53 BLEU score improvements when trained from scratch.\n"
      ],
      "doi": "10.21437/Interspeech.2021-2007",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "ananthanarayana21_interspeech": {
      "authors": [
        [
          "Tejaswini",
          "Ananthanarayana"
        ],
        [
          "Lipisha",
          "Chaudhary"
        ],
        [
          "Ifeoma",
          "Nwogu"
        ]
      ],
      "title": "Effects of Feature Scaling and Fusion on Sign Language Translation",
      "original": "1863",
      "page_count": 5,
      "order": 467,
      "p1": "2292",
      "pn": "2296",
      "abstract": [
        "Sign language translation without transcription has only recently started\nto gain attention. In our work, we focus on improving the state-of-the-art\ntranslation by introducing a multi-feature fusion architecture with\nenhanced input features. As sign language is challenging to segment,\nwe obtain the input features by extracting overlapping scaled segments\nacross the video and obtaining their 3D CNN representations. We exploit\nthe attention mechanism in the fusion architecture by initially learning\ndependencies between different frames of the same video and later fusing\nthem to learn the relations between different features from the same\nvideo. In addition to 3D CNN features, we also analyze pose-based features.<br\nstyle=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  Our robust methodology outperforms the state-of-the-art sign language\ntranslation model by achieving higher BLEU 3 &#8211; BLEU 4 scores\nand also outperforms the state-of-the-art sequence attention models\nby achieving a 43.54% increase in BLEU 4 score. We conclude that the\ncombined effects of feature scaling and feature fusion make our model\nmore robust in predicting longer n-grams which are crucial in continuous\nsign language translation.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1863",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "alenin21_interspeech": {
      "authors": [
        [
          "Alexander",
          "Alenin"
        ],
        [
          "Anton",
          "Okhotnikov"
        ],
        [
          "Rostislav",
          "Makarov"
        ],
        [
          "Nikita",
          "Torgashov"
        ],
        [
          "Ilya",
          "Shigabeev"
        ],
        [
          "Konstantin",
          "Simonchik"
        ]
      ],
      "title": "The ID R&amp;D System Description for Short-Duration Speaker Verification Challenge 2021",
      "original": "1553",
      "page_count": 5,
      "order": 468,
      "p1": "2297",
      "pn": "2301",
      "abstract": [
        "This paper describes ID R&amp;D team submission to the text-independent\ntask of the Short-duration Speaker Verification (SdSV) Challenge 2021.\nThe top performed system is a fusion of 9 Convolutional Neural Networks\nbased on the ResNet architecture. Experiments&#8217; results of optimal\nNN architecture search are shown. We also present and investigate the\nsubnetwork approach to solve the auxiliary tasks such as gender or\nlanguage detection. Verification scores refinement step using quality\nmeasurements of a trial pair allowed to further minimize the target\nmetrics. A comparative analysis of all systems used in the fusion has\nbeen provided on the VoxCeleb-1 test set, SdSV-2021 development and\nevaluation sets. The final submission achieves 0.69% EER and 0.0319\nminDCF on the challenge evaluation set.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1553"
    },
    "thienpondt21_interspeech": {
      "authors": [
        [
          "Jenthe",
          "Thienpondt"
        ],
        [
          "Brecht",
          "Desplanques"
        ],
        [
          "Kris",
          "Demuynck"
        ]
      ],
      "title": "Integrating Frequency Translational Invariance in TDNNs and Frequency Positional Information in 2D ResNets to Enhance Speaker Verification",
      "original": "1570",
      "page_count": 5,
      "order": 469,
      "p1": "2302",
      "pn": "2306",
      "abstract": [
        "This paper describes the IDLab submission for the text-independent\ntask of the Short-duration Speaker Verification Challenge 2021 (SdSVC-21).\nThis speaker verification competition focuses on short duration test\nrecordings and cross-lingual trials, along with the constraint of limited\navailability of in-domain DeepMine Farsi training data. Currently,\nboth Time Delay Neural Networks (TDNNs) and ResNets achieve state-of-the-art\nresults in speaker verification. These architectures are structurally\nvery different and the construction of hybrid networks looks a promising\nway forward. We introduce a 2D convolutional stem in a strong ECAPA-TDNN\nbaseline to transfer some of the strong characteristics of a ResNet\nbased model to this hybrid CNN-TDNN architecture. Similarly, we incorporate\nabsolute frequency positional encodings in an SE-ResNet34 architecture.\nThese learnable feature map biases along the frequency axis offer this\narchitecture a straightforward way to exploit frequency positional\ninformation. We also propose a frequency-wise variant of Squeeze-Excitation\n(SE) which better preserves frequency-specific information when rescaling\nthe feature maps. Both modified architectures significantly outperform\ntheir corresponding baseline on the SdSVC-21 evaluation data and the\noriginal VoxCeleb1 test set. A four system fusion containing the two\nimproved architectures achieved a third place in the final SdSVC-21\nTask 2 ranking.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1570",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification:14 Special Sessions"
    },
    "gusev21_interspeech": {
      "authors": [
        [
          "Aleksei",
          "Gusev"
        ],
        [
          "Alisa",
          "Vinogradova"
        ],
        [
          "Sergey",
          "Novoselov"
        ],
        [
          "Sergei",
          "Astapov"
        ]
      ],
      "title": "SdSVC Challenge 2021: Tips and Tricks to Boost the Short-Duration Speaker Verification System Performance",
      "original": "1737",
      "page_count": 5,
      "order": 470,
      "p1": "2307",
      "pn": "2311",
      "abstract": [
        "This paper presents speaker recognition (SR) systems for the text-independent\nspeaker verification under the cross-lingual (English vs Persian) task\n(task 2) of the Short-duration Speaker Verification Challenge (SdSVC)\n2021.<br style=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  We present the description of applied ResNet-like and ECAPA-TDNN-like\ntopology design solutions as well as an analysis of multi-session scoring\ntechniques benchmarked on the SdSVC challenge datasets. We overview\nvarious modifications of the basic ResNet-like architecture and training\nstrategies, allowing us to obtain the improved quality of speaker verification.\nAlso, we introduce the alpha query expansion-based technique (&#945;QE)\nto the enrollment embeddings aggregation at test time, which results\nin a 0.042 minDCF improvement from 0.12 to 0.078 for the ECAPA-TDNN\nsystem compared to the embeddings mean. We also propose a trial-level\ndistance-based non-parametric imposter/target detector (KrTC) used\nto filter out the worst enrollment samples at test time to further\nimprove the performance of the system.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1737",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification:14 Special Sessions"
    },
    "kang21_interspeech": {
      "authors": [
        [
          "Woo Hyun",
          "Kang"
        ],
        [
          "Nam Soo",
          "Kim"
        ]
      ],
      "title": "Team02 Text-Independent Speaker Verification System for SdSV Challenge 2021",
      "original": "0249",
      "page_count": 5,
      "order": 471,
      "p1": "2312",
      "pn": "2316",
      "abstract": [
        "In this paper, we provide description of our submitted systems to the\nShort Duration Speaker Verification (SdSV) Challenge 2021 Task 2. The\nchallenge provides a difficult set of cross-language text-independent\nspeaker verification trials. Our submissions employ ResNet-based embedding\nnetworks which are trained using various strategies exploiting both\nin-domain and out-of-domain datasets. The results show that using the\nrecently proposed joint factor embedding (JFE) scheme can enhance the\nperformance by disentangling the language-dependent information from\nthe speaker embedding. However, upon analyzing the speaker embeddings,\nit was found that there exists a clear discrepancy between the in-domain\nand out-of-domain datasets. Therefore, among our submitted systems,\nthe best performance was achieved by pre-training the embedding system\nusing out-of-domain dataset and fine-tuning it with only the in-domain\ndata, which resulted in a MinDCF of 0.142716 on the SdSV2021 evaluation\nset.\n"
      ],
      "doi": "10.21437/Interspeech.2021-249",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification:14 Special Sessions"
    },
    "qin21_interspeech": {
      "authors": [
        [
          "Xiaoyi",
          "Qin"
        ],
        [
          "Chao",
          "Wang"
        ],
        [
          "Yong",
          "Ma"
        ],
        [
          "Min",
          "Liu"
        ],
        [
          "Shilei",
          "Zhang"
        ],
        [
          "Ming",
          "Li"
        ]
      ],
      "title": "Our Learned Lessons from Cross-Lingual Speaker Verification: The CRMI-DKU System Description for the Short-Duration Speaker Verification Challenge 2021",
      "original": "0398",
      "page_count": 5,
      "order": 472,
      "p1": "2317",
      "pn": "2321",
      "abstract": [
        "In this paper, we present our CRMI-DKU system description for the Short-duration\nSpeaker Verification Challenge (SdSVC) 2021. We introduce the whole\npipeline of our cross-lingual speaker verification system, including\ndata preprocessing, training strategy, utterance-level speaker embedding\nextractor, domain-adaptation, and score calibration. We also propose\nmethods to learn language-invariant features and perform domain adaptation\nto reduce the cross-lingual mismatch. In addition, we explore a semi-supervised\nmethod to utilize the unlabeled training data. The final submitted\nscore level fusion system achieves 0.0476 minDCF and 0.98% EER on the\nevaluation set.\n"
      ],
      "doi": "10.21437/Interspeech.2021-398",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification:14 Special Sessions"
    },
    "zhang21o_interspeech": {
      "authors": [
        [
          "Peng",
          "Zhang"
        ],
        [
          "Peng",
          "Hu"
        ],
        [
          "Xueliang",
          "Zhang"
        ]
      ],
      "title": "Investigation of IMU&amp;Elevoc Submission for the Short-Duration Speaker Verification Challenge 2021",
      "original": "0743",
      "page_count": 5,
      "order": 473,
      "p1": "2322",
      "pn": "2326",
      "abstract": [
        "In this paper, we present the IMU&amp;Elevoc systems submitted to the\nShort-duration Verification Challenge (SdSVC) 2021. Our submissions\nfocus on both text-dependent speaker verification (Task 1) and text-independent\nspeaker verification (Task 2). First, we investigate several frame-level\nfeature extractor architectures based on ResNet, Res2Net and TDNN.\nThen, we integrate Squeeze-Excitation block and dimension cardinality\nto further improve the Res2Net-based backbone network. In particular,\nwe probe an effective transfer learning strategy that overcomes the\nlack of Task 1 datasets and improves in-domain performance. A knowledge\ndistillation method fusing multiple models is proposed to obtain a\nstronger single model. Experimental results on the SdSVC 2021 show\nthat our primary system yields 0.0500MinDCF in Task 1 (ranked as 4th)\nand 0.0448 MinDCF in Task 2 (ranked as 6th).\n"
      ],
      "doi": "10.21437/Interspeech.2021-743"
    },
    "yan21_interspeech": {
      "authors": [
        [
          "Jie",
          "Yan"
        ],
        [
          "Shengyu",
          "Yao"
        ],
        [
          "Yiqian",
          "Pan"
        ],
        [
          "Wei",
          "Chen"
        ]
      ],
      "title": "The Sogou System for Short-Duration Speaker Verification Challenge 2021",
      "original": "0965",
      "page_count": 5,
      "order": 474,
      "p1": "2327",
      "pn": "2331",
      "abstract": [
        "In this paper we present our system for the task 2 of the Short-duration\nSpeaker Verification (SdSV) Challenge 2021. This task focuses on benchmarking\nand varying degrees of phonetic variability analysis of short-duration\nspeaker recognition system. The main difficulty exists in the variance\nbetween cross-lingual trials, along with the limited in-domain Farsi\ntraining data. Based on the state-of-the-art ResNetSE speaker embedding\nnetwork, we propose a novel network architecture with in-domain data\nfinetuning and novel scoring methods, and achieve significant improvement\nover the ResNetSE baselines. Furthermore, score calibration on duration\nefficiently improve the robustness. Finally, our system with fusion\nof 10 subsystems achieve satisfying results in MinDCF and EER of 0.0394\nand 0.84% respectively on the SdSVC evaluation set.\n"
      ],
      "doi": "10.21437/Interspeech.2021-965",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification:14 Special Sessions"
    },
    "han21c_interspeech": {
      "authors": [
        [
          "Bing",
          "Han"
        ],
        [
          "Zhengyang",
          "Chen"
        ],
        [
          "Zhikai",
          "Zhou"
        ],
        [
          "Yanmin",
          "Qian"
        ]
      ],
      "title": "The SJTU System for Short-Duration Speaker Verification Challenge 2021",
      "original": "2136",
      "page_count": 5,
      "order": 475,
      "p1": "2332",
      "pn": "2336",
      "abstract": [
        "This paper presents the SJTU system for both text-dependent and text-independent\ntasks in short-duration speaker verification (SdSV) challenge 2021.\nIn this challenge, we explored different strong embedding extractors\nto extract robust speaker embedding. For text-independent task, language-dependent\nadaptive snorm is explored to improve the system performance under\nthe cross-lingual verification condition. For text-dependent task,\nwe mainly focus on the in-domain fine-tuning strategies based on the\nmodel pre-trained on large-scale out-of-domain data. In order to improve\nthe distinction between different speakers uttering the same phrase,\nwe proposed several novel phrase-aware fine-tuning strategies and phrase-aware\nneural PLDA. With such strategies, the system performance is further\nimproved. Finally, we fused the scores of different systems, and our\nfusion systems achieved 0.0473 in Task1 (rank 3) and 0.0581 in Task2\n(rank 8) on the primary evaluation metric.\n"
      ],
      "doi": "10.21437/Interspeech.2021-2136",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification:14 Special Sessions"
    },
    "cho21_interspeech": {
      "authors": [
        [
          "Sungjae",
          "Cho"
        ],
        [
          "Soo-Young",
          "Lee"
        ]
      ],
      "title": "Multi-Speaker Emotional Text-to-Speech Synthesizer",
      "original": "8008",
      "page_count": 2,
      "order": 476,
      "p1": "2337",
      "pn": "2338",
      "abstract": [
        "We present a methodology to train our multi-speaker emotional text-to-speech\nsynthesizer that can express speech for 10 speakers&#8217; 7 different\nemotions. All silences from audio samples are removed prior to learning.\nThis results in fast learning by our model. Curriculum learning is\napplied to train our model efficiently. Our model is first trained\nwith a large single-speaker neutral dataset, and then trained with\nneutral speech from all speakers. Finally, our model is trained using\ndatasets of emotional speech from all speakers. In each stage, training\nsamples of each speaker-emotion pair have equal probability to appear\nin mini-batches. Through this procedure, our model can synthesize speech\nfor all targeted speakers and emotions. Our synthesized audio sets\nare available on our web page.\n"
      ]
    },
    "prazak21_interspeech": {
      "authors": [
        [
          "Ale\u0161",
          "Pra\u017e\u00e1k"
        ],
        [
          "Zden\u011bk",
          "Loose"
        ],
        [
          "Josef V.",
          "Psutka"
        ],
        [
          "Vlasta",
          "Radov\u00e1"
        ],
        [
          "Josef",
          "Psutka"
        ],
        [
          "Jan",
          "\u0160vec"
        ]
      ],
      "title": "Live TV Subtitling Through Respeaking",
      "original": "8009",
      "page_count": 2,
      "order": 477,
      "p1": "2339",
      "pn": "2340",
      "abstract": [
        "In this paper, we describe our solution for live TV subtitling. The\nsubtitling system uses the respeaking concept with respeakers closely\ntied with the automatic speech recognition system. The ASR is specially\ntailored to the live subtitling task by using respeaker-specific acoustic\nmodels and TV-show-dependent language models. The output stream of\nASR could be online modified by keyboard shortcuts controlled by the\nrespeaker. The whole subtitling service is used by Czech Television\nto provide high-quality subtitles of live shows for people with hearing\nimpairments.\n"
      ]
    },
    "fragner21_interspeech": {
      "authors": [
        [
          "Stefan",
          "Fragner"
        ],
        [
          "Tobias",
          "Topar"
        ],
        [
          "Maximilian",
          "Giller"
        ],
        [
          "Lukas",
          "Pfeifenberger"
        ],
        [
          "Franz",
          "Pernkopf"
        ]
      ],
      "title": "Autonomous Robot for Measuring Room Impulse Responses",
      "original": "8010",
      "page_count": 2,
      "order": 478,
      "p1": "2341",
      "pn": "2342",
      "abstract": [
        "Far-field speech recognition for e.g. home automation or smart assistants\nhas to cope with moving speakers in reverberant environments. Simulating\nstationary or even moving speakers in realistic environments enables\nto make speech processing technology more robust. This paper introduces\nan autonomous robot for recording a database of Room Impulse Responses\n(RIRs) at a high spatial resolution. This supports the creation of\nrealistic simulation environments. These RIRs can be exploited to generate\nmulti-channel speech mixtures of static or moving speakers for various\napplications.\n"
      ]
    },
    "beskow21_interspeech": {
      "authors": [
        [
          "Jonas",
          "Beskow"
        ],
        [
          "Charlie",
          "Caper"
        ],
        [
          "Johan",
          "Ehrenfors"
        ],
        [
          "Nils",
          "Hagberg"
        ],
        [
          "Anne",
          "Jansen"
        ],
        [
          "Chris",
          "Wood"
        ]
      ],
      "title": "Expressive Robot Performance Based on Facial Motion Capture",
      "original": "8011",
      "page_count": 2,
      "order": 479,
      "p1": "2343",
      "pn": "2344",
      "abstract": [
        "The Furhat robot is a social robot that uses facial projection technology\nto achieve a high degree of expressivity and flexibility. In this demonstration,\nwe will present new features that takes this facial expressiveness\nfurther. A new face engine for the robot is presented which not only\ndrastically improves the visual fidelity of the face and the eyes,\nit also adds increased flexibility when it comes to designing new robotic\ncharacters as well as modifying existing ones. Most importantly, we\nwill present a new toolset and a workflow that allows users to record\ntheir own face motion and incorporate them into skills (i.e. custom\nrobot applications) as gestures, prompts or entire canned performances.\n"
      ]
    },
    "dominguez21_interspeech": {
      "authors": [
        [
          "M\u00f3nica",
          "Dom\u00ednguez"
        ],
        [
          "Juan",
          "Soler-Company"
        ],
        [
          "Leo",
          "Wanner"
        ]
      ],
      "title": "ThemePro 2.0: Showcasing the Role of Thematic Progression in Engaging Human-Computer Interaction",
      "original": "8012",
      "page_count": 2,
      "order": 480,
      "p1": "2345",
      "pn": "2346",
      "abstract": [
        "Structuring speech into informative units is certainly a desirable\nfeature in efficient human-machine communication. This paper introduces\nThemePro 2.0, a toolkit that pre-processes long monologues into smaller\ncohesive units to be consumed by the text-to-speech module within a\nconversational agent. The methodology used is based upon the text&#8217;s\ndiscourse structure modelled as thematic progression patterns. As shown\nin the demonstration, thematic progression modelling captures the underlying\ninformation structure at the discourse level and is, therefore, instrumental\nfor cohesive speech output in the TTS component.\n"
      ]
    },
    "guruju21_interspeech": {
      "authors": [
        [
          "Sai",
          "Guruju"
        ],
        [
          "Jithendra",
          "Vepa"
        ]
      ],
      "title": "Addressing Compliance in Call Centers with Entity Extraction",
      "original": "8013",
      "page_count": 2,
      "order": 481,
      "p1": "2347",
      "pn": "2348",
      "abstract": [
        "Call centers record and store customer-agent conversations for the\npurpose of coaching, quality assurance and to comply with <i>Industry\nRegulations</i>. Good amount of these audio recordings contain sensitive\ninformation pertaining to their customers&#8217; financial or personal\ndetails. To ensure data security, compliance and to reduce the risk\nof abuse/theft, it becomes important to identify such instances in\naudio recordings and mask these segments. To automate this process,\nwe propose a cascaded system; first, Automatic Speech Recognition (ASR)\ngenerates transcript and text-to-audio alignment information for an\naudio recording. Then, Entity Extraction is performed on generated\ntranscripts to identify and locate sensitive information, and the corresponding\nsensitive segments are masked in audio recordings using alignment information.\nWe introduce a novel system for selective masking of sensitive information\nin both audio and transcript.\n"
      ]
    },
    "gogineni21_interspeech": {
      "authors": [
        [
          "Krishnachaitanya",
          "Gogineni"
        ],
        [
          "Tarun Reddy",
          "Yadama"
        ],
        [
          "Jithendra",
          "Vepa"
        ]
      ],
      "title": "Audio Segmentation Based Conversational Silence Detection for Contact Center Calls",
      "original": "8014",
      "page_count": 2,
      "order": 482,
      "p1": "2349",
      "pn": "2350",
      "abstract": [
        "In a typical contact-center call, more than 35% of the call has neither\nthe contact-center agent nor the customer speaking, we usually refer\nto such areas in the call as <i>Conversational Silences. Conversational\nsilences</i> comprise mostly of hold-music, automatic-recorded-messages,\nor just silences when the agent or customer is engaged in some off-call\nwork. Most of these conversational silences negatively affect important\nKPIs for call-centers, like dead-airs affect customer satisfaction,\nlong-holds affect average call handling time and so on. In this paper\nwe showcase how Observe.AI helps contact-centers identify agents who\nare breaching accepted levels of conversational silences by using an\nin-house Audio Segmenter system paired with an NLP system to classify\nthe contexts around these <i>Conversational Silences</i>. This solution\nis provided by Observe.AI to hundreds of contact centers who use it\nto improve their average call handling time and customer satisfaction\nscores.\n"
      ]
    },
    "raj21b_interspeech": {
      "authors": [
        [
          "Desh",
          "Raj"
        ],
        [
          "Sanjeev",
          "Khudanpur"
        ]
      ],
      "title": "Reformulating DOVER-Lap Label Mapping as a Graph Partitioning Problem",
      "original": "0323",
      "page_count": 5,
      "order": 483,
      "p1": "2351",
      "pn": "2355",
      "abstract": [
        "We recently proposed DOVER-Lap, a method for combining overlap-aware\nspeaker diarization system outputs. DOVER-Lap improved upon its predecessor\nDOVER by using a label mapping method based on globally-informed greedy\nsearch. In this paper, we analyze this label mapping in the framework\nof a maximum orthogonal graph partitioning problem, and present three\ninferences. First, we show that DOVER-Lap label mapping is exponential\nin the input size, which poses a challenge when combining a large number\nof hypotheses. We then revisit the DOVER label mapping algorithm and\npropose a modification which performs similar to DOVER-Lap while being\ncomputationally tractable. We also derive an approximation bound for\nthe algorithm in terms of the maximum number of hypotheses speakers.\nFinally, we describe a randomized local search algorithm which provides\na near-optimal (1-&#949;)-approximate solution to the problem with\nhigh probability. We empirically demonstrate the effectiveness of our\nmethods on the AMI meeting corpus. Our code is publicly available.\n"
      ],
      "doi": "10.21437/Interspeech.2021-323",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "tak21_interspeech": {
      "authors": [
        [
          "Hemlata",
          "Tak"
        ],
        [
          "Jee-weon",
          "Jung"
        ],
        [
          "Jose",
          "Patino"
        ],
        [
          "Massimiliano",
          "Todisco"
        ],
        [
          "Nicholas",
          "Evans"
        ]
      ],
      "title": "Graph Attention Networks for Anti-Spoofing",
      "original": "0993",
      "page_count": 5,
      "order": 484,
      "p1": "2356",
      "pn": "2360",
      "abstract": [
        "The cues needed to detect spoofing attacks against automatic speaker\nverification are often located in specific spectral sub-bands or temporal\nsegments. Previous works show the potential to learn these using either\nspectral or temporal self-attention mechanisms but not the relationships\nbetween neighbouring sub-bands or segments. This paper reports our\nuse of graph attention networks (GATs) to model these relationships\nand to improve spoofing detection performance. GATs leverage a self-attention\nmechanism over graph structured data to model the data manifold and\nthe relationships between nodes. Our graph is constructed from representations\nproduced by a ResNet. Nodes in the graph represent information either\nin specific sub-bands or temporal segments. Experiments performed on\nthe ASVspoof 2019 logical access database show that our GAT-based model\nwith temporal attention outperforms all of our baseline single systems.\nFurthermore, GAT-based systems are complementary to a set of existing\nsystems. The fusion of GAT-based models with more conventional countermeasures\ndelivers a 47% relative improvement in performance compared to the\nbest performing single GAT system.\n"
      ],
      "doi": "10.21437/Interspeech.2021-993",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "mingote21_interspeech": {
      "authors": [
        [
          "Victoria",
          "Mingote"
        ],
        [
          "Antonio",
          "Miguel"
        ],
        [
          "Alfonso",
          "Ortega"
        ],
        [
          "Eduardo",
          "Lleida"
        ]
      ],
      "title": "Log-Likelihood-Ratio Cost Function as Objective Loss for Speaker Verification Systems",
      "original": "1085",
      "page_count": 5,
      "order": 485,
      "p1": "2361",
      "pn": "2365",
      "abstract": [
        "Many recent studies in Speaker Verification (SV) have been focused\non the design of the most appropriate training loss function, which\nplays an important role to improve the recognition ability of the systems.\nHowever, the verification loss functions created often do not take\ninto account the performance measures which are used for the final\nsystem evaluation. For this reason, this paper presents an alternative\napproach to optimize the parameters of a neural network using a loss\nfunction based on the log-likelihood-ratio cost function (CLLR). This\nfunction is an application-independent metric that measures the cost\nof soft detection decisions over all the operating points. Thus, prior\nor relevance cost parameters assumptions are not employed to obtain\nit. Moreover, this metric has a differentiable expression, so no approximation\nis needed to use it as the objective loss to train a neural network.\nCLLR function as optimization loss was tested on the RSR2015-Part II\ndatabase for text-dependent speaker verification, providing competitive\nresults without using score normalization and outperforming other similar\nloss functions as Cross-Entropy combined with Ring Loss, as well as\nour previous loss function based on an approximation of the Detection\nCost Function (DCF).\n"
      ],
      "doi": "10.21437/Interspeech.2021-1085",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "peng21c_interspeech": {
      "authors": [
        [
          "Junyi",
          "Peng"
        ],
        [
          "Xiaoyang",
          "Qu"
        ],
        [
          "Rongzhi",
          "Gu"
        ],
        [
          "Jianzong",
          "Wang"
        ],
        [
          "Jing",
          "Xiao"
        ],
        [
          "Luk\u00e1\u0161",
          "Burget"
        ],
        [
          "Jan",
          "\u010cernock\u00fd"
        ]
      ],
      "title": "Effective Phase Encoding for End-To-End Speaker Verification",
      "original": "2025",
      "page_count": 5,
      "order": 486,
      "p1": "2366",
      "pn": "2370",
      "abstract": [
        "The widely used magnitude spectrum based features have shown their\nsuperiority in the field of speech processing. In contrast, the importance\nof phase spectrum is always ignored. This is because the patterns hidden\nin phase cannot be intuitively modelled and interpreted, due to phase\nwrapping phenomenon. In this paper, we explore novel phase spectrum\nbased features, named Learnable Group Delay (LearnGD), to capture useful\ninformation in speech signals. Specifically, firstly, the negative\nof the spectral derivative of the phase spectrum, called group delay\n(GD), is used to unwrap the phase. Then, to suppress the spiky nature\nof GD, which is caused by its roots close to the unit circle in the\nZ domain, a carefully designed light convolutional smoothing layer\nis employed to reconstruct the GD. Finally, an exponential hyper-parameter\nis introduced to reconstruct GD features to restore the spectrum range\nand generate LearnGD features. For performance evaluation, speaker\nverification experiments are conducted on the VoxCeleb2 corpus. Compared\nto the traditional acoustic feature derived from the magnitude spectrum,\nthe proposed phase-based features reach a 27.8% relative improvement\nin terms of EER. Furthermore, experimental results on TIMIT phoneme\nrecognition task also demonstrate the effectiveness of our proposed\nphase-based features.\n"
      ],
      "doi": "10.21437/Interspeech.2021-2025",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "nguyen21d_interspeech": {
      "authors": [
        [
          "Ha",
          "Nguyen"
        ],
        [
          "Yannick",
          "Est\u00e8ve"
        ],
        [
          "Laurent",
          "Besacier"
        ]
      ],
      "title": "Impact of Encoding and Segmentation Strategies on End-to-End Simultaneous Speech Translation",
      "original": "0608",
      "page_count": 5,
      "order": 487,
      "p1": "2371",
      "pn": "2375",
      "abstract": [
        "Boosted by the simultaneous translation shared task at IWSLT 2020,\npromising end-to-end online speech translation approaches were recently\nproposed. They consist in incrementally encoding a speech input (in\na source language) and decoding the corresponding text (in a target\nlanguage) with the best possible trade-off between latency and translation\nquality. This paper investigates two key aspects of end-to-end simultaneous\nspeech translation: (a) how to encode efficiently the continuous speech\nflow, and (b) how to segment the speech flow in order to alternate\noptimally between reading (R: encoding input) and writing (W: decoding\noutput) operations. We extend our previously proposed end-to-end online\ndecoding strategy and show that while replacing BLSTM by ULSTM encoding\ndegrades performance in offline mode, it actually improves both efficiency\nand performance in online mode. We also measure the impact of different\nmethods to segment the speech signal (using fixed interval boundaries,\noracle word boundaries or randomly set boundaries) and show that our\nbest end-to-end online decoding strategy is surprisingly the one that\nalternates R/W operations on fixed size blocks on our English-German\nspeech translation setup.\n"
      ],
      "doi": "10.21437/Interspeech.2021-608",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "machacek21_interspeech": {
      "authors": [
        [
          "Dominik",
          "Mach\u00e1\u010dek"
        ],
        [
          "Mat\u00fa\u0161",
          "\u017dilinec"
        ],
        [
          "Ond\u0159ej",
          "Bojar"
        ]
      ],
      "title": "Lost in Interpreting: Speech Translation from Source or Interpreter?",
      "original": "2232",
      "page_count": 5,
      "order": 488,
      "p1": "2376",
      "pn": "2380",
      "abstract": [
        "Interpreters facilitate multi-lingual meetings but the affordable set\nof languages is often smaller than what is needed. Automatic simultaneous\nspeech translation can extend the set of provided languages. We investigate\nif such an automatic system should rather follow the original speaker,\nor an interpreter to achieve better translation quality at the cost\nof increased delay.<br style=\"mso-data-placement:same-cell;\" /><br\nstyle=\"mso-data-placement:same-cell;\" />\n  To answer the question,\nwe release Europarl Simultaneous Interpreting Corpus (ESIC), 10 hours\nof recordings and transcripts of European Parliament speeches in English,\nwith simultaneous interpreting into Czech and German. We evaluate quality\nand latency of speaker-based and interpreter-based spoken translation\nsystems from English to Czech. We study the differences in implicit\nsimplification and summarization of the human interpreter compared\nto a machine translation system trained to shorten the output to some\nextent. Finally, we perform human evaluation to measure information\nloss of each of these approaches.\n"
      ],
      "doi": "10.21437/Interspeech.2021-2232",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "pouthier21_interspeech": {
      "authors": [
        [
          "Baptiste",
          "Pouthier"
        ],
        [
          "Laurent",
          "Pilati"
        ],
        [
          "Leela K.",
          "Gudupudi"
        ],
        [
          "Charles",
          "Bouveyron"
        ],
        [
          "Frederic",
          "Precioso"
        ]
      ],
      "title": "Active Speaker Detection as a Multi-Objective Optimization with Uncertainty-Based Multimodal Fusion",
      "original": "0080",
      "page_count": 5,
      "order": 489,
      "p1": "2381",
      "pn": "2385",
      "abstract": [
        "It is now well established from a variety of studies that there is\na significant benefit from combining video and audio data in detecting\nactive speakers. However, either of the modalities can potentially\nmislead audiovisual fusion by inducing unreliable or deceptive information.\nThis paper outlines active speaker detection as a multi-objective learning\nproblem to leverage best of each modalities using a novel self-attention,\nuncertainty-based multimodal fusion scheme. Results obtained show that\nthe proposed multi-objective learning architecture outperforms traditional\napproaches in improving both mAP and AUC scores. We further demonstrate\nthat our fusion strategy surpasses, in active speaker detection, other\nmodality fusion methods reported in various disciplines. We finally\nshow that the proposed method significantly improves the state-of-the-art\non the AVA-ActiveSpeaker dataset.\n"
      ],
      "doi": "10.21437/Interspeech.2021-80",
      "author_area_id": "11",
      "author_area_label": "Spoken Dialog Systems and Conversational Analysis"
    },
    "wallbridge21_interspeech": {
      "authors": [
        [
          "Sarenne",
          "Wallbridge"
        ],
        [
          "Peter",
          "Bell"
        ],
        [
          "Catherine",
          "Lai"
        ]
      ],
      "title": "It&#8217;s Not What You Said, it&#8217;s How You Said it: Discriminative Perception of Speech as a Multichannel Communication System",
      "original": "1658",
      "page_count": 5,
      "order": 490,
      "p1": "2386",
      "pn": "2390",
      "abstract": [
        "People convey information extremely effectively through spoken interaction\nusing multiple channels of information transmission: the lexical channel\nof <i>what</i> is said, and the non-lexical channel of <i>how</i> it\nis said. We propose studying human perception of spoken communication\nas a means to better understand how information is encoded across these\nchannels, focusing on the question <i>What characteristics of communicative\ncontext affect listener&#8217;s expectations of speech?</i>. To investigate\nthis, we present a novel behavioural task testing whether listeners\ncan discriminate between the true utterance in a dialogue and utterances\nsampled from other contexts with the same lexical content. We characterize\nhow perception &#8212; and subsequent discriminative capability &#8212;\nis affected by different degrees of additional contextual information\nacross both the lexical and non-lexical channel of speech. Results\ndemonstrate that people can effectively discriminate between different\nprosodic realisations, that non-lexical context is informative, and\nthat this channel provides more salient information than the lexical\nchannel, highlighting the importance of the non-lexical channel in\nspoken interaction.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1658",
      "author_area_id": "11",
      "author_area_label": "Spoken Dialog Systems and Conversational Analysis"
    },
    "michael21_interspeech": {
      "authors": [
        [
          "Thilo",
          "Michael"
        ],
        [
          "Gabriel",
          "Mittag"
        ],
        [
          "Andreas",
          "B\u00fctow"
        ],
        [
          "Sebastian",
          "M\u00f6ller"
        ]
      ],
      "title": "Extending the Fullband E-Model Towards Background Noise, Bursty Packet Loss, and Conversational Degradations",
      "original": "0314",
      "page_count": 5,
      "order": 491,
      "p1": "2391",
      "pn": "2395",
      "abstract": [
        "Quality engineering of speech communication services in the full speech\ntransmission band (0&#8211;20,000 Hz) is facilitated by the fullband\nE-model, a planning tool that predicts overall quality on the basis\nof parameters describing the setting of the service. We presented a\nfirst version of this model at Interspeech 2019, which has since then\nbeen standardized by the International Telecommunication Union in ITU-T\nRec. G.107.2. Whereas that model was limited to predict the effects\nof speech codecs, random packet loss, and transmission delay, more\nrealistic settings such as ambient background noise, bursty packet\nloss, as well as interactive conversational degradations could not\nbe predicted. Based on the results of two new listening-only and conversational\ntests, we present an approach to extend the E-model to better predict\nthese effects in the present paper. The results show that background\nnoise effects at both sending and receiving side can be predicted well,\nwhereas bursty packet loss predictions still have some limitations\nwhich result from the available database. Finally, approaches from\nconversational analysis help to better predict the effects of delay\non conversational quality.\n"
      ],
      "doi": "10.21437/Interspeech.2021-314",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "bergler21_interspeech": {
      "authors": [
        [
          "Christian",
          "Bergler"
        ],
        [
          "Manuel",
          "Schmitt"
        ],
        [
          "Andreas",
          "Maier"
        ],
        [
          "Helena",
          "Symonds"
        ],
        [
          "Paul",
          "Spong"
        ],
        [
          "Steven R.",
          "Ness"
        ],
        [
          "George",
          "Tzanetakis"
        ],
        [
          "Elmar",
          "N\u00f6th"
        ]
      ],
      "title": "ORCA-SLANG: An Automatic Multi-Stage Semi-Supervised Deep Learning Framework for Large-Scale Killer Whale Call Type Identification",
      "original": "0616",
      "page_count": 5,
      "order": 492,
      "p1": "2396",
      "pn": "2400",
      "abstract": [
        "Identification of animal-specific vocalization patterns is an imperative\nrequirement to decode animal communication. In bioacoustics, passive\nacoustic recording setups are increasingly deployed to acquire large-scale\ndatasets. Previous knowledge about established animal-specific call\ntypes is usually present due to historically conducted research. However,\ntime- and human-resource constraints, combined with a lack of available\nmachine-based approaches, only allow manual analysis of comparatively\nsmall data corpora and strongly distort the actual data representation\nand information value. Such data limitations cause restrictions in\nterms of identifying existing population-, group-, and individual-specific\ncall types, sub-categories, as well as unseen vocalization patterns.\nThus, machine learning forms the basis for animal-specific call type\nrecognition, to facilitate more profound insights into communication.\nThe current study is the first fusing task-specific neural networks\nto develop a fully automated, multi-stage, deep-learning-based framework,\nentitled ORCA-SLANG, performing semi-supervised call type identification\nin one of the largest animal-specific bioacoustic archives &#8212;\nthe Orchive. Orca/noise segmentation, denoising, and subsequent feature\nlearning provide robust representations for semi-supervised clustering/classification.\nThis results in a machine-annotated call type data repository containing\n235,369 unique calls.\n"
      ],
      "doi": "10.21437/Interspeech.2021-616",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "boes21_interspeech": {
      "authors": [
        [
          "Wim",
          "Boes"
        ],
        [
          "Hugo",
          "Van hamme"
        ]
      ],
      "title": "Audiovisual Transfer Learning for Audio Tagging and Sound Event Detection",
      "original": "0695",
      "page_count": 5,
      "order": 493,
      "p1": "2401",
      "pn": "2405",
      "abstract": [
        "We study the merit of transfer learning for two sound recognition problems,\ni.e., audio tagging and sound event detection. Employing feature fusion,\nwe adapt a baseline system utilizing only spectral acoustic inputs\nto also make use of pretrained auditory and visual features, extracted\nfrom networks built for different tasks and trained with external data.<br\nstyle=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  We perform experiments with these modified models on an audiovisual\nmulti-label data set, of which the training partition contains a large\nnumber of unlabeled samples and a smaller amount of clips with weak\nannotations, indicating the clip-level presence of 10 sound categories\nwithout specifying the temporal boundaries of the active auditory events.<br\nstyle=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  For clip-based audio tagging, this transfer learning method grants\nmarked improvements. Addition of the visual modality on top of audio\nalso proves to be advantageous in this context.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  When it comes to generating\ntranscriptions of audio recordings, the benefit of pretrained features\ndepends on the requested temporal resolution: for coarse-grained sound\nevent detection, their utility remains notable. But when more fine-grained\npredictions are required, performance gains are strongly reduced due\nto a mismatch between the problem at hand and the goals of the models\nfrom which the pretrained vectors were obtained.\n"
      ],
      "doi": "10.21437/Interspeech.2021-695",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "nessler21_interspeech": {
      "authors": [
        [
          "Natalia",
          "Nessler"
        ],
        [
          "Milos",
          "Cernak"
        ],
        [
          "Paolo",
          "Prandoni"
        ],
        [
          "Pablo",
          "Mainar"
        ]
      ],
      "title": "Non-Intrusive Speech Quality Assessment with Transfer Learning and Subject-Specific Scaling",
      "original": "1685",
      "page_count": 5,
      "order": 494,
      "p1": "2406",
      "pn": "2410",
      "abstract": [
        "In communication systems, it is crucial to estimate the perceived quality\nof audio and speech. The industrial standards for many years have been\nPESQ, 3QUEST, and POLQA, which are intrusive methods. This restricts\nthe possibilities of using these metrics in real-world conditions,\nwhere we might not have access to the clean reference signal. In this\nwork, we develop a new non-intrusive metric based on crowd-sourced\ndata. We build a new speech dataset by combining publicly available\nspeech, noises, and reverberations. Then we follow the ITU P.808 recommendation\nto label the dataset with mean opinion scores (MOS). Finally, we train\na deep neural network to estimate the MOS from the speech data in a\nnon-intrusive way. We propose two novelties in our work. First, we\nexplore transfer learning by pre-training a model using a larger set\nof POLQA scores and finetuning with the smaller (and thus cheaper)\nhuman-labeled set. Secondly, we perform a subject-specific scaling\nin the MOS scores to adjust for their different subjective scales.\nOur model yields better accuracy than PESQ, POLQA, and other non-intrusive\nmethods when evaluated on the independent VCTK test set. We also report\nmisleading POLQA scores for reverberant speech.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1685",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "oncescu21_interspeech": {
      "authors": [
        [
          "Andreea-Maria",
          "Oncescu"
        ],
        [
          "A. Sophia",
          "Koepke"
        ],
        [
          "Jo\u00e3o F.",
          "Henriques"
        ],
        [
          "Zeynep",
          "Akata"
        ],
        [
          "Samuel",
          "Albanie"
        ]
      ],
      "title": "Audio Retrieval with Natural Language Queries",
      "original": "2227",
      "page_count": 5,
      "order": 495,
      "p1": "2411",
      "pn": "2415",
      "abstract": [
        "We consider the task of retrieving audio using free-form natural language\nqueries. To study this problem, which has received limited attention\nin the existing literature, we introduce challenging new benchmarks\nfor text-based audio retrieval using text annotations sourced from\nthe  AudioCaps and  Clotho datasets. We then employ these benchmarks\nto establish baselines for cross-modal audio retrieval, where we demonstrate\nthe benefits of pre-training on diverse audio tasks. We hope that our\nbenchmarks will inspire further research into cross-modal text-based\naudio retrieval with free-form text queries.\n"
      ],
      "doi": "10.21437/Interspeech.2021-2227",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "giollo21_interspeech": {
      "authors": [
        [
          "Manuel",
          "Giollo"
        ],
        [
          "Deniz",
          "Gunceler"
        ],
        [
          "Yulan",
          "Liu"
        ],
        [
          "Daniel",
          "Willett"
        ]
      ],
      "title": "Bootstrap an End-to-End ASR System by Multilingual Training, Transfer Learning, Text-to-Text Mapping and Synthetic Audio",
      "original": "0198",
      "page_count": 5,
      "order": 496,
      "p1": "2416",
      "pn": "2420",
      "abstract": [
        "Bootstrapping speech recognition on limited data resources has been\nan area of active research for long. The recent transition to all-neural\nmodels and end-to-end (E2E) training brought along particular challenges\nas these models are known to be data hungry, but also came with opportunities\naround language-agnostic representations derived from multilingual\ndata as well as shared word-piece output representations across languages\nthat share script and roots. We investigate here the effectiveness\nof different strategies to bootstrap an RNN-Transducer (RNN-T) based\nautomatic speech recognition (ASR) system in the low resource regime,\nwhile exploiting the abundant resources available in other languages\nas well as the synthetic audio from a text-to-speech (TTS) engine.\nOur experiments demonstrate that transfer learning from a multilingual\nmodel, using a post-ASR text-to-text mapping and synthetic audio deliver\nadditive improvements, allowing us to bootstrap a model for a new language\nwith a fraction of the data that would otherwise be needed. The best\nsystem achieved a 46% relative word error rate (WER) reduction compared\nto the monolingual baseline, among which 25% relative WER improvement\nis attributed to the post-ASR text-to-text mappings and the TTS synthetic\ndata.\n"
      ],
      "doi": "10.21437/Interspeech.2021-198",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "pham21_interspeech": {
      "authors": [
        [
          "Ngoc-Quan",
          "Pham"
        ],
        [
          "Tuan-Nam",
          "Nguyen"
        ],
        [
          "Sebastian",
          "St\u00fcker"
        ],
        [
          "Alex",
          "Waibel"
        ]
      ],
      "title": "Efficient Weight Factorization for Multilingual Speech Recognition",
      "original": "0216",
      "page_count": 5,
      "order": 497,
      "p1": "2421",
      "pn": "2425",
      "abstract": [
        "End-to-end multilingual speech recognition involves using a single\nmodel training on a compositional speech corpus including many languages,\nresulting in a single neural network to handle transcribing different\nlanguages. Due to the fact that each language in the training data\nhas different characteristics, the shared network may struggle to optimize\nfor all various languages simultaneously. In this paper we propose\na novel multilingual architecture that targets the core operation in\nneural networks: linear transformation functions. The key idea of the\nmethod is to assign fast weight matrices for each language by decomposing\neach weight matrix into a shared component and a language dependent\ncomponent. The latter is then factorized into vectors using rank-1\nassumptions to reduce the number of parameters per language. This efficient\nfactorization scheme is proved to be effective in two multilingual\nsettings with 7 and 27 languages, reducing the word error rates by\n26% and 27% rel. for two popular architectures LSTM and Transformer,\nrespectively.\n"
      ],
      "doi": "10.21437/Interspeech.2021-216",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "conneau21_interspeech": {
      "authors": [
        [
          "Alexis",
          "Conneau"
        ],
        [
          "Alexei",
          "Baevski"
        ],
        [
          "Ronan",
          "Collobert"
        ],
        [
          "Abdelrahman",
          "Mohamed"
        ],
        [
          "Michael",
          "Auli"
        ]
      ],
      "title": "Unsupervised Cross-Lingual Representation Learning for Speech Recognition",
      "original": "0329",
      "page_count": 5,
      "order": 498,
      "p1": "2426",
      "pn": "2430",
      "abstract": [
        "This paper presents XLSR which learns cross-lingual speech representations\nby pretraining a single model from the raw waveform of speech in multiple\nlanguages. We build on wav2vec 2.0 which is trained by solving a contrastive\ntask over masked latent speech representations and jointly learns a\nquantization of the latents shared across languages. The resulting\nmodel is fine-tuned on labeled data and experiments show that cross-lingual\npretraining significantly outperforms monolingual pretraining. On the\nCommonVoice benchmark, XLSR shows a relative phoneme error rate reduction\nof 72% compared to the best known results. On BABEL, our approach improves\nword error rate by 16% relative compared to a comparable system. Our\napproach enables a single multilingual speech recognition model which\nis competitive to strong individual models. We hope to catalyze research\nin low-resource speech understanding by releasing XLSR-53, a large\nmodel pretrained in 53 languages.\n"
      ],
      "doi": "10.21437/Interspeech.2021-329",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "hayakawa21_interspeech": {
      "authors": [
        [
          "Tomoaki",
          "Hayakawa"
        ],
        [
          "Chee Siang",
          "Leow"
        ],
        [
          "Akio",
          "Kobayashi"
        ],
        [
          "Takehito",
          "Utsuro"
        ],
        [
          "Hiromitsu",
          "Nishizaki"
        ]
      ],
      "title": "Language and Speaker-Independent Feature Transformation for End-to-End Multilingual Speech Recognition",
      "original": "0390",
      "page_count": 5,
      "order": 499,
      "p1": "2431",
      "pn": "2435",
      "abstract": [
        "This paper proposes a method to improve the performance of multilingual\nautomatic speech recognition (ASR) systems through language- and speaker-independent\nfeature transformation in a framework of end-to-end (E2E) ASR. Specifically,\nwe propose a multi-task training method that combines a language recognizer\nand a speaker recognizer with an E2E ASR system based on connectionist\ntemporal classification (CTC) loss functions. We introduce the language\nand speaker recognition sub-tasks into the E2E ASR network and introduce\na gradient reversal layer (GRL) for each sub-task to achieve language\nand speaker-independent feature transformation. The evaluation results\nof the proposed method in the multilingual ASR system in six sorts\nof languages show that the proposed method achieves higher accuracy\nthan the ASR models for each language by introducing multi-tasking\nand GRL.\n"
      ],
      "doi": "10.21437/Interspeech.2021-390",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "n21_interspeech": {
      "authors": [
        [
          "Krishna D.",
          "N"
        ],
        [
          "Pinyi",
          "Wang"
        ],
        [
          "Bruno",
          "Bozza"
        ]
      ],
      "title": "Using Large Self-Supervised Models for Low-Resource Speech Recognition",
      "original": "0631",
      "page_count": 5,
      "order": 500,
      "p1": "2436",
      "pn": "2440",
      "abstract": [
        "Recently, self-supervised pre-training has shown significant improvements\nin many areas of machine learning, including speech and NLP. The self-supervised\nmodels are trained on a large amount of unlabelled data to learn higher-level\nrepresentations for downstream tasks. In this work, we investigate\nthe effectiveness of many self-supervised pre-trained models for the\nlow-resource speech recognition task. We adopt pre-trained wav2vec2.0\n[1] models for the speech recognition task for three Indian languages\nTelugu, Tamil, and Gujarati. We examine both English and multilingual\npre-trained models. Our experiments show that fine-tuning the multilingual\npre-trained model obtains an average relative reduction in WER of 2.88%\ncompared to the previous state-of-the-art supervised method. We carefully\nanalyze the generalization capability of multilingual pre-trained models\nfor both seen and unseen languages. We also show that fine-tuning with\nonly 25% of the training data gives competitive WER to the previous\nbest methods.\n"
      ],
      "doi": "10.21437/Interspeech.2021-631",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "kumar21e_interspeech": {
      "authors": [
        [
          "Mari Ganesh",
          "Kumar"
        ],
        [
          "Jom",
          "Kuriakose"
        ],
        [
          "Anand",
          "Thyagachandran"
        ],
        [
          "Arun Kumar",
          "A"
        ],
        [
          "Ashish",
          "Seth"
        ],
        [
          "Lodagala V.S.V. Durga",
          "Prasad"
        ],
        [
          "Saish",
          "Jaiswal"
        ],
        [
          "Anusha",
          "Prakash"
        ],
        [
          "Hema A.",
          "Murthy"
        ]
      ],
      "title": "Dual Script E2E Framework for Multilingual and Code-Switching ASR",
      "original": "0978",
      "page_count": 5,
      "order": 501,
      "p1": "2441",
      "pn": "2445",
      "abstract": [
        "India is home to multiple languages, and training automatic speech\nrecognition (ASR) systems is challenging. Over time, each language\nhas adopted words from other languages, such as English, leading to\ncode-mixing. Most Indian languages also have their own unique scripts,\nwhich poses a major limitation in training multilingual and code-switching\nASR systems.<br style=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  Inspired by results in text-to-speech synthesis, in this paper,\nwe use an in-house rule-based phoneme-level common label set (CLS)\nrepresentation to train multilingual and code-switching ASR for Indian\nlanguages. We propose two end-to-end (E2E) ASR systems. In the first\nsystem, the E2E model is trained on the CLS representation, and we\nuse a novel data-driven backend to recover the native language script.\nIn the second system, we propose a modification to the E2E model, wherein\nthe CLS representation and the native language characters are used\nsimultaneously for training. We show our results on the multilingual\nand code-switching (MUCS) ASR challenge 2021. Our best results achieve\n&#8776;6% and 5% improvement in word error rate over the baseline system\nfor the multilingual and code-switching tasks, respectively, on the\nchallenge development data.\n"
      ],
      "doi": "10.21437/Interspeech.2021-978",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components:14 Special Sessions"
    },
    "diwan21_interspeech": {
      "authors": [
        [
          "Anuj",
          "Diwan"
        ],
        [
          "Rakesh",
          "Vaideeswaran"
        ],
        [
          "Sanket",
          "Shah"
        ],
        [
          "Ankita",
          "Singh"
        ],
        [
          "Srinivasa",
          "Raghavan"
        ],
        [
          "Shreya",
          "Khare"
        ],
        [
          "Vinit",
          "Unni"
        ],
        [
          "Saurabh",
          "Vyas"
        ],
        [
          "Akash",
          "Rajpuria"
        ],
        [
          "Chiranjeevi",
          "Yarra"
        ],
        [
          "Ashish",
          "Mittal"
        ],
        [
          "Prasanta Kumar",
          "Ghosh"
        ],
        [
          "Preethi",
          "Jyothi"
        ],
        [
          "Kalika",
          "Bali"
        ],
        [
          "Vivek",
          "Seshadri"
        ],
        [
          "Sunayana",
          "Sitaram"
        ],
        [
          "Samarth",
          "Bharadwaj"
        ],
        [
          "Jai",
          "Nanavati"
        ],
        [
          "Raoul",
          "Nanavati"
        ],
        [
          "Karthik",
          "Sankaranarayanan"
        ]
      ],
      "title": "MUCS 2021: Multilingual and Code-Switching ASR Challenges for Low Resource Indian Languages",
      "original": "1339",
      "page_count": 5,
      "order": 502,
      "p1": "2446",
      "pn": "2450",
      "abstract": [
        "Recently, there is an increasing interest in multilingual automatic\nspeech recognition (ASR) where a speech recognition system caters to\nmultiple low resource languages by taking advantage of low amounts\nof labelled corpora in multiple languages. With multilingualism becoming\ncommon in today&#8217;s world, there has been increasing interest in\ncode-switching ASR as well. In code-switching, multiple languages are\nfreely interchanged within a single sentence or between sentences.\nThe success of low-resource multilingual and code-switching (MUCS)\nASR often depends on the variety of languages in terms of their acoustics,\nlinguistic characteristics as well as the amount of data available\nand how these are carefully considered in building the ASR system.\nIn this MUCS 2021 challenge, we would like to focus on building MUCS\nASR systems through two different subtasks related to a total of seven\nIndian languages, namely Hindi, Marathi, Odia, Tamil, Telugu, Gujarati\nand Bengali. For this purpose, we provide a total of &#126;600 hours\nof transcribed speech data, comprising train and test sets, in these\nlanguages, including two code-switched language pairs, Hindi-English\nand Bengali-English. We also provide baseline recipes for both the\nsubtasks with 30.73% and 32.45% word error rate on the MUCS test sets,\nrespectively.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1339"
    },
    "winata21_interspeech": {
      "authors": [
        [
          "Genta Indra",
          "Winata"
        ],
        [
          "Guangsen",
          "Wang"
        ],
        [
          "Caiming",
          "Xiong"
        ],
        [
          "Steven",
          "Hoi"
        ]
      ],
      "title": "Adapt-and-Adjust: Overcoming the Long-Tail Problem of Multilingual Speech Recognition",
      "original": "1390",
      "page_count": 5,
      "order": 503,
      "p1": "2451",
      "pn": "2455",
      "abstract": [
        "One crucial challenge of real-world multilingual speech recognition\nis the long-tailed distribution problem, where some resource-rich languages\nlike English have abundant training data, but a long tail of low-resource\nlanguages have varying amounts of limited training data. To overcome\nthe long-tail problem, in this paper, we propose Adapt-and-Adjust (A2),\na transformer-based multi-task learning framework for end-to-end multilingual\nspeech recognition. The A2 framework overcomes the long-tail problem\nvia three techniques: (1) exploiting a pretrained multilingual language\nmodel to improve the performance of low-resource languages; (2) proposing\ndual adapters consisting of both language-specific and language-agnostic\nadaptation with minimal additional parameters; and (3) overcoming the\nclass imbalance, either by imposing class priors in the loss during\ntraining or adjusting the logits of the softmax output during inference.\nExtensive experiments on the CommonVoice corpus show that A2 significantly\noutperforms conventional approaches.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1390",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "sailor21_interspeech": {
      "authors": [
        [
          "Hardik",
          "Sailor"
        ],
        [
          "Kiran Praveen",
          "T"
        ],
        [
          "Vikas",
          "Agrawal"
        ],
        [
          "Abhinav",
          "Jain"
        ],
        [
          "Abhishek",
          "Pandey"
        ]
      ],
      "title": "SRI-B End-to-End System for Multilingual and Code-Switching ASR Challenges for Low Resource Indian Languages",
      "original": "1578",
      "page_count": 5,
      "order": 504,
      "p1": "2456",
      "pn": "2460",
      "abstract": [
        "This paper describes SRI-B&#8217;s end-to-end Automated Speech Recognition\n(ASR) system proposed for the subtask-1 on multilingual ASR challenges\nfor Indian languages. Our end-to-end (E2E) ASR model is based on the\ntransformer architecture trained by jointly minimizing Connectionist\nTemporal Classification (CTC) &amp; Cross-Entropy (CE) losses. A conventional\nmultilingual model which is trained by pooling data from multiple languages\nhelps in terms of generalization, but it comes at the expense of performance\ndegradation compared to their monolingual counterparts. In our experiments,\na multilingual model is trained by conditioning the input features\nusing a language-specific embedding vector. These language-specific\nembedding vectors are obtained by training a language classifier using\nan attention-based transformer architecture, and then considering its\nbottleneck features as language identification (LID) embeddings. We\nfurther adapt the multilingual system with language specific data to\nreduce the degradation on specific languages. We propose a novel hypothesis\nelimination strategy based on LID scores and length-normalized probabilities\nthat optimally select the model from the pool of available models.\nThe experimental results show that the proposed multilingual training\nand hypothesis elimination strategy gives an average 3.02% of relative\nword error recognition (WER) improvement for the blind set over the\nchallenge hybrid ASR baseline system.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1578"
    },
    "li21f_interspeech": {
      "authors": [
        [
          "Xinjian",
          "Li"
        ],
        [
          "Juncheng",
          "Li"
        ],
        [
          "Florian",
          "Metze"
        ],
        [
          "Alan W.",
          "Black"
        ]
      ],
      "title": "Hierarchical Phone Recognition with Compositional Phonetics",
      "original": "1803",
      "page_count": 5,
      "order": 505,
      "p1": "2461",
      "pn": "2465",
      "abstract": [
        "There is growing interest in building phone recognition systems for\nlow-resource languages as the majority of languages do not have any\nwriting systems. Phone recognition systems proposed so far typically\nderive their phone inventory from the training languages, therefore\nthe derived inventory could only cover a limited number of phones existing\nin the world. It fails to recognize unseen phones in low-resource or\nzero-resource languages. In this work, we tackle this problem with\na hierarchical model, in which we explicitly model three different\nentities in a hierarchical manner: phoneme, phone, and phonological\narticulatory attributes. In particular, we decompose phones into articulatory\nattributes and compute the phone embedding from the attribute embedding.\nThe model would first predict the distribution over the phones using\ntheir embeddings, next, the language-independent phones are aggregated\nto the language-dependent phonemes and then optimized by the CTC loss.\nThis compositional approach enables us to recognize phones even they\ndo not appear in the training set. We evaluate our model on 47 unseen\nlanguages and find the proposed model outperforms baselines by 13.1%\nPER.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1803",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "chowdhury21_interspeech": {
      "authors": [
        [
          "Shammur Absar",
          "Chowdhury"
        ],
        [
          "Amir",
          "Hussein"
        ],
        [
          "Ahmed",
          "Abdelali"
        ],
        [
          "Ahmed",
          "Ali"
        ]
      ],
      "title": "Towards One Model to Rule All: Multilingual Strategy for Dialectal Code-Switching Arabic ASR",
      "original": "1809",
      "page_count": 5,
      "order": 506,
      "p1": "2466",
      "pn": "2470",
      "abstract": [
        "With the advent of globalization, there is an increasing demand for\nmultilingual automatic speech recognition (ASR), handling language\nand dialectal variation of spoken content. Recent studies show its\nefficacy over monolingual systems. In this study, we design a large\nmultilingual end-to-end ASR using self-attention based conformer architecture.\nWe trained the system using Arabic (Ar), English (En) and French (Fr)\nlanguages. We evaluate the system performance handling: (i) monolingual\n(Ar, En and Fr); (ii) multi-dialectal (Modern Standard Arabic, along\nwith dialectal variation such as Egyptian and Moroccan); (iii) code-switching\n&#8212; cross-lingual (Ar-En/Fr) and dialectal (MSA-Egyptian dialect)\ntest cases, and compare with current state-of-the-art systems. Furthermore,\nwe investigate the influence of different embedding/character representations\nincluding character vs word-piece; shared vs distinct input symbol\nper language. Our findings demonstrate the strength of such a model\nby outperforming state-of-the-art monolingual dialectal Arabic and\ncode-switching Arabic ASR.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1809",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "yan21b_interspeech": {
      "authors": [
        [
          "Brian",
          "Yan"
        ],
        [
          "Siddharth",
          "Dalmia"
        ],
        [
          "David R.",
          "Mortensen"
        ],
        [
          "Florian",
          "Metze"
        ],
        [
          "Shinji",
          "Watanabe"
        ]
      ],
      "title": "Differentiable Allophone Graphs for Language-Universal Speech Recognition",
      "original": "1944",
      "page_count": 5,
      "order": 507,
      "p1": "2471",
      "pn": "2475",
      "abstract": [
        "Building language-universal speech recognition systems entails producing\nphonological units of spoken sound that can be shared across languages.\nWhile speech annotations at the language-specific phoneme or surface\nlevels are readily available, annotations at a universal phone level\nare relatively rare and difficult to produce. In this work, we present\na general framework to derive phone-level supervision from only phonemic\ntranscriptions and phone-to-phoneme mappings with <i>learnable</i>\nweights represented using weighted finite-state transducers, which\nwe call <i>differentiable allophone graphs</i>. By training multilingually,\nwe build a universal phone-based speech recognition model with interpretable\nprobabilistic phone-to-phoneme mappings for each language. These phone-based\nsystems with learned allophone graphs can be used by linguists to document\nnew languages, build phone-based lexicons that capture rich pronunciation\nvariations, and re-evaluate the allophone mappings of seen language.\nWe demonstrate the aforementioned benefits of our proposed framework\nwith a system trained on 7 diverse languages.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1944",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "martin21_interspeech": {
      "authors": [
        [
          "Vincent P.",
          "Martin"
        ],
        [
          "Jean-Luc",
          "Rouas"
        ],
        [
          "Florian",
          "Boyer"
        ],
        [
          "Pierre",
          "Philip"
        ]
      ],
      "title": "Automatic Speech Recognition Systems Errors for Objective Sleepiness Detection Through Voice",
      "original": "0291",
      "page_count": 5,
      "order": 508,
      "p1": "2476",
      "pn": "2480",
      "abstract": [
        "Chronic sleepiness, and specifically Excessive Daytime Sleepiness (EDS),\nimpacts everyday life and increases the risks of accidents. Compared\nwith traditional measures (EEG), the detection of objective EDS through\nvoice benefits from its ease to be implemented in ecological conditions\nand to be sober in terms of data processing and costs. Contrary to\nprevious works focusing on short-term sleepiness estimation, this study\nfocuses on long-term sleepiness detection through voice. Using the\nMultiple Sleep Latency Test corpus, this study introduces new features\nbased on Automatic Speech Recognition systems errors, in an attempt\nto replace hand-labeled reading mistakes features. We also introduce\na selection feature pipeline inspired by clinical validation practices\nallowing ASR features to perform on par with the state-of-the-art systems\non short-term sleepiness detection through voice (73.2% of UAR). Moreover,\nwe give insights on the decision process during classification and\nthe specificity of the system regarding the threshold delimiting the\ntwo sleepiness classes, Sleepy and Non-Sleepy.\n"
      ],
      "doi": "10.21437/Interspeech.2021-291",
      "author_area_id": "3",
      "author_area_label": "Paralinguistics in Speech and Language: Human and Automatic Analysis and Processing"
    },
    "gillick21_interspeech": {
      "authors": [
        [
          "Jon",
          "Gillick"
        ],
        [
          "Wesley",
          "Deng"
        ],
        [
          "Kimiko",
          "Ryokai"
        ],
        [
          "David",
          "Bamman"
        ]
      ],
      "title": "Robust Laughter Detection in Noisy Environments",
      "original": "0353",
      "page_count": 5,
      "order": 509,
      "p1": "2481",
      "pn": "2485",
      "abstract": [
        "We investigate the problem of automatically identifying and extracting\nlaughter from audio files in noisy environments. We conduct an empirical\nevaluation of several machine learning models using audio data of varying\nsound quality, finding that while previously published methods work\nrelatively well in controlled environments, performance drops precipitously\nin real-world settings with background noise. In the process, we contribute\na new dataset of laughter annotations on top of the existing AudioSet\ncorpus, with precise segmentations for the start and end points of\neach laugh, and we present a new approach to laughter detection that\nperforms comparatively well in uncontrolled environments. We discuss\nthe utility of our approach as well as the importance of understanding\nthe variability of model performance in a range of real-world testing\nenvironments.\n"
      ],
      "doi": "10.21437/Interspeech.2021-353",
      "author_area_id": "3",
      "author_area_label": "Paralinguistics in Speech and Language: Human and Automatic Analysis and Processing"
    },
    "nagano21_interspeech": {
      "authors": [
        [
          "Mizuki",
          "Nagano"
        ],
        [
          "Yusuke",
          "Ijima"
        ],
        [
          "Sadao",
          "Hiroya"
        ]
      ],
      "title": "Impact of Emotional State on Estimation of Willingness to Buy from Advertising Speech",
      "original": "0827",
      "page_count": 5,
      "order": 510,
      "p1": "2486",
      "pn": "2490",
      "abstract": [
        "The characteristics of a speaker&#8217;s voice can affect the perceived\nimpression or behavior of the listener. Previous studies of consumer\nbehavior have shown that this can be well explained by the emotion-mediated\nbehavior model. However, few studies of the emotion-mediated behavior\nmodel have used advertising speech. In this paper, we examine whether\nthe stimulus-organism-response theory using emotional state can explain\nwillingness to buy from advertising speech stimulus. The subjects listened\nto speech with modified speech features (mean F0, speech rate, spectral\ntilt, or standard deviation of F0) and rated their willingness to buy\nthe products advertised in the speech and their own perceived emotions\n(pleasure, arousal, dominance). We found that the emotions partially\nmediate the influence of speech features on the willingness to buy.\nThese results will be useful for developing a method of speech synthesis\nto increase people&#8217;s willingness to buy.\n"
      ],
      "doi": "10.21437/Interspeech.2021-827",
      "author_area_id": "3",
      "author_area_label": "Paralinguistics in Speech and Language: Human and Automatic Analysis and Processing"
    },
    "alsofyani21_interspeech": {
      "authors": [
        [
          "Huda",
          "Alsofyani"
        ],
        [
          "Alessandro",
          "Vinciarelli"
        ]
      ],
      "title": "Stacked Recurrent Neural Networks for Speech-Based Inference of Attachment Condition in School Age Children",
      "original": "0904",
      "page_count": 5,
      "order": 511,
      "p1": "2491",
      "pn": "2495",
      "abstract": [
        "In Attachment Theory, children that have a positive perception of their\nparents are said to be secure, while the others are said to be insecure.\nOnce adult, unless identified and supported early enough, insecure\nchildren have higher chances to experience major issues (e.g., suicidal\ntendencies and antisocial behavior). For this reason, this article\nproposes a speech-based automatic approach for the recognition of attachment\nin school-age children. The experiments are based on stacked RNNs and\nhave involved 104 children of age between 5 and 9. The accuracy is\nup to 68.9% (F1 59.6%), meaning that the approach makes the right decision\ntwo times out of three, on average. To the best of our knowledge, this\nis the first work aimed at inferring attachment from speech in school-age\nchildren.\n"
      ],
      "doi": "10.21437/Interspeech.2021-904",
      "author_area_id": "3",
      "author_area_label": "Paralinguistics in Speech and Language: Human and Automatic Analysis and Processing"
    },
    "aloshban21_interspeech": {
      "authors": [
        [
          "Nujud",
          "Aloshban"
        ],
        [
          "Anna",
          "Esposito"
        ],
        [
          "Alessandro",
          "Vinciarelli"
        ]
      ],
      "title": "Language or Paralanguage, This is the Problem: Comparing Depressed and Non-Depressed Speakers Through the Analysis of Gated Multimodal Units",
      "original": "0928",
      "page_count": 5,
      "order": 512,
      "p1": "2496",
      "pn": "2500",
      "abstract": [
        "Speech-based depression detection has attracted significant attention\nover the last years. A debated problem is whether it is better to use\nlanguage (what people say), paralanguage (how they say it) or a combination\nof the two. This article addresses the question through the analysis\nof a Gated Multimodal Unit trained to weight modalities according to\nhow effectively they account for the condition of a speaker (depressed\nor non-depressed). The experiments involved 29 individuals diagnosed\nwith depression and 30 non-depressed participants. Besides an accuracy\nof 83.0% (F1 score 80.0%), the results show that the Gated Multimodal\nUnit tends to give more weight to paralanguage. However, the relative\ncontribution of language tends to be higher, to a statistically significant\nextent, in the case of non-depressed speakers.\n"
      ],
      "doi": "10.21437/Interspeech.2021-928",
      "author_area_id": "3",
      "author_area_label": "Paralinguistics in Speech and Language: Human and Automatic Analysis and Processing"
    },
    "tammewar21_interspeech": {
      "authors": [
        [
          "Aniruddha",
          "Tammewar"
        ],
        [
          "Alessandra",
          "Cervone"
        ],
        [
          "Giuseppe",
          "Riccardi"
        ]
      ],
      "title": "Emotion Carrier Recognition from Personal Narratives",
      "original": "1100",
      "page_count": 5,
      "order": 513,
      "p1": "2501",
      "pn": "2505",
      "abstract": [
        "Personal Narratives (PN) &#8212; recollections of facts, events, and\nthoughts from one&#8217;s own experience &#8212; are often used in\neveryday conversations. So far, PNs have mainly been explored for tasks\nsuch as valence prediction or emotion classification (e.g. <i>happy,\nsad</i>). However, these tasks might overlook more fine-grained information\nthat could prove to be relevant for understanding PNs. In this work,\nwe propose a novel task for Narrative Understanding: Emotion Carrier\nRecognition (ECR). Emotion carriers, the text fragments that carry\nthe emotions of the narrator (e.g. <i>loss of a grandpa, high school\nreunion</i>), provide a fine-grained description of the emotion state.\nWe explore the task of ECR in a corpus of PNs manually annotated with\nemotion carriers and investigate different machine learning models\nfor the task. We propose evaluation strategies for ECR including metrics\nthat can be appropriate for different tasks.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1100",
      "author_area_id": "3",
      "author_area_label": "Paralinguistics in Speech and Language: Human and Automatic Analysis and Processing"
    },
    "condron21_interspeech": {
      "authors": [
        [
          "Scott",
          "Condron"
        ],
        [
          "Georgia",
          "Clarke"
        ],
        [
          "Anita",
          "Klementiev"
        ],
        [
          "Daniela",
          "Morse-Kopp"
        ],
        [
          "Jack",
          "Parry"
        ],
        [
          "Dimitri",
          "Palaz"
        ]
      ],
      "title": "Non-Verbal Vocalisation and Laughter Detection Using Sequence-to-Sequence Models and Multi-Label Training",
      "original": "1159",
      "page_count": 5,
      "order": 514,
      "p1": "2506",
      "pn": "2510",
      "abstract": [
        "Non-verbal vocalisations (NVVs) such as laughter are an important part\nof communication in social interactions and carry important information\nabout a speaker&#8217;s state or intention. There remains no clear\ndefinition of NVVs and there is no clearly defined protocol for transcribing\nor detecting NVVs. As such, the standard approach has been to focus\non detecting a single NVV such as laughter and map all other NVVs to\nan &#8220;other&#8221; class. In this paper we hypothesise that for\nthis task such an approach hurts performance, and that giving more\ninformation by using more classes is beneficial. To address this, we\npresent studies using sequence-to-sequence deep neural networks where\nwe include multiple NVV classes rather than mapping them to &#8220;other&#8221;\nand allow more than one label per sample. We show that this approach\nyields better performance than the standard approach on NVV detection.\nWe also evaluate the same model on laughter detection using frame-based\nand utterance-based metrics and show that the proposed approach yields\nstate-of-the-art performance on the ICSI corpus.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1159",
      "author_area_id": "3",
      "author_area_label": "Paralinguistics in Speech and Language: Human and Automatic Analysis and Processing"
    },
    "cai21_interspeech": {
      "authors": [
        [
          "Cong",
          "Cai"
        ],
        [
          "Mingyue",
          "Niu"
        ],
        [
          "Bin",
          "Liu"
        ],
        [
          "Jianhua",
          "Tao"
        ],
        [
          "Xuefei",
          "Liu"
        ]
      ],
      "title": "TDCA-Net: Time-Domain Channel Attention Network for Depression Detection",
      "original": "1176",
      "page_count": 5,
      "order": 515,
      "p1": "2511",
      "pn": "2515",
      "abstract": [
        "Depression is a psychiatric disorder and has many adverse effects on\nour society. Some studies have shown that speech signals are closely\nrelated to emotion and stress, and many speech-based automatic depression\ndetection methods have been proposed. However, previous work is based\non spectrogram or hand-crafted features, which may lose some useful\ninformation related to depression patterns. And there is no evidence\nthat the filter bank designed from perceptual evidence is optimal for\ndepression detection. In order to learn the more discriminative feature\nrepresentation related to depression, we propose an end-to-end time-domain\nchannel attention network (TDCA-Net) for depression detection. The\nTDCA-Net directly models time-domain speech signals based on dilated\nconvolution block, which can increase the receptive field exponentially\nand aggregate multiscale contextual information associated with depression.\nBesides, we employ the efficient channel attention (ECA) module to\nmodel dependencies of channels and improve the sensitivity of the model\nto information related to depression. Experimental results on the AVEC2013\nand the AVEC2014 datasets illustrate the effectiveness of our method.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1176",
      "author_area_id": "3",
      "author_area_label": "Paralinguistics in Speech and Language: Human and Automatic Analysis and Processing"
    },
    "botelho21_interspeech": {
      "authors": [
        [
          "Catarina",
          "Botelho"
        ],
        [
          "Alberto",
          "Abad"
        ],
        [
          "Tanja",
          "Schultz"
        ],
        [
          "Isabel",
          "Trancoso"
        ]
      ],
      "title": "Visual Speech for Obstructive Sleep Apnea Detection",
      "original": "1717",
      "page_count": 5,
      "order": 516,
      "p1": "2516",
      "pn": "2520",
      "abstract": [
        "Obstructive sleep apnea (OSA) affects almost one billion people worldwide\nand limits peoples&#8217; quality of life substantially. Furthermore,\nit is responsible for significant morbidity and mortality associated\nwith hypertension, cardiovascular diseases, work and traffic accidents.\nThus, the early detection of OSA can save lives. In our previous work\nwe used speech as biomarker for automatic OSA detection. More recently,\nwe leveraged the fact that OSA patients have anatomical and functional\nabnormalities of the upper airway and an altered craniofacial morphology,\nand therefore explore information from facial images for OSA detection.\nIn this work, we propose to combine speech and facial image information\nto detect OSA from YouTube vlogs. This in-the-wild data poses an inexpensive\nalternative to standard data collected for medical applications, which\nis often scarce, imbalanced and costly to acquire. Besides speech and\nfacial images, we propose to include <i>visual speech</i> as a third\nmodality, inspired by the emerging field of silent computational paralinguistics.\nWe hypothesize that embeddings trained from lip reading integrate information\non the craniofacial structure, on speech articulation and breathing\npatterns, thus containing relevant cues for OSA detection. Fusion of\nthe three modalities achieves an accuracy of 82.5% at the speaker level.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1717",
      "author_area_id": "3",
      "author_area_label": "Paralinguistics in Speech and Language: Human and Automatic Analysis and Processing"
    },
    "maruri21_interspeech": {
      "authors": [
        [
          "Hector A. Cordourier",
          "Maruri"
        ],
        [
          "Sinem",
          "Aslan"
        ],
        [
          "Georg",
          "Stemmer"
        ],
        [
          "Nese",
          "Alyuz"
        ],
        [
          "Lama",
          "Nachman"
        ]
      ],
      "title": "Analysis of Contextual Voice Changes in Remote Meetings",
      "original": "1932",
      "page_count": 5,
      "order": 517,
      "p1": "2521",
      "pn": "2525",
      "abstract": [
        "People participating in remote meetings in open spaces might choose\nto speak with a restrained voice due to concerns around privacy or\ndisturbing others. These contextual voice changes might impact the\nquality of communications. To investigate how people adjust their voices\nin certain situations, we performed an exploratory data collection\nstudy with 41 participants in 18 simulated remote meetings. A scenario\nwas provided to the participants to naturally trigger contextual voice\nchanges. We collected multi-modal data from the participants including\nin-situ labels for the voice quality. We implemented content analysis,\nt-test, and linear regression to analyze the multi-modal data. Results\nshowed that the participants primarily preferred to use soft voice\nover whispered voice to avoid being overheard during the meetings.\nSpeaking softly was often sufficient to successfully conceal private\nconversations, while using whispered voice had only a negative impact\non the intelligibility. Overall, we found that participants perceived\nsoft voice as less pleasant to listen to than normal voice during meetings\nand discovered factors related to speaker demographics and meeting\ncontext that impacted the concealing behavior (soft or whispered).\nFor our future research, we will expand to different scenarios and\nconsider the impact of audio feedback on voice concealing.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1932",
      "author_area_id": "3",
      "author_area_label": "Paralinguistics in Speech and Language: Human and Automatic Analysis and Processing"
    },
    "seneviratne21_interspeech": {
      "authors": [
        [
          "Nadee",
          "Seneviratne"
        ],
        [
          "Carol",
          "Espy-Wilson"
        ]
      ],
      "title": "Speech Based Depression Severity Level Classification Using a Multi-Stage Dilated CNN-LSTM Model",
      "original": "1967",
      "page_count": 5,
      "order": 518,
      "p1": "2526",
      "pn": "2530",
      "abstract": [
        "Speech based depression classification has gained immense popularity\nover the recent years. However, most of the classification studies\nhave focused on binary classification to distinguish depressed subjects\nfrom non-depressed subjects. In this paper, we formulate the depression\nclassification task as a severity level classification problem to provide\nmore granularity to the classification outcomes. We use articulatory\ncoordination features (ACFs) developed to capture the changes of neuromotor\ncoordination that happens as a result of psychomotor slowing, a necessary\nfeature of Major Depressive Disorder. The ACFs derived from the vocal\ntract variables (TVs) are used to train a dilated Convolutional Neural\nNetwork based depression classification model to obtain segment-level\npredictions. Then, we propose a Recurrent Neural Network based approach\nto obtain session-level predictions from segment-level predictions.\nWe show that strengths of the segment-wise classifier are amplified\nwhen a session-wise classifier is trained on embeddings obtained from\nit. The model trained on ACFs derived from TVs show relative improvement\nof 27.47% in Unweighted Average Recall (UAR) at the session-level classification\ntask, compared to the ACFs derived from Mel Frequency Cepstral Coefficients\n(MFCCs).\n"
      ],
      "doi": "10.21437/Interspeech.2021-1967",
      "author_area_id": "3",
      "author_area_label": "Paralinguistics in Speech and Language: Human and Automatic Analysis and Processing"
    },
    "kim21g_interspeech": {
      "authors": [
        [
          "Ho-Gyeong",
          "Kim"
        ],
        [
          "Min-Joong",
          "Lee"
        ],
        [
          "Hoshik",
          "Lee"
        ],
        [
          "Tae Gyoon",
          "Kang"
        ],
        [
          "Jihyun",
          "Lee"
        ],
        [
          "Eunho",
          "Yang"
        ],
        [
          "Sung Ju",
          "Hwang"
        ]
      ],
      "title": "Multi-Domain Knowledge Distillation via Uncertainty-Matching for End-to-End ASR Models",
      "original": "1169",
      "page_count": 5,
      "order": 519,
      "p1": "2531",
      "pn": "2535",
      "abstract": [
        "Knowledge Distillation basically matches predictive distributions of\nstudent and teacher networks to improve performance in an environment\nwith model capacity and/or data constraints. However, it is well known\nthat predictive distribution of neural networks not only tends to be\noverly confident, but also cannot directly model various factors properly\nthat contribute to uncertainty. Recently, deep learning studies based\non uncertainty have been successful in various fields, especially in\nseveral computer vision tasks. The prediction probability can implicitly\nshow the information about how confident the network is, however, we\ncan explicitly utilize confidence of the output by modeling the uncertainty\nof the network. In this paper, we propose a novel knowledge distillation\nmethod for automatic speech recognition that directly models and transfers\nthe uncertainty inherent in data observation such as speaker variations\nor confusing pronunciations. Moreover, we investigate an effect of\ntransferring knowledge more effectively using multiple teachers learned\nfrom various domains. Evaluated on WSJ which is the standard benchmark\ndataset with limited instances, the proposed knowledge distillation\nmethod achieves significant improvements over student baseline models.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1169",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "macoskey21_interspeech": {
      "authors": [
        [
          "Jonathan",
          "Macoskey"
        ],
        [
          "Grant P.",
          "Strimel"
        ],
        [
          "Ariya",
          "Rastrow"
        ]
      ],
      "title": "Learning a Neural Diff for Speech Models",
      "original": "1575",
      "page_count": 5,
      "order": 520,
      "p1": "2536",
      "pn": "2540",
      "abstract": [
        "As more speech processing applications execute locally on edge devices,\na set of resource constraints must be considered. In this work we address\none of these constraints, namely over-the-network data budgets for\ntransferring models from server to device. We present neural update\napproaches for release of subsequent speech model generations abiding\nby a data budget. We detail two architecture-agnostic methods which\nlearn compact representations for transmission to devices. We experimentally\nvalidate our techniques with results on two tasks (automatic speech\nrecognition and spoken language understanding) on open source data\nsets by demonstrating when applied in succession, our budgeted updates\noutperform comparable model compression baselines by significant margins.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1575",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "zhang21p_interspeech": {
      "authors": [
        [
          "Shucong",
          "Zhang"
        ],
        [
          "Erfan",
          "Loweimi"
        ],
        [
          "Peter",
          "Bell"
        ],
        [
          "Steve",
          "Renals"
        ]
      ],
      "title": "Stochastic Attention Head Removal: A Simple and Effective Method for Improving Transformer Based ASR Models",
      "original": "0280",
      "page_count": 5,
      "order": 521,
      "p1": "2541",
      "pn": "2545",
      "abstract": [
        "Recently, Transformer based models have shown competitive automatic\nspeech recognition (ASR) performance. One key factor in the success\nof these models is the multi-head attention mechanism. However, for\ntrained models, we have previously observed that many attention matrices\nare close to diagonal, indicating the redundancy of the corresponding\nattention heads. We have also found that some architectures with reduced\nnumbers of attention heads have better performance. Since the search\nfor the best structure is time prohibitive, we propose to randomly\nremove attention heads during training and keep all attention heads\nat test time, thus the final model is an ensemble of models with different\narchitectures. The proposed method also forces each head independently\nlearn the most useful patterns. We apply the proposed method to train\nTransformer based and Convolution-augmented Transformer (Conformer)\nbased ASR models. Our method gives consistent performance gains over\nstrong baselines on the Wall Street Journal, AISHELL, Switchboard and\nAMI datasets. To the best of our knowledge, we have achieved state-of-the-art\nend-to-end Transformer based model performance on Switchboard and AMI.\n"
      ],
      "doi": "10.21437/Interspeech.2021-280",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "xue21b_interspeech": {
      "authors": [
        [
          "Jiabin",
          "Xue"
        ],
        [
          "Tieran",
          "Zheng"
        ],
        [
          "Jiqing",
          "Han"
        ]
      ],
      "title": "Model-Agnostic Fast Adaptive Multi-Objective Balancing Algorithm for Multilingual Automatic Speech Recognition Model Training",
      "original": "0355",
      "page_count": 5,
      "order": 522,
      "p1": "2546",
      "pn": "2550",
      "abstract": [
        "This paper regards multilingual automatic speech recognition model\ntraining as a multi-objective problem because learning different languages\nmay conflict, necessitating a trade-off. Most previous works on multilingual\nASR model training mainly used data sampling to balance the performance\nof multiple languages but ignore the conflicts between different languages,\nresulting in an imbalance in multiple languages. The language-specific\nparameters of the multilingual ASR model are updated by the single\nlanguage gradients while the update of the shared parameter is jointly\ndetermined by the gradient of every language on its shared parameter,\nnamely shared gradient. Therefore, we propose a model-agnostic fast\nadaptive (MAFA) multi-objective balancing algorithm to balance multiple\nlanguages by avoiding the mutual interferences between their shared\ngradients. In the algorithm, based on the decrease in the training\nloss, we dynamically normalize the shared gradient magnitudes representing\nthe speed of learning to balance the learning speed. To evenly learn\nmultiple languages, the language with the worst performance is selected,\nand a balancing gradient nearest to the normalized gradient of the\nselected language and positively correlated with other normalized ones\nis obtained to eliminate the mutual interferences. The model trained\nby MAFA outperforms the baseline model on the Common Voice corpus.\n"
      ],
      "doi": "10.21437/Interspeech.2021-355",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "chang21b_interspeech": {
      "authors": [
        [
          "Heng-Jui",
          "Chang"
        ],
        [
          "Hung-yi",
          "Lee"
        ],
        [
          "Lin-shan",
          "Lee"
        ]
      ],
      "title": "Towards Lifelong Learning of End-to-End ASR",
      "original": "0563",
      "page_count": 5,
      "order": 523,
      "p1": "2551",
      "pn": "2555",
      "abstract": [
        "Automatic speech recognition (ASR) technologies today are primarily\noptimized for given datasets; thus, any changes in the application\nenvironment (e.g., acoustic conditions or topic domains) may inevitably\ndegrade the performance. We can collect new data describing the new\nenvironment and fine-tune the system, but this naturally leads to higher\nerror rates for the earlier datasets, referred to as catastrophic forgetting.\nThe concept of lifelong learning (LLL) aiming to enable a machine to\nsequentially learn new tasks from new datasets describing the changing\nreal world without forgetting the previously learned knowledge is thus\nbrought to attention. This paper reports, to our knowledge, the first\neffort to extensively consider and analyze the use of various approaches\nof LLL in end-to-end (E2E) ASR, including proposing novel methods in\nsaving data for past domains to mitigate the catastrophic forgetting\nproblem. An overall relative reduction of 28.7% in WER was achieved\ncompared to the fine-tuning baseline when sequentially learning on\nthree very different benchmark corpora. This can be the first step\ntoward the highly desired ASR technologies capable of synchronizing\nwith the continuously changing real world.\n"
      ],
      "doi": "10.21437/Interspeech.2021-563",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "leal21_interspeech": {
      "authors": [
        [
          "Isabel",
          "Leal"
        ],
        [
          "Neeraj",
          "Gaur"
        ],
        [
          "Parisa",
          "Haghani"
        ],
        [
          "Brian",
          "Farris"
        ],
        [
          "Pedro J.",
          "Moreno"
        ],
        [
          "Manasa",
          "Prasad"
        ],
        [
          "Bhuvana",
          "Ramabhadran"
        ],
        [
          "Yun",
          "Zhu"
        ]
      ],
      "title": "Self-Adaptive Distillation for Multilingual Speech Recognition: Leveraging Student Independence",
      "original": "0614",
      "page_count": 5,
      "order": 524,
      "p1": "2556",
      "pn": "2560",
      "abstract": [
        "With a large population of the world speaking more than one language,\nmultilingual automatic speech recognition (ASR) has gained popularity\nin the recent years. While lower resource languages can benefit from\nquality improvements in a multilingual ASR system, including unrelated\nor higher resource languages in the mix often results in performance\ndegradation. In this paper, we propose distilling from multiple teachers,\nwith each language using its best teacher during training, to tackle\nthis problem. We introduce <i>self-adaptive</i> distillation, a novel\ntechnique for automatic weighting of the distillation loss that uses\nthe student/ teachers confidences. We analyze the effectiveness of\nthe proposed techniques on two real world use-cases and show that the\nperformance of the multilingual ASR models can be improved by up to\n11.5% without any increase in model capacity. Furthermore, we show\nthat when our methods are combined with increase in model capacity,\nwe can achieve quality gains of up to 20.7%.\n"
      ],
      "doi": "10.21437/Interspeech.2021-614",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "xu21f_interspeech": {
      "authors": [
        [
          "Hainan",
          "Xu"
        ],
        [
          "Kartik",
          "Audhkhasi"
        ],
        [
          "Yinghui",
          "Huang"
        ],
        [
          "Jesse",
          "Emond"
        ],
        [
          "Bhuvana",
          "Ramabhadran"
        ]
      ],
      "title": "Regularizing Word Segmentation by Creating Misspellings",
      "original": "0648",
      "page_count": 5,
      "order": 525,
      "p1": "2561",
      "pn": "2565",
      "abstract": [
        "This work focuses on improving subword segmentation algorithms for\nend-to-end speech recognition models, and makes two major contributions.\nFirstly, we propose a novel word segmentation algorithm. The algorithm\nuses the same vocabulary generated by a regular wordpiece model, is\neasily extensible and supports a variety of regularization techniques\nin the segmentation space, and outperforms the regular wordpiece model.\nSecondly, we propose a number of novel regularization methods that\nintroduce randomness into the tokenization algorithm, which bring further\nimprovements in speech recognition accuracy, with relative gains up\nto 8.4% compared to the original wordpiece model. We analyze the methods\nand show that our proposed methods are equivalent to a sophisticated\nform of label smoothing, which performs smoothing based on the prefix\nstructures of subword units. A noteworthy discovery from this work\nis that creating artificial misspellings in words results in the best\nperformance among all the methods, which could inspire future research\nfor strategies in this area.\n"
      ],
      "doi": "10.21437/Interspeech.2021-648",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "wang21t_interspeech": {
      "authors": [
        [
          "Peidong",
          "Wang"
        ],
        [
          "Tara N.",
          "Sainath"
        ],
        [
          "Ron J.",
          "Weiss"
        ]
      ],
      "title": "Multitask Training with Text Data for End-to-End Speech Recognition",
      "original": "0683",
      "page_count": 5,
      "order": 526,
      "p1": "2566",
      "pn": "2570",
      "abstract": [
        "We propose a multitask training method for attention-based end-to-end\nspeech recognition models. We regularize the decoder in a listen, attend,\nand spell model by multitask training it on both audio-text and text-only\ndata. Trained on the 100-hour subset of LibriSpeech, the proposed method,\nwithout requiring an additional language model, leads to an 11% relative\nperformance improvement over the baseline and approaches the performance\nof language model shallow fusion on the test-clean evaluation set.\nWe observe a similar trend on the whole 960-hour LibriSpeech training\nset. Analyses of different types of errors and sample output sentences\ndemonstrate that the proposed method can incorporate language level\ninformation, suggesting its effectiveness in real-world applications.\n"
      ],
      "doi": "10.21437/Interspeech.2021-683",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "chen21j_interspeech": {
      "authors": [
        [
          "Xianzhao",
          "Chen"
        ],
        [
          "Hao",
          "Ni"
        ],
        [
          "Yi",
          "He"
        ],
        [
          "Kang",
          "Wang"
        ],
        [
          "Zejun",
          "Ma"
        ],
        [
          "Zongxia",
          "Xie"
        ]
      ],
      "title": "Emitting Word Timings with HMM-Free End-to-End System in Automatic Speech Recognition",
      "original": "0894",
      "page_count": 5,
      "order": 527,
      "p1": "2571",
      "pn": "2575",
      "abstract": [
        "Word timings, which mark the start and end times of each word in ASR\nresults, play an important part in many applications, such as computer\nassisted language learning. To date, end-to-end (E2E) systems outperform\nconventional DNN-HMM hybrid systems in ASR accuracy but have challenges\nto obtain accurate word timings. In this paper, we propose a two-pass\nmethod to estimate word timings under an E2E-based LAS modeling framework,\nwhich is completely free of using the DNN-HMM ASR system. Specifically,\nwe first employ the LAS system to obtain word-piece transcripts of\nthe input audio, we then compute forced-alignments with a frame-level-based\nword-piece classifier. In order to make the classifier yield accurate\nword-piece timing results, we propose a novel objective function to\nlearn the classifier, utilizing the spike timings of the connectionist\ntemporal classification (CTC) model. On Librispeech data, our E2E-based\nLAS system achieves 2.8%/7.0% WERs, while its word timing (start/end)\naccuracy are 99.0%/95.3% and 98.6%/93.7% on  test-clean and  test-other\ntwo test sets respectively. Compared with a DNN-HMM hybrid ASR system\n(here, TDNN), the LAS system is better in ASR performance, and the\ngenerated word timings are close to what the TDNN ASR system presents.\n"
      ],
      "doi": "10.21437/Interspeech.2021-894",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "droppo21_interspeech": {
      "authors": [
        [
          "Jasha",
          "Droppo"
        ],
        [
          "Oguz",
          "Elibol"
        ]
      ],
      "title": "Scaling Laws for Acoustic Models",
      "original": "1644",
      "page_count": 5,
      "order": 528,
      "p1": "2576",
      "pn": "2580",
      "abstract": [
        "There is a recent trend in machine learning to increase model quality\nby growing models to sizes previously thought to be unreasonable. Recent\nwork has shown that autoregressive generative models with cross-entropy\nobjective functions exhibit smooth power-law relationships, or scaling\nlaws, that predict model quality from model size, training set size,\nand the available compute budget. These scaling laws allow one to choose\nnearly optimal hyper-parameters given constraints on available training\ndata, model parameter count, or training computation budget. In this\npaper, we demonstrate that acoustic models trained with an auto-predictive\ncoding loss behave as if they are subject to similar scaling laws.\nWe extend previous work to jointly predict loss due to model size,\nto training set size, and to the inherent &#8220;irreducible loss&#8221;\nof the task. We find that the scaling laws accurately match model performance\nover two orders of magnitude in both model size and training set size,\nand make predictions about the limits of model performance.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1644",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "billa21_interspeech": {
      "authors": [
        [
          "Jayadev",
          "Billa"
        ]
      ],
      "title": "Leveraging Non-Target Language Resources to Improve ASR Performance in a Target Language",
      "original": "1657",
      "page_count": 5,
      "order": 529,
      "p1": "2581",
      "pn": "2585",
      "abstract": [
        "This paper investigates approaches to improving automatic speech recognition\n(ASR) performance in a target language using resources in other languages.\nIn particular, we assume that we have untranscribed speech in a different\nlanguage and a well trained ASR system in yet another language. Concretely,\nwe structure this as a multi-task problem, where the primary task is\nacoustic model training in the target language, and the secondary task\nis also acoustic model training but using a synthetic data set. The\nsynthetic data set consists of pseudo transcripts generated by decoding\nthe untranscribed speech using a well trained ASR model. We compare\nand contrast this with using labeled data sets, i.e. matched audio\nand human-generated transcripts, and show that our approach compares\nfavorably. In most cases, we see performance improvements, and in some\ncases, depending on the selection of languages and nature of speech\ndata, performance exceeds that of systems using labeled data sets as\nthe secondary task. When extended to larger sets of data, we show that\nthe mismatched data approach performs similarly to in-language semi-supervised\ntraining (SST) when the secondary task pseudo transcripts are generated\nby ASR models trained on large diverse data sets.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1657",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "fasoli21_interspeech": {
      "authors": [
        [
          "Andrea",
          "Fasoli"
        ],
        [
          "Chia-Yu",
          "Chen"
        ],
        [
          "Mauricio",
          "Serrano"
        ],
        [
          "Xiao",
          "Sun"
        ],
        [
          "Naigang",
          "Wang"
        ],
        [
          "Swagath",
          "Venkataramani"
        ],
        [
          "George",
          "Saon"
        ],
        [
          "Xiaodong",
          "Cui"
        ],
        [
          "Brian",
          "Kingsbury"
        ],
        [
          "Wei",
          "Zhang"
        ],
        [
          "Zolt\u00e1n",
          "T\u00fcske"
        ],
        [
          "Kailash",
          "Gopalakrishnan"
        ]
      ],
      "title": "4-Bit Quantization of LSTM-Based Speech Recognition Models",
      "original": "1962",
      "page_count": 5,
      "order": 530,
      "p1": "2586",
      "pn": "2590",
      "abstract": [
        "We investigate the impact of aggressive low-precision representations\nof weights and activations in two families of large LSTM-based architectures\nfor Automatic Speech Recognition (ASR): hybrid Deep Bidirectional LSTM\n- Hidden Markov Models (DBLSTM-HMMs) and Recurrent Neural Network -\nTransducers (RNN-Ts). Using a 4-bit integer representation, a na&#239;ve\nquantization approach applied to the LSTM portion of these models results\nin significant Word Error Rate (WER) degradation. On the other hand,\nwe show that minimal accuracy loss is achievable with an appropriate\nchoice of quantizers and initializations. In particular, we customize\nquantization schemes depending on the local properties of the network,\nimproving recognition performance while limiting computational time.\nWe demonstrate our solution on the Switchboard (SWB) and CallHome (CH)\ntest sets of the NIST Hub5-2000 evaluation. DBLSTM-HMMs trained with\n300 or 2000 hours of SWB data achieves &#60;0.5% and &#60;1% average\nWER degradation, respectively. On the more challenging RNN-T models,\nour quantization strategy limits degradation in 4-bit inference to\n1.3%.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1962",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "masumura21_interspeech": {
      "authors": [
        [
          "Ryo",
          "Masumura"
        ],
        [
          "Daiki",
          "Okamura"
        ],
        [
          "Naoki",
          "Makishima"
        ],
        [
          "Mana",
          "Ihori"
        ],
        [
          "Akihiko",
          "Takashima"
        ],
        [
          "Tomohiro",
          "Tanaka"
        ],
        [
          "Shota",
          "Orihashi"
        ]
      ],
      "title": "Unified Autoregressive Modeling for Joint End-to-End Multi-Talker Overlapped Speech Recognition and Speaker Attribute Estimation",
      "original": "2043",
      "page_count": 5,
      "order": 531,
      "p1": "2591",
      "pn": "2595",
      "abstract": [
        "In this paper, we present a novel modeling method for single-channel\nmulti-talker overlapped automatic speech recognition (ASR) systems.\nFully neural network based end-to-end models have dramatically improved\nthe performance of multi-taker overlapped ASR tasks. One promising\napproach for end-to-end modeling is autoregressive modeling with serialized\noutput training in which transcriptions of multiple speakers are recursively\ngenerated one after another. This enables us to naturally capture relationships\nbetween speakers. However, the conventional modeling method cannot\nexplicitly take into account the speaker attributes of individual utterances\nsuch as gender and age information. In fact, the performance deteriorates\nwhen each speaker is the same gender or is close in age. To address\nthis problem, we propose unified autoregressive modeling for joint\nend-to-end multi-talker overlapped ASR and speaker attribute estimation.\nOur key idea is to handle gender and age estimation tasks within the\nunified autoregressive modeling. In the proposed method, transformer-based\nautoregressive model recursively generates not only textual tokens\nbut also attribute tokens of each speaker. This enables us to effectively\nutilize speaker attributes for improving multi-talker overlapped ASR.\nExperiments on Japanese multi-talker overlapped ASR tasks demonstrate\nthe effectiveness of the proposed method.\n"
      ],
      "doi": "10.21437/Interspeech.2021-2043",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "meng21_interspeech": {
      "authors": [
        [
          "Zhong",
          "Meng"
        ],
        [
          "Yu",
          "Wu"
        ],
        [
          "Naoyuki",
          "Kanda"
        ],
        [
          "Liang",
          "Lu"
        ],
        [
          "Xie",
          "Chen"
        ],
        [
          "Guoli",
          "Ye"
        ],
        [
          "Eric",
          "Sun"
        ],
        [
          "Jinyu",
          "Li"
        ],
        [
          "Yifan",
          "Gong"
        ]
      ],
      "title": "Minimum Word Error Rate Training with Language Model Fusion for End-to-End Speech Recognition",
      "original": "2075",
      "page_count": 5,
      "order": 532,
      "p1": "2596",
      "pn": "2600",
      "abstract": [
        "Integrating external language models (LMs) into end-to-end (E2E) models\nremains a challenging task for domain-adaptive speech recognition.\nRecently, internal language model estimation (ILME)-based LM fusion\nhas shown significant word error rate (WER) reduction from Shallow\nFusion by subtracting a weighted internal LM score from an interpolation\nof E2E model and external LM scores during beam search. However, on\ndifferent test sets, the optimal LM interpolation weights vary over\na wide range and have to be tuned extensively on well-matched validation\nsets. In this work, we perform LM fusion in the minimum WER (MWER)\ntraining of an E2E model to obviate the need for LM weights tuning\nduring inference. Besides MWER training with Shallow Fusion (MWER-SF),\nwe propose a novel MWER training with ILME (MWER-ILME) where the ILME-based\nfusion is conducted to generate N-best hypotheses and their posteriors.\nAdditional gradient is induced when internal LM is engaged in MWER-ILME\nloss computation. During inference, LM weights pre-determined in MWER\ntraining enable robust LM integrations on test sets from different\ndomains. Experimented with 30K-hour trained transformer transducers,\nMWER-ILME achieves on average 8.8% and 5.8% relative WER reductions\nfrom MWER and MWER-SF training, respectively, on 6 different test sets.\n"
      ],
      "doi": "10.21437/Interspeech.2021-2075",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "jiang21b_interspeech": {
      "authors": [
        [
          "Dongcheng",
          "Jiang"
        ],
        [
          "Chao",
          "Zhang"
        ],
        [
          "Philip C.",
          "Woodland"
        ]
      ],
      "title": "Variable Frame Rate Acoustic Models Using Minimum Error Reinforcement Learning",
      "original": "2198",
      "page_count": 5,
      "order": 533,
      "p1": "2601",
      "pn": "2605",
      "abstract": [
        "Frame selection in automatic speech recognition (ASR) systems can potentially\nimprove the trade-off between speed and accuracy relative to fixed\nlow frame rate methods. In this paper, a sequence training approach\nbased on minimum error and reinforcement learning is proposed for a\nhybrid ASR system to operate at a variable frame rate, and uses a frame\nselection controller to predict the number of frames to skip before\ntaking the next inference action. The controller is integrated into\nthe acoustic model in a multi-task training framework as an additional\nregression task and the controller output can be used for distribution\ncharacterisation during reinforcement learning exploration. The reinforcement\nlearning objective minimises a combined measure of the phone error\nand average frame rate. ASR experiments using British English multi-genre\nbroadcast (MGB3) data show that the proposed approach achieved a smaller\nframe rate than using a fixed 1/3 low frame rate method and was able\nto reduce the word error rate relative to both fixed low frame rate\nand full frame rate systems.\n"
      ],
      "doi": "10.21437/Interspeech.2021-2198",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "kaland21_interspeech": {
      "authors": [
        [
          "Constantijn",
          "Kaland"
        ],
        [
          "Matthew",
          "Gordon"
        ]
      ],
      "title": "How f0 and Phrase Position Affect Papuan Malay Word Identification",
      "original": "0006",
      "page_count": 5,
      "order": 534,
      "p1": "2606",
      "pn": "2610",
      "abstract": [
        "This paper reports a perception experiment on Papuan Malay, an Eastern\nIndonesian language for which phrase prosody is largely underresearched.\nWhile phrase-final f0 movements are the most prominent ones in this\nlanguage, it remains to be seen to what extent they signal phrase boundaries\n(demarcating) or whether they contribute to the prosodic prominence\nof words in that position (highlighting). Crucially, it is unclear\nwhether these functions can actually be teased apart. In an attempt\nto investigate this issue, a word identification experiment was carried\nout using manipulated and original f0 word contours in phrase-medial\nand phrase-final positions. Results indicate that Papuan Malay listeners\nrecognize words faster in phrase-final position, although the shape\nof the f0 movement did not significantly affect response latencies.\nThe outcomes are discussed in a typological perspective, with particular\nattention to Trade Malay languages.\n"
      ],
      "doi": "10.21437/Interspeech.2021-6",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "jespersen21_interspeech": {
      "authors": [
        [
          "Anna Bothe",
          "Jespersen"
        ],
        [
          "Pavel",
          "\u0160turm"
        ],
        [
          "M\u00ed\u0161a",
          "Hejn\u00e1"
        ]
      ],
      "title": "On the Feasibility of the Danish Model of Intonational Transcription: Phonetic Evidence from Jutlandic Danish",
      "original": "0190",
      "page_count": 5,
      "order": 535,
      "p1": "2611",
      "pn": "2615",
      "abstract": [
        "Most of our knowledge of Danish f0 variation and intonation is based\non the work of Gr&#248;nnum and colleagues, who developed an a-phonological\nmodel in which a series of repeated &#8220;default&#8221; contours\nare superpositioned onto an overarching f0 slope. The current paper\ntests a range of predictions stemming from this model, most importantly\nthe adequacy of analysing f0 modulations as a string of repeated contours\ndiffering in range but not in shape. To facilitate comparison with\nearlier work in the area, our material is based on read speech, 45\nspeakers of Jutland Danish participated in the experiment. Analyses\nof f0 in sentences of differing complexity supplied little evidence\nin favour of the existence of default contours. Instead, our acoustic\ndata revealed an array of f0 shapes associated with various prosodic\nanchor points, which are influenced in both range and shape by positional\ncontext and the presence or absence of focus.\n"
      ],
      "doi": "10.21437/Interspeech.2021-190"
    },
    "meli21_interspeech": {
      "authors": [
        [
          "Adrien",
          "M\u00e9li"
        ],
        [
          "Nicolas",
          "Ballier"
        ],
        [
          "Achille",
          "Falaise"
        ],
        [
          "Alice",
          "Henderson"
        ]
      ],
      "title": "An Experiment in Paratone Detection in a Prosodically Annotated EAP Spoken Corpus",
      "original": "0294",
      "page_count": 5,
      "order": 536,
      "p1": "2616",
      "pn": "2620",
      "abstract": [
        "This article describes an experiment in paratone detection based on\na spoken corpus of English for Academic Purposes (EAP) recently automatically\nre-annotated with prosodic information. The Momel and INTSINT annotations\nwere carried out using SPPAS. The EIIDA corpus was chosen as it offered\nlong uninterrupted stretches of speech of academic presentations. We\ndescribe the clustering method adopted for automatic detection, contrasting\na supervised and an unsupervised method of paratone boundary detection.\nWe showcase the relevance of the annotation scheme followed for this\ncorpus and contribute to the investigation of the phonostyle of lecture\ndelivery. We discuss the relevance of clustering methods applied to\nthe labels of the pitch targets for the analysis of paratones.\n"
      ],
      "doi": "10.21437/Interspeech.2021-294",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "gerazov21_interspeech": {
      "authors": [
        [
          "Branislav",
          "Gerazov"
        ],
        [
          "Michael",
          "Wagner"
        ]
      ],
      "title": "ProsoBeast Prosody Annotation Tool",
      "original": "0304",
      "page_count": 5,
      "order": 537,
      "p1": "2621",
      "pn": "2625",
      "abstract": [
        "The labelling of speech corpora is a laborious and time-consuming process.\nThe ProsoBeast Annotation Tool seeks to ease and accelerate this process\nby providing an interactive 2D representation of the prosodic landscape\nof the data, in which contours are distributed based on their similarity.\nThis interactive map allows the user to inspect and label the utterances.\nThe tool integrates several state-of-the-art methods for dimensionality\nreduction and feature embedding, including variational autoencoders.\nThe user can use these to find a good representation for their data.\nIn addition, as most of these methods are stochastic, each can be used\nto generate an unlimited number of different prosodic maps. The web\napp then allows the user to seamlessly switch between these alternative\nrepresentations in the annotation process. Experiments with a sample\nprosodically rich dataset have shown that the tool manages to find\ngood representations of varied data and is helpful both for annotation\nand label correction. The tool is released as free software for use\nby the community.\n"
      ],
      "doi": "10.21437/Interspeech.2021-304",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "tran21_interspeech": {
      "authors": [
        [
          "Trang",
          "Tran"
        ],
        [
          "Mari",
          "Ostendorf"
        ]
      ],
      "title": "Assessing the Use of Prosody in Constituency Parsing of Imperfect Transcripts",
      "original": "0373",
      "page_count": 5,
      "order": 538,
      "p1": "2626",
      "pn": "2630",
      "abstract": [
        "This work explores constituency parsing on automatically recognized\ntranscripts of conversational speech. The neural parser is based on\na sentence encoder that leverages word vectors contextualized with\nprosodic features, jointly learning prosodic feature extraction with\nparsing. We assess the utility of the prosody in parsing on imperfect\ntranscripts, i.e. transcripts with automatic speech recognition (ASR)\nerrors, by applying the parser in an N-best reranking framework. In\nexperiments on Switchboard, we obtain 13&#8211;15% of the oracle N-best\ngain relative to parsing the 1-best ASR output, with insignificant\nimpact on word recognition error rate. Prosody provides a significant\npart of the gain, and analyses suggest that it leads to more grammatical\nutterances via recovering function words.\n"
      ],
      "doi": "10.21437/Interspeech.2021-373",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "liu21i_interspeech": {
      "authors": [
        [
          "Roger Cheng-yen",
          "Liu"
        ],
        [
          "Feng-fan",
          "Hsieh"
        ],
        [
          "Yueh-chin",
          "Chang"
        ]
      ],
      "title": "Targeted and Targetless Neutral Tones in Taiwanese Southern Min",
      "original": "0434",
      "page_count": 5,
      "order": 539,
      "p1": "2631",
      "pn": "2635",
      "abstract": [
        "This article is an acoustic study on the two types of neutral tone\nin Taiwanese Southern Min (TSM). Recording materials included a set\nof verb-clitic constructions with different preceding tones and clitics.\nPitch contours in different conditions were compared using Smoothing\nSpline ANOVA. Our results confirmed that Type 1 neutral tone (NT1)\nhas a low pitch target and that Type 2 neutral tone (NT2) is contextually\ndependent. Whether NT1 or NT2 is chosen has been treated as the lexical\nidiosyncrasy of the clitics in question, with idiolectal and dialectal\nvariations. However, we found in this study that the onsets have a\nbearing on determining the type of neutral tone: the more sonorous\nthe onset, the more possible it is for the clitic to be in NT2. In\nsum, the two distinct types of neutral tones in TSM not only are unusual\namong the neutral tones in Sinitic languages, but they also offer novel\ndata for the consonant-tone interaction.\n"
      ],
      "doi": "10.21437/Interspeech.2021-434",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "gosy21_interspeech": {
      "authors": [
        [
          "M\u00e1ria",
          "G\u00f3sy"
        ],
        [
          "K\u00e1lm\u00e1n",
          "Abari"
        ]
      ],
      "title": "The Interaction of Word Complexity and Word Duration in an Agglutinative Language",
      "original": "0594",
      "page_count": 5,
      "order": 540,
      "p1": "2636",
      "pn": "2640",
      "abstract": [
        "The mental lexicon comprises the representations of various words either\nin a morphologically decomposed form, or in a conceptually non-decomposed\nform. The durations of mono-morphemic and multimorphemic words are\nassumed to contain information on the routes of their lexical access.<br\nstyle=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  The durations of Hungarian nouns with various lengths produced\nspontaneously by 10 young and 10 elderly speakers (with 55 years of\ndifference between them) were measured. Findings showed significant\ndifferences depending on the words&#8217; complexity and on age. The\nnouns both with and without suffixes were significantly longer in old\nthan in young speakers. The durational differences depending on age\nwere more pronounced in monomorphemic nouns as opposed to multimorphemic\nnouns. Along with the increasing number of syllables of the nouns,\nold speakers produced increasingly longer simple nouns (stems) than\nyoung ones did.<br style=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  We suggest that multimorphemic nouns are accessed decompositionally\nin spontaneous utterances when the stem activation is followed by the\nactivation of the suffixes. The specific storage and the corresponding\nlexical access of the morphemes explain the longer durations of the\ninflected nouns.\n"
      ],
      "doi": "10.21437/Interspeech.2021-594",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "pan21b_interspeech": {
      "authors": [
        [
          "Ho-hsien",
          "Pan"
        ],
        [
          "Shao-ren",
          "Lyu"
        ]
      ],
      "title": "Taiwan Min Nan (Taiwanese) Checked Tones Sound Change",
      "original": "0672",
      "page_count": 5,
      "order": 541,
      "p1": "2641",
      "pn": "2645",
      "abstract": [
        "The multifaced changes of Taiwan Min Nan (TMN) checked sandhi tones,\nS3 and S5 were investigated as well as the checked base tones, B3 and\nB5. Simultaneous EGG data, CQ_H and acoustic data, including duration,\nf0 offset at 80% vowel interval, and spectral tilt H1<SUP>*</SUP>-A3<SUP>*</SUP>\nfrom forty male and female speakers above 40 and under 30 years of\nage were analyzed. Though different measures progress at different\npaces, in general, as the coda stops [p, t, k, &#660;] from full stop\nclosure, to energy damping and finally to complete deletion, vowel\nduration lengthening, f0 offset lowering, and more modal phonation\nwere observed. Gender effects were found on f0 offset and CQ_H offset.\nThe pace of progress is more advanced for base tone B5 with glottal\ncoda stops. After coda deletion, the contexts conditioning the anticipatory\nco-articulation were removed and vowel and tone characteristics were\nmodified to be similar to those found in open syllables.\n"
      ],
      "doi": "10.21437/Interspeech.2021-672",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "jakob21_interspeech": {
      "authors": [
        [
          "Moritz",
          "Jakob"
        ],
        [
          "Bettina",
          "Braun"
        ],
        [
          "Katharina",
          "Zahner-Ritter"
        ]
      ],
      "title": "In-Group Advantage in the Perception of Emotions: Evidence from Three Varieties of German",
      "original": "1172",
      "page_count": 5,
      "order": 542,
      "p1": "2646",
      "pn": "2650",
      "abstract": [
        "Various studies on the perception of vocally expressed emotions have\nshown that recognition rates are higher if speaker and listener belong\nto the same cultural or linguistic group. This so-called <i>in-group\nadvantage</i> is commonly attributed to prosodic differences in the\nexpression of emotion across groups. Evidence comes mostly from using\ncross-linguistic and/or cross-cultural study designs. Previous research\nsuggests that varieties of German differ in their use of prosody and\ncan be discriminated based on prosodic features alone. In this paper,\nwe tested whether emotion recognition rates differ across varieties\nof German: Listeners from three dialectal areas (Hamburg, Vienna, Zurich)\nidentified emotions on semantically neutral sentences (choosing between\nanger, happiness, relief, surprise or &#8220;other&#8221;), spoken\nby actors from the three regions. Correctness rates show that emotions\nare recognized better if speakers and listeners are native speakers\nof the same variety. However, further analyses suggest that the in-group\nadvantage does not surface consistently across individual emotions.\nTo explain these results, the prosodic realization of the sentences\nwas tested for interactions between emotion and variety. Here, intensity\nseemed to differ most across varieties and emotions. Importantly, we\nshow that the in-group advantage extends from cultural groups to dialectal\ngroups of a language.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1172",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "gobl21_interspeech": {
      "authors": [
        [
          "Christer",
          "Gobl"
        ]
      ],
      "title": "The LF Model in the Frequency Domain for Glottal Airflow Modelling Without Aliasing Distortion",
      "original": "1625",
      "page_count": 5,
      "order": 543,
      "p1": "2651",
      "pn": "2655",
      "abstract": [
        "Many of the commonly used voice source models are based on piecewise\nelementary functions defined in the time domain. The discrete-time\nimplementation of such models generally causes aliasing distortion,\nwhich make them less useful for certain applications. This paper presents\na method which eliminates this distortion. The key component of the\nproposed method is the frequency domain description of the source model.\nBy deploying the Laplace transform and phasor arithmetic, closed-form\nexpressions of the source model spectrum can be derived. This facilitates\nthe calculation of the spectrum directly from the model parameters,\nwhich in turn makes it possible to obtain the ideal discrete spectrum\nof the model given the sampling frequency used. This discrete spectrum\nis entirely free of aliasing distortion, and the inverse discrete Fourier\ntransform is used to compute the sampled glottal flow pulse. The proposed\nmethod was applied to the widely used LF model, and the complete Laplace\ntransform of the model is presented. Also included are closed-form\nexpressions of the amplitude spectrum and the phase spectrum for the\ncalculation of the LF model spectrum.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1625",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "wagner21_interspeech": {
      "authors": [
        [
          "Michael",
          "Wagner"
        ],
        [
          "Alvaro Iturralde",
          "Zurita"
        ],
        [
          "Sijia",
          "Zhang"
        ]
      ],
      "title": "Parsing Speech for Grouping and Prominence, and the Typology of Rhythm",
      "original": "1684",
      "page_count": 5,
      "order": 544,
      "p1": "2656",
      "pn": "2660",
      "abstract": [
        "Humans appear to be wired to perceive acoustic events rhythmically.\nEnglish speakers, for example, tend to perceive alternating short and\nlong sounds as a series of binary groups with a final beat (iambs),\nand alternating soft and loud sounds as a series of trochees. This\ngeneralization, often called the &#8216;Iambic-trochaic Law&#8217;\n(ITL), although viewed as an auditory universal by some, has been argued\nto be shaped by language experience. Earlier work on the ITL had a\ncrucial limitation, in that it did not tease apart the percepts of\ngrouping and prominence, which the notions of iamb and trochee inherently\nconfound. We explore how intensity and duration relate to percepts\nof prominence and grouping in six languages (English, French, German,\nJapanese, Mandarin, and Spanish). The results show that the ITL is\nnot universal, and that cue interpretation is shaped by language experience.\nHowever, there are also invariances: Duration appears relatively robust\nacross languages as a cue to prominence (longer syllables are perceived\nas stressed), and intensity for grouping (louder syllables are perceived\nas initial). The results show the beginnings of a rhythmic typology\nbased on how the dimensions of grouping and prominence are cued.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1684",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "mumtaz21_interspeech": {
      "authors": [
        [
          "Benazir",
          "Mumtaz"
        ],
        [
          "Massimiliano",
          "Canzi"
        ],
        [
          "Miriam",
          "Butt"
        ]
      ],
      "title": "Prosody of Case Markers in Urdu",
      "original": "1776",
      "page_count": 5,
      "order": 545,
      "p1": "2661",
      "pn": "2665",
      "abstract": [
        "This paper studies the prosody of case clitics in Urdu, for which various\ndifferent claims exist in the literature. We conducted a production\nexperiment and controlled for effects potentially arising from the\nphonetics of the case clitics, the syntactic function they express\nand clausal position. We find that case clitics are incorporated into\nthe prosodic phrase of the noun and that they become part of the overall\nLH contour found on accentual phrases in Urdu/Hindi. We also find some\ndifferences across case type and position which we tie to information\nstructural effects.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1776",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "stefansdottir21_interspeech": {
      "authors": [
        [
          "Brynhildur",
          "Stefansdottir"
        ],
        [
          "Francesco",
          "Burroni"
        ],
        [
          "Sam",
          "Tilsen"
        ]
      ],
      "title": "Articulatory Characteristics of Icelandic Voiced Fricative Lenition: Gradience, Categoricity, and Speaker/Gesture-Specific Effects",
      "original": "1903",
      "page_count": 5,
      "order": 546,
      "p1": "2666",
      "pn": "2670",
      "abstract": [
        "Icelandic voiced fricatives frequently reduce in connected speech.\nHowever, systematic investigations of the phenomenon from acoustic\nand articulatory perspectives are lacking. To further the understanding\nof this lenition process, we present electromagnetic articulography\nand acoustic data from four speakers concerning the intervocalic realization\nof the dental and velar fricatives. The results show that lenition\nis mostly gradient, but some speakers and places of articulation exhibit\ntwo distinct modes suggesting a categorical distinction. Moreover,\nin some tokens, the fricative constriction is absent from the articulatory\ntrajectories. Finally, the relation between lenition and speech rate,\nstyle, and stress is also subject to speaker- and gesture-specific\neffects. We conclude by evaluating how our findings challenge the common\nassumptions, made in the literature, that lenition is a change in gestural\ntarget or a perceptually driven phenomenon.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1903",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "johnson21_interspeech": {
      "authors": [
        [
          "Khia A.",
          "Johnson"
        ]
      ],
      "title": "Leveraging the Uniformity Framework to Examine Crosslinguistic Similarity for Long-Lag Stops in Spontaneous Cantonese-English Bilingual Speech",
      "original": "1780",
      "page_count": 5,
      "order": 547,
      "p1": "2671",
      "pn": "2675",
      "abstract": [
        "While crosslinguistic influence is widespread in bilingual speech production,\nit is less clear which aspects of representation are shared across\nlanguages, if any. Most prior work examines phonetically distinct yet\nphonologically similar sounds, for which phonetic convergence suggests\na cross-language link within individuals [1]. Convergence is harder\nto assess when sounds are already similar, as with English and Cantonese\ninitial long-lag stops. Here, the articulatory uniformity framework\n[2, 3, 4] is leveraged to assess whether bilinguals share an underlying\nlaryngeal feature across languages, and describe the nature of cross-language\nlinks. Using the SpiCE corpus of spontaneous Cantonese-English bilingual\nspeech [5], this paper asks whether Cantonese-English bilinguals exhibit\nuniform voice-onset time for long-lag stops within and across languages.\nResults indicate moderate patterns of uniformity within-language &#8212;\nreplicating prior work [2, 6] &#8212; and weaker patterns across languages.\nThe analysis, however, raises many questions, as correlations were\ngenerally lower compared to prior work, and talkers did not adhere\nto expected ordinal relationships by place of articulation. Talkers\nalso retained clear differences for /t/ and /k/, despite expectations\nof similarity. Yet at the same time, more of the overall variation\nseems to derive from individual-specific differences. While many questions\nremain, the uniformity framework shows promise.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1780",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "sivaraman21_interspeech": {
      "authors": [
        [
          "Aswin",
          "Sivaraman"
        ],
        [
          "Sunwoo",
          "Kim"
        ],
        [
          "Minje",
          "Kim"
        ]
      ],
      "title": "Personalized Speech Enhancement Through Self-Supervised Data Augmentation and Purification",
      "original": "1868",
      "page_count": 5,
      "order": 548,
      "p1": "2676",
      "pn": "2680",
      "abstract": [
        "Training personalized speech enhancement models is innately a no-shot\nlearning problem due to privacy constraints and limited access to noise-free\nspeech from the target user. If there is an abundance of unlabeled\nnoisy speech from the test-time user, one may train a personalized\nspeech enhancement model using self-supervised learning. One straightforward\napproach to model personalization is to use the target speaker&#8217;s\nnoisy recordings as pseudo-sources. Then, a pseudo denoising model\nlearns to remove injected training noises and recover the pseudo-sources.\nHowever, this approach is volatile as it depends on the quality of\nthe pseudo-sources, which may be too noisy. To remedy this, we propose\na data purification step that refines the self-supervised approach.\nWe first train an SNR predictor model to estimate the frame-by-frame\nSNR of the pseudo-sources. Then, we convert the predictor&#8217;s estimates\ninto weights that adjust the pseudo-sources&#8217; frame-by-frame contribution\ntowards training the personalized model. We empirically show that the\nproposed data purification step improves the usability of the speaker-specific\nnoisy data in the context of personalized speech enhancement. Our approach\nmay be seen as privacy-preserving as it does not rely on any clean\nspeech recordings or speaker embeddings.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1868",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "saddler21_interspeech": {
      "authors": [
        [
          "Mark R.",
          "Saddler"
        ],
        [
          "Andrew",
          "Francl"
        ],
        [
          "Jenelle",
          "Feather"
        ],
        [
          "Kaizhi",
          "Qian"
        ],
        [
          "Yang",
          "Zhang"
        ],
        [
          "Josh H.",
          "McDermott"
        ]
      ],
      "title": "Speech Denoising with Auditory Models",
      "original": "1973",
      "page_count": 5,
      "order": 549,
      "p1": "2681",
      "pn": "2685",
      "abstract": [
        "Contemporary speech enhancement predominantly relies on audio transforms\nthat are trained to reconstruct a clean speech waveform. The development\nof high-performing neural network sound recognition systems has raised\nthe possibility of using deep feature representations as &#8216;perceptual&#8217;\nlosses with which to train denoising systems. We explored their utility\nby first training deep neural networks to classify either spoken words\nor environmental sounds from audio. We then trained an audio transform\nto map noisy speech to an audio waveform that minimized the difference\nin the deep feature representations between the output audio and the\ncorresponding clean audio. The resulting transforms removed noise substantially\nbetter than baseline methods trained to reconstruct clean waveforms,\nand also outperformed previous methods using deep feature losses. However,\na similar benefit was obtained simply by using losses derived from\nthe filter bank inputs to the deep networks. The results show that\ndeep features can guide speech enhancement, but suggest that they do\nnot yet outperform simple alternatives that do not involve learned\nfeatures.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1973",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "eskimez21b_interspeech": {
      "authors": [
        [
          "Sefik Emre",
          "Eskimez"
        ],
        [
          "Xiaofei",
          "Wang"
        ],
        [
          "Min",
          "Tang"
        ],
        [
          "Hemin",
          "Yang"
        ],
        [
          "Zirun",
          "Zhu"
        ],
        [
          "Zhuo",
          "Chen"
        ],
        [
          "Huaming",
          "Wang"
        ],
        [
          "Takuya",
          "Yoshioka"
        ]
      ],
      "title": "Human Listening and Live Captioning: Multi-Task Training for Speech Enhancement",
      "original": "0220",
      "page_count": 5,
      "order": 550,
      "p1": "2686",
      "pn": "2690",
      "abstract": [
        "With the surge of online meetings, it has become more critical than\never to provide high-quality speech audio and live captioning under\nvarious noise conditions. However, most monaural speech enhancement\n(SE) models introduce processing artifacts and thus degrade the performance\nof downstream tasks, including automatic speech recognition (ASR).\nThis paper proposes a multi-task training framework to make the SE\nmodels unharmful to ASR. Because most ASR training samples do not have\ncorresponding clean signal references, we alternately perform two model\nupdate steps called SE-step and ASR-step. The SE-step uses clean and\nnoisy signal pairs and a signal-based loss function. The ASR-step applies\na pre-trained ASR model to training signals enhanced with the SE model.\nA cross-entropy loss between the ASR output and reference transcriptions\nis calculated to update the SE model parameters. Experimental results\nwith realistic large-scale settings using ASR models trained on 75,000-hour\ndata show that the proposed framework improves the word error rate\nfor the SE output by 11.82% with little compromise in the SE quality.\nPerformance analysis is also carried out by changing the ASR model,\nthe data used for the ASR-step, and the schedule of the two update\nsteps.\n"
      ],
      "doi": "10.21437/Interspeech.2021-220",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "xu21g_interspeech": {
      "authors": [
        [
          "Xinmeng",
          "Xu"
        ],
        [
          "Yang",
          "Wang"
        ],
        [
          "Dongxiang",
          "Xu"
        ],
        [
          "Yiyuan",
          "Peng"
        ],
        [
          "Cong",
          "Zhang"
        ],
        [
          "Jie",
          "Jia"
        ],
        [
          "Binbin",
          "Chen"
        ]
      ],
      "title": "Multi-Stage Progressive Speech Enhancement Network",
      "original": "0520",
      "page_count": 5,
      "order": 551,
      "p1": "2691",
      "pn": "2695",
      "abstract": [
        "Speech enhancement is a fundamental way to separate and generate clean\nspeech from adverse environment where the received speech is seriously\ncorrupted by noise. This paper applies a novel progressive network\nfor speech enhancement by using multi-stage structure, where each stage\ncontains a channel attention block followed by dilated encoder-decoder\nconvolutional network with gated linear units. In addition, each stage\ngenerates a prediction that is refined by a supervised attention block.\nWhat is more, a fusion block is inserted between original inputs and\noutputs of previous stage. Multi-stage architecture is introduced to\nsequentially invoke multiple deep-learning networks, and its key ingredient\nis the information exchange between different stages. Thus, a more\nflexible and robust outputs can be generated. Experimental results\nshow that the proposed architecture obtains consistently better performance\nthan recent state-of-the-art models in terms of both PESQ and STOI\nscores.\n"
      ],
      "doi": "10.21437/Interspeech.2021-520",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "chang21c_interspeech": {
      "authors": [
        [
          "Oscar",
          "Chang"
        ],
        [
          "Dung N.",
          "Tran"
        ],
        [
          "Kazuhito",
          "Koishida"
        ]
      ],
      "title": "Single-Channel Speech Enhancement Using Learnable Loss Mixup",
      "original": "0859",
      "page_count": 5,
      "order": 552,
      "p1": "2696",
      "pn": "2700",
      "abstract": [
        "Generalization remains a major problem in supervised learning of single-channel\nspeech enhancement. In this work, we propose <i>learnable loss mixup\n(LLM)</i>, a simple and effortless training diagram, to improve the\ngeneralization of deep learning-based speech enhancement models. <i>Loss\nmixup</i>, of which <i>learnable loss mixup</i> is a special variant,\noptimizes a mixture of the loss functions of random sample pairs to\ntrain a model on virtual training data constructed from these pairs\nof samples. In <i>learnable loss mixup</i>, by conditioning on the\nmixed data, the loss functions are mixed using a non-linear mixing\nfunction automatically learned via neural parameterization. Our experimental\nresults on the VCTK benchmark show that <i>learnable loss mixup</i>\nachieves 3.26 PESQ, outperforming the state-of-the-art.\n"
      ],
      "doi": "10.21437/Interspeech.2021-859",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "zhang21q_interspeech": {
      "authors": [
        [
          "Xiao-Qi",
          "Zhang"
        ],
        [
          "Jun",
          "Du"
        ],
        [
          "Li",
          "Chai"
        ],
        [
          "Chin-Hui",
          "Lee"
        ]
      ],
      "title": "A Maximum Likelihood Approach to SNR-Progressive Learning Using Generalized Gaussian Distribution for LSTM-Based Speech Enhancement",
      "original": "0922",
      "page_count": 5,
      "order": 553,
      "p1": "2701",
      "pn": "2705",
      "abstract": [
        "A maximum likelihood (ML) approach to characterizing regression errors\nin each target layer of SNR progressive learning (PL) using long short-term\nmemory (LSTM) networks is proposed to improve performances of speech\nenhancement at low SNR levels. Each LSTM layer is guided to learn an\nintermediate target with a specific SNR gain. In contrast to using\npreviously proposed minimum squared error criterion (MMSE-PL-LSTM)\nwhich leads to an un-even distribution and a broad dynamic range of\nthe prediction errors, we model the errors with a generalized Gaussian\ndistribution (GGD) at all intermediate layers in the newly proposed\nML-PL-LSTM framework. The shape factors in GGD can be automatically\nupdated when training the LSTM networks in a layer-wise manner to estimate\nthe network parameters progressively. Tested on the CHiME-4 simulation\nset for speech enhancement in unseen noise conditions, the proposed\nML-PL-LSTM approach outperforms MMSE-PL-LSTM in terms of both PESQ\nand STOI measures. Furthermore, when evaluated on the CHiME-4 real\ntest set for speech recognition, using ML-enhanced speech also results\nin less word error rates than those obtained with MMSE-enhanced speech.\n"
      ],
      "doi": "10.21437/Interspeech.2021-922",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "agrawal21_interspeech": {
      "authors": [
        [
          "Vikas",
          "Agrawal"
        ],
        [
          "Shashi",
          "Kumar"
        ],
        [
          "Shakti P.",
          "Rath"
        ]
      ],
      "title": "Whisper Speech Enhancement Using Joint Variational Autoencoder for Improved Speech Recognition",
      "original": "0953",
      "page_count": 5,
      "order": 554,
      "p1": "2706",
      "pn": "2710",
      "abstract": [
        "Whispering is the natural choice of communication when one wants to\ninteract quietly and privately. Due to vast differences in acoustic\ncharacteristics of whisper and natural speech, there is drastic degradation\nin the performance of whisper speech when decoded by the Automatic\nSpeech Recognition (ASR) system trained on neutral speech. Recently,\nto handle this mismatched train and test scenario Denoising Autoencoders\n(DA) are used which gives some improvement. To improve over DA performance\nwe propose another method to map speech from whisper domain to neutral\nspeech domain via Joint Variational Auto-Encoder (JVAE). The proposed\nmethod requires time-aligned parallel data which is not available,\nso we developed an algorithm to convert parallel data to time-aligned\nparallel data. JVAE jointly learns the characteristics of whisper and\nneutral speech in a common latent space which significantly improves\nwhisper recognition accuracy and outperforms traditional autoencoder\nbased techniques. We benchmarked our method against two baselines,\nfirst being ASR trained on neutral speech and tested on whisper dataset\nand second being whisper test set mapped using DA and tested on same\nneutral ASR. We achieved an absolute improvement of 22.31% in Word\nError Rate (WER) over the first baseline and an absolute 5.52% improvement\nover DA.\n"
      ],
      "doi": "10.21437/Interspeech.2021-953",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "lee21d_interspeech": {
      "authors": [
        [
          "Lukas",
          "Lee"
        ],
        [
          "Youna",
          "Ji"
        ],
        [
          "Minjae",
          "Lee"
        ],
        [
          "Min-Seok",
          "Choi"
        ]
      ],
      "title": "DEMUCS-Mobile : On-Device Lightweight Speech Enhancement",
      "original": "1025",
      "page_count": 5,
      "order": 555,
      "p1": "2711",
      "pn": "2715",
      "abstract": [
        "As the importance of speech enhancement for real-world application\nincreases, the compactness of the model is also becoming a crucial\nstudy. In this paper, we present compression techniques to reduce the\nmodel size and applied them to the state-of-the-art real-time speech\nenhancement system. We successfully reduce the model size by actively\napplying channel pruning while maintaining performance. In particular,\nwe propose a method to prune more channels of convolutional neural\nnetworks (CNN) by utilizing gated linear unit (GLU) activation. In\naddition, lower-bit-quantization is applied to reduce model size, while\nminimizing performance degradation caused by quantization. We show\nthe performance of our proposed model on a mobile device where computing\nresources are limited. In particular, it is implemented to enable streaming,\nand speech enhancement works in real-time.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1025",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "kashyap21_interspeech": {
      "authors": [
        [
          "Madhav Mahesh",
          "Kashyap"
        ],
        [
          "Anuj",
          "Tambwekar"
        ],
        [
          "Krishnamoorthy",
          "Manohara"
        ],
        [
          "S.",
          "Natarajan"
        ]
      ],
      "title": "Speech Denoising Without Clean Training Data: A Noise2Noise Approach",
      "original": "1130",
      "page_count": 5,
      "order": 556,
      "p1": "2716",
      "pn": "2720",
      "abstract": [
        "This paper tackles the problem of the heavy dependence of clean speech\ndata required by deep learning based audio-denoising methods by showing\nthat it is possible to train deep speech denoising networks using only\nnoisy speech samples. Conventional wisdom dictates that in order to\nachieve good speech denoising performance, there is a requirement for\na large quantity of both noisy speech samples and perfectly clean speech\nsamples, resulting in a need for expensive audio recording equipment\nand extremely controlled soundproof recording studios. These requirements\npose significant challenges in data collection, especially in economically\ndisadvantaged regions and for low resource languages. This work shows\nthat speech denoising deep neural networks can be successfully trained\nutilizing only noisy training audio. Furthermore it is revealed that\nsuch training regimes achieve superior denoising performance over conventional\ntraining regimes utilizing clean training audio targets, in cases involving\ncomplex noise distributions and low Signal-to-Noise ratios (high noise\nenvironments). This is demonstrated through experiments studying the\nefficacy of our proposed approach over both real-world noises and synthetic\nnoises using the 20 layered Deep Complex U-Net architecture.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1130",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "dang21_interspeech": {
      "authors": [
        [
          "Feng",
          "Dang"
        ],
        [
          "Pengyuan",
          "Zhang"
        ],
        [
          "Hangting",
          "Chen"
        ]
      ],
      "title": "Improved Speech Enhancement Using a Complex-Domain GAN with Fused Time-Domain and Time-Frequency Domain Constraints",
      "original": "1134",
      "page_count": 5,
      "order": 557,
      "p1": "2721",
      "pn": "2725",
      "abstract": [
        "Complex-domain models have achieved promising results for speech enhancement\n(SE) tasks. Some complex-domain models consider only time-frequency\n(T-F) domain constraints and do not take advantage of the information\nat the time-domain waveform level. Some complex-domain models consider\nonly time-domain constraints and do not take into account T-F domain\nconstraints that have rich harmonic structure information. Indeed some\ncomplex-domain models consider both time-domain and T-F domain constraints\nbut only use the simple mean square loss as time-frequency-domain constraints.\nThis paper proposes a complex-domain-based speech enhancement method\nthat integrates time-domain constraints and T-F domain constraints\ninto a unified framework using a Generative Adversarial Network (GAN).\nThe proposed framework captures information at the time-domain waveform\nlevel features while paying attention to the harmonic structure by\ntime-domain and T-F domain constraints. We conducted experiments on\nthe Voice Bank + DEMAND dataset to evaluate the proposed method. Experimental\nresults show that the proposed method improves the PESQ score by 0.09\nand the STOI score by 1% over the strong baseline deep complex convolution\nrecurrent network (DCCRN) and outperforms the state-of-the-art GAN-based\nSE systems.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1134",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "zhang21r_interspeech": {
      "authors": [
        [
          "Xudong",
          "Zhang"
        ],
        [
          "Liang",
          "Zhao"
        ],
        [
          "Feng",
          "Gu"
        ]
      ],
      "title": "Speech Enhancement with Topology-Enhanced Generative Adversarial Networks (GANs)",
      "original": "1411",
      "page_count": 5,
      "order": 558,
      "p1": "2726",
      "pn": "2730",
      "abstract": [
        "Speech enhancement is one of the effective approaches in improving\nspeech quality. Neural network models have been widely used in speech\nenhancement, such as recurrent neural networks (RNNs), long short-term\nmemory networks (LSTMs), and generative adversarial networks (GANs).\nHowever, some of them either handle the speech noise removal tasks\nin the spectral domain or lack the waveform recovery capability. As\na result, the enhanced speeches still include noisy signals. In this\nstudy, we propose a topology-enhanced GAN model to tackle noisy speeches\nin an end-to-end structure. We use the topology features of speech\nwaves as additional constraints and modify the objective function of\nthe GAN by adding a penalty term. The penalty term is a Wasserstein\ndistance of topology features measuring the difference between the\ngenerated speech and the corresponding clean speech. We evaluate the\nproposed speech-enhanced model on the public speech data set with 56\nspeakers and 20 different types of noisy conditions. The experimental\nresults indicate that the topology features improve the performance\nof GANs on speech enhancement in metrics of PESQ, CBAK, COVL, and SSNR.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1411",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "bu21_interspeech": {
      "authors": [
        [
          "Suliang",
          "Bu"
        ],
        [
          "Yunxin",
          "Zhao"
        ],
        [
          "Shaojun",
          "Wang"
        ],
        [
          "Mei",
          "Han"
        ]
      ],
      "title": "Learning Speech Structure to Improve Time-Frequency Masks",
      "original": "1859",
      "page_count": 5,
      "order": 559,
      "p1": "2731",
      "pn": "2735",
      "abstract": [
        "Time-frequency (TF) masks are widely used in speech enhancement (SE).\nHowever, accurately estimating TF masks from noisy speech remains a\nchallenge to both statistical or neural network approaches. Statistical\nmodel-based mask estimation usually depends on a good parameter initialization,\nwhile NN-based mask estimation relies on setting proper and stable\nlearning targets. To address these issues, we propose a novel approach\nto extracting TF speech structures from clean speech data, and partition\na noisy speech spectrogram into mutually exclusive regions of core\nspeech, core noise, and transition. Using such region targets derived\nfrom clean speech, we train bidirectional LSTM to learn region prediction\nfrom noisy speech, which is easier to do than mask prediction. The\npredicted regions can further be used in place of masks in beamforming,\nor integrated with statistical and NN based mask estimation to constrain\nmask values and model parameter updates. Our experimental results on\nASR (CHiME-3) and SE (CHiME-3 and LibriSpeech) have demonstrated the\neffectiveness of our approach of learning speech region structure to\nimprove TF masks.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1859",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "kim21h_interspeech": {
      "authors": [
        [
          "Eesung",
          "Kim"
        ],
        [
          "Hyeji",
          "Seo"
        ]
      ],
      "title": "SE-Conformer: Time-Domain Speech Enhancement Using Conformer",
      "original": "2207",
      "page_count": 5,
      "order": 560,
      "p1": "2736",
      "pn": "2740",
      "abstract": [
        "Convolution-augmented transformer (conformer) has recently shown competitive\nresults in speech-domain applications, such as automatic speech recognition,\ncontinuous speech separation, and sound event detection. Conformer\ncan capture both the short and long-term temporal sequence information\nby attending to the whole sequence at once with multi-head self-attention\nand convolutional neural network. However, the effectiveness of conformer\nin speech enhancement has not been demonstrated. In this paper, we\npropose an end-to-end speech enhancement architecture (SE-Conformer),\nincorporating a convolutional encoder&#8211;decoder and conformer,\ndesigned to be directly applied to the time-domain signal. We performed\nevaluations on both the VoiceBank-DEMAND Corpus (VCTK) and Librispeech\ndatasets in terms of objective speech quality metrics. The experimental\nresults show that the proposed model outperforms other competitive\nbaselines in speech enhancement performance.\n"
      ],
      "doi": "10.21437/Interspeech.2021-2207",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "kongthaworn21_interspeech": {
      "authors": [
        [
          "Thananchai",
          "Kongthaworn"
        ],
        [
          "Burin",
          "Naowarat"
        ],
        [
          "Ekapol",
          "Chuangsuwanich"
        ]
      ],
      "title": "Spectral and Latent Speech Representation Distortion for TTS Evaluation",
      "original": "2258",
      "page_count": 5,
      "order": 561,
      "p1": "2741",
      "pn": "2745",
      "abstract": [
        "One of the main problems in the development of text-to-speech (TTS)\nsystems is its reliance on subjective measures, typically the Mean\nOpinion Score (MOS). MOS requires a large number of people to reliably\nrate each utterance, making the development process slow and expensive.\nRecent research on speech quality assessment tends to focus on training\nmodels to estimate MOS, which requires a large number of training data,\nsomething that might not be available in low-resource languages. We\npropose an objective assessment metric based on the DTW distance using\nthe spectrogram and the high-level features from an Automatic Speech\nRecognition (ASR) model to cover both acoustic and linguistic information.\nExperiments on Thai TTS and the Blizzard Challenge datasets show that\nour method outperformed other baselines in both utterance- and system-level\nby a large margin in terms of correlation coefficients. Our metric\nalso outperformed the best baseline by 9.58% when used in head-to-head\nutterance-level comparisons. Ablation studies suggest that the middle\nlayers of the ASR model are most suitable for TTS evaluation when used\nin conjunction with spectral features.\n"
      ],
      "doi": "10.21437/Interspeech.2021-2258",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "valentinibotinhao21_interspeech": {
      "authors": [
        [
          "Cassia",
          "Valentini-Botinhao"
        ],
        [
          "Simon",
          "King"
        ]
      ],
      "title": "Detection and Analysis of Attention Errors in Sequence-to-Sequence Text-to-Speech",
      "original": "0286",
      "page_count": 5,
      "order": 562,
      "p1": "2746",
      "pn": "2750",
      "abstract": [
        "Sequence-to-sequence speech synthesis models are notorious for gross\nerrors such as skipping and repetition, commonly associated with failures\nin the attention mechanism. While a lot has been done to improve attention\nand decrease errors, this paper focuses instead on automatic error\ndetection and analysis. We evaluated three objective metrics against\nerror detection scores collected by human listening. All metrics were\nderived from the synthesised attention matrix alone and do not require\na reference signal, relying on the expectation that errors occur when\nattention is dispersed or insufficient. Using one of this metrics as\nan analysis tool, we observed that gross errors are more likely to\noccur in longer sentences and in sentences with punctuation marks that\nindicate pause or break. We also found that mechanisms such as forcibly\nincremented attention have the potential for decreasing gross errors\nbut to the detriment of naturalness. The results of the error detection\nevaluation revealed that two of the evaluated metrics were able to\ndetect errors with a relatively high success rate, obtaining F-scores\nof up to 0.89 and 0.96.\n"
      ],
      "doi": "10.21437/Interspeech.2021-286",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "zandie21_interspeech": {
      "authors": [
        [
          "Rohola",
          "Zandie"
        ],
        [
          "Mohammad H.",
          "Mahoor"
        ],
        [
          "Julia",
          "Madsen"
        ],
        [
          "Eshrat S.",
          "Emamian"
        ]
      ],
      "title": "RyanSpeech: A Corpus for Conversational Text-to-Speech Synthesis",
      "original": "0341",
      "page_count": 5,
      "order": 563,
      "p1": "2751",
      "pn": "2755",
      "abstract": [
        "This paper introduces <i>RyanSpeech</i>, a new speech corpus for research\non automated text-to-speech (TTS) systems. Publicly available TTS corpora\nare often noisy, recorded with multiple speakers, or lack quality male\nspeech data. In order to meet the need for a high quality, publicly\navailable male speech corpus within the field of speech recognition,\nwe have designed and created <i>RyanSpeech</i> which contains textual\nmaterials from real-world conversational settings. These materials\ncontain over 10 hours of a professional male voice actor&#8217;s speech\nrecorded at 44.1 kHz. This corpus&#8217;s design and pipeline make\n<i>RyanSpeech</i> ideal for developing TTS systems in real-world applications.\nTo provide a baseline for future research, protocols, and benchmarks,\nwe trained 4 state-of-the-art speech models and a vocoder on <i>RyanSpeech</i>.\nThe results show 3.36 in mean opinion scores (MOS) in our best model.\nWe have made both the corpus and trained models for public use.\n"
      ],
      "doi": "10.21437/Interspeech.2021-341",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "shi21c_interspeech": {
      "authors": [
        [
          "Yao",
          "Shi"
        ],
        [
          "Hui",
          "Bu"
        ],
        [
          "Xin",
          "Xu"
        ],
        [
          "Shaoji",
          "Zhang"
        ],
        [
          "Ming",
          "Li"
        ]
      ],
      "title": "AISHELL-3: A Multi-Speaker Mandarin TTS Corpus",
      "original": "0755",
      "page_count": 5,
      "order": 564,
      "p1": "2756",
      "pn": "2760",
      "abstract": [
        "In this paper, we present AISHELL-3, a large-scale multi-speaker Mandarin\nspeech corpus which could be used to train multi-speaker Text-To-Speech\n(TTS) systems. The corpus contains roughly 85 hours of emotion-neutral\nrecordings spanning across 218 native Chinese mandarin speakers. Their\nauxiliary attributes such as gender, age group and native accents are\nexplicitly marked and provided in the corpus. Moreover, transcripts\nin Chinese character-level and pinyin-level are provided along with\nthe recordings. We also present some data processing strategies and\ntechniques which match with the characteristics of the presented corpus\nand conduct experiments on multiple speech-synthesis systems to assess\nthe quality of the generated speech samples, showing promising results.\nThe corpus is available online under Apache v2.0 license.\n"
      ],
      "doi": "10.21437/Interspeech.2021-755",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "eng21_interspeech": {
      "authors": [
        [
          "Nicholas",
          "Eng"
        ],
        [
          "C.T. Justine",
          "Hui"
        ],
        [
          "Yusuke",
          "Hioka"
        ],
        [
          "Catherine I.",
          "Watson"
        ]
      ],
      "title": "Comparing Speech Enhancement Techniques for Voice Adaptation-Based Speech Synthesis",
      "original": "0800",
      "page_count": 5,
      "order": 565,
      "p1": "2761",
      "pn": "2765",
      "abstract": [
        "This study investigates the use of speech enhancement techniques in\ncreating text-to-speech voices with degraded or noisy speech. A number\nof synthetic voices were created using speech that was first degraded\nby different noise types at various signal-to-noise ratios (SNRs),\nthen enhanced through four speech enhancement algorithms: Subspace,\nWiener filter, SEGAN and a DNN-based method. Subjective listening tests\nshow that the quality of the synthetic voices produced by subspace\nand the DNN-based method enhanced speech outperforms the quality of\nthe voices created using Wiener filter or SEGAN enhanced speech at\nlow SNRs, and speech enhanced by the subspace method results in higher\nquality synthetic speech at higher SNRs.\n"
      ],
      "doi": "10.21437/Interspeech.2021-800",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "cui21c_interspeech": {
      "authors": [
        [
          "Chenye",
          "Cui"
        ],
        [
          "Yi",
          "Ren"
        ],
        [
          "Jinglin",
          "Liu"
        ],
        [
          "Feiyang",
          "Chen"
        ],
        [
          "Rongjie",
          "Huang"
        ],
        [
          "Ming",
          "Lei"
        ],
        [
          "Zhou",
          "Zhao"
        ]
      ],
      "title": "EMOVIE: A Mandarin Emotion Speech Dataset with a Simple Emotional Text-to-Speech Model",
      "original": "1148",
      "page_count": 5,
      "order": 566,
      "p1": "2766",
      "pn": "2770",
      "abstract": [
        "Recently, there has been an increasing interest in neural speech synthesis.\nWhile the deep neural network achieves the state-of-the-art result\nin text-to-speech (TTS) tasks, how to generate a more emotional and\nmore expressive speech is becoming a new challenge to researchers due\nto the scarcity of high-quality emotion speech dataset and the lack\nof advanced emotional TTS model. In this paper, we first briefly introduce\nand publicly release a Mandarin emotion speech dataset including 9,724\nsamples with audio files and its emotion human-labeled annotation.\nAfter that, we propose a simple but efficient architecture for emotional\nspeech synthesis called EMSpeech. Unlike those models which need additional\nreference audio as input, our model could predict emotion labels just\nfrom the input text and generate more expressive speech conditioned\non the emotion embedding. In the experiment phase, we first validate\nthe effectiveness of our dataset by an emotion classification task.\nThen we train our model on the proposed dataset and conduct a series\nof subjective evaluations. Finally, by showing a comparable performance\nin the emotional speech synthesis task, we successfully demonstrate\nthe ability of the proposed model.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1148",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "rallabandi21_interspeech": {
      "authors": [
        [
          "Sai Sirisha",
          "Rallabandi"
        ],
        [
          "Abhinav",
          "Bharadwaj"
        ],
        [
          "Babak",
          "Naderi"
        ],
        [
          "Sebastian",
          "M\u00f6ller"
        ]
      ],
      "title": "Perception of Social Speaker Characteristics in Synthetic Speech",
      "original": "1229",
      "page_count": 5,
      "order": 567,
      "p1": "2771",
      "pn": "2775",
      "abstract": [
        "With the improved computational abilities, the usage of chatbots and\nconversational agents has become more prevalent. Therefore, it is essential\nthat these agents exhibit certain social speaker characteristics in\nthe generated speech. In this paper, we study the perception of such\nspeaker characteristics in two commercial Text-to-Speech (TTS) systems,\nAmazon Polly and Google TTS. We carried out a 15-item semantic differential\nscaling test. The factor analysis provided us with three underlying\ndimensions that can be perceived from synthetic speech, warmth, competence,\nand extraversion. Our results show that we can perceive both interpersonal\nrelationships and also personality traits from synthetic voices. Additionally,\nwe observed that the female participants perceived male voices to be\nmore responsible, energetic, relaxed, and enthusiastic. In comparison,\nmale participants found female voices to be more reliable, accessible,\nand confident. A discussion on the comparison of our results with that\nof the studies on natural speech is also provided.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1229",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "bakhturina21_interspeech": {
      "authors": [
        [
          "Evelina",
          "Bakhturina"
        ],
        [
          "Vitaly",
          "Lavrukhin"
        ],
        [
          "Boris",
          "Ginsburg"
        ],
        [
          "Yang",
          "Zhang"
        ]
      ],
      "title": "Hi-Fi Multi-Speaker English TTS Dataset",
      "original": "1599",
      "page_count": 5,
      "order": 568,
      "p1": "2776",
      "pn": "2780",
      "abstract": [
        "This paper introduces a new multi-speaker English dataset for training\ntext-to-speech models. The dataset is based on LibriVox audiobooks\nand Project Gutenberg texts, both in the public domain. The new dataset\ncontains about 292 hours of speech from 10 speakers with at least 17\nhours per speaker sampled at 44.1 kHz. To select speech samples with\nhigh quality, we considered audio recordings with a signal bandwidth\nof at least 13 kHz and a signal-to-noise ratio (SNR) of at least 32\ndB. The dataset is publicly released.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1599",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "tseng21b_interspeech": {
      "authors": [
        [
          "Wei-Cheng",
          "Tseng"
        ],
        [
          "Chien-yu",
          "Huang"
        ],
        [
          "Wei-Tsung",
          "Kao"
        ],
        [
          "Yist Y.",
          "Lin"
        ],
        [
          "Hung-yi",
          "Lee"
        ]
      ],
      "title": "Utilizing Self-Supervised Representations for MOS Prediction",
      "original": "2013",
      "page_count": 5,
      "order": 569,
      "p1": "2781",
      "pn": "2785",
      "abstract": [
        "Speech quality assessment has been a critical issue in speech processing\nfor decades. Existing automatic evaluations usually require clean references\nor parallel ground truth data, which is infeasible when the amount\nof data soars. Subjective tests, on the other hand, do not need any\nadditional clean or parallel data and correlates better to human perception.\nHowever, such a test is expensive and time-consuming because crowd\nwork is necessary. It thus becomes highly desired to develop an automatic\nevaluation approach that correlates well with human perception while\nnot requiring ground truth data. In this paper, we use self-supervised\npre-trained models for MOS prediction. We show their representations\ncan distinguish between clean and noisy audios. Then, we fine-tune\nthese pre-trained models followed by simple linear layers in an end-to-end\nmanner. The experiment results showed that our framework outperforms\nthe two previous state-of-the-art models by a significant improvement\non Voice Conversion Challenge 2018 and achieves comparable or superior\nperformance on Voice Conversion Challenge 2016. We also conducted an\nablation study to further investigate how each module benefits the\ntask. The experiment results are implemented and reproducible with\npublicly available toolkits.\n"
      ],
      "doi": "10.21437/Interspeech.2021-2013",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "mussakhojayeva21_interspeech": {
      "authors": [
        [
          "Saida",
          "Mussakhojayeva"
        ],
        [
          "Aigerim",
          "Janaliyeva"
        ],
        [
          "Almas",
          "Mirzakhmetov"
        ],
        [
          "Yerbolat",
          "Khassanov"
        ],
        [
          "Huseyin Atakan",
          "Varol"
        ]
      ],
      "title": "KazakhTTS: An Open-Source Kazakh Text-to-Speech Synthesis Dataset",
      "original": "2124",
      "page_count": 5,
      "order": 570,
      "p1": "2786",
      "pn": "2790",
      "abstract": [
        "This paper introduces a high-quality open-source speech synthesis dataset\nfor Kazakh, a low-resource language spoken by over 13 million people\nworldwide. The dataset consists of about 93 hours of transcribed audio\nrecordings spoken by two professional speakers (female and male). It\nis the first publicly available large-scale dataset developed to promote\nKazakh text-to-speech (TTS) applications in both academia and industry.\nIn this paper, we share our experience by describing the dataset development\nprocedures and faced challenges, and discuss important future directions.\nTo demonstrate the reliability of our dataset, we built baseline end-to-end\nTTS models and evaluated them using the subjective mean opinion score\n(MOS) measure. Evaluation results show that the best TTS models trained\non our dataset achieve MOS above 4 for both speakers, which makes them\napplicable for practical use. The dataset, training recipe, and pretrained\nTTS models are freely available.\n"
      ],
      "doi": "10.21437/Interspeech.2021-2124",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "taylor21_interspeech": {
      "authors": [
        [
          "Jason",
          "Taylor"
        ],
        [
          "Korin",
          "Richmond"
        ]
      ],
      "title": "Confidence Intervals for ASR-Based TTS Evaluation",
      "original": "2203",
      "page_count": 5,
      "order": 571,
      "p1": "2791",
      "pn": "2795",
      "abstract": [
        "Automatic speech recognition (ASR) is increasingly used to evaluate\nthe intelligibility of text-to-speech synthesis (TTS). ASR is less\ncostly than traditional listening tests, but questions remain about\nits reliability. We re-evaluate the Blizzard Challenge&#8217;s intelligibility\ntasks in English since 2011 using ASR. Re-analysing transcriptions\ncollected by paid in-lab participants, online volunteers and Amazon\nMechanical Turkers (the latter used only in 2011), we compare their\nword error rates (WERs) and statistically-significant system-groupings\nwith those generated by an open-source, Transformer-based ASR model.\nThis ASR model consistently decodes test stimuli with more reliable\nWERs than the Blizzard Challenge&#8217;s (mostly non-native) speech\nexperts and online volunteers. The model also groups systems according\nto statistical significance similarly to the paid in-lab participants.\nUsing surplus semantically unpredictable sentences (SUS) submitted\nevery year to the challenge, we investigate how confidence intervals\nin ASR WERs change as the number of transcribed stimuli increases.\nWe plot the Frobenius norm of pairwise significance matrices with increasing\nstimuli. We find that finer groupings of systems are detected as confidence\nintervals narrow. The number of stimuli where p-values start to converge\nranges from 400&#8211;800 stimuli. We conclude that, with enough stimuli,\nASR can be more reliable than humans.\n"
      ],
      "doi": "10.21437/Interspeech.2021-2203"
    },
    "reddy21_interspeech": {
      "authors": [
        [
          "Chandan K.A.",
          "Reddy"
        ],
        [
          "Harishchandra",
          "Dubey"
        ],
        [
          "Kazuhito",
          "Koishida"
        ],
        [
          "Arun",
          "Nair"
        ],
        [
          "Vishak",
          "Gopal"
        ],
        [
          "Ross",
          "Cutler"
        ],
        [
          "Sebastian",
          "Braun"
        ],
        [
          "Hannes",
          "Gamper"
        ],
        [
          "Robert",
          "Aichner"
        ],
        [
          "Sriram",
          "Srinivasan"
        ]
      ],
      "title": "INTERSPEECH 2021 Deep Noise Suppression Challenge",
      "original": "1609",
      "page_count": 5,
      "order": 572,
      "p1": "2796",
      "pn": "2800",
      "abstract": [
        "The Deep Noise Suppression (DNS) challenge was designed to unify the\nresearch efforts in the area of noise suppression targeted for human\nperception. We recently organized a DNS challenge special session at\nINTERSPEECH 2020 and ICASSP 2021. We open-sourced training and test\ndatasets for the wideband scenario along with a subjective evaluation\nframework based on ITU-T standard P.808, which was used to evaluate\nparticipants of the challenge. Many researchers from academia and industry\nmade significant contributions to push the field forward, yet even\nthe best noise suppressor was far from achieving superior speech quality\nin challenging scenarios. In this version of the challenge organized\nat INTERSPEECH 2021, we expanded our training and test datasets to\naccommodate fullband scenarios and challenging test conditions. We\nused ITU-T P.835 to evaluate the challenge winners as it gives additional\ninformation about the quality of processed speech and residual noise.\nThe two tracks in this challenge focused on real-time denoising for\n(i) wideband, and (ii) fullband scenarios. We also made available a\nreliable non-intrusive objective speech quality metric for wideband\ncalled DNSMOS for the participants to use during their development\nphase.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1609",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "li21g_interspeech": {
      "authors": [
        [
          "Andong",
          "Li"
        ],
        [
          "Wenzhe",
          "Liu"
        ],
        [
          "Xiaoxue",
          "Luo"
        ],
        [
          "Guochen",
          "Yu"
        ],
        [
          "Chengshi",
          "Zheng"
        ],
        [
          "Xiaodong",
          "Li"
        ]
      ],
      "title": "A Simultaneous Denoising and Dereverberation Framework with Target Decoupling",
      "original": "1137",
      "page_count": 5,
      "order": 573,
      "p1": "2801",
      "pn": "2805",
      "abstract": [
        "Background noise and room reverberation are regarded as two major factors\nto degrade the subjective speech quality. In this paper, we propose\nan integrated framework to address simultaneous denoising and dereverberation\nunder complicated scenario environments. It adopts a chain optimization\nstrategy and designs four sub-stages accordingly. In the first two\nstages, we decouple the multi-task learning w.r.t. complex spectrum\ninto magnitude and phase, and only implement noise and reverberation\nremoval in the magnitude domain. Based on the estimated priors above,\nwe further polish the spectrum in the third stage, where both magnitude\nand phase information are explicitly repaired with the residual learning.\nDue to the data mismatch and nonlinear effect of DNNs, the residual\nnoise often exists in the DNN-processed spectrum. To resolve the problem,\nwe adopt a light-weight algorithm as the post-processing module to\ncapture and suppress the residual noise in the non-active regions.\nIn the Interspeech 2021 Deep Noise Suppression (DNS) Challenge, our\nsubmitted system ranked top-1 for the real-time track in terms of Mean\nOpinion Score (MOS) with ITU-T P.835 framework.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1137",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement:14 Special Sessions"
    },
    "xu21h_interspeech": {
      "authors": [
        [
          "Ziyi",
          "Xu"
        ],
        [
          "Maximilian",
          "Strake"
        ],
        [
          "Tim",
          "Fingscheidt"
        ]
      ],
      "title": "Deep Noise Suppression with Non-Intrusive PESQNet Supervision Enabling the Use of Real Training Data",
      "original": "0936",
      "page_count": 5,
      "order": 574,
      "p1": "2806",
      "pn": "2810",
      "abstract": [
        "Data-driven speech enhancement employing deep neural networks (DNNs)\ncan provide state-of-the-art performance even in the presence of non-stationary\nnoise. During the training process, most of the speech enhancement\nneural networks are trained in a fully supervised way with losses requiring\nnoisy speech to be synthesized by clean speech and additive noise.\nHowever, in a real implementation, only the noisy speech mixture is\navailable, which leads to the question, how such data could be advantageously\nemployed in training. In this work, we propose an end-to-end non-intrusive\n PESQNet DNN which estimates perceptual evaluation of speech quality\n(PESQ) scores, allowing a reference-free loss for real data. As a further\nnovelty, we combine the  PESQNet loss with denoising and dereverberation\nloss terms, and train a complex mask-based fully convolutional recurrent\nneural network (FCRN) in a &#8220;weakly&#8221; supervised way, each\ntraining cycle employing some synthetic data, some real data, and again\nsynthetic data to keep the  PESQNet up-to-date. In a subjective listening\ntest, our proposed framework outperforms the Interspeech 2021 Deep\nNoise Suppression (DNS) Challenge baseline overall by 0.09 MOS points\nand in particular by 0.45 background noise MOS points.\n"
      ],
      "doi": "10.21437/Interspeech.2021-936",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement:14 Special Sessions"
    },
    "le21b_interspeech": {
      "authors": [
        [
          "Xiaohuai",
          "Le"
        ],
        [
          "Hongsheng",
          "Chen"
        ],
        [
          "Kai",
          "Chen"
        ],
        [
          "Jing",
          "Lu"
        ]
      ],
      "title": "DPCRN: Dual-Path Convolution Recurrent Network for Single Channel Speech Enhancement",
      "original": "0296",
      "page_count": 5,
      "order": 575,
      "p1": "2811",
      "pn": "2815",
      "abstract": [
        "The dual-path RNN (DPRNN) was proposed to more effectively model extremely\nlong sequences for speech separation in the time domain. By splitting\nlong sequences to smaller chunks and applying intra-chunk and inter-chunk\nRNNs, the DPRNN reached promising performance in speech separation\nwith a limited model size. In this paper, we combine the DPRNN module\nwith Convolution Recurrent Network (CRN) and design a model called\nDual-Path Convolution Recurrent Network (DPCRN) for speech enhancement\nin the time-frequency domain. We replace the RNNs in the CRN with DPRNN\nmodules, where the intra-chunk RNNs are used to model the spectrum\npattern in a single frame and the inter-chunk RNNs are used to model\nthe dependence between consecutive frames. With only 0.8M parameters,\nthe submitted DPCRN model achieves an overall mean opinion score (MOS)\nof 3.57 in the wide band scenario track of the Interspeech 2021 Deep\nNoise Suppression (DNS) challenge. Evaluations on some other test sets\nalso show the efficacy of our model.\n"
      ],
      "doi": "10.21437/Interspeech.2021-296",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement:14 Special Sessions"
    },
    "lv21_interspeech": {
      "authors": [
        [
          "Shubo",
          "Lv"
        ],
        [
          "Yanxin",
          "Hu"
        ],
        [
          "Shimin",
          "Zhang"
        ],
        [
          "Lei",
          "Xie"
        ]
      ],
      "title": "DCCRN+: Channel-Wise Subband DCCRN with SNR Estimation for Speech Enhancement",
      "original": "1482",
      "page_count": 5,
      "order": 576,
      "p1": "2816",
      "pn": "2820",
      "abstract": [
        "Deep complex convolution recurrent network (DCCRN), which extends CRN\nwith complex structure, has achieved superior performance in MOS evaluation\nin Interspeech 2020 deep noise suppression challenge (DNS2020). This\npaper further extends DCCRN with the following significant revisions.\nWe first extend the model to sub-band processing where the bands are\nsplit and merged by learnable neural network filters instead of engineered\nFIR filters, leading to a faster noise suppressor trained in an end-to-end\nmanner. Then the LSTM is further substituted with a complex TF-LSTM\nto better model temporal dependencies along both time and frequency\naxes. Moreover, instead of simply concatenating the output of each\nencoder layer to the input of the corresponding decoder layer, we use\nconvolution blocks to first aggregate essential information from the\nencoder output before feeding it to the decoder layers. We specifically\nformulate the decoder with an extra <i>a priori</i> SNR estimation\nmodule to maintain good speech quality while removing noise. Finally\na post-processing module is adopted to further suppress the unnatural\nresidual noise. The new model, named DCCRN+, has surpassed the original\nDCCRN as well as several competitive models in terms of PESQ and DNSMOS,\nand has achieved superior performance in the new Interspeech 2021 DNS\nchallenge.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1482",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement:14 Special Sessions"
    },
    "zhang21s_interspeech": {
      "authors": [
        [
          "Kanghao",
          "Zhang"
        ],
        [
          "Shulin",
          "He"
        ],
        [
          "Hao",
          "Li"
        ],
        [
          "Xueliang",
          "Zhang"
        ]
      ],
      "title": "DBNet: A Dual-Branch Network Architecture Processing on Spectrum and Waveform for Single-Channel Speech Enhancement",
      "original": "1042",
      "page_count": 5,
      "order": 577,
      "p1": "2821",
      "pn": "2825",
      "abstract": [
        "In real acoustic environment, speech enhancement is an arduous task\nto improve the quality and intelligibility of speech interfered by\nbackground noise and reverberation. Over the past years, deep learning\nhas shown great potential on speech enhancement. In this paper, we\npropose a novel real-time framework called DBNet which is a dual-branch\nstructure with alternate interconnection. Each branch incorporates\nan encoder-decoder architecture with skip connections. The two branches\nare responsible for spectrum and waveform modeling, respectively. A\nbridge layer is adopted to exchange information between the two branches.\nSystematic evaluation and comparison show that the proposed system\nsubstantially outperforms related algorithms under very challenging\nenvironments. And in INTERSPEECH 2021 Deep Noise Suppression (DNS)\nchallenge, the proposed system ranks the top 8 in real-time track 1\nin terms of the Mean Opinion Score (MOS) of the ITU-T P.835 framework.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1042",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement:14 Special Sessions"
    },
    "zhang21t_interspeech": {
      "authors": [
        [
          "Xu",
          "Zhang"
        ],
        [
          "Xinlei",
          "Ren"
        ],
        [
          "Xiguang",
          "Zheng"
        ],
        [
          "Lianwu",
          "Chen"
        ],
        [
          "Chen",
          "Zhang"
        ],
        [
          "Liang",
          "Guo"
        ],
        [
          "Bing",
          "Yu"
        ]
      ],
      "title": "Low-Delay Speech Enhancement Using Perceptually Motivated Target and Loss",
      "original": "1410",
      "page_count": 5,
      "order": 578,
      "p1": "2826",
      "pn": "2830",
      "abstract": [
        "Speech enhancement approaches based on deep neural network have outperformed\nthe traditional signal processing methods. This paper presents a low-delay\nspeech enhancement method that employs a new perceptually motivated\ntraining target and loss function. The proposed approach can achieve\nsimilar speech enhancement performance compared to the state-of-the-art\napproaches, but with significantly less latency and computational complexities.\nJudged by the MOS tests conducted by the INTERSPEECH 2021 Deep Noise\nSuppression Challenge organizer, the proposed method is ranked the\n2<SUP>nd</SUP> place for Background Noise MOS, and the 6<SUP>th</SUP>\nplace for overall MOS.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1410",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement:14 Special Sessions"
    },
    "oostermeijer21_interspeech": {
      "authors": [
        [
          "Koen",
          "Oostermeijer"
        ],
        [
          "Qing",
          "Wang"
        ],
        [
          "Jun",
          "Du"
        ]
      ],
      "title": "Lightweight Causal Transformer with Local Self-Attention for Real-Time Speech Enhancement",
      "original": "0668",
      "page_count": 5,
      "order": 579,
      "p1": "2831",
      "pn": "2835",
      "abstract": [
        "In this paper, we describe a novel speech enhancement transformer architecture.\nThe model uses local causal self-attention, which makes it lightweight\nand therefore particularly well-suited for real-time speech enhancement\nin computation resource-limited environments. In addition, we provide\nseveral ablation studies that focus on different parts of the model\nand the loss function to figure out which modifications yield best\nimprovements. Using this knowledge, we propose a final version of our\narchitecture, that we sent in to the INTERSPEECH 2021 DNS Challenge,\nwhere it achieved competitive results, despite using only 2% of the\nmaximally allowed computation. Furthermore, we performed experiments\nto compare it with with LSTM and CNN models, that had 127% and 257%\nmore parameters, respectively. Despite this difference in model size,\nwe achieved significant improvements on the considered speech quality\nand intelligibility measures.\n"
      ],
      "doi": "10.21437/Interspeech.2021-668",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "ristea21_interspeech": {
      "authors": [
        [
          "Nicolae-C\u0103t\u0103lin",
          "Ristea"
        ],
        [
          "Radu Tudor",
          "Ionescu"
        ]
      ],
      "title": "Self-Paced Ensemble Learning for Speech and Audio Classification",
      "original": "0155",
      "page_count": 5,
      "order": 580,
      "p1": "2836",
      "pn": "2840",
      "abstract": [
        "Combining multiple machine learning models into an ensemble is known\nto provide superior performance levels compared to the individual components\nforming the ensemble. This is because models can complement each other\nin taking better decisions. Instead of just combining the models, we\npropose a self-paced ensemble learning scheme in which models learn\nfrom each other over several iterations. During the self-paced learning\nprocess based on pseudo-labeling, in addition to improving the individual\nmodels, our ensemble also gains knowledge about the target domain.\nTo demonstrate the generality of our self-paced ensemble learning (SPEL)\nscheme, we conduct experiments on three audio tasks. Our empirical\nresults indicate that SPEL significantly outperforms the baseline ensemble\nmodels. We also show that applying self-paced learning on individual\nmodels is less effective, illustrating the idea that models in the\nensemble actually learn from each other.\n"
      ],
      "doi": "10.21437/Interspeech.2021-155",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "kojima21_interspeech": {
      "authors": [
        [
          "Atsushi",
          "Kojima"
        ]
      ],
      "title": "Knowledge Distillation for Streaming Transformer&#8211;Transducer",
      "original": "0175",
      "page_count": 5,
      "order": 581,
      "p1": "2841",
      "pn": "2845",
      "abstract": [
        "We explore knowledge distillation methods from nonstreaming to streaming\nTransformer&#8211;Transducer (T&#8211;T) models. Streaming T&#8211;T\ntruncates future context. It leads to recognition quality degradation\ncompared with the original T&#8211;T. In this work, we explore knowledge\ndistillation, which minimizes internal representations in all Transformer\nlayers between nonstreaming and streaming T&#8211;T models. In the\nexperiment, we compared two different methods: the minimization of\nthe L2 distance of hidden vectors and the minimization of the L2 distance\nof heads. All experiments were conducted using the public LibriSpeech\ncorpus. Results of the experiment showed that hidden vector similarity-based\nknowledge distillation is better than multi-head similarity-based knowledge\ndistillation. We observed 3.5% and 2.1% relative reductions in word\nerror rate compared with the original streaming T&#8211;T in test-clean\nset and test-other set, respectively.\n"
      ],
      "doi": "10.21437/Interspeech.2021-175",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "lohrenz21_interspeech": {
      "authors": [
        [
          "Timo",
          "Lohrenz"
        ],
        [
          "Zhengyang",
          "Li"
        ],
        [
          "Tim",
          "Fingscheidt"
        ]
      ],
      "title": "Multi-Encoder Learning and Stream Fusion for Transformer-Based End-to-End Automatic Speech Recognition",
      "original": "0555",
      "page_count": 5,
      "order": 582,
      "p1": "2846",
      "pn": "2850",
      "abstract": [
        "Stream fusion, also known as system combination, is a common technique\nin automatic speech recognition for traditional hybrid hidden Markov\nmodel approaches, yet mostly unexplored for modern deep neural network\nend-to-end model architectures. Here, we investigate various fusion\ntechniques for the all-attention-based encoder-decoder architecture\nknown as the transformer, striving to achieve optimal fusion by investigating\ndifferent fusion levels in an example single-microphone setting with\nfusion of standard magnitude and phase features. We introduce a novel\nmulti-encoder learning method that performs a weighted combination\nof two encoder-decoder multi-head attention outputs <i>only</i> during\ntraining. Employing then only the magnitude feature encoder in inference,\nwe are able to show consistent improvement on Wall Street Journal (WSJ)\nwith language model and on Librispeech, without increase in runtime\nor parameters. Combining two such multi-encoder trained models by a\nsimple late fusion in inference, we achieve state-of-the-art performance\nfor transformer-based models on WSJ with a significant WER reduction\nof 19% relative compared to the current benchmark approach.\n"
      ],
      "doi": "10.21437/Interspeech.2021-555",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "zaiem21_interspeech": {
      "authors": [
        [
          "Salah",
          "Zaiem"
        ],
        [
          "Titouan",
          "Parcollet"
        ],
        [
          "Slim",
          "Essid"
        ]
      ],
      "title": "Conditional Independence for Pretext Task Selection in Self-Supervised Speech Representation Learning",
      "original": "1027",
      "page_count": 5,
      "order": 583,
      "p1": "2851",
      "pn": "2855",
      "abstract": [
        "Through solving pretext tasks, self-supervised learning (SSL) leverages\nunlabeled data to extract useful latent representations replacing traditional\ninput features in the downstream task. A common pretext task consists\nin pretraining a SSL model on pseudo-labels derived from the original\nsignal. This technique is particularly relevant for speech data where\nvarious meaningful signal processing features may serve as pseudo-labels.\nHowever, the process of selecting pseudo-labels, for speech or other\ntypes of data, remains mostly unexplored and currently relies on observing\nthe results on the final downstream task. Nevertheless, this methodology\nis not sustainable at scale due to substantial computational (hence\ncarbon) costs. Thus, this paper introduces a practical and theoretical\nframework to select relevant pseudo-labels with respect to a given\ndownstream task. More precisely, we propose a functional estimator\nof the pseudo-label utility grounded in the conditional independence\ntheory, which does not require any training. The experiments conducted\non speaker recognition and automatic speech recognition validate our\nestimator, showing a significant correlation between the performance\nobserved on the downstream task and the utility estimates obtained\nwith our approach, facilitating the prospection of relevant pseudo-labels\nfor self-supervised speech representation learning.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1027",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "zeineldeen21_interspeech": {
      "authors": [
        [
          "Mohammad",
          "Zeineldeen"
        ],
        [
          "Aleksandr",
          "Glushko"
        ],
        [
          "Wilfried",
          "Michel"
        ],
        [
          "Albert",
          "Zeyer"
        ],
        [
          "Ralf",
          "Schl\u00fcter"
        ],
        [
          "Hermann",
          "Ney"
        ]
      ],
      "title": "Investigating Methods to Improve Language Model Integration for Attention-Based Encoder-Decoder ASR Models",
      "original": "1255",
      "page_count": 5,
      "order": 584,
      "p1": "2856",
      "pn": "2860",
      "abstract": [
        "Attention-based encoder-decoder (AED) models learn an implicit internal\nlanguage model (ILM) from the training transcriptions. The integration\nwith an external LM trained on much more unpaired text usually leads\nto better performance. A Bayesian interpretation as in the hybrid autoregressive\ntransducer (HAT) suggests dividing by the prior of the discriminative\nacoustic model, which corresponds to this implicit LM, similarly as\nin the hybrid hidden Markov model approach. The implicit LM cannot\nbe calculated efficiently in general and it is yet unclear what are\nthe best methods to estimate it. In this work, we compare different\napproaches from the literature and propose several novel methods to\nestimate the ILM directly from the AED model. Our proposed methods\noutperform all previous approaches. We also investigate other methods\nto suppress the ILM mainly by decreasing the capacity of the AED model,\nlimiting the label context, and also by training the AED model together\nwith a pre-existing LM.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1255",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "vyas21b_interspeech": {
      "authors": [
        [
          "Apoorv",
          "Vyas"
        ],
        [
          "Srikanth",
          "Madikeri"
        ],
        [
          "Herv\u00e9",
          "Bourlard"
        ]
      ],
      "title": "Comparing CTC and LFMMI for Out-of-Domain Adaptation of wav2vec 2.0 Acoustic Model",
      "original": "1683",
      "page_count": 5,
      "order": 585,
      "p1": "2861",
      "pn": "2865",
      "abstract": [
        "In this work, we investigate if the wav2vec 2.0 self-supervised pretraining\nhelps mitigate the overfitting issues with connectionist temporal classification\n(CTC) training to reduce its performance gap with flat-start lattice-free\nMMI (E2E-LFMMI) for automatic speech recognition with limited training\ndata. Towards that objective, we use the pretrained wav2vec 2.0 BASE\nmodel and fine-tune it on three different datasets including out-of-domain\n(Switchboard) and cross-lingual (Babel) scenarios. Our results show\nthat for supervised adaptation of the wav2vec 2.0 model, both E2E-LFMMI\nand CTC achieve similar results; significantly outperforming the baselines\ntrained only with supervised data. Fine-tuning the wav2vec 2.0 model\nwith E2E-LFMMI and CTC we obtain the following relative WER improvements\nover the supervised baseline trained with E2E-LFMMI. We get relative\nimprovements of 40% and 44% on the clean-set and 64% and 58% on the\ntest set of Librispeech (100h) respectively. On Switchboard (300h)\nwe obtain relative improvements of 33% and 35% respectively. Finally,\nfor Babel languages, we obtain relative improvements of 26% and 23%\non Swahili (38h) and 18% and 17% on Tagalog (84h) respectively.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1683",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "moine21_interspeech": {
      "authors": [
        [
          "Cl\u00e9ment Le",
          "Moine"
        ],
        [
          "Nicolas",
          "Obin"
        ],
        [
          "Axel",
          "Roebel"
        ]
      ],
      "title": "Speaker Attentive Speech Emotion Recognition",
      "original": "0573",
      "page_count": 5,
      "order": 586,
      "p1": "2866",
      "pn": "2870",
      "abstract": [
        "Speech Emotion Recognition (SER) task has known significant improvements\nover the last years with the advent of Deep Neural Networks (DNNs).\nHowever, even the most successful methods are still rather failing\nwhen adaptation to specific speakers and scenarios is needed, inevitably\nleading to poorer performances when compared to humans. In this paper,\nwe present novel work based on the idea of teaching the emotion recognition\nnetwork about speaker identity. Our system is a combination of two\nACRNN classifiers respectively dedicated to speaker and emotion recognition.\nThe first informs the latter through a Self Speaker Attention (SSA)\nmechanism that is shown to considerably help to focus on emotional\ninformation of the speech signal. Speaker-dependant experiments on\nsocial attitudes database Att-HACK and IEMOCAP corpus demonstrate the\neffectiveness of the proposed method and achieve the state-of-the-art\nperformance in terms of unweighted average recall.\n"
      ],
      "doi": "10.21437/Interspeech.2021-573",
      "author_area_id": "3",
      "author_area_label": "Paralinguistics in Speech and Language: Human and Automatic Analysis and Processing"
    },
    "leem21_interspeech": {
      "authors": [
        [
          "Seong-Gyun",
          "Leem"
        ],
        [
          "Daniel",
          "Fulford"
        ],
        [
          "Jukka-Pekka",
          "Onnela"
        ],
        [
          "David",
          "Gard"
        ],
        [
          "Carlos",
          "Busso"
        ]
      ],
      "title": "Separation of Emotional and Reconstruction Embeddings on Ladder Network to Improve Speech Emotion Recognition Robustness in Noisy Conditions",
      "original": "1438",
      "page_count": 5,
      "order": 587,
      "p1": "2871",
      "pn": "2875",
      "abstract": [
        "When <i>speech emotion recognition</i> (SER) is applied in an actual\napplication, the system should be able to cope with audio acquired\nin a noisy, unconstrained environment. Most studies on noise-robust\nSER require a parallel dataset with emotion labels, which is impractical\nto collect, or use speech with artificially added noise, which does\nnot resemble practical conditions. This study builds upon the ladder\nnetwork formulation, which can effectively compensate the environmental\ndifferences between a clean speech corpus and real-life recordings.\nThis study proposes a decoupled ladder network, which increases the\nrobustness of the SER system against the influences of non-stationary\nbackground noise by decoupling the last hidden layer embedding into\nemotion and reconstruction embeddings. This novel implementation allows\nthe emotion embedding to focus exclusively on building a discriminative\nrepresentation, without worrying about the reconstruction task. We\nintroduce a noisy version of the MSP-Podcast database, which contains\naudio segments collected with a smartphone that simultaneously records\nsentences from the corpus and non-stationary noise at different <i>signal-to-noise\nratios</i> (SNRs). We test the effectiveness of our proposed model\nwith this corpus, showing that the decoupled ladder network can increase\nthe performance of the regular ladder network when dealing with noisy\nrecordings.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1438",
      "author_area_id": "3",
      "author_area_label": "Paralinguistics in Speech and Language: Human and Automatic Analysis and Processing"
    },
    "georgiou21_interspeech": {
      "authors": [
        [
          "Efthymios",
          "Georgiou"
        ],
        [
          "Georgios",
          "Paraskevopoulos"
        ],
        [
          "Alexandros",
          "Potamianos"
        ]
      ],
      "title": "M<SUP>3</SUP>: MultiModal Masking Applied to Sentiment Analysis",
      "original": "1739",
      "page_count": 5,
      "order": 588,
      "p1": "2876",
      "pn": "2880",
      "abstract": [
        "A common issue when training multimodal architectures is that not all\nmodalities contribute equally to the model&#8217;s prediction and the\nnetwork tends to over-rely on the strongest modality. In this work,\nwe present M<SUP>3</SUP>, a training procedure based on modality masking\nfor deep multimodal architectures. During network training, we randomly\nselect one modality and mask its features, forcing the model to make\nits prediction in the absence of this modality. This structured regularization\nallows the network to better exploit complementary information in input\nmodalities. We implement M<SUP>3</SUP> as a generic layer that can\nbe integrated with any multimodal architecture. Our experiments show\nthat M<SUP>3</SUP> outperforms other masking schemes and improves performance\nfor our strong baseline. We evaluate M<SUP>3</SUP> for multimodal sentiment\nanalysis on CMU-MOSEI, achieving results comparable to the state-of-the-art.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1739"
    },
    "klejch21_interspeech": {
      "authors": [
        [
          "Ond\u0159ej",
          "Klejch"
        ],
        [
          "Electra",
          "Wallington"
        ],
        [
          "Peter",
          "Bell"
        ]
      ],
      "title": "The CSTR System for Multilingual and Code-Switching ASR Challenges for Low Resource Indian Languages",
      "original": "1035",
      "page_count": 5,
      "order": 589,
      "p1": "2881",
      "pn": "2885",
      "abstract": [
        "This paper describes the CSTR submission to the Multilingual and Code-Switching\nASR Challenges at Interspeech 2021. For the multilingual track of the\nchallenge, we trained a multilingual CNN-TDNN acoustic model for Gujarati,\nHindi, Marathi, Odia, Tamil and Telugu and subsequently fine-tuned\nthe model on monolingual training data. A language model built on a\nmixture of training and CommonCrawl data was used for decoding. We\nalso demonstrate that crawled data from YouTube can be successfully\nused to improve the performance of the acoustic model with semi-supervised\ntraining. These models together with confidence based language identification\nachieve the average WER of 18.1%, a 41% relative improvement compared\nto the provided multilingual baseline model. For the code-switching\ntrack of the challenge we again train a multilingual model on Bengali\nand Hindi technical lectures and we employ a language model trained\non CommonCrawl Bengali and Hindi data mixed with in-domain English\ndata, using a novel transliteration method to generate pronunciations\nfor the English terms. The final model improves by 18% and 34% relative\ncompared to our multilingual baseline. Both our systems were among\nthe top-ranked entries to the challenge.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1035",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components:14 Special Sessions"
    },
    "zhou21d_interspeech": {
      "authors": [
        [
          "Wei",
          "Zhou"
        ],
        [
          "Mohammad",
          "Zeineldeen"
        ],
        [
          "Zuoyun",
          "Zheng"
        ],
        [
          "Ralf",
          "Schl\u00fcter"
        ],
        [
          "Hermann",
          "Ney"
        ]
      ],
      "title": "Acoustic Data-Driven Subword Modeling for End-to-End Speech Recognition",
      "original": "1623",
      "page_count": 5,
      "order": 590,
      "p1": "2886",
      "pn": "2890",
      "abstract": [
        "Subword units are commonly used for end-to-end automatic speech recognition\n(ASR), while a fully acoustic-oriented subword modeling approach is\nsomewhat missing. We propose an acoustic data-driven subword modeling\n(ADSM) approach that adapts the advantages of several text-based and\nacoustic-based subword methods into one pipeline. With a fully acoustic-oriented\nlabel design and learning process, ADSM produces acoustic-structured\nsubword units and acoustic-matched target sequence for further ASR\ntraining. The obtained ADSM labels are evaluated with different end-to-end\nASR approaches including CTC, RNN-Transducer and attention models.\nExperiments on the LibriSpeech corpus show that ADSM clearly outperforms\nboth byte pair encoding (BPE) and pronunciation-assisted subword modeling\n(PASM) in all cases. Detailed analysis shows that ADSM achieves acoustically\nmore logical word segmentation and more balanced sequence length, and\nthus, is suitable for both time-synchronous and label-synchronous models.\nWe also briefly describe how to apply acoustic-based subword regularization\nand unseen text segmentation using ADSM.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1623",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "zhou21e_interspeech": {
      "authors": [
        [
          "Wei",
          "Zhou"
        ],
        [
          "Albert",
          "Zeyer"
        ],
        [
          "Andr\u00e9",
          "Merboldt"
        ],
        [
          "Ralf",
          "Schl\u00fcter"
        ],
        [
          "Hermann",
          "Ney"
        ]
      ],
      "title": "Equivalence of Segmental and Neural Transducer Modeling: A Proof of Concept",
      "original": "1671",
      "page_count": 5,
      "order": 591,
      "p1": "2891",
      "pn": "2895",
      "abstract": [
        "With the advent of direct models in automatic speech recognition (ASR),\nthe formerly prevalent frame-wise acoustic modeling based on hidden\nMarkov models (HMM) diversified into a number of modeling architectures\nlike encoder-decoder attention models, transducer models and segmental\nmodels (direct HMM). While transducer models stay with a frame-level\nmodel definition, segmental models are defined on the level of label\nsegments directly. While (soft-)attention-based models avoid explicit\nalignment, transducer and segmental approach internally do model alignment,\neither by segment hypotheses or, more implicitly, by emitting so-called\nblank symbols. In this work, we prove that the widely used class of\nRNN-Transducer models and segmental models (direct HMM) are equivalent\nand therefore show equal modeling power. It is shown that blank probabilities\ntranslate into segment length probabilities and vice versa. In addition,\nwe provide initial experiments investigating decoding and beam-pruning,\ncomparing time-synchronous and label-/segment-synchronous search strategies\nand their properties using the same underlying model.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1671",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "khosravani21_interspeech": {
      "authors": [
        [
          "Abbas",
          "Khosravani"
        ],
        [
          "Philip N.",
          "Garner"
        ],
        [
          "Alexandros",
          "Lazaridis"
        ]
      ],
      "title": "Modeling Dialectal Variation for Swiss German Automatic Speech Recognition",
      "original": "1735",
      "page_count": 5,
      "order": 592,
      "p1": "2896",
      "pn": "2900",
      "abstract": [
        "We describe a speech recognition system for Swiss German, a dialectal\nspoken language in German-speaking Switzerland. Swiss German has no\nstandard orthography, with a significant variation in its written form.\nTo alleviate the uncertainty associated with this variability, we automatically\ngenerate a lexicon from which multiple written forms of a given word\nin any dialect can be generated. The lexicon is built from a small\n(incomplete) handcrafted lexicon designed by linguistic experts and\ncontains forms of common words in various Swiss German dialects. We\nexploit the powerful speech representation of self-supervised acoustic\npre-training (wav2vec) to address the low-resource nature of the spoken\ndialects. The proposed approach results in an overall relative improvement\nof 9% word error rate compared to one based on an expert-generated\nlexicon for our TV Box voice assistant application.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1735",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "egorova21_interspeech": {
      "authors": [
        [
          "Ekaterina",
          "Egorova"
        ],
        [
          "Hari Krishna",
          "Vydana"
        ],
        [
          "Luk\u00e1\u0161",
          "Burget"
        ],
        [
          "Jan",
          "\u010cernock\u00fd"
        ]
      ],
      "title": "Out-of-Vocabulary Words Detection with Attention and CTC Alignments in an End-to-End ASR System",
      "original": "1756",
      "page_count": 5,
      "order": 593,
      "p1": "2901",
      "pn": "2905",
      "abstract": [
        "This work explores the effectiveness of detecting positions of out-of-vocabulary\nwords (OOVs) in a decoded utterance using attention weights and CTC\nper-frame outputs of an end-to-end system predicting word sequences.\nWe show that the end-to-end approach can be effective for the task\nof OOV detection. CTC alignments are shown to provide better temporal\ninformation about the positions of OOV words than attention, and therefore\nare more suitable for the task. The detected positions of OOV occurrences\nare utilized for the recurrent OOV recovery task in which probabilistic\nrepresentations of the pronunciations of the detected OOVs are clustered\nin order to find repeating words. Improved detection results are shown\nto correlate with better performance of the recovery of recurrent OOVs.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1756",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "wiesner21_interspeech": {
      "authors": [
        [
          "Matthew",
          "Wiesner"
        ],
        [
          "Mousmita",
          "Sarma"
        ],
        [
          "Ashish",
          "Arora"
        ],
        [
          "Desh",
          "Raj"
        ],
        [
          "Dongji",
          "Gao"
        ],
        [
          "Ruizhe",
          "Huang"
        ],
        [
          "Supreet",
          "Preet"
        ],
        [
          "Moris",
          "Johnson"
        ],
        [
          "Zikra",
          "Iqbal"
        ],
        [
          "Nagendra",
          "Goel"
        ],
        [
          "Jan",
          "Trmal"
        ],
        [
          "Leibny Paola Garc\u00eda",
          "Perera"
        ],
        [
          "Sanjeev",
          "Khudanpur"
        ]
      ],
      "title": "Training Hybrid Models on Noisy Transliterated Transcripts for Code-Switched Speech Recognition",
      "original": "2127",
      "page_count": 5,
      "order": 594,
      "p1": "2906",
      "pn": "2910",
      "abstract": [
        "In this paper, we describe the JHU-GoVivace submission for subtask\n2 (code-switching task) of the Multilingual and Code-switching ASR\nchallenges for low resource Indian languages. We built a hybrid HMM-DNN\nsystem with several improvements over the provided baseline in terms\nof lexical, language, and acoustic modeling. For lexical modeling,\nwe investigate using unified pronunciations and phonesets derived from\nthe baseline lexicon and publicly available Wikipron lexicons in Bengali\nand Hindi to expand the pronunciation lexicons. We explore several\nneural network architectures, along with supervised pretraining and\nmultilingual training for acoustic modeling. We also describe how we\nused large externally crawled web text for language modeling. Since\nthe challenge data contain artefacts such as misalignments, various\ndata cleanup methods are explored, including acoustic-driven pronunciation\nlearning to help discover Indian-accented pronunciations for English\nwords as well as transcribed punctuation. As a result of these efforts,\nour best systems achieve transliterated WERs of 19.5% and 23.2% on\nthe non-duplicated development sets for Hindi-English and Bengali-English,\nrespectively.\n"
      ],
      "doi": "10.21437/Interspeech.2021-2127",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components:14 Special Sessions"
    },
    "xue21c_interspeech": {
      "authors": [
        [
          "Wei",
          "Xue"
        ],
        [
          "Roeland van",
          "Hout"
        ],
        [
          "Fleur",
          "Boogmans"
        ],
        [
          "Mario",
          "Ganzeboom"
        ],
        [
          "Catia",
          "Cucchiarini"
        ],
        [
          "Helmer",
          "Strik"
        ]
      ],
      "title": "Speech Intelligibility of Dysarthric Speech: Human Scores and Acoustic-Phonetic Features",
      "original": "1189",
      "page_count": 5,
      "order": 595,
      "p1": "2911",
      "pn": "2915",
      "abstract": [
        "We investigated speech intelligibility in dysarthric and non-dysarthric\nspeakers as measured by two commonly used metrics, ratings through\nthe Visual Analogue Scale (VAS) and word accuracy (AcW) through orthographic\ntranscriptions. To gain a better understanding of how acoustic-phonetic\ncorrelates could be employed to obtain more objective measures of speech\nintelligibility and a better classification of dysarthric and non-dysarthric\nspeakers, we studied the relation between these measures of intelligibility\nand some important acoustic-phonetic correlates. We found that the\ntwo intelligibility measures are related, but distinct, and that they\nmight refer to different components of the intelligibility construct.\nThe acoustic-phonetic features showed no difference in the mean values\nbetween the two speaker types at the utterance level, but more than\nhalf of them played a role in classifying the two speaker types. We\ncomputed an acoustic-phonetic probability index (API) at the speaker\nlevel. API is moderately correlated to VAS ratings but not correlated\nto AcW. In addition, API and VAS complement each other in classifying\ndysarthric and non-dysarthric speakers. This suggests that the intelligibility\nmeasures assigned by human raters and acoustic-phonetic features relate\nto different constructs of intelligibility.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1189",
      "author_area_id": "13",
      "author_area_label": " Speech, Voice, and Hearing Disorders"
    },
    "kim21i_interspeech": {
      "authors": [
        [
          "Young-Kyung",
          "Kim"
        ],
        [
          "Rimita",
          "Lahiri"
        ],
        [
          "Md.",
          "Nasir"
        ],
        [
          "So Hyun",
          "Kim"
        ],
        [
          "Somer",
          "Bishop"
        ],
        [
          "Catherine",
          "Lord"
        ],
        [
          "Shrikanth S.",
          "Narayanan"
        ]
      ],
      "title": "Analyzing Short Term Dynamic Speech Features for Understanding Behavioral Traits of Children with Autism Spectrum Disorder",
      "original": "2111",
      "page_count": 5,
      "order": 596,
      "p1": "2916",
      "pn": "2920",
      "abstract": [
        "Computational methodologies have shown promise in advancing diagnostic\nand intervention research in the domain of <i>Autism Spectrum Disorder\n(ASD)</i>. Prior works have investigated speech features to assess\ndisorder severity and also to differentiate between children with and\nwithout an ASD diagnosis. In this work, we explore short term dynamic\nfunctionals of speech features both within and across speakers to understand\nif local changes in speech provide information toward phenotyping of\nASD.We compare the contributions of static and dynamic functionals\nrepresenting conversational speech toward the clinical diagnosis state.\nOur results show that predictions obtained from a combination of dynamic\nand static functionals have comparable or superior performance to the\npredictions obtained from just static speech functionals. We also analyze\nthe relationship between speech production and ASD diagnosis through\ncorrelation analyses between speech functionals and manually-derived\nbehavioral codes related to autism severity. The experimental results\nsupport the notion that dynamic speech functionals capture complementary\ninformation which can facilitate enriched analysis of clinically-meaningful\nbehavioral inference tasks.\n"
      ],
      "doi": "10.21437/Interspeech.2021-2111",
      "author_area_id": "13",
      "author_area_label": " Speech, Voice, and Hearing Disorders"
    },
    "jesko21_interspeech": {
      "authors": [
        [
          "Waldemar",
          "J\u0119\u015bko"
        ]
      ],
      "title": "Vocalization Recognition of People with Profound Intellectual and Multiple Disabilities (PIMD) Using Machine Learning Algorithms",
      "original": "1239",
      "page_count": 5,
      "order": 597,
      "p1": "2921",
      "pn": "2925",
      "abstract": [
        "We investigate vocalization recognition for people with Profound Intellectual\nand Multiple Disabilities using various machine learning algorithms.\nThe amount of training data available for people with PIMD is typically\nsignificantly limited. Due to this fact, data augmentation process\nwas used. Various types of Machine Learning algorithms were tested:\nk-NN, NB, DT, RDF, MLP and LSTM. During research we also tested various\nregularization techniques to improve recognition performance. The best\nresults were obtained in case of MLP network with dropout and batch\nnormalization: 90%.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1239",
      "author_area_id": "13",
      "author_area_label": " Speech, Voice, and Hearing Disorders"
    },
    "fivela21_interspeech": {
      "authors": [
        [
          "Barbara Gili",
          "Fivela"
        ],
        [
          "Vincenzo",
          "Sallustio"
        ],
        [
          "Silvia",
          "Pede"
        ],
        [
          "Danilo",
          "Patrocinio"
        ]
      ],
      "title": "Phonetic Complexity, Speech Accuracy and Intelligibility Assessment of Italian Dysarthric Speech",
      "original": "1862",
      "page_count": 5,
      "order": 598,
      "p1": "2926",
      "pn": "2930",
      "abstract": [
        "Intelligibility is the degree to which the speech of a person may be\nunderstood by a listener, and is related to functional limitation and\ndisability. In protocols for the clinical assessment of dysarthria,\nintelligibility checks are included, as well as evaluations of speech\naccuracy, which is more directly related to the disease severity. However,\nboth evaluations are usually based on subjective ratings.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  Aim of this work is\nchecking the correlation between intelligibility judgements, subjectively\nassigned as it may be the case in clinical procedures, and acoustic\nmeasures related to linguistically contrasting units. Two novelties\ncharacterize this work: a) acoustic measurements considered in the\npaper relate to both segments (vowel and consonants) and prosodic-intonational\nphonological events (e.g., pitch accents), that is linguistically relevant\nspeech units; b) contexts of increasing phonetic-phonological complexity\nare considered, in order for the phonetic characteristics to challenge\nproduction accuracy, possibly affecting the realization of phonological\nfeatures and intelligibility. Increasing complexity is expected to\nchallenge intelligibility indeed and to have an impact on the correlation\nbetween intelligibility rates and acoustic measures. Results are preliminary,\nbut confirm both 1) the correlation between acoustic measures of linguistically\nrelevant events and speech intelligibility, as for both the segmental\nand the prosodic-intonational level, and 2) the role of increasing\nphonetic-phonological complexity in enhancing the above mentioned correlation.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1862",
      "author_area_id": "13",
      "author_area_label": " Speech, Voice, and Hearing Disorders"
    },
    "ng21_interspeech": {
      "authors": [
        [
          "Si-Ioi",
          "Ng"
        ],
        [
          "Cymie Wing-Yee",
          "Ng"
        ],
        [
          "Jingyu",
          "Li"
        ],
        [
          "Tan",
          "Lee"
        ]
      ],
      "title": "Detection of Consonant Errors in Disordered Speech Based on Consonant-Vowel Segment Embedding",
      "original": "1305",
      "page_count": 5,
      "order": 599,
      "p1": "2931",
      "pn": "2935",
      "abstract": [
        "Speech sound disorder (SSD) refers to a type of developmental disorder\nin young children who encounter persistent difficulties in producing\ncertain speech sounds at the expected age. Consonant errors are the\nmajor indicator of SSD in clinical assessment. Previous studies on\nautomatic assessment of SSD revealed that detection of speech errors\nconcerning short and transitory consonants is less satisfactory. This\npaper investigates a neural network based approach to detecting consonant\nerrors in disordered speech using consonant-vowel (CV) diphone segment\nin comparison to using consonant monophone segment. The underlying\nassumption is that the vowel part of a CV segment carries important\ninformation of co-articulation from the consonant. Speech embeddings\nare extracted from CV segments by a recurrent neural network model.\nThe similarity scores between the embeddings of the test segment and\nthe reference segments are computed to determine if the test segment\nis the expected consonant or not. Experimental results show that using\nCV segments achieves improved performance on detecting speech errors\nconcerning those &#8220;difficult&#8221; consonants reported in the\nprevious studies.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1305",
      "author_area_id": "13",
      "author_area_label": " Speech, Voice, and Hearing Disorders"
    },
    "hair21_interspeech": {
      "authors": [
        [
          "Adam",
          "Hair"
        ],
        [
          "Guanlong",
          "Zhao"
        ],
        [
          "Beena",
          "Ahmed"
        ],
        [
          "Kirrie J.",
          "Ballard"
        ],
        [
          "Ricardo",
          "Gutierrez-Osuna"
        ]
      ],
      "title": "Assessing Posterior-Based Mispronunciation Detection on Field-Collected Recordings from Child Speech Therapy Sessions",
      "original": "0069",
      "page_count": 5,
      "order": 600,
      "p1": "2936",
      "pn": "2940",
      "abstract": [
        "A critical component of child speech therapy is home practice with\na caregiver, who can provide feedback. However, caregivers oftentimes\nstruggle with accurately rating speech and with perceiving pronunciation\nerrors. One potential solution for this issue is to embed automatic\nmispronunciation-detection (MPD) algorithms within digital speech therapy\napplications. To address the need for MPD within child speech therapy,\nwe investigated posterior-based mispronunciation detection using a\ncustom corpus of disordered speech from children that had been manually\nannotated by an expert clinician. Namely, we trained a family of phoneme-specific\nlogistic regression classifiers (LRC) and support vector machines (SVM)\non log posterior probability and log posterior ratio features. Our\nresults show that these classifiers outperformed baseline Goodness\nof Pronunciation scoring by 11% and 10%, respectively. Even more importantly,\nin an offline test, the LRC and SVM classifiers outperformed student\nclinicians at identifying mispronunciations by 18% and 16%, respectively.\nThese results suggest that posterior-based mispronunciation detection\nmay be suitable to provide at-home therapy feedback for children.\n"
      ],
      "doi": "10.21437/Interspeech.2021-69",
      "author_area_id": "13",
      "author_area_label": " Speech, Voice, and Hearing Disorders"
    },
    "mirheidari21_interspeech": {
      "authors": [
        [
          "Bahman",
          "Mirheidari"
        ],
        [
          "Yilin",
          "Pan"
        ],
        [
          "Daniel",
          "Blackburn"
        ],
        [
          "Ronan",
          "O\u2019Malley"
        ],
        [
          "Heidi",
          "Christensen"
        ]
      ],
      "title": "Identifying Cognitive Impairment Using Sentence Representation Vectors",
      "original": "0915",
      "page_count": 5,
      "order": 601,
      "p1": "2941",
      "pn": "2945",
      "abstract": [
        "The widely used word vectors can be extended at the sentence level\nto perform a wide range of natural language processing (NLP) tasks.\nRecently the Bidirectional Encoder Representations from Transformers\n(BERT) language representation achieved state-of-the-art performance\nfor these applications. The model is trained with punctuated and well-formed\n(writ-ten) text, however, the performance of the model drops significantly\nwhen the input text is the &#8212; erroneous and unpunctuated &#8212;\noutput of automatic speech recognition (ASR).  We use a sliding window\nand averaging approach for pre-processing text for BERT to extract\nfeatures for classifying three diagnostic categories relating to cognitive\nimpairment: neurodegenerative dis-order (ND), mild cognitive impairment\n(MCI), and healthy controls (HC). The in-house dataset contains the\naudio recordings of an intelligent virtual agent (IVA) who asks the\nparticipants several conversational questions prompts in addition to\ngiving a picture description prompt. For the three-way classification,\nwe achieve a 73.88% F-score (accuracy: 76.53%) using the pre-trained,\nuncased base BERT and for the two-way classifier (HC vs. ND) we achieve\n89.80% (accuracy: 90%). We further improve these by using a prompt\nselection technique, reaching the F-scores of 79.98% (accuracy: 81.63%)\nand 93.56% (accuracy: 93.75%) respectively.\n"
      ],
      "doi": "10.21437/Interspeech.2021-915",
      "author_area_id": "13",
      "author_area_label": " Speech, Voice, and Hearing Disorders"
    },
    "yue21b_interspeech": {
      "authors": [
        [
          "Zhengjun",
          "Yue"
        ],
        [
          "Jon",
          "Barker"
        ],
        [
          "Heidi",
          "Christensen"
        ],
        [
          "Cristina",
          "McKean"
        ],
        [
          "Elaine",
          "Ashton"
        ],
        [
          "Yvonne",
          "Wren"
        ],
        [
          "Swapnil",
          "Gadgil"
        ],
        [
          "Rebecca",
          "Bright"
        ]
      ],
      "title": "Parental Spoken Scaffolding and Narrative Skills in Crowd-Sourced Storytelling Samples of Young Children",
      "original": "1297",
      "page_count": 5,
      "order": 602,
      "p1": "2946",
      "pn": "2950",
      "abstract": [
        "A novel crowdsourcing project to gather children&#8217;s storytelling\nbased language samples using a mobile app was undertaken across the\nUnited Kingdom. Parents&#8217; scaffolding of children&#8217;s narratives\nwas observed in many of the samples. This study was designed to examine\nthe relationship of scaffolding and young children&#8217;s narrative\nlanguage ability in a story retell context which is analysed at the\nmacro-structural (total macro-structure score), the micro-structural\n(mean length of utterances in morphemes) and verbal productivity (total\nnumber of utterances) levels. Young children with and without scaffolding\nwere statistically compared. The interaction between the level of scaffolding\nsupport, the grammar complexity and the narrative structure was explored.\nA bidirectional relationship was observed between scaffolding and young\nchildren&#8217;s narrative language ability. Young children with better\nperformance were observed to receive less scaffolding from parents.\nScaffolding was shown to support early narrative development of young\nchildren and was more able to benefit those with low-level grammatical\ncomplexity skills. It is crucial to encourage parental scaffolding\nto be well-attuned to the child&#8217;s narrative ability.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1297",
      "author_area_id": "13",
      "author_area_label": " Speech, Voice, and Hearing Disorders"
    },
    "xia21_interspeech": {
      "authors": [
        [
          "Tong",
          "Xia"
        ],
        [
          "Jing",
          "Han"
        ],
        [
          "Lorena",
          "Qendro"
        ],
        [
          "Ting",
          "Dang"
        ],
        [
          "Cecilia",
          "Mascolo"
        ]
      ],
      "title": "Uncertainty-Aware COVID-19 Detection from Imbalanced Sound Data",
      "original": "1320",
      "page_count": 5,
      "order": 603,
      "p1": "2951",
      "pn": "2955",
      "abstract": [
        "Recently, sound-based COVID-19 detection studies have shown great promise\nto achieve scalable and prompt digital pre-screening. However, there\nare still two unsolved issues hindering the practice. First, collected\ndatasets for model training are often imbalanced, with a considerably\nsmaller proportion of users tested positive, making it harder to learn\nrepresentative and robust features. Second, deep learning models are\ngenerally overconfident in their predictions. Clinically, false predictions\naggravate healthcare costs. Estimation of the uncertainty of screening\nwould aid this. To handle these issues, we propose an ensemble framework\nwhere multiple deep learning models for sound-based COVID-19 detection\nare developed from different but balanced subsets from original data.\nAs such, data are utilized more effectively compared to traditional\nup-sampling and down-sampling approaches: an AUC of 0.74 with a sensitivity\nof 0.68 and a specificity of 0.69 is achieved. Simultaneously, we estimate\nuncertainty from the disagreement across multiple models. It is shown\nthat false predictions often yield higher uncertainty, enabling us\nto suggest the users with certainty higher than a threshold to repeat\nthe audio test on their phones or to take clinical tests if digital\ndiagnosis still fails. This study paves the way for a more robust sound-based\nCOVID-19 automated screening system.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1320",
      "author_area_id": "13",
      "author_area_label": " Speech, Voice, and Hearing Disorders"
    },
    "wang21u_interspeech": {
      "authors": [
        [
          "Disong",
          "Wang"
        ],
        [
          "Liqun",
          "Deng"
        ],
        [
          "Yu Ting",
          "Yeung"
        ],
        [
          "Xiao",
          "Chen"
        ],
        [
          "Xunying",
          "Liu"
        ],
        [
          "Helen",
          "Meng"
        ]
      ],
      "title": "Unsupervised Domain Adaptation for Dysarthric Speech Detection via Domain Adversarial Training and Mutual Information Minimization",
      "original": "2139",
      "page_count": 5,
      "order": 604,
      "p1": "2956",
      "pn": "2960",
      "abstract": [
        "Dysarthric speech detection (DSD) systems aim to detect characteristics\nof the neuromotor disorder from speech. Such systems are particularly\nsusceptible to domain mismatch where the training and testing data\ncome from the source and target domains respectively, but the two domains\nmay differ in terms of speech stimuli, disease etiology, etc. It is\nhard to acquire labelled data in the target domain, due to high costs\nof annotating sizeable datasets. This paper makes a first attempt to\nformulate cross-domain DSD as an unsupervised domain adaptation (UDA)\nproblem. We use labelled source-domain data and unlabelled target-domain\ndata, and propose a multi-task learning strategy, including dysarthria\npresence classification (DPC), domain adversarial training (DAT) and\nmutual information minimization (MIM), which aim to learn dysarthria-discriminative\nand domain-invariant biomarker embeddings. Specifically, DPC helps\nbiomarker embeddings capture critical indicators of dysarthria; DAT\nforces biomarker embeddings to be indistinguishable in source and target\ndomains; and MIM further reduces the correlation between biomarker\nembeddings and domain-related cues. By treating the UASPEECH and TORGO\ncorpora respectively as the source and target domains, experiments\nshow that the incorporation of UDA attains absolute increases of 22.2%\nand 20.0% respectively in utterance-level weighted average recall and\nspeaker-level accuracy.\n"
      ],
      "doi": "10.21437/Interspeech.2021-2139",
      "author_area_id": "13",
      "author_area_label": " Speech, Voice, and Hearing Disorders"
    },
    "bhattacharjee21_interspeech": {
      "authors": [
        [
          "Tanuka",
          "Bhattacharjee"
        ],
        [
          "Jhansi",
          "Mallela"
        ],
        [
          "Yamini",
          "Belur"
        ],
        [
          "Nalini",
          "Atchayaram"
        ],
        [
          "Ravi",
          "Yadav"
        ],
        [
          "Pradeep",
          "Reddy"
        ],
        [
          "Dipanjan",
          "Gope"
        ],
        [
          "Prasanta Kumar",
          "Ghosh"
        ]
      ],
      "title": "Source and Vocal Tract Cues for Speech-Based Classification of Patients with Parkinson&#8217;s Disease and Healthy Subjects",
      "original": "2008",
      "page_count": 5,
      "order": 605,
      "p1": "2961",
      "pn": "2965",
      "abstract": [
        "Parkinson&#8217;s disease (PD) affects both source and vocal tract\ncomponents of speech. Various speech cues explored in literature for\nautomatic classification of individuals with PD and healthy controls\n(HC) implicitly carry information about both these components. This\nwork explicitly analyzes the contribution of source and vocal tract\nattributes toward automatic PD vs. HC classification, which has not\nbeen done earlier to the best of our knowledge. Here fundamental frequency\n(f<SUB>o</SUB>) is used to capture source information. For quantifying\nvocal tract information, speech waveforms are converted to unvoiced\nforms and mel-frequency cepstral coefficients (MFCC), denoted by voicing-removed\nMFCC, are obtained from them. Experimental results suggest that (1)\nthe relative merit of source and vocal tract cues in classifying PD\nvs. HC largely depends on the speech task being considered, (2) both\ncues complement each other across all tasks, (3) while MFCC encodes\nboth source and vocal tract features, source information captured by\nf<SUB>o</SUB> is different and further complements MFCC when the classifiers\nare trained and tested under clean or matched noise conditions, thereby\nenabling the feature-level fusion of f<SUB>o</SUB> and MFCC to achieve\nthe best classification accuracy, (4) under unseen noise conditions,\nf<SUB>o</SUB> alone proves to be a highly noise-robust feature.\n"
      ],
      "doi": "10.21437/Interspeech.2021-2008",
      "author_area_id": "13",
      "author_area_label": " Speech, Voice, and Hearing Disorders"
    },
    "haulcy21_interspeech": {
      "authors": [
        [
          "R\u2019mani",
          "Haulcy"
        ],
        [
          "James",
          "Glass"
        ]
      ],
      "title": "CLAC: A Speech Corpus of Healthy English Speakers",
      "original": "1810",
      "page_count": 5,
      "order": 606,
      "p1": "2966",
      "pn": "2970",
      "abstract": [
        "This paper introduces the Crowdsourced Language Assessment Corpus (CLAC),\na speech corpus consisting of audio recordings and automatically-generated\ntranscripts for several speech and language tasks, as well as metadata\nfor each of the speakers. The CLAC was created to provide the community\nwith a collection of audio samples from various speakers that could\nbe used to learn a general representation for speech from healthy subjects,\nas well as complement other health-related speech datasets, which tend\nto be limited. In this paper, we describe the data collection protocol\nand summarize the contents of the dataset. We also extract timing metrics\nfrom the recordings of each task to explore what those metrics look\nlike for a large, English-speaking population. Lastly, we provide an\nexample of how the dataset can be used by comparing the metrics to\nthose extracted from a small sample of Frontotemporal Dementia subjects.\nWe hope that this dataset will help advance the state of the art in\nthe health and speech domain.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1810",
      "author_area_id": "13",
      "author_area_label": " Speech, Voice, and Hearing Disorders"
    },
    "nortje21_interspeech": {
      "authors": [
        [
          "Leanne",
          "Nortje"
        ],
        [
          "Herman",
          "Kamper"
        ]
      ],
      "title": "Direct Multimodal Few-Shot Learning of Speech and Images",
      "original": "0049",
      "page_count": 5,
      "order": 607,
      "p1": "2971",
      "pn": "2975",
      "abstract": [
        "We propose direct multimodal few-shot models that learn a shared embedding\nspace of spoken words and images from only a few paired examples. Imagine\nan agent is shown an image along with a spoken word describing the\nobject in the picture, e.g. <i>pen, book</i> and <i>eraser</i>. After\nobserving a few paired examples of each class, the model is asked to\nidentify the &#8220;book&#8221; in a set of unseen pictures. Previous\nwork used a two-step indirect approach relying on speech-speech and\nimage-image comparisons across the support set of given speech-image\npairs. Instead, we propose two direct models which learn a single multimodal\nspace where inputs from different modalities are directly comparable:\na multimodal triplet network (MTriplet) and a multimodal correspondence\nautoencoder (MCAE). To train these direct models, we <i>mine</i> speech-image\npairs by using the support set to pair up unlabelled in-domain speech\nand images. In a speech-to-image digit matching task, direct models\noutperform indirect models, with the MTriplet achieving the best multimodal\nfive-shot accuracy. We show that the improvements are due to the combination\nof unsupervised and transfer learning in the direct models, and the\nabsence of two-step compounding errors.\n"
      ],
      "doi": "10.21437/Interspeech.2021-49",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "sanabria21_interspeech": {
      "authors": [
        [
          "Ramon",
          "Sanabria"
        ],
        [
          "Austin",
          "Waters"
        ],
        [
          "Jason",
          "Baldridge"
        ]
      ],
      "title": "Talk, Don&#8217;t Write: A Study of Direct Speech-Based Image Retrieval",
      "original": "0096",
      "page_count": 5,
      "order": 608,
      "p1": "2976",
      "pn": "2980",
      "abstract": [
        "Speech-based image retrieval has been studied as a proxy for joint\nrepresentation learning, usually without emphasis on retrieval itself.\nAs such, it is unclear how well speech-based retrieval can work in\npractice &#8212; both in an absolute sense and versus alternative strategies\nthat combine automatic speech recognition (ASR) with strong text encoders.\nIn this work, we extensively study and expand choices of encoder architectures,\ntraining methodology (including unimodal and multimodal pretraining),\nand other factors. Our experiments cover different types of speech\nin three datasets: Flickr Audio, Places Audio, and Localized Narratives.\nOur best model configuration achieves large gains over state of the\nart, e.g., pushing recall-at-one from 21.8% to 33.2% for Flickr Audio\nand 27.6% to 53.4% for Places Audio. We also show our best speech-based\nmodels can match or exceed cascaded ASR-to-text encoding when speech\nis spontaneous, accented, or otherwise hard to automatically transcribe.\n"
      ],
      "doi": "10.21437/Interspeech.2021-96",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "zhao21b_interspeech": {
      "authors": [
        [
          "Huan",
          "Zhao"
        ],
        [
          "Kaili",
          "Ma"
        ]
      ],
      "title": "A Fast Discrete Two-Step Learning Hashing for Scalable Cross-Modal Retrieval",
      "original": "0287",
      "page_count": 5,
      "order": 609,
      "p1": "2981",
      "pn": "2985",
      "abstract": [
        "Recently, some cross-modal hashing methods are proposed to search data\nfor different modality effectively. Hashing has received wide attention\nbecause of its low storage and high efficiency. Hashing-based methods\nproject the data instances from different modalities into a Hamming\nspace to learn hash codes for retrieval between different modality.\nAlthough obtaining promising performance, hashing-based methods have\nstill several common limitations. First, they learn the hash codes\nby constructing semantic similarity matrices, resulting in the loss\nof information. Second, most existing methods simultaneously learn\nthe hash codes and the hash functions, which bring a high computational\ncomplexity. Third, they utilize the relaxation-based optimization strategy\nto generate the hash codes which leads to the large quantization error\nof the hash codes. To solve the above problems, we propose a novel\nfast supervised hashing method, termed Fast Discrete Two-Step Learning\nHashing (FDTLH) for scalable cross-modal retrieval, which learns the\ndiscriminative hash codes by adopting a effective two-step learning\nscheme. Extensive experiments show that the FDTLH outperforms several\nstate-of-the-art hashing methods in terms of retrieval performance\nand learning efficiency.\n"
      ],
      "doi": "10.21437/Interspeech.2021-287",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "wang21v_interspeech": {
      "authors": [
        [
          "Jianrong",
          "Wang"
        ],
        [
          "Ziyue",
          "Tang"
        ],
        [
          "Xuewei",
          "Li"
        ],
        [
          "Mei",
          "Yu"
        ],
        [
          "Qiang",
          "Fang"
        ],
        [
          "Li",
          "Liu"
        ]
      ],
      "title": "Cross-Modal Knowledge Distillation Method for Automatic Cued Speech Recognition",
      "original": "0432",
      "page_count": 5,
      "order": 610,
      "p1": "2986",
      "pn": "2990",
      "abstract": [
        "Cued Speech (CS) is a visual communication system for the deaf or hearing\nimpaired people. It combines lip movements with hand cues to obtain\na complete phonetic repertoire. Current deep learning based methods\non automatic CS recognition suffer from a common problem, which is\nthe data scarcity. Until now, there are only two public single speaker\ndatasets for French (238 sentences) and British English (97 sentences).\nIn this work, we propose a cross-modal knowledge distillation method\nwith teacher-student structure, which transfers audio speech information\nto CS to overcome the limited data problem. Firstly, we pretrain a\nteacher model for CS recognition with a large amount of open source\naudio speech data, and simultaneously pretrain the feature extractors\nfor lips and hands using CS data. Then, we distill the knowledge from\nteacher model to the student model with frame-level and sequence-level\ndistillation strategies. Importantly, for frame-level, we exploit multi-task\nlearning to weigh losses automatically, to obtain the balance coefficient.\nBesides, we establish a five-speaker British English CS dataset for\nthe first time. The proposed method is evaluated on French and British\nEnglish CS datasets, showing superior CS recognition performance to\nthe state-of-the-art (SOTA) by a large margin.\n"
      ],
      "doi": "10.21437/Interspeech.2021-432",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "olaleye21_interspeech": {
      "authors": [
        [
          "Kayode",
          "Olaleye"
        ],
        [
          "Herman",
          "Kamper"
        ]
      ],
      "title": "Attention-Based Keyword Localisation in Speech Using Visual Grounding",
      "original": "0435",
      "page_count": 5,
      "order": 611,
      "p1": "2991",
      "pn": "2995",
      "abstract": [
        "Visually grounded speech models learn from images paired with spoken\ncaptions. By tagging images with soft text labels using a trained visual\nclassifier with a fixed vocabulary, previous work has shown that it\nis possible to train a model that can <i>detect</i> whether a particular\ntext keyword occurs in speech utterances or not. Here we investigate\nwhether visually grounded speech models can also do keyword <i>localisation</i>:\npredicting where, within an utterance, a given textual keyword occurs\nwithout any explicit text-based or alignment supervision. We specifically\nconsider whether incorporating attention into a convolutional model\nis beneficial for localisation. Although absolute localisation performance\nwith visually supervised models is still modest (compared to using\nunordered bag-of-word text labels for supervision), we show that attention\nprovides a large gain in performance over previous visually grounded\nmodels. As in many other speech-image studies, we find that many of\nthe incorrect localisations are due to semantic confusions, e.g. locating\nthe word &#8216;backstroke&#8217; for the query keyword &#8216;swimming&#8217;.\n"
      ],
      "doi": "10.21437/Interspeech.2021-435",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "khorrami21_interspeech": {
      "authors": [
        [
          "Khazar",
          "Khorrami"
        ],
        [
          "Okko",
          "R\u00e4s\u00e4nen"
        ]
      ],
      "title": "Evaluation of Audio-Visual Alignments in Visually Grounded Speech Models",
      "original": "0496",
      "page_count": 5,
      "order": 612,
      "p1": "2996",
      "pn": "3000",
      "abstract": [
        "Systems that can find correspondences between multiple modalities,\nsuch as between speech and images, have great potential to solve different\nrecognition and data analysis tasks in an unsupervised manner. This\nwork studies multimodal learning in the context of visually grounded\nspeech (VGS) models, and focuses on their recently demonstrated capability\nto extract spatiotemporal alignments between spoken words and the corresponding\nvisual objects without ever been explicitly trained for object localization\nor word recognition. As the main contributions, we formalize the alignment\nproblem in terms of an audio-visual alignment tensor that is based\non earlier VGS work, introduce systematic metrics for evaluating model\nperformance in aligning visual objects and spoken words, and propose\na new VGS model variant for the alignment task utilizing cross-modal\nattention layer. We test our model and a previously proposed model\nin the alignment task using SPEECH-COCO captions coupled with MSCOCO\nimages. We compare the alignment performance using our proposed evaluation\nmetrics to the semantic retrieval task commonly used to evaluate VGS\nmodels. We show that cross-modal attention layer not only helps the\nmodel to achieve higher semantic cross-modal retrieval performance,\nbut also leads to substantial improvements in the alignment performance\nbetween image object and spoken words.\n"
      ],
      "doi": "10.21437/Interspeech.2021-496",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "chen21k_interspeech": {
      "authors": [
        [
          "Hang",
          "Chen"
        ],
        [
          "Jun",
          "Du"
        ],
        [
          "Yu",
          "Hu"
        ],
        [
          "Li-Rong",
          "Dai"
        ],
        [
          "Bao-Cai",
          "Yin"
        ],
        [
          "Chin-Hui",
          "Lee"
        ]
      ],
      "title": "Automatic Lip-Reading with Hierarchical Pyramidal Convolution and Self-Attention for Image Sequences with No Word Boundaries",
      "original": "0723",
      "page_count": 5,
      "order": 613,
      "p1": "3001",
      "pn": "3005",
      "abstract": [
        "In this paper, we propose a novel deep learning architecture for improving\nword-level lip-reading. We first incorporate multi-scale processing\ninto spatial feature extraction for lip-reading using hierarchical\npyramidal convolution (HPConv) and self-attention. Specifically, HPConv\nis proposed to replace the conventional convolution features, leading\nto an improvement over the model&#8217;s ability to discover fine-grained\nlip movements. Next to deal with fixed-length image sequences representing\nwords in a given database, a self-attention mechanism is proposed to\nintegrate local information in all lip frames without assuming known\nword boundaries, so that our deep models automatically utilize key\nfeature in relevant frames of a given word. Experiments on the Lip\nReading in the Wild corpus show that our proposed architecture achieves\nan accuracy of 86.83%, yielding a relative error rate reduction of\nabout 10% from that obtained with a state-of-the-art scheme of averaging\nframe scores for information fusion. A detailed analysis of the experimental\nresults also confirms that weights learned from self-attention tend\nto be zero at both sides of an image sequence and focus non-zero weights\nin the middle part of a given word.\n"
      ],
      "doi": "10.21437/Interspeech.2021-723",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "rouditchenko21b_interspeech": {
      "authors": [
        [
          "Andrew",
          "Rouditchenko"
        ],
        [
          "Angie",
          "Boggust"
        ],
        [
          "David",
          "Harwath"
        ],
        [
          "Samuel",
          "Thomas"
        ],
        [
          "Hilde",
          "Kuehne"
        ],
        [
          "Brian",
          "Chen"
        ],
        [
          "Rameswar",
          "Panda"
        ],
        [
          "Rogerio",
          "Feris"
        ],
        [
          "Brian",
          "Kingsbury"
        ],
        [
          "Michael",
          "Picheny"
        ],
        [
          "James",
          "Glass"
        ]
      ],
      "title": "Cascaded Multilingual Audio-Visual Learning from Videos",
      "original": "1352",
      "page_count": 5,
      "order": 614,
      "p1": "3006",
      "pn": "3010",
      "abstract": [
        "In this paper, we explore self-supervised audio-visual models that\nlearn from instructional videos. Prior work has shown that these models\ncan relate spoken words and sounds to visual content after training\non a large-scale dataset of videos, but they were only trained and\nevaluated on videos in English. To learn multilingual audio-visual\nrepresentations, we propose a cascaded approach that leverages a model\ntrained on English videos and applies it to audio-visual data in other\nlanguages, such as Japanese videos. With our cascaded approach, we\nshow an improvement in retrieval performance of nearly 10&#215; compared\nto training on the Japanese videos solely. We also apply the model\ntrained on English videos to Japanese and Hindi spoken captions of\nimages, achieving state-of-the-art performance.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1352",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "ma21c_interspeech": {
      "authors": [
        [
          "Pingchuan",
          "Ma"
        ],
        [
          "Rodrigo",
          "Mira"
        ],
        [
          "Stavros",
          "Petridis"
        ],
        [
          "Bj\u00f6rn W.",
          "Schuller"
        ],
        [
          "Maja",
          "Pantic"
        ]
      ],
      "title": "LiRA: Learning Visual Speech Representations from Audio Through Self-Supervision",
      "original": "1360",
      "page_count": 5,
      "order": 615,
      "p1": "3011",
      "pn": "3015",
      "abstract": [
        "The large amount of audiovisual content being shared online today has\ndrawn substantial attention to the prospect of audio-visual self-supervised\nlearning. Recent works have focused on each of these modalities separately,\nwhile others have attempted to model both simultaneously in a cross-modal\nfashion. However, comparatively little attention has been given to\nleveraging one modality as a training objective to learn from the other.\nIn this work, we propose Learning visual speech Representations from\nAudio via self-supervision (LiRA). Specifically, we train a ResNet+Conformer\nmodel to predict acoustic features from unlabelled visual speech. We\nfind that this pre-trained model can be leveraged towards word-level\nand sentence-level lip-reading through feature extraction and fine-tuning\nexperiments. We show that our approach significantly outperforms other\nself-supervised methods on the Lip Reading in the Wild (LRW) dataset\nand achieves state-of-the-art performance on Lip Reading Sentences\n2 (LRS2) using only a fraction of the total labelled data.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1360",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "rose21_interspeech": {
      "authors": [
        [
          "Richard",
          "Rose"
        ],
        [
          "Olivier",
          "Siohan"
        ],
        [
          "Anshuman",
          "Tripathi"
        ],
        [
          "Otavio",
          "Braga"
        ]
      ],
      "title": "End-to-End Audio-Visual Speech Recognition for Overlapping Speech",
      "original": "1621",
      "page_count": 5,
      "order": 616,
      "p1": "3016",
      "pn": "3020",
      "abstract": [
        "This paper investigates an end-to-end audio-visual (A/V) modeling approach\nfor transcribing utterances in scenarios where there are overlapping\nspeech utterances from multiple talkers. It assumes that overlapping\naudio signals and video signals in the form of mouth-tracks aligned\nwith speech are available for overlapping talkers. The approach builds\non previous work in audio-only multi-talker ASR. In that work, a conventional\nrecurrent neural network transducer (RNN-T) architecture was extended\nto include a masking model for separation of encoded audio features\nand multiple label encoders to encode transcripts from overlapping\nspeakers. It is shown here that incorporating an attention weighted\ncombination of visual features in A/V multi-talker RNN-T models significantly\nimproves speaker disambiguation in ASR on overlapping speech relative\nto audio-only performance. The A/V multi-talker ASR systems described\nhere are trained and evaluated on a two speaker A/V overlapping speech\ndataset created from YouTube videos. A 17% reduction in WER was observed\nfor A/V multi-talker models relative to audio-only multi-talker models.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1621",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "wu21e_interspeech": {
      "authors": [
        [
          "Yifei",
          "Wu"
        ],
        [
          "Chenda",
          "Li"
        ],
        [
          "Song",
          "Yang"
        ],
        [
          "Zhongqin",
          "Wu"
        ],
        [
          "Yanmin",
          "Qian"
        ]
      ],
      "title": "Audio-Visual Multi-Talker Speech Recognition in a Cocktail Party",
      "original": "2128",
      "page_count": 5,
      "order": 617,
      "p1": "3021",
      "pn": "3025",
      "abstract": [
        "Speech from microphones is vulnerable in a complex acoustic environment\ndue to noise and reverberation, while the cameras are not. Thus, utilizing\nthe visual modality in the &#8220;cocktail party&#8221; scenario with\nmulti-talkers has become a promising and popular approach. In this\npaper, we have explored the incorporating of visual modality into the\nend-to-end multi-talker speech recognition task. We propose two methods\nbased on the modality fusion position, which are encoder-based fusion\nand decoder-based fusion. And for each method, advanced audio-visual\nfusion techniques including attention mechanism and dual decoder have\nbeen explored to find the best usage of the visual modality. With the\nproposed methods, our best audio-visual multi-talker automatic speech\nrecognition (ASR) model gets almost &#126;50.0% word error rate (WER)\nreduction compared to the audio-only multi-talker ASR system.\n"
      ],
      "doi": "10.21437/Interspeech.2021-2128",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "chen21l_interspeech": {
      "authors": [
        [
          "Sanyuan",
          "Chen"
        ],
        [
          "Yu",
          "Wu"
        ],
        [
          "Zhuo",
          "Chen"
        ],
        [
          "Jian",
          "Wu"
        ],
        [
          "Takuya",
          "Yoshioka"
        ],
        [
          "Shujie",
          "Liu"
        ],
        [
          "Jinyu",
          "Li"
        ],
        [
          "Xiangzhan",
          "Yu"
        ]
      ],
      "title": "Ultra Fast Speech Separation Model with Teacher Student Learning",
      "original": "0142",
      "page_count": 5,
      "order": 618,
      "p1": "3026",
      "pn": "3030",
      "abstract": [
        "Transformer has been successfully applied to speech separation recently\nwith its strong long-dependency modeling capacity using a self-attention\nmechanism. However, Transformer tends to have heavy run-time costs\ndue to the deep encoder layers, which hinders its deployment on edge\ndevices. A small Transformer model with fewer encoder layers is preferred\nfor computational efficiency, but it is prone to performance degradation.\nIn this paper, an ultra fast speech separation Transformer model is\nproposed to achieve both better performance and efficiency with teacher\nstudent learning (T-S learning). We introduce layer-wise T-S learning\nand objective shifting mechanisms to guide the small student model\nto learn intermediate representations from the large teacher model.\nCompared with the small Transformer model trained from scratch, the\nproposed T-S learning method reduces the word error rate (WER) by more\nthan 5% for both multi-channel and single-channel speech separation\non LibriCSS dataset. Utilizing more unlabeled speech data, our ultra\nfast speech separation models achieve more than 10% relative WER reduction.\n"
      ],
      "doi": "10.21437/Interspeech.2021-142",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "ali21_interspeech": {
      "authors": [
        [
          "Murtiza",
          "Ali"
        ],
        [
          "Ashwani",
          "Koul"
        ],
        [
          "Karan",
          "Nathwani"
        ]
      ],
      "title": "Group Delay Based Re-Weighted Sparse Recovery Algorithms for Robust and High-Resolution Source Separation in DOA Framework",
      "original": "0164",
      "page_count": 5,
      "order": 619,
      "p1": "3031",
      "pn": "3035",
      "abstract": [
        "Sparse Recovery (SR) algorithms have been used widely for direction-of-arrival\n(DOA) estimation in spatially contiguous plane wave for their robust\nperformance. But these algorithms have proven to be computationally\ncostly. With a few sensors and at low SNRs, the noise dominates the\ndata singular vectors and the sparse estimation of contiguous sources\nis incorrect. The magnitude spectrum-based re-weighted sparse recovery\n(RWSR) algorithms improve the robustness by re-weighting the sparse\nestimates. However, their efficiency degrades with decreasing the number\nof sensors at low SNRs. Therefore, this paper exhibits the significance\nof the phase spectrum, in the form of group-delay, for sparse and robust\nsource estimation using RWSR algorithms for spatially contiguous sources.\nFurther, an optimal re-weighted methodology based on simultaneously\nminimizing average-root-mean-square-error and maximizing the probability\nof separation is also proposed. The simulation results are carried\nout for Gaussian noise to demonstrate the excellent performance of\nthe proposed algorithms.\n"
      ],
      "doi": "10.21437/Interspeech.2021-164",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "han21d_interspeech": {
      "authors": [
        [
          "Cong",
          "Han"
        ],
        [
          "Yi",
          "Luo"
        ],
        [
          "Chenda",
          "Li"
        ],
        [
          "Tianyan",
          "Zhou"
        ],
        [
          "Keisuke",
          "Kinoshita"
        ],
        [
          "Shinji",
          "Watanabe"
        ],
        [
          "Marc",
          "Delcroix"
        ],
        [
          "Hakan",
          "Erdogan"
        ],
        [
          "John R.",
          "Hershey"
        ],
        [
          "Nima",
          "Mesgarani"
        ],
        [
          "Zhuo",
          "Chen"
        ]
      ],
      "title": "Continuous Speech Separation Using Speaker Inventory for Long Recording",
      "original": "0338",
      "page_count": 5,
      "order": 620,
      "p1": "3036",
      "pn": "3040",
      "abstract": [
        "Leveraging additional speaker information to facilitate speech separation\nhas received increasing attention in recent years. Recent research\nincludes extracting target speech by using the target speaker&#8217;s\nvoice snippet and jointly separating all participating speakers by\nusing a pool of additional speaker signals, which is known as speech\nseparation using speaker inventory (SSUSI). However, all these systems\nideally assume that the pre-enrolled speaker signals are available\nand are only evaluated on simple data configurations. In realistic\nmulti-talker conversations, the speech signal contains a large proportion\nof non-overlapped regions, where we can derive robust speaker embedding\nof individual talkers. In this work, we adopt the SSUSI model in long\nrecordings and propose a self-informed, clustering-based inventory\nforming scheme for long recording, where the speaker inventory is fully\nbuilt from the input signal without the need for external speaker signals.\nExperiment results on simulated noisy reverberant long recording datasets\nshow that the proposed method can significantly improve the separation\nperformance across various conditions.\n"
      ],
      "doi": "10.21437/Interspeech.2021-338",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "yuan21_interspeech": {
      "authors": [
        [
          "Weitao",
          "Yuan"
        ],
        [
          "Shengbei",
          "Wang"
        ],
        [
          "Xiangrui",
          "Li"
        ],
        [
          "Masashi",
          "Unoki"
        ],
        [
          "Wenwu",
          "Wang"
        ]
      ],
      "title": "Crossfire Conditional Generative Adversarial Networks for Singing Voice Extraction",
      "original": "0433",
      "page_count": 5,
      "order": 621,
      "p1": "3041",
      "pn": "3045",
      "abstract": [
        "Generative adversarial networks (GANs) and Conditional GANs (cGANs)\nhave recently been applied for singing voice extraction (SVE), since\nthey can accurately model the vocal distributions and effectively utilize\na large amount of unlabelled datasets. However, current GANs/cGANs\nbased SVE frameworks have no explicit mechanism to eliminate the mutual\ninterferences between different sources. In this work, we introduce\na novel &#8216;crossfire&#8217; criterion into GANs to complement its\nstandard adversarial training, which forms a dual-objective GANs, namely\nCrossfire GANs (Cr-GANs). In addition, we design a Generalized Projection\nMethod (GPM) for cGANs based frameworks to extract more effective conditional\ninformation for SVE. Using the proposed GPM, we extend our Cr-GANs\nto conditional version, i.e., Crossfire Conditional GANs (Cr-cGANs).\nThe proposed methods were evaluated on the DSD100 and CCMixter datasets.\nThe numerical results have shown that the &#8216;crossfire&#8217; criterion\nand GPM are beneficial to each other and considerably improve the separation\nperformance of existing GANs/cGANs based SVE methods.\n"
      ],
      "doi": "10.21437/Interspeech.2021-433",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "wang21w_interspeech": {
      "authors": [
        [
          "Kai",
          "Wang"
        ],
        [
          "Hao",
          "Huang"
        ],
        [
          "Ying",
          "Hu"
        ],
        [
          "Zhihua",
          "Huang"
        ],
        [
          "Sheng",
          "Li"
        ]
      ],
      "title": "End-to-End Speech Separation Using Orthogonal Representation in Complex and Real Time-Frequency Domain",
      "original": "0504",
      "page_count": 5,
      "order": 622,
      "p1": "3046",
      "pn": "3050",
      "abstract": [
        "Traditional single channel speech separation in the time-frequency\n(T-F) domain often faces the problem of phase reconstruction. Due to\nthe fact that the real-valued network is not suitable for dealing with\ncomplex-valued representation, the performance of the T-F domain speech\nseparation method is often constrained from reaching the state-of-the-art.\nIn this paper, we propose improved speech separation methods in both\ncomplex and real T-F domain using orthogonal representation. For the\ncomplex-valued case, we combine the deep complex network (DCN) and\nConv-TasNet to design an end-to-end complex-valued model. Specifically,\nwe incorporate short-time Fourier transform (STFT) and learnable complex\nlayers to build a hybrid encoder-decoder structure, and use a DCN based\nseparator. Then we present the importance of weights orthogonality\nin the T-F domain transformation and propose a multi-segment orthogonality\n(MSO) architecture for further improvements. For the real-valued case,\nwe performed separation in real T-F domain by introducing the short-time\nDCT (STDCT) with orthogonal representation as well. Experimental results\nshow that the proposed complex model outperforms the baseline Conv-TasNet\nwith a comparable parameter size by 1.8 dB, and the STDCT-based real-valued\nT-F model by 1.2 dB, showing the advantages of speech separation in\nthe T-F domain.\n"
      ],
      "doi": "10.21437/Interspeech.2021-504",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "nakagome21_interspeech": {
      "authors": [
        [
          "Yu",
          "Nakagome"
        ],
        [
          "Masahito",
          "Togami"
        ],
        [
          "Tetsuji",
          "Ogawa"
        ],
        [
          "Tetsunori",
          "Kobayashi"
        ]
      ],
      "title": "Efficient and Stable Adversarial Learning Using Unpaired Data for Unsupervised Multichannel Speech Separation",
      "original": "0523",
      "page_count": 5,
      "order": 623,
      "p1": "3051",
      "pn": "3055",
      "abstract": [
        "This study presents a framework to enable efficient and stable adversarial\nlearning of unsupervised multichannel source separation models. When\nthe paired data, i.e., the mixture and the corresponding clean speech,\nare not available for training, it is promising to exploit generative\nadversarial networks (GANs), where a source separation system is treated\nas a generator and trained to bring the distribution of the separated\n(fake) speech closer to that of the clean (real) speech. The separated\nspeech, however, contains many errors, especially when the system is\ntrained unsupervised and can be easily distinguished from the clean\nspeech. A real/fake binary discriminator therefore will stop the adversarial\nlearning process unreasonably early. This study aims to balance the\nconvergence of the generator and discriminator to achieve efficient\nand stable learning. For that purpose, the autoencoder-based discriminator\nand more stable adversarial loss, which are designed in boundary equilibrium\nGAN (BEGAN), are introduced. In addition, generator-specific distortions\nare added to real examples so that the models can be trained to focus\nonly on source separation. Experimental comparisons demonstrated that\nthe present stabilizing learning techniques improved the performance\nof multiple unsupervised source separation systems.\n"
      ],
      "doi": "10.21437/Interspeech.2021-523",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "huang21h_interspeech": {
      "authors": [
        [
          "Sung-Feng",
          "Huang"
        ],
        [
          "Shun-Po",
          "Chuang"
        ],
        [
          "Da-Rong",
          "Liu"
        ],
        [
          "Yi-Chen",
          "Chen"
        ],
        [
          "Gene-Ping",
          "Yang"
        ],
        [
          "Hung-yi",
          "Lee"
        ]
      ],
      "title": "Stabilizing Label Assignment for Speech Separation by Self-Supervised Pre-Training",
      "original": "0763",
      "page_count": 5,
      "order": 624,
      "p1": "3056",
      "pn": "3060",
      "abstract": [
        "Speech separation has been well developed, with the very successful\npermutation invariant training (PIT) approach, although the frequent\nlabel assignment switching happening during PIT training remains to\nbe a problem when better convergence speed and achievable performance\nare desired. In this paper, we propose to perform self-supervised pre-training\nto stabilize the label assignment in training the speech separation\nmodel. Experiments over several types of self-supervised approaches,\nseveral typical speech separation models and two different datasets\nshowed that very good improvements are achievable if a proper self-supervised\napproach is chosen.\n"
      ],
      "doi": "10.21437/Interspeech.2021-763",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "wang21x_interspeech": {
      "authors": [
        [
          "Fan-Lin",
          "Wang"
        ],
        [
          "Yu-Huai",
          "Peng"
        ],
        [
          "Hung-Shin",
          "Lee"
        ],
        [
          "Hsin-Min",
          "Wang"
        ]
      ],
      "title": "Dual-Path Filter Network: Speaker-Aware Modeling for Speech Separation",
      "original": "0858",
      "page_count": 5,
      "order": 625,
      "p1": "3061",
      "pn": "3065",
      "abstract": [
        "Speech separation has been extensively studied to deal with the cocktail\nparty problem in recent years. All related approaches can be divided\ninto two categories: time-frequency domain methods and time domain\nmethods. In addition, some methods try to generate speaker vectors\nto support source separation. In this study, we propose a new model\ncalled dual-path filter network (DPFN). Our model focuses on the post-processing\nof speech separation to improve speech separation performance. DPFN\nis composed of two parts: the speaker module and the separation module.\nFirst, the speaker module infers the identities of the speakers. Then,\nthe separation module uses the speakers&#8217; information to extract\nthe voices of individual speakers from the mixture. DPFN constructed\nbased on DPRNN-TasNet is not only superior to DPRNN-TasNet, but also\navoids the problem of permutation-invariant training (PIT).\n"
      ],
      "doi": "10.21437/Interspeech.2021-858",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "wu21f_interspeech": {
      "authors": [
        [
          "Jian",
          "Wu"
        ],
        [
          "Zhuo",
          "Chen"
        ],
        [
          "Sanyuan",
          "Chen"
        ],
        [
          "Yu",
          "Wu"
        ],
        [
          "Takuya",
          "Yoshioka"
        ],
        [
          "Naoyuki",
          "Kanda"
        ],
        [
          "Shujie",
          "Liu"
        ],
        [
          "Jinyu",
          "Li"
        ]
      ],
      "title": "Investigation of Practical Aspects of Single Channel Speech Separation for ASR",
      "original": "0921",
      "page_count": 5,
      "order": 626,
      "p1": "3066",
      "pn": "3070",
      "abstract": [
        "Speech separation has been successfully applied as a front-end processing\nmodule of conversation transcription systems thanks to its ability\nto handle overlapped speech and its flexibility to combine with downstream\ntasks such as automatic speech recognition (ASR). However, a speech\nseparation model often introduces target speech distortion, resulting\nin a sub-optimum word error rate (WER). In this paper, we describe\nour efforts to improve the performance of a single channel speech separation\nsystem. Specifically, we investigate a two-stage training scheme that\nfirstly applies a feature level optimization criterion for pre-training,\nfollowed by an ASR-oriented optimization criterion using an end-to-end\n(E2E) speech recognition model. Meanwhile, to keep the model light-weight,\nwe introduce a modified teacher-student learning technique for model\ncompression. By combining those approaches, we achieve a absolute average\nWER improvement of 2.70% and 0.77% using models with less than 10M\nparameters compared with the previous state-of-the-art results on the\nLibriCSS dataset for utterance-wise evaluation and continuous evaluation,\nrespectively.\n"
      ],
      "doi": "10.21437/Interspeech.2021-921",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "luo21c_interspeech": {
      "authors": [
        [
          "Yi",
          "Luo"
        ],
        [
          "Nima",
          "Mesgarani"
        ]
      ],
      "title": "Implicit Filter-and-Sum Network for End-to-End Multi-Channel Speech Separation",
      "original": "1158",
      "page_count": 5,
      "order": 627,
      "p1": "3071",
      "pn": "3075",
      "abstract": [
        "Various neural network architectures have been proposed in recent years\nfor the task of multi-channel speech separation. Among them, the filter-and-sum\nnetwork (FaSNet) performs end-to-end time-domain filter-and-sum beamforming\nand has shown effective in both ad-hoc and fixed microphone array geometries.\nHowever, whether such explicit beamforming operation is a necessary\nand valid formulation remains unclear. In this paper, we investigate\nthe beamforming operation and show that it is not necessary. To further\nimprove the performance, we change the explicit waveform-level filter-and-sum\noperation into an implicit feature-level filter-and-sum operation around\na context of features. A feature-level normalized cross correlation\n(fNCC) feature is also proposed to better match the implicit operation\nfor an improved performance. Experiment results on a simulated ad-hoc\nmicrophone array dataset show that the proposed modification to the\nFaSNet, which we refer to as the implicit filter-and-sum network (iFaSNet),\nachieve better performance than the explicit FaSNet with a similar\nmodel size and a faster training and inference speed.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1158",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "xu21i_interspeech": {
      "authors": [
        [
          "Yong",
          "Xu"
        ],
        [
          "Zhuohuang",
          "Zhang"
        ],
        [
          "Meng",
          "Yu"
        ],
        [
          "Shi-Xiong",
          "Zhang"
        ],
        [
          "Dong",
          "Yu"
        ]
      ],
      "title": "Generalized Spatio-Temporal RNN Beamformer for Target Speech Separation",
      "original": "0430",
      "page_count": 5,
      "order": 628,
      "p1": "3076",
      "pn": "3080",
      "abstract": [
        "Although the conventional mask-based minimum variance distortionless\nresponse (MVDR) could reduce the non-linear distortion, the residual\nnoise level of the MVDR separated speech is still high. In this paper,\nwe propose a spatio-temporal recurrent neural network based beamformer\n(RNN-BF) for target speech separation. This new beamforming framework\ndirectly learns the beamforming weights from the estimated speech and\nnoise spatial covariance matrices. Leveraging on the temporal modeling\ncapability of RNNs, the RNN-BF could automatically accumulate the statistics\nof the speech and noise covariance matrices to learn the frame-level\nbeamforming weights in a recursive way. An RNN-based generalized eigenvalue\n(RNN-GEV) beamformer and a more generalized RNN beamformer (GRNN-BF)\nare proposed. We further improve the RNN-GEV and the GRNN-BF by using\nlayer normalization to replace the commonly used mask normalization\non the covariance matrices. The proposed GRNN-BF obtains better performance\nagainst prior arts in terms of speech quality (PESQ), speech-to-noise\nratio (SNR) and word error rate (WER).\n"
      ],
      "doi": "10.21437/Interspeech.2021-430",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "liu21j_interspeech": {
      "authors": [
        [
          "Yi Chieh",
          "Liu"
        ],
        [
          "Eunjung",
          "Han"
        ],
        [
          "Chul",
          "Lee"
        ],
        [
          "Andreas",
          "Stolcke"
        ]
      ],
      "title": "End-to-End Neural Diarization: From Transformer to Conformer",
      "original": "1909",
      "page_count": 5,
      "order": 629,
      "p1": "3081",
      "pn": "3085",
      "abstract": [
        "We propose a new end-to-end neural diarization (EEND) system that is\nbased on Conformer, a recently proposed neural architecture that combines\nconvolutional mappings and Transformer to model both local and global\ndependencies in speech. We first show that data augmentation and convolutional\nsubsampling layers enhance the original self-attentive EEND in the\nTransformer-based EEND, and then Conformer gives an additional gain\nover the Transformer-based EEND. However, we notice that the Conformer-based\nEEND does not generalize as well from simulated to real conversation\ndata as the Transformer-based model. This leads us to quantify the\nmismatch between simulated data and real speaker behavior in terms\nof temporal statistics reflecting turn-taking between speakers, and\ninvestigate its correlation with diarization error. By mixing simulated\nand real data in EEND training, we mitigate the mismatch further, with\nConformer-based EEND achieving 24% error reduction over the baseline\nSA-EEND system, and 10% improvement over the best augmented Transformer-based\nsystem, on two-speaker CALLHOME data.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1909",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "jung21_interspeech": {
      "authors": [
        [
          "Jee-weon",
          "Jung"
        ],
        [
          "Hee-Soo",
          "Heo"
        ],
        [
          "Youngki",
          "Kwon"
        ],
        [
          "Joon Son",
          "Chung"
        ],
        [
          "Bong-Jin",
          "Lee"
        ]
      ],
      "title": "Three-Class Overlapped Speech Detection Using a Convolutional Recurrent Neural Network",
      "original": "0149",
      "page_count": 5,
      "order": 630,
      "p1": "3086",
      "pn": "3090",
      "abstract": [
        "In this work, we propose an overlapped speech detection system trained\nas a three-class classifier. Unlike conventional systems that perform\nbinary classification as to whether or not a frame contains overlapped\nspeech, the proposed approach classifies into three classes: non-speech,\nsingle speaker speech, and overlapped speech. By training a network\nwith the more detailed label definition, the model can learn a better\nnotion on deciding the number of speakers included in a given frame.\nA convolutional recurrent neural network architecture is explored to\nbenefit from both convolutional layer&#8217;s capability to model local\npatterns and recurrent layer&#8217;s ability to model sequential information.\nThe proposed overlapped speech detection model establishes a state-of-the-art\nperformance with a precision of 0.6648 and a recall of 0.3222 on the\nDIHARD II evaluation set, showing a 20% increase in recall along with\nhigher precision. In addition, we also introduce a simple approach\nto utilize the proposed overlapped speech detection model for speaker\ndiarization which ranked third place in the Track 1 of the DIHARD III\nchallenge.\n"
      ],
      "doi": "10.21437/Interspeech.2021-149",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "wan21_interspeech": {
      "authors": [
        [
          "Xucheng",
          "Wan"
        ],
        [
          "Kai",
          "Liu"
        ],
        [
          "Huan",
          "Zhou"
        ]
      ],
      "title": "Online Speaker Diarization Equipped with Discriminative Modeling and Guided Inference",
      "original": "0261",
      "page_count": 5,
      "order": 631,
      "p1": "3091",
      "pn": "3095",
      "abstract": [
        "Despite considerable efforts, online speaker diarization remains an\nongoing challenge. In this study, we propose to tackle the challenge\nfrom two perspectives, to endow diarization model with discriminability\nand to rectify less-reliable online inference with guidance. Specifically,\nbased on the current prior art, UIS-RNN, two enhancement approaches\nare proposed to concretize our motivations. The effectiveness of our\nproposals is experimentally validated by results on the AMI evaluation\nset. With substantial relative improvement of 48.7%, our online speaker\ndiarization system significantly outperformed its baseline. More impressively,\nits performance in terms of diarization error rate is better than most\nstate-of-the-art offline systems.\n"
      ],
      "doi": "10.21437/Interspeech.2021-261",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "takashima21_interspeech": {
      "authors": [
        [
          "Yuki",
          "Takashima"
        ],
        [
          "Yusuke",
          "Fujita"
        ],
        [
          "Shota",
          "Horiguchi"
        ],
        [
          "Shinji",
          "Watanabe"
        ],
        [
          "Leibny Paola Garc\u00eda",
          "Perera"
        ],
        [
          "Kenji",
          "Nagamatsu"
        ]
      ],
      "title": "Semi-Supervised Training with Pseudo-Labeling for End-To-End Neural Diarization",
      "original": "0384",
      "page_count": 5,
      "order": 632,
      "p1": "3096",
      "pn": "3100",
      "abstract": [
        "In this paper, we present a semi-supervised training technique using\npseudo-labeling for end-to-end neural diarization (EEND). The EEND\nsystem has shown promising performance compared with traditional clustering-based\nmethods, especially in the case of overlapping speech. However, to\nget a well-tuned model, EEND requires labeled data for all the joint\nspeech activities of every speaker at each time frame in a recording.\nIn this paper, we explore a pseudo-labeling approach that employs unlabeled\ndata. First, we propose an iterative pseudo-label method for EEND,\nwhich trains the model using unlabeled data of a target condition.\nThen, we also propose a committee-based training method to improve\nthe performance of EEND. To evaluate our proposed method, we conduct\nthe experiments of model adaptation using labeled and unlabeled data.\nExperimental results on the CALLHOME dataset show that our proposed\npseudo-label achieved a 37.4% relative diarization error rate reduction\ncompared to a seed model. Moreover, we analyzed the results of semi-supervised\nadaptation with pseudo-labeling. We also show the effectiveness of\nour approach on the third DIHARD dataset.\n"
      ],
      "doi": "10.21437/Interspeech.2021-384",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "kwon21b_interspeech": {
      "authors": [
        [
          "Youngki",
          "Kwon"
        ],
        [
          "Jee-weon",
          "Jung"
        ],
        [
          "Hee-Soo",
          "Heo"
        ],
        [
          "You Jin",
          "Kim"
        ],
        [
          "Bong-Jin",
          "Lee"
        ],
        [
          "Joon Son",
          "Chung"
        ]
      ],
      "title": "Adapting Speaker Embeddings for Speaker Diarisation",
      "original": "0448",
      "page_count": 5,
      "order": 633,
      "p1": "3101",
      "pn": "3105",
      "abstract": [
        "The goal of this paper is to adapt speaker embeddings for solving the\nproblem of speaker diarisation. The quality of speaker embeddings is\nparamount to the performance of speaker diarisation systems. Despite\nthis, prior works in the field have directly used embeddings designed\nonly to be effective on the speaker verification task. In this paper,\nwe propose three techniques that can be used to better adapt the speaker\nembeddings for diarisation: dimensionality reduction, attention-based\nembedding aggregation, and non-speech clustering. A wide range of experiments\nis performed on various challenging datasets. The results demonstrate\nthat all three techniques contribute positively to the performance\nof the diarisation system achieving an average relative improvement\nof 25.07% in terms of diarisation error rate over the baseline.\n"
      ],
      "doi": "10.21437/Interspeech.2021-448"
    },
    "wang21y_interspeech": {
      "authors": [
        [
          "Yu-Xuan",
          "Wang"
        ],
        [
          "Jun",
          "Du"
        ],
        [
          "Maokui",
          "He"
        ],
        [
          "Shu-Tong",
          "Niu"
        ],
        [
          "Lei",
          "Sun"
        ],
        [
          "Chin-Hui",
          "Lee"
        ]
      ],
      "title": "Scenario-Dependent Speaker Diarization for DIHARD-III Challenge",
      "original": "0516",
      "page_count": 5,
      "order": 634,
      "p1": "3106",
      "pn": "3110",
      "abstract": [
        "In this study, we propose a scenario-dependent speaker diarization\napproach to handling the diversified scenarios of 11 domains encountered\nin DIHARD-III challenge with a divide-and-conquer strategy. First,\nusing a ResNet-based audio domain classifier, all domains in DIHARD-III\nchallenge could be divided into several scenarios by different impact\nfactors, such as background noise level, speaker number, and speaker\noverlap ratio. In each scenario, different combinations of techniques\nare designed, aiming at achieving the best performance in terms of\nboth diarization error rate (DER) and run-time efficiency. For low\nsignal-to-noise-ration (SNR) scenarios, speech enhancement based on\na progressive learning network with multiple intermediate SNR targets\nis adopted for pre-processing. Conventional clustering-based speaker\ndiarization is utilized to mainly handle speech segments with non-overlapping\nspeakers, while separation-based or neural speaker diarization is used\nto cope with the overlapping speech regions, which is combined with\nan iterative fine-tuning strategy to boost the generalization ability.\nWe also explore post-processing to perform system fusion and selection.\nFor DIHARD-III challenge, our scenario-dependent system won the first\nplace among all submitted systems, and significantly outperforms the\nstate-of-the-art clustering-based speaker diarization system, yielding\nrelative DER reductions of 32.17% and 28.34% on development set and\nevaluation set on Track 1, respectively.\n"
      ],
      "doi": "10.21437/Interspeech.2021-516",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "bredin21_interspeech": {
      "authors": [
        [
          "Herv\u00e9",
          "Bredin"
        ],
        [
          "Antoine",
          "Laurent"
        ]
      ],
      "title": "End-To-End Speaker Segmentation for Overlap-Aware Resegmentation",
      "original": "0560",
      "page_count": 5,
      "order": 635,
      "p1": "3111",
      "pn": "3115",
      "abstract": [
        "Speaker segmentation consists in partitioning a conversation between\none or more speakers into speaker turns. Usually addressed as the late\ncombination of three sub-tasks (voice activity detection, speaker change\ndetection, and overlapped speech detection), we propose to train an\nend-to-end segmentation model that does it directly. Inspired by the\noriginal end-to-end neural speaker diarization approach (EEND), the\ntask is modeled as a multi-label classification problem using permutation-invariant\ntraining. The main difference is that our model operates on short audio\nchunks (5 seconds) but at a much higher temporal resolution (every\n16ms). Experiments on multiple speaker diarization datasets conclude\nthat our model can be used with great success on both voice activity\ndetection and overlapped speech detection. Our proposed model can also\nbe used as a post-processing step, to detect and correctly assign overlapped\nspeech regions. Relative diarization error rate improvement over the\nbest considered baseline (VBx) reaches 17% on AMI, 13% on DIHARD 3,\nand 13% on VoxConverse.\n"
      ],
      "doi": "10.21437/Interspeech.2021-560",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "xue21d_interspeech": {
      "authors": [
        [
          "Yawen",
          "Xue"
        ],
        [
          "Shota",
          "Horiguchi"
        ],
        [
          "Yusuke",
          "Fujita"
        ],
        [
          "Yuki",
          "Takashima"
        ],
        [
          "Shinji",
          "Watanabe"
        ],
        [
          "Leibny Paola Garc\u00eda",
          "Perera"
        ],
        [
          "Kenji",
          "Nagamatsu"
        ]
      ],
      "title": "Online Streaming End-to-End Neural Diarization Handling Overlapping Speech and Flexible Numbers of Speakers",
      "original": "0708",
      "page_count": 5,
      "order": 636,
      "p1": "3116",
      "pn": "3120",
      "abstract": [
        "We propose a streaming diarization method based on an end-to-end neural\ndiarization (EEND) model, which handles flexible numbers of speakers\nand overlapping speech. In our previous study, the speaker-tracing\nbuffer (STB) mechanism was proposed to achieve a chunk-wise streaming\ndiarization using a pre-trained EEND model. STB traces the speaker\ninformation in previous chunks to map the speakers in a new chunk.\nHowever, it only worked with two-speaker recordings. In this paper,\nwe propose an extended STB for flexible numbers of speakers, FLEX-STB.\nThe proposed method uses a zero-padding followed by speaker-tracing,\nwhich alleviates the difference in the number of speakers between a\nbuffer and a current chunk. We also examine buffer update strategies\nto select important frames for tracing multiple speakers. Experiments\non CALLHOME and DIHARD II datasets show that the proposed method achieves\ncomparable performance to the offline EEND method with 1-second latency.\nThe results also show that our proposed method outperforms recently\nproposed chunk-wise diarization methods based on EEND (BW-EDA-EEND).\n"
      ],
      "doi": "10.21437/Interspeech.2021-708",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "anidjar21_interspeech": {
      "authors": [
        [
          "Or Haim",
          "Anidjar"
        ],
        [
          "Itshak",
          "Lapidot"
        ],
        [
          "Chen",
          "Hajaj"
        ],
        [
          "Amit",
          "Dvir"
        ]
      ],
      "title": "A Thousand Words are Worth More Than One Recording: <i>Word-Embedding</i> Based Speaker Change Detection",
      "original": "0087",
      "page_count": 5,
      "order": 637,
      "p1": "3121",
      "pn": "3125",
      "abstract": [
        "Speaker Change Detection (SCD) is the task of segmenting an input audio-recording\naccording to speaker interchanges. This task is essential for many\napplications, such as automatic voice transcription or Speaker Diarization\n(SD). This paper focuses on the essential task of audio segmentation\nand suggests a word-embedding-based solution for the SCD problem. Moreover,\nwe show how to use our approach in order to outperform voice-based\nsolutions for the SD problem. We empirically show that our method can\naccurately identify the speaker-turns in an audio-recording with 82.12%\nand 89.02% success in the Recall and F1-score measures.\n"
      ],
      "doi": "10.21437/Interspeech.2021-87"
    },
    "futamata21_interspeech": {
      "authors": [
        [
          "Kosuke",
          "Futamata"
        ],
        [
          "Byeongseon",
          "Park"
        ],
        [
          "Ryuichi",
          "Yamamoto"
        ],
        [
          "Kentaro",
          "Tachibana"
        ]
      ],
      "title": "Phrase Break Prediction with Bidirectional Encoder Representations in Japanese Text-to-Speech Synthesis",
      "original": "0252",
      "page_count": 5,
      "order": 638,
      "p1": "3126",
      "pn": "3130",
      "abstract": [
        "We propose a novel phrase break prediction method that combines implicit\nfeatures extracted from a pre-trained large language model, a.k.a BERT,\nand explicit features extracted from BiLSTM with linguistic features.\nIn conventional BiLSTM-based methods, word representations and/or sentence\nrepresentations are used as independent components. The proposed method\ntakes account of both representations to extract the latent semantics,\nwhich cannot be captured by previous methods. The objective evaluation\nresults show that the proposed method obtains an absolute improvement\nof 3.2 points for the F1 score compared with BiLSTM-based conventional\nmethods using linguistic features. Moreover, the perceptual listening\ntest results verify that a TTS system that applied our proposed method\nachieved a mean opinion score of 4.39 in prosody naturalness, which\nis highly competitive with the score of 4.37 for synthesized speech\nwith ground-truth phrase breaks.\n"
      ],
      "doi": "10.21437/Interspeech.2021-252",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "vallesperez21_interspeech": {
      "authors": [
        [
          "Iv\u00e1n",
          "Vall\u00e9s-P\u00e9rez"
        ],
        [
          "Julian",
          "Roth"
        ],
        [
          "Grzegorz",
          "Beringer"
        ],
        [
          "Roberto",
          "Barra-Chicote"
        ],
        [
          "Jasha",
          "Droppo"
        ]
      ],
      "title": "Improving Multi-Speaker TTS Prosody Variance with a Residual Encoder and Normalizing Flows",
      "original": "0562",
      "page_count": 5,
      "order": 639,
      "p1": "3131",
      "pn": "3135",
      "abstract": [
        "Text-to-speech systems recently achieved almost indistinguishable quality\nfrom human speech. However, the prosody of those systems is generally\nflatter than natural speech, producing samples with low expressiveness.\nDisentanglement of speaker id and prosody is crucial in text-to-speech\nsystems to improve on naturalness and produce more variable syntheses.\nThis paper proposes a new neural text-to-speech model that approaches\nthe disentanglement problem by conditioning a <i>Tacotron2</i>-like\narchitecture on flow-normalized speaker embeddings, and by substituting\nthe reference encoder with a new learned latent distribution responsible\nfor modeling the intra-sentence variability due to the prosody. By\nremoving the reference encoder dependency, the speaker-leakage problem\ntypically happening in this kind of systems disappears, producing more\ndistinctive syntheses at inference time. The new model achieves significantly\nhigher prosody variance than the baseline in a set of quantitative\nprosody features, as well as higher speaker distinctiveness, without\ndecreasing the speaker intelligibility. Finally, we observe that the\nnormalized speaker embeddings enable much richer speaker interpolations,\nsubstantially improving the distinctiveness of the new interpolated\nspeakers.\n"
      ],
      "doi": "10.21437/Interspeech.2021-562",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "du21b_interspeech": {
      "authors": [
        [
          "Chenpeng",
          "Du"
        ],
        [
          "Kai",
          "Yu"
        ]
      ],
      "title": "Rich Prosody Diversity Modelling with Phone-Level Mixture Density Network",
      "original": "0802",
      "page_count": 5,
      "order": 640,
      "p1": "3136",
      "pn": "3140",
      "abstract": [
        "Generating natural speech with a diverse and smooth prosody pattern\nis a challenging task. Although random sampling with phone-level prosody\ndistribution has been investigated to generate different prosody patterns,\nthe diversity of the generated speech is still very limited and far\nfrom what can be achieved by humans. This is largely due to the use\nof uni-modal distribution, such as single Gaussian, in the prior works\nof phone-level prosody modelling. In this work, we propose a novel\napproach that models phone-level prosodies with GMM based mixture density\nnetwork (GMM-MDN). Experiments on the LJSpeech dataset demonstrate\nthat phone-level prosodies can precisely control the synthetic speech\nand GMM-MDN can generate a more natural and smooth prosody pattern\nthan a single Gaussian. Subjective evaluations further show that the\nproposed approach not only achieves better naturalness, but also significantly\nimproves the prosody diversity in synthetic speech without the need\nof manual control.\n"
      ],
      "doi": "10.21437/Interspeech.2021-802",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "fujita21_interspeech": {
      "authors": [
        [
          "Kenichi",
          "Fujita"
        ],
        [
          "Atsushi",
          "Ando"
        ],
        [
          "Yusuke",
          "Ijima"
        ]
      ],
      "title": "Phoneme Duration Modeling Using Speech Rhythm-Based Speaker Embeddings for Multi-Speaker Speech Synthesis",
      "original": "0826",
      "page_count": 5,
      "order": 641,
      "p1": "3141",
      "pn": "3145",
      "abstract": [
        "This paper proposes a novel speech-rhythm-based method for speaker\nembeddings. Conventionally spectral feature-based speaker embedding\nvectors such as the x-vector are used as auxiliary information for\nmulti-speaker speech synthesis. However, speech synthesis with conventional\nembeddings has difficulty reproducing the target speaker&#8217;s speech\nrhythm, one of the important factors among speaker characteristics,\nbecause spectral features do not explicitly include speech rhythm.\nIn this paper, speaker embeddings that take speech rhythm information\ninto account are introduced to achieve phoneme duration modeling using\na few utterances by the target speaker. A novel point of the proposed\nmethod is that rhythm-based embeddings are extracted with phonemes\nand their durations. They are extracted with a speaker identification\nmodel similar to the conventional spectral feature-based one. We conducted\ntwo experiments: speaker embeddings generation and speech synthesis\nwith generated embeddings. We show that the proposed model has an EER\nof 10.3% in speaker identification even with only speech rhythm. Visualizing\nthe embeddings shows that utterances with similar rhythms are also\nsimilar in their speaker embeddings. The results of an objective and\nsubjective evaluation on speech synthesis demonstrate that the proposed\nmethod can synthesize speech with speech rhythm closer to the target\nspeaker.\n"
      ],
      "doi": "10.21437/Interspeech.2021-826",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "zou21_interspeech": {
      "authors": [
        [
          "Yuxiang",
          "Zou"
        ],
        [
          "Shichao",
          "Liu"
        ],
        [
          "Xiang",
          "Yin"
        ],
        [
          "Haopeng",
          "Lin"
        ],
        [
          "Chunfeng",
          "Wang"
        ],
        [
          "Haoyu",
          "Zhang"
        ],
        [
          "Zejun",
          "Ma"
        ]
      ],
      "title": "Fine-Grained Prosody Modeling in Neural Speech Synthesis Using ToBI Representation",
      "original": "0883",
      "page_count": 5,
      "order": 642,
      "p1": "3146",
      "pn": "3150",
      "abstract": [
        "Benefiting from the great development of deep learning, modern neural\ntext-to-speech (TTS) models can generate speech indistinguishable from\nnatural speech. However, The generated utterances often keep an average\nprosodic style of the database instead of having rich prosodic variation.\nFor pitch-stressed languages, such as English, accurate intonation\nand stress are important for conveying semantic information. In this\nwork, we propose a fine-grained prosody modeling method in neural speech\nsynthesis with ToBI (Tones and Break Indices) representation. The proposed\nsystem consists of a text frontend for ToBI prediction and a Tacotron-based\nTTS module for prosody modeling. By introducing the ToBI representation,\nwe can control the system to synthesize speech with accurate intonation\nand stress at syllable level. Compared with the two baselines (Tacotron\nand unsupervised method), experiments show that our model can generate\nmore natural speech with more accurate prosody, as well as effectively\ncontrol the stress, intonation, and pause of the speech.\n"
      ],
      "doi": "10.21437/Interspeech.2021-883",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "sharma21b_interspeech": {
      "authors": [
        [
          "Mayank",
          "Sharma"
        ],
        [
          "Yogesh",
          "Virkar"
        ],
        [
          "Marcello",
          "Federico"
        ],
        [
          "Roberto",
          "Barra-Chicote"
        ],
        [
          "Robert",
          "Enyedi"
        ]
      ],
      "title": "Intra-Sentential Speaking Rate Control in Neural Text-To-Speech for Automatic Dubbing",
      "original": "1012",
      "page_count": 5,
      "order": 643,
      "p1": "3151",
      "pn": "3155",
      "abstract": [
        "Automatically dubbed speech of a video involves: (i) segmenting the\ntarget sentences into phrases to reflect the speech-pause arrangement\nused by the original speaker, and (ii) adjusting the speaking rate\nof the synthetic voice at the phrase-level to match the exact timing\nof each corresponding source phrase. In this work, we investigate a\npost-segmentation approach to control the speaking rate of neural Text-to-Speech\n(TTS) at the phrase-level after generating the entire sentence. Our\npost-segmentation method relies on the attention matrix generated by\nthe context generation step to perform a force-alignment over pause\nmarkers inserted in the input text. We show that: (i) our approach\ncan be more accurate than applying an off-the-shelf forced aligner,\nand (ii) post-segmentation method permits generation more fluent speech\nthan pre-segmentation approach described in [1].\n"
      ],
      "doi": "10.21437/Interspeech.2021-1012",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "zhang21u_interspeech": {
      "authors": [
        [
          "Guangyan",
          "Zhang"
        ],
        [
          "Ying",
          "Qin"
        ],
        [
          "Daxin",
          "Tan"
        ],
        [
          "Tan",
          "Lee"
        ]
      ],
      "title": "Applying the Information Bottleneck Principle to Prosodic Representation Learning",
      "original": "1049",
      "page_count": 5,
      "order": 644,
      "p1": "3156",
      "pn": "3160",
      "abstract": [
        "This paper describes a novel design of a neural network-based speech\ngeneration model for learning prosodic representation. The problem\nof representation learning is formulated according to the information\nbottleneck (IB) principle. A modified VQ-VAE quantized layer is incorporated\nin the speech generation model to control the IB capacity and adjust\nthe balance between reconstruction power and disentangle capability\nof the learned representation. The proposed model is able to learn\nword-level prosodic representations from speech data. With an optimized\nIB capacity, the learned representations not only are adequate to reconstruct\nthe original speech but also can be used to transfer the prosody onto\ndifferent textual content. Extensive results of the objective and subjective\nevaluation are presented to demonstrate the effect of IB capacity control,\nthe effectiveness, and potential usage of the learned prosodic representation\nin controllable neural speech generation.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1049",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "baird21_interspeech": {
      "authors": [
        [
          "Alice",
          "Baird"
        ],
        [
          "Silvan",
          "Mertes"
        ],
        [
          "Manuel",
          "Milling"
        ],
        [
          "Lukas",
          "Stappen"
        ],
        [
          "Thomas",
          "Wiest"
        ],
        [
          "Elisabeth",
          "Andr\u00e9"
        ],
        [
          "Bj\u00f6rn W.",
          "Schuller"
        ]
      ],
      "title": "A Prototypical Network Approach for Evaluating Generated Emotional Speech",
      "original": "1123",
      "page_count": 5,
      "order": 645,
      "p1": "3161",
      "pn": "3165",
      "abstract": [
        "The collection of emotional speech data is a time-consuming and costly\nendeavour. Generative networks can be applied to augment the limited\naudio data artificially. However, it is challenging to evaluate generated\naudio for its similarity to source data, as current quantitative metrics\nare not necessarily suited to the audio domain. We explore the use\nof a prototypical network to evaluate four classes of generated emotional\naudio with this in mind. We first extract spectrogram images from \nWaveGan generated audio and other audio augmentation approaches, comparing\nsimilarity to the class prototype and diversity within the embedding\nspace. Furthermore, we augment the source training set with each augmentation\ntype and perform a classification to explore the generated audio plausibility.\nResults suggest that quality and diversity can be quantitatively observed\nwith this approach. In the chosen context, we see that  WaveGan generated\ndata is recognisable as a source data class (F<SUB>1</SUB>-score 43.6%),\nand the samples add similar diversity as unseen source data. This result\nleads to more plausible data for augmentation of the source training\nset &#8212; achieving up to 63.9% F<SUB>1</SUB> which is a 3.5% improvement\nover the source data baseline.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1123",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation:14 Special Sessions"
    },
    "yoshinaga21_interspeech": {
      "authors": [
        [
          "Tsukasa",
          "Yoshinaga"
        ],
        [
          "Kohei",
          "Tada"
        ],
        [
          "Kazunori",
          "Nozaki"
        ],
        [
          "Akiyoshi",
          "Iida"
        ]
      ],
      "title": "A Simplified Model for the Vocal Tract of [s] with Inclined Incisors",
      "original": "0231",
      "page_count": 5,
      "order": 646,
      "p1": "3166",
      "pn": "3170",
      "abstract": [
        "To examine the effects of inclined incisors on the phonation of [s],\na simplified vocal tract model is proposed, and the acoustic characteristics\nwith different maxillary incisor angles are predicted by the model.\nAs a control model, a realistic vocal tract replica of [s] was constructed\nfrom medical images, and the angle of the maxillary incisor was changed\nfrom the original position up to 30&#176;. The simplified model was\nconstructed with a rectangular flow channel using the average dimensions\nof the vocal tracts for five Japanese subjects. Both geometries were\nset in an anechoic chamber, and sounds generated from the geometries\nwere recorded with a microphone. The results showed that amplitudes\nof the sound generated by the realistic geometry were decreased by\nincreasing the incisor angle, and this tendency agreed well with the\nsimplified model. Moreover, the slope value of the decrease in overall\npressure levels estimated by the model was consistent with that of\nthe realistic geometry, indicating the capability of estimating the\neffects of inclined incisors with dental prostheses on the production\nof [s] by using the simplified model.\n"
      ],
      "doi": "10.21437/Interspeech.2021-231",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production and Acquisition  by Human Listeners"
    },
    "arai21b_interspeech": {
      "authors": [
        [
          "Takayuki",
          "Arai"
        ]
      ],
      "title": "Vocal-Tract Models to Visualize the Airstream of Human Breath and Droplets While Producing Speech",
      "original": "0449",
      "page_count": 5,
      "order": 647,
      "p1": "3171",
      "pn": "3175",
      "abstract": [
        "Due to the COVID-19 pandemic, visualizing the airstream of human breath\nduring speech production has become extremely important from the viewpoint\nof preventing infection. In addition, visualizing droplets and the\nlarger drops expelled when we speak consonantal sounds may help for\nthe same reason. One visualization technique is to pass a laser sheet\nthrough the droplet cloud produced by a human speaker. However, the\nlaser poses certain health risks for human beings. Therefore, we developed\nan alternative method to passing a laser against a human body in which\nwe utilize physical models of the human vocal tract. First, we tested\na head-shaped model with a lung model from our previous study to visualize\nthe exhaled breath during vowel production (with and without a mask).\nThen, we implemented an extended version of the anatomical-type vocal-tract\nmodel introduced in our previous study. With this newly developed model,\nlips are made of the same flexible material that was used to form the\ntongue part in the previous model. We also attached these lips to another\nprevious model for producing sounds including /b/. Finally, the lip\nmodels were tested to visualize the droplet cloud including expelled\ndrops present while producing a bilabial plosive sound.\n"
      ],
      "doi": "10.21437/Interspeech.2021-449",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production and Acquisition  by Human Listeners"
    },
    "tanji21_interspeech": {
      "authors": [
        [
          "Ryo",
          "Tanji"
        ],
        [
          "Hidefumi",
          "Ohmura"
        ],
        [
          "Kouichi",
          "Katsurada"
        ]
      ],
      "title": "Using Transposed Convolution for Articulatory-to-Acoustic Conversion from Real-Time MRI Data",
      "original": "0906",
      "page_count": 5,
      "order": 648,
      "p1": "3176",
      "pn": "3180",
      "abstract": [
        "We herein propose a deep neural network-based model for articulatory-to-acoustic\nconversion from real-time MRI data. Although rtMRI, which can record\nentire articulatory organs with a high resolution, has an advantage\nin articulatory-to-acoustic conversion, it has a relatively low sampling\nrate. To address this, we incorporated the super-resolution technique\nin the temporal dimension with a transposed convolution. With the use\nof transposed convolution, the resolution can be increased by applying\nthe inversion process of resolution reduction of a standard CNN. To\nevaluate the performance on the datasets with different temporal resolutions,\nwe conducted experiments using two datasets: USC-TIMIT and Japanese\nrtMRI dataset. Results of the experiments performed using mel-cepstrum\ndistortion and PESQ showed that transposed convolution is effective\nfor generating accurate acoustic features. We also confirmed that increasing\nthe magnification of the super-resolution leads to an improvement in\nthe PESQ score.\n"
      ],
      "doi": "10.21437/Interspeech.2021-906",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production and Acquisition  by Human Listeners"
    },
    "inaam21_interspeech": {
      "authors": [
        [
          "Rafia",
          "Inaam"
        ],
        [
          "Tsukasa",
          "Yoshinaga"
        ],
        [
          "Takayuki",
          "Arai"
        ],
        [
          "Hiroshi",
          "Yokoyama"
        ],
        [
          "Akiyoshi",
          "Iida"
        ]
      ],
      "title": "Comparison Between Lumped-Mass Modeling and Flow Simulation of the Reed-Type Artificial Vocal Fold",
      "original": "0929",
      "page_count": 5,
      "order": 649,
      "p1": "3181",
      "pn": "3185",
      "abstract": [
        "The sound generated by a reed-type artificial vocal fold was predicted\nby a one-mass modeling and numerical flow simulation to examine the\nsound generation mechanisms of the artificial vocal fold. For the one-mass\nmodeling, the reed oscillation was modeled with an equivalent spring\nconstant, and the flow rate was estimated by Bernoulli&#8217;s equation.\nFor the flow simulation, the flow and acoustic fields were predicted\nwith compressible Navier-Stokes Equations, while the reed oscillation\nwas calculated by a one-dimensional beam equation. The experimentation\nwas conducted by measuring the sound of an artificial vocal fold in\nan anechoic chamber. The results of the acoustic measurement showed\nthat the sound amplitudes in the flow simulation agreed well with the\nexperiment, while the one-mass model underestimated the amplitudes\nin a higher frequency range. Reed displacement and flow rate comparisons\nindicated that the flow retention in the reed retainer caused the asymmetry\nin the flow rate waveform, hence producing larger amplitudes for the\nflow simulation in the higher frequency range. The flow simulation\nenabled to predict this flow retention which cannot be modeled in the\none-dimensional one-mass model, and it is anticipated to apply the\nflow simulation to develop a better artificial vocal fold.\n"
      ],
      "doi": "10.21437/Interspeech.2021-929",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production and Acquisition  by Human Listeners"
    },
    "werner21_interspeech": {
      "authors": [
        [
          "Raphael",
          "Werner"
        ],
        [
          "Susanne",
          "Fuchs"
        ],
        [
          "J\u00fcrgen",
          "Trouvain"
        ],
        [
          "Bernd",
          "M\u00f6bius"
        ]
      ],
      "title": "Inhalations in Speech: Acoustic and Physiological Characteristics",
      "original": "1262",
      "page_count": 5,
      "order": 650,
      "p1": "3186",
      "pn": "3190",
      "abstract": [
        "This paper examines the acoustic properties of breath noises in speech\npauses in relation to similar speech segments and with regard to their\ninhalation speed. We measured intensity, center of gravity, and formants,\nas well as kinematic data (via Respiratory Inductance Plethysmography)\nfor inhalations, aspirations of stops, glottal fricatives, and schwa\nvowels. We find that inhalations within speech are louder than those\ninitiating speech, share spectral properties (center of gravity) with\nthe aspiration phase of /k/-realizations, and generally involve a more\nopen vocal tract (higher F1) than schwa-realizations. Intensity, center\nof gravity, and F1 are found to be positively correlated to inhalation\nspeed. Overall, we conclude that jaw openness and inhalation speed\nare major contributors to inhalation noises in speech pauses.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1262",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production and Acquisition  by Human Listeners"
    },
    "xu21j_interspeech": {
      "authors": [
        [
          "Anqi",
          "Xu"
        ],
        [
          "Daniel van",
          "Niekerk"
        ],
        [
          "Branislav",
          "Gerazov"
        ],
        [
          "Paul Konstantin",
          "Krug"
        ],
        [
          "Santitham",
          "Prom-on"
        ],
        [
          "Peter",
          "Birkholz"
        ],
        [
          "Yi",
          "Xu"
        ]
      ],
      "title": "Model-Based Exploration of Linking Between Vowel Articulatory Space and Acoustic Space",
      "original": "1422",
      "page_count": 5,
      "order": 651,
      "p1": "3191",
      "pn": "3195",
      "abstract": [
        "While the acoustic vowel space has been extensively studied in previous\nresearch, little is known about the high-dimensional articulatory space\nof vowels. The articulatory imaging techniques are limited to tracking\nonly a few key articulators, leaving the rest of the articulators unmonitored.\nIn the present study, we attempted to develop a detailed articulatory\nspace obtained by training a 3D articulatory synthesizer to learn eleven\nBritish English vowels. An analysis-by-synthesis strategy was used\nto acoustically optimize vocal tract parameters that represent twenty\narticulatory dimensions. The results show that tongue height and retraction,\nlarynx location and lip roundness are the most perceptually distinctive\narticulatory dimensions. Yet, even for these dimensions, there is a\nfair amount of articulatory overlap between vowels, unlike the fine-grained\nacoustic space. This method opens up the possibility of using modelling\nto investigate the link between speech production and perception.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1422",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production and Acquisition  by Human Listeners"
    },
    "elmers21_interspeech": {
      "authors": [
        [
          "Mikey",
          "Elmers"
        ],
        [
          "Raphael",
          "Werner"
        ],
        [
          "Beeke",
          "Muhlack"
        ],
        [
          "Bernd",
          "M\u00f6bius"
        ],
        [
          "J\u00fcrgen",
          "Trouvain"
        ]
      ],
      "title": "Take a Breath: Respiratory Sounds Improve Recollection in Synthetic Speech",
      "original": "1496",
      "page_count": 5,
      "order": 652,
      "p1": "3196",
      "pn": "3200",
      "abstract": [
        "This study revisits Whalen et al. (1995, JASA) by evaluating English\nspeaking participants in a perception experiment to determine if their\nrecollection is affected by including breath noises in sentences generated\nby a speech synthesis system. Whalen found an improvement in recollection\nfor sentences that were preceded by a breath noise compared to sentences\nwithout one. While Whalen and colleagues used formant synthesis to\nrender the English sentences, we use a modern concatenative synthesis\nsystem. The present study uses inhalations of three different lengths:\n0 ms (no breath noise), 300 ms (short breath noise), and 600 ms (long\nbreath noise). Our results are consistent with Whalen and colleagues\nfor the 600 ms condition, but not for the 300 ms condition, indicating\nthat not all inhalations improved recollection. The present study also\nfound a significant effect for sentence length, illustrating that shorter\nsentences have higher accuracy for recollection than longer sentences.\nOverall, the present study indicates that respiratory sounds are important\nto the recollection of synthesized speech and that researchers should\nfocus on longer and more complex types of speech, such as paragraphs\nor dialogues, for future studies.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1496",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production and Acquisition  by Human Listeners"
    },
    "chen21m_interspeech": {
      "authors": [
        [
          "Taijing",
          "Chen"
        ],
        [
          "Adam",
          "Lammert"
        ],
        [
          "Benjamin",
          "Parrell"
        ]
      ],
      "title": "Modeling Sensorimotor Adaptation in Speech Through Alterations to Forward and Inverse Models",
      "original": "1746",
      "page_count": 5,
      "order": 653,
      "p1": "3201",
      "pn": "3205",
      "abstract": [
        "When speakers are exposed to auditory feedback perturbations of a particular\nvowel, they not only adapt their productions of that vowel but also\ntransfer this change to other, untrained, vowels. However, current\nmodels of speech sensorimotor adaptation, which rely on changes in\nthe feedforward control of specific speech units, are unable to account\nfor this type of generalization. Here, we developed a neural-network\nbased model to simulate speech sensorimotor adaptation, and assess\nwhether updates to internal control models can account for observed\npatterns of generalization. Based on a dataset generated from the Maeda\nplant, we trained two independent neural networks: 1) an inverse model,\nwhich generates motor commands for desired acoustic outcomes and 2)\na forward model, which maps motor commands to acoustic outcomes (prediction).\nWhen vowel formant perturbations were given, both forward and inverse\nmodels were updated when there was a mismatch between predicted and\nperceived output. Our results replicate behavioral experiments: the\nmodel altered its production to counteract the perturbation, and showed\ngradient transfer of this learning dependent on acoustic distance between\ntraining and test vowels. These results suggest that updating paired\nforward and inverse models provides a plausible account for sensorimotor\nadaptation in speech.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1746",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production and Acquisition  by Human Listeners"
    },
    "kawahara21_interspeech": {
      "authors": [
        [
          "Hideki",
          "Kawahara"
        ],
        [
          "Toshie",
          "Matsui"
        ],
        [
          "Kohei",
          "Yatabe"
        ],
        [
          "Ken-Ichi",
          "Sakakibara"
        ],
        [
          "Minoru",
          "Tsuzaki"
        ],
        [
          "Masanori",
          "Morise"
        ],
        [
          "Toshio",
          "Irino"
        ]
      ],
      "title": "Mixture of Orthogonal Sequences Made from Extended Time-Stretched Pulses Enables Measurement of Involuntary Voice Fundamental Frequency Response to Pitch Perturbation",
      "original": "2073",
      "page_count": 5,
      "order": 654,
      "p1": "3206",
      "pn": "3210",
      "abstract": [
        "Auditory feedback plays an essential role in the regulation of the\nfundamental frequency of voiced sounds. The fundamental frequency also\nresponds to auditory stimulation other than the speaker&#8217;s voice.\nWe propose to use this response of the fundamental frequency of sustained\nvowels to frequency-modulated test signals for investigating involuntary\ncontrol of voice pitch. This involuntary response is difficult to identify\nand isolate by the conventional paradigm, which uses step-shaped pitch\nperturbation. We recently developed a versatile measurement method\nusing a mixture of orthogonal sequences made from a set of extended\ntime-stretched pulses (TSP). In this article, we extended our approach\nand designed a set of test signals using the mixture to modulate the\nfundamental frequency of artificial signals. For testing the response,\nthe experimenter presents the modulated signal aurally while the subject\nis voicing sustained vowels. We developed a tool for conducting this\ntest quickly and interactively. We make the tool available as an open-source\nand also provide executable GUI-based applications. Preliminary tests\nrevealed that the proposed method consistently provides compensatory\nresponses with about 100 ms latency, representing involuntary control.\nFinally, we discuss future applications of the proposed method for\nobjective and non-invasive auditory response measurements.\n"
      ],
      "doi": "10.21437/Interspeech.2021-2073",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production and Acquisition  by Human Listeners"
    },
    "you21c_interspeech": {
      "authors": [
        [
          "Chenyu",
          "You"
        ],
        [
          "Nuo",
          "Chen"
        ],
        [
          "Yuexian",
          "Zou"
        ]
      ],
      "title": "Contextualized Attention-Based Knowledge Transfer for Spoken Conversational Question Answering",
      "original": "0110",
      "page_count": 5,
      "order": 655,
      "p1": "3211",
      "pn": "3215",
      "abstract": [
        "Spoken conversational question answering (SCQA) requires machines to\nmodel the flow of multi-turn conversation given the speech utterances\nand text corpora. Different from traditional text question answering\n(QA) tasks, SCQA involves audio signal processing, passage comprehension,\nand contextual understanding. However, ASR systems introduce unexpected\nnoisy signals to the transcriptions, which result in performance degradation\non SCQA. To overcome the problem, we propose CADNet, a novel contextualized\nattention-based distillation approach, which applies both cross-attention\nand self-attention to obtain ASR-robust contextualized embedding representations\nof the passage and dialogue history for performance improvements. We\nalso introduce the spoken conventional knowledge distillation framework\nto distill the ASR-robust knowledge from the estimated probabilities\nof the <i>teacher</i> model to the <i>student</i>. We conduct extensive\nexperiments on the Spoken-CoQA dataset and demonstrate that our approach\nachieves remarkable performance in this task.\n"
      ],
      "doi": "10.21437/Interspeech.2021-110",
      "author_area_id": "11",
      "author_area_label": "Spoken Dialog Systems and Conversational Analysis"
    },
    "duan21_interspeech": {
      "authors": [
        [
          "Wenying",
          "Duan"
        ],
        [
          "Xiaoxi",
          "He"
        ],
        [
          "Zimu",
          "Zhou"
        ],
        [
          "Hong",
          "Rao"
        ],
        [
          "Lothar",
          "Thiele"
        ]
      ],
      "title": "Injecting Descriptive Meta-Information into Pre-Trained Language Models with Hypernetworks",
      "original": "0229",
      "page_count": 5,
      "order": 656,
      "p1": "3216",
      "pn": "3220",
      "abstract": [
        "Pre-trained language models have been widely adopted as backbones in\nvarious natural language processing tasks. However, existing pre-trained\nlanguage models ignore the descriptive meta-information in the text\nsuch as the distinction between the title and the mainbody, leading\nto over-weighted attention to insignificant text. In this paper, we\npropose a hypernetwork-based architecture to model the descriptive\nmeta-information and integrate it into pre-trained language models.\nEvaluations on three natural language processing tasks show that our\nmethod notably improves the performance of pre-trained language models\nand achieves the state-of-the-art results on keyphrase extraction.\n"
      ],
      "doi": "10.21437/Interspeech.2021-229",
      "author_area_id": "11",
      "author_area_label": "Spoken Dialog Systems and Conversational Analysis"
    },
    "rohmatillah21_interspeech": {
      "authors": [
        [
          "Mahdin",
          "Rohmatillah"
        ],
        [
          "Jen-Tzung",
          "Chien"
        ]
      ],
      "title": "Causal Confusion Reduction for Robust Multi-Domain Dialogue Policy",
      "original": "0534",
      "page_count": 5,
      "order": 657,
      "p1": "3221",
      "pn": "3225",
      "abstract": [
        "In the multi-domain dialogue system, dialog policy plays an important\nrole since it determines the suitable actions based on the user&#8217;s\ngoals. However, in many recent works, most of the dialogue optimizations,\nespecially that use reinforcement learning (RL) methods, do not perform\nwell. The main problem is that the initial step of optimization that\ninvolves the behavior cloning (BC) methods suffer from the causal confusion\nproblem, which means that the agent misidentifies true cause of an\nexpert action in current state. This paper proposes a novel method\nto improve the performance of BC method in dialogue system. Instead\nof only predicting correct action given a state from dataset, we introduce\nthe auxiliary tasks to predict both of current belief state and recent\nuser utterance in order to reduce causal confusion of the expert action\nin the dataset since those features are important in every dialog turn.\nExperiments on ConvLab-2 shows that, by using this method, all of RL\nbased optimizations are improved. Furthermore, the agent based on the\nproximal policy optimization shows very significant improvement with\nthe help of the proposed BC agent weights both in policy evaluation\nas well as in end-to-end system evaluation.\n"
      ],
      "doi": "10.21437/Interspeech.2021-534",
      "author_area_id": "11",
      "author_area_label": "Spoken Dialog Systems and Conversational Analysis"
    },
    "fujie21_interspeech": {
      "authors": [
        [
          "Shinya",
          "Fujie"
        ],
        [
          "Hayato",
          "Katayama"
        ],
        [
          "Jin",
          "Sakuma"
        ],
        [
          "Tetsunori",
          "Kobayashi"
        ]
      ],
      "title": "Timing Generating Networks: Neural Network Based Precise Turn-Taking Timing Prediction in Multiparty Conversation",
      "original": "0874",
      "page_count": 5,
      "order": 658,
      "p1": "3226",
      "pn": "3230",
      "abstract": [
        "A brand new neural network based precise timing generation framework,\nnamed the Timing Generating Network (TGN), is proposed and applied\nto turn-taking timing decision problems. Although turn-taking problems\nhave conventionally been formalized as users&#8217; end-of-turn detection,\nthis approach cannot estimate the precise timing at which a spoken\ndialogue system should take a turn to start its utterance. Since several\nconventional approaches estimate precise timings but the estimation\nexecuted only at/after the end of preceding user&#8217;s utterance,\nthey highly depend on the accuracy of intermediate decision modules,\nsuch as voice activity detection, etc. The advantages of the TGN are\nthat its parameters are tunable via error backpropagation as it is\ndescribed in a differentiable form as a whole, and it is free from\ninter-module error propagation as it has no deterministic intermediate\nmodules. The experimental results show that the proposed system is\nsuperior to a conventional turn-taking system that adopts the hard\ndecisions on user&#8217;s voice activity detection and response time\nestimation.\n"
      ],
      "doi": "10.21437/Interspeech.2021-874",
      "author_area_id": "11",
      "author_area_label": "Spoken Dialog Systems and Conversational Analysis"
    },
    "chen21n_interspeech": {
      "authors": [
        [
          "Kehan",
          "Chen"
        ],
        [
          "Zezhong",
          "Li"
        ],
        [
          "Suyang",
          "Dai"
        ],
        [
          "Wei",
          "Zhou"
        ],
        [
          "Haiqing",
          "Chen"
        ]
      ],
      "title": "Human-to-Human Conversation Dataset for Learning Fine-Grained Turn-Taking Action",
      "original": "0994",
      "page_count": 5,
      "order": 659,
      "p1": "3231",
      "pn": "3235",
      "abstract": [
        "Conducting natural turn-taking behavior takes a crucial part in the\nuser experience of modern spoken dialogue systems. One way to build\nsuch system is to learn those behaviors from real-world human-to-human\ndialogues, which have the most diverse and fine-grained turn-taking\nactions than any manual constructed sessions.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  In this paper, we\npropose a Dataset &#8212; FTAD which could be used to learn turn-taking\npolicies directly from human. First, we design an annotation mechanism\nto transform existing human-to-human dialogue session into structural\ndata with most fine-grained turn-taking actions reserved. Then we explored\na set of supervised learning tasks on it, showing the challenge and\npotential of learning complete fine-grained turn-taking policies based\non such data.\n"
      ],
      "doi": "10.21437/Interspeech.2021-994",
      "author_area_id": "11",
      "author_area_label": "Spoken Dialog Systems and Conversational Analysis"
    },
    "sundararaman21_interspeech": {
      "authors": [
        [
          "Mukuntha Narayanan",
          "Sundararaman"
        ],
        [
          "Ayush",
          "Kumar"
        ],
        [
          "Jithendra",
          "Vepa"
        ]
      ],
      "title": "PhonemeBERT: Joint Language Modelling of Phoneme Sequence and ASR Transcript",
      "original": "1582",
      "page_count": 5,
      "order": 660,
      "p1": "3236",
      "pn": "3240",
      "abstract": [
        "Recent years have witnessed significant improvement in ASR systems\nto recognize spoken utterances. However, it is still a challenging\ntask for noisy and out-of-domain data, where ASR errors are prevalent\nin the transcribed text. These errors significantly degrade the performance\nof downstream tasks such as intent and sentiment detection. In this\nwork, we propose a BERT-style language model, referred to as PhonemeBERT\nthat learns a joint language model with phoneme sequence and ASR transcript\nto learn phonetic-aware representations that are robust to ASR errors.\nWe show that PhonemeBERT leverages phoneme sequences as additional\nfeatures that outperform word-only models on downstream tasks. We evaluate\nour approach extensively by generating noisy data for three benchmark\ndatasets &#8212; Stanford Sentiment Treebank, TREC and ATIS for sentiment,\nquestion and intent classification tasks respectively in addition to\na real-life sentiment dataset. The results of the proposed approach\nbeats the state-of-the-art baselines comprehensively on each dataset.\nAdditionally, we show that PhonemeBERT can also be utilized as a pre-trained\nencoder in a low-resource setup where we only have ASR-transcripts\nfor the downstream tasks.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1582",
      "author_area_id": "11",
      "author_area_label": "Spoken Dialog Systems and Conversational Analysis"
    },
    "luo21d_interspeech": {
      "authors": [
        [
          "Hongyin",
          "Luo"
        ],
        [
          "James",
          "Glass"
        ],
        [
          "Garima",
          "Lalwani"
        ],
        [
          "Yi",
          "Zhang"
        ],
        [
          "Shang-Wen",
          "Li"
        ]
      ],
      "title": "Joint Retrieval-Extraction Training for Evidence-Aware Dialog Response Selection",
      "original": "1689",
      "page_count": 5,
      "order": 661,
      "p1": "3241",
      "pn": "3245",
      "abstract": [
        "Neural dialog response selection models infer by scoring each candidate\nresponse given the dialog context, and the cross-encoder method yields\nstate-of-the-art (SOTA) results for the task. In the method, the candidate\nscores are computed by feeding the output embedding of the first token\nin the input sequence, which is a concatenation of response and context,\nto a linear layer for making prediction. However, the embeddings of\nthe other tokens in the sequence are not modeled explicitly, and inferring\nthe candidate scores only with the first token makes the result not\ninterpretable. To address the challenge, we propose a Retrieval-EXtraction\nencoder (REX) for dialog response selection. We augment the existing\nfirst-token- or sequence- based retrieval approach with an extraction\nloss. The loss provides gradient signal from each token during training\nand allows the model to learn token-level evidence and to select response\nbased on important keywords. We show that REX achieves the new SOTA\nin the dialog response selection task. Also, our qualitative analysis\nsuggests that REX highlights evidence it infers selections from and\nmakes the inference result interpretable.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1689",
      "author_area_id": "11",
      "author_area_label": "Spoken Dialog Systems and Conversational Analysis"
    },
    "shenoy21_interspeech": {
      "authors": [
        [
          "Ashish",
          "Shenoy"
        ],
        [
          "Sravan",
          "Bodapati"
        ],
        [
          "Monica",
          "Sunkara"
        ],
        [
          "Srikanth",
          "Ronanki"
        ],
        [
          "Katrin",
          "Kirchhoff"
        ]
      ],
      "title": "Adapting Long Context NLM for ASR Rescoring in Conversational Agents",
      "original": "1849",
      "page_count": 5,
      "order": 662,
      "p1": "3246",
      "pn": "3250",
      "abstract": [
        "Neural Language Models (NLM), when trained and evaluated with context\nspanning multiple utterances, have been shown to consistently outperform\nboth conventional n-gram language models and NLMs that use limited\ncontext. In this paper, we investigate various techniques to incorporate\nturn based context history into both recurrent (LSTM) and Transformer-XL\nbased NLMs. For recurrent based NLMs, we explore context carry over\nmechanism and feature based augmentation, where we incorporate other\nforms of contextual information such as bot response and system dialogue\nacts as classified by a Natural Language Understanding (NLU) model.\nTo mitigate the sharp nearby, fuzzy far away problem with contextual\nNLM, we propose the use of attention layer over lexical metadata to\nimprove feature based augmentation. Additionally, we adapt our contextual\nNLM towards user provided on-the-fly speech patterns by leveraging\nencodings from a large pre-trained masked language model and performing\nfusion with a Transformer-XL based NLM. We test our proposed models\nusing N-best rescoring of ASR hypotheses of task-oriented dialogues\nand also evaluate on downstream NLU tasks such as intent classification\nand slot labeling. The best performing model shows a relative WER between\n1.6% and 9.1% and a slot labeling F1 score improvement of 4% over non-contextual\nbaselines.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1849",
      "author_area_id": "11",
      "author_area_label": "Spoken Dialog Systems and Conversational Analysis"
    },
    "li21h_interspeech": {
      "authors": [
        [
          "Jing",
          "Li"
        ],
        [
          "Binling",
          "Wang"
        ],
        [
          "Yiming",
          "Zhi"
        ],
        [
          "Zheng",
          "Li"
        ],
        [
          "Lin",
          "Li"
        ],
        [
          "Qingyang",
          "Hong"
        ],
        [
          "Dong",
          "Wang"
        ]
      ],
      "title": "Oriental Language Recognition (OLR) 2020: Summary and Analysis",
      "original": "2171",
      "page_count": 5,
      "order": 663,
      "p1": "3251",
      "pn": "3255",
      "abstract": [
        "The fifth Oriental Language Recognition (OLR) Challenge focuses on\nlanguage recognition in a variety of complex environments to promote\nits development. The OLR 2020 Challenge includes three tasks: (1) cross-channel\nlanguage identification, (2) dialect identification, and (3) noisy\nlanguage identification. We choose <i>C<SUB>avg</SUB></i> as the principle\nevaluation metric, and the Equal Error Rate (EER) as the secondary\nmetric. There were 58 teams participating in this challenge and one\nthird of the teams submitted valid results. Compared with the best\nbaseline, the <i>C<SUB>avg</SUB></i> values of Top 1 system for the\nthree tasks were relatively reduced by 82%, 62% and 48%, respectively.\nThis paper describes the three tasks, the database profile, and the\nfinal results. We also outline the novel approaches that improve the\nperformance of language recognition systems most significantly, such\nas the utilization of auxiliary information.\n"
      ],
      "doi": "10.21437/Interspeech.2021-2171",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification:14 Special Sessions"
    },
    "duroselle21b_interspeech": {
      "authors": [
        [
          "Rapha\u00ebl",
          "Duroselle"
        ],
        [
          "Md.",
          "Sahidullah"
        ],
        [
          "Denis",
          "Jouvet"
        ],
        [
          "Irina",
          "Illina"
        ]
      ],
      "title": "Language Recognition on Unknown Conditions: The LORIA-Inria-MULTISPEECH System for AP20-OLR Challenge",
      "original": "0276",
      "page_count": 5,
      "order": 664,
      "p1": "3256",
      "pn": "3260",
      "abstract": [
        "We describe the LORIA-Inria-MULTISPEECH system submitted to the Oriental\nLanguage Recognition AP20-OLR Challenge. This system has been specifically\ndesigned to be robust to unknown conditions: channel mismatch (task\n1) and noisy conditions (task 3). Three sets of studies have been carried\nout for elaborating the system: design of multilingual bottleneck features,\nselection of robust features by evaluating language recognition performance\non an unobserved channel, and design of the final models with different\nloss functions which exploit channel diversity within the training\nset. Key factors for robustness to unknown conditions are data augmentation\ntechniques, stochastic weight averaging, and regularization of TDNNs\nwith domain robustness loss functions. The final system is the combination\nof four TDNNs using bottleneck features and one GMM using SDC-MFCC\nfeatures. Within the AP20-OLR Challenge, it achieves the top performance\nfor tasks 1 and 3 with a <i>C<SUB>avg</SUB></i> of respectively 0.0239\nand 0.0374. This validates the approach for generalization to unknown\nconditions.\n"
      ],
      "doi": "10.21437/Interspeech.2021-276",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification:14 Special Sessions"
    },
    "kong21b_interspeech": {
      "authors": [
        [
          "Tianlong",
          "Kong"
        ],
        [
          "Shouyi",
          "Yin"
        ],
        [
          "Dawei",
          "Zhang"
        ],
        [
          "Wang",
          "Geng"
        ],
        [
          "Xin",
          "Wang"
        ],
        [
          "Dandan",
          "Song"
        ],
        [
          "Jinwen",
          "Huang"
        ],
        [
          "Huiyu",
          "Shi"
        ],
        [
          "Xiaorui",
          "Wang"
        ]
      ],
      "title": "Dynamic Multi-Scale Convolution for Dialect Identification",
      "original": "0056",
      "page_count": 5,
      "order": 665,
      "p1": "3261",
      "pn": "3265",
      "abstract": [
        "Time Delay Neural Networks (TDNN)-based methods are widely used in\ndialect identification. However, in previous work with TDNN application,\nsubtle variant is being neglected in different feature scales. To address\nthis issue, we propose a new architecture, named dynamic multi-scale\nconvolution, which consists of dynamic kernel convolution, local multi-scale\nlearning, and global multi-scale pooling. Dynamic kernel convolution\ncaptures features between short-term and long-term context adaptively.\nLocal multi-scale learning, which represents multi-scale features at\na granular level, is able to increase the range of receptive fields\nfor convolution operation. Besides, global multi-scale pooling is applied\nto aggregate features from different bottleneck layers in order to\ncollect information from multiple aspects. The proposed architecture\nsignificantly outperforms state-of-the-art system on the AP20-OLR-dialect-task\nof oriental language recognition (OLR) challenge 2020, with the best\naverage cost performance (<i>C<SUB>avg</SUB></i>) of 0.067 and the\nbest equal error rate (EER) of 6.52%. Compared with the known best\nresults, our method achieves 9% of <i>C<SUB>avg</SUB></i> and 45% of\nEER relative improvement, respectively. Furthermore, the parameters\nof proposed model are 91% fewer than the best known model.\n"
      ],
      "doi": "10.21437/Interspeech.2021-56",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification:14 Special Sessions"
    },
    "wang21z_interspeech": {
      "authors": [
        [
          "Ding",
          "Wang"
        ],
        [
          "Shuaishuai",
          "Ye"
        ],
        [
          "Xinhui",
          "Hu"
        ],
        [
          "Sheng",
          "Li"
        ],
        [
          "Xinkang",
          "Xu"
        ]
      ],
      "title": "An End-to-End Dialect Identification System with Transfer Learning from a Multilingual Automatic Speech Recognition Model",
      "original": "0374",
      "page_count": 5,
      "order": 666,
      "p1": "3266",
      "pn": "3270",
      "abstract": [
        "In this paper, we propose an end-to-end (E2E) dialect identification\nsystem trained using transfer learning from a multilingual automatic\nspeech recognition (ASR) model. This is also an extension of our submitted\nsystem to the Oriental Language Recognition Challenge 2020 (AP20-OLR).\nWe verified its applicability using the dialect identification (DID)\ntask of the AP20-OLR. First, we trained a robust conformer-based joint\nconnectionist temporal classification (CTC) /attention multilingual\nE2E ASR model using the training corpora of eight languages, independent\nof the target dialects. Second, we initialized the E2E-based classifier\nwith the ASR model&#8217;s shared encoder using a transfer learning\napproach. Finally, we trained the classifier on the target dialect\ncorpus. We obtained the final classifier by selecting the best model\nfrom the following: (1) the averaged model in term of the loss values;\nand (2) the averaged model in term of classification accuracy.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  Our experiments on\nthe DID test-set of the AP20-OLR demonstrated that significant identification\nimprovements were achieved for three Chinese dialects. The performances\nof our system outperforms the winning team of the AP20-OLR, with the\nlargest relative reductions of 19.5% in <i>C<SUB>avg</SUB></i> and\n25.2% in EER.\n"
      ],
      "doi": "10.21437/Interspeech.2021-374",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification:14 Special Sessions"
    },
    "yu21b_interspeech": {
      "authors": [
        [
          "Haibin",
          "Yu"
        ],
        [
          "Jing",
          "Zhao"
        ],
        [
          "Song",
          "Yang"
        ],
        [
          "Zhongqin",
          "Wu"
        ],
        [
          "Yuting",
          "Nie"
        ],
        [
          "Wei-Qiang",
          "Zhang"
        ]
      ],
      "title": "Language Recognition Based on Unsupervised Pretrained Models",
      "original": "0807",
      "page_count": 5,
      "order": 667,
      "p1": "3271",
      "pn": "3275",
      "abstract": [
        "Unsupervised pretrained models have been proven to rival or even outperform\nsupervised systems in various speech recognition tasks. However, their\nperformance for language recognition is still left to be explored.\nIn this paper, we construct several language recognition systems based\non existing unsupervised pretraining approaches, and explore their\ncredibility and performance to learn high-level generalization of language.\nWe discover that unsupervised pretrained models capture expressive\nand highly linear-separable features. With these representations, language\nrecognition can perform well even when the classifiers are relatively\nsimple or only a small amount of labeled data is available. Although\nlinear classifiers are usable, neural nets with RNN structures improve\nthe results. Meanwhile, unsupervised pretrained models are able to\ngain refined representations on audio frame level that are strongly\ncoupled with the acoustic features of the input sequence. Therefore\nthese features contain redundant information of speakers and channels\nwith few relations to the identity of the language. This nature of\nunsupervised pretrained models causes a performance degradation in\nlanguage recognition tasks on cross-channel tests.\n"
      ],
      "doi": "10.21437/Interspeech.2021-807",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification:14 Special Sessions"
    },
    "li21i_interspeech": {
      "authors": [
        [
          "Zheng",
          "Li"
        ],
        [
          "Yan",
          "Liu"
        ],
        [
          "Lin",
          "Li"
        ],
        [
          "Qingyang",
          "Hong"
        ]
      ],
      "title": "Additive Phoneme-Aware Margin Softmax Loss for Language Recognition",
      "original": "1167",
      "page_count": 5,
      "order": 668,
      "p1": "3276",
      "pn": "3280",
      "abstract": [
        "This paper proposes an additive phoneme-aware margin softmax (APM-Softmax)\nloss to train the multi-task learning network with phonetic information\nfor language recognition. In additive margin softmax (AM-Softmax) loss,\nthe margin is set as a constant during the entire training for all\ntraining samples, and that is a suboptimal method since the recognition\ndifficulty varies in training samples. In additive angular margin softmax\n(AAM-Softmax) loss, the additional angular margin is set as a constant\nas well. In this paper, we propose an APM-Softmax loss for language\nrecognition with phoneitc multi-task learning, in which the additive\nphoneme-aware margin is automatically tuned for different training\nsamples. More specifically, the margin of language recognition is adjusted\naccording to the results of phoneme recognition. Experiments are reported\non Oriental Language Recognition (OLR) datasets, and the proposed method\nimproves AM-Softmax loss and AAM-Softmax loss in different language\nrecognition testing conditions.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1167",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification:14 Special Sessions"
    },
    "jahchan21_interspeech": {
      "authors": [
        [
          "Nataly",
          "Jahchan"
        ],
        [
          "Florentin",
          "Barbier"
        ],
        [
          "Ariyanidevi Dharma",
          "Gita"
        ],
        [
          "Khaled",
          "Khelif"
        ],
        [
          "Estelle",
          "Delpech"
        ]
      ],
      "title": "Towards an Accent-Robust Approach for ATC Communications Transcription",
      "original": "0333",
      "page_count": 5,
      "order": 669,
      "p1": "3281",
      "pn": "3285",
      "abstract": [
        "Air Traffic Control (ATC) communications are a typical example where\nAutomatic Speech Recognition could face various challenges: audio data\nare quite noisy due to the characteristics of capturing mechanisms.\nAll speakers involved use a specific English-based phraseology and\na significant number of pilots and controllers are non-native English\nspeakers. The aim of this work is to enhance pilot-ATC communications\nby adding a Speech to Text (STT) capability that will transcribe ATC\nspeech into text on the cockpit interfaces to help the pilot understand\nATC speech in a more optimal manner (be able to verify what he/she\nheard on the radio by looking at the text transcription, be able to\ndecipher non-native English accents from controllers, not lose time\nasking the ATC to repeat the message several times). In this paper,\nwe first describe an accent analysis study which was carried out both\non a theoretical level but also with the help of feedback from several\nhundred airline pilots. Then, we present the dataset that was set up\nfor this work. Finally, we describe the experiments we have implemented\nand the impact of the speaker accent on the performance of a speech\nto text engine.\n"
      ],
      "doi": "10.21437/Interspeech.2021-333",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications:14 Special Sessions"
    },
    "szoke21_interspeech": {
      "authors": [
        [
          "Igor",
          "Sz\u00f6ke"
        ],
        [
          "Santosh",
          "Kesiraju"
        ],
        [
          "Ond\u0159ej",
          "Novotn\u00fd"
        ],
        [
          "Martin",
          "Kocour"
        ],
        [
          "Karel",
          "Vesel\u00fd"
        ],
        [
          "Jan",
          "\u010cernock\u00fd"
        ]
      ],
      "title": "Detecting English Speech in the Air Traffic Control Voice Communication",
      "original": "1033",
      "page_count": 5,
      "order": 670,
      "p1": "3286",
      "pn": "3290",
      "abstract": [
        "Developing in-cockpit voice enabled applications require a real-world\ndataset with labels and annotations. We launched a community platform\nfor collecting the Air-Traffic Control (ATC) speech, world-wide in\nthe ATCO<SUP>2</SUP> project. Filtering out non-English speech is one\nof the main components in the data processing pipeline. The proposed\nEnglish Language Detection (ELD) system is based on the embeddings\nfrom Bayesian subspace multinomial model. It is trained on the word\nconfusion network from an ASR system. It is robust, easy to train,\nand light weighted. We achieved 0.0439 equal-error-rate (EER), a 50%\nrelative reduction as compared to the state-of-the-art acoustic ELD\nsystem based on x-vectors, in the in-domain scenario. Further, we achieved\nan EER of 0.1352, a 33% relative reduction as compared to the acoustic\nELD, in the unseen language (out-of-domain) condition. We plan to publish\nthe evaluation dataset from the ATCO<SUP>2</SUP> project.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1033",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications:14 Special Sessions"
    },
    "ohneiser21_interspeech": {
      "authors": [
        [
          "Oliver",
          "Ohneiser"
        ],
        [
          "Seyyed Saeed",
          "Sarfjoo"
        ],
        [
          "Hartmut",
          "Helmke"
        ],
        [
          "Shruthi",
          "Shetty"
        ],
        [
          "Petr",
          "Motlicek"
        ],
        [
          "Matthias",
          "Kleinert"
        ],
        [
          "Heiko",
          "Ehr"
        ],
        [
          "\u0160ar\u016bnas",
          "Murauskas"
        ]
      ],
      "title": "Robust Command Recognition for Lithuanian Air Traffic Control Tower Utterances",
      "original": "0935",
      "page_count": 5,
      "order": 671,
      "p1": "3291",
      "pn": "3295",
      "abstract": [
        "The maturity of automatic speech recognition (ASR) systems at controller\nworking positions is currently a highly relevant technological topic\nin air traffic control (ATC). However, ATC service providers are less\ninterested in pure word error rate (WER). They want to see benefits\nof ASR applications for ATC. Such applications transform recognized\nword sequences into semantic meanings, i.e., a number of related concepts\nsuch as callsign, type, value, unit, etc., which are combined to form\ncommands. Digitized concepts or recognized commands can enter ATC systems\nbased on an ontology for utterance annotation agreed between European\nATC stakeholders. Command recognition (CR) has already been performed\nin approach control. However, spoken utterances of tower controllers\nare longer, include more free speech, and contain other command types\nthan in approach. An automatic CR rate of 95.8% is achievable on perfect\nword recognition, i.e., manually transcribed audio recordings (gold\ntranscriptions), taken from Lithuanian controllers in a multiple remote\ntower environment. This paper presents CR results for various speech-to-text\nmodels with different WERs on tower utterances. Although WERs were\naround 9%, we achieve CR rates of 85%. CR rates only slightly decrease\nwith higher WERs, which enables to bring ASR applications closer to\noperational ATC environment.\n"
      ],
      "doi": "10.21437/Interspeech.2021-935",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications:14 Special Sessions"
    },
    "zuluagagomez21_interspeech": {
      "authors": [
        [
          "Juan",
          "Zuluaga-Gomez"
        ],
        [
          "Iuliia",
          "Nigmatulina"
        ],
        [
          "Amrutha",
          "Prasad"
        ],
        [
          "Petr",
          "Motlicek"
        ],
        [
          "Karel",
          "Vesel\u00fd"
        ],
        [
          "Martin",
          "Kocour"
        ],
        [
          "Igor",
          "Sz\u00f6ke"
        ]
      ],
      "title": "Contextual Semi-Supervised Learning: An Approach to Leverage Air-Surveillance and Untranscribed ATC Data in ASR Systems",
      "original": "1373",
      "page_count": 5,
      "order": 672,
      "p1": "3296",
      "pn": "3300",
      "abstract": [
        "Air traffic management and specifically air-traffic control (ATC) rely\nmostly on voice communications between Air Traffic Controllers (ATCos)\nand pilots. In most cases, these voice communications follow a well-defined\ngrammar that could be leveraged in Automatic Speech Recognition (ASR)\ntechnologies. The callsign used to address an airplane is an essential\npart of all ATCo-pilot communications. We propose a two-step approach\nto add contextual knowledge during semi-supervised training to reduce\nthe ASR system error rates at recognizing the part of the utterance\nthat contains the callsign. Initially, we represent in a WFST the contextual\nknowledge (i.e. air-surveillance data) of an ATCo-pilot communication.\nThen, during Semi-Supervised Learning (SSL) the contextual knowledge\nis added by second-pass decoding (i.e. lattice re-scoring). Results\nshow that &#8216;unseen domains&#8217; (e.g. data from airports not\npresent in the supervised training data) are further aided by contextual\nSSL when compared to standalone SSL. For this task, we introduce the\nCallsign Word Error Rate (CA-WER) as an evaluation metric, which only\nassesses ASR performance of the spoken callsign in an utterance. We\nobtained a 32.1% CA-WER relative improvement applying SSL with an additional\n17.5% CA-WER improvement by adding contextual knowledge during SSL\non a challenging ATC-based test set gathered from LiveATC.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1373",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications:14 Special Sessions"
    },
    "kocour21_interspeech": {
      "authors": [
        [
          "Martin",
          "Kocour"
        ],
        [
          "Karel",
          "Vesel\u00fd"
        ],
        [
          "Alexander",
          "Blatt"
        ],
        [
          "Juan Zuluaga",
          "Gomez"
        ],
        [
          "Igor",
          "Sz\u00f6ke"
        ],
        [
          "Jan",
          "\u010cernock\u00fd"
        ],
        [
          "Dietrich",
          "Klakow"
        ],
        [
          "Petr",
          "Motlicek"
        ]
      ],
      "title": "Boosting of Contextual Information in ASR for Air-Traffic Call-Sign Recognition",
      "original": "1619",
      "page_count": 5,
      "order": 673,
      "p1": "3301",
      "pn": "3305",
      "abstract": [
        "Contextual adaptation of ASR can be very beneficial for multi-accent\nand often noisy Air-Traffic Control (ATC) speech. Our focus is call-sign\nrecognition, which can be used to track conversations of ATC operators\nwith individual airplanes. We developed a two-stage boosting strategy,\nconsisting of HCLG boosting and Lattice boosting. Both are implemented\nas WFST compositions and the contextual information is specific to\neach utterance. In HCLG boosting we give score discounts to individual\nwords, while in Lattice boosting the score discounts are given to word\nsequences. The context data have origin in surveillance database of\nOpenSky Network. From this, we obtain lists of call-signs that are\nmade more likely to appear in the best hypothesis of ASR. This also\nimproves the accuracy of the NLU module that recognizes the call-signs\nfrom the best hypothesis of ASR.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  As part of ATCO<SUP>2</SUP>\nproject, we collected liveatc test set2. The boosting of call-signs\nleads to 4.7% absolute WER improvement and 27.1% absolute increase\nof Call-Sign recognition Accuracy (CSA). Our best result of 82.9% CSA\nis quite good, given that the data is noisy, and WER 28.4% is relatively\nhigh. We believe there is still room for improvement.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1619",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications:14 Special Sessions"
    },
    "elie21_interspeech": {
      "authors": [
        [
          "Benjamin",
          "Elie"
        ],
        [
          "Jodie",
          "Gauvain"
        ],
        [
          "Jean-Luc",
          "Gauvain"
        ],
        [
          "Lori",
          "Lamel"
        ]
      ],
      "title": "Modeling the Effect of Military Oxygen Masks on Speech Characteristics",
      "original": "1650",
      "page_count": 5,
      "order": 674,
      "p1": "3306",
      "pn": "3310",
      "abstract": [
        "Wearing an oxygen mask changes the speech production of speakers. It\nindeed modifies the vocal apparatus and perturbs the articulatory movements\nof the speaker. This paper studies the impact of the oxygen mask of\nmilitary aircraft pilots on formant trajectories, both dynamically\n(variations of the formants at a utterance level) and globally (mean\nvalue at the utterance level) for 12 speakers.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  A comparative analysis\nof speech collected with and without an oxygen mask shows that the\nmask has a significant impact on the formant trajectories, both on\nthe mean values and on the formant variations at the utterance level.\nThis impact is strongly dependent on the speaker and also on the mask\nmodel. These observations suggest that the articulatory movements of\nthe speaker are modified by the presence of the mask.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  These observations\nare validated via a preliminary ASR experiment that uses a data augmentation\ntechnique based on articulatory perturbations that are driven by our\nexperimental observations.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1650",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "milde21_interspeech": {
      "authors": [
        [
          "Benjamin",
          "Milde"
        ],
        [
          "Tim",
          "Fischer"
        ],
        [
          "Steffen",
          "Remus"
        ],
        [
          "Chris",
          "Biemann"
        ]
      ],
      "title": "MoM: Minutes of Meeting Bot",
      "original": "8015",
      "page_count": 2,
      "order": 675,
      "p1": "3311",
      "pn": "3312",
      "abstract": [
        "We present MoM (Minutes of Meeting) bot, an automatic meeting transcription\nsystem with real-time recognition, summarization and visualization\ncapabilities. MoM works without any cloud processing and does not require\na network connection. Every processing step is local, even its speech\nrecognition component, to address privacy concerns of meetings. MoM\ncan be used to assisted writing a (summarized) protocol of a meeting,\nbut may also help the hearing-impaired to follow a discussion. We address\nmeeting-related issues, e.g. local vocabulary of an organization or\ncompany with active learning of G2P models and custom vocabulary extensions.\n"
      ]
    },
    "wilbrandt21_interspeech": {
      "authors": [
        [
          "Alexander",
          "Wilbrandt"
        ],
        [
          "Simon",
          "Stone"
        ],
        [
          "Peter",
          "Birkholz"
        ]
      ],
      "title": "Articulatory Data Recorder: A Framework for Real-Time Articulatory Data Recording",
      "original": "8016",
      "page_count": 2,
      "order": 676,
      "p1": "3313",
      "pn": "3314",
      "abstract": [
        "Articulatory data can be collected using numerous modalities, such\nas video, ultrasound, electromagnetic articulography, or palatographic\ntechniques. Every measurement technique requires software to visualize\nthe incoming data and export the data for further analysis. This has\nled to an increase of available recording software over the past decades,\nincluding properly maintained software in regular use but also many\nabandoned and dead projects. In this paper, we present a new framework\nfor real-time, simultaneous recording of acoustic and articulatory\ndata. With the release of the Articulatory Data Recorder, our aim is\nto provide the experimental phonetics and articulatory research community\nwith a common framework that is simple to use and easy to extend. It\nis specifically designed to cover the most common use cases in experimental\nphonetics: Elicit speech utterances using text prompts and record simultaneous\naudio and articulatory data. By following the FURPS+-system, we offer\na combination of high performance and a low barrier of entrance for\nenrollment of any new articulatory measurement technique. The current\nversion already supports various palatographic measurement techniques\nin use at our institute and future work will incorporate feedback and\nfeature requests from the community.\n"
      ]
    },
    "codinafilba21_interspeech": {
      "authors": [
        [
          "Joan",
          "Codina-Filb\u00e0"
        ],
        [
          "Guillermo",
          "C\u00e1mbara"
        ],
        [
          "Alex",
          "Peir\u00f3-Lilja"
        ],
        [
          "Jens",
          "Grivolla"
        ],
        [
          "Roberto",
          "Carlini"
        ],
        [
          "Mireia",
          "Farr\u00fas"
        ]
      ],
      "title": "The INGENIOUS Multilingual Operations App",
      "original": "8017",
      "page_count": 2,
      "order": 677,
      "p1": "3315",
      "pn": "3316",
      "abstract": [
        "This paper presents the integration of a speech-to-speech translation\nservice into a Telegram bot as a part of the EU funded INGENIOUS project.\nThe bot is thought as a multilingual communication channel where First\nResponders talk in their own language and receive other&#8217;s messages\nin English. The Speech-to-Speech translation system is currently being\nadapted to the emergency domains, so it will correctly deal with emergency\ncodes and geographical data.\n"
      ]
    },
    "rownicka21_interspeech": {
      "authors": [
        [
          "Joanna",
          "Rownicka"
        ],
        [
          "Kilian",
          "Sprenkamp"
        ],
        [
          "Antonio",
          "Tripiana"
        ],
        [
          "Volodymyr",
          "Gromoglasov"
        ],
        [
          "Timo P.",
          "Kunz"
        ]
      ],
      "title": "Digital Einstein Experience: Fast Text-to-Speech for Conversational AI",
      "original": "8018",
      "page_count": 2,
      "order": 678,
      "p1": "3317",
      "pn": "3318",
      "abstract": [
        "We describe our approach to create and deliver a custom voice for a\nconversational AI use-case. More specifically, we provide a voice for\na Digital Einstein character, to enable human-computer interaction\nwithin the digital conversation experience. To create the voice which\nfits the context well, we first design a voice character and we produce\nthe recordings which correspond to the desired speech attributes. We\nthen model the voice. Our solution utilizes Fastspeech 2 for log-scaled\nmel-spectrogram prediction from phonemes and Parallel WaveGAN to generate\nthe waveforms. The system supports a character input and gives a speech\nwaveform at the output. We use a custom dictionary for selected words\nto ensure their proper pronunciation. Our proposed cloud architecture\nenables for fast voice delivery, making it possible to talk to the\ndigital version of Albert Einstein in real-time.\n"
      ]
    },
    "geislinger21_interspeech": {
      "authors": [
        [
          "Robert",
          "Geislinger"
        ],
        [
          "Benjamin",
          "Milde"
        ],
        [
          "Timo",
          "Baumann"
        ],
        [
          "Chris",
          "Biemann"
        ]
      ],
      "title": "Live Subtitling for BigBlueButton with Open-Source Software",
      "original": "8019",
      "page_count": 2,
      "order": 679,
      "p1": "3319",
      "pn": "3320",
      "abstract": [
        "We present an open source plugin for live subtitling in the popular\nopen source video conferencing software BigBlueButton. Our plugin decodes\neach speaker&#8217;s audio stream separately and in parallel, thereby\nobliviating the need for speaker diarization and seamlessly handling\noverlapped talk. Any Kaldi-compatible nnet3 model can be used with\nour plugin and we demonstrate it using freely available TDNN-HMM-based\nASR models for English and German. Our subtitles can be used as they\nare (e.g., in loud environments) or can form the basis for further\nNLP processes. Our tool can also simplify the collection of remotely\nrecorded multi-party dialogue corpora.\n"
      ]
    },
    "nicmanis21_interspeech": {
      "authors": [
        [
          "D\u0101vis",
          "Nicmanis"
        ],
        [
          "Askars",
          "Salimbajevs"
        ]
      ],
      "title": "Expressive Latvian Speech Synthesis for Dialog Systems",
      "original": "8020",
      "page_count": 2,
      "order": 680,
      "p1": "3321",
      "pn": "3322",
      "abstract": [
        "To fully enable spoken human-computer interaction, the text-to-speech\n(TTS) component of such a system must produce natural human-like speech\nand adjust the prosody according to the dialog context.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  While the current\npublicly available TTS services can produce natural-sounding speech,\nthey usually lack emotional expressiveness.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  In this paper, we\npresent an expressive speech synthesis prototype for the Latvian language.\nThe prototype is integrated into our chatbot management system and\nenables bot designers to specify the stylistic information for each\nbot response, thus making the interaction with the chatbot more natural.\n"
      ]
    },
    "kachare21_interspeech": {
      "authors": [
        [
          "Pramod H.",
          "Kachare"
        ],
        [
          "Prem C.",
          "Pandey"
        ],
        [
          "Vishal",
          "Mane"
        ],
        [
          "Hirak",
          "Dasgupta"
        ],
        [
          "K.S.",
          "Nataraj"
        ],
        [
          "Akshada",
          "Rathod"
        ],
        [
          "Sheetal K.",
          "Pathak"
        ]
      ],
      "title": "ViSTAFAE: A Visual Speech-Training Aid with Feedback of Articulatory Efforts",
      "original": "8022",
      "page_count": 2,
      "order": 681,
      "p1": "3323",
      "pn": "3324",
      "abstract": [
        "An app is presented as a speech-training aid for providing visual feedback\nof articulatory efforts using information obtained from the utterances&#8217;\naudiovisual recording. It has two panels to enable comparison between\nthe articulatory efforts of the learner and the teacher or a pre-recorded\nreference speaker. The visual feedback consists of a slow-motion animation\nof lateral vocal tract shape, level, and pitch, and time-aligned display\nof the frontal view of the speaker&#8217;s face along with playback\nof the time-scaled speech signal. The app comprises a graphical user\ninterface and modules for signal acquisition, analysis, and animation.\nIt is developed using Python as a Windows-based app and may be accessed\nremotely through a web browser.\n"
      ]
    },
    "livescu21_interspeech": {
      "authors": [
        [
          "Karen",
          "Livescu"
        ]
      ],
      "title": "Learning Speech Models from Multi-Modal Data",
      "original": "abs16",
      "page_count": 0,
      "order": 682,
      "p1": "0",
      "pn": "",
      "abstract": [
        "Speech is usually recorded as an acoustic signal, but it often appears\nin context with other signals.  In addition to the acoustic signal,\nwe may have available a corresponding visual scene, the video of the\nspeaker, physiological signals such as the speaker&#8217;s movements\nor neural recordings, or other related signals.  It is often possible\nto learn a better speech model or representation by considering the\ncontext provided by these additional signals, or to learn with less\ntraining data.  Typical approaches to training from multi-modal data\nare based on the idea that models or representations of each modality\nshould be in some sense predictive of the other modalities. Multi-modal\napproaches can also take advantage of the fact that the sources of\nnoise or nuisance variables are different in different measurement\nmodalities, so an additional (non-acoustic) modality can help learn\na speech representation that suppresses such noise.  This talk will\nsurvey several lines of work in this area, both older and newer.  It\nwill cover some basic techniques from machine learning and statistics,\nas well as specific models and applications for speech.\n"
      ]
    },
    "elhilali21_interspeech": {
      "authors": [
        [
          "Mounya",
          "Elhilali"
        ]
      ],
      "title": "Adaptive Listening to Everyday Soundscapes",
      "original": "abs17",
      "page_count": 0,
      "order": 683,
      "p1": "0",
      "pn": "",
      "abstract": [
        "As we navigate our everyday life, we are continuously parsing through\na cacophony of sounds that are constantly impinging on our senses.\nThis ability to sieve through everyday sounds and pick-out signals\nof interest may seem intuitive and effortless, but it is a real feat\nthat involves complex brain networks that balance the sensory signal\nwith our goals, expectations, attentional state and prior knowledge\n(what we hear, what we want to hear, what we expect to hear, what we\nknow). A similar challenge faces computer systems that need to adapt\nto dynamic inputs, evolving objectives and novel surrounds. A growing\nbody of work in neuroscience has been amending our views of processing\nin the brain; replacing the conventional view of &#8216;static&#8217;\nprocessing with a more &#8216;active&#8217; and malleable mapping that\nrapidly adapts to the task at hand and listening conditions. After\nall, humans and most animals are not specialists, but generalists whose\nperception is shaped by experience, context and changing behavioral\ndemands. The talk will discuss theoretical formulations of these adaptive\nprocesses and lessons to leverage attentional feedback in algorithms\nfor detecting and separating sounds of interest (e.g. speech, music)\namidst competing distractors.\n"
      ]
    },
    "ribeiro21b_interspeech": {
      "authors": [
        [
          "Vinicius",
          "Ribeiro"
        ],
        [
          "Karyna",
          "Isaieva"
        ],
        [
          "Justine",
          "Leclere"
        ],
        [
          "Pierre-Andr\u00e9",
          "Vuissoz"
        ],
        [
          "Yves",
          "Laprie"
        ]
      ],
      "title": "Towards the Prediction of the Vocal Tract Shape from the Sequence of Phonemes to be Articulated",
      "original": "0184",
      "page_count": 5,
      "order": 684,
      "p1": "3325",
      "pn": "3329",
      "abstract": [
        "In this work, we address the prediction of speech articulators&#8217;\ntemporal geometric position from the sequence of phonemes to be articulated.\nWe start from a set of real-time MRI sequences uttered by a female\nFrench speaker. The contours of five articulators were tracked automatically\nin each of the frames in the MRI video. Then, we explore the capacity\nof a bidirectional GRU to correctly predict each articulator&#8217;s\nshape and position given the sequence of phonemes and their duration.\nWe propose a 5-fold cross-validation experiment to evaluate the generalization\ncapacity of the model. In a second experiment, we evaluate our model&#8217;s\ndata efficiency by reducing training data. We evaluate the point-to-point\nEuclidean distance and the Pearson&#8217;s correlations along time\nbetween the predicted and the target shapes. We also evaluate produced\nshapes of the critical articulators of specific phonemes. We show that\nour model can achieve good results with minimal data, producing very\nrealistic vocal tract shapes.\n"
      ],
      "doi": "10.21437/Interspeech.2021-184",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production and Acquisition  by Human Listeners"
    },
    "blandin21_interspeech": {
      "authors": [
        [
          "R\u00e9mi",
          "Blandin"
        ],
        [
          "Marc",
          "Arnela"
        ],
        [
          "Simon",
          "F\u00e9lix"
        ],
        [
          "Jean-Baptiste",
          "Doc"
        ],
        [
          "Peter",
          "Birkholz"
        ]
      ],
      "title": "Comparison of the Finite Element Method, the Multimodal Method and the Transmission-Line Model for the Computation of Vocal Tract Transfer Functions",
      "original": "0975",
      "page_count": 5,
      "order": 685,
      "p1": "3330",
      "pn": "3334",
      "abstract": [
        "The acoustic properties of vocal tract are usually characterized by\nits transfer function from the input acoustic volume flow at the glottis\nto the radiated acoustic pressure. These transfer functions can be\ncomputed with acoustic models. Three-dimensional acoustic simulation\nare used to take into account accurately the three-dimensional vocal\ntract shape and to generate valid results even at high frequency. Finite\nelement models, finite difference methods, three-dimensional waveguide\nmeshes, or the multimodal method have been used for this purpose. However,\nthese methods require much more computation time than simple one-dimensional\nmodels. Among these methods, the multimodal method can achieve the\nshortest computation times. However, all the previous implementations\nhad limitations regarding the geometrical shapes and the losses. In\nthis work, we evaluate a new implementation that intends to overcome\nthese limitations. Vowel transfer functions obtained with this new\nimplementation are compared with a transmission-line model and a proven,\nrobust and highly accurate method: the finite element method. While\nthe finite element method remains the most reliable, the multimodal\nmethod generates similar transfer functions in much less time. The\ntransmission line model gives valid results for the four first resonances.\n"
      ],
      "doi": "10.21437/Interspeech.2021-975",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production and Acquisition  by Human Listeners"
    },
    "wagner21b_interspeech": {
      "authors": [
        [
          "Petra",
          "Wagner"
        ],
        [
          "Sina",
          "Zarrie\u00df"
        ],
        [
          "Joana",
          "Cholin"
        ]
      ],
      "title": "Effects of Time Pressure and Spontaneity on Phonotactic Innovations in German Dialogues",
      "original": "1539",
      "page_count": 5,
      "order": 686,
      "p1": "3335",
      "pn": "3339",
      "abstract": [
        "Speech variation is often explained by speakers&#8217; balancing of\nproduction constraints (favoring phonetic reduction of high frequency,\nexpected items) and listener orientation (favoring more canonical productions\nfor low frequency, unexpected items). Less well understood are processes\ninvolving a structural reorganization of articulatory plans due to\nre-syllabification, e.g., resulting from processes involving massive\nreduction, epenthesis or metathesis. In this paper, we want to focus\non two kinds of re-syllabifications: (1) within-system innovations,\nin which non-canonical forms occur, and (2) beyond-system inventions,\nwhich do not follow the phonotactic constraints of the language under\nconsideration. We examine these processes in a corpus of spontaneous\nand read dyadic interactions of German, in which time pressure was\ncontrolled as an additional factor. Results show that spontaneity and\ntime pressure will mostly lead to within-system innovations, favoring\nhighly trained, unmarked articulatory routines, while minimizing information\nloss. However, occasionally speakers leave the beaten paths of highly\ntrained articulatory routines, and invent novel phonotactic sequences\nwhich are at odds with the phonotactic grammar of German. Our results\nare discussed in the light of their implications for contemporary models\nof speech production.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1539",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production and Acquisition  by Human Listeners"
    },
    "medina21_interspeech": {
      "authors": [
        [
          "Salvador",
          "Medina"
        ],
        [
          "Sarah",
          "Taylor"
        ],
        [
          "Mark",
          "Tiede"
        ],
        [
          "Alexander",
          "Hauptmann"
        ],
        [
          "Iain",
          "Matthews"
        ]
      ],
      "title": "Importance of Parasagittal Sensor Information in Tongue Motion Capture Through a Diphonic Analysis",
      "original": "1732",
      "page_count": 5,
      "order": 687,
      "p1": "3340",
      "pn": "3344",
      "abstract": [
        "Our study examines the information obtained by adding two parasagittal\nsensors to the standard midsagittal configuration of an Electromagnetic\nArticulography (EMA) observation of lingual articulation. In this work,\nwe present a large and phonetically balanced corpus obtained from an\nEMA recording session of a single English native speaker reading 1899\nsentences from the Harvard and TIMIT corpora. According to a statistical\nanalysis of the diphones produced during the recording session, the\nmotion captured by the parasagittal sensors has a low correlation to\nthe midsagittal sensors in the mediolateral direction. We perform a\ngeometric analysis of the lateral tongue by the measure of its width\nand using a proxy of the tongue&#8217;s curvature that is computed\nusing the Menger curvature. To provide a better understanding of the\ntongue sensor motion we present dynamic visualizations of all diphones.\nFinally, we present a summary of the velocity information computed\nfrom the tongue sensor information.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1732",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production and Acquisition  by Human Listeners"
    },
    "georges21_interspeech": {
      "authors": [
        [
          "Marc-Antoine",
          "Georges"
        ],
        [
          "Laurent",
          "Girin"
        ],
        [
          "Jean-Luc",
          "Schwartz"
        ],
        [
          "Thomas",
          "Hueber"
        ]
      ],
      "title": "Learning Robust Speech Representation with an Articulatory-Regularized Variational Autoencoder",
      "original": "1604",
      "page_count": 5,
      "order": 688,
      "p1": "3345",
      "pn": "3349",
      "abstract": [
        "It is increasingly considered that human speech perception and production\nboth rely on articulatory representations. In this paper, we investigate\nwhether this type of representation could improve the performances\nof a deep generative model (here a variational autoencoder) trained\nto encode and decode acoustic speech features. First we develop an\narticulatory model able to associate articulatory parameters describing\nthe jaw, tongue, lips and velum configurations with vocal tract shapes\nand spectral features. Then we incorporate these articulatory parameters\ninto a variational autoencoder applied on spectral features by using\na regularization technique that constrains part of the latent space\nto represent articulatory trajectories. We show that this articulatory\nconstraint improves model training by decreasing time to convergence\nand reconstruction loss at convergence, and yields better performance\nin a speech denoising task.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1604",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production and Acquisition  by Human Listeners"
    },
    "weston21_interspeech": {
      "authors": [
        [
          "Heather",
          "Weston"
        ],
        [
          "Laura L.",
          "Koenig"
        ],
        [
          "Susanne",
          "Fuchs"
        ]
      ],
      "title": "Changes in Glottal Source Parameter Values with Light to Moderate Physical Load",
      "original": "1881",
      "page_count": 5,
      "order": 689,
      "p1": "3350",
      "pn": "3354",
      "abstract": [
        "Engaging in everyday physical activities, like walking, initiates physiological\nprocesses that also affect parts of the body used for speech. However,\nit is currently unclear to what extent such activities affect phonatory\nprocesses, and in turn, the voice. The present exploratory study investigates\nhow selected glottal source parameters are affected by light and moderate\nphysical activity. Recordings of sustained vowel /a/ were obtained\nfrom 39 female speakers of German at rest, and during low-intensity\nand moderate-intensity cycling. Ten glottal source parameters thought\nto reflect different physiological states were investigated using VoiceSauce.\nEven during light activity, significant increases were found in f0,\nstrength of excitation and H1, and a decrease in harmonics-to-noise\nratio at higher frequencies. During moderate-intensity activity, significant\neffects were stronger and found for most parameters. However, considerable\nintra- and interspeaker variability was observed. These findings may\nbe relevant for applications in automatic speaker-state recognition.\nThey also underscore the importance of investigating individual-level\nresponses to better understand stress&#8211;voice interactions.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1881",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "vali21_interspeech": {
      "authors": [
        [
          "Mohammad Hassan",
          "Vali"
        ],
        [
          "Tom",
          "B\u00e4ckstr\u00f6m"
        ]
      ],
      "title": "End-to-End Optimized Multi-Stage Vector Quantization of Spectral Envelopes for Speech and Audio Coding",
      "original": "0867",
      "page_count": 5,
      "order": 690,
      "p1": "3355",
      "pn": "3359",
      "abstract": [
        "Spectral envelope modeling is an instrumental part of speech and audio\ncodecs, which can be used to enable efficient entropy coding of spectral\ncomponents. Overall optimization of codecs, including envelope models,\nhas however been difficult due to the complicated interactions between\ndifferent modules of the codec. In this paper, we study an end-to-end\noptimization methodology to optimize all modules in a codec integrally\nwith respect to each other while capturing all these complex interactions\nwith a global loss function. For the quantization of the spectral envelope\nparameters with a fixed bitrate, we use multi-stage vector quantization\nwhich gives high quality, but yet has a computational complexity which\ncan be realistically applied in embedded devices. The obtained results\ndemonstrate benefits in terms of PESQ and PSNR in comparison to the\n3GPP EVS, as well as our recently proposed PyAWNeS codecs.\n"
      ],
      "doi": "10.21437/Interspeech.2021-867",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "nareddula21_interspeech": {
      "authors": [
        [
          "Santhan Kumar Reddy",
          "Nareddula"
        ],
        [
          "Subrahmanyam",
          "Gorthi"
        ],
        [
          "Rama Krishna Sai S.",
          "Gorthi"
        ]
      ],
      "title": "Fusion-Net: Time-Frequency Information Fusion Y-Network for Speech Enhancement",
      "original": "1184",
      "page_count": 5,
      "order": 691,
      "p1": "3360",
      "pn": "3364",
      "abstract": [
        "This paper proposes a deep learning-based densely connected Y-Net as\nan effective network architecture for the fusion of time and frequency\ndomain loss functions for speech enhancement. The proposed architecture\nperforms speech enhancement in the time domain while fusing information\nfrom the frequency domain. Y-network consists of an encoder branch\nfollowed by two decoder branches, where the first and second decoder\nloss functions enforce speech enhancement in time and frequency domains\nrespectively. Each layer of the proposed network is formed with densely\nconnected blocks comprising dilated and causal convolutions for significant\nfeature collection and error backpropagation. The proposed model is\ntrained on a publicly available data set of 28 speakers with 40 different\nnoise conditions. The evaluations are performed on an independent,\nunseen test set of 2 speakers and 20 different noise conditions. The\nresults from the proposed method are compared with five state-of-the-art\nmethods using various metrics. The proposed method has resulted in\nan overall perceptual evaluation of speech quality of 3.4. It has outperformed\nthe existing methods by a significant margin in terms of all the evaluation\nmetrics.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1184",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "marcinek21_interspeech": {
      "authors": [
        [
          "\u013dubo\u0161",
          "Marcinek"
        ],
        [
          "Michael",
          "Stone"
        ],
        [
          "Rebecca",
          "Millman"
        ],
        [
          "Patrick",
          "Gaydecki"
        ]
      ],
      "title": "N-MTTL SI Model: Non-Intrusive Multi-Task Transfer Learning-Based Speech Intelligibility Prediction Model with Scenery Classification",
      "original": "1878",
      "page_count": 5,
      "order": 692,
      "p1": "3365",
      "pn": "3369",
      "abstract": [
        "The application of speech enhancement algorithms for hearing aids may\nnot always be beneficial to increasing speech intelligibility. Therefore,\na prior environment classification could be important. However, previous\nspeech intelligibility models do not provide any additional information\nregarding the reason for a decrease in speech intelligibility. We propose\na unique non-intrusive multi-task transfer learning-based speech intelligibility\nprediction model with scenery classification (N-MTTL SI model). The\nsolution combines a Mel-spectrogram analysis of the degraded speech\nsignal with transfer learning and multi-task learning to provide simultaneous\nspeech intelligibility prediction (task 1) and scenery classification\nof ten real-world noise conditions (task 2). The model utilises a pre-trained\nResNet architecture as an encoder for feature extraction. The prediction\naccuracy of the N-MTTL SI model for both tasks is high. Specifically,\nRMSE of speech intelligibility predictions for seen and unseen conditions\nis 3.76% and 4.06%. The classification accuracy is 98%. In addition,\nthe proposed solution demonstrates the potential of using pre-trained\ndeep learning models in the domain of speech intelligibility prediction.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1878",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "xia21b_interspeech": {
      "authors": [
        [
          "Yangyang",
          "Xia"
        ],
        [
          "Li-Wei",
          "Chen"
        ],
        [
          "Alexander",
          "Rudnicky"
        ],
        [
          "Richard M.",
          "Stern"
        ]
      ],
      "title": "Temporal Context in Speech Emotion Recognition",
      "original": "1840",
      "page_count": 5,
      "order": 693,
      "p1": "3370",
      "pn": "3374",
      "abstract": [
        "We investigate the importance of temporal context for speech emotion\nrecognition (SER). Two SER systems trained on traditional and learned\nfeatures, respectively, are developed to predict categorical labels\nof emotion. For traditional acoustical features, we study the combination\nof filterbank features and prosodic features and the impact on SER\nwhen the temporal context of these features is expanded by learnable\nspectro-temporal receptive fields (STRFs). Experiments show that the\nsystem trained on learnable STRFs outperforms other reported systems\nevaluated with a similar setup. We also demonstrate that the wav2vec\nfeatures, pretrained with long temporal context, are superior to traditional\nfeatures. We then introduce a novel segment-based learning objective\nto constrain our classifier to extract local emotion features from\nthe large temporal context. Combined with the learning objective and\nfine-tuning strategy, our top-line system using wav2vec features reaches\nstate-of-the-art performance on the IEMOCAP dataset.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1840",
      "author_area_id": "3",
      "author_area_label": "Paralinguistics in Speech and Language: Human and Automatic Analysis and Processing"
    },
    "li21j_interspeech": {
      "authors": [
        [
          "Hang",
          "Li"
        ],
        [
          "Wenbiao",
          "Ding"
        ],
        [
          "Zhongqin",
          "Wu"
        ],
        [
          "Zitao",
          "Liu"
        ]
      ],
      "title": "Learning Fine-Grained Cross Modality Excitement for Speech Emotion Recognition",
      "original": "0158",
      "page_count": 5,
      "order": 694,
      "p1": "3375",
      "pn": "3379",
      "abstract": [
        "Speech emotion recognition is a challenging task because the emotion\nexpression is complex, multimodal and fine-grained. In this paper,\nwe propose a novel multimodal deep learning approach to perform fine-grained\nemotion recognition from real-life speeches. We design a temporal alignment\nmean-max pooling mechanism to capture the subtle and fine-grained emotions\nimplied in every utterance. In addition, we propose a cross modality\nexcitement module to conduct sample-specific adjustment on cross modality\nembeddings and adaptively recalibrate the corresponding values by its\naligned latent features from the other modality. Our proposed model\nis evaluated on two well-known real-world speech emotion recognition\ndatasets. The results demonstrate that our approach is superior on\nthe prediction tasks for multimodal speech utterances, and it outperforms\na wide range of baselines in terms of prediction accuracy. Furthermore,\nwe conduct detailed ablation studies to show that our temporal alignment\nmean-max pooling mechanism and cross modality excitement significantly\ncontribute to the promising results. In order to encourage the research\nreproducibility, we make the code publicly available.\n"
      ],
      "doi": "10.21437/Interspeech.2021-158",
      "author_area_id": "3",
      "author_area_label": "Paralinguistics in Speech and Language: Human and Automatic Analysis and Processing"
    },
    "vaaras21_interspeech": {
      "authors": [
        [
          "Einari",
          "Vaaras"
        ],
        [
          "Sari",
          "Ahlqvist-Bj\u00f6rkroth"
        ],
        [
          "Konstantinos",
          "Drossos"
        ],
        [
          "Okko",
          "R\u00e4s\u00e4nen"
        ]
      ],
      "title": "Automatic Analysis of the Emotional Content of Speech in Daylong Child-Centered Recordings from a Neonatal Intensive Care Unit",
      "original": "0303",
      "page_count": 5,
      "order": 695,
      "p1": "3380",
      "pn": "3384",
      "abstract": [
        "Researchers have recently started to study how the emotional speech\nheard by young infants can affect their developmental outcomes. As\na part of this research, hundreds of hours of daylong recordings from\npreterm infants&#8217; audio environments were collected from two hospitals\nin Finland and Estonia in the context of so-called APPLE study. In\norder to analyze the emotional content of speech in such a massive\ndataset, an automatic speech emotion recognition (SER) system is required.\nHowever, there are no emotion labels or existing in-domain SER systems\nto be used for this purpose. In this paper, we introduce this initially\nunannotated large-scale real-world audio dataset and describe the development\nof a functional SER system for the Finnish subset of the data. We explore\nthe effectiveness of alternative state-of-the-art techniques to deploy\na SER system to a new domain, comparing cross-corpus generalization,\nWGAN-based domain adaptation, and active learning in the task. As a\nresult, we show that the best-performing models are able to achieve\na classification performance of 73.4% unweighted average recall (UAR)\nand 73.2% UAR for a binary classification for valence and arousal,\nrespectively. The results also show that active learning achieves the\nmost consistent performance compared to the two alternatives.\n"
      ],
      "doi": "10.21437/Interspeech.2021-303",
      "author_area_id": "3",
      "author_area_label": "Paralinguistics in Speech and Language: Human and Automatic Analysis and Processing"
    },
    "qian21_interspeech": {
      "authors": [
        [
          "Fan",
          "Qian"
        ],
        [
          "Jiqing",
          "Han"
        ]
      ],
      "title": "Multimodal Sentiment Analysis with Temporal Modality Attention",
      "original": "0487",
      "page_count": 5,
      "order": 696,
      "p1": "3385",
      "pn": "3389",
      "abstract": [
        "Multimodal sentiment analysis is an important research that involves\nintegrating information from multiple modalities to identify a speaker\nunderlying attitude. The core challenge is to model cross-modal interactions\nwhich span across both the different modalities and time. Although\ngreat progress has been made, the existing methods are still not sufficient\nfor modeling cross-modal interactions. Inspired by previous research\nin cognitive neuroscience that humans perceive intentions through focusing\non different modalities over time, in this paper we propose a novel\nattention mechanism called Temporal Modality Attention (TMA) to simulate\nthis process. Cross-modal interactions are modeled using this human-like\nTMA mechanism which focuses on specific modalities dynamically as recurrent\nmodeling proceed. To verify the effectiveness of TMA, we conduct comprehensive\nexperiments on multiple benchmark datasets for multimodal sentiment\nanalysis. The results show a consistently significant improvement compared\nto the baseline models.\n"
      ],
      "doi": "10.21437/Interspeech.2021-487",
      "author_area_id": "3",
      "author_area_label": "Paralinguistics in Speech and Language: Human and Automatic Analysis and Processing"
    },
    "t21_interspeech": {
      "authors": [
        [
          "Mani Kumar",
          "T"
        ],
        [
          "Enrique",
          "Sanchez"
        ],
        [
          "Georgios",
          "Tzimiropoulos"
        ],
        [
          "Timo",
          "Giesbrecht"
        ],
        [
          "Michel",
          "Valstar"
        ]
      ],
      "title": "Stochastic Process Regression for Cross-Cultural Speech Emotion Recognition",
      "original": "0610",
      "page_count": 5,
      "order": 697,
      "p1": "3390",
      "pn": "3394",
      "abstract": [
        "In this work, we pose continuous apparent emotion recognition from\nspeech as a problem of learning distributions of functions, and do\nso using Stochastic Processes Regression. We presume that the relation\nbetween speech signals and their corresponding emotion labels is governed\nby some underlying stochastic process, in contrast to existing speech\nemotion recognition methods that are mostly based on deterministic\nregression models (static or recurrent). We treat each training sequence\nas an instance of the underlying stochastic process which we aim to\ndiscover using a neural latent variable model, which approximates the\ndistribution of functions with a stochastic latent variable using an\nencoder-decoder composition: the encoder infers the distribution over\nthe latent variable, which the decoder uses to predict the distribution\nof output emotion labels. To this end, we build on the previously proposed\nNeural Processes theory by using (a). noisy label predictions of a\nbackbone instead of ground truth labels for latent variable inference\nand (b). recurrent encoder-decoder models to alleviate the effect of\ncommonly encountered temporal misalignment between audio features and\nemotion labels due to annotator reaction lag. We validated our method\non AVEC&#8217;19 cross-cultural emotion recognition dataset, achieving\nstate-of-the-art results.\n"
      ],
      "doi": "10.21437/Interspeech.2021-610",
      "author_area_id": "3",
      "author_area_label": "Paralinguistics in Speech and Language: Human and Automatic Analysis and Processing"
    },
    "li21k_interspeech": {
      "authors": [
        [
          "Haoqi",
          "Li"
        ],
        [
          "Yelin",
          "Kim"
        ],
        [
          "Cheng-Hao",
          "Kuo"
        ],
        [
          "Shrikanth S.",
          "Narayanan"
        ]
      ],
      "title": "Acted vs. Improvised: Domain Adaptation for Elicitation Approaches in Audio-Visual Emotion Recognition",
      "original": "0666",
      "page_count": 5,
      "order": 698,
      "p1": "3395",
      "pn": "3399",
      "abstract": [
        "Key challenges in developing generalized automatic emotion recognition\nsystems include scarcity of labeled data and lack of gold-standard\nreferences. Even for the cues that are labeled as the same emotion\ncategory, the variability of associated expressions can be high depending\non the elicitation context e.g., emotion elicited during improvised\nconversations vs. acted sessions with predefined scripts. In this work,\nwe regard the emotion elicitation approach as domain knowledge, and\nexplore domain transfer learning techniques on emotional utterances\ncollected under different emotion elicitation approaches, particularly\nwith limited labeled target samples. Our emotion recognition model\ncombines the gradient reversal technique with an entropy loss function\nas well as the softlabel loss, and the experiment results show that\ndomain transfer learning methods can be employed to alleviate the domain\nmismatch between different elicitation approaches. Our work provides\nnew insights into emotion data collection, particularly the impact\nof its elicitation strategies, and the importance of domain adaptation\nin emotion recognition aiming for generalized systems.\n"
      ],
      "doi": "10.21437/Interspeech.2021-666",
      "author_area_id": "3",
      "author_area_label": "Paralinguistics in Speech and Language: Human and Automatic Analysis and Processing"
    },
    "pepino21_interspeech": {
      "authors": [
        [
          "Leonardo",
          "Pepino"
        ],
        [
          "Pablo",
          "Riera"
        ],
        [
          "Luciana",
          "Ferrer"
        ]
      ],
      "title": "Emotion Recognition from Speech Using wav2vec 2.0 Embeddings",
      "original": "0703",
      "page_count": 5,
      "order": 699,
      "p1": "3400",
      "pn": "3404",
      "abstract": [
        "Emotion recognition datasets are relatively small, making the use of\ndeep learning techniques challenging. In this work, we propose a transfer\nlearning method for speech emotion recognition (SER) where features\nextracted from pre-trained wav2vec 2.0 models are used as input to\nshallow neural networks to recognize emotions from speech. We propose\na way to combine the output of several layers from the pre-trained\nmodel, producing richer speech representations than the model&#8217;s\noutput alone. We evaluate the proposed approaches on two standard emotion\ndatabases, IEMOCAP and RAVDESS, and compare different feature extraction\ntechniques using two wav2vec 2.0 models: a generic one, and one finetuned\nfor speech recognition. We also experiment with different shallow architectures\nfor our speech emotion recognition model, and report baseline results\nusing traditional features. Finally, we show that our best performing\nmodels have better average recall than previous approaches that use\ndeep neural networks trained on spectrograms and waveforms or shallow\nneural networks trained on features extracted from wav2vec 1.0.\n"
      ],
      "doi": "10.21437/Interspeech.2021-703",
      "author_area_id": "3",
      "author_area_label": "Paralinguistics in Speech and Language: Human and Automatic Analysis and Processing"
    },
    "liu21k_interspeech": {
      "authors": [
        [
          "Jiawang",
          "Liu"
        ],
        [
          "Haoxiang",
          "Wang"
        ]
      ],
      "title": "Graph Isomorphism Network for Speech Emotion Recognition",
      "original": "1154",
      "page_count": 5,
      "order": 700,
      "p1": "3405",
      "pn": "3409",
      "abstract": [
        "Previous deep learning approaches such as Convolutional Neural Network\n(CNN) and Long Short-Term Memory (LSTM) have been broadly used in speech\nemotion recognition (SER). In these approaches, speech signals are\ngenerally modeled in the Euclidean space. In this paper, a novel SER\nmodel (LSTM-GIN) is proposed, which applies Graph Isomorphism Network\n(GIN) on LSTM outputs for global emotion modeling in the non-Euclidean\nspace. In our LSTM-GIN model, speech signals are represented as graph-structured\ndata so that we can better extract global feature representation. The\ndeep frame-level features generated from the bidirectional LSTM are\nconverted into an undirected graph with nodes represented by frame-level\nfeatures and connections defined according to temporal relations between\nspeech frames. GIN is adopted to classify the graph representations\nof utterances, as it is proved of excellent discriminative power in\ncomparative experiments. We conduct experiments on the IEMOCAP dataset,\nand the results show that our proposed LSTM-GIN model surpasses other\nrecent graph-based models and deep learning models by achieving 64.65%\nof weighted accuracy (WA) and 65.53% of unweighted accuracy (UA).\n"
      ],
      "doi": "10.21437/Interspeech.2021-1154",
      "author_area_id": "3",
      "author_area_label": "Paralinguistics in Speech and Language: Human and Automatic Analysis and Processing"
    },
    "kumawat21_interspeech": {
      "authors": [
        [
          "Pooja",
          "Kumawat"
        ],
        [
          "Aurobinda",
          "Routray"
        ]
      ],
      "title": "Applying TDNN Architectures for Analyzing Duration Dependencies on Speech Emotion Recognition",
      "original": "2168",
      "page_count": 5,
      "order": 701,
      "p1": "3410",
      "pn": "3414",
      "abstract": [
        "We have analyzed the Time Delay Neural Network (TDNN) based architectures\nfor speech emotion classification. TDNN models efficiently capture\nthe temporal information and provide an utterance level prediction.\nEmotions are dynamic in nature and require temporal context for reliable\nprediction. In our work, we have applied the TDNN based x-vector and\nemphasized channel attention, propagation &amp; aggregation based TDNN\n(ECAPA-TDNN) architectures for speech emotion identification with RAVDESS,\nEmo-DB, and IEMOCAP databases. The results show that the TDNN architectures\nare very efficient for predicting emotion classes and ECAPA-TDNN outperforms\nthe TDNN based x-vector architecture. Next, we investigated the performance\nof ECAPA-TDNN with various training chunk durations and test utterance\ndurations. We have identified that in spite of very promising emotion\nrecognition performance the TDNN models have a strong training chunk\nduration-based bias. Earlier research work revealed that individual\nemotion class accuracy depends largely on the test utterance duration.\nMost of these studies were based on frame level emotions predictions.\nHowever, utterance level based emotion recognition is relatively less\nexplored. The results show that even with the TDNN models, the accuracy\nof the different emotion classes is dependent on the utterance duration.\n"
      ],
      "doi": "10.21437/Interspeech.2021-2168",
      "author_area_id": "3",
      "author_area_label": "Paralinguistics in Speech and Language: Human and Automatic Analysis and Processing"
    },
    "keesing21_interspeech": {
      "authors": [
        [
          "Aaron",
          "Keesing"
        ],
        [
          "Yun Sing",
          "Koh"
        ],
        [
          "Michael",
          "Witbrock"
        ]
      ],
      "title": "Acoustic Features and Neural Representations for Categorical Emotion Recognition from Speech",
      "original": "2217",
      "page_count": 5,
      "order": 702,
      "p1": "3415",
      "pn": "3419",
      "abstract": [
        "Many features have been proposed for use in speech emotion recognition,\nfrom signal processing features to bag-of-audio-words (BoAW) models\nto abstract neural representations. Some of these feature types have\nnot been directly compared across a large number of speech corpora\nto determine performance differences. We propose a full factorial design\nand to compare speech processing features, BoAW and neural representations\non 17 emotional speech datasets. We measure the performance of features\nin a categorical emotion classification problem for each dataset, using\nspeaker-independent cross-validation with diverse classifiers. Results\nshow statistically significant differences between features and between\nclassifiers, with large effect sizes between features. In particular,\nstandard acoustic feature sets still perform competitively to neural\nrepresentations, while neural representations have a larger range of\nperformance, and BoAW features lie in the middle. The best and worst\nneural representations were wav2veq and VGGish, respectively, with\nwav2vec performing best out of all tested features. These results indicate\nthat standard acoustic feature sets are still very useful baselines\nfor emotional classification, but high quality neural speech representations\ncan be better.\n"
      ],
      "doi": "10.21437/Interspeech.2021-2217",
      "author_area_id": "3",
      "author_area_label": "Paralinguistics in Speech and Language: Human and Automatic Analysis and Processing"
    },
    "shon21_interspeech": {
      "authors": [
        [
          "Suwon",
          "Shon"
        ],
        [
          "Pablo",
          "Brusco"
        ],
        [
          "Jing",
          "Pan"
        ],
        [
          "Kyu J.",
          "Han"
        ],
        [
          "Shinji",
          "Watanabe"
        ]
      ],
      "title": "Leveraging Pre-Trained Language Model for Speech Sentiment Analysis",
      "original": "1723",
      "page_count": 5,
      "order": 703,
      "p1": "3420",
      "pn": "3424",
      "abstract": [
        "In this paper, we explore the use of pre-trained language models to\nlearn sentiment information of written texts for speech sentiment analysis.\nFirst, we investigate how useful a pre-trained language model would\nbe in a 2-step pipeline approach employing Automatic Speech Recognition\n(ASR) and transcripts-based sentiment analysis separately. Second,\nwe propose a pseudo label-based semi-supervised training strategy using\na language model on an end-to-end speech sentiment approach to take\nadvantage of a large, but unlabeled speech dataset for training. Although\nspoken and written texts have different linguistic characteristics,\nthey can complement each other in understanding sentiment. Therefore,\nthe proposed system can not only model acoustic characteristics to\nbear sentiment-specific information in speech signals, but learn latent\ninformation to carry sentiments in the text representation. In these\nexperiments, we demonstrate the proposed approaches improve F1 scores\nconsistently compared to systems without a language model. Moreover,\nwe also show that the proposed framework can reduce 65% of human supervision\nby leveraging a large amount of data without human sentiment annotation\nand boost performance in a low-resource condition where the human sentiment\nannotation is not available enough.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1723",
      "author_area_id": "3",
      "author_area_label": "Paralinguistics in Speech and Language: Human and Automatic Analysis and Processing"
    },
    "hou21b_interspeech": {
      "authors": [
        [
          "Wenxin",
          "Hou"
        ],
        [
          "Jindong",
          "Wang"
        ],
        [
          "Xu",
          "Tan"
        ],
        [
          "Tao",
          "Qin"
        ],
        [
          "Takahiro",
          "Shinozaki"
        ]
      ],
      "title": "Cross-Domain Speech Recognition with Unsupervised Character-Level Distribution Matching",
      "original": "0057",
      "page_count": 5,
      "order": 704,
      "p1": "3425",
      "pn": "3429",
      "abstract": [
        "End-to-end automatic speech recognition (ASR) can achieve promising\nperformance with large-scale training data. However, it is known that\ndomain mismatch between training and testing data often leads to a\ndegradation of recognition accuracy. In this work, we focus on the\nunsupervised domain adaptation for ASR and propose CMatch, a Character-level\ndistribution matching method to perform fine-grained adaptation between\neach character in two domains. First, to obtain labels for the features\nbelonging to each character, we achieve frame-level label assignment\nusing the Connectionist Temporal Classification (CTC) pseudo labels.\nThen, we match the character-level distributions using Maximum Mean\nDiscrepancy. We train our algorithm using the self-training technique.\nExperiments on the Libri-Adapt dataset show that our proposed approach\nachieves 14.39% and 16.50% relative Word Error Rate (WER) reduction\non both cross-device and cross-environment ASR. We also comprehensively\nanalyze the different strategies for frame-level label assignment and\nTransformer adaptations.\n"
      ],
      "doi": "10.21437/Interspeech.2021-57",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "kanda21_interspeech": {
      "authors": [
        [
          "Naoyuki",
          "Kanda"
        ],
        [
          "Guoli",
          "Ye"
        ],
        [
          "Yu",
          "Wu"
        ],
        [
          "Yashesh",
          "Gaur"
        ],
        [
          "Xiaofei",
          "Wang"
        ],
        [
          "Zhong",
          "Meng"
        ],
        [
          "Zhuo",
          "Chen"
        ],
        [
          "Takuya",
          "Yoshioka"
        ]
      ],
      "title": "Large-Scale Pre-Training of End-to-End Multi-Talker ASR for Meeting Transcription with Single Distant Microphone",
      "original": "0102",
      "page_count": 5,
      "order": 705,
      "p1": "3430",
      "pn": "3434",
      "abstract": [
        "Transcribing meetings containing overlapped speech with only a single\ndistant microphone (SDM) has been one of the most challenging problems\nfor automatic speech recognition (ASR). While various approaches have\nbeen proposed, all previous studies on the monaural overlapped speech\nrecognition problem were based on either simulation data or small-scale\nreal data. In this paper, we extensively investigate a two-step approach\nwhere we first pre-train a serialized output training (SOT)-based multi-talker\nASR by using large-scale simulation data and then fine-tune the model\nwith a small amount of real meeting data. Experiments are conducted\nby utilizing 75 thousand (K) hours of our internal single-talker recording\nto simulate a total of 900K hours of multi-talker audio segments for\nsupervised pre-training. With fine-tuning on the 70 hours of the AMI-SDM\ntraining data, our SOT ASR model achieves a word error rate (WER) of\n21.2% for the AMI-SDM evaluation set while automatically counting speakers\nin each test segment. This result is not only significantly better\nthan the previous state-of-the-art WER of 36.4% with oracle utterance\nboundary information but also better than a result by a similarly fine-tuned\nsingle-talker ASR model applied to beamformed audio.\n"
      ],
      "doi": "10.21437/Interspeech.2021-102",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "lu21b_interspeech": {
      "authors": [
        [
          "Liang",
          "Lu"
        ],
        [
          "Zhong",
          "Meng"
        ],
        [
          "Naoyuki",
          "Kanda"
        ],
        [
          "Jinyu",
          "Li"
        ],
        [
          "Yifan",
          "Gong"
        ]
      ],
      "title": "On Minimum Word Error Rate Training of the Hybrid Autoregressive Transducer",
      "original": "0161",
      "page_count": 5,
      "order": 706,
      "p1": "3435",
      "pn": "3439",
      "abstract": [
        "Hybrid Autoregressive Transducer (HAT) is a recently proposed end-to-end\nacoustic model that extends the standard Recurrent Neural Network Transducer\n(RNN-T) for the purpose of the external language model (LM) fusion.\nIn HAT, the blank probability and the label probability are estimated\nusing two separate probability distributions, which provides a more\naccurate solution for internal LM score estimation, and thus works\nbetter when combining with an external LM. Previous work mainly focuses\non HAT model training with the negative log-likelihood loss, while\nin this paper, we study the minimum word error rate (MWER) training\nof HAT &#8212; a criterion that is closer to the evaluation metric\nfor speech recognition, and has been successfully applied to other\ntypes of end-to-end models such as sequence-to-sequence (S2S) and RNN-T\nmodels. From experiments with around 30,000 hours of training data,\nwe show that MWER training can improve the accuracy of HAT models,\nwhile at the same time, improving the robustness of the model against\nthe decoding hyper-parameters such as length normalization and decoding\nbeam during inference.\n"
      ],
      "doi": "10.21437/Interspeech.2021-161",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "kim21j_interspeech": {
      "authors": [
        [
          "Jaeyoung",
          "Kim"
        ],
        [
          "Han",
          "Lu"
        ],
        [
          "Anshuman",
          "Tripathi"
        ],
        [
          "Qian",
          "Zhang"
        ],
        [
          "Hasim",
          "Sak"
        ]
      ],
      "title": "Reducing Streaming ASR Model Delay with Self Alignment",
      "original": "0322",
      "page_count": 5,
      "order": 707,
      "p1": "3440",
      "pn": "3444",
      "abstract": [
        "Reducing prediction delay for streaming end-to-end ASR models with\nminimal performance regression is a challenging problem. Constrained\nalignment is a well-known existing approach that penalizes predicted\nword boundaries using external low-latency acoustic models. On the\ncontrary, recently proposed FastEmit is a sequence-level delay regularization\nscheme encouraging vocabulary tokens over blanks without any reference\nalignments. Although all these schemes are successful in reducing delay,\nASR word error rate (WER) often severely degrades after applying these\ndelay constraining schemes. In this paper, we propose a novel delay\nconstraining method, named self alignment. Self alignment does not\nrequire external alignment models. Instead, it utilizes Viterbi forced-alignments\nfrom the trained model to find the lower latency alignment direction.\nFrom LibriSpeech evaluation, self alignment outperformed existing schemes:\n25% and 56% less delay compared to FastEmit and constrained alignment\nat the similar word error rate. For Voice Search evaluation, 12% and\n25% delay reductions were achieved compared to FastEmit and constrained\nalignment with more than 2% WER improvements.\n"
      ],
      "doi": "10.21437/Interspeech.2021-322",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "diwan21b_interspeech": {
      "authors": [
        [
          "Anuj",
          "Diwan"
        ],
        [
          "Preethi",
          "Jyothi"
        ]
      ],
      "title": "Reduce and Reconstruct: ASR for Low-Resource Phonetic Languages",
      "original": "0644",
      "page_count": 5,
      "order": 708,
      "p1": "3445",
      "pn": "3449",
      "abstract": [
        "This work presents a seemingly simple but effective technique to improve\nlow-resource ASR systems for phonetic languages. By identifying sets\nof acoustically similar graphemes in these languages, we first reduce\nthe output alphabet of the ASR system using linguistically meaningful\nreductions and then reconstruct the original alphabet using a standalone\nmodule. We demonstrate that this lessens the burden and improves the\nperformance of low-resource end-to-end ASR systems (because only reduced-alphabet\npredictions are needed) and that it is possible to design a very simple\nbut effective reconstruction module that recovers sequences in the\noriginal alphabet from sequences in the reduced alphabet. We present\na finite state transducer-based reconstruction module that operates\non the 1-best ASR hypothesis in the reduced alphabet. We demonstrate\nthe efficacy of our proposed technique using ASR systems for two Indian\nlanguages, Gujarati and Telugu. With access to only 10 hrs of speech\ndata, we obtain relative WER reductions of up to 7% compared to systems\nthat do not use any reduction.\n"
      ],
      "doi": "10.21437/Interspeech.2021-644",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "fukuda21_interspeech": {
      "authors": [
        [
          "Takashi",
          "Fukuda"
        ],
        [
          "Samuel",
          "Thomas"
        ]
      ],
      "title": "Knowledge Distillation Based Training of Universal ASR Source Models for Cross-Lingual Transfer",
      "original": "0796",
      "page_count": 5,
      "order": 709,
      "p1": "3450",
      "pn": "3454",
      "abstract": [
        "In this paper we introduce a novel knowledge distillation based framework\nfor training universal source models. In our proposed approach for\nautomatic speech recognition (ASR), multilingual source models are\nfirst trained using multiple language-dependent resources before being\nused to initialize language specific target models in low resource\nsettings. For the proposed source models to be effective in cross-lingual\ntransfer to novel target languages, the training framework encourages\nthe models to perform accurate universal phone classification while\nignoring any language-dependent characteristics present in the training\ndata set. These two goals are achieved by applying knowledge distillation\nto improve the models&#8217; universal phone classification performance\nalong with a shuffling mechanism that alleviates any language specific\ndependencies that might be learned. The benefits of this proposed technique\nare demonstrated in several practical settings, where either large\namounts or only limited quantities of unbalanced multilingual data\nresources are available for source model creation. Compared to a conventional\nknowledge transfer learning method, the proposed approaches achieve\na relative WER reduction of 8&#8211;10% in streaming ASR settings for\nvarious low resource target languages.\n"
      ],
      "doi": "10.21437/Interspeech.2021-796",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "ray21_interspeech": {
      "authors": [
        [
          "Swayambhu Nath",
          "Ray"
        ],
        [
          "Minhua",
          "Wu"
        ],
        [
          "Anirudh",
          "Raju"
        ],
        [
          "Pegah",
          "Ghahremani"
        ],
        [
          "Raghavendra",
          "Bilgi"
        ],
        [
          "Milind",
          "Rao"
        ],
        [
          "Harish",
          "Arsikere"
        ],
        [
          "Ariya",
          "Rastrow"
        ],
        [
          "Andreas",
          "Stolcke"
        ],
        [
          "Jasha",
          "Droppo"
        ]
      ],
      "title": "Listen with Intent: Improving Speech Recognition with Audio-to-Intent Front-End",
      "original": "0836",
      "page_count": 5,
      "order": 710,
      "p1": "3455",
      "pn": "3459",
      "abstract": [
        "Comprehending the overall intent of an utterance helps a listener recognize\nthe individual words spoken. Inspired by this fact, we perform a novel\nstudy of the impact of explicitly incorporating intent representations\nas additional information to improve a recurrent neural network-transducer\n(RNN-T) based automatic speech recognition (ASR) system. An audio-to-intent\n(A2I) model encodes the intent of the utterance in the form of embeddings\nor posteriors, and these are used as auxiliary inputs for RNN-T training\nand inference. Experimenting with a 50k-hour far-field English speech\ncorpus, this study shows that when running the system in <i>non-streaming</i>\nmode, where intent representation is extracted from the entire utterance\nand then used to bias streaming RNN-T search from the start, it provides\na 5.56% relative word error rate reduction (WERR). On the other hand,\na <i>streaming</i> system using per-frame intent posteriors as extra\ninputs for the RNN-T ASR system yields a 3.33% relative WERR. A further\ndetailed analysis of the streaming system indicates that our proposed\nmethod brings especially good gain on media-playing related intents\n(e.g. 9.12% relative WERR on PlayMusicIntent).\n"
      ],
      "doi": "10.21437/Interspeech.2021-836",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "lu21c_interspeech": {
      "authors": [
        [
          "Zhiyun",
          "Lu"
        ],
        [
          "Wei",
          "Han"
        ],
        [
          "Yu",
          "Zhang"
        ],
        [
          "Liangliang",
          "Cao"
        ]
      ],
      "title": "Exploring Targeted Universal Adversarial Perturbations to End-to-End ASR Models",
      "original": "1668",
      "page_count": 5,
      "order": 711,
      "p1": "3460",
      "pn": "3464",
      "abstract": [
        "Although end-to-end automatic speech recognition (e2e ASR) models are\nwidely deployed in many applications, there have been very few studies\nto understand models&#8217; robustness against adversarial perturbations.\nIn this paper, we explore whether a targeted universal perturbation\nvector exists for e2e ASR models. Our goal is to find perturbations\nthat can mislead the models to predict the given targeted transcript\nsuch as &#8220;thank you&#8221; or empty string on any input utterance.\nWe study two different attacks, namely additive and prepending perturbations,\nand their performances on the state-of-the-art LAS, CTC and RNN-T models.\nWe find that LAS is the most vulnerable to perturbations among the\nthree models. RNN-T is more robust against additive perturbations,\nespecially on long utterances. And CTC is robust against both additive\nand prepending perturbations. To attack RNN-T, we find prepending perturbation\nis more effective than the additive perturbation, and can mislead the\nmodels to predict the same short target on utterances of arbitrary\nlength.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1668",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "delrio21_interspeech": {
      "authors": [
        [
          "Miguel",
          "Del Rio"
        ],
        [
          "Natalie",
          "Delworth"
        ],
        [
          "Ryan",
          "Westerman"
        ],
        [
          "Michelle",
          "Huang"
        ],
        [
          "Nishchal",
          "Bhandari"
        ],
        [
          "Joseph",
          "Palakapilly"
        ],
        [
          "Quinten",
          "McNamara"
        ],
        [
          "Joshua",
          "Dong"
        ],
        [
          "Piotr",
          "\u017belasko"
        ],
        [
          "Miguel",
          "Jett\u00e9"
        ]
      ],
      "title": "Earnings-21: A Practical Benchmark for ASR in the Wild",
      "original": "1915",
      "page_count": 5,
      "order": 712,
      "p1": "3465",
      "pn": "3469",
      "abstract": [
        "Commonly used speech corpora inadequately challenge academic and commercial\nASR systems. In particular, speech corpora lack metadata needed for\ndetailed analysis and WER measurement. In response, we present <i>Earnings-21</i>,\na 39-hour corpus of earnings calls containing entity-dense speech from\nnine different financial sectors. This corpus is intended to benchmark\nASR systems in the wild with special attention towards named entity\nrecognition. We benchmark four commercial ASR models, two internal\nmodels built with open-source tools, and an open-source LibriSpeech\nmodel and discuss their differences in performance on <i>Earnings-21</i>.\nUsing our recently released <i>fstalign</i> tool, we provide a candid\nanalysis of each model&#8217;s recognition capabilities under different\npartitions. Our analysis finds that ASR accuracy for certain NER categories\nis poor, presenting a significant impediment to transcript comprehension\nand usage. <i>Earnings-21</i> bridges academic and commercial ASR system\nevaluation and enables further research on entity modeling and WER\non real world audio.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1915",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "sun21c_interspeech": {
      "authors": [
        [
          "Eric",
          "Sun"
        ],
        [
          "Jinyu",
          "Li"
        ],
        [
          "Zhong",
          "Meng"
        ],
        [
          "Yu",
          "Wu"
        ],
        [
          "Jian",
          "Xue"
        ],
        [
          "Shujie",
          "Liu"
        ],
        [
          "Yifan",
          "Gong"
        ]
      ],
      "title": "Improving Multilingual Transformer Transducer Models by Reducing Language Confusions",
      "original": "1949",
      "page_count": 5,
      "order": 713,
      "p1": "3470",
      "pn": "3474",
      "abstract": [
        "In end-to-end multilingual speech recognition, the hypotheses in one\nlanguage could include word tokens from other languages. Language confusions\nhappen even more frequently when language identifier (LID) is not present\nduring inference. In this paper, we explore to reduce language confusions\nwithout using LID in model inference by creating models with multiple\noutput heads and use sequence probability to select the correct head\nfor output hypotheses. We propose head grouping to merge several language\noutputs into one head to save runtime cost. Head groups are decided\nby the distances among language clusters learned through language embedding\nvectors to separate confusable languages apart. We further propose\nprediction network sharing for languages from the same family. By jointly\napplying head grouping and prediction network sharing, training data\nfrom the same family languages is better shared while confusable languages\nare divided into different heads to reduce language confusions. Our\nexperiments demonstrate that our multilingual transformer transducer\nmodels based on multi-head outputs achieve on average 7.8% and 10.9%\nrelative word error rate reductions without LID being used in inference\nfrom one-head baseline model with affordably increased runtime cost\non 10 European languages.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1949",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "ali21b_interspeech": {
      "authors": [
        [
          "Ahmed",
          "Ali"
        ],
        [
          "Shammur Absar",
          "Chowdhury"
        ],
        [
          "Amir",
          "Hussein"
        ],
        [
          "Yasser",
          "Hifny"
        ]
      ],
      "title": "Arabic Code-Switching Speech Recognition Using Monolingual Data",
      "original": "2231",
      "page_count": 5,
      "order": 714,
      "p1": "3475",
      "pn": "3479",
      "abstract": [
        "Code-switching in automatic speech recognition (ASR) is an important\nchallenge due to globalization. Recent research in multilingual ASR\nshows potential improvement over monolingual systems. We study key\nissues related to multilingual modeling for ASR through a series of\nlarge-scale ASR experiments. Our innovative framework deploys a multi-graph\napproach in the weighted finite state transducers (WFST) framework.\nWe compare our WFST decoding strategies with a transformer sequence\nto sequence system trained on the same data. Given a code-switching\nscenario between Arabic and English languages, our results show that\nthe WFST decoding approaches were more suitable for the intersentential\ncode-switching datasets. In addition, the transformer system performed\nbetter for intrasentential code-switching task. With this study, we\nrelease an artificially generated development and test sets, along\nwith ecological code-switching test set, to benchmark the ASR performance.\n"
      ],
      "doi": "10.21437/Interspeech.2021-2231",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "eisenberg21_interspeech": {
      "authors": [
        [
          "Aviad",
          "Eisenberg"
        ],
        [
          "Boaz",
          "Schwartz"
        ],
        [
          "Sharon",
          "Gannot"
        ]
      ],
      "title": "Online Blind Audio Source Separation Using Recursive Expectation-Maximization",
      "original": "0662",
      "page_count": 5,
      "order": 715,
      "p1": "3480",
      "pn": "3484",
      "abstract": [
        "The challenging problem of online multi-microphone blind audio source\nseparation (BASS) in noisy environment is addressed in this paper.\nWe present a sequential, non-iterative, algorithm based on the recursive\nEM (REM) framework. In the proposed algorithm, the compete-data, which\nconstitutes the separated sources and residual noise, is estimated\nin the E-step by applying a multichannel Wiener filter (MCWF); and\nthe corresponding parameters, comprised of acoustic transfer functions\n(ATFs) relating the sources and the microphones and power spectral\ndensities (PSDs) of the desired sources, are sequentially estimated\nin the M-step. The separated speech signals are further enhanced using\nmatched-filter beamformers. The performance of the algorithm is demonstrated\nin terms of the separation capabilities, the resulting speech intelligibility\nand the ability to track the direction of arrival (DOA) of the moving\nsources.\n"
      ],
      "doi": "10.21437/Interspeech.2021-662",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "luo21e_interspeech": {
      "authors": [
        [
          "Yi",
          "Luo"
        ],
        [
          "Cong",
          "Han"
        ],
        [
          "Nima",
          "Mesgarani"
        ]
      ],
      "title": "Empirical Analysis of Generalized Iterative Speech Separation Networks",
      "original": "1161",
      "page_count": 5,
      "order": 716,
      "p1": "3485",
      "pn": "3489",
      "abstract": [
        "Although most existing speech separation networks are designed as a\none-pass pipeline where the sources are directly estimated from the\nmixture, multi-pass or iterative pipelines have been shown to be effective\nby designing multiple rounds of separation and utilizing separation\noutputs from a previous iteration as additional inputs for the next\niteration. Moreover, such iterative separation pipeline can also be\nextended to a more general framework where a training objective designed\nto minimize the discrepancy between the estimated and target sources\nis applied to different parts of the network. In this paper, we empirically\ninvestigate the effect of such generalized iterative separation pipeline\nby adjusting its configuration in multiple aspects in both training\nand inference phases. For the training phase, we compare the separation\nperformance of both time-domain and frequency-domain networks with\ndifferent numbers of iterations following the recent discussions on\nthe model architecture organizations. We also evaluate the effect of\nparameter sharing across iterations and the necessity of additional\ntraining objectives. For the inference phase, we measure the separation\nperformance of various numbers of iterations. Our results show that\niterative speech separation is a promising direction and deserves more\nin-depth analysis and exploration.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1161",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "neumann21_interspeech": {
      "authors": [
        [
          "Thilo von",
          "Neumann"
        ],
        [
          "Keisuke",
          "Kinoshita"
        ],
        [
          "Christoph",
          "Boeddeker"
        ],
        [
          "Marc",
          "Delcroix"
        ],
        [
          "Reinhold",
          "Haeb-Umbach"
        ]
      ],
      "title": "Graph-PIT: Generalized Permutation Invariant Training for Continuous Separation of Arbitrary Numbers of Speakers",
      "original": "1177",
      "page_count": 5,
      "order": 717,
      "p1": "3490",
      "pn": "3494",
      "abstract": [
        "Automatic transcription of meetings requires handling of overlapped\nspeech, which calls for continuous speech separation (CSS) systems.\nThe uPIT criterion was proposed for utterance-level separation with\nneural networks and introduces the constraint that the total number\nof speakers must not exceed the number of output channels. When processing\nmeeting-like data in a segment-wise manner, i.e., by separating overlapping\nsegments independently and stitching adjacent segments to continuous\noutput streams, this constraint has to be fulfilled for any segment.\nIn this contribution, we show that this constraint can be significantly\nrelaxed. We propose a novel graph-based PIT criterion, which casts\nthe assignment of utterances to output channels in a graph coloring\nproblem. It only requires that the number of concurrently active speakers\nmust not exceed the number of output channels. As a consequence, the\nsystem can process an arbitrary number of speakers and arbitrarily\nlong segments and thus can handle more diverse scenarios. Further,\nthe stitching algorithm for obtaining a consistent output order in\nneighboring segments is of less importance and can even be eliminated\ncompletely, not the least reducing the computational effort. Experiments\non meeting-style WSJ data show improvements in recognition performance\nover using the uPIT criterion.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1177",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "zhang21v_interspeech": {
      "authors": [
        [
          "Jisi",
          "Zhang"
        ],
        [
          "C\u0103t\u0103lin",
          "Zoril\u0103"
        ],
        [
          "Rama",
          "Doddipatla"
        ],
        [
          "Jon",
          "Barker"
        ]
      ],
      "title": "Teacher-Student MixIT for Unsupervised and Semi-Supervised Speech Separation",
      "original": "1243",
      "page_count": 5,
      "order": 718,
      "p1": "3495",
      "pn": "3499",
      "abstract": [
        "In this paper, we introduce a novel semi-supervised learning framework\nfor end-to-end speech separation. The proposed method first uses mixtures\nof unseparated sources and the mixture invariant training (MixIT) criterion\nto train a teacher model. The teacher model then estimates separated\nsources that are used to train a student model with standard permutation\ninvariant training (PIT). The student model can be fine-tuned with\nsupervised data, i.e., paired artificial mixtures and clean speech\nsources, and further improved via model distillation. Experiments with\nsingle and multi channel mixtures show that the teacher-student training\nresolves the over-separation problem observed in the original MixIT\nmethod. Further, the semi-supervised performance is comparable to a\nfully-supervised separation system trained using ten times the amount\nof supervised data.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1243",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "delcroix21_interspeech": {
      "authors": [
        [
          "Marc",
          "Delcroix"
        ],
        [
          "Jorge Bennasar",
          "V\u00e1zquez"
        ],
        [
          "Tsubasa",
          "Ochiai"
        ],
        [
          "Keisuke",
          "Kinoshita"
        ],
        [
          "Shoko",
          "Araki"
        ]
      ],
      "title": "Few-Shot Learning of New Sound Classes for Target Sound Extraction",
      "original": "1369",
      "page_count": 5,
      "order": 719,
      "p1": "3500",
      "pn": "3504",
      "abstract": [
        "Target sound extraction consists of extracting the sound of a target\nacoustic event (AE) class from a mixture of AE sounds. It can be realized\nusing a neural network that extracts the target sound conditioned on\na 1-hot vector that represents the desired AE class. With this approach,\nembedding vectors associated with the AE classes are directly optimized\nfor the extraction of sound classes seen during training. However,\nit is not easy to extend this framework to new AE classes, i.e. unseen\nduring training. Recently, speech, music, or AE sound extraction based\non enrollment audio of the desired sound offers the potential of extracting\nany target sound in a mixture given only a short audio signal of a\nsimilar sound. In this work, we propose combining 1-hot- and enrollment-based\ntarget sound extraction, allowing optimal performance for seen AE classes\nand simple extension to new classes. In experiments with synthesized\nsound mixtures generated with the Freesound Dataset (FSD) datasets,\nwe demonstrate the benefit of the combined framework for both seen\nand new AE classes. Besides, we also propose adapting the embedding\nvectors obtained from a few enrollment audio samples (few-shot) to\nfurther improve performance on new classes.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1369",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "han21e_interspeech": {
      "authors": [
        [
          "Cong",
          "Han"
        ],
        [
          "Yi",
          "Luo"
        ],
        [
          "Nima",
          "Mesgarani"
        ]
      ],
      "title": "Binaural Speech Separation of Moving Speakers With Preserved Spatial Cues",
      "original": "1372",
      "page_count": 5,
      "order": 720,
      "p1": "3505",
      "pn": "3509",
      "abstract": [
        "Binaural speech separation algorithms designed for augmented hearing\ntechnologies need to both improve the signal-to-noise ratio of individual\nspeakers and preserve their perceived location in space. The majority\nof binaural speech separation methods assume nonmoving speakers. As\na result, their application to real-world scenarios with freely moving\nspeakers requires block-wise adaptation which relies on short-term\ncontextual information and limits their performance. In this study,\nwe propose an alternative approach for utterance-level source separation\nwith moving speakers and in reverberant conditions. Our model makes\nuse of spectral and spatial features of speakers in a larger context\ncompared to the block-wise adaption methods. The model can implicitly\ntrack speakers within the utterance without the need for explicit tracking\nmodules. Experimental results on simulated moving multitalker speech\nshow that the proposed method can significantly outperform block-wise\nadaptation methods in both separation performance and preserving the\ninteraural cues across multiple conditions, which makes it suitable\nfor real-world augmented hearing applications.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1372",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "hu21_interspeech": {
      "authors": [
        [
          "Shell Xu",
          "Hu"
        ],
        [
          "Md. Rifat",
          "Arefin"
        ],
        [
          "Viet-Nhat",
          "Nguyen"
        ],
        [
          "Alish",
          "Dipani"
        ],
        [
          "Xaq",
          "Pitkow"
        ],
        [
          "Andreas Savas",
          "Tolias"
        ]
      ],
      "title": "AvaTr: One-Shot Speaker Extraction with Transformers",
      "original": "1378",
      "page_count": 5,
      "order": 721,
      "p1": "3510",
      "pn": "3514",
      "abstract": [
        "To extract the voice of a target speaker when mixed with a variety\nof other sounds, such as white and ambient noises or the voices of\ninterfering speakers, we extend the Transformer network [1] to attend\nthe most relevant information with respect to the target speaker given\nthe characteristics of his or her voices as a form of contextual information.\nThe idea has a natural interpretation in terms of the <i>selective\nattention theory</i> [2]. Specifically, we propose two models to incorporate\nthe voice characteristics in Transformer based on different insights\nof where the feature selection should take place. Both models yield\nexcellent performance, on par or better than published state-of-the-art\nmodels on the <i>speaker extraction</i> task, including separating\nspeech of novel speakers not seen during training.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1378",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "sarkar21_interspeech": {
      "authors": [
        [
          "Saurjya",
          "Sarkar"
        ],
        [
          "Emmanouil",
          "Benetos"
        ],
        [
          "Mark",
          "Sandler"
        ]
      ],
      "title": "Vocal Harmony Separation Using Time-Domain Neural Networks",
      "original": "1531",
      "page_count": 5,
      "order": 722,
      "p1": "3515",
      "pn": "3519",
      "abstract": [
        "Polyphonic vocal recordings are an inherently challenging source separation\ntask due to the melodic structure of the vocal parts and unique timbre\nof its constituents. In this work we utilise a time-domain neural network\narchitecture re-purposed from speech separation research and modify\nit to separate <i>a capella</i> mixtures at a high sampling rate. We\nuse four-part (soprano, alto, tenor and bass) <i>a capella</i> recordings\nof Bach Chorales and Barbershop Quartets for our experiments. Unlike\ncurrent deep learning based choral separation models where the training\nobjective is to separate constituent sources based on their class,\nwe train our model using a permutation invariant objective. Using this\nwe achieve state-of-the-art results for choral music separation. We\nintroduce a novel method to estimate harmonic overlap between sung\nmusical notes as a measure of task complexity. We also present an analysis\nof the impact of randomised mixing, input lengths and filterbank lengths\nfor our task. Our results show a moderate negative correlation between\nthe harmonic overlap of the target sources and source separation performance.\nWe report that training our models with randomly mixed musically-incoherent\nmixtures drastically reduces the performance of vocal harmony separation\nas it decreases the average harmonic overlap presented during training.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1531",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "maciejewski21_interspeech": {
      "authors": [
        [
          "Matthew",
          "Maciejewski"
        ],
        [
          "Shinji",
          "Watanabe"
        ],
        [
          "Sanjeev",
          "Khudanpur"
        ]
      ],
      "title": "Speaker Verification-Based Evaluation of Single-Channel Speech Separation",
      "original": "1924",
      "page_count": 5,
      "order": 723,
      "p1": "3520",
      "pn": "3524",
      "abstract": [
        "Speech enhancement techniques typically focus on intrinsic metrics\nof signal quality. The overwhelming majority of deep learning-based\nsingle-channel speech separation studies, for instance, have relied\non a single class of metrics to evaluate the systems by. These metrics,\nusually variants of Signal-to-Distortion Ratio (SDR), measure fidelity\nto the &#8220;ground truth&#8221; waveform. This can be problematic,\nnot only for lack of diversity in evaluation metrics, but also in cases\nwhere a perfect ground truth waveform may be unavailable. In this work,\nwe explore the value of speaker verification as an extrinsic metric\nof separation quality, with additional utility as evidence of the benefits\nof separation as pre-processing for downstream tasks.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1924",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "lan21_interspeech": {
      "authors": [
        [
          "Tian",
          "Lan"
        ],
        [
          "Yuxin",
          "Qian"
        ],
        [
          "Yilan",
          "Lyu"
        ],
        [
          "Refuoe",
          "Mokhosi"
        ],
        [
          "Wenxin",
          "Tai"
        ],
        [
          "Qiao",
          "Liu"
        ]
      ],
      "title": "Improved Speech Separation with Time-and-Frequency Cross-Domain Feature Selection",
      "original": "2246",
      "page_count": 5,
      "order": 724,
      "p1": "3525",
      "pn": "3529",
      "abstract": [
        "Most deep learning-based monaural speech separation models only use\neither spectrograms or time domain speech signal as the input feature.\nThe recently proposed cross-domain network (CDNet) demonstrates that\nconcatenated frequency domain and time domain features helps to reach\nbetter performance. Although concatenation is a widely used feature\nfusion method, it has been proved that using frequency domain and time\ndomain features to reconstruct signal makes minor difference compared\nwith only using time domain feature in CDNet. To make better use of\nfrequency domain feature in decoder, we propose using selection weights\nto select and fuse features from different domains and unify the features\nused in separator and decoder. In this paper, we propose using trainable\nweights or the global information calculated from the different domain\nfeatures to generate selection weights. Given that our proposed models\nuse element-wise fusing in the encoder, only one deconvolution layer\nin the decoder is needed to reconstruct signals. Experiments show that\nproposed methods achieve encouraging results on the large and challenging\nLibri2Mix dataset with a small increasing in parameters, which proves\nthe frequency domain information is beneficial for signal reconstruction.\nFurthermore, proposed method has shown good generalizability on the\nunmatched VCTK2Mix dataset.\n"
      ],
      "doi": "10.21437/Interspeech.2021-2246",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "deng21c_interspeech": {
      "authors": [
        [
          "Chengyun",
          "Deng"
        ],
        [
          "Shiqian",
          "Ma"
        ],
        [
          "Yongtao",
          "Sha"
        ],
        [
          "Yi",
          "Zhang"
        ],
        [
          "Hui",
          "Zhang"
        ],
        [
          "Hui",
          "Song"
        ],
        [
          "Fei",
          "Wang"
        ]
      ],
      "title": "Robust Speaker Extraction Network Based on Iterative Refined Adaptation",
      "original": "2250",
      "page_count": 5,
      "order": 725,
      "p1": "3530",
      "pn": "3534",
      "abstract": [
        "Speaker extraction aims to extract target speech signal from a multi-talker\nenvironment with interference speakers and surrounding noise, given\na reference speech from target speaker. Most speaker extraction systems\nachieve satisfactory performance in the closed condition. Such systems\nsuffer from performance degradation given unseen target speakers and/or\nmismatched reference speech. In this paper we propose a novel strategy\nnamed Iterative Refined Adaptation (IRA) to improve the robustness\nand generalization capability of speaker extraction systems in the\naforementioned scenarios. Given an initial speaker embedding encoded\nby an auxiliary network, the extraction network can obtain a latent\nrepresentation of the target speaker as the feedback of the auxiliary\nnetwork to refine the speaker embedding, which provides more accurate\nguidance for the extraction network. Experiments show that the network\nwith IRA confirm the superior performance over comparison approaches\nin terms of SI-SDRi and PESQ on WSJ0-2mix-extr and WHAM! dataset.\n"
      ],
      "doi": "10.21437/Interspeech.2021-2250",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "wang21aa_interspeech": {
      "authors": [
        [
          "Wupeng",
          "Wang"
        ],
        [
          "Chenglin",
          "Xu"
        ],
        [
          "Meng",
          "Ge"
        ],
        [
          "Haizhou",
          "Li"
        ]
      ],
      "title": "Neural Speaker Extraction with Speaker-Speech Cross-Attention Network",
      "original": "2260",
      "page_count": 5,
      "order": 726,
      "p1": "3535",
      "pn": "3539",
      "abstract": [
        "In this paper, we propose a novel time-domain speaker-speech cross-attention\nnetwork as a variant of SpEx [1] architecture, that features speaker-speech\ncross-attention. The speaker-speech cross-attention network consists\nof speech semantic layers that capture the high-level dependency of\naudio feature, and cross-attention layers that fuse speaker embedding\nand speech features to estimate the speaker mask. We implement cross-attention\nlayers with both parallel and sequential concatenation techniques.\nExperiments show that the proposed models consistently outperform the\nstate-of-the-art time-domain speaker extraction baseline on WSJ0-2mix\ndataset.\n"
      ],
      "doi": "10.21437/Interspeech.2021-2260",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "rigal21_interspeech": {
      "authors": [
        [
          "R\u00e9mi",
          "Rigal"
        ],
        [
          "Jacques",
          "Chodorowski"
        ],
        [
          "Beno\u00eet",
          "Zerr"
        ]
      ],
      "title": "Deep Audio-Visual Speech Separation Based on Facial Motion",
      "original": "1560",
      "page_count": 5,
      "order": 727,
      "p1": "3540",
      "pn": "3544",
      "abstract": [
        "We present a deep neural network that relies on facial motion and time-domain\naudio for isolating speech signals from a mixture of speeches and background\nnoises. Recent studies in deep learning-based audio-visual speech separation\nand speech enhancement have proven that leveraging visual information\nin addition to audio can yield substantial improvement to the prediction\nquality and robustness. We propose to use facial motion, inferred from\noptical flow techniques, as a visual feature input for our model. Combined\nwith state-of-the-art audio-only speech separation approaches, we demonstrate\nthat facial motion significantly improves the speech quality as well\nas the versatility of the model. Our proposed method offers a signal-to-distortion\nimprovement of up to 4.2 dB on two-speaker mixtures when compared to\nother audio-visual approaches.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1560",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "singh21_interspeech": {
      "authors": [
        [
          "Prachi",
          "Singh"
        ],
        [
          "Rajat",
          "Varma"
        ],
        [
          "Venkat",
          "Krishnamohan"
        ],
        [
          "Srikanth Raj",
          "Chetupalli"
        ],
        [
          "Sriram",
          "Ganapathy"
        ]
      ],
      "title": "LEAP Submission for the Third DIHARD Diarization Challenge",
      "original": "0728",
      "page_count": 5,
      "order": 728,
      "p1": "3545",
      "pn": "3549",
      "abstract": [
        "The LEAP submission for DIHARD-III challenge is described in this paper.\nThe proposed system is composed of a speech bandwidth classifier, and\ndiarization systems fine-tuned for narrowband and wideband speech separately.\nWe use an end-to-end speaker diarization system for the narrowband\nconversational telephone speech recordings. For the wideband multi-speaker\nrecordings, we use a neural embedding based clustering approach, similar\nto the baseline system. The embeddings are extracted from a time-delay\nneural network (called x-vectors) followed by the graph based path\nintegral clustering (PIC) approach. The LEAP system showed 24% and\n18% relative improvements for Track-1 and Track-2 respectively over\nthe baseline system provided by the organizers. This paper describes\nthe challenge submission, the post-evaluation analysis and improvements\nobserved on the DIHARD-III dataset.\n"
      ],
      "doi": "10.21437/Interspeech.2021-728",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "zhang21w_interspeech": {
      "authors": [
        [
          "Shiliang",
          "Zhang"
        ],
        [
          "Siqi",
          "Zheng"
        ],
        [
          "Weilong",
          "Huang"
        ],
        [
          "Ming",
          "Lei"
        ],
        [
          "Hongbin",
          "Suo"
        ],
        [
          "Jinwei",
          "Feng"
        ],
        [
          "Zhijie",
          "Yan"
        ]
      ],
      "title": "Investigation of Spatial-Acoustic Features for Overlapping Speech Detection in Multiparty Meetings",
      "original": "0747",
      "page_count": 5,
      "order": 729,
      "p1": "3550",
      "pn": "3554",
      "abstract": [
        "In this paper, we propose an overlapping speech detection (OSD) system\nfor real multiparty meetings. Different from previous works on single-channel\nrecordings or simulated data, we conduct research on real multi-channel\ndata recorded by an 8-microphone array. We investigate how spatial\ninformation provided by multi-channel beamforming can benefit OSD.\nSpecifically, we propose a two-stream DFSMN to jointly model acoustic\nand spatial features. Instead of performing frame-level OSD, we try\nto perform segment-level OSD. We come up with an attention pooling\nlayer to model speech segments with variable length. Experimental results\nshow that two-stream DFSMN with attention pooling can effectively model\nacoustic-spatial feature and significantly boost the performance of\nOSD, result in 3.5% (from 85.57% to 89.12%) absolute detection accuracy\nimprovement compared to the baseline system.\n"
      ],
      "doi": "10.21437/Interspeech.2021-747",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "he21c_interspeech": {
      "authors": [
        [
          "Maokui",
          "He"
        ],
        [
          "Desh",
          "Raj"
        ],
        [
          "Zili",
          "Huang"
        ],
        [
          "Jun",
          "Du"
        ],
        [
          "Zhuo",
          "Chen"
        ],
        [
          "Shinji",
          "Watanabe"
        ]
      ],
      "title": "Target-Speaker Voice Activity Detection with Improved i-Vector Estimation for Unknown Number of Speaker",
      "original": "0750",
      "page_count": 5,
      "order": 730,
      "p1": "3555",
      "pn": "3559",
      "abstract": [
        "Target-speaker voice activity detection (TS-VAD) has recently shown\npromising results for speaker diarization on highly overlapped speech.\nHowever, the original model requires a fixed (and known) number of\nspeakers, which limits its application to real conversations. In this\npaper, we extend TS-VAD to speaker diarization with unknown numbers\nof speakers. This is achieved by two steps: first, an initial diarization\nsystem is applied for speaker number estimation, followed by TS-VAD\nnetwork output masking according to this estimate. We further investigate\ndifferent diarization methods, including clustering-based and region\nproposal networks, for estimating the initial i-vectors. Since these\nsystems have complementary strengths, we propose a fusion-based method\nto combine frame-level decisions from the systems for an improved initialization.\nWe demonstrate through experiments on variants of the LibriCSS meeting\ncorpus that our proposed approach can improve the DER by up to 50%\nrelative across varying numbers of speakers. This improvement also\nresults in better downstream ASR performance approaching that using\noracle segments.\n"
      ],
      "doi": "10.21437/Interspeech.2021-750",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "dawalatabad21_interspeech": {
      "authors": [
        [
          "Nauman",
          "Dawalatabad"
        ],
        [
          "Mirco",
          "Ravanelli"
        ],
        [
          "Fran\u00e7ois",
          "Grondin"
        ],
        [
          "Jenthe",
          "Thienpondt"
        ],
        [
          "Brecht",
          "Desplanques"
        ],
        [
          "Hwidong",
          "Na"
        ]
      ],
      "title": "ECAPA-TDNN Embeddings for Speaker Diarization",
      "original": "0941",
      "page_count": 5,
      "order": 731,
      "p1": "3560",
      "pn": "3564",
      "abstract": [
        "Learning robust speaker embeddings is a crucial step in speaker diarization.\nDeep neural networks can accurately capture speaker discriminative\ncharacteristics and popular deep embeddings such as x-vectors are nowadays\na fundamental component of modern diarization systems. Recently, some\nimprovements over the standard TDNN architecture used for x-vectors\nhave been proposed. The ECAPA-TDNN model, for instance, has shown impressive\nperformance in the speaker verification domain, thanks to a carefully\ndesigned neural model.<br style=\"mso-data-placement:same-cell;\" /><br\nstyle=\"mso-data-placement:same-cell;\" />\n  In this work, we extend,\nfor the first time, the use of the ECAPA-TDNN model to speaker diarization.\nMoreover, we improved its robustness with a powerful augmentation scheme\nthat concatenates several contaminated versions of the same signal\nwithin the same training batch. The ECAPA-TDNN model turned out to\nprovide robust speaker embeddings under both close-talking and distant-talking\nconditions. Our results on the popular AMI meeting corpus show that\nour system significantly outperforms recently proposed approaches.\n"
      ],
      "doi": "10.21437/Interspeech.2021-941",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "kinoshita21_interspeech": {
      "authors": [
        [
          "Keisuke",
          "Kinoshita"
        ],
        [
          "Marc",
          "Delcroix"
        ],
        [
          "Naohiro",
          "Tawara"
        ]
      ],
      "title": "Advances in Integration of End-to-End Neural and Clustering-Based Diarization for Real Conversational Speech",
      "original": "1004",
      "page_count": 5,
      "order": 732,
      "p1": "3565",
      "pn": "3569",
      "abstract": [
        "Recently, we proposed a novel speaker diarization method called End-to-End-Neural-Diarization-vector\nclustering (EEND-vector clustering) that integrates clustering-based\nand end-to-end neural network-based diarization approaches into one\nframework. The proposed method combines advantages of both frameworks,\ni.e. high diarization performance and handling of overlapped speech\nbased on EEND, and robust handling of long recordings with an arbitrary\nnumber of speakers based on clustering-based approaches. However, the\nmethod was only evaluated so far on simulated 2-speaker meeting-like\ndata. This paper is to (1) report recent advances we made to this framework,\nincluding newly introduced robust constrained clustering algorithms,\nand (2) experimentally show that the method can now outperform competitive\ndiarization methods such as Encoder-Decoder Attractor (EDA)-EEND, on\nCALLHOME data which comprises real conversational speech data including\noverlapped speech and an arbitrary number of speakers. By further analyzing\nthe experimental results, this paper also discusses pros and cons of\nthe proposed method and reveals potential for further improvement.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1004",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "ryant21_interspeech": {
      "authors": [
        [
          "Neville",
          "Ryant"
        ],
        [
          "Prachi",
          "Singh"
        ],
        [
          "Venkat",
          "Krishnamohan"
        ],
        [
          "Rajat",
          "Varma"
        ],
        [
          "Kenneth",
          "Church"
        ],
        [
          "Christopher",
          "Cieri"
        ],
        [
          "Jun",
          "Du"
        ],
        [
          "Sriram",
          "Ganapathy"
        ],
        [
          "Mark",
          "Liberman"
        ]
      ],
      "title": "The Third DIHARD Diarization Challenge",
      "original": "1208",
      "page_count": 5,
      "order": 733,
      "p1": "3570",
      "pn": "3574",
      "abstract": [
        "DIHARD III was the third in a series of speaker diarization challenges\nintended to improve the robustness of diarization systems to variability\nin recording equipment, noise conditions, and conversational domain.\nSpeaker diarization was evaluated under two speech activity conditions\n(diarization from a reference speech activity vs. diarization from\nscratch) and 11 diverse domains. The domains span a range of recording\nconditions and interaction types, including read audio-books, meeting\nspeech, clinical interviews, web videos, and, for the first time, conversational\ntelephone speech. A total of 30 organizations (forming 21 teams) from\nindustry and academia submitted 499 valid system outputs. The evaluation\nresults indicate that speaker diarization has improved markedly since\nDIHARD I, particularly for two-party interactions, but that for many\ndomains (e.g., web video) the problem remains far from solved.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1208",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "leung21_interspeech": {
      "authors": [
        [
          "Tsun-Yat",
          "Leung"
        ],
        [
          "Lahiru",
          "Samarakoon"
        ]
      ],
      "title": "Robust End-to-End Speaker Diarization with Conformer and Additive Margin Penalty",
      "original": "1377",
      "page_count": 5,
      "order": 734,
      "p1": "3575",
      "pn": "3579",
      "abstract": [
        "Traditionally, a speaker diarization system has multiple components\nto extract and cluster speaker embeddings. However, end-to-end diarization\nis more desirable as it facilitates optimizing one model in contrast\nto multiple components in a traditional set up. Moreover, end-to-end\ndiarization systems are capable of handling overlapped speech. Recently\nproposed self-attentive end-to-end diarization model with encoder-decoder\nbased attractors (EEND-EDA) is capable of processing speech from an\nunknown number of speakers, and has reported comparable performances\nto traditional systems. In this work, we aim to improve the EEND-EDA\nmodel. First, we increase the robustness of the model by incorporating\nan additive margin penalty for minimizing the intra-class variance.\nSecond, we propose to replace the Transformer encoders with Conformer\nencoders to capture local information. Third, we propose to use convolutional\nsubsampling and upsampling instead of manual subsampling only. Our\nproposed improvements report 21.6% relative reduction in DER on the\nevaluation full set of the track 2 of the DIHARD III challenge.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1377",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "obrien21_interspeech": {
      "authors": [
        [
          "Benjamin",
          "O\u2019Brien"
        ],
        [
          "Natalia",
          "Tomashenko"
        ],
        [
          "Ana\u00efs",
          "Chanclu"
        ],
        [
          "Jean-Fran\u00e7ois",
          "Bonastre"
        ]
      ],
      "title": "Anonymous Speaker Clusters: Making Distinctions Between Anonymised Speech Recordings with Clustering Interface",
      "original": "1588",
      "page_count": 5,
      "order": 735,
      "p1": "3580",
      "pn": "3584",
      "abstract": [
        "Our study examined the performance of evaluators tasked to group natural\nand anonymised speech recordings into clusters based on their perceived\nsimilarities. Speech stimuli were selected from the VCTK corpus; two\nsystems developed for the VoicePrivacy 2020 Challenge were used for\nanonymisation. The Baseline-1 (B1) system was developed by using x-vectors\nand neural waveform models, while the Baseline-2 (B2) system relied\non digital-signal-processing techniques. 74 evaluators completed three\ntrials composed of 16 recordings with either natural or anonymised\nspeech generated from a single system. F-measure and cluster purity\nmetrics were used to assess evaluator accuracy. Probabilistic linear\ndiscriminant analysis (PLDA) scores from an automatic speaker verification\nsystem were generated to quantify similarity between recordings and\nused to correlate subjective results. Our findings showed that non-native\nEnglish speaking evaluators significantly lowered their F-measure means\nwhen presented anonymised recordings. We observed no significance for\ncluster purity. Pearson correlation procedures revealed that PLDA scores\ngenerated from natural and B2-anonymised speech recordings correlated\npositively to F-measure and cluster purity metrics. These findings\nshow evaluators were able to use the interface to cluster natural and\nanonymised speech recordings and suggest anonymisation systems modelled\nlike B1 are more effective at suppressing identifiable speech characteristics.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1588",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "karra21_interspeech": {
      "authors": [
        [
          "Kiran",
          "Karra"
        ],
        [
          "Alan",
          "McCree"
        ]
      ],
      "title": "Speaker Diarization Using Two-Pass Leave-One-Out Gaussian PLDA Clustering of DNN Embeddings",
      "original": "1807",
      "page_count": 5,
      "order": 736,
      "p1": "3585",
      "pn": "3589",
      "abstract": [
        "Many modern systems for speaker diarization, such as the recently-developed\nVBx approach, rely on clustering of DNN speaker embeddings followed\nby resegmentation. Two problems with this approach are that the DNN\nis not directly optimized for this task, and the parameters need significant\nretuning for different applications. We have recently presented progress\nin this direction with a Leave-One-Out Gaussian PLDA (LGP) clustering\nalgorithm and an approach to training the DNN such that embeddings\ndirectly optimize performance of this scoring method. This paper presents\na new two-pass version of this system, where the second pass uses finer\ntime resolution to significantly improve overall performance. For the\nCallhome corpus, we achieve the first published error rate below 4%\nwithout any task-dependent parameter tuning. We also show significant\nprogress towards a robust single solution for multiple diarization\ntasks.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1807"
    },
    "hong21_interspeech": {
      "authors": [
        [
          "Zhenhou",
          "Hong"
        ],
        [
          "Jianzong",
          "Wang"
        ],
        [
          "Xiaoyang",
          "Qu"
        ],
        [
          "Jie",
          "Liu"
        ],
        [
          "Chendong",
          "Zhao"
        ],
        [
          "Jing",
          "Xiao"
        ]
      ],
      "title": "Federated Learning with Dynamic Transformer for Text to Speech",
      "original": "2039",
      "page_count": 5,
      "order": 737,
      "p1": "3590",
      "pn": "3594",
      "abstract": [
        "Text to speech (TTS) is a crucial task for user interaction, but TTS\nmodel training relies on a sizable set of high-quality original datasets.\nDue to privacy and security issues, the original datasets are usually\nunavailable directly. Recently, federated learning proposes a popular\ndistributed machine learning paradigm with an enhanced privacy protection\nmechanism. It offers a practical and secure framework for data owners\nto collaborate with others, thus obtaining a better global model trained\non the larger dataset. However, due to the high complexity of transformer\nmodels, the convergence process becomes slow and unstable in the federated\nlearning setting. Besides, the transformer model trained in federated\nlearning is costly communication and limited computational speed on\nclients, impeding its popularity. To deal with these challenges, we\npropose the federated dynamic transformer. On the one hand, the performance\nis greatly improved comparing with the federated transformer, approaching\ncentralize-trained Transformer-TTS when increasing clients number.\nOn the other hand, it achieves faster and more stable convergence in\nthe training phase and significantly reduces communication time. Experiments\non the LJSpeech dataset also strongly prove our method&#8217;s advantage.\n"
      ],
      "doi": "10.21437/Interspeech.2021-2039"
    },
    "nguyen21e_interspeech": {
      "authors": [
        [
          "Huu-Kim",
          "Nguyen"
        ],
        [
          "Kihyuk",
          "Jeong"
        ],
        [
          "Seyun",
          "Um"
        ],
        [
          "Min-Jae",
          "Hwang"
        ],
        [
          "Eunwoo",
          "Song"
        ],
        [
          "Hong-Goo",
          "Kang"
        ]
      ],
      "title": "LiteTTS: A Lightweight Mel-Spectrogram-Free Text-to-Wave Synthesizer Based on Generative Adversarial Networks",
      "original": "0188",
      "page_count": 5,
      "order": 738,
      "p1": "3595",
      "pn": "3599",
      "abstract": [
        "In this paper, we propose a lightweight end-to-end text-to-speech model\nthat can generate high-quality speech at breakneck speed. In our proposed\nmodel, a feature prediction module and a waveform generation module\nare combined within a single framework. The feature prediction module,\nwhich consists of two independent sub-modules, estimates latent space\nembeddings for input text and prosodic information, and the waveform\ngeneration module generates speech waveforms by conditioning on the\nestimated latent space embeddings. Unlike conventional approaches that\nestimate prosodic information using a pre-trained model, our model\njointly trains the prosodic embedding network with the speech waveform\ngeneration task using an effective domain transfer technique. Experimental\nresults show that our proposed model can generate samples 7 times faster\nthan real-time, and about 1.6 times faster than FastSpeech 2, as we\nuse only 13.4 million parameters. We confirm that the generated speech\nquality is still of a high standard as evaluated by mean opinion scores.\n"
      ],
      "doi": "10.21437/Interspeech.2021-188"
    },
    "tang21b_interspeech": {
      "authors": [
        [
          "Chuanxin",
          "Tang"
        ],
        [
          "Chong",
          "Luo"
        ],
        [
          "Zhiyuan",
          "Zhao"
        ],
        [
          "Dacheng",
          "Yin"
        ],
        [
          "Yucheng",
          "Zhao"
        ],
        [
          "Wenjun",
          "Zeng"
        ]
      ],
      "title": "Zero-Shot Text-to-Speech for Text-Based Insertion in Audio Narration",
      "original": "0189",
      "page_count": 5,
      "order": 739,
      "p1": "3600",
      "pn": "3604",
      "abstract": [
        "Given a piece of speech and its transcript text, text-based speech\nediting aims to generate speech that can be seamlessly inserted into\nthe given speech by editing the transcript. Existing methods adopt\na two-stage approach: synthesize the input text using a generic text-to-speech\n(TTS) engine and then transform the voice to the desired voice using\nvoice conversion (VC). A major problem of this framework is that VC\nis a challenging problem which usually needs a moderate amount of parallel\ntraining data to work satisfactorily. In this paper, we propose a one-stage\ncontext-aware framework to generate natural and coherent target speech\nwithout any training data of the target speaker. In particular, we\nmanage to perform accurate zero-shot duration prediction for the inserted\ntext. The predicted duration is used to regulate both text embedding\nand speech embedding. Then, based on the aligned cross-modality input,\nwe directly generate the mel-spectrogram of the edited speech with\na transformer-based decoder. Subjective listening tests show that despite\nthe lack of training data for the speaker, our method has achieved\nsatisfactory results. It outperforms a recent zero-shot TTS engine\nby a large margin.\n"
      ],
      "doi": "10.21437/Interspeech.2021-189",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "jeong21_interspeech": {
      "authors": [
        [
          "Myeonghun",
          "Jeong"
        ],
        [
          "Hyeongju",
          "Kim"
        ],
        [
          "Sung Jun",
          "Cheon"
        ],
        [
          "Byoung Jin",
          "Choi"
        ],
        [
          "Nam Soo",
          "Kim"
        ]
      ],
      "title": "Diff-TTS: A Denoising Diffusion Model for Text-to-Speech",
      "original": "0469",
      "page_count": 5,
      "order": 740,
      "p1": "3605",
      "pn": "3609",
      "abstract": [
        "Although neural text-to-speech (TTS) models have attracted a lot of\nattention and succeeded in generating human-like speech, there is still\nroom for improvements to its naturalness and architectural efficiency.\nIn this work, we propose a novel non-autoregressive TTS model, namely\nDiff-TTS, which achieves highly natural and efficient speech synthesis.\nGiven the text, Diff-TTS exploits a denoising diffusion framework to\ntransform the noise signal into a mel-spectrogram via diffusion time\nsteps. In order to learn the mel-spectrogram distribution conditioned\non the text, we present a likelihood-based optimization method for\nTTS. Furthermore, to boost up the inference speed, we leverage the\naccelerated sampling method that allows Diff-TTS to generate raw waveforms\nmuch faster without significantly degrading perceptual quality. Through\nexperiments, we verified that Diff-TTS generates 28 times faster than\nthe real-time with a single NVIDIA 2080Ti GPU.\n"
      ],
      "doi": "10.21437/Interspeech.2021-469",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "bae21_interspeech": {
      "authors": [
        [
          "Jae-Sung",
          "Bae"
        ],
        [
          "Taejun",
          "Bak"
        ],
        [
          "Young-Sun",
          "Joo"
        ],
        [
          "Hoon-Young",
          "Cho"
        ]
      ],
      "title": "Hierarchical Context-Aware Transformers for Non-Autoregressive Text to Speech",
      "original": "0471",
      "page_count": 5,
      "order": 741,
      "p1": "3610",
      "pn": "3614",
      "abstract": [
        "In this paper, we propose methods for improving the modeling performance\nof a Transformer-based non-autoregressive text-to-speech (TNA-TTS)\nmodel. Although the text encoder and audio decoder handle different\ntypes and lengths of data (i.e., text and audio), the TNA-TTS models\nare not designed considering these variations. Therefore, to improve\nthe modeling performance of the TNA-TTS model we propose a hierarchical\nTransformer structure-based text encoder and audio decoder that are\ndesigned to accommodate the characteristics of each module. For the\ntext encoder, we constrain each self-attention layer so the encoder\nfocuses on a text sequence from the local to the global scope. Conversely,\nthe audio decoder constrains its self-attention layers to focus in\nthe reverse direction, i.e., from global to local scope. Additionally,\nwe further improve the pitch modeling accuracy of the audio decoder\nby providing sentence and word-level pitch as conditions. Various objective\nand subjective evaluations verified that the proposed method outperformed\nthe baseline TNA-TTS.\n"
      ],
      "doi": "10.21437/Interspeech.2021-471",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "polyak21_interspeech": {
      "authors": [
        [
          "Adam",
          "Polyak"
        ],
        [
          "Yossi",
          "Adi"
        ],
        [
          "Jade",
          "Copet"
        ],
        [
          "Eugene",
          "Kharitonov"
        ],
        [
          "Kushal",
          "Lakhotia"
        ],
        [
          "Wei-Ning",
          "Hsu"
        ],
        [
          "Abdelrahman",
          "Mohamed"
        ],
        [
          "Emmanuel",
          "Dupoux"
        ]
      ],
      "title": "Speech Resynthesis from Discrete Disentangled Self-Supervised Representations",
      "original": "0475",
      "page_count": 5,
      "order": 742,
      "p1": "3615",
      "pn": "3619",
      "abstract": [
        "We propose using self-supervised discrete representations for the task\nof speech resynthesis. To generate disentangled representation, we\nseparately extract low-bitrate representations for speech content,\nprosodic information, and speaker identity. This allows to synthesize\nspeech in a controllable manner. We analyze various state-of-the-art,\nself-supervised representation learning methods and shed light on the\nadvantages of each method while considering reconstruction quality\nand disentanglement properties. Specifically, we evaluate the F0 reconstruction,\nspeaker identification performance (for both resynthesis and voice\nconversion), recordings&#8217; intelligibility, and overall quality\nusing subjective human evaluation. Lastly, we demonstrate how these\nrepresentations can be used for an ultra-lightweight speech codec.\nUsing the obtained representations, we can get to a rate of 365 bits\nper second while providing better speech quality than the baseline\nmethods. Audio samples are publicly available.\n"
      ],
      "doi": "10.21437/Interspeech.2021-475",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "karanasou21_interspeech": {
      "authors": [
        [
          "Penny",
          "Karanasou"
        ],
        [
          "Sri",
          "Karlapati"
        ],
        [
          "Alexis",
          "Moinet"
        ],
        [
          "Arnaud",
          "Joly"
        ],
        [
          "Ammar",
          "Abbas"
        ],
        [
          "Simon",
          "Slangen"
        ],
        [
          "Jaime",
          "Lorenzo-Trueba"
        ],
        [
          "Thomas",
          "Drugman"
        ]
      ],
      "title": "A Learned Conditional Prior for the VAE Acoustic Space of a TTS System",
      "original": "0528",
      "page_count": 5,
      "order": 743,
      "p1": "3620",
      "pn": "3624",
      "abstract": [
        "Many factors influence speech yielding different renditions of a given\nsentence. Generative models, such as variational autoencoders (VAEs),\ncapture this variability and allow multiple renditions of the same\nsentence via sampling. The degree of prosodic variability depends heavily\non the prior that is used when sampling. In this paper, we propose\na novel method to compute an informative prior for the VAE latent space\nof a neural text-to-speech (TTS) system. By doing so, we aim to sample\nwith more prosodic variability, while gaining controllability over\nthe latent space&#8217;s structure.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  By using as prior\nthe posterior distribution of a secondary VAE, which we condition on\na speaker vector, we can sample from the primary VAE taking explicitly\nthe conditioning into account and resulting in samples from a specific\nregion of the latent space for each condition (i.e. speaker). A formal\npreference test demonstrates significant preference of the proposed\napproach over standard Conditional VAE. We also provide visualisations\nof the latent space where well-separated condition-specific clusters\nappear, as well as ablation studies to better understand the behaviour\nof the system.\n"
      ],
      "doi": "10.21437/Interspeech.2021-528",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "paul21_interspeech": {
      "authors": [
        [
          "Dipjyoti",
          "Paul"
        ],
        [
          "Sankar",
          "Mukherjee"
        ],
        [
          "Yannis",
          "Pantazis"
        ],
        [
          "Yannis",
          "Stylianou"
        ]
      ],
      "title": "A Universal Multi-Speaker Multi-Style Text-to-Speech via Disentangled Representation Learning Based on R&#233;nyi Divergence Minimization",
      "original": "0660",
      "page_count": 5,
      "order": 744,
      "p1": "3625",
      "pn": "3629",
      "abstract": [
        "In this paper, we present a universal multi-speaker, multi-style Text-to-Speech\n(TTS) synthesis system which is able to generate speech from text with\nspeaker characteristics and speaking style similar to a given reference\nsignal. Training is conducted on non-parallel data and generates voices\nin an unsupervised manner, i.e., neither style annotation nor speaker\nlabel are required. To avoid leaking content information into the style\nembeddings (referred to as &#8220;content leakage&#8221;) and leaking\nspeaker information into style embeddings (referred to as &#8220;style\nleakage&#8221;) we suggest a novel R&#233;nyi Divergence based Disentangled\nRepresentation framework through adversarial learning. Similar to mutual\ninformation minimization, the proposed approach explicitly estimates\nvia a variational formula and then minimizes the R&#233;nyi divergence\nbetween the joint distribution and the product of marginals for the\ncontent-style and style-speaker pairs. By doing so, content, style\nand speaker spaces become representative and (ideally) independent\nof each other. Our proposed system greatly reduces content leakage\nby improving the word error rate by approximately 17&#8211;19% relative\nto the baseline system. In MOS-speech-quality, the proposed algorithm\nachieves an improvement of about 16&#8211;20% whereas MOS-style-similarly\nboost up 15% relative performance.\n"
      ],
      "doi": "10.21437/Interspeech.2021-660",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "wu21g_interspeech": {
      "authors": [
        [
          "Yi-Chiao",
          "Wu"
        ],
        [
          "Cheng-Hung",
          "Hu"
        ],
        [
          "Hung-Shin",
          "Lee"
        ],
        [
          "Yu-Huai",
          "Peng"
        ],
        [
          "Wen-Chin",
          "Huang"
        ],
        [
          "Yu",
          "Tsao"
        ],
        [
          "Hsin-Min",
          "Wang"
        ],
        [
          "Tomoki",
          "Toda"
        ]
      ],
      "title": "Relational Data Selection for Data Augmentation of Speaker-Dependent Multi-Band MelGAN Vocoder",
      "original": "0806",
      "page_count": 5,
      "order": 745,
      "p1": "3630",
      "pn": "3634",
      "abstract": [
        "Nowadays, neural vocoders can generate very high-fidelity speech when\na bunch of training data is available. Although a speaker-dependent\n(SD) vocoder usually outperforms a speaker-independent (SI) vocoder,\nit is impractical to collect a large amount of data of a specific target\nspeaker for most real-world applications. To tackle the problem of\nlimited target data, a data augmentation method based on speaker representation\nand similarity measurement of speaker verification is proposed in this\npaper. The proposed method selects utterances that have similar speaker\nidentity to the target speaker from an external corpus, and then combines\nthe selected utterances with the limited target data for SD vocoder\nadaptation. The evaluation results show that, compared with the vocoder\nadapted using only limited target data, the vocoder adapted using augmented\ndata improves both the quality and similarity of synthesized speech.\n"
      ],
      "doi": "10.21437/Interspeech.2021-806",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "chung21_interspeech": {
      "authors": [
        [
          "Hyunseung",
          "Chung"
        ],
        [
          "Sang-Hoon",
          "Lee"
        ],
        [
          "Seong-Whan",
          "Lee"
        ]
      ],
      "title": "Reinforce-Aligner: Reinforcement Alignment Search for Robust End-to-End Text-to-Speech",
      "original": "0831",
      "page_count": 5,
      "order": 746,
      "p1": "3635",
      "pn": "3639",
      "abstract": [
        "Text-to-speech (TTS) synthesis is the process of producing synthesized\nspeech from text or phoneme input. Traditional TTS models contain multiple\nprocessing steps and require external aligners, which provide attention\nalignments of phoneme-to-frame sequences. As the complexity increases\nand efficiency decreases with every additional step, there is expanding\ndemand in modern synthesis pipelines for end-to-end TTS with efficient\ninternal aligners. In this work, we propose an end-to-end text-to-waveform\nnetwork with a novel reinforcement learning based duration search method.\nOur proposed generator is feed-forward and the aligner trains the agent\nto make optimal duration predictions by receiving active feedback from\nactions taken to maximize cumulative reward. We demonstrate accurate\nalignments of phoneme-to-frame sequence generated from trained agents\nenhance fidelity and naturalness of synthesized audio. Experimental\nresults also show the superiority of our proposed model compared to\nother state-of-the-art TTS models with internal and external aligners.\n"
      ],
      "doi": "10.21437/Interspeech.2021-831",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "lin21g_interspeech": {
      "authors": [
        [
          "Shilun",
          "Lin"
        ],
        [
          "Fenglong",
          "Xie"
        ],
        [
          "Li",
          "Meng"
        ],
        [
          "Xinhui",
          "Li"
        ],
        [
          "Li",
          "Lu"
        ]
      ],
      "title": "Triple M: A Practical Text-to-Speech Synthesis System with Multi-Guidance Attention and Multi-Band Multi-Time LPCNet",
      "original": "0851",
      "page_count": 5,
      "order": 747,
      "p1": "3640",
      "pn": "3644",
      "abstract": [
        "In this work, a robust and efficient text-to-speech (TTS) synthesis\nsystem named Triple M is proposed for large-scale online application.\nThe key components of Triple M are: 1) A sequence-to-sequence model\nadopts a novel multi-guidance attention to transfer complementary advantages\nfrom guiding attention mechanisms to the basic attention mechanism\nwithout in-domain performance loss and online service modification.\nCompared with single attention mechanism, multi-guidance attention\nnot only brings better naturalness to long sentence synthesis, but\nalso reduces the word error rate by 26.8%. 2) A new efficient multi-band\nmulti-time vocoder framework, which reduces the computational complexity\nfrom 2.8 to 1.0 GFLOP and speeds up LPCNet by 2.75&#215; on a single\nCPU.\n"
      ],
      "doi": "10.21437/Interspeech.2021-851",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "casanova21b_interspeech": {
      "authors": [
        [
          "Edresson",
          "Casanova"
        ],
        [
          "Christopher",
          "Shulby"
        ],
        [
          "Eren",
          "G\u00f6lge"
        ],
        [
          "Nicolas Michael",
          "M\u00fcller"
        ],
        [
          "Frederico Santos de",
          "Oliveira"
        ],
        [
          "Arnaldo",
          "Candido Jr."
        ],
        [
          "Anderson da Silva",
          "Soares"
        ],
        [
          "Sandra Maria",
          "Aluisio"
        ],
        [
          "Moacir Antonelli",
          "Ponti"
        ]
      ],
      "title": "SC-GlowTTS: An Efficient Zero-Shot Multi-Speaker Text-To-Speech Model",
      "original": "1774",
      "page_count": 5,
      "order": 748,
      "p1": "3645",
      "pn": "3649",
      "abstract": [
        "In this paper, we propose SC-GlowTTS: an efficient zero-shot multi-speaker\ntext-to-speech model that improves similarity for speakers unseen during\ntraining. We propose a speaker-conditional architecture that explores\na flow-based decoder that works in a zero-shot scenario. As text encoders,\nwe explore a dilated residual convolutional-based encoder, gated convolutional-based\nencoder, and transformer-based encoder. Additionally, we have shown\nthat adjusting a GAN-based vocoder for the spectrograms predicted by\nthe TTS model on the training dataset can significantly improve the\nsimilarity and speech quality for new speakers. Our model converges\nusing only 11 speakers, reaching state-of-the-art results for similarity\nwith new speakers, as well as high speech quality.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1774",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "palmer21_interspeech": {
      "authors": [
        [
          "Ian",
          "Palmer"
        ],
        [
          "Andrew",
          "Rouditchenko"
        ],
        [
          "Andrei",
          "Barbu"
        ],
        [
          "Boris",
          "Katz"
        ],
        [
          "James",
          "Glass"
        ]
      ],
      "title": "Spoken ObjectNet: A Bias-Controlled Spoken Caption Dataset",
      "original": "0245",
      "page_count": 5,
      "order": 749,
      "p1": "3650",
      "pn": "3654",
      "abstract": [
        "Visually-grounded spoken language datasets can enable models to learn\ncross-modal correspondences with very weak supervision. However, modern\naudio-visual datasets contain biases that undermine the real-world\nperformance of models trained on that data. We introduce Spoken ObjectNet,\nwhich is designed to remove some of these biases and provide a way\nto better evaluate how effectively models will perform in real-world\nscenarios. This dataset expands upon ObjectNet, which is a bias-controlled\nimage dataset that features similar image classes to those present\nin ImageNet.<br style=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  We detail our data collection pipeline, which features several\nmethods to improve caption quality, including automated language model\nchecks. Lastly, we show baseline results on image retrieval and audio\nretrieval tasks. These results show that models trained on other datasets\nand then evaluated on Spoken ObjectNet tend to perform poorly due to\nbiases in other datasets that the models have learned. We also show\nevidence that the performance decrease is due to the dataset controls,\nand not the transfer setting.\n"
      ],
      "doi": "10.21437/Interspeech.2021-245",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "salesky21_interspeech": {
      "authors": [
        [
          "Elizabeth",
          "Salesky"
        ],
        [
          "Matthew",
          "Wiesner"
        ],
        [
          "Jacob",
          "Bremerman"
        ],
        [
          "Roldano",
          "Cattoni"
        ],
        [
          "Matteo",
          "Negri"
        ],
        [
          "Marco",
          "Turchi"
        ],
        [
          "Douglas W.",
          "Oard"
        ],
        [
          "Matt",
          "Post"
        ]
      ],
      "title": "The Multilingual TEDx Corpus for Speech Recognition and Translation",
      "original": "0011",
      "page_count": 5,
      "order": 750,
      "p1": "3655",
      "pn": "3659",
      "abstract": [
        "We present the Multilingual TEDx corpus, built to support speech recognition\n(ASR) and speech translation (ST) research across many non-English\nsource languages. The corpus is a collection of audio recordings from\nTEDx talks in 8 source languages. We segment transcripts into sentences\nand align them to the source-language audio and target-language translations.\nThe corpus is released along with open-sourced code enabling extension\nto new talks and languages as they become available. Our corpus creation\nmethodology can be applied to more languages than previous work, and\ncreates multi-way parallel evaluation sets. We provide baselines in\nmultiple ASR and ST settings, including multilingual models to improve\ntranslation performance for low-resource language pairs.\n"
      ],
      "doi": "10.21437/Interspeech.2021-11",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "mortensen21_interspeech": {
      "authors": [
        [
          "David R.",
          "Mortensen"
        ],
        [
          "Jordan",
          "Picone"
        ],
        [
          "Xinjian",
          "Li"
        ],
        [
          "Kathleen",
          "Siminyu"
        ]
      ],
      "title": "Tusom2021: A Phonetically Transcribed Speech Dataset from an Endangered Language for Universal Phone Recognition Experiments",
      "original": "1435",
      "page_count": 5,
      "order": 751,
      "p1": "3660",
      "pn": "3664",
      "abstract": [
        "There is growing interest in ASR systems that can recognize phones\nin a language-independent fashion [1, 2, 3]. There is additionally\ninterest in building language technologies for low-resource and endangered\nlanguages. However, there is a paucity of realistic data that can be\nused to test such systems and technologies. This paper presents a publicly\navailable, phonetically transcribed corpus of 2255 utterances (words\nand short phrases) in the endangered Tangkhulic language East Tusom\n(no ISO 639-3 code), a Tibeto-Burman language variety spoken mostly\nin India. Because the dataset is transcribed in terms of phones, rather\nthan phonemes, it is a better match for universal phone recognition\nsystems than many larger (phonemically transcribed) datasets. This\npaper describes the dataset and the methodology used to produce it.\nIt further presents basic benchmarks of state-of-the-art universal\nphone recognition systems on the dataset as baselines for future experiments.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1435",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "fu21b_interspeech": {
      "authors": [
        [
          "Yihui",
          "Fu"
        ],
        [
          "Luyao",
          "Cheng"
        ],
        [
          "Shubo",
          "Lv"
        ],
        [
          "Yukai",
          "Jv"
        ],
        [
          "Yuxiang",
          "Kong"
        ],
        [
          "Zhuo",
          "Chen"
        ],
        [
          "Yanxin",
          "Hu"
        ],
        [
          "Lei",
          "Xie"
        ],
        [
          "Jian",
          "Wu"
        ],
        [
          "Hui",
          "Bu"
        ],
        [
          "Xin",
          "Xu"
        ],
        [
          "Jun",
          "Du"
        ],
        [
          "Jingdong",
          "Chen"
        ]
      ],
      "title": "AISHELL-4: An Open Source Dataset for Speech Enhancement, Separation, Recognition and Speaker Diarization in Conference Scenario",
      "original": "1397",
      "page_count": 5,
      "order": 752,
      "p1": "3665",
      "pn": "3669",
      "abstract": [
        "In this paper, we present AISHELL-4, a sizable real-recorded Mandarin\nspeech dataset collected by 8-channel circular microphone array for\nspeech processing in conference scenario. The dataset consists of 211\nrecorded meeting sessions, each containing 4 to 8 speakers, with a\ntotal length of 120 hours. This dataset aims to bridge the advanced\nresearch on multi-speaker processing and the practical application\nscenario in three aspects. With real recorded meetings, AISHELL-4 provides\nrealistic acoustics and rich natural speech characteristics in conversation\nsuch as short pause, speech overlap, quick speaker turn, noise, etc.\nMeanwhile, accurate transcription and speaker voice activity are provided\nfor each meeting in AISHELL-4. This allows the researchers to explore\ndifferent aspects in meeting processing, ranging from individual tasks\nsuch as speech front-end processing, speech recognition and speaker\ndiarization, to multi-modality modeling and joint optimization of relevant\ntasks. Given most open source dataset for multi-speaker tasks are in\nEnglish, AISHELL-4 is the only Mandarin dataset for conversation speech,\nproviding additional value for data diversity in speech community.\nWe also release a PyTorch-based training and evaluation framework as\nbaseline system to promote reproducible research in this field.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1397",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "chen21o_interspeech": {
      "authors": [
        [
          "Guoguo",
          "Chen"
        ],
        [
          "Shuzhou",
          "Chai"
        ],
        [
          "Guan-Bo",
          "Wang"
        ],
        [
          "Jiayu",
          "Du"
        ],
        [
          "Wei-Qiang",
          "Zhang"
        ],
        [
          "Chao",
          "Weng"
        ],
        [
          "Dan",
          "Su"
        ],
        [
          "Daniel",
          "Povey"
        ],
        [
          "Jan",
          "Trmal"
        ],
        [
          "Junbo",
          "Zhang"
        ],
        [
          "Mingjie",
          "Jin"
        ],
        [
          "Sanjeev",
          "Khudanpur"
        ],
        [
          "Shinji",
          "Watanabe"
        ],
        [
          "Shuaijiang",
          "Zhao"
        ],
        [
          "Wei",
          "Zou"
        ],
        [
          "Xiangang",
          "Li"
        ],
        [
          "Xuchen",
          "Yao"
        ],
        [
          "Yongqing",
          "Wang"
        ],
        [
          "Zhao",
          "You"
        ],
        [
          "Zhiyong",
          "Yan"
        ]
      ],
      "title": "GigaSpeech: An Evolving, Multi-Domain ASR Corpus with 10,000 Hours of Transcribed Audio",
      "original": "1965",
      "page_count": 5,
      "order": 753,
      "p1": "3670",
      "pn": "3674",
      "abstract": [
        "This paper introduces GigaSpeech, an evolving, multi-domain English\nspeech recognition corpus with 10,000 hours of high quality labeled\naudio suitable for supervised training, and 33,000 hours of total audio\nsuitable for semi-supervised and unsupervised training. Around 33,000\nhours of transcribed audio is first collected from audiobooks, podcasts\nand YouTube, covering both read and spontaneous speaking styles, and\na variety of topics, such as arts, science, sports, etc. A new forced\nalignment and segmentation pipeline is proposed to create sentence\nsegments suitable for speech recognition training, and to filter out\nsegments with low-quality transcription. For system training, GigaSpeech\nprovides five subsets of different sizes, 10h, 250h, 1000h, 2500h,\nand 10000h. For our 10,000-hour <i>XL</i> training subset, we cap the\nword error rate at 4% during the filtering/ validation stage, and for\nall our other smaller training subsets, we cap it at 0%. The <i>DEV</i>\nand <i>TEST</i> evaluation sets, on the other hand, are re-processed\nby professional human transcribers to ensure high transcription quality.\nBaseline systems are provided for popular speech recognition toolkits,\nnamely Athena, ESPnet, Kaldi and Pika.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1965",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "kim21k_interspeech": {
      "authors": [
        [
          "You Jin",
          "Kim"
        ],
        [
          "Hee-Soo",
          "Heo"
        ],
        [
          "Soyeon",
          "Choe"
        ],
        [
          "Soo-Whan",
          "Chung"
        ],
        [
          "Yoohwan",
          "Kwon"
        ],
        [
          "Bong-Jin",
          "Lee"
        ],
        [
          "Youngki",
          "Kwon"
        ],
        [
          "Joon Son",
          "Chung"
        ]
      ],
      "title": "Look Who&#8217;s Talking: Active Speaker Detection in the Wild",
      "original": "2041",
      "page_count": 5,
      "order": 754,
      "p1": "3675",
      "pn": "3679",
      "abstract": [
        "In this work, we present a novel audio-visual dataset for active speaker\ndetection in the wild. A speaker is considered active when his or her\nface is visible and the voice is audible simultaneously. Although active\nspeaker detection is a crucial pre-processing step for many audio-visual\ntasks, there is no existing active speaker detection dataset to evaluate\nthe performance using natural human speech. We therefore curate the\n<i>Active Speakers in the Wild</i> (ASW) dataset which contains videos\nand co-occurring speech segments with dense speech activity labels.\nVideos and timestamps of audible segments are parsed and adopted from\nVoxConverse, an existing speaker diarisation dataset that consists\nof videos in the wild. Face tracks are extracted from the videos and\nactive segments are annotated based on the timestamps of VoxConverse\nin a semi-automatic way. Two reference systems, one is self-supervised\nand the other is supervised system, are evaluated on the dataset to\nprovide the baseline performances of ASW. Cross-domain evaluation and\ncase study are conducted, in order to show the negative effect of the\ndubbed videos that are excluded in ASW.\n"
      ],
      "doi": "10.21437/Interspeech.2021-2041",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "ahmed21_interspeech": {
      "authors": [
        [
          "Beena",
          "Ahmed"
        ],
        [
          "Kirrie J.",
          "Ballard"
        ],
        [
          "Denis",
          "Burnham"
        ],
        [
          "Tharmakulasingam",
          "Sirojan"
        ],
        [
          "Hadi",
          "Mehmood"
        ],
        [
          "Dominique",
          "Estival"
        ],
        [
          "Elise",
          "Baker"
        ],
        [
          "Felicity",
          "Cox"
        ],
        [
          "Joanne",
          "Arciuli"
        ],
        [
          "Titia",
          "Benders"
        ],
        [
          "Katherine",
          "Demuth"
        ],
        [
          "Barbara",
          "Kelly"
        ],
        [
          "Chlo\u00e9",
          "Diskin-Holdaway"
        ],
        [
          "Mostafa",
          "Shahin"
        ],
        [
          "Vidhyasaharan",
          "Sethu"
        ],
        [
          "Julien",
          "Epps"
        ],
        [
          "Chwee Beng",
          "Lee"
        ],
        [
          "Eliathamby",
          "Ambikairajah"
        ]
      ],
      "title": "AusKidTalk: An Auditory-Visual Corpus of 3- to 12-Year-Old Australian Children&#8217;s Speech",
      "original": "2000",
      "page_count": 5,
      "order": 755,
      "p1": "3680",
      "pn": "3684",
      "abstract": [
        "Here we present AusKidTalk [1], an audio-visual (AV) corpus of Australian\nchildren&#8217;s speech collected to facilitate the development of\nspeech based technological solutions for children. It builds upon the\ntechnology and expertise developed through the collection of an earlier\ncorpus of Australian adult speech, AusTalk [2,3]. This multi-site initiative\nwas established to remedy the dire shortage of children&#8217;s speech\ncorpora in Australia and around the world that are sufficiently sized\nto train accurate automated speech processing tools for children. We\nare collecting &#126;600 hours of speech from children aged 3&#8211;12\nyears that includes single word and sentence productions as well as\nnarrative and emotional speech. In this paper, we discuss the key requirements\nfor AusKidTalk and how we designed the recording setup and protocol\nto meet them. We also discuss key findings from our feasibility study\nof the recording protocol, recording tools, and user interface.\n"
      ],
      "doi": "10.21437/Interspeech.2021-2000",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "fallgren21_interspeech": {
      "authors": [
        [
          "Per",
          "Fallgren"
        ],
        [
          "Jens",
          "Edlund"
        ]
      ],
      "title": "Human-in-the-Loop Efficiency Analysis for Binary Classification in Edyson",
      "original": "0045",
      "page_count": 5,
      "order": 756,
      "p1": "3685",
      "pn": "3689",
      "abstract": [
        "Edyson is a human-in-the-loop (HITL) tool for browsing and annotating\nlarge amounts of audio data quickly. It builds on temporally disassembled\naudio and massively multi-component audio environments to overcome\nthe cumbersome time constraints that come with linear exploration of\nlarge audio data. This study adds the following contributions to Edyson:\n1) We add the new use case of HITL binary classification by sample;\n2) We explore the new domain oceanic hydrophone recordings with whale\nsong, along with speech activity detection in noisy audio; 3) We propose\na repeatable method of analysing the efficiency of HITL in Edyson for\nbinary classification, specifically designed to measure the return\non human time spent in a given domain. We exemplify this method on\ntwo domains, and show that for a manageable initial cost in terms of\nHITL, it does differentiate between suitable and unsuitable domains\nfor our new use case &#8212; a valuable insight when working with large\ncollections of audio.\n"
      ],
      "doi": "10.21437/Interspeech.2021-45",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "ryumina21_interspeech": {
      "authors": [
        [
          "Elena",
          "Ryumina"
        ],
        [
          "Oxana",
          "Verkholyak"
        ],
        [
          "Alexey",
          "Karpov"
        ]
      ],
      "title": "Annotation Confidence vs. Training Sample Size: Trade-Off Solution for Partially-Continuous Categorical Emotion Recognition",
      "original": "1636",
      "page_count": 5,
      "order": 757,
      "p1": "3690",
      "pn": "3694",
      "abstract": [
        "Commonly adapted design of emotional corpora includes multiple annotations\nfor the same instance from several annotators. Most of the previous\nstudies assume the ground truth to be an average between all labels\nor the most frequently used label. Current study shows that this approach\nmay not be optimal for training. By filtering training data according\nto the level of annotation agreement, it is possible to increase the\nperformance of the system even on unreliable test samples. However,\nincreasing the annotation confidence inevitably leads to a loss of\ndata. Therefore, balancing the trade-off between annotation quality\nand sample size requires careful investigation. This study presents\nexperimental findings of audio-visual emotion classification on a recently\nintroduced RAMAS dataset, which contains rich categorical partially-continuous\nannotation for 6 basic emotions, and reveals important conclusions\nabout optimal formulation of ground truth. By applying the proposed\napproach, it is possible to achieve classification accuracy of UAR=70.51%\non the speech utterances with more than 60% agreement, which surpasses\npreviously reported values on this corpus in the literature.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1636",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "garcesdiazmunio21_interspeech": {
      "authors": [
        [
          "Gon\u00e7al V.",
          "Garc\u00e9s D\u00edaz-Mun\u00edo"
        ],
        [
          "Joan-Albert",
          "Silvestre-Cerd\u00e0"
        ],
        [
          "Javier",
          "Jorge"
        ],
        [
          "Adri\u00e0 Gim\u00e9nez",
          "Pastor"
        ],
        [
          "Javier",
          "Iranzo-S\u00e1nchez"
        ],
        [
          "Pau",
          "Baquero-Arnal"
        ],
        [
          "Nahuel",
          "Rosell\u00f3"
        ],
        [
          "Alejandro",
          "P\u00e9rez-Gonz\u00e1lez-de-Martos"
        ],
        [
          "Jorge",
          "Civera"
        ],
        [
          "Albert",
          "Sanchis"
        ],
        [
          "Alfons",
          "Juan"
        ]
      ],
      "title": "Europarl-ASR: A Large Corpus of Parliamentary Debates for Streaming ASR Benchmarking and Speech Data Filtering/Verbatimization",
      "original": "1905",
      "page_count": 5,
      "order": 758,
      "p1": "3695",
      "pn": "3699",
      "abstract": [
        "We introduce Europarl-ASR, a large speech and text corpus of parliamentary\ndebates including 1 300 hours of transcribed speeches and 70 million\ntokens of text in English extracted from European Parliament sessions.\nThe training set is labelled with the Parliament&#8217;s non-fully-verbatim\nofficial transcripts, time-aligned. As verbatimness is critical for\nacoustic model training, we also provide automatically noise-filtered\nand automatically verbatimized transcripts of all speeches based on\nspeech data filtering and verbatimization techniques. Additionally,\n18 hours of transcribed speeches were manually verbatimized to build\nreliable speaker-dependent and speaker-independent development/test\nsets for streaming ASR benchmarking. The availability of manual non-verbatim\nand verbatim transcripts for dev/test speeches makes this corpus useful\nfor the assessment of automatic filtering and verbatimization techniques.\nThis paper describes the corpus and its creation, and provides off-line\nand streaming ASR baselines for both the speaker-dependent and speaker-independent\ntasks using the three training transcription sets. The corpus is publicly\nreleased under an open licence.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1905",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "kapoor21_interspeech": {
      "authors": [
        [
          "Parul",
          "Kapoor"
        ],
        [
          "Rudrabha",
          "Mukhopadhyay"
        ],
        [
          "Sindhu B.",
          "Hegde"
        ],
        [
          "Vinay",
          "Namboodiri"
        ],
        [
          "C.V.",
          "Jawahar"
        ]
      ],
      "title": "Towards Automatic Speech to Sign Language Generation",
      "original": "1094",
      "page_count": 5,
      "order": 759,
      "p1": "3700",
      "pn": "3704",
      "abstract": [
        "We aim to solve the highly challenging task of generating continuous\nsign language videos solely from speech segments for the first time.\nRecent efforts in this space have focused on generating such videos\nfrom human-annotated text transcripts without considering other modalities.\nHowever, replacing speech with sign language proves to be a practical\nsolution while communicating with people suffering from hearing loss.\nTherefore, we eliminate the need of using text as input and design\ntechniques that work for more natural, continuous, freely uttered speech\ncovering an extensive vocabulary. Since the current datasets are inadequate\nfor generating sign language directly from speech, we collect and release\nthe first Indian sign language dataset comprising speech-level annotations,\ntext transcripts, and the corresponding sign-language videos. Next,\nwe propose a multi-tasking transformer network trained to generate\nsigner&#8217;s poses from speech segments. With speech-to-text as an\nauxiliary task and an additional cross-modal discriminator, our model\nlearns to generate continuous sign pose sequences in an end-to-end\nmanner. Extensive experiments and comparisons with other baselines\ndemonstrate the effectiveness of our approach. We also conduct additional\nablation studies to analyze the effect of different modules of our\nnetwork. A demo video containing several results is attached to the\nsupplementary material.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1094",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "cho21b_interspeech": {
      "authors": [
        [
          "Won Ik",
          "Cho"
        ],
        [
          "Seok Min",
          "Kim"
        ],
        [
          "Hyunchang",
          "Cho"
        ],
        [
          "Nam Soo",
          "Kim"
        ]
      ],
      "title": "kosp2e: Korean Speech to English Translation Corpus",
      "original": "1040",
      "page_count": 5,
      "order": 760,
      "p1": "3705",
      "pn": "3709",
      "abstract": [
        "Most speech-to-text (S2T) translation studies use English speech as\na source, which makes it difficult for non-English speakers to take\nadvantage of the S2T technologies. For some languages, this problem\nwas tackled through corpus construction, but the farther linguistically\nfrom English or the more under-resourced, this deficiency and underrepresentedness\nbecomes more significant. In this paper, we introduce <i>kosp2e</i>\n(read as &#8216;kospi&#8217;), a corpus that allows Korean speech to\nbe translated into English text in an end-to-end manner. We adopt open\nlicense speech recognition corpus, translation corpus, and spoken language\ncorpora to make our dataset freely available to the public, and check\nthe performance through the pipeline and training-based approaches.\nUsing pipeline and various end-to-end schemes, we obtain the highest\nBLEU of 21.3 and 18.0 for each based on the English hypothesis, validating\nthe feasibility of our data. We plan to supplement annotations for\nother target languages through community contributions in the future.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1040",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "zhang21x_interspeech": {
      "authors": [
        [
          "Junbo",
          "Zhang"
        ],
        [
          "Zhiwen",
          "Zhang"
        ],
        [
          "Yongqing",
          "Wang"
        ],
        [
          "Zhiyong",
          "Yan"
        ],
        [
          "Qiong",
          "Song"
        ],
        [
          "Yukai",
          "Huang"
        ],
        [
          "Ke",
          "Li"
        ],
        [
          "Daniel",
          "Povey"
        ],
        [
          "Yujun",
          "Wang"
        ]
      ],
      "title": "speechocean762: An Open-Source Non-Native English Speech Corpus for Pronunciation Assessment",
      "original": "1259",
      "page_count": 5,
      "order": 761,
      "p1": "3710",
      "pn": "3714",
      "abstract": [
        "This paper introduces a new open-source speech corpus named &#8220;speechocean762&#8221;\ndesigned for pronunciation assessment use, consisting of 5000 English\nutterances from 250 non-native speakers, where half of the speakers\nare children. Five experts annotated each of the utterances at sentence-level,\nword-level and phoneme-level. A baseline system is released in open\nsource to illustrate the phoneme-level pronunciation assessment workflow\non this corpus. This corpus is allowed to be used freely for commercial\nand non-commercial purposes. It is available for free download from\nOpenSLR, and the corresponding baseline system is published in the\nKaldi speech recognition toolkit.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1259",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "fan21b_interspeech": {
      "authors": [
        [
          "Ruchao",
          "Fan"
        ],
        [
          "Wei",
          "Chu"
        ],
        [
          "Peng",
          "Chang"
        ],
        [
          "Jing",
          "Xiao"
        ],
        [
          "Abeer",
          "Alwan"
        ]
      ],
      "title": "An Improved Single Step Non-Autoregressive Transformer for Automatic Speech Recognition",
      "original": "1955",
      "page_count": 5,
      "order": 762,
      "p1": "3715",
      "pn": "3719",
      "abstract": [
        "Non-autoregressive mechanisms can significantly decrease inference\ntime for speech transformers, especially when the single step variant\nis applied. Previous work on CTC alignment-based single step non-autoregressive\ntransformer (CASS-NAT) has shown a large real time factor (RTF) improvement\nover autoregressive transformers (AT). In this work, we propose several\nmethods to improve the accuracy of the end-to-end CASS-NAT, followed\nby performance analyses. First, convolution augmented self-attention\nblocks are applied to both the encoder and decoder modules. Second,\nwe propose to expand the trigger mask (acoustic boundary) for each\ntoken to increase the robustness of CTC alignments. In addition, iterated\nloss functions are used to enhance the gradient update of low-layer\nparameters. Without using an external language model, the WERs of the\nimproved CASS-NAT, when using the three methods, are 3.1%/7.2% on Librispeech\ntest clean/other sets and the CER is 5.4% on the Aishell1 test set,\nachieving a 7%&#126;21% relative WER/CER improvement. For the analyses,\nwe plot attention weight distributions in the decoders to visualize\nthe relationships between token-level acoustic embeddings. When the\nacoustic embeddings are visualized, we find that they have a similar\nbehavior to word embeddings, which explains why the improved CASS-NAT\nperforms similarly to AT.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1955",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components:14 Special Sessions"
    },
    "guo21_interspeech": {
      "authors": [
        [
          "Pengcheng",
          "Guo"
        ],
        [
          "Xuankai",
          "Chang"
        ],
        [
          "Shinji",
          "Watanabe"
        ],
        [
          "Lei",
          "Xie"
        ]
      ],
      "title": "Multi-Speaker ASR Combining Non-Autoregressive Conformer CTC and Conditional Speaker Chain",
      "original": "2155",
      "page_count": 5,
      "order": 763,
      "p1": "3720",
      "pn": "3724",
      "abstract": [
        "Non-autoregressive (NAR) models have achieved a large inference computation\nreduction and comparable results with autoregressive (AR) models on\nvarious sequence to sequence tasks. However, there has been limited\nresearch aiming to explore the NAR approaches on sequence to multi-sequence\nproblems, like multi-speaker automatic speech recognition (ASR). In\nthis study, we extend our proposed conditional chain model to NAR multi-speaker\nASR. Specifically, the output of each speaker is inferred one-by-one\nusing both the input mixture speech and previously-estimated conditional\nspeaker features. In each step, a NAR connectionist temporal classification\n(CTC) encoder is used to perform parallel computation. With this design,\nthe total inference steps will be restricted to the number of mixed\nspeakers. Besides, we also adopt the Conformer and incorporate an intermediate\nCTC loss to improve the performance. Experiments on WSJ0-Mix and LibriMix\ncorpora show that our model outperforms other NAR models with only\na slight increase of latency, achieving WERs of 22.3% and 24.9%, respectively.\nMoreover, by including the data of variable numbers of speakers, our\nmodel can even better than the PIT-Conformer AR model with only 1/7\nlatency, obtaining WERs of 19.9% and 34.3% on WSJ0-2mix and WSJ0-3mix\nsets. All of our codes are publicly available.\n"
      ],
      "doi": "10.21437/Interspeech.2021-2155",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components:14 Special Sessions"
    },
    "ng21b_interspeech": {
      "authors": [
        [
          "Edwin G.",
          "Ng"
        ],
        [
          "Chung-Cheng",
          "Chiu"
        ],
        [
          "Yu",
          "Zhang"
        ],
        [
          "William",
          "Chan"
        ]
      ],
      "title": "Pushing the Limits of Non-Autoregressive Speech Recognition",
      "original": "0337",
      "page_count": 5,
      "order": 764,
      "p1": "3725",
      "pn": "3729",
      "abstract": [
        "We combine recent advancements in end-to-end speech recognition to\nnon-autoregressive automatic speech recognition. We push the limits\nof non-autoregressive state-of-the-art results for multiple datasets:\nLibriSpeech, Fisher+Switchboard and Wall Street Journal. Key to our\nrecipe, we leverage CTC on giant Conformer neural network architectures\nwith SpecAugment and wav2vec2 pre-training. We achieve 1.8%/3.6% WER\non LibriSpeech test/test-other sets, 5.1%/9.8% WER on Switchboard,\nand 3.4% on the Wall Street Journal, all without a language model.\n"
      ],
      "doi": "10.21437/Interspeech.2021-337",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components:14 Special Sessions"
    },
    "liu21l_interspeech": {
      "authors": [
        [
          "Alexander H.",
          "Liu"
        ],
        [
          "Yu-An",
          "Chung"
        ],
        [
          "James",
          "Glass"
        ]
      ],
      "title": "Non-Autoregressive Predictive Coding for Learning Speech Representations from Local Dependencies",
      "original": "0349",
      "page_count": 5,
      "order": 765,
      "p1": "3730",
      "pn": "3734",
      "abstract": [
        "Self-supervised speech representations have been shown to be effective\nin a variety of speech applications. However, existing representation\nlearning methods generally rely on the autoregressive model and/or\nobserved global dependencies while generating the representation. In\nthis work, we propose Non-Autoregressive Predictive Coding (NPC), a\nself-supervised method, to learn a speech representation in a non-autoregressive\nmanner by relying only on local dependencies of speech. NPC has a conceptually\nsimple objective and can be implemented easily with the introduced\nMasked Convolution Blocks. NPC offers a significant speedup for inference\nsince it is parallelizable in time and has a fixed inference time for\neach time step regardless of the input sequence length. We discuss\nand verify the effectiveness of NPC by theoretically and empirically\ncomparing it with other methods. We show that the NPC representation\nis comparable to other methods in our experiments while being more\nefficient.\n"
      ],
      "doi": "10.21437/Interspeech.2021-349",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components:14 Special Sessions"
    },
    "nozaki21_interspeech": {
      "authors": [
        [
          "Jumon",
          "Nozaki"
        ],
        [
          "Tatsuya",
          "Komatsu"
        ]
      ],
      "title": "Relaxing the Conditional Independence Assumption of CTC-Based ASR by Conditioning on Intermediate Predictions",
      "original": "0911",
      "page_count": 5,
      "order": 766,
      "p1": "3735",
      "pn": "3739",
      "abstract": [
        "This paper proposes a method to relax the conditional independence\nassumption of connectionist temporal classification (CTC)-based automatic\nspeech recognition (ASR) models. We train a CTC-based ASR model with\nauxiliary CTC losses in intermediate layers in addition to the original\nCTC loss in the last layer. During both training and inference, each\ngenerated prediction in the intermediate layers is summed to the input\nof the next layer to condition the prediction of the last layer on\nthose intermediate predictions. Our method is easy to implement and\nretains the merits of CTC-based ASR: a simple model architecture and\nfast decoding speed. We conduct experiments on three different ASR\ncorpora. Our proposed method improves a standard CTC model significantly\n(e.g., more than 20% relative word error rate reduction on the WSJ\ncorpus) with a little computational overhead. Moreover, for the TEDLIUM2\ncorpus and the AISHELL-1 corpus, it achieves a comparable performance\nto a strong autoregressive model with beam search, but the decoding\nspeed is at least 30 times faster.\n"
      ],
      "doi": "10.21437/Interspeech.2021-911",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components:14 Special Sessions"
    },
    "fujita21b_interspeech": {
      "authors": [
        [
          "Yuya",
          "Fujita"
        ],
        [
          "Tianzi",
          "Wang"
        ],
        [
          "Shinji",
          "Watanabe"
        ],
        [
          "Motoi",
          "Omachi"
        ]
      ],
      "title": "Toward Streaming ASR with Non-Autoregressive Insertion-Based Model",
      "original": "1131",
      "page_count": 5,
      "order": 767,
      "p1": "3740",
      "pn": "3744",
      "abstract": [
        "Neural end-to-end (E2E) models have become a promising technique to\nrealize practical automatic speech recognition (ASR) systems. When\nrealizing such a system, one important issue is the segmentation of\naudio to deal with streaming input or long recording. After audio segmentation,\nthe ASR model with a small real-time factor (RTF) is preferable because\nthe latency of the system can be faster. Recently, E2E ASR based on\nnon-autoregressive models becomes a promising approach since it can\ndecode an N-length token sequence with less than N iterations. We propose\na system to concatenate audio segmentation and non-autoregressive ASR\nto realize high accuracy and low RTF ASR. As a non-autoregressive ASR,\nthe insertion-based model is used. In addition, instead of concatenating\nseparated models for segmentation and ASR, we introduce a new architecture\nthat realizes audio segmentation and non-autoregressive ASR by a single\nneural network. Experimental results on Japanese and English dataset\nshow that the method achieved a reasonable trade-off between accuracy\nand RTF compared with baseline autoregressive Transformer and connectionist\ntemporal classification.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1131",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components:14 Special Sessions"
    },
    "lee21e_interspeech": {
      "authors": [
        [
          "Jaesong",
          "Lee"
        ],
        [
          "Jingu",
          "Kang"
        ],
        [
          "Shinji",
          "Watanabe"
        ]
      ],
      "title": "Layer Pruning on Demand with Intermediate CTC",
      "original": "1171",
      "page_count": 5,
      "order": 768,
      "p1": "3745",
      "pn": "3749",
      "abstract": [
        "Deploying an end-to-end automatic speech recognition (ASR) model on\nmobile/embedded devices is a challenging task, since the device computational\npower and energy consumption requirements are dynamically changed in\npractice. To overcome the issue, we present a training and pruning\nmethod for ASR based on the connectionist temporal classification (CTC)\nwhich allows reduction of model depth at run-time without any extra\nfine-tuning. To achieve the goal, we adopt two regularization methods,\nintermediate CTC and stochastic depth, to train a model whose performance\ndoes not degrade much after pruning. We present an in-depth analysis\nof layer behaviors using singular vector canonical correlation analysis\n(SVCCA), and efficient strategies for finding layers which are safe\nto prune. Using the proposed method, we show that a Transformer-CTC\nmodel can be pruned in various depth on demand, improving real-time\nfactor from 0.005 to 0.002 on GPU, while each pruned sub-model maintains\nthe accuracy of individually trained model of the same depth.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1171",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components:14 Special Sessions"
    },
    "li21l_interspeech": {
      "authors": [
        [
          "Song",
          "Li"
        ],
        [
          "Beibei",
          "Ouyang"
        ],
        [
          "Fuchuan",
          "Tong"
        ],
        [
          "Dexin",
          "Liao"
        ],
        [
          "Lin",
          "Li"
        ],
        [
          "Qingyang",
          "Hong"
        ]
      ],
      "title": "Real-Time End-to-End Monaural Multi-Speaker Speech Recognition",
      "original": "1449",
      "page_count": 5,
      "order": 769,
      "p1": "3750",
      "pn": "3754",
      "abstract": [
        "The rising interest in single-channel multi-speaker speech separation\nhas triggered the development of end-to-end multi-speaker automatic\nspeech recognition (ASR). However, until now, most systems have adopted\nautoregressive mechanisms for decoding, resulting in slow decoding\nspeed, which is not conducive to the application of multi-speaker speech\nrecognition in real-world environments. In this paper, we first comprehensively\ninvestigate and compare the mainstream end-to-end multi-speaker speech\nrecognition systems. Secondly, we improve the recently proposed non-autoregressive\nend-to-end speech recognition model Mask-CTC, and introduce it to multi-speaker\nspeech recognition to achieve real-time decoding. Our experiments on\nthe LibriMix data set show that under the premise of the same amount\nof parameters, the non-autoregressive model achieves performance close\nto that of the autoregressive model while having a faster decoding\nspeed.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1449",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "wang21ba_interspeech": {
      "authors": [
        [
          "Tianzi",
          "Wang"
        ],
        [
          "Yuya",
          "Fujita"
        ],
        [
          "Xuankai",
          "Chang"
        ],
        [
          "Shinji",
          "Watanabe"
        ]
      ],
      "title": "Streaming End-to-End ASR Based on Blockwise Non-Autoregressive Models",
      "original": "1556",
      "page_count": 5,
      "order": 770,
      "p1": "3755",
      "pn": "3759",
      "abstract": [
        "Non-autoregressive (NAR) modeling has gained more and more attention\nin speech processing. With recent state-of-the-art attention-based\nautomatic speech recognition (ASR) structure, NAR can realize promising\nreal-time factor (RTF) improvement with only small degradation of accuracy\ncompared to the autoregressive (AR) models. However, the recognition\ninference needs to wait for the completion of a full speech utterance,\nwhich limits their applications on low latency scenarios. To address\nthis issue, we propose a novel end-to-end streaming NAR speech recognition\nsystem by combining blockwise-attention and connectionist temporal\nclassification with mask-predict (Mask-CTC) NAR. During inference,\nthe input audio is separated into small blocks and then processed in\na blockwise streaming way. To address the insertion and deletion error\nat the edge of the output of each block, we apply an overlapping decoding\nstrategy with a dynamic mapping trick that can produce more coherent\nsentences. Experimental results show that the proposed method improves\nonline ASR recognition in low latency conditions compared to vanilla\nMask-CTC. Moreover, it can achieve a much faster inference speed compared\nto the AR attention-based models. All of our codes will be publicly\navailable.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1556",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components:14 Special Sessions"
    },
    "beliaev21_interspeech": {
      "authors": [
        [
          "Stanislav",
          "Beliaev"
        ],
        [
          "Boris",
          "Ginsburg"
        ]
      ],
      "title": "TalkNet: Non-Autoregressive Depth-Wise Separable Convolutional Model for Speech Synthesis",
      "original": "1770",
      "page_count": 5,
      "order": 771,
      "p1": "3760",
      "pn": "3764",
      "abstract": [
        "We propose TalkNet, a non-autoregressive convolutional neural model\nfor speech synthesis with explicit pitch and duration prediction. The\nmodel consists of three feed-forward convolutional networks. The first\nnetwork predicts grapheme durations. An input text is then expanded\nby repeating each symbol according to the predicted duration. The second\nnetwork predicts pitch value for every mel frame. The third network\ngenerates a mel-spectrogram from the expanded text conditioned on predicted\npitch. All networks are based on 1D depth-wise separable convolutional\narchitecture. The explicit duration prediction eliminates word skipping\nand repeating. The quality of the generated speech nearly matches the\nbest auto-regressive models &#8212; TalkNet trained on the LJSpeech\ndataset got a MOS of 4.08. The model has only 13.2M parameters, almost\n2&#215; less than the present state-of-the-art text-to-speech models.\nThe non-autoregressive architecture allows for fast training and inference.\nThe small model size and fast inference make TalkNet an attractive\ncandidate for embedded speech synthesis.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1770",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "chen21p_interspeech": {
      "authors": [
        [
          "Nanxin",
          "Chen"
        ],
        [
          "Yu",
          "Zhang"
        ],
        [
          "Heiga",
          "Zen"
        ],
        [
          "Ron J.",
          "Weiss"
        ],
        [
          "Mohammad",
          "Norouzi"
        ],
        [
          "Najim",
          "Dehak"
        ],
        [
          "William",
          "Chan"
        ]
      ],
      "title": "WaveGrad 2: Iterative Refinement for Text-to-Speech Synthesis",
      "original": "1897",
      "page_count": 5,
      "order": 772,
      "p1": "3765",
      "pn": "3769",
      "abstract": [
        "This paper introduces <i>WaveGrad 2</i>, a non-autoregressive generative\nmodel for text-to-speech synthesis. WaveGrad 2 is trained to estimate\nthe gradient of the log conditional density of the waveform given a\nphoneme sequence. The model takes an input phoneme sequence, and through\nan iterative refinement process, generates an audio waveform. This\ncontrasts to the original WaveGrad vocoder which conditions on mel-spectrogram\nfeatures, generated by a separate model. The iterative refinement process\nstarts from Gaussian noise, and through a series of refinement steps\n(e.g., 50 steps), progressively recovers the audio sequence. WaveGrad\n2 offers a natural way to trade-off between inference speed and sample\nquality, through adjusting the number of refinement steps. Experiments\nshow that the model can generate high fidelity audio, approaching the\nperformance of a state-of-the-art neural TTS system. We also report\nvarious ablation studies over different model configurations. Audio\nsamples are publicly available.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1897",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "chen21q_interspeech": {
      "authors": [
        [
          "Nanxin",
          "Chen"
        ],
        [
          "Piotr",
          "\u017belasko"
        ],
        [
          "Laureano",
          "Moro-Vel\u00e1zquez"
        ],
        [
          "Jes\u00fas",
          "Villalba"
        ],
        [
          "Najim",
          "Dehak"
        ]
      ],
      "title": "Align-Denoise: Single-Pass Non-Autoregressive Speech Recognition",
      "original": "1906",
      "page_count": 5,
      "order": 773,
      "p1": "3770",
      "pn": "3774",
      "abstract": [
        "Deep autoregressive models start to become comparable or superior to\nthe conventional systems for automatic speech recognition. However,\nfor the inference computation, they still suffer from inference speed\nissue due to their token-by-token decoding characteristic. Non-autoregressive\nmodels greatly improve decoding speed by supporting decoding within\na constant number of iterations. For example, Align-Refine was proposed\nto improve the performance of the non-autoregressive system by refining\nthe alignment iteratively. In this work, we propose a new perspective\nto connect Align-Refine and denoising autoencoder. We introduce a novel\nnoisy distribution to sample the alignment directly instead of obtaining\nit from the decoder output. The experimental results reveal that the\nproposed Align-Denoise speeds up both training and inference with performance\nimprovement up to 5% relatively using single-pass decoding.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1906",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components:14 Special Sessions"
    },
    "lu21d_interspeech": {
      "authors": [
        [
          "Hui",
          "Lu"
        ],
        [
          "Zhiyong",
          "Wu"
        ],
        [
          "Xixin",
          "Wu"
        ],
        [
          "Xu",
          "Li"
        ],
        [
          "Shiyin",
          "Kang"
        ],
        [
          "Xunying",
          "Liu"
        ],
        [
          "Helen",
          "Meng"
        ]
      ],
      "title": "VAENAR-TTS: Variational Auto-Encoder Based Non-AutoRegressive Text-to-Speech Synthesis",
      "original": "2121",
      "page_count": 5,
      "order": 774,
      "p1": "3775",
      "pn": "3779",
      "abstract": [
        "This paper describes a variational auto-encoder based non-autoregressive\ntext-to-speech (VAENAR-TTS) model. The autoregressive TTS (AR-TTS)\nmodels based on the sequence-to-sequence architecture can generate\nhigh-quality speech, but their sequential decoding process can be time-consuming.\nRecently, non-autoregressive TTS (NAR-TTS) models have been shown to\nbe more efficient with the parallel decoding process. However, these\nNAR-TTS models rely on phoneme-level durations to generate a hard alignment\nbetween the text and the spectrogram. Obtaining duration labels, either\nthrough forced alignment or knowledge distillation, is cumbersome.\nFurthermore, hard alignment based on phoneme expansion can degrade\nthe naturalness of the synthesized speech. In contrast, the proposed\nmodel of VAENAR-TTS is an end-to-end approach that does not require\nphoneme-level durations. The VAENAR-TTS model does not contain recurrent\nstructures and is completely non-autoregressive in both the training\nand inference phases. Based on the VAE architecture, the alignment\ninformation is encoded in the latent variable, and attention-based\nsoft alignment between the text and the latent variable is used in\nthe decoder to reconstruct the spectrogram. Experiments show that VAENAR-TTS\nachieves state-of-the-art synthesis quality, while the synthesis speed\nis comparable with other NAR-TTS models.\n"
      ],
      "doi": "10.21437/Interspeech.2021-2121",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components:14 Special Sessions"
    },
    "luz21_interspeech": {
      "authors": [
        [
          "Saturnino",
          "Luz"
        ],
        [
          "Fasih",
          "Haider"
        ],
        [
          "Sofia de la",
          "Fuente"
        ],
        [
          "Davida",
          "Fromm"
        ],
        [
          "Brian",
          "MacWhinney"
        ]
      ],
      "title": "Detecting Cognitive Decline Using Speech Only: The ADReSSo Challenge",
      "original": "1220",
      "page_count": 5,
      "order": 775,
      "p1": "3780",
      "pn": "3784",
      "abstract": [
        "Building on the success of the ADReSS Challenge at Interspeech 2020,\nwhich attracted the participation of 34 teams from across the world,\nthe ADReSSo Challenge targets three difficult automatic prediction\nproblems of societal and medical relevance, namely: detection of Alzheimer&#8217;s\nDementia, inference of cognitive testing scores, and prediction of\ncognitive decline. This paper presents these prediction tasks in detail,\ndescribes the datasets used, and reports the results of the baseline\nclassification and regression models we developed for each task. A\ncombination of acoustic and linguistic features extracted directly\nfrom audio recordings, without human intervention, yielded a baseline\naccuracy of 78.87% for the AD classification task, a root mean squared\nerror (RMSE) of 5.28 for prediction of cognitive scores , and 68.75%\naccuracy (F<SUB>1</SUB> = 66.67) for the cognitive decline prediction\ntask.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1220",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications:14 Special Sessions"
    },
    "pereztoro21_interspeech": {
      "authors": [
        [
          "P.A.",
          "P\u00e9rez-Toro"
        ],
        [
          "S.P.",
          "Bayerl"
        ],
        [
          "T.",
          "Arias-Vergara"
        ],
        [
          "J.C.",
          "V\u00e1squez-Correa"
        ],
        [
          "P.",
          "Klumpp"
        ],
        [
          "M.",
          "Schuster"
        ],
        [
          "Elmar",
          "N\u00f6th"
        ],
        [
          "J.R.",
          "Orozco-Arroyave"
        ],
        [
          "K.",
          "Riedhammer"
        ]
      ],
      "title": "Influence of the Interviewer on the Automatic Assessment of Alzheimer&#8217;s Disease in the Context of the ADReSSo Challenge",
      "original": "1589",
      "page_count": 5,
      "order": 776,
      "p1": "3785",
      "pn": "3789",
      "abstract": [
        "Alzheimer&#8217;s Disease (AD) results from the progressive loss of\nneurons in the hippocampus, which affects the capability to produce\ncoherent language. It affects lexical, grammatical, and semantic processes\nas well as speech fluency. This paper considers the analyses of speech\nand language for the assessment of AD in the context of the Alzheimer&#8217;s\nDementia Recognition through Spontaneous Speech (ADReSSo) 2021 challenge.\nWe propose to extract acoustic features such as X-vectors, prosody,\nand emotional embeddings as well as linguistic features such as perplexity,\nand word-embeddings. The data consist of speech recordings from AD\npatients and healthy controls. The transcriptions are obtained using\na commercial automatic speech recognition system. We outperform baseline\nresults on the test set, both for the classification and the Mini-Mental\nState Examination (MMSE) prediction. We achieved a classification accuracy\nof 80% and an RMSE of 4.56 in the regression. Additionally, we found\nstrong evidence for the influence of the interviewer on classification\nresults. In cross-validation on the training set, we get classification\nresults of 85% accuracy using the combined speech of the interviewer\nand the participant. Using interviewer speech only we still get an\naccuracy of 78%. Thus, we provide strong evidence for interviewer influence\non classification results.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1589",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications:14 Special Sessions"
    },
    "zhu21e_interspeech": {
      "authors": [
        [
          "Youxiang",
          "Zhu"
        ],
        [
          "Abdelrahman",
          "Obyat"
        ],
        [
          "Xiaohui",
          "Liang"
        ],
        [
          "John A.",
          "Batsis"
        ],
        [
          "Robert M.",
          "Roth"
        ]
      ],
      "title": "WavBERT: Exploiting Semantic and Non-Semantic Speech Using Wav2vec and BERT for Dementia Detection",
      "original": "0332",
      "page_count": 5,
      "order": 777,
      "p1": "3790",
      "pn": "3794",
      "abstract": [
        "In this paper, we exploit semantic and non-semantic information from\npatient&#8217;s speech data using Wav2vec and Bidirectional Encoder\nRepresentations from Transformers (BERT) for dementia detection. We\nfirst propose a basic WavBERT model by extracting semantic information\nfrom speech data using Wav2vec, and analyzing the semantic information\nusing BERT for dementia detection. While the basic model discards the\nnon-semantic information, we propose extended WavBERT models that convert\nthe output of Wav2vec to the input to BERT for preserving the non-semantic\ninformation in dementia detection. Specifically, we determine the locations\nand lengths of inter-word pauses using the number of blank tokens from\nWav2vec where the threshold for setting the pauses is automatically\ngenerated via BERT. We further design a pre-trained embedding conversion\nnetwork that converts the output embedding of Wav2vec to the input\nembedding of BERT, enabling the fine-tuning of WavBERT with non-semantic\ninformation. Our evaluation results using the ADReSSo dataset showed\nthat the WavBERT models achieved the highest accuracy of 83.1% in the\nclassification task, the lowest Root-Mean-Square Error (RMSE) score\nof 4.44 in the regression task, and a mean F1 of 70.91% in the progression\ntask. We confirmed the effectiveness of WavBERT models exploiting both\nsemantic and non-semantic speech.\n"
      ],
      "doi": "10.21437/Interspeech.2021-332",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications:14 Special Sessions"
    },
    "gauder21_interspeech": {
      "authors": [
        [
          "Lara",
          "Gauder"
        ],
        [
          "Leonardo",
          "Pepino"
        ],
        [
          "Luciana",
          "Ferrer"
        ],
        [
          "Pablo",
          "Riera"
        ]
      ],
      "title": "Alzheimer Disease Recognition Using Speech-Based Embeddings From Pre-Trained Models",
      "original": "0753",
      "page_count": 5,
      "order": 778,
      "p1": "3795",
      "pn": "3799",
      "abstract": [
        "This paper describes our submission to the ADreSSo Challenge, which\nfocuses on the problem of automatic recognition of Alzheimer&#8217;s\nDisease (AD) from speech. The audio samples contain speech from the\nsubjects describing a picture with the guidance of an experimenter.\nOur approach to the problem is based on the use of embeddings extracted\nfrom different pre-trained models &#8212; trill, allosaurus, and wav2vec\n2.0 &#8212; which were trained to solve different speech tasks. These\nfeatures are modeled with a neural network that takes short segments\nof speech as input, generating an AD score per segment. The final score\nfor an audio file is given by the average over all segments in the\nfile. We include ablation results to show the performance of different\nfeature types individually and in combination, a study of the effect\nof the segment size, and an analysis of statistical significance. Our\nresults on the test data for the challenge reach an accuracy of 78.9%,\noutperforming both the acoustic and linguistic baselines provided by\nthe organizers.\n"
      ],
      "doi": "10.21437/Interspeech.2021-753",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications:14 Special Sessions"
    },
    "balagopalan21_interspeech": {
      "authors": [
        [
          "Aparna",
          "Balagopalan"
        ],
        [
          "Jekaterina",
          "Novikova"
        ]
      ],
      "title": "Comparing Acoustic-Based Approaches for Alzheimer&#8217;s Disease Detection",
      "original": "0759",
      "page_count": 5,
      "order": 779,
      "p1": "3800",
      "pn": "3804",
      "abstract": [
        "Robust strategies for Alzheimer&#8217;s disease (AD) detection is important,\ngiven the high prevalence of AD. In this paper, we study the performance\nand generalizability of three approaches for AD detection from speech\non the recent ADReSSo challenge dataset:1) using conventional acoustic\nfeatures 2) using novel pre-trained acoustic embeddings 3) combining\nacoustic features and embeddings. We find that while feature-based\napproaches have a higher precision, classification approaches relying\non the combination of embeddings and features prove to have a higher,\nand more balanced performance across multiple metrics of performance.\nOur best model, using such a combined approach, outperforms the acoustic\nbaseline in the challenge by 2.8%.\n"
      ],
      "doi": "10.21437/Interspeech.2021-759",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications:14 Special Sessions"
    },
    "qiao21_interspeech": {
      "authors": [
        [
          "Yu",
          "Qiao"
        ],
        [
          "Xuefeng",
          "Yin"
        ],
        [
          "Daniel",
          "Wiechmann"
        ],
        [
          "Elma",
          "Kerz"
        ]
      ],
      "title": "Alzheimer&#8217;s Disease Detection from Spontaneous Speech Through Combining Linguistic Complexity and (Dis)Fluency Features with Pretrained Language Models",
      "original": "1415",
      "page_count": 5,
      "order": 780,
      "p1": "3805",
      "pn": "3809",
      "abstract": [
        "In this paper, we combined linguistic complexity and (dis)fluency features\nwith pretrained language models for the task of Alzheimer&#8217;s disease\ndetection of the 2021 ADReSSo (Alzheimer&#8217;s Dementia Recognition\nthrough Spontaneous Speech) challenge. An accuracy of 83.1% was achieved\non the test set, which amounts to an improvement of 4.23% over the\nbaseline model. Our best-performing model that integrated component\nmodels using a stacking ensemble technique performed equally well on\ncross-validation and test data, indicating that it is robust against\noverfitting.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1415",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications:14 Special Sessions"
    },
    "pan21c_interspeech": {
      "authors": [
        [
          "Yilin",
          "Pan"
        ],
        [
          "Bahman",
          "Mirheidari"
        ],
        [
          "Jennifer M.",
          "Harris"
        ],
        [
          "Jennifer C.",
          "Thompson"
        ],
        [
          "Matthew",
          "Jones"
        ],
        [
          "Julie S.",
          "Snowden"
        ],
        [
          "Daniel",
          "Blackburn"
        ],
        [
          "Heidi",
          "Christensen"
        ]
      ],
      "title": "Using the Outputs of Different Automatic Speech Recognition Paradigms for Acoustic- and BERT-Based Alzheimer&#8217;s Dementia Detection Through Spontaneous Speech",
      "original": "1519",
      "page_count": 5,
      "order": 781,
      "p1": "3810",
      "pn": "3814",
      "abstract": [
        "Exploring acoustic and linguistic information embedded in spontaneous\nspeech recordings has proven to be efficient for automatic Alzheimer&#8217;s\ndementia detection. Acoustic features can be extracted directly from\nthe audio recordings, however, linguistic features, in fully automatic\nsystems, need to be extracted from transcripts generated by an automatic\nspeech recognition (ASR) system. We explore two state-of-the-art ASR\nparadigms, Wav2vec2.0 (for transcription and feature extraction) and\ntime delay neural networks (TDNN) on the ADReSSo dataset containing\nrecordings of people describing the Cookie Theft (CT) picture. As no\nmanual transcripts are provided, we train an ASR system using our in-house\nCT data. We further investigate the use of confidence scores and multiple\nASR hypotheses to guide and augment the input for the BERT-based classification.\nIn total, five models are proposed for exploring how to use the audio\nrecordings only for acoustic and linguistic information extraction.\nThe test results on best acoustic-only and best linguistic-only are\n74.65% and 84.51% respectively (representing a 15% and 9% relative\nincrease to published baseline results).\n"
      ],
      "doi": "10.21437/Interspeech.2021-1519"
    },
    "syed21_interspeech": {
      "authors": [
        [
          "Zafi Sherhan",
          "Syed"
        ],
        [
          "Muhammad Shehram Shah",
          "Syed"
        ],
        [
          "Margaret",
          "Lech"
        ],
        [
          "Elena",
          "Pirogova"
        ]
      ],
      "title": "Tackling the ADRESSO Challenge 2021: The MUET-RMIT System for Alzheimer&#8217;s Dementia Recognition from Spontaneous Speech",
      "original": "1572",
      "page_count": 5,
      "order": 782,
      "p1": "3815",
      "pn": "3819",
      "abstract": [
        "This paper addresses the Interspeech Alzheimer&#8217;s Dementia Recognition\nthrough Spontaneous Speech only (ADReSSo) challenge 2021. The objective\nof our study is to propose the approach to a three task automated screening\nthat will aid in distinguishing between healthy individuals and subjects\nwith dementia. The first task is to differentiate between speech recordings\nfrom individuals with dementia. The second task requires participants\nto estimate the Mini-Mental State Examination (MMSE) score based on\nan individual&#8217;s speech. The third task requires participants\nto leverage speech recordings to identify whether individuals have\nsuffered from cognitive decline. Here, we propose a system based on\nfunctionals of deep textual embeddings with special preprocessing steps\nintegrating the effect of silence segments. We report that the developed\nsystem outperforms the challenge baseline for all three tasks. For\nTask 1, we achieve an accuracy of 84.51% compared to the baseline of\n77.46%, for Task 2, we achieve a root-mean-square-error (RMSE) of 4.35\ncompared to the baseline of 5.28, and for Task 3, we achieve an average-f1score\nof 73.80% compared to the baseline of 66.67%. These results are a testament\nof the effectiveness of our proposed system.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1572",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications:14 Special Sessions"
    },
    "rohanian21_interspeech": {
      "authors": [
        [
          "Morteza",
          "Rohanian"
        ],
        [
          "Julian",
          "Hough"
        ],
        [
          "Matthew",
          "Purver"
        ]
      ],
      "title": "Alzheimer&#8217;s Dementia Recognition Using Acoustic, Lexical, Disfluency and Speech Pause Features Robust to Noisy Inputs",
      "original": "1633",
      "page_count": 5,
      "order": 783,
      "p1": "3820",
      "pn": "3824",
      "abstract": [
        "We present two multimodal fusion-based deep learning models that consume\nASR transcribed speech and acoustic data simultaneously to classify\nwhether a speaker in a structured diagnostic task has Alzheimer&#8217;s\nDisease and to what degree, evaluating the ADReSSo challenge 2021 data.\nOur best model, a BiLSTM with highway layers using words, word probabilities,\ndisfluency features, pause information, and a variety of acoustic features,\nachieves an accuracy of 84% and RSME error prediction of 4.26 on MMSE\ncognitive scores. While predicting cognitive decline is more challenging,\nour models show improvement using the multimodal approach and word\nprobabilities, disfluency, and pause information over word-only models.\nWe show considerable gains for AD classification using multimodal fusion\nand gating, which can effectively deal with noisy inputs from acoustic\nfeatures and ASR hypotheses.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1633",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications:14 Special Sessions"
    },
    "pappagari21_interspeech": {
      "authors": [
        [
          "Raghavendra",
          "Pappagari"
        ],
        [
          "Jaejin",
          "Cho"
        ],
        [
          "Sonal",
          "Joshi"
        ],
        [
          "Laureano",
          "Moro-Vel\u00e1zquez"
        ],
        [
          "Piotr",
          "\u017belasko"
        ],
        [
          "Jes\u00fas",
          "Villalba"
        ],
        [
          "Najim",
          "Dehak"
        ]
      ],
      "title": "Automatic Detection and Assessment of Alzheimer Disease Using Speech and Language Technologies in Low-Resource Scenarios",
      "original": "1850",
      "page_count": 5,
      "order": 784,
      "p1": "3825",
      "pn": "3829",
      "abstract": [
        "In this study, we analyze the use of speech and speaker recognition\ntechnologies and natural language processing to detect Alzheimer disease\n(AD) and estimate mini-mental status evaluation (MMSE) scores. We used\nspeech recordings from Interspeech 2021 ADReSSo challenge dataset.\nOur work focuses on adapting state-of-the-art speaker recognition and\nlanguage models individually and later collectively to examine their\ncomplementary behavior for the tasks. We used speech embedding techniques\nsuch as x-vectors and prosody features to characterize the speech signals.\nWe also employed automatic speech recognition (ASR) with interpolated\nlanguage models to obtain transcriptions used to fine-tune the BERT\nmodels that classify and assess the speakers. Our results indicate\nthat the fusion of scores obtained from the multiple acoustic and linguistic\nmodels provides the best detection results, suggesting that they contain\ncomplementary information. A separate analysis of the models indicates\nthat linguistic models outperform acoustic models in detection and\nprediction tasks. However, acoustic models can provide better results\nthan linguistic models under certain circumstances due to the errors\nin ASR transcriptions, which indicates that the performance of linguistic\nmodels relies on the performance of ASRs. Our best models provide 84.51%\naccuracy in automatic detection of AD and 3.85 RMSE in MMSE prediction.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1850",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications:14 Special Sessions"
    },
    "chen21r_interspeech": {
      "authors": [
        [
          "Jun",
          "Chen"
        ],
        [
          "Jieping",
          "Ye"
        ],
        [
          "Fengyi",
          "Tang"
        ],
        [
          "Jiayu",
          "Zhou"
        ]
      ],
      "title": "Automatic Detection of Alzheimer&#8217;s Disease Using Spontaneous Speech Only",
      "original": "2002",
      "page_count": 5,
      "order": 785,
      "p1": "3830",
      "pn": "3834",
      "abstract": [
        "Alzheimer&#8217;s disease (AD) is a neurodegenerative syndrome which\naffects tens of millions of elders worldwide. Although there is no\ntreatment currently available, early recognition can improve the lives\nof people with AD and their caretakers and families. To find a cost-effective\nand easy-to-use method for dementia detection and address the dementia\nclassification task of InterSpeech 2021 ADReSSo (Alzheimer&#8217;s\nDementia Recognition through Spontaneous Speech only) challenge, we\nconduct a systematic comparison of approaches to detection of cognitive\nimpairment based on spontaneous speech. We investigated the characteristics\nof acoustic modality and linguistic modality directly based on the\naudio recordings of narrative speech, and explored a variety of modality\nfusion strategies. With an ensemble over top-10 classifiers on the\ntraining set, we achieved an accuracy of 81.69% compared to the baseline\nof 78.87% on the test set. The results suggest that although transcription\nerrors will be introduced through automatic speech recognition, integrating\ntextual information generally improves classification performance.\nBesides, ensemble methods can boost both the accuracy and the robustness\nof models.\n"
      ],
      "doi": "10.21437/Interspeech.2021-2002",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications:14 Special Sessions"
    },
    "wang21ca_interspeech": {
      "authors": [
        [
          "Ning",
          "Wang"
        ],
        [
          "Yupeng",
          "Cao"
        ],
        [
          "Shuai",
          "Hao"
        ],
        [
          "Zongru",
          "Shao"
        ],
        [
          "K.P.",
          "Subbalakshmi"
        ]
      ],
      "title": "Modular Multi-Modal Attention Network for Alzheimer&#8217;s Disease Detection Using Patient Audio and Language Data",
      "original": "2024",
      "page_count": 5,
      "order": 786,
      "p1": "3835",
      "pn": "3839",
      "abstract": [
        "In this work, we propose a modular multi-modal architecture to automatically\ndetect Alzheimer&#8217;s disease using the dataset provided in the\nADReSSo challenge. Both acoustic and text-based features are used in\nthis architecture. Since the dataset provides only audio samples of\ncontrols and patients, we use Google cloud-based speech-to-text API\nto automatically transcribe the audio files to extract text-based features.\nSeveral kinds of audio features are extracted using standard packages.\nThe proposed approach consists of 4 networks: C-attention-acoustic\nnetwork (for acoustic features only), C-Attention-FT network (for linguistic\nfeatures only), C-Attention-Embedding network (for language embeddings\nand acoustic embeddings), and a unified network (uses all of those\nfeatures). The architecture combines attention networks and a convolutional\nneural network (C-Attention network) in order to process these features.\nExperimental results show that the C-Attention-Unified network with\nLinguistic features and X-Vector embeddings achieves the best accuracy\nof 80.28% and F1 score of 0.825 on the test dataset.\n"
      ],
      "doi": "10.21437/Interspeech.2021-2024",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications:14 Special Sessions"
    },
    "gong21d_interspeech": {
      "authors": [
        [
          "Rong",
          "Gong"
        ],
        [
          "Carl",
          "Quillen"
        ],
        [
          "Dushyant",
          "Sharma"
        ],
        [
          "Andrew",
          "Goderre"
        ],
        [
          "Jos\u00e9",
          "La\u00ednez"
        ],
        [
          "Ljubomir",
          "Milanovi\u0107"
        ]
      ],
      "title": "Self-Attention Channel Combinator Frontend for End-to-End Multichannel Far-Field Speech Recognition",
      "original": "1190",
      "page_count": 5,
      "order": 787,
      "p1": "3840",
      "pn": "3844",
      "abstract": [
        "When a sufficiently large far-field training data is presented, jointly\noptimizing a multichannel frontend and an end-to-end (E2E) Automatic\nSpeech Recognition (ASR) backend shows promising results. Recent literature\nhas shown traditional beamformer designs, such as MVDR (Minimum Variance\nDistortionless Response) or fixed beamformers can be successfully integrated\nas the frontend into an E2E ASR system with learnable parameters. In\nthis work, we propose the self-attention channel combinator (SACC)\nASR frontend, which leverages the self-attention mechanism to combine\nmultichannel audio signals in the magnitude spectral domain. Experiments\nconducted on a multichannel playback test data shows that the SACC\nachieved a 9.3% WERR compared to a state-of-the-art fixed beamformer-based\nfrontend, both jointly optimized with a ContextNet-based ASR backend.\nWe also demonstrate the connection between the SACC and the traditional\nbeamformers, and analyze the intermediate outputs of the SACC.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1190",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "gretter21_interspeech": {
      "authors": [
        [
          "R.",
          "Gretter"
        ],
        [
          "Marco",
          "Matassoni"
        ],
        [
          "D.",
          "Falavigna"
        ],
        [
          "A.",
          "Misra"
        ],
        [
          "C.W.",
          "Leong"
        ],
        [
          "K.",
          "Knill"
        ],
        [
          "L.",
          "Wang"
        ]
      ],
      "title": "ETLT 2021: Shared Task on Automatic Speech Recognition for Non-Native Children&#8217;s Speech",
      "original": "1237",
      "page_count": 5,
      "order": 788,
      "p1": "3845",
      "pn": "3849",
      "abstract": [
        "The paper presents the Second ASR Challenge for Non-native Children&#8217;s\nSpeech proposed as a Special Session at Interspeech 2021, following\nthe successful first challenge at Interspeech 2020. The goal of the\nchallenge is to advance research on non-native children&#8217;s speech\nrecognition technology, as speech technology still struggles when applied\nto both children and non-native speakers. The audio data consists of\nspoken responses provided by L2 students in the context of both English\nand German speaking proficiency examinations, the latter language added\nfor 2021. Additional training data and a new evaluation set was released\nfor L2 English recorded by speakers of different native languages.\nParticipants could build systems for one or both languages. Each had\na closed track where a predetermined set of audio and linguistic resources\nwere selected, and an open track where additional data was allowed.\nAfter a description of the released corpora, the paper analyzes the\nresults achieved by the participating systems. Some issues suggested\nfrom these results are discussed.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1237",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation:14 Special Sessions"
    },
    "rumberg21_interspeech": {
      "authors": [
        [
          "Lars",
          "Rumberg"
        ],
        [
          "Hanna",
          "Ehlert"
        ],
        [
          "Ulrike",
          "L\u00fcdtke"
        ],
        [
          "J\u00f6rn",
          "Ostermann"
        ]
      ],
      "title": "Age-Invariant Training for End-to-End Child Speech Recognition Using Adversarial Multi-Task Learning",
      "original": "1241",
      "page_count": 5,
      "order": 789,
      "p1": "3850",
      "pn": "3854",
      "abstract": [
        "Automatic speech recognition for children&#8217;s speech is a challenging\ntask mainly due to scarcity of publicly available child speech corpora\nand wide inter- and intra-speaker variability in terms of acoustic\nand linguistic characteristics of children&#8217;s speech. We propose\na framework for age-invariant training of the acoustic model of end-to-end\nspeech recognition systems based on adversarial multi-task learning.\nWe use age information additionally to just differentiating between\nthe child and adult domains and thus force the acoustic model to learn\nage invariant features. Our results on publicly available data sets\nshow that this leads to better leveraging of existing data during training.\nWe further show that usage of adversarial multitask learning should\nnot necessarily be regarded as a substitute for traditional feature\nspace adaptation methods, but that both should be used together for\nbest performance.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1241",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "cornell21_interspeech": {
      "authors": [
        [
          "Samuele",
          "Cornell"
        ],
        [
          "Alessio",
          "Brutti"
        ],
        [
          "Marco",
          "Matassoni"
        ],
        [
          "Stefano",
          "Squartini"
        ]
      ],
      "title": "Learning to Rank Microphones for Distant Speech Recognition",
      "original": "1315",
      "page_count": 5,
      "order": 790,
      "p1": "3855",
      "pn": "3859",
      "abstract": [
        "Fully exploiting ad-hoc microphone networks for distant speech recognition\nis still an open issue. Empirical evidence shows that being able to\nselect the best microphone leads to significant improvements in recognition\nwithout any additional effort on front-end processing. Current channel\nselection techniques either rely on signal, decoder or posterior-based\nfeatures. Signal-based features are inexpensive to compute but do not\nalways correlate with recognition performance. Instead decoder and\nposterior-based features exhibit better correlation but require substantial\ncomputational resources.<br style=\"mso-data-placement:same-cell;\" /><br\nstyle=\"mso-data-placement:same-cell;\" />\n  In this work, we tackle\nthe channel selection problem by proposing MicRank, a learning to rank\nframework where a neural network is trained to rank the available channels\nusing directly the recognition performance on the training set. The\nproposed approach is agnostic with respect to the array geometry and\ntype of recognition back-end. We investigate different learning to\nrank strategies using a synthetic dataset developed on purpose and\nthe CHiME-6 data. Results show that the proposed approach considerably\nimproves over previous selection techniques, reaching comparable and\nin some instances better performance than oracle signal-based measures.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1315",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "gelin21_interspeech": {
      "authors": [
        [
          "Lucile",
          "Gelin"
        ],
        [
          "Thomas",
          "Pellegrini"
        ],
        [
          "Julien",
          "Pinquier"
        ],
        [
          "Morgane",
          "Daniel"
        ]
      ],
      "title": "Simulating Reading Mistakes for Child Speech Transformer-Based Phone Recognition",
      "original": "2202",
      "page_count": 5,
      "order": 791,
      "p1": "3860",
      "pn": "3864",
      "abstract": [
        "Current performance of automatic speech recognition (ASR) for children\nis below that of the latest systems dedicated to adult speech. Child\nspeech is particularly difficult to recognise, and substantial corpora\nare missing to train acoustic models. Furthermore, in the scope of\nour reading assistant for 5&#8211;8-year-old children learning to read,\nmodels need to cope with disfluencies and reading mistakes, which remain\nconsiderable challenges even for state-of-the-art ASR systems. In this\npaper, we adapt an end-to-end Transformer acoustic model to speech\nfrom children learning to read. Transfer learning (TL) with a small\namount of child speech improves the phone error rate (PER) by 48.7%\nrelative over an adult model and outperforms a TL-adapted DNN-HMM model\nby 21.0% relative PER. Multi-objective training with a Connectionist\nTemporal Classification (CTC) function further reduces the PER by 4.8%\nrelative. We propose a method of reading mistakes data augmentation,\nwhere we simulate word-level repetitions and substitutions with phonetically\nor graphically close words. Combining these two types of reading mistakes\nreaches a 19.9% PER, with a 13.1% relative improvement over the baseline.\nA detailed analysis shows that both the CTC multi-objective training\nand the augmentation with synthetic repetitions help the attention\nmechanisms better detect children&#8217;s disfluencies.\n"
      ],
      "doi": "10.21437/Interspeech.2021-2202",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "stephenson21_interspeech": {
      "authors": [
        [
          "Brooke",
          "Stephenson"
        ],
        [
          "Thomas",
          "Hueber"
        ],
        [
          "Laurent",
          "Girin"
        ],
        [
          "Laurent",
          "Besacier"
        ]
      ],
      "title": "Alternate Endings: Improving Prosody for Incremental Neural TTS with Predicted Future Text Input",
      "original": "0275",
      "page_count": 5,
      "order": 792,
      "p1": "3865",
      "pn": "3869",
      "abstract": [
        "Inferring the prosody of a word in text-to-speech synthesis requires\ninformation about its surrounding context. In incremental text-to-speech\nsynthesis, where the synthesizer produces an output before it has access\nto the complete input, the full context is often unknown which can\nresult in a loss of naturalness. In this paper, we investigate whether\nthe use of predicted future text from a transformer language model\ncan attenuate this loss in a neural TTS system. We compare several\ntest conditions of next future word: (a) unknown (zero-word), (b) language\nmodel predicted, (c) randomly predicted and (d) ground-truth. We measure\nthe prosodic features (pitch, energy and duration) and find that predicted\ntext provides significant improvements over a zero-word lookahead,\nbut only slight gains over random-word lookahead. We confirm these\nresults with a perceptive test.\n"
      ],
      "doi": "10.21437/Interspeech.2021-275",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "rijn21_interspeech": {
      "authors": [
        [
          "Pol van",
          "Rijn"
        ],
        [
          "Silvan",
          "Mertes"
        ],
        [
          "Dominik",
          "Schiller"
        ],
        [
          "Peter M.C.",
          "Harrison"
        ],
        [
          "Pauline",
          "Larrouy-Maestri"
        ],
        [
          "Elisabeth",
          "Andr\u00e9"
        ],
        [
          "Nori",
          "Jacoby"
        ]
      ],
      "title": "Exploring Emotional Prototypes in a High Dimensional TTS Latent Space",
      "original": "1538",
      "page_count": 5,
      "order": 793,
      "p1": "3870",
      "pn": "3874",
      "abstract": [
        "Recent TTS systems are able to generate prosodically varied and realistic\nspeech. However, it is unclear how this prosodic variation contributes\nto the perception of speakers&#8217; emotional states. Here we use\nthe recent psychological paradigm &#8216;Gibbs Sampling with People&#8217;\nto search the prosodic latent space in a trained Global Style Token\nTacotron model to explore prototypes of emotional prosody. Participants\nare recruited online and collectively manipulate the latent space of\nthe generative speech model in a sequentially adaptive way so that\nthe stimulus presented to one group of participants is determined by\nthe response of the previous groups. We demonstrate that (1) particular\nregions of the model&#8217;s latent space are reliably associated with\nparticular emotions, (2) the resulting emotional prototypes are well-recognized\nby a separate group of human raters, and (3) these emotional prototypes\ncan be effectively transferred to new sentences. Collectively, these\nexperiments demonstrate a novel approach to the understanding of emotional\nspeech by providing a tool to explore the relation between the latent\nspace of generative models and human semantics.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1538",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation:14 Special Sessions"
    },
    "mohan21_interspeech": {
      "authors": [
        [
          "Devang S. Ram",
          "Mohan"
        ],
        [
          "Vivian",
          "Hu"
        ],
        [
          "Tian Huey",
          "Teh"
        ],
        [
          "Alexandra",
          "Torresquintero"
        ],
        [
          "Christopher G.R.",
          "Wallis"
        ],
        [
          "Marlene",
          "Staib"
        ],
        [
          "Lorenzo",
          "Foglianti"
        ],
        [
          "Jiameng",
          "Gao"
        ],
        [
          "Simon",
          "King"
        ]
      ],
      "title": "Ctrl-P: Temporal Control of Prosodic Variation for Speech Synthesis",
      "original": "1583",
      "page_count": 5,
      "order": 794,
      "p1": "3875",
      "pn": "3879",
      "abstract": [
        "Text does not fully specify the spoken form, so text-to-speech models\nmust be able to learn from speech data that vary in ways not explained\nby the corresponding text. One way to reduce the amount of unexplained\nvariation in training data is to provide acoustic information as an\nadditional learning signal. When generating speech, modifying this\nacoustic information enables multiple distinct renditions of a text\nto be produced.<br style=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  Since much of the unexplained variation is in the prosody, we\npropose a model that generates speech explicitly conditioned on the\nthree primary acoustic correlates of prosody: F<SUB>0</SUB>, energy\nand duration. The model is flexible about how the values of these features\nare specified: they can be externally provided, or predicted from text,\nor predicted then subsequently modified.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  Compared to a model\nthat employs a variational auto-encoder to learn unsupervised latent\nfeatures, our model provides more interpretable, temporally-precise,\nand disentangled control. When automatically predicting the acoustic\nfeatures from text, it generates speech that is more natural than that\nfrom a Tacotron 2 model with reference encoder. Subsequent human-in-the-loop\nmodification of the predicted acoustic features can significantly further\nincrease naturalness.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1583",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "torresquintero21_interspeech": {
      "authors": [
        [
          "Alexandra",
          "Torresquintero"
        ],
        [
          "Tian Huey",
          "Teh"
        ],
        [
          "Christopher G.R.",
          "Wallis"
        ],
        [
          "Marlene",
          "Staib"
        ],
        [
          "Devang S. Ram",
          "Mohan"
        ],
        [
          "Vivian",
          "Hu"
        ],
        [
          "Lorenzo",
          "Foglianti"
        ],
        [
          "Jiameng",
          "Gao"
        ],
        [
          "Simon",
          "King"
        ]
      ],
      "title": "ADEPT: A Dataset for Evaluating Prosody Transfer",
      "original": "1610",
      "page_count": 5,
      "order": 795,
      "p1": "3880",
      "pn": "3884",
      "abstract": [
        "Text-to-speech is now able to achieve near-human naturalness and research\nfocus has shifted to increasing expressivity. One popular method is\nto transfer the prosody from a reference speech sample. There have\nbeen considerable advances in using prosody transfer to generate more\nexpressive speech, but the field lacks a clear definition of what successful\nprosody transfer means and a method for measuring it. We introduce\na dataset of prosodically-varied reference natural speech samples for\nevaluating prosody transfer. The samples include global variations\nreflecting emotion and interpersonal attitude, and local variations\nreflecting topical emphasis, propositional attitude, syntactic phrasing\nand marked tonicity. The corpus only includes prosodic variations that\nlisteners are able to distinguish with reasonable accuracy, and we\nreport these figures as a benchmark against which text-to-speech prosody\ntransfer can be compared. We conclude the paper with a demonstration\nof our proposed evaluation methodology, using the corpus to evaluate\ntwo text-to-speech models that perform prosody transfer.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1610",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "trang21_interspeech": {
      "authors": [
        [
          "Nguyen Thi Thu",
          "Trang"
        ],
        [
          "Nguyen Hoang",
          "Ky"
        ],
        [
          "Albert",
          "Rilliard"
        ],
        [
          "Christophe",
          "d'Alessandro"
        ]
      ],
      "title": "Prosodic Boundary Prediction Model for Vietnamese Text-To-Speech",
      "original": "0125",
      "page_count": 5,
      "order": 796,
      "p1": "3885",
      "pn": "3889",
      "abstract": [
        "This research aims to build a prosodic boundary prediction model for\nimproving the naturalness of Vietnamese speech synthesis. This model\ncan be used directly to predict prosodic boundaries in the synthesis\nphase of the statistical parametric or end-to-end speech systems. Beside\nconventional features related to Part-Of-Speech (POS), this paper proposes\ntwo efficient features to predict prosodic boundaries: syntactic blocks\nand syntactic links, based on a thorough analysis of a Vietnamese dataset.\nSyntactic blocks are syntactic phrases whose sizes are bounded in their\nconstituent syntactic tree. A syntactic link of two adjacent words\nis calculated based on the distance between them in the syntax tree.\nThe experimental results show that the two proposed predictors improve\nthe quality of the boundary prediction model using a decision tree\nclassification algorithm, about 36.4% (F1 score) higher than the model\nwith only POS features. The final boundary prediction model with POS,\nsyntactic block, and syntactic link features using the LightGBM algorithm\ngives the best F1-score results at 87.0% in test data. The proposed\nmodel helps the TTS systems, developed by either HMM-based, DNN-based,\nor End-to-end speech synthesis techniques, improve about 0.3 MOS points\n(i.e. 6 to 10%) compared to the ones without the proposed model.\n"
      ],
      "doi": "10.21437/Interspeech.2021-125",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "dovrat21_interspeech": {
      "authors": [
        [
          "Shaked",
          "Dovrat"
        ],
        [
          "Eliya",
          "Nachmani"
        ],
        [
          "Lior",
          "Wolf"
        ]
      ],
      "title": "Many-Speakers Single Channel Speech Separation with Optimal Permutation Training",
      "original": "0493",
      "page_count": 5,
      "order": 797,
      "p1": "3890",
      "pn": "3894",
      "abstract": [
        "Single channel speech separation has experienced great progress in\nthe last few years. However, training neural speech separation for\na large number of speakers (e.g., more than 10 speakers) is out of\nreach for the current methods, which rely on the Permutation Invariant\nTraining (PIT). In this work, we present a permutation invariant training\nthat employs the Hungarian algorithm in order to train with an O(C<SUP>3</SUP>)\ntime complexity, where C is the number of speakers, in comparison to\nO(C!) of PIT based methods. Furthermore, we present a modified architecture\nthat can handle the increased number of speakers. Our approach separates\nup to 20 speakers and improves the previous results for large C by\na wide margin.\n"
      ],
      "doi": "10.21437/Interspeech.2021-493",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "fras21_interspeech": {
      "authors": [
        [
          "Mieszko",
          "Fra\u015b"
        ],
        [
          "Marcin",
          "Witkowski"
        ],
        [
          "Konrad",
          "Kowalczyk"
        ]
      ],
      "title": "Combating Reverberation in NTF-Based Speech Separation Using a Sub-Source Weighted Multichannel Wiener Filter and Linear Prediction",
      "original": "1230",
      "page_count": 5,
      "order": 798,
      "p1": "3895",
      "pn": "3899",
      "abstract": [
        "Sound source separation (SS) from the microphone signals capturing\nspeech in reverberant conditions is a formidable task. This paper addresses\nthe problem of joint separation and dereverberation of speech using\nthe multichannel Wiener filter (MWF) that is tailored to the sub-source\nmodeling of each speech source with a full-rank mixing matrix. Specifically,\nthe parameters of the proposed sub-source-weighted (SSW) spatial filter\nare estimated using the sub-source based expectation maximization (EM)\nalgorithm with multiplicative updates (MU) and the localization prior\ndistribution (LP) on the mixing matrix (SSEM-MU-LP). In addition, we\nstrengthen dereverberation by incorporating a Generalized Weighted\nPrediction Error (GWPE) algorithm. The proposed method is evaluated\nusing a large dataset of two-channel recordings of clean speech convolved\nwith both real and synthesized impulse responses. The results of the\nexperiments show the superior performance of the proposed method in\nreverberant conditions in comparison to using the standard NTF-based\nseparation with the vanilla MWF in terms of signal-to-distortion ratio\n(improvement of 3&#8211;5.6 dB) and other commonly used sound separation\nmetrics.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1230",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "strauss21_interspeech": {
      "authors": [
        [
          "Martin",
          "Strauss"
        ],
        [
          "Jouni",
          "Paulus"
        ],
        [
          "Matteo",
          "Torcoli"
        ],
        [
          "Bernd",
          "Edler"
        ]
      ],
      "title": "A Hands-On Comparison of DNNs for Dialog Separation Using Transfer Learning from Music Source Separation",
      "original": "1418",
      "page_count": 5,
      "order": 799,
      "p1": "3900",
      "pn": "3904",
      "abstract": [
        "This paper describes a hands-on comparison on using state-of-the-art\nmusic source separation deep neural networks (DNNs) before and after\ntask-specific fine-tuning for separating speech content from non-speech\ncontent in broadcast audio (i.e., dialog separation). The music separation\nmodels are selected as they share the number of channels (2) and sampling\nrate (44.1 kHz or higher) with the considered broadcast content, and\nvocals separation in music is considered as a parallel for dialog separation\nin the target application domain. These similarities are assumed to\nenable transfer learning between the tasks. Three models pre-trained\non music (Open-Unmix, Spleeter, and Conv-TasNet) are considered in\nthe experiments, and fine-tuned with real broadcast data. The performance\nof the models is evaluated before and after fine-tuning with computational\nevaluation metrics (SI-SIRi, SI-SDRi, 2f-model), as well as with a\nlistening test simulating an application where the non-speech signal\nis partially attenuated, e.g., for better speech intelligibility. The\nevaluations include two reference systems specifically developed for\ndialog separation. The results indicate that pre-trained music source\nseparation models can be used for dialog separation to some degree,\nand that they benefit from the fine-tuning, reaching a performance\nclose to task-specific solutions.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1418",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "borsdorf21b_interspeech": {
      "authors": [
        [
          "Marvin",
          "Borsdorf"
        ],
        [
          "Chenglin",
          "Xu"
        ],
        [
          "Haizhou",
          "Li"
        ],
        [
          "Tanja",
          "Schultz"
        ]
      ],
      "title": "GlobalPhone Mix-To-Separate Out of 2: A Multilingual 2000 Speakers Mixtures Database for Speech Separation",
      "original": "1552",
      "page_count": 5,
      "order": 800,
      "p1": "3905",
      "pn": "3909",
      "abstract": [
        "Monaural speech separation has been well studied on various databases.\nHowever, these databases mostly concern English speech. Research in\nmulti-speaker scenarios, such as speech recognition, speaker recognition,\nspeaker diarization, and speech separation calls for speaker mixtures\ndatabases comprising multiple languages. In this paper, we propose\na new extensive multilingual database for speech separation tasks derived\nfrom the GlobalPhone 2000 Speaker Package, called &#8220;GlobalPhone\nMix-to-Separate out of 2&#8221; (GlobalPhoneMS2). We describe the construction\nof the database and conduct speech separation experiments in monolingual\nand multilingual as well as seen and unseen languages settings. When\ntrained on a multilingual dataset, the networks improve their performances\nfor unseen languages, and across almost all seen languages. We show\nthat replacing a monolingual dataset with a trilingual one, while keeping\nthe data size roughly the same, helps to improve the performance in\nmost cases. We attribute this to a larger diversity in speech, language,\nspeaker, and recording characteristics. Based on the GlobalPhoneMS2\ndatabase, speech separation results for two-speaker mixing scenarios\nare reported in 22 spoken languages for the first time.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1552",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "tsukada21_interspeech": {
      "authors": [
        [
          "Kimiko",
          "Tsukada"
        ],
        [
          "",
          "Yurong"
        ],
        [
          "Joo-Yeon",
          "Kim"
        ],
        [
          "Jeong-Im",
          "Han"
        ],
        [
          "John",
          "Hajek"
        ]
      ],
      "title": "Cross-Linguistic Perception of the Japanese Singleton/Geminate Contrast: Korean, Mandarin and Mongolian Compared",
      "original": "0021",
      "page_count": 5,
      "order": 801,
      "p1": "3910",
      "pn": "3914",
      "abstract": [
        "The perception of Japanese consonant length contrasts (i.e. short/singleton\nvs long/geminate) by native and non-native speakers was compared to\nexamine the extent to which difficult foreign language (FL) sounds\nare processed accurately. Three groups of participants had Korean,\nMandarin or Mongolian as their first language (L1) and had no experience\nwith Japanese. Unlike Japanese, Mandarin and Mongolian do not use consonant\nlength contrastively. The phonemic status of consonant length in Korean\nis debatable. Further, unlike Japanese and Mandarin which predominantly\nuse open syllables and restrict the occurrence of consonants in coda\nposition, Korean and Mongolian permit a wide range of consonants in\nthat syllable position. Via the AXB task, the participants&#8217; discrimination\naccuracy of Japanese consonant length contrasts was assessed and compared\nto that of a group of 10 native Japanese speakers who served as controls.\nThe Japanese group was at near ceiling with little individual variation.\nThe Mongolian (but not Korean and Mandarin) group did not significantly\ndiffer from the control group when the target token (X) contained a\ngeminate. All non-native groups were significantly less accurate than\nthe control group when X contained a singleton. These results were\ninterpreted as reflecting the participants&#8217; L1 quantity system.\n"
      ],
      "doi": "10.21437/Interspeech.2021-21",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production and Acquisition  by Human Listeners"
    },
    "korzekwa21_interspeech": {
      "authors": [
        [
          "Daniel",
          "Korzekwa"
        ],
        [
          "Roberto",
          "Barra-Chicote"
        ],
        [
          "Szymon",
          "Zaporowski"
        ],
        [
          "Grzegorz",
          "Beringer"
        ],
        [
          "Jaime",
          "Lorenzo-Trueba"
        ],
        [
          "Alicja",
          "Serafinowicz"
        ],
        [
          "Jasha",
          "Droppo"
        ],
        [
          "Thomas",
          "Drugman"
        ],
        [
          "Bozena",
          "Kostek"
        ]
      ],
      "title": "Detection of Lexical Stress Errors in Non-Native (L2) English with Data Augmentation and Attention",
      "original": "0086",
      "page_count": 5,
      "order": 802,
      "p1": "3915",
      "pn": "3919",
      "abstract": [
        "This paper describes two novel complementary techniques that improve\nthe detection of lexical stress errors in non-native (L2) English speech:\nattention-based feature extraction and data augmentation based on Neural\nText-To-Speech (TTS). In a classical approach, audio features are usually\nextracted from fixed regions of speech such as the syllable nucleus.\nWe propose an attention-based deep learning model that automatically\nderives optimal syllable-level representation from frame-level and\nphoneme-level audio features. Training this model is challenging because\nof the limited amount of incorrect stress patterns. To solve this problem,\nwe propose to augment the training set with incorrectly stressed words\ngenerated with Neural TTS. Combining both techniques achieves 94.8%\nprecision and 49.2% recall for the detection of incorrectly stressed\nwords in L2 English speech of Slavic and Baltic speakers.\n"
      ],
      "doi": "10.21437/Interspeech.2021-86",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production and Acquisition  by Human Listeners"
    },
    "braun21_interspeech": {
      "authors": [
        [
          "Bettina",
          "Braun"
        ],
        [
          "Nicole",
          "Deh\u00e9"
        ],
        [
          "Marieke",
          "Einfeldt"
        ],
        [
          "Daniela",
          "Wochner"
        ],
        [
          "Katharina",
          "Zahner-Ritter"
        ]
      ],
      "title": "Testing Acoustic Voice Quality Classification Across Languages and Speech Styles",
      "original": "0315",
      "page_count": 5,
      "order": 803,
      "p1": "3920",
      "pn": "3924",
      "abstract": [
        "Many studies relate acoustic voice quality measures to perceptual classification.\nWe extend this line of research by training a classifier on a balanced\nset of perceptually annotated voice quality categories with high inter-rater\nagreement, and test it on speech samples from a different language\nand on a different speech style. Annotations were done on continuous\nspeech from different laboratory settings. In Experiment 1, we trained\na random forest with Standard Chinese and German recordings labelled\nas modal, breathy, or glottalized. The model had an accuracy of 78.7%\non unseen data from the same sample (most important variables were\nharmonics-to-noise ratio, cepstral-peak prominence, and H1-A2). This\nmodel was then used to classify data from a different language (Icelandic,\nExperiment 2) and to classify a different speech style (German infant-directed\nspeech (IDS), Experiment 3). Cross-linguistic generalizability was\nhigh for Icelandic (78.6% accuracy), but lower for German IDS (71.7%\naccuracy). Accuracy of recordings of adult-directed speech from the\nsame speakers as in Experiment 3 (77%, Experiment 4) suggests that\nit is the special speech style of IDS, rather than the recording setting\nthat led to lower performance. Results are discussed in terms of efficiency\nof coding and generalizability across languages and speech styles.\n"
      ],
      "doi": "10.21437/Interspeech.2021-315",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production and Acquisition  by Human Listeners"
    },
    "zhang21y_interspeech": {
      "authors": [
        [
          "Qianyutong",
          "Zhang"
        ],
        [
          "Kexin",
          "Lyu"
        ],
        [
          "Zening",
          "Chen"
        ],
        [
          "Ping",
          "Tang"
        ]
      ],
      "title": "Acquisition of Prosodic Focus Marking by Three- to Six-Year-Old Children Learning Mandarin Chinese",
      "original": "0316",
      "page_count": 4,
      "order": 804,
      "p1": "3925",
      "pn": "3928",
      "abstract": [
        "Prosodic focus plays an important role during speech communication,\ndelivering speakers&#8217; pragmatical intention to emphasize key information,\nespecially in contrastive scenarios. Previous studies exploring children&#8217;s\nacquisition of prosodic focus have generally focused on Germanic and\nRomance languages, while it was unclear when children learning Mandarin\nChinese were able to correctly interpret the pragmatic meaning of prosodic\nfocus and integrate it into speech comprehension. The current study\nexplored Mandarin-learning 3&#8211;6-year-olds&#8217; online interpretation\nof prosodic focus to identify contrastive referents. Twenty 3&#8211;4-year-olds,\n23 5&#8211;6-year-olds, and 22 adult controls were tested. The visual-world\nparadigm was adopted, where participants were instructed to search\nfor target pictures while listening to contrastive objects in discourse\nsequences, e.g., <i>Find the red cat. Now, find the PURPLE/purple cat</i>,\nwhere the second adjective was produced with or without prosodic focus.\nParticipants&#8217; fixation patterns were recorded via eye-trackers.\nThe results showed that while adults and 5&#8211;6 years showed faster\nfixation toward target pictures in the presence of prosodic focus,\nthis was not the case for 3&#8211;4 years. These results indicated\nthat Mandarin-learning children at 5&#8211;6 years have acquired the\npragmatic meaning of prosodic focus and utilize it to guide their identification\nof contrastive referents.\n"
      ],
      "doi": "10.21437/Interspeech.2021-316",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production and Acquisition  by Human Listeners"
    },
    "mirzaei21_interspeech": {
      "authors": [
        [
          "Maryam Sadat",
          "Mirzaei"
        ],
        [
          "Kourosh",
          "Meshgi"
        ]
      ],
      "title": "Adaptive Listening Difficulty Detection for L2 Learners Through Moderating ASR Resources",
      "original": "0372",
      "page_count": 5,
      "order": 805,
      "p1": "3929",
      "pn": "3933",
      "abstract": [
        "Teaching listening skills to those learning a second language (L2)\nis one of the most challenging tasks mainly because predicting L2 listening\ndifficulties is not always straightforward. Complex processes are involved\nin decoding connected speech, constructing meaning, and comprehending\nthe audio material. Many studies have attempted to identify the significant\nfactors leading to listening difficulties, yet, a comprehensive model\nis to be constructed. We argue that an automatic speech recognition\n(ASR) system with limited training can be viewed as a rough model for\nan L2 listener with particular language proficiency. We proposed a\nmethod to select the training samples for the ASR system to match the\nmistakes of L2 listeners when listening to the authentic listening\nmaterials. This model can predict the learners&#8217; listening difficulties,\nthus allowing for generating tailored captions to assist them with\nL2 listening.\n"
      ],
      "doi": "10.21437/Interspeech.2021-372",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production and Acquisition  by Human Listeners"
    },
    "ding21b_interspeech": {
      "authors": [
        [
          "Hongwei",
          "Ding"
        ],
        [
          "Binghuai",
          "Lin"
        ],
        [
          "Liyuan",
          "Wang"
        ]
      ],
      "title": "F<SUB>0</SUB> Patterns of L2 English Speech by Mandarin Chinese Learners",
      "original": "0581",
      "page_count": 5,
      "order": 806,
      "p1": "3934",
      "pn": "3938",
      "abstract": [
        "Prosodic speech characteristics are important in the evaluation of\nboth intelligibility and naturalness of oral proficiency for learners\nof English as a Second Language (ESL). Different f<SUB>0</SUB> movement\npatterns between native and Mandarin Chinese learners have been an\nimportant research topic for second-language (L2) English speech learning.\nHowever, previous studies have seldom examined f<SUB>0</SUB> movement\npatterns between lower-level and higher-level Mandarin ESL learners.\nThe current study compared f<SUB>0</SUB> change patterns extracted\nfrom the same 20 English sentences read by 20 lower- and 20 higher-\nlevel Mandarin ESL learners, and 20 native English speakers from a\nspeech database. Appropriate procedures were applied to ensure a more\naccurate estimation of f<SUB>0</SUB> values and to catch characteristic\ndeviation in f<SUB>0</SUB> movement patterns of ESL learners. The results\nshowed that lower-level Mandarin speakers displayed more frequent f<SUB>0</SUB>\nfluctuations and smaller standard deviation of intervals between f<SUB>0</SUB>\npeaks than both native speakers and higher-level learners. The special\ncharacteristic of many smaller &#8220;ripples&#8221; on pitch contours\nof lower-level L2 English speech resembles Mandarin Chinese f<SUB>0</SUB>\nmovements, which suggests a negative transfer from the first language\n(L1) Mandarin. The findings can shed light on the assessment and learning\nof L2 English prosody by Mandarin ESL learners.\n"
      ],
      "doi": "10.21437/Interspeech.2021-581"
    },
    "lin21h_interspeech": {
      "authors": [
        [
          "Binghuai",
          "Lin"
        ],
        [
          "Liyuan",
          "Wang"
        ]
      ],
      "title": "A Neural Network-Based Noise Compensation Method for Pronunciation Assessment",
      "original": "0843",
      "page_count": 5,
      "order": 807,
      "p1": "3939",
      "pn": "3943",
      "abstract": [
        "Automatic pronunciation assessment plays an important role in computer-assisted\npronunciation training (CAPT). Goodness of pronunciation (GOP) based\non automatic speech recognition (ASR) has been commonly used in pronunciation\nassessment. It has been found that GOP normally shows deteriorating\nperformance under noisy conditions. Traditional noise compensation\nmethods, which compensate distorted GOP under noisy situations based\non the Gaussian mixture model (GMM) or other simple mapping functions,\nignore contextual influence and phonemic attributes of the utterance.\nThis usually leads to a lack of robustness with changed conditions.\nIn this paper, we adopt a bidirectional long short-term (BLSTM) network\ncombining phonemic attributes to conduct the compensation for distorted\nGOP under noisy conditions. We evaluate the model performance based\non English words recorded by Chinese learners in clean and noisy situations.\nExperimental results show the proposed model outperforms the traditional\nbaselines in Pearson correlation coefficient (PCC) and accuracy for\npronunciation assessment under various noisy conditions.\n"
      ],
      "doi": "10.21437/Interspeech.2021-843",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production and Acquisition  by Human Listeners"
    },
    "kudera21_interspeech": {
      "authors": [
        [
          "Jacek",
          "Kudera"
        ],
        [
          "Philip",
          "Georgis"
        ],
        [
          "Bernd",
          "M\u00f6bius"
        ],
        [
          "Tania",
          "Avgustinova"
        ],
        [
          "Dietrich",
          "Klakow"
        ]
      ],
      "title": "Phonetic Distance and Surprisal in Multilingual Priming: Evidence from Slavic",
      "original": "1003",
      "page_count": 5,
      "order": 808,
      "p1": "3944",
      "pn": "3948",
      "abstract": [
        "This study reveals the relation between surprisal, phonetic distance,\nand latency based on a multilingual, short-term priming framework.\nFour Slavic languages (Bulgarian, Czech, Polish, and Russian) are investigated\nacross two priming conditions: associative and phonetic priming, involving\ntrue cognates and near-homophones, respectively. This research is grounded\nin the methodology of information theory and proposes new methods for\nquantifying differences between meaningful lexical primes and targets\nfor closely related languages. It also outlines the influence of phonetic\ndistance between cognate and noncognate pairs of primes and targets\non response times in a cross-lingual lexical decision task. The experimental\nresults show that phonetic distance moderates response times only in\nPolish and Czech, whereas the surprisal-based correspondence effect\nis an accurate predictor of latency for all tested languages. The information-theoretic\napproach of quantifying feature-based alternations between Slavic cognates\nand near-homophones appears to be a valid method for latency moderation\nin the auditory modality. The outcomes of this study suggest that the\nsurprisal-based (un)expectedness of spoken stimuli is an accurate predictor\nof human performance in multilingual lexical decision tasks.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1003",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production and Acquisition  by Human Listeners"
    },
    "zhang21z_interspeech": {
      "authors": [
        [
          "Yuqing",
          "Zhang"
        ],
        [
          "Zhu",
          "Li"
        ],
        [
          "Binghuai",
          "Lin"
        ],
        [
          "Jinsong",
          "Zhang"
        ]
      ],
      "title": "A Preliminary Study on Discourse Prosody Encoding in L1 and L2 English Spontaneous Narratives",
      "original": "1082",
      "page_count": 5,
      "order": 809,
      "p1": "3949",
      "pn": "3953",
      "abstract": [
        "Relatively little attention has been devoted to the discourse-level\nprosodic encoding and speech planning in second language (L2) speech.\nThis study reports a preliminary study on learners&#8217; discourse\nprosody encoding pattern and makes a comparison with that of native\nspeakers. Using a corpus of spontaneously produced picture story narratives,\nwe analyzed general characteristics of prosodic units (PUs) and explored\nrelationships between pitch encoding (cross-boundary f0 heights and\nf0 slopes) of PUs and the semantic completeness of PUs in English spontaneous\nspeech by native speakers, beginning learners, and advanced learners.\nThe results indicated that beginning learners showed neither sensitivity\nto semantic units in discourse (DUs) in their f0 encoding nor distinct\nsigns of pitch-related preplanning based on DUs, suggesting improper\nphrasing of the least proficient non-native speakers. Both native speakers\nand advanced learners were sensitive to the initiation and termination\nof DUs in their prosodic encoding; however, only native speakers showed\nclear signs of DU-based preplanning. We argue that the observed between-group\ndifferences in L1 and L2 speech might be attributed to differences\nin the scope of speech planning, i.e., compared with native speakers,\nwho mostly produce complete semantic units, learners&#8217; speech\nis produced step by step with pauses between phrases.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1082",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production and Acquisition  by Human Listeners"
    },
    "wu21h_interspeech": {
      "authors": [
        [
          "Minglin",
          "Wu"
        ],
        [
          "Kun",
          "Li"
        ],
        [
          "Wai-Kim",
          "Leung"
        ],
        [
          "Helen",
          "Meng"
        ]
      ],
      "title": "Transformer Based End-to-End Mispronunciation Detection and Diagnosis",
      "original": "1467",
      "page_count": 5,
      "order": 810,
      "p1": "3954",
      "pn": "3958",
      "abstract": [
        "This paper introduces two Transformer-based architectures for Mispronunciation\nDetection and Diagnosis (MDD). The first Transformer architecture (T-1)\nis a standard setup with an encoder, a decoder, a projection part and\nthe Cross Entropy (CE) loss. T-1 takes in Mel-Frequency Cepstral Coefficients\n(MFCC) as input. The second architecture (T-2) is based on wav2vec\n2.0, a pretraining framework. T-2 is composed of a CNN feature encoder,\nseveral Transformer blocks capturing contextual speech representations,\na projection part and the Connectionist Temporal Classification (CTC)\nloss. Unlike T-1, T-2 takes in raw audio data as input. Both models\nare trained in an end-to-end manner. Experiments are conducted on the\nCU-CHLOE corpus, where T-1 achieves a Phone Error Rate (PER) of 8.69%\nand F-measure of 77.23%; and T-2 achieves a PER of 5.97% and F-measure\nof 80.98%. Both models significantly outperform the previously proposed\nAGPM and CNN-RNN-CTC models, with PERs at 11.1% and 12.1% respectively,\nand F-measures at 72.61% and 74.65% respectively.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1467",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production and Acquisition  by Human Listeners"
    },
    "graham21_interspeech": {
      "authors": [
        [
          "Calbert",
          "Graham"
        ]
      ],
      "title": "L1 Identification from L2 Speech Using Neural Spectrogram Analysis",
      "original": "1545",
      "page_count": 5,
      "order": 811,
      "p1": "3959",
      "pn": "3963",
      "abstract": [
        "It is well-known that the characteristics of L2 speech are highly influenced\nby the speakers&#8217; L1. The main objective of this study was to\nuncover discriminative speech features to identify the L1 background\nof a speaker from their L2 English speech. Traditional phonetic approaches\ntend to compare speakers based on a pre-selected set of acoustic features,\nwhich may not be sufficient to capture all the unique traces of the\nL1 in the L2 speech for forensic speaker profiling purposes. Convolutional\nNeural Network (CNN) has the potential to remedy this issue through\nthe automatic processing of the visual spectrogram.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  This paper reports\na series of CNN classification experiments modelled on spectrogram\nimages. The classification problem consisted of determining whether\nEnglish speech samples are spoken by a native speaker of English, Japanese,\nDutch, French, or Polish. Both phonetically transcribed and untranscribed\nspeech data were used.<br style=\"mso-data-placement:same-cell;\" /><br\nstyle=\"mso-data-placement:same-cell;\" />\n  Overall, results showed\nthat the CNN achieved a high level of accuracy in identifying the speakers&#8217;\nL1s based on spectrogram pictures without explicit phonetic segmentation.\nHowever, the results also showed that training the classifiers on certain\ncombinations of phonetically modelled spectrogram images, which would\nmake features more transparent, can produce results with comparable\naccuracy rates.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1545",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production and Acquisition  by Human Listeners"
    },
    "oh21b_interspeech": {
      "authors": [
        [
          "Miran",
          "Oh"
        ],
        [
          "Dani",
          "Byrd"
        ],
        [
          "Shrikanth S.",
          "Narayanan"
        ]
      ],
      "title": "Leveraging Real-Time MRI for Illuminating Linguistic Velum Action",
      "original": "1823",
      "page_count": 5,
      "order": 812,
      "p1": "3964",
      "pn": "3968",
      "abstract": [
        "Velum actions are critical to differentiating oral and nasal sounds\nin spoken language; specifically in the latter, the velum is lowered\nto open the nasal port and allow nasal airflow. However, details on\nhow the velum is lowered for nasal production in speech are scarce.\nState-of-the-art real-time Magnetic Resonance Imaging (rtMRI) can directly\nimage the entirety of the moving vocal tract, providing spatiotemporal\nkinematic data of articulatory actions. Most instrumental studies of\nspeech production explore oral constriction actions such as lip or\ntongue movements. RtMRI makes possible a quantitative assessment of\nnon-oral and non-constriction actions, such as velum (and larynx) dynamics.\nThis paper illustrates articulatory aspects of consonant nasality,\nwhich have previously been inferred from acoustic or aerodynamic data.\nVelum actions are quantified in spatial and temporal domains: i) vertical\nand horizontal velum positions during nasal consonant production are\nquantified to measure, respectively, the degree of velum lowering and\nvelic opening, and ii) duration intervals for velum lowering, plateau,\nand raising are obtained to understand which portion of the velum action\nis lengthened to generate phonologically long nasality. Findings demonstrate\nthat velum action tracking using rtMRI can illuminate linguistic modulations\nof nasality strength and length.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1823",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "liu21m_interspeech": {
      "authors": [
        [
          "Zirui",
          "Liu"
        ],
        [
          "Yi",
          "Xu"
        ]
      ],
      "title": "Segmental Alignment of English Syllables with Singleton and Cluster Onsets",
      "original": "0187",
      "page_count": 5,
      "order": 813,
      "p1": "3969",
      "pn": "3973",
      "abstract": [
        "Recent research has shown fresh evidence that consonant and vowel are\nsynchronised at the syllable onset, as predicted by a number of theoretical\nmodels. The finding was made by using a minimal contrast paradigm to\ndetermine segment onset in Mandarin CV syllables, which differed from\nthe conventional method of detecting gesture onset with a velocity\nthreshold [1]. It has remained unclear, however, if CV co-onset also\noccurs between the nucleus vowel and a consonant cluster, as predicted\nby the articulatory syllable model [2]. This study applied the minimal\ncontrast paradigm to British English in both CV and clusterV (CLV)\nsyllables, and analysed the spectral patterns with signal chopping\nin conjunction with recurrent neural networks (RNN) with long short-term\nmemory (LSTM) [3]. Results show that vowel onset is synchronised with\nthe onset of the first consonant in a cluster, thus supporting the\narticulatory syllable model.\n"
      ],
      "doi": "10.21437/Interspeech.2021-187",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "hejna21_interspeech": {
      "authors": [
        [
          "M\u00ed\u0161a",
          "Hejn\u00e1"
        ]
      ],
      "title": "Exploration of Welsh English Pre-Aspiration: How Wide-Spread is it?",
      "original": "0685",
      "page_count": 5,
      "order": 814,
      "p1": "3974",
      "pn": "3978",
      "abstract": [
        "This study investigates how widespread pre-aspiration and local breathiness\nare in English spoken in Wales, by speakers identifying as Welsh. While\nthe main purpose is to establish whether the phenomenon is generally\npresent in Welsh English, the data also enables us to explore whether\npre-aspiration might be conditioned by sex/gender, age, and the ability\nto speak Welsh. An acoustic corpus of 45 speakers producing word-final\nplosives and fricatives is analysed.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  Pre-aspiration and\nlocal breathiness are produced by all speakers, representing 32 towns\nand 16 areas included in the analyses. Pre-aspiration and breathiness\nare more frequent and longer in L1 and L2 Welsh speakers than those\nwho do not speak Welsh at all. In general, no statistically significant\nsex and age effects emerge.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  In addition, a gradient\nallophony between pre-aspiration and glottalisation is reported for\nall speakers in the plosive context: the more frequently they glottalise,\nthe less frequent the pre-aspiration. In fricatives, most speakers\ndo not glottalise. Regarding those who do, 1. some display no relationship\nbetween pre-aspiration and glottalisation, and 2. a minority display\neither an indication of gradient allophony between the two, or 3. a\npositive correlation.\n"
      ],
      "doi": "10.21437/Interspeech.2021-685",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "muhlack21_interspeech": {
      "authors": [
        [
          "Beeke",
          "Muhlack"
        ],
        [
          "Mikey",
          "Elmers"
        ],
        [
          "Heiner",
          "Drenhaus"
        ],
        [
          "J\u00fcrgen",
          "Trouvain"
        ],
        [
          "Marjolein van",
          "Os"
        ],
        [
          "Raphael",
          "Werner"
        ],
        [
          "Margarita",
          "Ryzhova"
        ],
        [
          "Bernd",
          "M\u00f6bius"
        ]
      ],
      "title": "Revisiting Recall Effects of Filler Particles in German and English",
      "original": "1056",
      "page_count": 5,
      "order": 815,
      "p1": "3979",
      "pn": "3983",
      "abstract": [
        "This paper reports on two experiments that partially replicate an experiment\nby Fraundorf and Watson (2011, J Mem. Lang.) on the recall effect of\nfiller particles. Their subjects listened to three passages of a story,\neither with or without filler particles, which they had to retell afterwards.\nThey analysed the subjects&#8217; retelling in terms of whether important\nplot points were remembered or not. For their English data, they found\nthat filler particles facilitate the recall of the plot points significantly\ncompared to stories that did not include filler particles. As this\nseems to be a convincing experimental design, we aimed at evaluating\nthis method as a web-based experiment which may, if found to be suitable,\neasily be applied to other languages. Furthermore, we investigated\nwhether their results are found in German as well (Experiment 1), and\nevaluated whether filler duration has an effect on recall performance\n(Experiment 2). Our results could not replicate the findings of the\noriginal study: in fact, the opposite effect was found for German.\nIn Experiment 1, participants performed better on recall in the fluent\ncondition, while no significant results were found for English in Experiment\n2.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1056",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "ge21b_interspeech": {
      "authors": [
        [
          "Chunyu",
          "Ge"
        ],
        [
          "Yixuan",
          "Xiong"
        ],
        [
          "Peggy",
          "Mok"
        ]
      ],
      "title": "How Reliable Are Phonetic Data Collected Remotely? Comparison of Recording Devices and Environments on Acoustic Measurements",
      "original": "1122",
      "page_count": 5,
      "order": 816,
      "p1": "3984",
      "pn": "3988",
      "abstract": [
        "The COVID-19 pandemic posed an unprecedented challenge to phonetic\nresearch. On-site collection of speech data is difficult, if not impossible.\nThe advancement of technology in mobile devices and online conference\nplatforms offers the opportunity to collect data remotely. This paper\naims to answer the question of how reliable speech data collected remotely\nare based on controlled speech. Seven devices, including smartphones\nand laptops, were used to record speech simultaneously, locally or\non the cloud using ZOOM, both in a sound-attenuated lab and a conference\nroom. Common acoustic measurements were made on these recordings. Local\nrecordings proved to be reliable in duration, but not for recordings\nmade on the cloud. Different devices have comparable performances in\nF0 and F1. The values acquired by different devices differ a lot for\nF2 and higher formants, spectral moments, and voice quality measures.\nThese differences can lead to erroneous interpretation of segmental\nand voice quality contrasts. The recordings made remotely by smartphones\nand locally using ZOOM can be useful in studying prosody, but should\nbe used with care for segments.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1122",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "huang21i_interspeech": {
      "authors": [
        [
          "Jing",
          "Huang"
        ],
        [
          "Feng-fan",
          "Hsieh"
        ],
        [
          "Yueh-chin",
          "Chang"
        ]
      ],
      "title": "A Cross-Dialectal Comparison of Apical Vowels in Beijing Mandarin, Northeastern Mandarin and Southwestern Mandarin: An EMA and Ultrasound Study",
      "original": "1326",
      "page_count": 5,
      "order": 817,
      "p1": "3989",
      "pn": "3993",
      "abstract": [
        "This paper is a comparative study of the articulation of the &#8220;apical\nvowels&#8221; in three Mandarin dialects: Beijing Mandarin (BJM), Northeastern\nMandarin (NEM), and Southwestern Mandarin (SWM), using co-registered\nEMA and ultrasound. Data from 5 BJM speakers, 5 NEM speakers and 4\nSWM speakers in their twenties were analyzed and discussed. Our recording\nmaterials include the dental and retroflex apical vowels, and their\n<i>er</i>-suffixed forms. Results suggest that distinct lingual configurations\nare found among the three dialects of Mandarin, even though these apical\nvowels are not perceptually distinguishable. Specifically, the dental\napical vowel [&#x27f;] has a grooved tongue shape in BJM, a retracted\ntongue dorsum in NEM, and a relatively flat tongue shape in SWM. The\nretroflex apical vowel [&#x285;] has a domed tongue shape as well as\na bunched tongue body in NEM, while a slightly domed tongue posture\nis found in SWM. Moreover, the retroflex apical vowel [&#x285;] is,\narticulatorily speaking, very similar to the <i>er</i>-suffix in BJM\n(cf. [10]). In sum, we observed yet another instance of the articulatory-acoustic\nmismatch.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1326"
    },
    "gibson21_interspeech": {
      "authors": [
        [
          "Mark",
          "Gibson"
        ],
        [
          "Oihane",
          "Muxika"
        ],
        [
          "Marianne",
          "Pouplier"
        ]
      ],
      "title": "Dissecting the Aero-Acoustic Parameters of Open Articulatory Transitions",
      "original": "1379",
      "page_count": 5,
      "order": 818,
      "p1": "3994",
      "pn": "3998",
      "abstract": [
        "We capitalize on previously recorded kinematic and acoustic data for\nthree languages (Georgian (GE), Spanish (SP) and Moroccan Arabic (MA))\nthat exhibit open articulatory transitions between the consonants in\nclusters in order to dissect the aero-acoustic parameters of the transitions\nin each language. These particular languages are of interest because\nthey show similar patterns of interarticulatory timing in clusters,\noffering the unique opportunity to examine the acoustics of open transitions\ncross-linguistically. Our analysis centers on word initial clusters\n(/kl/ and /gl/), from which we extract relativized temporal values\nrelevant to clusters and spectral parameters related to open articulatory\ntransitions. We report baseline results using linear mixed effects\nmodels, then train a Random Forest model in a supervised learning environment\non the significant variables. After training, test tokens are introduced\nin order to test whether the model can categorize the language based\non the spectral and temporal parameters, and rank variables in terms\nof their feature importance. The results show that the model can categorize\nthe data to the correct language with a 95.59% accuracy rate, where\nnormalized zero-crossing (nzcr), modifications of the amplitude envelope\n(&#916;E), and intensity ratio ranked highest in feature importance.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1379",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "gully21_interspeech": {
      "authors": [
        [
          "Amelia J.",
          "Gully"
        ]
      ],
      "title": "Quantifying Vocal Tract Shape Variation and its Acoustic Impact: A Geometric Morphometric Approach",
      "original": "1400",
      "page_count": 5,
      "order": 819,
      "p1": "3999",
      "pn": "4003",
      "abstract": [
        "The shape of the vocal tract varies considerably between individuals.\nThe relationship between detailed variation in vocal tract shape and\nthe acoustics of speech is not yet well understood, despite its potential\nfor increasing understanding in the fields of voice biometrics, forensic\nspeech science, and personalised speech synthesis. One reason that\nthis topic has not yet been extensively explored is that 3D vocal tract\nshape is difficult to quantify robustly. Geometric morphometrics is\na technique developed in evolutionary biology for statistically valid\nquantification and comparison of anatomical shapes. This study makes\nuse of 3D magnetic resonance imaging data of the vocal tracts of eight\nindividuals, and accompanying audio recordings, combined with geometric\nmorphometric techniques to determine whether the method offers useful\ninformation for speech science. The results suggest a linear relationship\nbetween the shapes of the vocal tract and output spectra, and there\nis evidence of possible sexual dimorphism and allometry (a systematic\nvariation of shape with size) in the vocal tract, although due to the\nlimited sample size the results did not reach statistical significance.\nThe results suggest that geometric morphometrics can provide useful\ninformation about the vocal tract, and justify further study using\nthis technique.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1400",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "guevararukoz21_interspeech": {
      "authors": [
        [
          "Adriana",
          "Guevara-Rukoz"
        ],
        [
          "Shi",
          "Yu"
        ],
        [
          "Sharon",
          "Peperkamp"
        ]
      ],
      "title": "Speech Perception and Loanword Adaptations: The Case of Copy-Vowel Epenthesis",
      "original": "1481",
      "page_count": 5,
      "order": 820,
      "p1": "4004",
      "pn": "4008",
      "abstract": [
        "Japanese allows for almost no consonants in syllable codas. In loanwords,\nillegal codas are transformed into onsets by means of vowel epenthesis.\nThe default epenthetic vowel in loanwords is [&#x26F;], and previous\nwork has shown that this [&#x26F;]-epenthesis reflects Japanese listeners&#8217;\nperception of illegal coda consonants. Here, we focus on one of the\nnon-default cases: following coda [&#231;] and [x] the epenthetic vowel\nis a copy of the preceding vowel. Using an identification and a discrimination\ntask, we provide evidence for the perceptual origin of this copy vowel\nphenomenon: After [&#231;] and [x], Japanese listeners perceive more\noften an epenthetic copy vowel than the default vowel [&#x26F;], whereas\nafter [k] it is the reverse.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1481",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "guo21b_interspeech": {
      "authors": [
        [
          "Zhe-chen",
          "Guo"
        ],
        [
          "Rajka",
          "Smiljanic"
        ]
      ],
      "title": "Speakers Coarticulate Less When Facing Real and Imagined Communicative Difficulties: An Analysis of Read and Spontaneous Speech from the LUCID Corpus",
      "original": "1640",
      "page_count": 5,
      "order": 821,
      "p1": "4009",
      "pn": "4013",
      "abstract": [
        "This study investigated coarticulation of read and spontaneous speech\nin different communicative contexts from the LUCID corpus. Spontaneous\nspeech samples were from Southern British English speakers who completed\nan interactive spot-the-differences task with no communicative barrier\n(NB), with their voice vocoded (VOC), and with a partner who heard\ntheir speech in babble (BABBLE) or was a non-native English speaker\n(L2). The same speakers also read sentences in a casual (READ-CO) and\nclear (READ-CL) speaking style. Tokens of a pre-defined set of keywords\nwere extracted from the speech samples and consonant-vowel sequences\nin these tokens were analyzed using a whole-spectrum measure of coarticulation.\nResults showed that coarticulatory resistance in the six communicative\ncontexts from highest to lowest was: BABBLE &#62; VOC, L2, READ-CL\n&#62; NB, READ-CO. Thus, in response to communicative barriers, be\nthey real or imaginary, speakers coarticulated less, in line with the\nmodels of targeted speaker adaptations (the H&amp;H theory [1] and\nAdaptive Speaker Framework [2]).\n"
      ],
      "doi": "10.21437/Interspeech.2021-1640",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "meister21_interspeech": {
      "authors": [
        [
          "Einar",
          "Meister"
        ],
        [
          "Lya",
          "Meister"
        ]
      ],
      "title": "Developmental Changes of Vowel Acoustics in Adolescents",
      "original": "1649",
      "page_count": 5,
      "order": 822,
      "p1": "4014",
      "pn": "4018",
      "abstract": [
        "The paper explores the developmental changes of vowel acoustics in\nEstonian adolescents as a function of age and gender. Formant frequencies\nF1&#8211;F4 and the duration of vowels were measured from read speech\nsamples of 305 native Estonian subjects (173 girls and 132 boys) aged\nfrom 10 to 18 years. GAM framework was applied for the statistical\nanalysis. The results show that both the formant frequencies and the\nvowel space area decrease gradually from 10 to 15 years in both gender\ngroups and the quality of vowels stabilizes at the age of 15&#8211;18\nyears, whereas gender-specific differences emerge around the age of\n12&#8211;13. Age-related change in the duration of vowels shows similar\npatterns with formants, however, with no gender difference. The findings\nare in line with the results reported for adolescent speech in other\nlanguages. The analysis results based on speech samples of the subjects\nwith normal linguistic development can be considered reference data\nfor distinguishing between normal and abnormal speech development.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1649",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "dapolito21_interspeech": {
      "authors": [
        [
          "Sonia",
          "d'Apolito"
        ],
        [
          "Barbara Gili",
          "Fivela"
        ]
      ],
      "title": "Context and Co-Text Influence on the Accuracy Production of Italian L2 Non-Native Sounds",
      "original": "1724",
      "page_count": 5,
      "order": 823,
      "p1": "4019",
      "pn": "4023",
      "abstract": [
        "Accuracy in production of non-native sounds is analyzed by considering\nthe influence of L1, context and co-text on Italian L2 speech. While\nthe L1 influence is often described in the literature, careful investigations\non how production accuracy may change in different contexts and co-texts\nare needed. This paper describes two experiments on how French learners\nof Italian as L2 (advanced/beginners) realize geminates depending on\ndifferent contexts (the global contexts, e.g., the tasks) and co-texts\n(the amount of information available syntagmatically).<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  Acoustic data acquired\nby recording 4 advanced and 4 beginner Italian-L2 learners (and 3 Italian\nnatives as control) were analyzed as for the duration of the target\nconsonant and preceding vowel, as well as speech articulation rate,\ntaken as indexes of geminate production accuracy.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  Results confirm the\nstrongest influence of L1 in beginners&#8217; production, and depict\na complex interplay of context and co- text. Adding information in\nco-text may induce different effects on speech production, depending\non the local context, that is on the speakers&#8217; communication\nneeds during speech production. Specifically, a &#8220;rich&#8221;\nco-text may favor a decrease in production accuracy or, on the contrary,\nan increase, depending on the need the speaker have to highlight/contrast\ninformation.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1724",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "heeringa21_interspeech": {
      "authors": [
        [
          "Wilbert",
          "Heeringa"
        ],
        [
          "Hans Van de",
          "Velde"
        ]
      ],
      "title": "A New Vowel Normalization for Sociophonetics",
      "original": "1846",
      "page_count": 5,
      "order": 824,
      "p1": "4024",
      "pn": "4028",
      "abstract": [
        "Several studies have shown that in sociophonetic research Lobanov&#8217;s\nspeaker normalization method outperforms other methods for normalizing\nvowel formants of speakers. An advantage of Lobanov&#8217;s method\ncompared to the method that was introduced by Watt &amp; Fabricius\nin 2002 is that it is independent of the shape of the vowel space area,\nand also normalizes to the dispersion of the vowels. However, it does\ndepend on the distribution of the vowels within the vowel space. When\nusing Lobanov normalization the formant values are converted to z-scores.\nWe present a method where the &#181; in the z-score formula is replaced\nby the center of the convex hull that encloses the vowels, and the\n&#963; is obtained on the basis of the points that constitute the convex\nhull. When normalizing measurements of two real data sets, and of a\nseries of randomly generated data sets, we found that our method improved\nin matching vowel spaces in size and overlap.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1846",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "billington21_interspeech": {
      "authors": [
        [
          "Rosey",
          "Billington"
        ],
        [
          "Hywel",
          "Stoakes"
        ],
        [
          "Nick",
          "Thieberger"
        ]
      ],
      "title": "The Pacific Expansion: Optimizing Phonetic Transcription of Archival Corpora",
      "original": "2167",
      "page_count": 5,
      "order": 825,
      "p1": "4029",
      "pn": "4033",
      "abstract": [
        "For most of the world&#8217;s languages, detailed phonetic analyses\nacross different aspects of the sound system do not exist, due in part\nto limitations in available speech data and tools for efficiently processing\nsuch data for low-resource languages. Archival language documentation\ncollections offer opportunities to extend the scope and scale of phonetic\nresearch on low-resource languages, and developments in methods for\nautomatic recognition and alignment of speech facilitate the preparation\nof phonetic corpora based on these collections. We present a case study\napplying speech modelling and forced alignment methods to narrative\ndata for Nafsan, an Oceanic language of central Vanuatu. We examine\nthe accuracy of the forced-aligned phonetic labelling based on limited\nspeech data used in the modelling process, and compare acoustic and\ndurational measures of 17,851 vowel tokens for 11 speakers with previous\nexperimental phonetic data for Nafsan. Results point to the suitability\nof archival data for large-scale studies of phonetic variation in low-resource\nlanguages, and also suggest that this approach can feasibly be used\nas a starting point in expanding to phonetic comparisons across closely-related\nOceanic languages.\n"
      ],
      "doi": "10.21437/Interspeech.2021-2167",
      "author_area_id": "2",
      "author_area_label": "Phonetics, Phonology, and Prosody"
    },
    "tian21_interspeech": {
      "authors": [
        [
          "Zhengkun",
          "Tian"
        ],
        [
          "Jiangyan",
          "Yi"
        ],
        [
          "Ye",
          "Bai"
        ],
        [
          "Jianhua",
          "Tao"
        ],
        [
          "Shuai",
          "Zhang"
        ],
        [
          "Zhengqi",
          "Wen"
        ]
      ],
      "title": "FSR: Accelerating the Inference Process of Transducer-Based Models by Applying Fast-Skip Regularization",
      "original": "1367",
      "page_count": 5,
      "order": 826,
      "p1": "4034",
      "pn": "4038",
      "abstract": [
        "Transducer-based models, such as RNN-Transducer and transformer-transducer,\nhave achieved great success in speech recognition. A typical transducer\nmodel decodes the output sequence conditioned on the current acoustic\nstate and previously predicted tokens step by step. Statistically,\nThe number of blank tokens in the prediction results accounts for nearly\n90% of all tokens. It takes a lot of computation and time to predict\nthe blank tokens, but only the non-blank tokens will appear in the\nfinal output sequence. Therefore, we propose a method named fast-skip\nregularization, which tries to align the blank position predicted by\na transducer with that predicted by a connectionist temporal classification\n(CTC) model. During the inference, the transducer model can predict\nthe blank tokens in advance by a simple CTC project layer without many\ncomplicated forward calculations of the transducer decoder and then\nskip them, which will reduce the computation and improve the inference\nspeed greatly. All experiments are conducted on a public Chinese mandarin\ndataset AISHELL-1. The results show that the fast-skip regularization\ncan indeed help the transducer model learn the blank position alignments.\nBesides, the inference with fast-skip can be speeded up nearly 4 times\nwith only a little performance degradation.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1367",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "mitrofanov21_interspeech": {
      "authors": [
        [
          "Anton",
          "Mitrofanov"
        ],
        [
          "Mariya",
          "Korenevskaya"
        ],
        [
          "Ivan",
          "Podluzhny"
        ],
        [
          "Yuri",
          "Khokhlov"
        ],
        [
          "Aleksandr",
          "Laptev"
        ],
        [
          "Andrei",
          "Andrusenko"
        ],
        [
          "Aleksei",
          "Ilin"
        ],
        [
          "Maxim",
          "Korenevsky"
        ],
        [
          "Ivan",
          "Medennikov"
        ],
        [
          "Aleksei",
          "Romanenko"
        ]
      ],
      "title": "LT-LM: A Novel Non-Autoregressive Language Model for Single-Shot Lattice Rescoring",
      "original": "1716",
      "page_count": 5,
      "order": 827,
      "p1": "4039",
      "pn": "4043",
      "abstract": [
        "Neural network-based language models are commonly used in rescoring\napproaches to improve the quality of modern automatic speech recognition\n(ASR) systems. Most of the existing methods are computationally expensive\nsince they use autoregressive language models. We propose a novel rescoring\napproach, which processes the entire lattice in a single call to the\nmodel. The key feature of our rescoring policy is a novel non-autoregressive\nLattice Transformer Language Model (LT-LM). This model takes the whole\nlattice as an input and predicts a new language score for each arc.\nAdditionally, we propose the artificial lattices generation approach\nto incorporate a large amount of text data in the LT-LM training process.\nOur single-shot rescoring performs orders of magnitude faster than\nother rescoring methods in our experiments. It is more than 300 times\nfaster than pruned RNNLM lattice rescoring and N-best rescoring while\nslightly inferior in terms of WER.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1716",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "allauzen21_interspeech": {
      "authors": [
        [
          "Cyril",
          "Allauzen"
        ],
        [
          "Ehsan",
          "Variani"
        ],
        [
          "Michael",
          "Riley"
        ],
        [
          "David",
          "Rybach"
        ],
        [
          "Hao",
          "Zhang"
        ]
      ],
      "title": "A Hybrid Seq-2-Seq ASR Design for On-Device and Server Applications",
      "original": "0658",
      "page_count": 5,
      "order": 828,
      "p1": "4044",
      "pn": "4048",
      "abstract": [
        "This paper proposes and evaluates alternative speech recognition design\nstrategies using the hybrid autoregressive transducer (HAT) model.\nThe different strategies are designed with special attention to the\nchoice of modeling units and to the integration of different types\nof external language models during first-pass beam-search or second-pass\nre-scoring. These approaches are compared on a large-scale voice search\ntask and the recognition quality over the head and tail of speech data\nis analyzed. Our experiments show decent improvements in WER over common\nspeech phrases and significant gains on uncommon ones compared to the\nstate-of-the-art approaches.\n"
      ],
      "doi": "10.21437/Interspeech.2021-658",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "inaguma21b_interspeech": {
      "authors": [
        [
          "Hirofumi",
          "Inaguma"
        ],
        [
          "Tatsuya",
          "Kawahara"
        ]
      ],
      "title": "VAD-Free Streaming Hybrid CTC/Attention ASR for Unsegmented Recording",
      "original": "1107",
      "page_count": 5,
      "order": 829,
      "p1": "4049",
      "pn": "4053",
      "abstract": [
        "In this work, we propose novel decoding algorithms to enable streaming\nautomatic speech recognition (ASR) on unsegmented long-form recordings\nwithout voice activity detection (VAD), based on monotonic chunkwise\nattention (MoChA) with an auxiliary connectionist temporal classification\n(CTC) objective. We propose a <i>block-synchronous</i> beam search\ndecoding to take advantage of efficient batched output-synchronous\nand low-latency input-synchronous searches. We also propose a VAD-free\ninference algorithm that leverages CTC probabilities to determine a\nsuitable timing to reset the model states to tackle the vulnerability\nto long-form data. Experimental evaluations demonstrate that the block-synchronous\ndecoding achieves comparable accuracy to the label-synchronous one.\nMoreover, the VAD-free inference can recognize long-form speech robustly\nfor up to a few hours.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1107",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "yao21_interspeech": {
      "authors": [
        [
          "Zhuoyuan",
          "Yao"
        ],
        [
          "Di",
          "Wu"
        ],
        [
          "Xiong",
          "Wang"
        ],
        [
          "Binbin",
          "Zhang"
        ],
        [
          "Fan",
          "Yu"
        ],
        [
          "Chao",
          "Yang"
        ],
        [
          "Zhendong",
          "Peng"
        ],
        [
          "Xiaoyu",
          "Chen"
        ],
        [
          "Lei",
          "Xie"
        ],
        [
          "Xin",
          "Lei"
        ]
      ],
      "title": "WeNet: Production Oriented Streaming and Non-Streaming End-to-End Speech Recognition Toolkit",
      "original": "1983",
      "page_count": 5,
      "order": 830,
      "p1": "4054",
      "pn": "4058",
      "abstract": [
        "In this paper, we propose an open source speech recognition toolkit\ncalled WeNet, in which a new two-pass approach named U2 is implemented\nto unify streaming and non-streaming end-to-end (E2E) speech recognition\nin a single model. The main motivation of WeNet is to close the gap\nbetween the research and deployment of E2E speech recognition models.\nWeNet provides an efficient way to ship automatic speech recognition\n(ASR) applications in real-world scenarios, which is the main difference\nand advantage to other open source E2E speech recognition toolkits.\nWe develop a hybrid connectionist temporal classification (CTC)/attention\narchitecture with transformer or conformer as encoder and an attention\ndecoder to rescore the CTC hypotheses. To achieve streaming and non-streaming\nin a unified model, we use a dynamic chunk-based attention strategy\nwhich allows the self-attention to focus on the right context with\nrandom length. Our experiments on the AISHELL-1 dataset show that our\nmodel achieves 5.03% relative character error rate (CER) reduction\nin non-streaming ASR compared to a standard non-streaming transformer.\nAfter model quantification, our model achieves reasonable RTF and latency\nat runtime. The toolkit is publicly available.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1983",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "tanaka21b_interspeech": {
      "authors": [
        [
          "Tomohiro",
          "Tanaka"
        ],
        [
          "Ryo",
          "Masumura"
        ],
        [
          "Mana",
          "Ihori"
        ],
        [
          "Akihiko",
          "Takashima"
        ],
        [
          "Takafumi",
          "Moriya"
        ],
        [
          "Takanori",
          "Ashihara"
        ],
        [
          "Shota",
          "Orihashi"
        ],
        [
          "Naoki",
          "Makishima"
        ]
      ],
      "title": "Cross-Modal Transformer-Based Neural Correction Models for Automatic Speech Recognition",
      "original": "1992",
      "page_count": 5,
      "order": 831,
      "p1": "4059",
      "pn": "4063",
      "abstract": [
        "We propose a cross-modal transformer-based neural correction models\nthat refines the output of an automatic speech recognition (ASR) system\nso as to exclude ASR errors. Generally, neural correction models are\ncomposed of encoder-decoder networks, which can directly model sequence-to-sequence\nmapping problems. The most successful method is to use both input speech\nand its ASR output text as the input contexts for the encoder-decoder\nnetworks. However, the conventional method cannot take into account\nthe relationships between these two different modal inputs because\nthe input contexts are separately encoded for each modal. To effectively\nleverage the correlated information between the two different modal\ninputs, our proposed models encode two different contexts jointly on\nthe basis of cross-modal self-attention using a transformer. We expect\nthat cross-modal self-attention can effectively capture the relationships\nbetween two different modals for refining ASR hypotheses. We also introduce\na shallow fusion technique to efficiently integrate the first-pass\nASR model and our proposed neural correction model. Experiments on\nJapanese natural language ASR tasks demonstrated that our proposed\nmodels achieve better ASR performance than conventional neural correction\nmodels.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1992",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "lee21f_interspeech": {
      "authors": [
        [
          "Mun-Hak",
          "Lee"
        ],
        [
          "Joon-Hyuk",
          "Chang"
        ]
      ],
      "title": "Deep Neural Network Calibration for E2E Speech Recognition System",
      "original": "0176",
      "page_count": 5,
      "order": 832,
      "p1": "4064",
      "pn": "4068",
      "abstract": [
        "Cross-entropy loss, which is commonly used in deep-neural-network-based\n(DNN) classification model training, induces models to assign a high\nprobability value to one class. Networks trained in this fashion tend\nto be overconfident, which causes a problem in the decoding process\nof the speech recognition system, as it uses the combined probability\ndistribution of multiple independently trained networks. Overconfidence\nin neural networks can be quantified as a calibration error, which\nis the difference between the output probability of a model and the\nlikelihood of obtaining an actual correct answer. We show that the\ndeep-learning-based components of an end-to-end (E2E) speech recognition\nsystem with high classification accuracy contain calibration errors\nand quantify them using various calibration measures. In addition,\nit was experimentally shown that the calibration function, which was\nbeing trained to minimize calibration errors effectively mitigates\nthose of the speech recognition system, and as a result, can improve\nthe performance of beam-search during decoding.\n"
      ],
      "doi": "10.21437/Interspeech.2021-176",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "li21m_interspeech": {
      "authors": [
        [
          "Qiujia",
          "Li"
        ],
        [
          "Yu",
          "Zhang"
        ],
        [
          "Bo",
          "Li"
        ],
        [
          "Liangliang",
          "Cao"
        ],
        [
          "Philip C.",
          "Woodland"
        ]
      ],
      "title": "Residual Energy-Based Models for End-to-End Speech Recognition",
      "original": "0690",
      "page_count": 5,
      "order": 833,
      "p1": "4069",
      "pn": "4073",
      "abstract": [
        "End-to-end models with auto-regressive decoders have shown impressive\nresults for automatic speech recognition (ASR). These models formulate\nthe sequence-level probability as a product of the conditional probabilities\nof all individual tokens given their histories. However, the performance\nof locally normalised models can be sub-optimal because of factors\nsuch as exposure bias. Consequently, the model distribution differs\nfrom the underlying data distribution. In this paper, the residual\nenergy-based model (R-EBM) is proposed to complement the auto-regressive\nASR model to close the gap between the two distributions. Meanwhile,\nR-EBMs can also be regarded as utterance-level confidence estimators,\nwhich may benefit many downstream tasks. Experiments on a 100hr LibriSpeech\ndataset show that R-EBMs can reduce the word error rates (WERs) by\n8.2%/6.7% while improving areas under precision-recall curves of confidence\nscores by 12.6%/28.4% on test-clean/test-other sets. Furthermore, on\na state-of-the-art model using self-supervised learning (wav2vec 2.0),\nR-EBMs still significantly improves both the WER and confidence estimation\nperformance.\n"
      ],
      "doi": "10.21437/Interspeech.2021-690",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "qiu21b_interspeech": {
      "authors": [
        [
          "David",
          "Qiu"
        ],
        [
          "Yanzhang",
          "He"
        ],
        [
          "Qiujia",
          "Li"
        ],
        [
          "Yu",
          "Zhang"
        ],
        [
          "Liangliang",
          "Cao"
        ],
        [
          "Ian",
          "McGraw"
        ]
      ],
      "title": "Multi-Task Learning for End-to-End ASR Word and Utterance Confidence with Deletion Prediction",
      "original": "1207",
      "page_count": 5,
      "order": 834,
      "p1": "4074",
      "pn": "4078",
      "abstract": [
        "Confidence scores are very useful for downstream applications of automatic\nspeech recognition (ASR) systems. Recent works have proposed using\nneural networks to learn word or utterance confidence scores for end-to-end\nASR. In those studies, word confidence by itself does not model deletions,\nand utterance confidence does not take advantage of word-level training\nsignals. This paper proposes to jointly learn word confidence, word\ndeletion, and utterance confidence. Empirical results show that multi-task\nlearning with all three objectives improves confidence metrics (NCE,\nAUC, RMSE) without the need for increasing the model size of the confidence\nestimation module. Using the utterance-level confidence for rescoring\nalso decreases the word error rates on Google&#8217;s Voice Search\nand Long-tail Maps datasets by 3&#8211;5% relative, without needing\na dedicated neural rescorer.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1207",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "ollerenshaw21_interspeech": {
      "authors": [
        [
          "Anna",
          "Ollerenshaw"
        ],
        [
          "Md. Asif",
          "Jalal"
        ],
        [
          "Thomas",
          "Hain"
        ]
      ],
      "title": "Insights on Neural Representations for End-to-End Speech Recognition",
      "original": "1516",
      "page_count": 5,
      "order": 835,
      "p1": "4079",
      "pn": "4083",
      "abstract": [
        "End-to-end automatic speech recognition (ASR) models aim to learn a\ngeneralised speech representation. However, there are limited tools\navailable to understand the internal functions and the effect of hierarchical\ndependencies within the model architecture. It is crucial to understand\nthe correlations between the layer-wise representations, to derive\ninsights on the relationship between neural representations and performance.\nPrevious investigations of network similarities using correlation analysis\ntechniques have not been explored for End-to-End ASR models. This paper\nanalyses and explores the internal dynamics between layers during training\nwith CNN, LSTM and Transformer based approaches using Canonical correlation\nanalysis (CCA) and centered kernel alignment (CKA) for the experiments.\nIt was found that neural representations within CNN layers exhibit\nhierarchical correlation dependencies as layer depth increases but\nthis is mostly limited to cases where neural representation correlates\nmore closely. This behaviour is not observed in LSTM architecture,\nhowever there is a bottom-up pattern observed across the training process,\nwhile Transformer encoder layers exhibit irregular coefficiency correlation\nas neural depth increases. Altogether, these results provide new insights\ninto the role that neural architectures have upon speech recognition\nperformance. More specifically, these techniques can be used as indicators\nto build better performing speech recognition models.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1516",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "afshan21_interspeech": {
      "authors": [
        [
          "Amber",
          "Afshan"
        ],
        [
          "Kshitiz",
          "Kumar"
        ],
        [
          "Jian",
          "Wu"
        ]
      ],
      "title": "Sequence-Level Confidence Classifier for ASR Utterance Accuracy and Application to Acoustic Models",
      "original": "1666",
      "page_count": 5,
      "order": 836,
      "p1": "4084",
      "pn": "4088",
      "abstract": [
        "Scores from traditional confidence classifiers (CCs) in automatic speech\nrecognition (ASR) systems lack universal interpretation and vary with\nupdates to the underlying confidence or acoustic models (AMs). In this\nwork, we build interpretable confidence scores with an objective to\nclosely align with ASR accuracy. We propose a new sequence-level CC\nwith a richer context providing CC scores highly correlated with ASR\naccuracy and scores stable across CC updates. Hence, expanding CC applications.\nRecently, AM customization has gained traction with the widespread\nuse of unified models. Conventional adaptation strategies that customize\nAM expect well-matched data for the target domain with gold-standard\ntranscriptions. We propose a cost-effective method of using CC scores\nto select an optimal adaptation data set, where we maximize ASR gains\nfrom minimal data. We study data in various confidence ranges and optimally\nchoose data for AM adaptation with KL-Divergence regularization. On\nthe Microsoft voice search task, data selection for supervised adaptation\nusing the sequence-level confidence scores achieves word error rate\nreduction (WERR) of 8.5% for row-convolution LSTM (RC-LSTM) and 5.2%\nfor latency-controlled bidirectional LSTM (LC-BLSTM). In the semi-supervised\ncase, with ASR hypotheses as labels, our method provides WERR of 5.9%\nand 2.8% for RC-LSTM and LC-BLSTM, respectively.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1666",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "tjandra21_interspeech": {
      "authors": [
        [
          "Andros",
          "Tjandra"
        ],
        [
          "Ruoming",
          "Pang"
        ],
        [
          "Yu",
          "Zhang"
        ],
        [
          "Shigeki",
          "Karita"
        ]
      ],
      "title": "Unsupervised Learning of Disentangled Speech Content and Style Representation",
      "original": "1936",
      "page_count": 5,
      "order": 837,
      "p1": "4089",
      "pn": "4093",
      "abstract": [
        "Speech is influenced by a number of underlying factors, which can be\nbroadly categorized into linguistic contents and speaking styles. However,\ncollecting the labeled data that annotates both content and style is\nan expensive and time-consuming task. Here, we present an approach\nfor unsupervised learning of speech representation disentangling contents\nand styles. Our model consists of: (1) a local encoder that captures\nper-frame information; (2) a global encoder that captures per-utterance\ninformation; and (3) a conditional decoder that reconstructs speech\ngiven local and global latent variables. Our experiments show that\n(1) the local latent variables encode speech contents, as reconstructed\nspeech can be recognized by ASR with low word error rates (WER), even\nwith a different global encoding; (2) the global latent variables encode\nspeaker style, as reconstructed speech shares speaker identity with\nthe source utterance of the global encoding. Additionally, we demonstrate\na useful application from our pre-trained model, where we can train\na speaker recognition model from the global latent variables and achieve\nhigh accuracy by fine-tuning with as few data as one label per speaker.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1936",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "choi21_interspeech": {
      "authors": [
        [
          "Eunbi",
          "Choi"
        ],
        [
          "Hwa-Yeon",
          "Kim"
        ],
        [
          "Jong-Hwan",
          "Kim"
        ],
        [
          "Jae-Min",
          "Kim"
        ]
      ],
      "title": "Label Embedding for Chinese Grapheme-to-Phoneme Conversion",
      "original": "0885",
      "page_count": 5,
      "order": 838,
      "p1": "4094",
      "pn": "4098",
      "abstract": [
        "Chinese grapheme-to-phoneme (G2P) conversion plays a significant role\nin text-to-speech systems by generating pronunciations corresponding\nto Chinese input characters. The main challenge in Chinese G2P conversion\nis polyphone disambiguation, which requires selecting the appropriate\npronunciation among several candidates. In polyphone disambiguation,\ncalculating probabilities for the entire pronunciations is unnecessary\nsince each Chinese character has only a few (mostly two or three) candidate\npronunciations. In this study, we introduce a label embedding approach\nthat matches the character embedding with the closest label embedding\namong the possible candidates. Specifically, negative sampling and\ntriplet loss were applied to maximize the difference between the correct\nembedding and the other candidate embeddings. Experimental results\nshow that the label embedding approach improved the polyphone disambiguation\naccuracy by 4.50% and 1.74% on two datasets compared to the one-hot\nlabel classification approach. Moreover, the bidirectional long short-term\nmemory model with the label embedding approach outperformed the previous\nmost advanced model, BERT, demonstrating outstanding performance in\npolyphone disambiguation. Lastly, we discuss the effect of contextual\ninformation in character embeddings on the G2P conversion task.\n"
      ],
      "doi": "10.21437/Interspeech.2021-885",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "zhang21aa_interspeech": {
      "authors": [
        [
          "Haiteng",
          "Zhang"
        ]
      ],
      "title": "PDF: Polyphone Disambiguation in Chinese by Using FLAT",
      "original": "1087",
      "page_count": 5,
      "order": 839,
      "p1": "4099",
      "pn": "4103",
      "abstract": [
        "Polyphone disambiguation is an essential procedure in the front-end\nmodule of the Chinese text-to-speech (TTS) system. It serves to predict\nthe pronunciation of the input polyphonic character. In the Chinese\nTTS system, a well-designed pronunciation dictionary plays a crucial\nrole in supplying pinyin to words. However, the conventional system\nis unable to fully utilize the pronunciation dictionary while modelling\nbecause of the unavoidable Chinese segment errors and model structure.\nIn this paper, we proposed a system named PDF: Polyphone Disambiguation\nby using FLAT. The proposed model encodes both the input character\nsequence and dictionary matched words of the sentence, enabling the\nmodel to both avoid segment errors and leverage the well-designed pronunciation\ndictionary in the model. Additionally, we also use the pre-trained\nlanguage model (PLM) as an encoder to extract the contextual information\nof input sequence. The experimental results verified the effectiveness\nof the proposed PDF model. Our system obtains an improvement in accuracy\nby 0.98% compared to Bert on an open-source dataset. The experiential\nresults demonstrate that leveraging pronunciation dictionary while\nmodelling helps improve the performance of polyphone disambiguation\nsystem.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1087"
    },
    "li21n_interspeech": {
      "authors": [
        [
          "Junjie",
          "Li"
        ],
        [
          "Zhiyu",
          "Zhang"
        ],
        [
          "Minchuan",
          "Chen"
        ],
        [
          "Jun",
          "Ma"
        ],
        [
          "Shaojun",
          "Wang"
        ],
        [
          "Jing",
          "Xiao"
        ]
      ],
      "title": "Improving Polyphone Disambiguation for Mandarin Chinese by Combining Mix-Pooling Strategy and Window-Based Attention",
      "original": "1232",
      "page_count": 5,
      "order": 840,
      "p1": "4104",
      "pn": "4108",
      "abstract": [
        "In this paper, we propose a novel system based on word-level features\nand window-based attention for polyphone disambiguation, which is a\nfundamental task for Grapheme-to-phoneme (G2P) conversion of Mandarin\nChinese. The framework aims to combine a pre-trained language model\nwith explicit word-level information in order to get meaningful context\nextraction. Particularly, we employ a pre-trained bidirectional encoder\nfrom Transformers (BERT) model to extract character-level features,\nand an external Chinese word segmentation (CWS) tool is used to obtain\nthe word units. We adopt a mixed pooling mechanism to convert character-level\nfeatures into word-level features based on the segmentation results.\nA window-based attention module is utilized to incorporate contextual\nword-level features for the polyphonic characters. Experimental results\nshow that our method achieves an accuracy of 99.06% on an open benchmark\ndataset for Mandarin Chinese polyphone disambiguation, which outperforms\nthe baseline systems.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1232",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "shi21d_interspeech": {
      "authors": [
        [
          "Yi",
          "Shi"
        ],
        [
          "Congyi",
          "Wang"
        ],
        [
          "Yu",
          "Chen"
        ],
        [
          "Bin",
          "Wang"
        ]
      ],
      "title": "Polyphone Disambiguation in Mandarin Chinese with Semi-Supervised Learning",
      "original": "0502",
      "page_count": 5,
      "order": 841,
      "p1": "4109",
      "pn": "4113",
      "abstract": [
        "The majority of Chinese characters are monophonic, while a special\ngroup of characters, called polyphonic characters, have multiple pronunciations.\nAs a prerequisite of performing speech-related generative tasks, the\ncorrect pronunciation must be identified among several candidates.\nThis process is called Polyphone Disambiguation. Although the problem\nhas been well explored with both knowledge-based and learning-based\napproaches, it remains challenging due to the lack of publicly available\nlabeled datasets and the irregular nature of polyphone in Mandarin\nChinese. In this paper, we propose a novel semi-supervised learning\n(SSL) framework for Mandarin Chinese polyphone disambiguation that\ncan potentially leverage unlimited unlabeled text data. We explore\nthe effect of various proxy labeling strategies including entropy-thresholding\nand lexicon-based labeling. Qualitative and quantitative experiments\ndemonstrate that our method achieves state-of-the-art performance.\nIn addition, we publish a novel dataset specifically for the polyphone\ndisambiguation task to promote further researches.\n"
      ],
      "doi": "10.21437/Interspeech.2021-502"
    },
    "chen21s_interspeech": {
      "authors": [
        [
          "Yue",
          "Chen"
        ],
        [
          "Zhen-Hua",
          "Ling"
        ],
        [
          "Qing-Feng",
          "Liu"
        ]
      ],
      "title": "A Neural-Network-Based Approach to Identifying Speakers in Novels",
      "original": "0609",
      "page_count": 5,
      "order": 842,
      "p1": "4114",
      "pn": "4118",
      "abstract": [
        "Identifying speakers in novels aims at determining who says a quote\nin a given context by text analysis. This task is important for speech\nsynthesis systems to assign appropriate voices to the quotes when producing\naudiobooks. However, existing approaches stick with manual features\nand traditional machine learning classifiers, which constrain the accuracy\nof speaker identification. In this paper, we propose a method to tackle\nthis challenging problem with the help of deep learning. We formulate\nspeaker identification as a scoring task and build a candidate scoring\nnetwork (CSN) based on BERT. Candidate-specific segments are put forward\nto eliminate redundant context information. Moreover, a revision algorithm\nis designed utilizing the speaker alternation pattern in two-party\ndialogues. Experiments have been conducted using the dataset built\non the Chinese novel <i>World of Plainness</i>. The results show that\nour proposed method reaches a new state-of-the-art performance with\nan identification accuracy of 82.5%, which outperforms the baseline\nusing manual features by 12%.\n"
      ],
      "doi": "10.21437/Interspeech.2021-609",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "zhou21f_interspeech": {
      "authors": [
        [
          "Xiao",
          "Zhou"
        ],
        [
          "Zhen-Hua",
          "Ling"
        ],
        [
          "Li-Rong",
          "Dai"
        ]
      ],
      "title": "UnitNet-Based Hybrid Speech Synthesis",
      "original": "1092",
      "page_count": 5,
      "order": 843,
      "p1": "4119",
      "pn": "4123",
      "abstract": [
        "This paper presents a hybrid speech synthesis method based on UnitNet,\na unified sequence-to-sequence (Seq2Seq) acoustic model for both statistical\nparametric speech synthesis (SPSS) and concatenative speech synthesis\n(CSS). This method combines CSS and SPSS approaches to synthesize different\nsegments in an utterance. Comparing with the Tacotron2 model for Seq2Seq\nspeech synthesis, UnitNet utilizes the phone boundaries of training\ndata and its decoder contains autoregressive structures at both phone\nand frame levels. This hierarchical architecture can not only extract\nembedding vectors for representing phone-sized units in the corpus\nbut also measure the dependency among consecutive units, which makes\nUnitNet capable of guiding the selection of phone-sized units for CSS.\nFurthermore, hybrid synthesis can be achieved by integrating the units\ngenerated by SPSS into the framework of CSS for the target phones without\nappropriate candidates in the corpus. Experimental results show that\nUnitNet can achieve comparable naturalness with Tacotron2 for SPSS\nand outperform our previous Tacotron-based method for CSS. Besides,\nthe naturalness and inference efficiency of SPSS can be further improved\nthrough hybrid synthesis.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1092",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "novitasari21_interspeech": {
      "authors": [
        [
          "Sashi",
          "Novitasari"
        ],
        [
          "Sakriani",
          "Sakti"
        ],
        [
          "Satoshi",
          "Nakamura"
        ]
      ],
      "title": "Dynamically Adaptive Machine Speech Chain Inference for TTS in Noisy Environment: Listen and Speak Louder",
      "original": "0946",
      "page_count": 5,
      "order": 844,
      "p1": "4124",
      "pn": "4128",
      "abstract": [
        "Although machine speech chains were originally proposed to mimic a\nclosed-loop human speech chain mechanism with auditory feedback, the\nexisting machine speech chains are only utilized as a semi-supervised\nlearning method that allows automatic speech recognition (ASR) and\ntext-to-speech synthesis systems (TTS) to support each other given\nunpaired data. During inference, however, ASR and TTS are still performed\nseparately. This paper focuses on machine speech chain inferences in\na noisy environment. In human communication, speakers tend to talk\nmore loudly in noisy environments, a phenomenon known as the Lombard\neffect. Simulating the Lombard effect, we implement a machine speech\nchain that enables TTS to speak louder in a noisy condition given auditory\nfeedback. The auditory feedback includes speech-to-noise ratio prediction\nand ASR loss as a speech intelligibility measurement. To the best of\nour knowledge, this is the first deep learning framework that mimics\nhuman speech perception and production behaviors in a noisy environment.\n"
      ],
      "doi": "10.21437/Interspeech.2021-946",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "zhang21ba_interspeech": {
      "authors": [
        [
          "Haozhe",
          "Zhang"
        ],
        [
          "Zhihua",
          "Huang"
        ],
        [
          "Zengqiang",
          "Shang"
        ],
        [
          "Pengyuan",
          "Zhang"
        ],
        [
          "Yonghong",
          "Yan"
        ]
      ],
      "title": "LinearSpeech: Parallel Text-to-Speech with Linear Complexity",
      "original": "1192",
      "page_count": 5,
      "order": 845,
      "p1": "4129",
      "pn": "4133",
      "abstract": [
        "Non-autoregressive text to speech models such as FastSpeech can synthesize\nspeech significantly faster than previous autoregressive models with\ncomparable quality. However, the memory and time complexity <i>O(N<SUP>2</SUP>)</i>\nof self-attention hinders FastSpeech from generating long sequences,\nwhere <i>N</i> is the length of mel-spectrograms. In this work, we\npropose LinearSpeech, an efficient parallel text-to-speech model with\nmemory and computational complexity <i>O(N)</i>. Firstly, we replace\nstandard attention modules in decoder of the model with linear attention\nmodules to reduce the time and memory cost. Secondly, we add a novel\npositional encoding to standard and linear attention modules, which\nenable the model to learn the order of input sequence and synthesizing\nlong mel-spectrograms. Furthermore, we use reversible residual layers\ninstead of the standard residuals, which reduce the memory consumption\nin training stage. In our experiments, LinearSpeech can be trained\nwith doubled batch size than FastSpeech with similar number of parameters.\nAt inference, LinearSpeech achieves more than 2.0&#215; inference speedup\non CPU when synthesizing mel-spectrograms longer than 3,500. And our\nmodel can synthesize 5.5&#215; longer mel-spectrograms than FastSpeech\nwhen running out of 12GB GPU memory. Our subjective listening test\nalso shows that the speech quality of LinearSpeech is comparable to\nFastSpeech.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1192",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "mansbach21_interspeech": {
      "authors": [
        [
          "Noa",
          "Mansbach"
        ],
        [
          "Evgeny Hershkovitch",
          "Neiterman"
        ],
        [
          "Amos",
          "Azaria"
        ]
      ],
      "title": "An Agent for Competing with Humans in a Deceptive Game Based on Vocal Cues",
      "original": "0083",
      "page_count": 5,
      "order": 846,
      "p1": "4134",
      "pn": "4138",
      "abstract": [
        "In this work we present the development of an autonomous agent capable\nof competing with humans in a deception-based game. The agent predicts\nwhether a given statement is true or false based on vocal cues. To\nthis end, we develop a game for collecting a large scale and high quality\nlabeled sound data-set in a controlled environment in English and Hebrew.\nWe develop a model that can detect deception based on vocal statements\nfrom the participants of the experiment, and show that the model is\nmore accurate than humans.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  We develop an agent\nthat uses the developed deception model and interacts with humans within\nour deceptive environment. We show that our agent significantly outperforms\na simple agent that does not use the deception model; that is, it wins\nsignificantly more games when played against human players. In addition,\nwe use our model to detect whether a statement will be perceived as\na lie or not by human subjects, based on its vocal cues.\n"
      ],
      "doi": "10.21437/Interspeech.2021-83",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "fakhry21_interspeech": {
      "authors": [
        [
          "Ahmed",
          "Fakhry"
        ],
        [
          "Xinyi",
          "Jiang"
        ],
        [
          "Jaclyn",
          "Xiao"
        ],
        [
          "Gunvant",
          "Chaudhari"
        ],
        [
          "Asriel",
          "Han"
        ]
      ],
      "title": "A Multi-Branch Deep Learning Network for Automated Detection of COVID-19",
      "original": "0378",
      "page_count": 5,
      "order": 847,
      "p1": "4139",
      "pn": "4143",
      "abstract": [
        "Fast and affordable solutions for COVID-19 testing are necessary to\ncontain the spread of the global pandemic and help relieve the burden\non medical facilities. Currently, limited testing locations and expensive\nequipment pose difficulties for individuals seeking testing, especially\nin low-resource settings. Researchers have successfully presented models\nfor detecting COVID-19 infection status using audio samples recorded\nin clinical settings, suggesting that audio-based Artificial Intelligence\nmodels can be used to identify COVID-19. Such models have the potential\nto be deployed on smartphones for fast, widespread, and low-resource\ntesting. However, while previous studies have trained models on cleaned\naudio samples collected mainly from clinical settings, audio samples\ncollected from average smartphones may yield suboptimal quality data\nthat is different from the clean data that models were trained on.\nThis discrepancy may add a bias that affects COVID-19 status predictions.\nTo tackle this issue, we propose a multi-branch deep learning network\nthat is trained and tested on crowdsourced data where most of the data\nhas not been manually processed and cleaned. Furthermore, the model\nachieves state-of-art results for the COUGHVID dataset. After breaking\ndown results for each category, we have shown an AUC of 0.99 for audio\nsamples with COVID-19 positive labels.\n"
      ],
      "doi": "10.21437/Interspeech.2021-378"
    },
    "ma21d_interspeech": {
      "authors": [
        [
          "Youxuan",
          "Ma"
        ],
        [
          "Zongze",
          "Ren"
        ],
        [
          "Shugong",
          "Xu"
        ]
      ],
      "title": "RW-Resnet: A Novel Speech Anti-Spoofing Model Using Raw Waveform",
      "original": "0438",
      "page_count": 5,
      "order": 848,
      "p1": "4144",
      "pn": "4148",
      "abstract": [
        "In recent years, synthetic speech generated by advanced text-to-speech\n(TTS) and voice conversion (VC) systems has caused great harms to automatic\nspeaker verification (ASV) systems, urging us to design a synthetic\nspeech detection system to protect ASV systems. In this paper, we propose\na new speech anti-spoofing model named ResWavegram-Resnet (RW-Resnet).\nThe model contains two parts, Conv1D Resblocks and backbone Resnet34.\nThe Conv1D Resblock is based on the Conv1D block with a residual connection.\nFor the first part, we use the raw waveform as input and feed it to\nthe stacked Conv1D Resblocks to get the ResWavegram. Compared with\ntraditional methods, ResWavegram keeps all the information from the\naudio signal and has a stronger ability in extracting features. For\nthe second part, the extracted features are fed to the backbone Resnet34\nfor the spoofed or bonafide decision. The ASVspoof2019 logical access\n(LA) corpus is used to evaluate our proposed RW-Resnet. Experimental\nresults show that the RW-Resnet achieves better performance than other\nstate-of-the-art anti-spoofing models, which illustrates its effectiveness\nin detecting synthetic speech attacks.\n"
      ],
      "doi": "10.21437/Interspeech.2021-438",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "dhamyal21_interspeech": {
      "authors": [
        [
          "Hira",
          "Dhamyal"
        ],
        [
          "Ayesha",
          "Ali"
        ],
        [
          "Ihsan Ayyub",
          "Qazi"
        ],
        [
          "Agha Ali",
          "Raza"
        ]
      ],
      "title": "Fake Audio Detection in Resource-Constrained Settings Using Microfeatures",
      "original": "0524",
      "page_count": 5,
      "order": 849,
      "p1": "4149",
      "pn": "4153",
      "abstract": [
        "Fake audio generation has undergone remarkable improvement with the\nadvancement in deep neural network models. This has made it increasingly\nimportant to develop lightweight yet robust mechanisms for detecting\nfake audios, especially for resource-constrained settings such as on\nedge devices and embedded controllers as well as with low-resource\nlanguages. In this paper, we analyze two <i>microfeatures</i>: Voicing\nOnset Time (VOT) and coarticulation, to classify bonafide and synthesized\naudios. Using the ASVSpoof2019 LA dataset, we find that on average,\nVOT is higher in synthesized speech compared to bonafide speech and\nexhibits higher variance for multiple occurrences of the same stop\nconsonants. Further, we observe that vowels in CVC form in bonafide\nspeech have greater F1/F2 movement compared to similarly constrained\nvowels in synthesized speech. We also analyse the predictive power\nof VOT and coarticulation for detecting bonafide and synthesized speech\nand achieve equal error rates of 25.2% using VOT, 39.3% using coarticulation,\nand 23.5% using a fusion of both models. This is the first study analysing\nVOT and coarticulation as features for fake audio detection. We suggest\nthese microfeatures as standalone features for speaker-dependent forensics,\nvoice-biometrics, and for rapid pre-screening of suspicious audios,\nand as additional features in bigger feature sets for computationally\nintensive classifiers.\n"
      ],
      "doi": "10.21437/Interspeech.2021-524",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "yan21c_interspeech": {
      "authors": [
        [
          "Tianhao",
          "Yan"
        ],
        [
          "Hao",
          "Meng"
        ],
        [
          "Emilia",
          "Parada-Cabaleiro"
        ],
        [
          "Shuo",
          "Liu"
        ],
        [
          "Meishu",
          "Song"
        ],
        [
          "Bj\u00f6rn W.",
          "Schuller"
        ]
      ],
      "title": "Coughing-Based Recognition of Covid-19 with Spatial Attentive ConvLSTM Recurrent Neural Networks",
      "original": "0630",
      "page_count": 5,
      "order": 850,
      "p1": "4154",
      "pn": "4158",
      "abstract": [
        "The rapid emergence of COVID-19 has become a major public health threat\naround the world. Although early detection is crucial to reduce its\nspread, the existing diagnostic methods are still insufficient in bringing\nthe pandemic under control. Thus, more sophisticated systems, able\nto easily identify the infection from a larger variety of symptoms,\nsuch as cough, are urgently needed. Deep learning models can indeed\nconvey numerous signal features relevant to fight against the disease;\nyet, the performance of state-of-the-art approaches is still severely\nrestricted by the feature information loss typically due to the high\nnumber of layers. To mitigate this phenomenon, identifying the most\nrelevant feature areas by drawing into attention mechanisms becomes\nessential. In this paper, we introduce Spatial Attentive ConvLSTM-RNN\n(SACRNN), a novel algorithm that is using Convolutional Long-Short\nTerm Memory Recurrent Neural Networks with embedded attention that\nhas the ability to identify the most valuable features. The promising\nresults achieved by the fusion between the proposed model and a conventional\nAttentive Convolutional Recurrent Neural Network, on the automatic\nrecognition of COVID-19 coughing (73.2% of Unweighted Average Recall)\nshow the great potential of the presented approach in developing efficient\nsolutions to defeat the pandemic.\n"
      ],
      "doi": "10.21437/Interspeech.2021-630",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "paul21b_interspeech": {
      "authors": [
        [
          "Soumava",
          "Paul"
        ],
        [
          "Gurunath Reddy",
          "M"
        ],
        [
          "K. Sreenivasa",
          "Rao"
        ],
        [
          "Partha Pratim",
          "Das"
        ]
      ],
      "title": "Knowledge Distillation for Singing Voice Detection",
      "original": "0636",
      "page_count": 5,
      "order": 851,
      "p1": "4159",
      "pn": "4163",
      "abstract": [
        "Singing Voice Detection (SVD) has been an active area of research in\nmusic information retrieval (MIR). Currently, two deep neural network-based\nmethods, one based on CNN and the other on RNN, exist in literature\nthat learn optimized features for the voice detection (VD) task and\nachieve state-of-the-art performance on common datasets. Both these\nmodels have a huge number of parameters (1.4M for CNN and 65.7K for\nRNN) and hence not suitable for deployment on devices like smartphones\nor embedded sensors with limited capacity in terms of memory and computation\npower. The most popular method to address this issue is known as knowledge\ndistillation in deep learning literature (in addition to model compression)\nwhere a large pre-trained network known as the teacher is used to train\na smaller student network. Given the wide applications of SVD in music\ninformation retrieval, to the best of our knowledge, model compression\nfor practical deployment has not yet been explored. In this paper,\nefforts have been made to investigate this issue using both conventional\nas well as ensemble knowledge distillation techniques.\n"
      ],
      "doi": "10.21437/Interspeech.2021-636",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "takeda21_interspeech": {
      "authors": [
        [
          "Ryu",
          "Takeda"
        ],
        [
          "Kazunori",
          "Komatani"
        ]
      ],
      "title": "Age Estimation with Speech-Age Model for Heterogeneous Speech Datasets",
      "original": "0861",
      "page_count": 5,
      "order": 852,
      "p1": "4164",
      "pn": "4168",
      "abstract": [
        "This paper describes an age estimation method from speech signals for\nheterogeneous datasets. Although previous studies in the speech field\nevaluate age prediction models with held-out testing data within the\nsame dataset recorded in a consistent setting, such evaluation does\nnot measure real performance. The difficulty of heterogeneous datasets\nis overfitting caused by the corpus-specific properties: transfer function\nof the recording environment and distributions of age and speaker.\nWe propose a speech-age model and its integration with sequence neural\nnetworks (NNs). The speech-age model represents the ambiguity of age\nas a probability distribution, which also virtually extends the limited\nrange of age distribution of each corpus. A Bayesian generative model\nsuccessfully integrates the speech-age model and the NNs. We also applied\nmean normalization technique to cope with the transfer function problem.\nExperiments showed that our proposed method outperformed the baseline\nneural classifier for completely open test sets in the age distribution\nand recording setting.\n"
      ],
      "doi": "10.21437/Interspeech.2021-861",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "teh21_interspeech": {
      "authors": [
        [
          "Kah Kuan",
          "Teh"
        ],
        [
          "Huy Dat",
          "Tran"
        ]
      ],
      "title": "Open-Set Audio Classification with Limited Training Resources Based on Augmentation Enhanced Variational Auto-Encoder GAN with Detection-Classification Joint Training",
      "original": "1142",
      "page_count": 5,
      "order": 853,
      "p1": "4169",
      "pn": "4173",
      "abstract": [
        "In this paper, we propose a novel method to address practical problems\nwhen deploying audio classification systems in operations that are\nthe presence of unseen sound classes (open-set) and the limitation\nof training resources. To solve it, a novel method which embeds variational\nauto-encoder (VAE), data augmentation and detection-classification\njoint training into conventional GAN networks is proposed. The VAE\ninput to GAN-generator helps to generate realistic outlier samples\nwhich are not too far from in-distribution class and hence improve\nthe open-set discrimination capabilities of classifiers. Next, the\naugmentation enhanced GAN scheme developed in our previous work [4]\nfor close-set audio classification, will help to address the limited\ntraining resources by in cooperating the physical data augmentation\nto work together with traditional GAN produced samples to prevent overfitting\nand improve the optimization convergences. The detection-classification\njoint training further steps on advantages of VAE and Augmentation\nGAN to further improving the performances of detection and classification\ntasks. The experiments carried out on Google Speech Command database\nshow great improvements of open-set classification accuracy from 62.41%\nto 88.29% when using only 10% amount of training data.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1142",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "fukumori21_interspeech": {
      "authors": [
        [
          "Takahiro",
          "Fukumori"
        ]
      ],
      "title": "Deep Spectral-Cepstral Fusion for Shouted and Normal Speech Classification",
      "original": "1245",
      "page_count": 5,
      "order": 854,
      "p1": "4174",
      "pn": "4178",
      "abstract": [
        "Discrimination between shouted and normal speech is crucial in audio\nsurveillance and monitoring. Although deep neural networks are used\nin recent methods, traditional low-level speech features are applied,\nsuch as mel-frequency cepstral coefficients and the mel spectrum. This\npaper presents a deep spectral-cepstral fusion approach that learns\ndescriptive features for target classification from high-dimensional\nspectrograms and cepstrograms. We compare the following three types\nof architectures as base networks: convolutional neural networks (CNNs),\ngated recurrent unit (GRU) networks, and their combination (CNN-GRU).\nUsing a corpus comprising real shouts and speech, we present a comprehensive\ncomparison with conventional methods to verify the effectiveness of\nthe proposed feature learning method. The results of experiments conducted\nin various noisy environments demonstrate that the CNN-GRU based on\nour spectral-cepstral features achieves better classification performance\nthan single feature-based networks. This finding suggests the effectiveness\nof using high-dimensional sources for speech-type recognition in sound\nevent detection.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1245",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "baghel21_interspeech": {
      "authors": [
        [
          "Shikha",
          "Baghel"
        ],
        [
          "Mrinmoy",
          "Bhattacharjee"
        ],
        [
          "S.R. Mahadeva",
          "Prasanna"
        ],
        [
          "Prithwijit",
          "Guha"
        ]
      ],
      "title": "Automatic Detection of Shouted Speech Segments in Indian News Debates",
      "original": "1592",
      "page_count": 5,
      "order": 855,
      "p1": "4179",
      "pn": "4183",
      "abstract": [
        "Shouted speech detection is an essential pre-processing step in conventional\nspeech processing systems such as speech and speaker recognition, speaker\ndiarization, and others. Excitation source plays an important role\nin shouted speech production. This work explores feature computed from\nthe Integrated Linear Prediction Residual (ILPR) signal for shouted\nspeech detection in Indian news debates. The log spectrogram of ILPR\nsignal provides time-frequency characteristics of excitation source\nsignal. The proposed shouted speech detection system is deep network\nwith CNN-based autoencoder and attention-based classifier sub-modules.\nThe Autoencoder sub-network aids the classifier in learning discriminative\ndeep embeddings for better classification. The proposed classifier\nis equipped with attention mechanism and Bidirectional Gated Recurrent\nUnits. Classification results show that the proposed system with excitation\nfeature performs better than baseline log spectrogram computed from\nthe pre-emphasized speech signal. A score-level fusion of the classifiers\ntrained on the source feature and the baseline feature provides the\nbest performance. The performance of the proposed shouted speech detection\nis also evaluated at various speech segment durations.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1592",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "gao21c_interspeech": {
      "authors": [
        [
          "Yang",
          "Gao"
        ],
        [
          "Tyler",
          "Vuong"
        ],
        [
          "Mahsa",
          "Elyasi"
        ],
        [
          "Gaurav",
          "Bharaj"
        ],
        [
          "Rita",
          "Singh"
        ]
      ],
      "title": "Generalized Spoofing Detection Inspired from Audio Generation Artifacts",
      "original": "1705",
      "page_count": 5,
      "order": 856,
      "p1": "4184",
      "pn": "4188",
      "abstract": [
        "State-of-the-art methods for audio generation suffer from fingerprint\nartifacts and repeated inconsistencies across temporal and spectral\ndomains. Such artifacts could be well captured by the frequency domain\nanalysis over the spectrogram. Thus, we propose a novel use of long-range\nspectro-temporal modulation feature &#8212; 2D DCT over log-Mel spectrogram\nfor the audio deepfake detection. We show that this feature works better\nthan log-Mel spectrogram, CQCC, MFCC, as a suitable candidate to capture\nsuch artifacts. We employ spectrum augmentation and feature normalization\nto decrease overfitting and bridge the gap between training and test\ndataset along with this novel feature introduction. We developed a\nCNN-based baseline that achieved a 0.0849 t-DCF and outperformed the\npreviously top single systems reported in the ASVspoof 2019 challenge.\nFinally, by combining our baseline with our proposed 2D DCT spectro-temporal\nfeature, we decrease the t-DCF score down by 14% to 0.0737, making\nit a state-of-the-art system for spoofing detection. Furthermore, we\nevaluate our model using two external datasets, showing the proposed\nfeature&#8217;s generalization ability. We also provide analysis and\nablation studies for our proposed feature and results.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1705",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "chen21t_interspeech": {
      "authors": [
        [
          "Weiguang",
          "Chen"
        ],
        [
          "Van Tung",
          "Pham"
        ],
        [
          "Eng Siong",
          "Chng"
        ],
        [
          "Xionghu",
          "Zhong"
        ]
      ],
      "title": "Overlapped Speech Detection Based on Spectral and Spatial Feature Fusion",
      "original": "2138",
      "page_count": 5,
      "order": 857,
      "p1": "4189",
      "pn": "4193",
      "abstract": [
        "Overlapped speech is widely present in conversations and can cause\nsignificant performance degradation on speech processing such as diarization,\nenhancement, and recognition. Detection of overlapped speech, in particular\nwhen the speakers are in the far-field, is a challenging task as the\noverlapped part is usually short, and heavy reverberation and noise\nmay present in the conversation scenario. Existing solutions overwhelmingly\nrely on spectral features extracted from single microphone signal to\nperform the detection. In this paper, we propose a novel detection\napproach which is able to use a microphone array and fuse the spatial\nand spectral features extracted from multi-channel array signal. Two\ncategories of spatial features, directional statistics which are projected\nto spherical location grids and generalized cross-correlation function\nbased on phase transform (GCC-PHAT), are considered to model the speaker&#8217;s\nspatial characteristic. Such spatial features are then fused with the\nspectral features to detect the overlapped speech by using a Gated\nMultimodal Unit (GMU). The performance of the proposed approach is\nstudied under AMI and CHiME-6 corpora. Experimental results show that\nthe proposed feature fusion approach achieves better performance than\nmethods using spectral features only.\n"
      ],
      "doi": "10.21437/Interspeech.2021-2138",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "abdullah21_interspeech": {
      "authors": [
        [
          "Badr M.",
          "Abdullah"
        ],
        [
          "Marius",
          "Mosbach"
        ],
        [
          "Iuliia",
          "Zaitova"
        ],
        [
          "Bernd",
          "M\u00f6bius"
        ],
        [
          "Dietrich",
          "Klakow"
        ]
      ],
      "title": "Do Acoustic Word Embeddings Capture Phonological Similarity? An Empirical Study",
      "original": "0678",
      "page_count": 5,
      "order": 858,
      "p1": "4194",
      "pn": "4198",
      "abstract": [
        "Several variants of deep neural networks have been successfully employed\nfor building parametric models that project variable-duration spoken\nword segments onto fixed-size vector representations, or acoustic word\nembeddings (AWEs). However, it remains unclear to what degree we can\nrely on the distance in the emerging AWE space as an estimate of word-form\nsimilarity. In this paper, we ask: does the distance in the acoustic\nembedding space correlate with phonological dissimilarity? To answer\nthis question, we empirically investigate the performance of supervised\napproaches for AWEs with different neural architectures and learning\nobjectives. We train AWE models in controlled settings for two languages\n(German and Czech) and evaluate the embeddings on two tasks: word discrimination\nand phonological similarity. Our experiments show that (1) the distance\nin the embedding space in the best cases only moderately correlates\nwith phonological distance, and (2) improving the performance on the\nword discrimination task does not necessarily yield models that better\nreflect word phonological similarity. Our findings highlight the necessity\nto rethink the current intrinsic evaluations for AWEs.\n"
      ],
      "doi": "10.21437/Interspeech.2021-678",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "gao21d_interspeech": {
      "authors": [
        [
          "Zheng",
          "Gao"
        ],
        [
          "Radhika",
          "Arava"
        ],
        [
          "Qian",
          "Hu"
        ],
        [
          "Xibin",
          "Gao"
        ],
        [
          "Thahir",
          "Mohamed"
        ],
        [
          "Wei",
          "Xiao"
        ],
        [
          "Mohamed",
          "AbdelHady"
        ]
      ],
      "title": "Paraphrase Label Alignment for Voice Application Retrieval in Spoken Language Understanding",
      "original": "0097",
      "page_count": 5,
      "order": 859,
      "p1": "4199",
      "pn": "4203",
      "abstract": [
        "Spoken language understanding (SLU) smart assistants such as Amazon\nAlexa host hundreds of thousands of voice applications (skills) to\ndelight end-users and fulfill their utterance requests. Sometimes utterances\nfail to be claimed by smart assistants due to system problems such\nas model incapability or routing errors. The failure may lead to customer\nfrustration, dialog termination and eventually cause customer churn.\nTo avoid this, we design a skill retrieval system as a downstream service\nto suggest fallback skills to unclaimed utterances. If the suggested\nskill satisfies customer intent, the conversation will be recovered\nwith the assistant. For the sake of smooth customer experience, we\nonly present the most relevant skill to customers, resulting in partial\nobservation problem which constrains retrieval model training. To solve\nthis problem, we propose a two-step approach to automatically align\nclaimed utterance labels to unclaimed utterances. Extensive experiments\non two real-world datasets demonstrate that our proposed model significantly\noutperforms a number of strong alternatives.\n"
      ],
      "doi": "10.21437/Interspeech.2021-97",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "rikhye21_interspeech": {
      "authors": [
        [
          "Rajeev",
          "Rikhye"
        ],
        [
          "Quan",
          "Wang"
        ],
        [
          "Qiao",
          "Liang"
        ],
        [
          "Yanzhang",
          "He"
        ],
        [
          "Ding",
          "Zhao"
        ],
        [
          "Yiteng",
          "Huang"
        ],
        [
          "Arun",
          "Narayanan"
        ],
        [
          "Ian",
          "McGraw"
        ]
      ],
      "title": "Personalized Keyphrase Detection Using Speaker and Environment Information",
      "original": "0204",
      "page_count": 5,
      "order": 860,
      "p1": "4204",
      "pn": "4208",
      "abstract": [
        "In this paper, we introduce a streaming keyphrase detection system\nthat can be easily customized to accurately detect any phrase composed\nof words from a large vocabulary. The system is implemented with an\nend-to-end trained automatic speech recognition (ASR) model and a text-independent\nspeaker verification model. To address the challenge of detecting these\nkeyphrases under various noisy conditions, a speaker separation model\nis added to the feature frontend of the speaker verification model,\nand an adaptive noise cancellation (ANC) algorithm is included to exploit\ncross-microphone noise coherence. Our experiments show that the text-independent\nspeaker verification model largely reduces the false triggering rate\nof the keyphrase detection, while the speaker separation model and\nadaptive noise cancellation largely reduce false rejections.\n"
      ],
      "doi": "10.21437/Interspeech.2021-204",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "garg21_interspeech": {
      "authors": [
        [
          "Vineet",
          "Garg"
        ],
        [
          "Wonil",
          "Chang"
        ],
        [
          "Siddharth",
          "Sigtia"
        ],
        [
          "Saurabh",
          "Adya"
        ],
        [
          "Pramod",
          "Simha"
        ],
        [
          "Pranay",
          "Dighe"
        ],
        [
          "Chandra",
          "Dhir"
        ]
      ],
      "title": "Streaming Transformer for Hardware Efficient Voice Trigger Detection and False Trigger Mitigation",
      "original": "1428",
      "page_count": 5,
      "order": 861,
      "p1": "4209",
      "pn": "4213",
      "abstract": [
        "We present a unified and hardware efficient architecture for two stage\nvoice trigger detection (VTD) and false trigger mitigation (FTM) tasks.\nTwo stage VTD systems of voice assistants can get falsely activated\nto audio segments acoustically similar to the trigger phrase of interest.\nFTM systems cancel such activations by using post trigger audio context.\nTraditional FTM systems rely on automatic speech recognition lattices\nwhich are computationally expensive to obtain on device. We propose\na streaming transformer (TF) encoder architecture, which progressively\nprocesses incoming audio chunks and maintains audio context to perform\nboth VTD and FTM tasks using only acoustic features. The proposed joint\nmodel yields an average 18% relative reduction in false reject rate\n(FRR) for the VTD task at a given false alarm rate. Moreover, our model\nsuppresses 95% of the false triggers with an additional one second\nof post-trigger audio. Finally, on-device measurements show 32% reduction\nin runtime memory and 56% reduction in inference time compared to non-streaming\nversion of the model.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1428",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "mazumder21_interspeech": {
      "authors": [
        [
          "Mark",
          "Mazumder"
        ],
        [
          "Colby",
          "Banbury"
        ],
        [
          "Josh",
          "Meyer"
        ],
        [
          "Pete",
          "Warden"
        ],
        [
          "Vijay Janapa",
          "Reddi"
        ]
      ],
      "title": "Few-Shot Keyword Spotting in Any Language",
      "original": "1966",
      "page_count": 5,
      "order": 862,
      "p1": "4214",
      "pn": "4218",
      "abstract": [
        "We introduce a few-shot transfer learning method for keyword spotting\nin any language. Leveraging open speech corpora in nine languages,\nwe automate the extraction of a large multilingual keyword bank and\nuse it to train an embedding model. With just five training examples,\nwe fine-tune the embedding model for keyword spotting and achieve an\naverage F<SUB>1</SUB> score of 0.75 on keyword classification for 180\nnew keywords unseen by the embedding model in these nine languages.\nThis embedding model also generalizes to new languages. We achieve\nan average F<SUB>1</SUB> score of 0.65 on 5-shot models for 260 keywords\nsampled across 13 new languages unseen by the embedding model. We investigate\nstreaming accuracy for our 5-shot models in two contexts: keyword spotting\nand keyword search. Across 440 keywords in 22 languages, we achieve\nan average streaming keyword spotting accuracy of 87.4% with a false\nacceptance rate of 4.3%, and observe promising initial results on keyword\nsearch.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1966",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "wang21da_interspeech": {
      "authors": [
        [
          "Li",
          "Wang"
        ],
        [
          "Rongzhi",
          "Gu"
        ],
        [
          "Nuo",
          "Chen"
        ],
        [
          "Yuexian",
          "Zou"
        ]
      ],
      "title": "Text Anchor Based Metric Learning for Small-Footprint Keyword Spotting",
      "original": "0136",
      "page_count": 5,
      "order": 863,
      "p1": "4219",
      "pn": "4223",
      "abstract": [
        "Keyword Spotting (KWS) remains challenging to achieve the trade-off\nbetween small footprint and high accuracy. Recently proposed metric\nlearning approaches improved the generalizability of models for the\nKWS task, and 1D-CNN based KWS models have achieved the state-of-the-arts\n(SOTA) in terms of model size. However, for metric learning, due to\ndata limitations, the speech anchor is highly susceptible to the acoustic\nenvironment and speakers. Also, we note that the 1D-CNN models have\nlimited capability to capture long-term temporal acoustic features.\nTo address the above problems, we propose to utilize text anchors to\nimprove the stability of anchors. Furthermore, a new type of model\n(LG-Net) is exquisitely designed to promote long-short term acoustic\nfeature modeling based on 1D-CNN and self-attention. Experiments are\nconducted on Google Speech Commands Dataset version 1 (GSCDv1) and\n2 (GSCDv2). The results demonstrate that the proposed text anchor based\nmetric learning method shows consistent improvements over speech anchor\non representative CNN-based models. Moreover, our LG-Net model achieves\nSOTA accuracy of 97.67% and 96.79% on two datasets, respectively. It\nis encouraged to see that our lighter LG-Net with only 74k parameters\nobtains 96.82% KWS accuracy on the GSCDv1 and 95.77% KWS accuracy on\nthe GSCDv2.\n"
      ],
      "doi": "10.21437/Interspeech.2021-136",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "chen21u_interspeech": {
      "authors": [
        [
          "Yangbin",
          "Chen"
        ],
        [
          "Tom",
          "Ko"
        ],
        [
          "Jianping",
          "Wang"
        ]
      ],
      "title": "A Meta-Learning Approach for User-Defined Spoken Term Classification with Varying Classes and Examples",
      "original": "0147",
      "page_count": 5,
      "order": 864,
      "p1": "4224",
      "pn": "4228",
      "abstract": [
        "Recently we formulated a user-defined spoken term classification task\nas a few-shot learning task and tackled the task using Model-Agnostic\nMeta-Learning (MAML) algorithm. Our results show that the meta-learning\napproach performs much better than conventional supervised learning\nand transfer learning in the task, especially with limited training\ndata. In this paper, we extend our work by addressing a more practical\nproblem in the user-defined scenario where users can define any number\nof spoken terms and provide any number of enrollment audio examples\nfor each spoken term. From the perspective of few-shot learning, this\nis an N-way, K-shot problem with varying N and K. In our work, we relax\nthe values of N and K of each meta-task during training instead of\nassigning fixed values to them, which differs from what most meta-learning\nalgorithms do. We adopt a metric-based meta-learning algorithm named\nPrototypical Networks (ProtoNet) as it avoids exhaustive fine-tuning\nwhen N varies. Furthermore, we use the Max-Mahalanobis Center (MMC)\nloss as an effective regularizer to address the problem of ProtoNet\nunder the condition of varying K. Experiments on the Google Speech\nCommands dataset demonstrate that our proposed method outperforms the\nconventional N-way, K-shot setting in most testing tasks.\n"
      ],
      "doi": "10.21437/Interspeech.2021-147",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation:14 Special Sessions"
    },
    "lee21g_interspeech": {
      "authors": [
        [
          "Dongyub",
          "Lee"
        ],
        [
          "Byeongil",
          "Ko"
        ],
        [
          "Myeong Cheol",
          "Shin"
        ],
        [
          "Taesun",
          "Whang"
        ],
        [
          "Daniel",
          "Lee"
        ],
        [
          "Eunhwa",
          "Kim"
        ],
        [
          "Eunggyun",
          "Kim"
        ],
        [
          "Jaechoon",
          "Jo"
        ]
      ],
      "title": "Auxiliary Sequence Labeling Tasks for Disfluency Detection",
      "original": "0400",
      "page_count": 5,
      "order": 865,
      "p1": "4229",
      "pn": "4233",
      "abstract": [
        "Detecting disfluencies in spontaneous speech is an important preprocessing\nstep in natural language processing and speech recognition applications.\nExisting works for disfluency detection have focused on designing a\nsingle objective only for disfluency detection, while auxiliary objectives\nutilizing linguistic information of a word such as named entity or\npart-of-speech information can be effective. In this paper, we focus\non detecting disfluencies on spoken transcripts and propose a method\nutilizing named entity recognition (NER) and part-of-speech (POS) as\nauxiliary sequence labeling (SL) tasks for disfluency detection. First,\nwe investigate cases that utilizing linguistic information of a word\ncan prevent mispredicting important words and can be helpful for the\ncorrect detection of disfluencies. Second, we show that training a\ndisfluency detection model with auxiliary SL tasks can improve its\nF-score in disfluency detection. Then, we analyze which auxiliary SL\ntasks are influential depending on baseline models. Experimental results\non the widely used English Switchboard dataset show that our method\noutperforms the previous state-of-the-art in disfluency detection.\n"
      ],
      "doi": "10.21437/Interspeech.2021-400",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "zhou21g_interspeech": {
      "authors": [
        [
          "Hang",
          "Zhou"
        ],
        [
          "Wenchao",
          "Hu"
        ],
        [
          "Yu Ting",
          "Yeung"
        ],
        [
          "Xiao",
          "Chen"
        ]
      ],
      "title": "Energy-Friendly Keyword Spotting System Using Add-Based Convolution",
      "original": "0458",
      "page_count": 5,
      "order": 866,
      "p1": "4234",
      "pn": "4238",
      "abstract": [
        "Wake-up keyword of a keyword spotting (KWS) system represents brand\nname of a smart device. Performance of KWS is also crucial for modern\nspeech based human-device interaction. An on-device KWS with both high\naccuracy and low power consumption is desired. We propose a KWS with\nadd-based convolution layers, namely Add TC-ResNet. Add-based convolution\npaves a new way to reduce power consumption of KWS system, as addition\nis more energy efficient than multiplication at hardware level. On\nGoogle Speech Commands dataset V2, Add TC-ResNet achieves an accuracy\nof 97.1%, with 99% of multiplication operations are replaced by addition\noperations. The result is competitive to a state-of-the-art fully multiplication-based\nTC-ResNet KWS. We also investigate knowledge distillation and a mixed\naddition-multiplication design for the proposed KWS, which leads to\nfurther performance improvement.\n"
      ],
      "doi": "10.21437/Interspeech.2021-458",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "jia21b_interspeech": {
      "authors": [
        [
          "Yan",
          "Jia"
        ],
        [
          "Xingming",
          "Wang"
        ],
        [
          "Xiaoyi",
          "Qin"
        ],
        [
          "Yinping",
          "Zhang"
        ],
        [
          "Xuyang",
          "Wang"
        ],
        [
          "Junjie",
          "Wang"
        ],
        [
          "Dong",
          "Zhang"
        ],
        [
          "Ming",
          "Li"
        ]
      ],
      "title": "The 2020 Personalized Voice Trigger Challenge: Open Datasets, Evaluation Metrics, Baseline System and Results",
      "original": "0602",
      "page_count": 5,
      "order": 867,
      "p1": "4239",
      "pn": "4243",
      "abstract": [
        "The 2020 Personalized Voice Trigger Challenge (PVTC2020) addresses\ntwo different research problems in a unified setup: joint wake-up word\ndetection with speaker verification on close-talking single microphone\ndata and far-field multi-channel microphone array data. Specially,\nthe second task poses an additional cross-channel matching challenge\non top of the far-field condition. To simulate the real-life application\nscenario, the enrollment utterances are recorded from close-talking\ncell-phone only, while the test utterances are recorded from both the\nclose-talking cell-phone and the far-field microphone arrays. This\npaper introduces our challenge setup and the released database as well\nas the evaluation metrics. In addition, we present a sequential two\nstage end-to-end neural network baseline system trained with the proposed\ndatabase for speaker-dependent wake-up word detection. Results show\nthat state-of-the-art personalized voice trigger methods are still\nbased on the two stage design, however, this benchmark database could\nalso be used to evaluate multi-task joint learning methods. The official\nwebsite, the open-source baseline system and results of submitted systems\nhave been released.\n"
      ],
      "doi": "10.21437/Interspeech.2021-602",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "wang21ea_interspeech": {
      "authors": [
        [
          "Jingsong",
          "Wang"
        ],
        [
          "Yuxuan",
          "He"
        ],
        [
          "Chunyu",
          "Zhao"
        ],
        [
          "Qijie",
          "Shao"
        ],
        [
          "Wei-Wei",
          "Tu"
        ],
        [
          "Tom",
          "Ko"
        ],
        [
          "Hung-yi",
          "Lee"
        ],
        [
          "Lei",
          "Xie"
        ]
      ],
      "title": "Auto-KWS 2021 Challenge: Task, Datasets, and Baselines",
      "original": "0817",
      "page_count": 5,
      "order": 868,
      "p1": "4244",
      "pn": "4248",
      "abstract": [
        "Auto-KWS 2021 challenge calls for automated machine learning (AutoML)\nsolutions to automate the process of applying machine learning to a\ncustomized keyword spotting task. Compared with other keyword spotting\ntasks, Auto-KWS challenge has the following three characteristics:\n1) The challenge focuses on the problem of customized keyword spotting,\nwhere the target device can only be awakened by an enrolled speaker\nwith his/her specified keyword. The speaker can use any language and\naccent to define his keyword. 2) All data of the challenge is recorded\nin realistic environment to simulate different user scenarios. 3) Auto-KWS\nis a &#8220;code competition&#8221;, where participants need to submit\nAutoML solutions, then the platform automatically runs the enrollment\nand prediction steps with the submitted code. This challenge aims at\npromoting the development of a more personalized and flexible keyword\nspotting system. Two baseline systems are provided to all participants\nas references.\n"
      ],
      "doi": "10.21437/Interspeech.2021-817",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation:14 Special Sessions"
    },
    "berg21_interspeech": {
      "authors": [
        [
          "Axel",
          "Berg"
        ],
        [
          "Mark",
          "O\u2019Connor"
        ],
        [
          "Miguel Tairum",
          "Cruz"
        ]
      ],
      "title": "Keyword Transformer: A Self-Attention Model for Keyword Spotting",
      "original": "1286",
      "page_count": 5,
      "order": 869,
      "p1": "4249",
      "pn": "4253",
      "abstract": [
        "The Transformer architecture has been successful across many domains,\nincluding natural language processing, computer vision and speech recognition.\nIn keyword spotting, self-attention has primarily been used on top\nof convolutional or recurrent encoders. We investigate a range of ways\nto adapt the Transformer architecture to keyword spotting and introduce\nthe Keyword Transformer (KWT), a fully self-attentional architecture\nthat exceeds state-of-the-art performance across multiple tasks without\nany pre-training or additional data. Surprisingly, this simple architecture\noutperforms more complex models that mix convolutional, recurrent and\nattentive layers. KWT can be used as a drop-in replacement for these\nmodels, setting two new benchmark records on the Google Speech Commands\ndataset with 98.6% and 97.7% accuracy on the 12 and 35-command tasks\nrespectively.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1286",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "awasthi21_interspeech": {
      "authors": [
        [
          "Abhijeet",
          "Awasthi"
        ],
        [
          "Kevin",
          "Kilgour"
        ],
        [
          "Hassan",
          "Rom"
        ]
      ],
      "title": "Teaching Keyword Spotters to Spot New Keywords with Limited Examples",
      "original": "1395",
      "page_count": 5,
      "order": 870,
      "p1": "4254",
      "pn": "4258",
      "abstract": [
        "Learning to recognize new keywords with just a few examples is essential\nfor personalizing keyword spotting (KWS) models to a user&#8217;s choice\nof keywords. However, modern KWS models are typically trained on large\ndatasets and restricted to a small vocabulary of keywords, limiting\ntheir transferability to a broad range of unseen keywords. Towards\neasily customizable KWS models, we present KeySEM (Keyword Speech EMbedding),\na speech embedding model pre-trained on the task of recognizing a large\nnumber of keywords. Speech representations offered by KeySEM are highly\neffective for learning new keywords from a limited number of examples.\nComparisons with a diverse range of related work across several datasets\nshow that our method achieves consistently superior performance with\nfewer training examples. Although KeySEM was pre-trained only on English\nutterances, the performance gains also extend to datasets from four\nother languages indicating that KeySEM learns useful representations\nwell aligned with the task of keyword spotting. Finally, we demonstrate\nKeySEM&#8217;s ability to learn new keywords sequentially without requiring\nto re-train on previously learned keywords. Our experimental observations\nsuggest that KeySEM is well suited to on-device environments where\npost-deployment learning and ease of customization are often desirable.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1395",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "wang21fa_interspeech": {
      "authors": [
        [
          "Xin",
          "Wang"
        ],
        [
          "Junichi",
          "Yamagishi"
        ]
      ],
      "title": "A Comparative Study on Recent Neural Spoofing Countermeasures for Synthetic Speech Detection",
      "original": "0702",
      "page_count": 5,
      "order": 871,
      "p1": "4259",
      "pn": "4263",
      "abstract": [
        "A great deal of recent research effort on speech spoofing countermeasures\nhas been invested into back-end neural networks and training criteria.\nWe contribute to this effort with a comparative perspective in this\nstudy. Our comparison of countermeasure models on the ASVspoof 2019\nlogical access scenario takes into account common strategies to deal\nwith input trials of varied length, recently proposed margin-based\ntraining criteria, and widely used front ends. We also measured intra-model\ndifferences through multiple training-evaluation rounds with random\ninitialization. Our statistical analysis demonstrates that the performance\nof the same model may be statistically significantly different when\njust changing the random initial seed. We thus recommend similar statistical\nanalysis or reporting results of multiple runs for further research\non the database. Despite the intra-model differences, we observed a\nfew promising techniques, including average pooling, to efficiently\nprocess varied-length inputs and a new hyper-parameter-free loss function.\nThe two techniques led to the best single model in our experiment,\nwhich achieved an equal error rate of 1.92% and was significantly different\nin statistical sense from most of the other experimental models.\n"
      ],
      "doi": "10.21437/Interspeech.2021-702",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "zhang21ca_interspeech": {
      "authors": [
        [
          "Lin",
          "Zhang"
        ],
        [
          "Xin",
          "Wang"
        ],
        [
          "Erica",
          "Cooper"
        ],
        [
          "Junichi",
          "Yamagishi"
        ],
        [
          "Jose",
          "Patino"
        ],
        [
          "Nicholas",
          "Evans"
        ]
      ],
      "title": "An Initial Investigation for Detecting Partially Spoofed Audio",
      "original": "0738",
      "page_count": 5,
      "order": 872,
      "p1": "4264",
      "pn": "4268",
      "abstract": [
        "All existing databases of spoofed speech contain attack data that is\nspoofed in its entirety. In practice, it is entirely plausible that\nsuccessful attacks can be mounted with utterances that are only partially\nspoofed. By definition, partially-spoofed utterances contain a mix\nof both spoofed and bona fide segments, which will likely degrade the\nperformance of countermeasures trained with entirely spoofed utterances.\nThis hypothesis raises the obvious question: <i>&#8216;Can we detect\npartially-spoofed audio?&#8217;</i> This paper introduces a new database\nof partially-spoofed data, named PartialSpoof, to help address this\nquestion. This new database enables us to investigate and compare the\nperformance of countermeasures on both utterance- and segmental- level\nlabels. Experimental results using the utterance-level labels reveal\nthat the reliability of countermeasures trained to detect fully-spoofed\ndata is found to degrade substantially when tested with partially-spoofed\ndata, whereas training on partially-spoofed data performs reliably\nin the case of both fully- and partially-spoofed utterances. Additional\nexperiments using segmental-level labels show that spotting injected\nspoofed segments included in an utterance is a much more challenging\ntask even if the latest countermeasure models are used.\n"
      ],
      "doi": "10.21437/Interspeech.2021-738",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "xie21_interspeech": {
      "authors": [
        [
          "Yang",
          "Xie"
        ],
        [
          "Zhenchuan",
          "Zhang"
        ],
        [
          "Yingchun",
          "Yang"
        ]
      ],
      "title": "Siamese Network with wav2vec Feature for Spoofing Speech Detection",
      "original": "0847",
      "page_count": 5,
      "order": 873,
      "p1": "4269",
      "pn": "4273",
      "abstract": [
        "Automatic speaker verification is vulnerable to spoofing attacks with\nsynthesized or converted speech. Although high-performance anti-spoofing\ncountermeasures can achieve high accuracy when the training and testing\nspoofing attack examples are similarly distributed, their performance\ndegrades significantly when confronted with out-of-distribution spoofing\nspeech, which is created by increasingly advanced unseen speech synthesis\nand voice conversion methods. Since it is unrealistic to collect enough\nlabeled data from each new spoofing attack method, we argue that addressing\nthe problem of out-of-distribution generalization for spoofing speech\ndetection is essential. In this work, we propose a two-phase representation\nlearning system based on a Siamese network for spoofing speech detection\ntasks. During the representation learning phase, an embedding Siamese\nneural network is trained with the wav2vec features to distinguish\nwhether the speech samples in a pair belong to the same category. The\nproposed system decreases the equal error rate from the state-of-the-art\nresult of 4.07% to 1.15% on the ASVspoof 2019 evaluation set.\n"
      ],
      "doi": "10.21437/Interspeech.2021-847",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "cheng21b_interspeech": {
      "authors": [
        [
          "Xingliang",
          "Cheng"
        ],
        [
          "Mingxing",
          "Xu"
        ],
        [
          "Thomas Fang",
          "Zheng"
        ]
      ],
      "title": "Cross-Database Replay Detection in Terminal-Dependent Speaker Verification",
      "original": "0960",
      "page_count": 5,
      "order": 874,
      "p1": "4274",
      "pn": "4278",
      "abstract": [
        "The vulnerability of automatic speaker verification (ASV) systems against\nreplay attacks becomes a severe problem. Although various methods have\nbeen proposed for replay detection, the generalization capability is\nstill limited. For instance, a detection model trained on one database\nmay fully fail when tested on another database. In this paper, we adopt\nthe one-class learning technology to address the cross-database problem.\nDifferent from conventional two-class models that discriminate genuine\nspeeches from replay attacks, the one-class model focuses on the within-class\nvariance of genuine speeches, which is naturally robust to unseen attacks.\nIn this study, we choose the Gaussian mixture model (GMM) as the one-class\nmodel and design two utterance-level features which reduce the uncertainties\nof genuine class while still be distinguishable from non-genuine class.\nExperiments conducted on three public replay datasets show that, compared\nto the state-of-the-art methods, the proposed method demonstrates promising\ngeneralization capability under cross-database scenarios.\n"
      ],
      "doi": "10.21437/Interspeech.2021-960",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "zhang21da_interspeech": {
      "authors": [
        [
          "Yuxiang",
          "Zhang"
        ],
        [
          "Wenchao",
          "Wang"
        ],
        [
          "Pengyuan",
          "Zhang"
        ]
      ],
      "title": "The Effect of Silence and Dual-Band Fusion in Anti-Spoofing System",
      "original": "1281",
      "page_count": 5,
      "order": 875,
      "p1": "4279",
      "pn": "4283",
      "abstract": [
        "The current neural network based anti-spoofing systems have poor robustness.\nTheir performance degrades further after voice activity detection (VAD)\nperformed, making it difficult to be applied in practice. This work\ninvestigated the effect of silence at the beginning and end of speech,\nfinding that silent differences are part of the basis for countermeasures&#8217;\njudgements. The reason for the performance deterioration caused by\nVAD is also explored. The experimental results demonstrate that the\nneural network loses the information about silent segments after the\nVAD operation removes them. This can lead to more serious overfitting.\nIn order to solve the overfitting problem, the work in this paper also\nanalyzes the reasons for system overfitting from different frequency\nsub-bands. It is found that the high-frequency part of the feature\nis the main cause of system overfitting, while the low-frequency part\nis more robust but less accurate against known attacks. Therefore,\nwe propose the dual-band fusion anti-spoofing algorithm, which requires\nonly two sub-systems but outperforms all but one primary system submitted\nto the logical access condition of the ASVspoof 2019 challenge. Our\nsystem has an EER of 3.50% even after VAD operations performed, thus\ncan be put into practical application.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1281",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "peng21d_interspeech": {
      "authors": [
        [
          "Zhiyuan",
          "Peng"
        ],
        [
          "Xu",
          "Li"
        ],
        [
          "Tan",
          "Lee"
        ]
      ],
      "title": "Pairing Weak with Strong: Twin Models for Defending Against Adversarial Attack on Speaker Verification",
      "original": "1343",
      "page_count": 5,
      "order": 876,
      "p1": "4284",
      "pn": "4288",
      "abstract": [
        "Vulnerability of speaker verification (SV) systems under adversarial\nattack receives wide attention recently. Simple and effective countermeasures\nagainst such attack are yet to be developed. This paper formulates\nthe task of adversarial defense as a problem of attack detection. The\ndetection is made possible with the verification scores from a pair\nof purposely selected SV models. The twin-model design comprises a\nfragile model paired up with a relatively robust one. The two models\nshow prominent score inconsistency under adversarial attack. To detect\nthe score inconsistency, a simple one-class classifier is adopted.\nThe classifier is trained with normal speech samples, which not only\nbypasses the need of crafting adversarial samples but also prevents\nitself from over-fitting to the crafted samples, and hence makes the\ndetection robust to unseen attacks. Compared to single-model systems,\nthe proposed system shows consistent and significant performance improvement\nagainst different attack strategies. The false acceptance rates (FARs)\nare reduced from over 63.54% to 2.26% under the strongest attack. Our\napproach has practical benefits, e.g., no need to modify a well-deployed\nSV model even it is well-known and can be fully accessed by the adversary.\nMoreover, it can be combined with existing single-model countermeasures\nfor even stronger defenses.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1343",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "ling21_interspeech": {
      "authors": [
        [
          "Hefei",
          "Ling"
        ],
        [
          "Leichao",
          "Huang"
        ],
        [
          "Junrui",
          "Huang"
        ],
        [
          "Baiyan",
          "Zhang"
        ],
        [
          "Ping",
          "Li"
        ]
      ],
      "title": "Attention-Based Convolutional Neural Network for ASV Spoofing Detection",
      "original": "1404",
      "page_count": 5,
      "order": 877,
      "p1": "4289",
      "pn": "4293",
      "abstract": [
        "In recent years, automatic speaker verification (ASV) algorithms have\nundergone significant progress. They have been widely deployed in different\napplications, but the ASV systems are vulnerable to spoofing attacks,\nsuch as impersonation, replay, text-to-speech, voice conversion and\nthe recently emerged adversarial attacks. To improve the robustness\nof the ASV system, researchers have designed anti-spoofing systems\nto resist spoofing attacks. While previously proposed systems have\nshown to be effective for spoof attacks detection, they are all ensemble\nmethods based on different speech representations and architectures\nat the cost of increased model complexity, with similar performance\nnot being achieved with single systems. This paper proposes an attention-based\nsingle convolutional neural network to learn discriminative feature\nembedding for spoof detection, achieving performance comparable to\nensemble methods. The key idea is to decrease the information redundancy\namong channels and focus on the most informative sub-bands of speech\nrepresentations. The experiments show that our proposed single system\nachieves an equal error rate of 1.87% on the evaluation set of ASVspoof\n2019 Challenge, outperforming all single systems and comparable to\nthe second-ranked system (EER 1.86%) among all known systems.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1404",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "wu21i_interspeech": {
      "authors": [
        [
          "Haibin",
          "Wu"
        ],
        [
          "Yang",
          "Zhang"
        ],
        [
          "Zhiyong",
          "Wu"
        ],
        [
          "Dong",
          "Wang"
        ],
        [
          "Hung-yi",
          "Lee"
        ]
      ],
      "title": "Voting for the Right Answer: Adversarial Defense for Speaker Verification",
      "original": "1452",
      "page_count": 5,
      "order": 878,
      "p1": "4294",
      "pn": "4298",
      "abstract": [
        "Automatic speaker verification (ASV) is a well developed technology\nfor biometric identification, and has been ubiquitous implemented in\nsecurity-critic applications, such as banking and access control. However,\nprevious works have shown that ASV is under the radar of adversarial\nattacks, which are very similar to their original counterparts from\nhuman&#8217;s perception, yet will manipulate the ASV render wrong\nprediction. Due to the very late emergence of adversarial attacks for\nASV, effective countermeasures against them are limited. Given that\nthe security of ASV is of high priority, in this work, we propose the\nidea of &#8220;voting for the right answer&#8221; to prevent risky\ndecisions of ASV in blind spot areas, by employing random sampling\nand voting. Experimental results show that our proposed method improves\nthe robustness against both the limited-knowledge attackers by pulling\nthe adversarial samples out of the blind spots, and the sufficient-knowledge\nattackers by introducing randomness and increasing the attackers&#8217;\nbudgets.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1452",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "kinnunen21_interspeech": {
      "authors": [
        [
          "Tomi",
          "Kinnunen"
        ],
        [
          "Andreas",
          "Nautsch"
        ],
        [
          "Md.",
          "Sahidullah"
        ],
        [
          "Nicholas",
          "Evans"
        ],
        [
          "Xin",
          "Wang"
        ],
        [
          "Massimiliano",
          "Todisco"
        ],
        [
          "H\u00e9ctor",
          "Delgado"
        ],
        [
          "Junichi",
          "Yamagishi"
        ],
        [
          "Kong Aik",
          "Lee"
        ]
      ],
      "title": "Visualizing Classifier Adjacency Relations: A Case Study in Speaker Verification and Voice Anti-Spoofing",
      "original": "1522",
      "page_count": 5,
      "order": 879,
      "p1": "4299",
      "pn": "4303",
      "abstract": [
        "Whether it be for results summarization, or the analysis of classifier\nfusion, some means to compare different classifiers can often provide\nilluminating insight into their behaviour, (dis)similarity or complementarity.\nWe propose a simple method to derive 2D representation from detection\nscores produced by an arbitrary set of binary classifiers in response\nto a common dataset. Based upon rank correlations, our method facilitates\na visual comparison of classifiers with arbitrary scores and with close\nrelation to receiver operating characteristic (ROC) and detection error\ntrade-off (DET) analyses. While the approach is fully versatile and\ncan be applied to any detection task, we demonstrate the method using\nscores produced by automatic speaker verification and voice anti-spoofing\nsystems. The former are produced by a Gaussian mixture model system\ntrained with VoxCeleb data whereas the latter stem from submissions\nto the ASVspoof 2019 challenge.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1522",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "villalba21_interspeech": {
      "authors": [
        [
          "Jes\u00fas",
          "Villalba"
        ],
        [
          "Sonal",
          "Joshi"
        ],
        [
          "Piotr",
          "\u017belasko"
        ],
        [
          "Najim",
          "Dehak"
        ]
      ],
      "title": "Representation Learning to Classify and Detect Adversarial Attacks Against Speaker and Speech Recognition Systems",
      "original": "1759",
      "page_count": 5,
      "order": 880,
      "p1": "4304",
      "pn": "4308",
      "abstract": [
        "Adversarial attacks have become a major threat for machine learning\napplications. There is a growing interest in studying these attacks\nin the audio domain, e.g, speech and speaker recognition; and find\ndefenses against them. In this work, we focus on using representation\nlearning to classify/detect attacks w.r.t. the attack algorithm, threat\nmodel or signal-to-adversarial-noise ratio. We found that common attacks\nin the literature can be classified with accuracies as high as 90%.\nAlso, representations trained to classify attacks against speaker identification\ncan be used also to classify attacks against speaker verification and\nspeech recognition. We also tested an attack verification task, where\nwe need to decide whether two speech utterances contain the same attack.\nWe observed that our models did not generalize well to attack algorithms\nnot included in the attack representation model training. Motivated\nby this, we evaluated an unknown attack detection task. We were able\nto detect unknown attacks with equal error rates of about 19%, which\nis promising.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1759",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "zhang21ea_interspeech": {
      "authors": [
        [
          "You",
          "Zhang"
        ],
        [
          "Ge",
          "Zhu"
        ],
        [
          "Fei",
          "Jiang"
        ],
        [
          "Zhiyao",
          "Duan"
        ]
      ],
      "title": "An Empirical Study on Channel Effects for Synthetic Voice Spoofing Countermeasure Systems",
      "original": "1820",
      "page_count": 5,
      "order": 881,
      "p1": "4309",
      "pn": "4313",
      "abstract": [
        "Spoofing countermeasure (CM) systems are critical in speaker verification;\nthey aim to discern spoofing attacks from bona fide speech trials.\nIn practice, however, acoustic condition variability in speech utterances\nmay significantly degrade the performance of CM systems. In this paper,\nwe conduct a cross-dataset study on several state-of-the-art CM systems\nand observe significant performance degradation compared with their\nsingle-dataset performance. Observing differences of average magnitude\nspectra of bona fide utterances across the datasets, we hypothesize\nthat channel mismatch among these datasets is one important reason.\nWe then verify it by demonstrating a similar degradation of CM systems\ntrained on original but evaluated on channel-shifted data. Finally,\nwe propose several channel robust strategies (data augmentation, multi-task\nlearning, adversarial learning) for CM systems, and observe a significant\nperformance improvement on cross-dataset experiments.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1820",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "li21o_interspeech": {
      "authors": [
        [
          "Xu",
          "Li"
        ],
        [
          "Xixin",
          "Wu"
        ],
        [
          "Hui",
          "Lu"
        ],
        [
          "Xunying",
          "Liu"
        ],
        [
          "Helen",
          "Meng"
        ]
      ],
      "title": "Channel-Wise Gated Res2Net: Towards Robust Detection of Synthetic Speech Attacks",
      "original": "2125",
      "page_count": 5,
      "order": 882,
      "p1": "4314",
      "pn": "4318",
      "abstract": [
        "Existing approaches for anti-spoofing in automatic speaker verification\n(ASV) still lack generalizability to unseen attacks. The Res2Net approach\ndesigns a residual-like connection between feature groups within one\nblock, which increases the possible receptive fields and improves the\nsystem&#8217;s detection generalizability. However, such a residual-like\nconnection is performed by a direct addition between feature groups\nwithout channel-wise priority. We argue that the information across\nchannels may not contribute to spoofing cues equally, and the less\nrelevant channels are expected to be suppressed before adding onto\nthe next feature group, so that the system can generalize better to\nunseen attacks. This argument motivates the current work that presents\na novel, channel-wise gated Res2Net (CG-Res2Net), which modifies Res2Net\nto enable a channel-wise gating mechanism in the connection between\nfeature groups. This gating mechanism dynamically selects channel-wise\nfeatures based on the input, to suppress the less relevant channels\nand enhance the detection generalizability. Three gating mechanisms\nwith different structures are proposed and integrated into Res2Net.\nExperimental results conducted on ASVspoof 2019 logical access (LA)\ndemonstrate that the proposed CG-Res2Net significantly outperforms\nRes2Net on both the overall LA evaluation set and individual difficult\nunseen attacks, which also outperforms other state-of-the-art single\nsystems, depicting the effectiveness of our method.\n"
      ],
      "doi": "10.21437/Interspeech.2021-2125",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "ge21c_interspeech": {
      "authors": [
        [
          "Wanying",
          "Ge"
        ],
        [
          "Michele",
          "Panariello"
        ],
        [
          "Jose",
          "Patino"
        ],
        [
          "Massimiliano",
          "Todisco"
        ],
        [
          "Nicholas",
          "Evans"
        ]
      ],
      "title": "Partially-Connected Differentiable Architecture Search for Deepfake and Spoofing Detection",
      "original": "1187",
      "page_count": 5,
      "order": 883,
      "p1": "4319",
      "pn": "4323",
      "abstract": [
        "This paper reports the first successful application of a differentiable\narchitecture search (DARTS) approach to the deepfake and spoofing detection\nproblems. An example of neural architecture search, DARTS operates\nupon a continuous, differentiable search space which enables both the\narchitecture and parameters to be optimised via gradient descent. Solutions\nbased on partially-connected DARTS use random channel masking in the\nsearch space to reduce GPU time and automatically learn and optimise\ncomplex neural architectures composed of convolutional operations and\nresidual blocks. Despite being learned quickly with little human effort,\nthe resulting networks are competitive with the best performing systems\nreported in the literature. Some are also far less complex, containing\n85% fewer parameters than a Res2Net competitor.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1187",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "peterson21_interspeech": {
      "authors": [
        [
          "Kay",
          "Peterson"
        ],
        [
          "Audrey",
          "Tong"
        ],
        [
          "Yan",
          "Yu"
        ]
      ],
      "title": "OpenASR20: An Open Challenge for Automatic Speech Recognition of Conversational Telephone Speech in Low-Resource Languages",
      "original": "1930",
      "page_count": 5,
      "order": 884,
      "p1": "4324",
      "pn": "4328",
      "abstract": [
        "In 2020, the National Institute of Standards and Technology (NIST),\nin cooperation with the Intelligence Advanced Research Project Activity\n(IARPA), conducted an open challenge on automatic speech recognition\n(ASR) technology for low-resource languages on a challenging data type\n&#8212; conversational telephone speech. The OpenASR20 Challenge was\noffered for ten low-resource languages &#8212; Amharic, Cantonese,\nGuarani, Javanese, Kurmanji Kurdish, Mongolian, Pashto, Somali, Tamil,\nand Vietnamese. A total of nine teams from five countries fully participated,\nand 128 valid submissions were scored. This paper gives an overview\nof the challenge setup and procedures, as well as a summary of the\nresults. The results show overall high word error rate (WER), with\nthe best results on a severely constrained training data condition\nranging from 0.4 to 0.65, depending on the language. ASR with such\nlimited resources remains a challenging problem. Providing a computing\nplatform may be a way to level the playing field and encourage wider\nparticipation in challenges like OpenASR.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1930",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components:14 Special Sessions"
    },
    "madikeri21_interspeech": {
      "authors": [
        [
          "Srikanth",
          "Madikeri"
        ],
        [
          "Petr",
          "Motlicek"
        ],
        [
          "Herv\u00e9",
          "Bourlard"
        ]
      ],
      "title": "Multitask Adaptation with Lattice-Free MMI for Multi-Genre Speech Recognition of Low Resource Languages",
      "original": "1778",
      "page_count": 5,
      "order": 885,
      "p1": "4329",
      "pn": "4333",
      "abstract": [
        "In this paper, we develop Automatic Speech Recognition (ASR) systems\nfor multi-genre speech recognition of low-resource languages where\ntraining data is predominantly conversational speech but test data\ncan be in one of the following genres: news broadcast, topical broadcast\nand conversational speech. ASR for low-resource languages is often\ndeveloped by adapting a pre-trained model to a target language. When\ntraining data is predominantly from one genre and limited, the system&#8217;s\nperformance for other genres suffer. To handle such out-of-domain scenarios,\nwe employ multitask adaptation by using auxiliary conversational speech\ndata from other languages in addition to the target-language data.\nWe aim to (1) improve adaptation through implicit data augmentation\nby adding other languages as auxiliary tasks, and (2) prevent the acoustic\nmodel from overfitting to the dominant genre in the training set. Pre-trained\nparameters are obtained from a multilingual model trained with data\nfrom 18 languages using the Lattice-Free Maximum Mutual Information\n(LF-MMI) criterion. The adaptation is performed with the LF-MMI criterion.\nWe present results on MATERIAL datasets for three languages: Kazakh\nand Farsi and Pashto.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1778",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components:14 Special Sessions"
    },
    "zhu21f_interspeech": {
      "authors": [
        [
          "Qiu-shi",
          "Zhu"
        ],
        [
          "Jie",
          "Zhang"
        ],
        [
          "Ming-hui",
          "Wu"
        ],
        [
          "Xin",
          "Fang"
        ],
        [
          "Li-Rong",
          "Dai"
        ]
      ],
      "title": "An Improved Wav2Vec 2.0 Pre-Training Approach Using Enhanced Local Dependency Modeling for Speech Recognition",
      "original": "0067",
      "page_count": 5,
      "order": 886,
      "p1": "4334",
      "pn": "4338",
      "abstract": [
        "wav2vec 2.0 is a recently proposed self-supervised pre-training framework\nfor learning speech representation. It utilizes a transformer to learn\nglobal contextual representation, which is effective especially in\nlow-resource scenarios. Besides, it was shown that combining convolution\nneural network and transformer to model both local and global dependencies\nis beneficial for e.g., automatic speech recognition (ASR), natural\nlanguage processing (NLP). However, how to model the local and global\ndependence in pre-training models is still an open question in the\nspeech domain. In this paper, we therefore propose a new transformer\nencoder for enhancing the local dependency by combining convolution\nand self-attention modules. The transformer encoder first parallels\nthe convolution and self-attention modules, and then serialized with\nanother convolution module, sandwiched by a pair of feed forward modules.\nExperimental results show that the pre-trained model using the proposed\nmethod can reduce the word error rate (WER) compared to the reproduced\nwav2vec 2.0 at the cost of slightly increasing the size of training\nparameters.\n"
      ],
      "doi": "10.21437/Interspeech.2021-67",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components:14 Special Sessions"
    },
    "lin21i_interspeech": {
      "authors": [
        [
          "Hung-Pang",
          "Lin"
        ],
        [
          "Yu-Jia",
          "Zhang"
        ],
        [
          "Chia-Ping",
          "Chen"
        ]
      ],
      "title": "Systems for Low-Resource Speech Recognition Tasks in Open Automatic Speech Recognition and Formosa Speech Recognition Challenges",
      "original": "0358",
      "page_count": 5,
      "order": 887,
      "p1": "4339",
      "pn": "4343",
      "abstract": [
        "We, in the team name of NSYSU-MITLab, have participated in low-resource\nspeech recognition of the Open Automatic Speech Recognition Challenge\n2020 (OpenASR20) and Formosa Speech Recognition Challenge 2020 (FSR-2020).\nFor the tasks in the challenges, we build and compare end-to-end (E2E)\nsystems and Deep Neural Network Hidden Markov Model (DNN-HMM) systems.\nIn E2E systems, we implement an encoder with Conformer architecture\nand a decoder with Transformer architecture. In addition, a speaker\nclassifier with a gradient reversal layer is included in the training\nphase to improve the robustness to speaker variation. In DNN-HMM systems,\nwe implement the Time-Restricted Self-Attention and Factorized Time\nDelay Neural Networks for the DNN front-end acoustic representation\nlearning. In OpenASR20, the best word error rates we achieved are 61.45%\nfor Cantonese and 74.61% for Vietnamese. In FSR-2020, the best character\nerror rate we achieved is 43.4% for Taiwanese Southern Min Recommended\nCharacters and the best syllable error rate is 25.4% for Taiwan Minnanyu\nLuomazi Pinyin.\n"
      ],
      "doi": "10.21437/Interspeech.2021-358",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components:14 Special Sessions"
    },
    "zhao21c_interspeech": {
      "authors": [
        [
          "Jing",
          "Zhao"
        ],
        [
          "Zhiqiang",
          "Lv"
        ],
        [
          "Ambyera",
          "Han"
        ],
        [
          "Guan-Bo",
          "Wang"
        ],
        [
          "Guixin",
          "Shi"
        ],
        [
          "Jian",
          "Kang"
        ],
        [
          "Jinghao",
          "Yan"
        ],
        [
          "Pengfei",
          "Hu"
        ],
        [
          "Shen",
          "Huang"
        ],
        [
          "Wei-Qiang",
          "Zhang"
        ]
      ],
      "title": "The TNT Team System Descriptions of Cantonese and Mongolian for IARPA OpenASR20",
      "original": "1063",
      "page_count": 5,
      "order": 888,
      "p1": "4344",
      "pn": "4348",
      "abstract": [
        "This paper presents our work for OpenASR20 Challenge. We describe our\nAutomatic Speech Recognition (ASR) systems for Cantonese and Mongolian\nunder both constrained and unconstrained conditions. For constrained\ncondition, a hybrid NN-HMM ASR system play the main role, while for\nunconstrained condition, an end-to-end ASR system outperforms traditional\nhybrid systems significantly due to adequate training data. Besides,\nwe adapt to the challenging PSTN conditions using publicly available\nwideband dictated speech with similar accent, respectively for the\ntwo languages. Furthermore, data cleanup, language tailored features,\nmulti-band training, data augmentation, pre-training and system fusions\nare incorporated. Our submitted systems have achieved excellent performances\nfor the two conditions.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1063",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components:14 Special Sessions"
    },
    "alumae21_interspeech": {
      "authors": [
        [
          "Tanel",
          "Alum\u00e4e"
        ],
        [
          "Jiaming",
          "Kong"
        ]
      ],
      "title": "Combining Hybrid and End-to-End Approaches for the OpenASR20 Challenge",
      "original": "1086",
      "page_count": 5,
      "order": 889,
      "p1": "4349",
      "pn": "4353",
      "abstract": [
        "This paper describes the TalTech team submission to the OpenASR20 Challenge.\nOpenASR20 evaluated low-resource speech recognition technologies across\n10 languages, using only 10 hours of training data in the constrained\ncondition. Our ASR systems used hybrid CNN-TDNNF-based acoustic models,\ntrained with different data augmentation strategies. We used language\nmodel adaptation, recurrent neural network language models and lattice\ncombination for improving first pass results. The scores of our submissions\nwere the best across all teams in six out of ten languages. The paper\nalso describes post-evaluation experiments that focused on the unconstrained\ncondition. We show that optimized N-best list combination of a CNN-TDNNF\nbased system and a finetuned multilingual XLSR-53 model results in\nlarge reductions in word error rate. Using BABEL data and the combination\nof hybrid and end-to-end systems gives 12&#8211;22% relative improvement\nover the constrained condition results.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1086",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components:14 Special Sessions"
    },
    "morris21_interspeech": {
      "authors": [
        [
          "Ethan",
          "Morris"
        ],
        [
          "Robbie",
          "Jimerson"
        ],
        [
          "Emily",
          "Prud\u2019hommeaux"
        ]
      ],
      "title": "One Size Does Not Fit All in Resource-Constrained ASR",
      "original": "1970",
      "page_count": 5,
      "order": 890,
      "p1": "4354",
      "pn": "4358",
      "abstract": [
        "The application of deep neural networks to the task of acoustic modeling\nfor automatic speech recognition has resulted in dramatic decreases\nin ASR word error rates, enabling the use of this technology for interacting\nwith smart phones and personal home assistants in high-resource languages.\nDeveloping ASR models of this caliber, however, requires hundreds or\nthousands of hours of transcribed speech recordings, which presents\nchallenges for the vast majority of the world&#8217;s languages. In\nthis paper, we investigate the utility of three distinct architectures\nthat have previously been used for ASR in languages with limited training\nresources. We train and test these systems on publicly available ASR\ndatasets for several typologically and orthographically diverse languages,\nwhich were produced under a variety of conditions using different speech\ncollection strategies, practices, and equipment. Although these corpora\nare comparable in size, we find that no single ASR architecture outperforms\nall others. In addition, word error rates vary significantly, in some\ncases within the range of those typically reported for high-resource\nlanguages. Our results point to the importance of considering language-specific\nand corpus-specific factors and experimenting with multiple approaches\nwhen developing ASR systems for languages with limited training resources.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1970",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components:14 Special Sessions"
    },
    "cristia21_interspeech": {
      "authors": [
        [
          "Alejandrina",
          "Cristia"
        ]
      ],
      "title": "Child Language Acquisition Studied with Wearables",
      "original": "abs24",
      "page_count": 0,
      "order": 891,
      "p1": "0",
      "pn": "",
      "abstract": [
        "In recent years, the ease with which we can collect audio (and to a\nlesser extent visual information) with wearables has improved dramatically.\nThese allow unprecedented access to the speech that children produce,\nand that which they year. Although many conclusions drawn from short\nobservations seem to generalize to these naturalistic datasets, others\nappear questionable based on human annotations of data collected with\nwearables. Making the best of such recordings also requires unique\ntool development.\n"
      ]
    },
    "mikolov21_interspeech": {
      "authors": [
        [
          "Tom\u00e1\u0161",
          "Mikolov"
        ]
      ],
      "title": "Language Modeling and Artificial Intelligence",
      "original": "abs25",
      "page_count": 0,
      "order": 892,
      "p1": "0",
      "pn": "",
      "abstract": [
        "Statistical language modeling has been labeled as an AI-complete problem\nby many famous researchers of the past. However, despite all the progress\nmade in the last decade, it remains unclear how much progress towards\ntruly intelligent language models we made.\n"
      ]
    },
    "gimeno21_interspeech": {
      "authors": [
        [
          "Pablo",
          "Gimeno"
        ],
        [
          "Alfonso",
          "Ortega"
        ],
        [
          "Antonio",
          "Miguel"
        ],
        [
          "Eduardo",
          "Lleida"
        ]
      ],
      "title": "Unsupervised Representation Learning for Speech Activity Detection in the Fearless Steps Challenge 2021",
      "original": "0309",
      "page_count": 5,
      "order": 893,
      "p1": "4359",
      "pn": "4363",
      "abstract": [
        "In this paper, we describe the ViVoLab speech activity detection (SAD)\nsystem submitted to the Fearless Steps Challenge Phase III. This series\nof challenges have proposed a number of speech processing task dealing\nwith audio from Apollo space missions over the last few years. The\nfocus in this edition is set on the generalisation capabilities of\nthe systems, with new evaluation data from different channels. Our\nproposed submission is based on the use of the unsupervised representation\nlearning paradigm, seeking to obtain a new and more discriminative\naudio representation than traditional perceptual features such as log\nMel-filterbank energies. These new features are used to train different\nvariations of a convolutional recurrent neural network (CRNN). Experimental\nresults show that features learned via unsupervised learning provide\na much more robust representation, significantly reducing the mismatch\nobserved between development and evaluation partition results. Obtained\nresults largely outperform the organisation baseline, achieving a DCF\nmetric of 2.98% on the evaluation set and ranking third among all the\nparticipant teams.\n"
      ],
      "doi": "10.21437/Interspeech.2021-309",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals:14 Special Sessions"
    },
    "vuong21_interspeech": {
      "authors": [
        [
          "Tyler",
          "Vuong"
        ],
        [
          "Yangyang",
          "Xia"
        ],
        [
          "Richard M.",
          "Stern"
        ]
      ],
      "title": "The Application of Learnable STRF Kernels to the 2021 Fearless Steps Phase-03 SAD Challenge",
      "original": "0651",
      "page_count": 5,
      "order": 894,
      "p1": "4364",
      "pn": "4368",
      "abstract": [
        "We describe a deep-learning-based system developed for the Fearless\nSteps Phase-03 Speech Activity Detection (SAD) challenge. The system\nincludes both learnable spectro-temporal receptive fields (STRFs) and\nunconstrained 2-dimensional convolutional kernels in the first layer.\nExperiments show that the inclusion of learnable STRFs in the first\nlayer increases the system&#8217;s robustness to additive noise. Additionally,\nwe found that utilizing SpecAugment during training improves generalization\non unseen data. By incorporating these enhancements and others our\nsystem achieved the best score in the official SAD challenge.\n"
      ],
      "doi": "10.21437/Interspeech.2021-651",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals:14 Special Sessions"
    },
    "sarfjoo21_interspeech": {
      "authors": [
        [
          "Seyyed Saeed",
          "Sarfjoo"
        ],
        [
          "Srikanth",
          "Madikeri"
        ],
        [
          "Petr",
          "Motlicek"
        ]
      ],
      "title": "Speech Activity Detection Based on Multilingual Speech Recognition System",
      "original": "1058",
      "page_count": 5,
      "order": 895,
      "p1": "4369",
      "pn": "4373",
      "abstract": [
        "To better model the contextual information and increase the generalization\nability of the Speech Activity Detection (SAD) system, this paper leverages\na multilingual Automatic Speech Recognition (ASR) system to perform\nSAD. Sequence-discriminative training of Acoustic Model (AM) using\nLattice-Free Maximum Mutual Information (LF-MMI) loss function, effectively\nextracts the contextual information of the input acoustic frame. Multilingual\nAM training causes the robustness to noise and language variabilities.\nThe index of maximum output posterior is considered as a frame-level\nspeech/non-speech decision function. Majority voting and logistic regression\nare applied to fuse the language-dependent decisions. The multilingual\nASR is trained on 18 languages of BABEL datasets and the built SAD\nis evaluated on 3 different languages. On out-of-domain datasets, the\nproposed SAD model shows significantly better performance with respect\nto baseline models. On the Ester2 dataset, without using any in-domain\ndata, this model outperforms the WebRTC, phoneme recognizer based VAD\n(Phn_Rec), and Pyannote baselines (respectively by 7.1, 1.7, and 2.7%\nabsolute) in Detection Error Rate (DetER) metrics. Similarly, on the\nLiveATC dataset, this model outperforms the WebRTC, Phn_Rec, and Pyannote\nbaselines (respectively by 6.4, 10.0, and 3.7% absolutely) in DetER\nmetrics.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1058",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "luckenbaugh21_interspeech": {
      "authors": [
        [
          "Jarrod",
          "Luckenbaugh"
        ],
        [
          "Samuel",
          "Abplanalp"
        ],
        [
          "Rachel",
          "Gonzalez"
        ],
        [
          "Daniel",
          "Fulford"
        ],
        [
          "David",
          "Gard"
        ],
        [
          "Carlos",
          "Busso"
        ]
      ],
      "title": "Voice Activity Detection with Teacher-Student Domain Emulation",
      "original": "1234",
      "page_count": 5,
      "order": 896,
      "p1": "4374",
      "pn": "4378",
      "abstract": [
        "Transfer learning is a promising approach to increase performance for\nmany speech-based systems, including <i>voice activity detection</i>\n(VAD). Domain adaptation, a subfield of transfer learning, often improves\nmodel conditioning in the presence of a mismatch between train-test\nconditions. This study proposes a formulation for VAD based on the\nteacher-student training, where the teacher model, trained with clean\ndata, transfers knowledge to the student model trained with a noisy,\npaired version of the corpus resembling the test conditions. The models\nleverage temporal information using <i>recurrent neural networks</i>\n(RNN), implemented with either <i>bidirectional long short term memory</i>\n(BLSTM) or the modern, continuous-state Hopfield network. We provide\nevidence that in-domain noise emulation for domain adaptation is viable\nunder unconstrained audio channel conditions for VAD &#8220;in the\nwild.&#8221; Our application domain is in healthcare, where multimodal\nsensors, including microphones, from portable devices are used to automatically\npredict social isolation in patients affected by schizophrenia. We\nempirically show positive results for domain emulation when the training\nconditions are similar to the target domain. We also show that the\nHopfield network outperforms our best BLSTM for VAD on real-world benchmarks.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1234",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals"
    },
    "ghahabi21_interspeech": {
      "authors": [
        [
          "Omid",
          "Ghahabi"
        ],
        [
          "Volker",
          "Fischer"
        ]
      ],
      "title": "EML Online Speech Activity Detection for the Fearless Steps Challenge Phase-III",
      "original": "1456",
      "page_count": 4,
      "order": 897,
      "p1": "4379",
      "pn": "4382",
      "abstract": [
        "Speech Activity Detection (SAD), locating speech segments within an\naudio recording, is a main part of most speech technology applications.\nRobust SAD is usually more difficult in noisy conditions with varying\nsignal-to-noise ratios (SNR). The Fearless Steps challenge has recently\nprovided such data from the NASA Apollo-11 mission for different speech\nprocessing tasks including SAD. Most audio recordings are degraded\nby different kinds and levels of noise varying within and between channels.\nThis paper describes the EML online algorithm for the most recent phase\nof this challenge. The proposed algorithm can be trained both in a\nsupervised and unsupervised manner and assigns speech and non-speech\nlabels at runtime approximately every 0.1 sec. The experimental results\nshow a competitive accuracy on both development and evaluation datasets\nwith a real-time factor of about 0.002 using a single CPU machine.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1456"
    },
    "opatka21_interspeech": {
      "authors": [
        [
          "Kuba",
          "\u0141opatka"
        ],
        [
          "Katarzyna",
          "Kaszuba-Miotke"
        ],
        [
          "Piotr",
          "Klinke"
        ],
        [
          "Pawe\u0142",
          "Trella"
        ]
      ],
      "title": "Device Playback Augmentation with Echo Cancellation for Keyword Spotting",
      "original": "1316",
      "page_count": 5,
      "order": 898,
      "p1": "4383",
      "pn": "4387",
      "abstract": [
        "Keyword spotting (KWS) is required to operate in device playback conditions\nin which the device itself plays interfering signals. We propose a\nnew method to augment the training set and adapt the acoustic model\nto the playback environment. It is based on acoustic simulation which\nmodels the coupling between the device&#8217;s loudspeakers and microphones.\nThe employed model involves frequency response of the device, as well\nas room impulse response and nonlinear distortions introduced in the\nplayback path. Finally, we pass the simulated signals through Acoustic\nEcho Cancellation (AEC) to model the artifacts introduced by AEC algorithm.\nThe proposed method reduces False Rejection Rate in device playback\nnoise by 25&#8211;60% for a Time-Delay Neural Network-based KWS engine.\nIt is shown that the introduction of device characteristics and nonlinear\nfiltration is necessary to achieve improvement in playback conditions.\nThe augmentation scheme is highly independent of the architecture of\nthe KWS system.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1316",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "yusuf21_interspeech": {
      "authors": [
        [
          "Bolaji",
          "Yusuf"
        ],
        [
          "Alican",
          "Gok"
        ],
        [
          "Batuhan",
          "Gundogdu"
        ],
        [
          "Murat",
          "Saraclar"
        ]
      ],
      "title": "End-to-End Open Vocabulary Keyword Search",
      "original": "1399",
      "page_count": 5,
      "order": 899,
      "p1": "4388",
      "pn": "4392",
      "abstract": [
        "Recently, neural approaches to spoken content retrieval have become\npopular. However, they tend to be restricted in their vocabulary or\nin their ability to deal with imbalanced test settings. These restrictions\nlimit their applicability in keyword search, where the set of queries\nis not known beforehand, and where the system should return not just\nwhether an utterance contains a query but the exact location of any\nsuch occurrences. In this work, we propose a model directly optimized\nfor keyword search. The model takes a query and an utterance as input\nand returns a sequence of probabilities for each frame of the utterance\nof the query having occurred in that frame. Experiments show that the\nproposed model not only outperforms similar end-to-end models on a\ntask where the ratio of positive and negative trials is artificially\nbalanced, but it is also able to deal with the far more challenging\ntask of keyword search with its inherent imbalance. Furthermore, using\nour system to rescore the outputs an LVCSR-based keyword search system\nleads to significant improvements on the latter.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1399",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "merkx21_interspeech": {
      "authors": [
        [
          "Danny",
          "Merkx"
        ],
        [
          "Stefan L.",
          "Frank"
        ],
        [
          "Mirjam",
          "Ernestus"
        ]
      ],
      "title": "Semantic Sentence Similarity: Size does not Always Matter",
      "original": "1464",
      "page_count": 5,
      "order": 900,
      "p1": "4393",
      "pn": "4397",
      "abstract": [
        "This study addresses the question whether visually grounded speech\nrecognition (VGS) models learn to capture sentence semantics without\naccess to any prior linguistic knowledge. We produce synthetic and\nnatural spoken versions of a well known semantic textual similarity\ndatabase and show that our VGS model produces embeddings that correlate\nwell with human semantic similarity judgements. Our results show that\na model trained on a small image-caption database outperforms two models\ntrained on much larger databases, indicating that database size is\nnot all that matters. We also investigate the importance of having\nmultiple captions per image and find that this is indeed helpful even\nif the total number of images is lower, suggesting that paraphrasing\nis a valuable learning signal. While the general trend in the field\nis to create ever larger datasets to train models on, our findings\nindicate other characteristics of the database can just as important.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1464",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "svec21_interspeech": {
      "authors": [
        [
          "Jan",
          "\u0160vec"
        ],
        [
          "Lubo\u0161",
          "\u0160m\u00eddl"
        ],
        [
          "Josef V.",
          "Psutka"
        ],
        [
          "Ale\u0161",
          "Pra\u017e\u00e1k"
        ]
      ],
      "title": "Spoken Term Detection and Relevance Score Estimation Using Dot-Product of Pronunciation Embeddings",
      "original": "1704",
      "page_count": 5,
      "order": 901,
      "p1": "4398",
      "pn": "4402",
      "abstract": [
        "The paper describes a novel approach to Spoken Term Detection (STD)\nin large spoken archives using deep LSTM networks. The work is based\non the previous approach of using Siamese neural networks for STD and\nnaturally extends it to directly localize a spoken term and estimate\nits relevance score. The phoneme confusion network generated by a phoneme\nrecognizer is processed by the deep LSTM network which projects each\nsegment of the confusion network into an embedding space. The searched\nterm is projected into the same embedding space using another deep\nLSTM network. The relevance score is then computed using a simple dot-product\nin the embedding space and calibrated using a sigmoid function to predict\nthe probability of occurrence. The location of the searched term is\nthen estimated from the sequence of output probabilities. The deep\nLSTM networks are trained in a self-supervised manner from paired recognition\nhypotheses on word and phoneme levels. The method is experimentally\nevaluated on MALACH data in English and Czech languages.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1704",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "buet21_interspeech": {
      "authors": [
        [
          "Fran\u00e7ois",
          "Buet"
        ],
        [
          "Fran\u00e7ois",
          "Yvon"
        ]
      ],
      "title": "Toward Genre Adapted Closed Captioning",
      "original": "1762",
      "page_count": 5,
      "order": 902,
      "p1": "4403",
      "pn": "4407",
      "abstract": [
        "This paper studies the generation of intralingual closed captions from\nautomatic speech transcripts, with the aim to assess techniques for\nmulti-genre captioning. Captions and subtitles greatly vary in form\nand content depending on the programs genres and subtitling styles,\nresulting for instance in significantly different compression rates\nand lexical content. Borrowing ideas from the multi-domain machine\ntranslation literature, we implement and contrast several adaptation\nmethods on a diverse set of programs broadcast on the French public\nTV. Our results show that such multi-domain adaption techniques are\neffective and help to improve our automatic subtitling system.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1762"
    },
    "korzekwa21b_interspeech": {
      "authors": [
        [
          "Daniel",
          "Korzekwa"
        ],
        [
          "Jaime",
          "Lorenzo-Trueba"
        ],
        [
          "Thomas",
          "Drugman"
        ],
        [
          "Shira",
          "Calamaro"
        ],
        [
          "Bozena",
          "Kostek"
        ]
      ],
      "title": "Weakly-Supervised Word-Level Pronunciation Error Detection in Non-Native English Speech",
      "original": "0038",
      "page_count": 5,
      "order": 903,
      "p1": "4408",
      "pn": "4412",
      "abstract": [
        "We propose a weakly-supervised model for word-level mispronunciation\ndetection in non-native (L2) English speech. To train this model, phonetically\ntranscribed L2 speech is not required and we only need to mark mispronounced\nwords. The lack of phonetic transcriptions for L2 speech means that\nthe model has to learn only from a weak signal of word-level mispronunciations.\nBecause of that and due to the limited amount of mispronounced L2 speech,\nthe model is more likely to overfit. To limit this risk, we train it\nin a multi-task setup. In the first task, we estimate the probabilities\nof word-level mispronunciation. For the second task, we use a phoneme\nrecognizer trained on phonetically transcribed L1 speech that is easily\naccessible and can be automatically annotated. Compared to state-of-the-art\napproaches, we improve the accuracy of detecting word-level pronunciation\nerrors in AUC metric by 30% on the GUT Isle Corpus of L2 Polish speakers,\nand by 21.5% on the Isle Corpus of L2 German and Italian speakers.\n"
      ],
      "doi": "10.21437/Interspeech.2021-38",
      "author_area_id": "1",
      "author_area_label": "Speech Perception, Production and Acquisition  by Human Listeners"
    },
    "kanda21b_interspeech": {
      "authors": [
        [
          "Naoyuki",
          "Kanda"
        ],
        [
          "Guoli",
          "Ye"
        ],
        [
          "Yashesh",
          "Gaur"
        ],
        [
          "Xiaofei",
          "Wang"
        ],
        [
          "Zhong",
          "Meng"
        ],
        [
          "Zhuo",
          "Chen"
        ],
        [
          "Takuya",
          "Yoshioka"
        ]
      ],
      "title": "End-to-End Speaker-Attributed ASR with Transformer",
      "original": "0101",
      "page_count": 5,
      "order": 904,
      "p1": "4413",
      "pn": "4417",
      "abstract": [
        "This paper presents our recent effort on end-to-end speaker-attributed\nautomatic speech recognition, which jointly performs speaker counting,\nspeech recognition and speaker identification for monaural multi-talker\naudio. Firstly, we thoroughly update the model architecture that was\npreviously designed based on a long short-term memory (LSTM)-based\nattention encoder decoder by applying transformer architectures. Secondly,\nwe propose a speaker deduplication mechanism to reduce speaker identification\nerrors in highly overlapped regions. Experimental results on the LibriSpeechMix\ndataset shows that the transformer-based architecture is especially\ngood at counting the speakers and that the proposed model reduces the\nspeaker-attributed word error rate by 47% over the LSTM-based baseline.\nFurthermore, for the LibriCSS dataset, which consists of real recordings\nof overlapped speech, the proposed model achieves concatenated minimum-permutation\nword error rates of 11.9% and 16.3% with and without target speaker\nprofiles, respectively, both of which are the state-of-the-art results\nfor LibriCSS with the monaural setting.\n"
      ],
      "doi": "10.21437/Interspeech.2021-101",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "soltau21_interspeech": {
      "authors": [
        [
          "Hagen",
          "Soltau"
        ],
        [
          "Mingqiu",
          "Wang"
        ],
        [
          "Izhak",
          "Shafran"
        ],
        [
          "Laurent El",
          "Shafey"
        ]
      ],
      "title": "Understanding Medical Conversations: Rich Transcription, Confidence Scores &amp; Information Extraction",
      "original": "0691",
      "page_count": 5,
      "order": 905,
      "p1": "4418",
      "pn": "4422",
      "abstract": [
        "In this paper, we describe novel components for extracting clinically\nrelevant information from medical conversations which will be available\nas Google APIs. We describe a transformer-based Recurrent Neural Network\nTransducer (RNN-T) model tailored for long-form audio, which can produce\nrich transcriptions including speaker segmentation, speaker role labeling,\npunctuation and capitalization. On a representative test set, we compare\nperformance of RNN-T models with different encoders, units and streaming\nconstraints. Our transformer-based streaming model performs at about\n20% WER on the ASR task, 6% WDER on the diarization task, 43% SER on\nperiods, 52% SER on commas, 43% SER on question marks and 30% SER on\ncapitalization. Our recognizer is paired with a confidence model that\nutilizes both acoustic and lexical features from the recognizer. The\nmodel performs at about 0.37 NCE. Finally, we describe a RNN-T based\ntagging model. The performance of the model depends on the ontologies,\nwith F-scores of 0.90 for medications, 0.76 for symptoms, 0.75 for\nconditions, 0.76 for diagnosis, and 0.61 for treatments. While there\nis still room for improvement, our results suggest that these models\nare sufficiently accurate for practical applications.\n"
      ],
      "doi": "10.21437/Interspeech.2021-691"
    },
    "vidal21_interspeech": {
      "authors": [
        [
          "Jazm\u00edn",
          "Vidal"
        ],
        [
          "Cyntia",
          "Bonomi"
        ],
        [
          "Marcelo",
          "Sancinetti"
        ],
        [
          "Luciana",
          "Ferrer"
        ]
      ],
      "title": "Phone-Level Pronunciation Scoring for Spanish Speakers Learning English Using a GOP-DNN System",
      "original": "0745",
      "page_count": 5,
      "order": 906,
      "p1": "4423",
      "pn": "4427",
      "abstract": [
        "In today&#8217;s globalized world being able to communicate in English\nis crucial to many people. Computer assisted pronunciation training\n(CAPT) systems can help students achieve English proficiency by providing\nan accessible way to practice, offering personalized feedback. However,\nphone-level pronunciation scoring is still a very challenging task,\nwith performance far from that of human annotators. In this paper we\ncompare and present results on the Spanish subset of the L2-ARCTIC\ncorpus and the new Epa-DB database, both containing non-native English\nspeech by native Spanish speakers and intended for the development\nof pronunciation scoring systems. We show the most frequent errors\nin each database and compare performance of a state-of-the-art goodness\nof pronunciation (GOP) system. Results show that both databases have\nsimilar error patterns and that performance is similar for most phones,\ndespite differences in recording conditions. For the EpaDB database\nwe also present an analysis of the errors per target phone. This study\nvalidates the EpaDB collection and annotations, providing initial results\nand contributing to the advancement of a challenging low-resource task.\n"
      ],
      "doi": "10.21437/Interspeech.2021-745",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "xu21k_interspeech": {
      "authors": [
        [
          "Xiaoshuo",
          "Xu"
        ],
        [
          "Yueteng",
          "Kang"
        ],
        [
          "Songjun",
          "Cao"
        ],
        [
          "Binghuai",
          "Lin"
        ],
        [
          "Long",
          "Ma"
        ]
      ],
      "title": "Explore wav2vec 2.0 for Mispronunciation Detection",
      "original": "0777",
      "page_count": 5,
      "order": 907,
      "p1": "4428",
      "pn": "4432",
      "abstract": [
        "This paper presents an initial attempt to use self-supervised learning\nfor Mispronunciation Detection. Unlike existing methods that use speech\nrecognition corpus to train models, we exploit unlabeled data and utilize\na self-supervised learning technique, Wav2vec 2.0, for pretraining.\nAfter the pretraining process, the training process only requires a\nlittle pronunciation-labeled data for finetuning. Formulating Mispronunciation\nDetection as a binary classification task, we add convolutional and\npooling layers on the top of the pretrained model to detect mispronunciations\nof the given prompted texts within the alignment segmentations. The\ntraining process is simple and effective. Several experiments are conducted\nto validate the effectiveness of the pretrained method. Our approach\noutperforms existing methods on a public dataset L2-ARCTIC with a F1\nvalue of 0.610.\n"
      ],
      "doi": "10.21437/Interspeech.2021-777",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "ando21_interspeech": {
      "authors": [
        [
          "Shintaro",
          "Ando"
        ],
        [
          "Nobuaki",
          "Minematsu"
        ],
        [
          "Daisuke",
          "Saito"
        ]
      ],
      "title": "Lexical Density Analysis of Word Productions in Japanese English Using Acoustic Word Embeddings",
      "original": "0853",
      "page_count": 5,
      "order": 908,
      "p1": "4433",
      "pn": "4437",
      "abstract": [
        "In L2 pronunciation, what kind of phonetic errors are more influential\nto intelligibility reduction? Teachers say that learners&#8217; utterances\nbecome unintelligible when words are pronounced with such errors that\nmake the words misidentified as others. In this paper, we focus on\nJapanese English (JE), where the number of phonemes of the L1 (Japanese)\nis much smaller than that of the L2 (American English, AE). Since learners\noften substitute L1 phonemes when speaking in L2, some words are expected\nto be pronounced not distinctively enough in JE, which may result in\nword misidentification. This implies that words of JE will exist phonetically\ncloser to each other in a space where words are distributed. In this\npaper, lexical density analysis of JE and AE is carried out using acoustic\nword embeddings. Word productions in JE and AE, extracted from the\nERJ corpus, are mapped as points in an acoustic word embedding space\nobtained by network training with the WSJ corpus. Experiments show\nthat significantly higher density is found in JE than in AE and it\nis also found in poor learners than in good learners.\n"
      ],
      "doi": "10.21437/Interspeech.2021-853",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "lin21j_interspeech": {
      "authors": [
        [
          "Binghuai",
          "Lin"
        ],
        [
          "Liyuan",
          "Wang"
        ]
      ],
      "title": "Deep Feature Transfer Learning for Automatic Pronunciation Assessment",
      "original": "0931",
      "page_count": 5,
      "order": 909,
      "p1": "4438",
      "pn": "4442",
      "abstract": [
        "Automatic pronunciation assessment is commonly developed to evaluate\npronunciation quality of second language (L2) learners. Traditional\nmethods for automatic pronunciation assessment normally utilize speech\nfeatures such as Goodness of pronunciation (GOP), which may not provide\nsufficient information for the pronunciation proficiency assessment\n[1]. In this paper, we propose a transfer learning method for automatic\npronunciation assessment. We directly utilize the deep features from\nthe acoustic model instead of traditional features such as GOP, and\ntransfer the acoustic knowledge from ASR to a specific scoring module.\nThe scoring module is designed to consider the relationship among different\ngranularities in an utterance based on an attention mechanism. Only\nthis module is updated for faster transfer and adaptation of various\npronunciation assessment tasks. Experimental results based on the dataset\nrecorded by Chinese English-as-second-language (ESL) learners and the\nSpeechocean762 dataset demonstrate that the proposed method outperforms\nthe traditional GOP-based baselines in Pearson correlation coefficient\n(PCC) and yields parameter-efficient transfer for different pronunciation\nassessment tasks.\n"
      ],
      "doi": "10.21437/Interspeech.2021-931",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "zhang21fa_interspeech": {
      "authors": [
        [
          "Huayun",
          "Zhang"
        ],
        [
          "Ke",
          "Shi"
        ],
        [
          "Nancy F.",
          "Chen"
        ]
      ],
      "title": "Multilingual Speech Evaluation: Case Studies on English, Malay and Tamil",
      "original": "1258",
      "page_count": 5,
      "order": 910,
      "p1": "4443",
      "pn": "4447",
      "abstract": [
        "Speech evaluation is an essential component in computer-assisted language\nlearning (CALL). While speech evaluation on English has been popular,\nautomatic speech scoring on low resource languages remains challenging.\nWork in this area has focused on monolingual specific designs and handcrafted\nfeatures stemming from resource-rich languages like English. Such approaches\nare often difficult to generalize to other languages, especially if\nwe also want to consider suprasegmental qualities such as rhythm. In\nthis work, we examine three different languages that possess distinct\nrhythm patterns: English (stress-timed), Malay (syllable-timed), and\nTamil (mora-timed). We exploit robust feature representations inspired\nby music processing and vector representation learning. Empirical validations\nshow consistent gains for all three languages when predicting pronunciation,\nrhythm and intonation performance.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1258"
    },
    "peng21e_interspeech": {
      "authors": [
        [
          "Linkai",
          "Peng"
        ],
        [
          "Kaiqi",
          "Fu"
        ],
        [
          "Binghuai",
          "Lin"
        ],
        [
          "Dengfeng",
          "Ke"
        ],
        [
          "Jinsong",
          "Zhan"
        ]
      ],
      "title": "A Study on Fine-Tuning wav2vec2.0 Model for the Task of Mispronunciation Detection and Diagnosis",
      "original": "1344",
      "page_count": 5,
      "order": 911,
      "p1": "4448",
      "pn": "4452",
      "abstract": [
        "Mispronunciation detection and diagnosis (MDD) technology is a key\ncomponent of computer-assisted pronunciation training system (CAPT).\nThe mainstream method is based on deep neural network automatic speech\nrecognition. Unfortunately, the technique requires massive human-annotated\nspeech recordings for training. Due to the huge variations in mother\ntongue, age, and proficiency level among second language learners,\nit is difficult to gather a large amount of matching data for acoustic\nmodel training, which greatly limits the model performance. In this\npaper, we explore the use of Self-Supervised Pretraining (SSP) model\nwav2vec2.0 for MDD tasks. SSP utilizes a large unlabelled dataset to\nlearn general representation and can be applied in downstream tasks.\nWe conduct experiments using two publicly available datasets (TIMIT,\nL2-arctic) and our best system achieves 60.44% f1-score. Moreover,\nour method is able to achieve 55.52% f1-score with 3 times less data,\nwhich demonstrates the effectiveness of SSP on MDD.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1344",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "qiao21b_interspeech": {
      "authors": [
        [
          "Yu",
          "Qiao"
        ],
        [
          "Wei",
          "Zhou"
        ],
        [
          "Elma",
          "Kerz"
        ],
        [
          "Ralf",
          "Schl\u00fcter"
        ]
      ],
      "title": "The Impact of ASR on the Automatic Analysis of Linguistic Complexity and Sophistication in Spontaneous L2 Speech",
      "original": "1402",
      "page_count": 5,
      "order": 912,
      "p1": "4453",
      "pn": "4457",
      "abstract": [
        "In recent years, automated approaches to assessing linguistic complexity\nin second language (L2) writing have made significant progress in gauging\nlearner performance, predicting human ratings of the quality of learner\nproductions, and benchmarking L2 development. In contrast, there is\ncomparatively little work in the area of speaking, particularly with\nrespect to fully automated approaches to assessing L2 spontaneous speech.\nWhile the importance of a well-performing ASR system is widely recognized,\nlittle research has been conducted to investigate the impact of its\nperformance on subsequent automatic text analysis. In this paper, we\nfocus on this issue and examine the impact of using a state-of-the-art\nASR system for subsequent automatic analysis of linguistic complexity\nin spontaneously produced L2 speech. A set of 30 selected measures\nwere considered, falling into four categories: syntactic, lexical,\nn-gram frequency, and information-theoretic measures. The agreement\nbetween the scores for these measures obtained on the basis of ASR-generated\nvs. manual transcriptions was determined through correlation analysis.\nA more differential effect of ASR performance on specific types of\ncomplexity measures when controlling for task type effects is also\npresented.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1402",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "tanaka21c_interspeech": {
      "authors": [
        [
          "Tomohiro",
          "Tanaka"
        ],
        [
          "Ryo",
          "Masumura"
        ],
        [
          "Mana",
          "Ihori"
        ],
        [
          "Akihiko",
          "Takashima"
        ],
        [
          "Shota",
          "Orihashi"
        ],
        [
          "Naoki",
          "Makishima"
        ]
      ],
      "title": "End-to-End Rich Transcription-Style Automatic Speech Recognition with Semi-Supervised Learning",
      "original": "1981",
      "page_count": 5,
      "order": 913,
      "p1": "4458",
      "pn": "4462",
      "abstract": [
        "We propose a semi-supervised learning method for building end-to-end\nrich transcription-style automatic speech recognition (RT-ASR) systems\nfrom small-scale rich transcription-style and large-scale common transcription-style\ndatasets. In spontaneous speech tasks, various speech phenomena such\nas fillers, word fragments, laughter and coughs, etc. are often included.\nWhile common transcriptions do not give special awareness to these\nphenomena, rich transcriptions explicitly convert them into special\nphenomenon tokens as well as textual tokens. In previous studies, the\ntextual and phenomenon tokens were simultaneously estimated in an end-to-end\nmanner. However, it is difficult to build accurate RT-ASR systems because\nlarge-scale rich transcription-style datasets are often unavailable.\nTo solve this problem, our training method uses a limited rich transcription-style\ndataset and common transcription-style dataset simultaneously. The\nKey process in our semi-supervised learning is to convert the common\ntranscription-style dataset into a pseudo-rich transcription-style\ndataset. To this end, we introduce style tokens which control phenomenon\ntokens are generated or not into transformer-based autoregressive modeling.\nWe use this modeling for generating the pseudo-rich transcription-style\ndatasets and for building RT-ASR system from the pseudo and original\ndatasets. Our experiments on spontaneous ASR tasks showed the effectiveness\nof the proposed method.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1981",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "cumbal21_interspeech": {
      "authors": [
        [
          "Ronald",
          "Cumbal"
        ],
        [
          "Birger",
          "Moell"
        ],
        [
          "Jos\u00e9",
          "Lopes"
        ],
        [
          "Olov",
          "Engwall"
        ]
      ],
      "title": "&#8220;You don&#8217;t understand me!&#8221;: Comparing ASR Results for L1 and L2 Speakers of Swedish",
      "original": "2140",
      "page_count": 5,
      "order": 914,
      "p1": "4463",
      "pn": "4467",
      "abstract": [
        "The performance of Automatic Speech Recognition (ASR) systems has constantly\nincreased in state-of-the-art development. However, performance tends\nto decrease considerably in more challenging conditions (e.g., background\nnoise, multiple speaker social conversations) and with more atypical\nspeakers (e.g., children, non-native speakers or people with speech\ndisorders), which signifies that general improvements do not necessarily\ntransfer to applications that rely on ASR, e.g., educational software\nfor younger students or language learners. In this study, we focus\non the gap in performance between recognition results for native and\nnon-native, read and spontaneous, Swedish utterances transcribed by\ndifferent ASR services. We compare the recognition results using Word\nError Rate and analyze the linguistic factors that may generate the\nobserved transcription errors.\n"
      ],
      "doi": "10.21437/Interspeech.2021-2140",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "zhang21ga_interspeech": {
      "authors": [
        [
          "Yang",
          "Zhang"
        ],
        [
          "Evelina",
          "Bakhturina"
        ],
        [
          "Kyle",
          "Gorman"
        ],
        [
          "Boris",
          "Ginsburg"
        ]
      ],
      "title": "NeMo Inverse Text Normalization: From Development to Production",
      "original": "1571",
      "page_count": 5,
      "order": 915,
      "p1": "4468",
      "pn": "4472",
      "abstract": [
        "Inverse text normalization (ITN) converts spoken-domain automatic speech\nrecognition (ASR) output into written-domain text to improve the readability\nof the ASR output. Many state-of-the-art ITN systems use hand-written\nweighted finite-state transducer (WFST) grammars since this task has\nextremely low tolerance to unrecoverable errors. We introduce an open-source\nPython WFST-based library for ITN which enables a seamless path from\ndevelopment to production. We describe the specification of ITN grammar\nrules for English, but the library can be adapted for other languages.\nIt can also be used for written-to-spoken text normalization. We evaluate\nthe NeMo ITN library using a modified version of the Google Text normalization\ndataset.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1571"
    },
    "naijo21_interspeech": {
      "authors": [
        [
          "Satsuki",
          "Naijo"
        ],
        [
          "Akinori",
          "Ito"
        ],
        [
          "Takashi",
          "Nose"
        ]
      ],
      "title": "Improvement of Automatic English Pronunciation Assessment with Small Number of Utterances Using Sentence Speakability",
      "original": "1132",
      "page_count": 5,
      "order": 916,
      "p1": "4473",
      "pn": "4477",
      "abstract": [
        "The current Computer-Assisted Pronunciation Training (CAPT) system\nuses DNN-based speech recognition results to evaluate learner&#8217;s\npronunciation with high accuracy when using many utterances for the\nevaluation. However, when we use only a few utterances, the accuracy\nof the CAPT system deteriorates. One reason for the deterioration is\nthat the score calculated by a CAPT system is biased depending on the\npronunciation difficulty of the sentences when using a small number\nof utterances. In this study, we developed a CAPT system that takes\nthe sentence speakability (pronunciation difficulty of sentences) into\naccount. As a result, the correlation coefficient between the human\nevaluation and the machine score was 0.46 in the conventional method,\nwhile it improved to 0.57 with the proposed method.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1132",
      "author_area_id": "10",
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "haider21_interspeech": {
      "authors": [
        [
          "Fasih",
          "Haider"
        ],
        [
          "Saturnino",
          "Luz"
        ]
      ],
      "title": "Affect Recognition Through Scalogram and Multi-Resolution Cochleagram Features",
      "original": "1761",
      "page_count": 5,
      "order": 917,
      "p1": "4478",
      "pn": "4482",
      "abstract": [
        "An approach to the categorization of voice samples according to emotions\nexpressed by the speaker is proposed which uses Multi-Resolution Cochleagram\n(MRCG) and scalogram features in a novel way. Audio recordings from\nthe EmoDB, EMOVO and Savee Data-sets are employed in training and testing\nof predictive models consisting of different sets of speech features.\nThis study systematically evaluates the performance of the feature\nsets most commonly used in computational paralinguistic tasks (i.e.\n<i>emobase, eGeMAPS</i> and <i>ComParE</i>) in addition to MRCG- and\nscalogram-derived features and their fusion, across five different\nclassifiers. The datasets used in this evaluation include speech in\nthree different languages (German, Italian and English). MRCG features\noutperform the feature sets most commonly used in computational paralinguistic\ntasks, including <i>emobase, eGeMAPS</i> and <i>ComParE</i>, for the\nEmoDB (unweighted average recall, UAR = 59.15%) and SAVEE (UAR = 36.12%)\ndatasets, while <i>eGeMAPS</i> provides the best overall UAR (33.84%)\nfor the EMOVO dataset. A support vector machine (SVM) classifier yields\nthe best UAR for EmoDB (80.05%) through fusion of <i>emobase, eGeMAPS,\nComParE</i> and MRCG, and for EMOVO (40.31%), through fusion of <i>emobase</i>,\n<i>eGeMAPS</i> and <i>ComParE</i>. For SAVEE, random forests provide\nthe best result (46.55%) using the <i>ComParE</i> feature set.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1761",
      "author_area_id": "3",
      "author_area_label": "Paralinguistics in Speech and Language: Human and Automatic Analysis and Processing"
    },
    "liu21n_interspeech": {
      "authors": [
        [
          "Jiawang",
          "Liu"
        ],
        [
          "Haoxiang",
          "Wang"
        ]
      ],
      "title": "A Speech Emotion Recognition Framework for Better Discrimination of Confusions",
      "original": "0718",
      "page_count": 5,
      "order": 918,
      "p1": "4483",
      "pn": "4487",
      "abstract": [
        "Speech emotion recognition (SER) plays an important role in human-machine\ninteraction (HMI). Various methods have been proposed for the SER task.\nHowever, a common problem in most of the previous studies is some specific\nemotions are grossly misclassified. In this paper, we propose a novel\nSER framework aiming at discriminating the confusions by utilizing\ntriplet loss and data augmentation to enforce a CNN-LSTM model to emphasize\nmore on these emotions which are hard to be correctly classified. Ablation\nexperiments demonstrate the effectiveness of the proposed framework.\nOn Interactive Emotional Dyadic Motion Capture (IEMOCAP) dataset, our\nframework can achieve 79.52% of Weighted Accuracy (WA) and 78.30% of\nUnweighted Accuracy (UA). Compared to the other state-of-the-art models,\nour framework obtains more than 3.34% and 1.94% improvement on WA and\nUA respectively.\n"
      ],
      "doi": "10.21437/Interspeech.2021-718",
      "author_area_id": "3",
      "author_area_label": "Paralinguistics in Speech and Language: Human and Automatic Analysis and Processing"
    },
    "li21p_interspeech": {
      "authors": [
        [
          "Ruichen",
          "Li"
        ],
        [
          "Jinming",
          "Zhao"
        ],
        [
          "Qin",
          "Jin"
        ]
      ],
      "title": "Speech Emotion Recognition via Multi-Level Cross-Modal Distillation",
      "original": "0785",
      "page_count": 5,
      "order": 919,
      "p1": "4488",
      "pn": "4492",
      "abstract": [
        "Speech emotion recognition faces the problem that most of the existing\nspeech corpora are limited in scale and diversity due to the high annotation\ncost and label ambiguity. In this work, we explore the task of learning\nrobust speech emotion representations based on large unlabeled speech\ndata. Under a simple assumption that the internal emotional states\nacross different modalities are similar, we propose a method called\nMulti-level Cross-modal Emotion Distillation (MCED), which trains the\nspeech emotion model without any labeled speech emotion data by transferring\nemotion knowledge from a pretrained text emotion model. Extensive experiments\non two benchmark datasets, IEMOCAP and MELD, show that our proposed\nMCED can help learn effective speech emotion representations which\ngeneralize well on downstream speech emotion recognition tasks.\n"
      ],
      "doi": "10.21437/Interspeech.2021-785",
      "author_area_id": "3",
      "author_area_label": "Paralinguistics in Speech and Language: Human and Automatic Analysis and Processing"
    },
    "ito21_interspeech": {
      "authors": [
        [
          "Koichiro",
          "Ito"
        ],
        [
          "Takuya",
          "Fujioka"
        ],
        [
          "Qinghua",
          "Sun"
        ],
        [
          "Kenji",
          "Nagamatsu"
        ]
      ],
      "title": "Audio-Visual Speech Emotion Recognition by Disentangling Emotion and Identity Attributes",
      "original": "0809",
      "page_count": 5,
      "order": 920,
      "p1": "4493",
      "pn": "4497",
      "abstract": [
        "In this paper, we propose an audio-visual speech emotion recognition\n(AV-SER) that can suppress the disturbance from an identity attribute\nby disentangling an emotion attribute and an identity one. We developed\na model that first disentangles both attributes for each modality.\nIn order to achieve the disentanglement, we introduce a co-attention\nmodule to our model. Our model disentangles the emotion attribute by\ngiving the identity attribute as conditional features to the module.\nConversely, the identity attribute is also obtained with the emotion\nattribute as a condition. Our model then makes a prediction for each\nattribute from these disentangled features by considering both modalities.\nIn addition, to ensure the disentanglement capacity of our model, we\ntrain the model with an identification task as the auxiliary task and\nan SER task as the primary task alternately, and we update only the\npart of parameters responsible for each task. The experimental result\nshows the effectiveness of our method with the wild CMU-MOSEI dataset.\n"
      ],
      "doi": "10.21437/Interspeech.2021-809",
      "author_area_id": "3",
      "author_area_label": "Paralinguistics in Speech and Language: Human and Automatic Analysis and Processing"
    },
    "bose21_interspeech": {
      "authors": [
        [
          "Deboshree",
          "Bose"
        ],
        [
          "Vidhyasaharan",
          "Sethu"
        ],
        [
          "Eliathamby",
          "Ambikairajah"
        ]
      ],
      "title": "Parametric Distributions to Model Numerical Emotion Labels",
      "original": "1000",
      "page_count": 5,
      "order": 921,
      "p1": "4498",
      "pn": "4502",
      "abstract": [
        "It is common to represent emotional states as values on a set of numerical\nscales corresponding to attributes such as arousal and valence. Often\nthese labels are obtained from multiple annotators who record their\nperception of emotion in terms of these attributes. Combining these\nmultiple annotations by taking the mean, as is typical in affective\ncomputing systems ignores the inherent ambiguity in the labels. Recently\nit has been recognised that this ambiguity carries useful information\nand systems that employ distributions over the numerical scales to\nrepresent emotional states have been proposed. In this paper we show\nthat the common and widespread assumption that this distribution is\nGaussian may not be suitable since the underlying numerical scales\nare bounded. We then compare a range of well-known distributions defined\non bounded domains to ascertain which of them would be the most suitable\nalternative. Statistical measures are proposed to enable quantifiable\ncomparisons and the results are reported. All comparisons reported\nin the paper were carried out on the RECOLA dataset.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1000",
      "author_area_id": "3",
      "author_area_label": "Paralinguistics in Speech and Language: Human and Automatic Analysis and Processing"
    },
    "gao21e_interspeech": {
      "authors": [
        [
          "Yuan",
          "Gao"
        ],
        [
          "Jiaxing",
          "Liu"
        ],
        [
          "Longbiao",
          "Wang"
        ],
        [
          "Jianwu",
          "Dang"
        ]
      ],
      "title": "Metric Learning Based Feature Representation with Gated Fusion Model for Speech Emotion Recognition",
      "original": "1133",
      "page_count": 5,
      "order": 922,
      "p1": "4503",
      "pn": "4507",
      "abstract": [
        "Due to the lack of sufficient speech emotional data, the recognition\nperformance of existing speech emotion recognition (SER) approaches\nis relatively low and requires further improvement to meet the needs\nof real-life applications. For the problem of data scarcity, an increasingly\npopular solution is to transfer emotional information through pre-training\nmodels and extract additional features. However, the feature representation\nneeds further compression because the training object of unsupervised\nlearning is to reconstruct input, making the latent representation\ncontain non-affective information. In this paper, we introduce deep\nmetric learning to constrain the feature distribution of the pre-training\nmodel. Specifically, we propose a triplet loss to modify the representation\nextraction model as a pseudo-siamese network and achieve more efficient\nknowledge transfer for emotion recognition. Furthermore, we propose\na gated fusion method to learn the connection of features extracted\nfrom the pre-training model and supervised feature extraction model.\nWe conduct experiments on the common benchmarking dataset IEMOCAP to\nverify the performance of the proposed model. The experimental results\ndemonstrate the advantages of our model, outperforming the unsupervised\ntransfer learning system by 3.7% and 3.88% in weighted accuracy and\nunweighted accuracy, respectively.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1133",
      "author_area_id": "3",
      "author_area_label": "Paralinguistics in Speech and Language: Human and Automatic Analysis and Processing"
    },
    "cai21b_interspeech": {
      "authors": [
        [
          "Xingyu",
          "Cai"
        ],
        [
          "Jiahong",
          "Yuan"
        ],
        [
          "Renjie",
          "Zheng"
        ],
        [
          "Liang",
          "Huang"
        ],
        [
          "Kenneth",
          "Church"
        ]
      ],
      "title": "Speech Emotion Recognition with Multi-Task Learning",
      "original": "1852",
      "page_count": 5,
      "order": 923,
      "p1": "4508",
      "pn": "4512",
      "abstract": [
        "Speech emotion recognition (SER) classifies speech into emotion categories\nsuch as: <i>Happy, Angry, Sad</i> and <i>Neutral</i>. Recently, deep\nlearning has been applied to the SER task. This paper proposes a multi-task\nlearning (MTL) framework to simultaneously perform speech-to-text recognition\nand emotion classification, with an end-to-end deep neural model based\non wav2vec-2.0. Experiments on the IEMOCAP benchmark show that the\nproposed method achieves the state-of-the-art performance on the SER\ntask. In addition, an ablation study establishes the effectiveness\nof the proposed MTL framework.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1852",
      "author_area_id": "3",
      "author_area_label": "Paralinguistics in Speech and Language: Human and Automatic Analysis and Processing"
    },
    "seneviratne21b_interspeech": {
      "authors": [
        [
          "Nadee",
          "Seneviratne"
        ],
        [
          "Carol",
          "Espy-Wilson"
        ]
      ],
      "title": "Generalized Dilated CNN Models for Depression Detection Using Inverted Vocal Tract Variables",
      "original": "1960",
      "page_count": 5,
      "order": 924,
      "p1": "4513",
      "pn": "4517",
      "abstract": [
        "Depression detection using vocal biomarkers is a highly researched\narea. Articulatory coordination features (ACFs) are developed based\non the changes in neuromotor coordination due to psychomotor slowing,\na key feature of Major Depressive Disorder. However findings of existing\nstudies are mostly validated on a single database which limits the\ngeneralizability of results. Variability across different depression\ndatabases adversely affects the results in cross corpus evaluations\n(CCEs). We propose to develop a generalized classifier for depression\ndetection using a dilated Convolutional Neural Network which is trained\non ACFs extracted from two depression databases. We show that ACFs\nderived from Vocal Tract Variables (TVs) show promise as a robust set\nof features for depression detection. Our model achieves relative accuracy\nimprovements of &#126;10% compared to CCEs performed on models trained\non a single database. We extend the study to show that fusing TVs and\nMel-Frequency Cepstral Coefficients can further improve the performance\nof this classifier.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1960",
      "author_area_id": "3",
      "author_area_label": "Paralinguistics in Speech and Language: Human and Automatic Analysis and Processing"
    },
    "wang21ga_interspeech": {
      "authors": [
        [
          "Yuhua",
          "Wang"
        ],
        [
          "Guang",
          "Shen"
        ],
        [
          "Yuezhu",
          "Xu"
        ],
        [
          "Jiahang",
          "Li"
        ],
        [
          "Zhengdao",
          "Zhao"
        ]
      ],
      "title": "Learning Mutual Correlation in Multimodal Transformer for Speech Emotion Recognition",
      "original": "2004",
      "page_count": 5,
      "order": 925,
      "p1": "4518",
      "pn": "4522",
      "abstract": [
        "Various studies have confirmed the necessity and benefits of leveraging\nmultimodal features for SER, and the latest research results show that\nthe temporal information captured by the transformer is very useful\nfor improving multimodal speech emotion recognition. However, the dependency\nbetween different modalities and high-level temporal-feature learning\nusing a deeper transformer is yet to be investigated. Thus, we propose\na multimodal transformer with sharing weights for speech emotion recognition.\nThe proposed network shares the weights across the modalities in each\ntransformer layer to learn the correlation among multiple modalities.\nIn addition, since the emotion contained in a speech generally include\naudio and text features, both of which have not only internal dependence\nbut also mutual dependence, we design a deep multimodal attention mechanism\nto capture these two kinds of emotional dependence. We evaluated our\nmodel on the publicly available IEMOCAP dataset. The experimental results\ndemonstrate that the proposed model yielded a promising result.\n"
      ],
      "doi": "10.21437/Interspeech.2021-2004",
      "author_area_id": "3",
      "author_area_label": "Paralinguistics in Speech and Language: Human and Automatic Analysis and Processing"
    },
    "liu21o_interspeech": {
      "authors": [
        [
          "Jiaxing",
          "Liu"
        ],
        [
          "Yaodong",
          "Song"
        ],
        [
          "Longbiao",
          "Wang"
        ],
        [
          "Jianwu",
          "Dang"
        ],
        [
          "Ruiguo",
          "Yu"
        ]
      ],
      "title": "Time-Frequency Representation Learning with Graph Convolutional Network for Dialogue-Level Speech Emotion Recognition",
      "original": "2067",
      "page_count": 5,
      "order": 926,
      "p1": "4523",
      "pn": "4527",
      "abstract": [
        "With the development of speech emotion recognition (SER), dialogue-level\nSER (DSER) is more aligned with actual scenarios. In this paper, we\npropose a DSER approach that includes two stages of representation\nlearning: intra-utterance representation learning and inter-utterance\nrepresentation learning. In the intra-utterance representation learning\nstage, traditional convolutional neural network (CNN) has demonstrated\ngreat success. However, the basic design of a CNN restricts its ability\nto model the local and global information in the spectrogram. Therefore,\nwe propose a novel local-global representation learning method for\nthe intra-utterance stage. The local information is learned by a time-frequency\nconvolutional neural network (TFCNN), which we published previously.\nHere, we propose a time-frequency capsule neural network (TFCap) to\nmodel global information that can extract more stable global time-frequency\ninformation directly from spectrograms. In the inter-utterance stage,\na graph convolutional network (GCN) is introduced to explore the relations\nbetween utterances in a dialog. Our proposed methods were evaluated\non the IEMOCAP database. The proposed time-frequency based method in\nthe intra-utterance stage achieves an absolute increase of 9.35% compared\nto CNN. By integrating GCN in the inter-utterance stage, the proposed\napproach achieves an absolute increase of 4.05% compared to the model\nin the previous stage.\n"
      ],
      "doi": "10.21437/Interspeech.2021-2067",
      "author_area_id": "3",
      "author_area_label": "Paralinguistics in Speech and Language: Human and Automatic Analysis and Processing"
    },
    "mordido21_interspeech": {
      "authors": [
        [
          "Gon\u00e7alo",
          "Mordido"
        ],
        [
          "Matthijs",
          "Van keirsbilck"
        ],
        [
          "Alexander",
          "Keller"
        ]
      ],
      "title": "Compressing 1D Time-Channel Separable Convolutions Using Sparse Random Ternary Matrices",
      "original": "0141",
      "page_count": 5,
      "order": 927,
      "p1": "4528",
      "pn": "4532",
      "abstract": [
        "We demonstrate that 1&#215;1-convolutions in 1D time-channel separable\nconvolutions may be replaced by constant, sparse random ternary matrices\nwith weights in -1, 0, +1. Such layers do not perform any multiplications\nand do not require training. Moreover, the matrices may be generated\non the chip during computation and therefore do not require any memory\naccess. With the same parameter budget, we can afford deeper and more\nexpressive models, improving the Pareto frontiers of existing models\non several tasks. For command recognition on Google Speech Commands\nv1, we improve the state-of-the-art accuracy from 97.21% to 97.41%\nat the same network size. Alternatively, we can lower the cost of existing\nmodels. For speech recognition on Librispeech, we halve the number\nof weights to be trained while only sacrificing about 1% of the floating-point\nbaseline&#8217;s word error rate.\n"
      ],
      "doi": "10.21437/Interspeech.2021-141",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "cheng21c_interspeech": {
      "authors": [
        [
          "Mengli",
          "Cheng"
        ],
        [
          "Chengyu",
          "Wang"
        ],
        [
          "Jun",
          "Huang"
        ],
        [
          "Xiaobo",
          "Wang"
        ]
      ],
      "title": "Weakly Supervised Construction of ASR Systems from Massive Video Data",
      "original": "0007",
      "page_count": 5,
      "order": 928,
      "p1": "4533",
      "pn": "4537",
      "abstract": [
        "Despite the rapid development of deep learning models, for real-world\napplications, building large-scale Automatic Speech Recognition (ASR)\nsystems from scratch is still significantly challenging, mostly due\nto the time-consuming and financially-expensive process of annotating\na large amount of audio data with transcripts. Although several self-supervised\npre-training models have been proposed to learn speech representations,\napplying such models directly might be sub-optimal if more labeled,\ntraining data could be obtained without a large cost.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  In this paper, we\npresent VideoASR, a weakly supervised framework for constructing ASR\nsystems from massive video data. As user-generated videos often contain\nhuman-speech audio roughly aligned with subtitles, we consider videos\nas an important knowledge source, and propose an effective approach\nto extract high-quality audio aligned with transcripts from videos\nbased on text detection and Optical Character Recognition. The underlying\nASR models can be fine-tuned to fit any domain-specific target training\ndatasets after weakly supervised pre-training on automatically generated\ndatasets. Extensive experiments show that VideoASR can easily produce\nstate-of-the-art results on six public datasets for Mandarin speech\nrecognition. In addition, the VideoASR framework has been deployed\non the cloud to support various industrial-scale applications.\n"
      ],
      "doi": "10.21437/Interspeech.2021-7",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "kim21l_interspeech": {
      "authors": [
        [
          "Byeonggeun",
          "Kim"
        ],
        [
          "Simyung",
          "Chang"
        ],
        [
          "Jinkyu",
          "Lee"
        ],
        [
          "Dooyong",
          "Sung"
        ]
      ],
      "title": "Broadcasted Residual Learning for Efficient Keyword Spotting",
      "original": "0383",
      "page_count": 5,
      "order": 929,
      "p1": "4538",
      "pn": "4542",
      "abstract": [
        "Keyword spotting is an important research field because it plays a\nkey role in device wake-up and user interaction on smart devices. However,\nit is challenging to minimize errors while operating efficiently in\ndevices with limited resources such as mobile phones. We present a\n<i>broadcasted residual learning</i> method to achieve high accuracy\nwith small model size and computational load. Our method configures\nmost of the residual functions as 1D temporal convolution while still\nallows 2D convolution together using a broadcasted-residual connection\nthat expands temporal output to frequency-temporal dimension. This\nresidual mapping enables the network to effectively represent useful\naudio features with much less computation than conventional convolutional\nneural networks. We also propose a novel network architecture, Broadcasting-residual\nnetwork (BC-ResNet), based on broadcasted residual learning and describe\nhow to scale up the model according to the target device&#8217;s resources.\nBC-ResNets achieve state-of-the-art 98.0% and 98.7% top-1 accuracy\non Google speech command datasets v1 and v2, respectively, and consistently\noutperform previous approaches, using fewer computations and parameters.\n"
      ],
      "doi": "10.21437/Interspeech.2021-383",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "swaminathan21_interspeech": {
      "authors": [
        [
          "Rupak Vignesh",
          "Swaminathan"
        ],
        [
          "Brian",
          "King"
        ],
        [
          "Grant P.",
          "Strimel"
        ],
        [
          "Jasha",
          "Droppo"
        ],
        [
          "Athanasios",
          "Mouchtaris"
        ]
      ],
      "title": "CoDERT: Distilling Encoder Representations with Co-Learning for Transducer-Based Speech Recognition",
      "original": "0797",
      "page_count": 5,
      "order": 930,
      "p1": "4543",
      "pn": "4547",
      "abstract": [
        "We propose a simple yet effective method to compress an RNN-Transducer\n(RNN-T) through the well-known knowledge distillation paradigm. We\nshow that the transducer&#8217;s encoder outputs naturally have a high\nentropy and contain rich information about acoustically similar word-piece\nconfusions. This rich information is suppressed when combined with\nthe lower entropy decoder outputs to produce the joint network logits.\nConsequently, we introduce an auxiliary loss to distill the encoder\nlogits from a teacher transducer&#8217;s encoder, and explore training\nstrategies where this encoder distillation works effectively. We find\nthat tandem training of teacher and student encoders with an inplace\nencoder distillation outperforms the use of a pre-trained and static\nteacher transducer. We also report an interesting phenomenon we refer\nto as implicit distillation, that occurs when the teacher and student\nencoders share the same decoder. Our experiments show 5.37&#8211;8.4%\nrelative word error rate reductions (WERR) on in-house test sets, and\n5.05&#8211;6.18% relative WERRs on LibriSpeech test sets.\n"
      ],
      "doi": "10.21437/Interspeech.2021-797",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "gao21f_interspeech": {
      "authors": [
        [
          "Zhifu",
          "Gao"
        ],
        [
          "Yiwu",
          "Yao"
        ],
        [
          "Shiliang",
          "Zhang"
        ],
        [
          "Jun",
          "Yang"
        ],
        [
          "Ming",
          "Lei"
        ],
        [
          "Ian",
          "McLoughlin"
        ]
      ],
      "title": "Extremely Low Footprint End-to-End ASR System for Smart Device",
      "original": "0819",
      "page_count": 5,
      "order": 931,
      "p1": "4548",
      "pn": "4552",
      "abstract": [
        "Recently, end-to-end (E2E) speech recognition has become popular, since\nit can integrate the acoustic, pronunciation and language models into\na single neural network, which outperforms conventional models. Among\nE2E approaches, attention-based models, e.g. Transformer, have emerged\nas being superior. Such models have opened the door to deployment of\nASR on smart devices, however they still suffer from requiring a large\nnumber of model parameters. We propose an extremely low footprint E2E\nASR system for smart devices, to achieve the goal of satisfying resource\nconstraints without sacrificing recognition accuracy. We design cross-layer\nweight sharing to improve parameter efficiency and further exploit\nmodel compression methods including sparsification and quantization,\nto reduce memory storage and boost decoding efficiency. We evaluate\nour approaches on the public AISHELL-1 and AISHELL-2 benchmarks. On\nthe AISHELL-2 task, the proposed method achieves more than 10&#215;\ncompression (model size reduces from 248 to 24MB), at the cost of only\nminor performance loss (CER reduces from 6.49% to 6.92%).\n"
      ],
      "doi": "10.21437/Interspeech.2021-819",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "shangguan21_interspeech": {
      "authors": [
        [
          "Yuan",
          "Shangguan"
        ],
        [
          "Rohit",
          "Prabhavalkar"
        ],
        [
          "Hang",
          "Su"
        ],
        [
          "Jay",
          "Mahadeokar"
        ],
        [
          "Yangyang",
          "Shi"
        ],
        [
          "Jiatong",
          "Zhou"
        ],
        [
          "Chunyang",
          "Wu"
        ],
        [
          "Duc",
          "Le"
        ],
        [
          "Ozlem",
          "Kalinli"
        ],
        [
          "Christian",
          "Fuegen"
        ],
        [
          "Michael L.",
          "Seltzer"
        ]
      ],
      "title": "Dissecting User-Perceived Latency of On-Device E2E Speech Recognition",
      "original": "1887",
      "page_count": 5,
      "order": 932,
      "p1": "4553",
      "pn": "4557",
      "abstract": [
        "As speech-enabled devices such as smartphones and smart speakers become\nincreasingly ubiquitous, there is growing interest in building automatic\nspeech recognition (ASR) systems that can run directly on-device; end-to-end\n(E2E) speech recognition models such as recurrent neural network transducers\nand their variants have recently emerged as prime candidates for this\ntask. Apart from being accurate and compact, such systems need to decode\nspeech with low user-perceived latency (UPL), producing words as soon\nas they are spoken. This work examines the impact of various techniques\n&#8212; model architectures, training criteria, decoding hyperparameters,\nand endpointer parameters &#8212; on UPL. Our analyses suggest that\nmeasures of model size (parameters, input chunk sizes), or measures\nof computation (e.g., FLOPS, RTF) that reflect the model&#8217;s ability\nto process input frames are not always strongly correlated with observed\nUPL. Thus, conventional algorithmic latency measurements might be inadequate\nin accurately capturing latency observed when models are deployed on\nembedded devices. Instead, we find that factors affecting token emission\nlatency, and endpointing behavior have a larger impact on UPL. We achieve\nthe best trade-off between latency and word error rate when performing\nASR jointly with endpointing, while utilizing the recently proposed\nalignment regularization mechanism.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1887",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "macoskey21b_interspeech": {
      "authors": [
        [
          "Jonathan",
          "Macoskey"
        ],
        [
          "Grant P.",
          "Strimel"
        ],
        [
          "Jinru",
          "Su"
        ],
        [
          "Ariya",
          "Rastrow"
        ]
      ],
      "title": "Amortized Neural Networks for Low-Latency Speech Recognition",
      "original": "0712",
      "page_count": 5,
      "order": 933,
      "p1": "4558",
      "pn": "4562",
      "abstract": [
        "We introduce Amortized Neural Networks (AmNets), a compute cost- and\nlatency-aware network architecture particularly well-suited for sequence\nmodeling tasks. We apply AmNets to the Recurrent Neural Network Transducer\n(RNN-T) to reduce compute cost and latency for an automatic speech\nrecognition (ASR) task. The AmNets RNN-T architecture enables the network\nto dynamically switch between encoder branches on a frame-by-frame\nbasis. Branches are constructed with variable levels of compute cost\nand model capacity. Here, we achieve variable compute for two well-known\ncandidate techniques: one using sparse pruning and the other using\nmatrix factorization. Frame-by-frame switching is determined by an\narbitrator network that requires negligible compute overhead. We present\nresults using both architectures on LibriSpeech data and show that\nour proposed architecture can reduce inference cost by up to 45% and\nlatency to nearly real-time without incurring a loss in accuracy.\n"
      ],
      "doi": "10.21437/Interspeech.2021-712",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "botros21_interspeech": {
      "authors": [
        [
          "Rami",
          "Botros"
        ],
        [
          "Tara N.",
          "Sainath"
        ],
        [
          "Robert",
          "David"
        ],
        [
          "Emmanuel",
          "Guzman"
        ],
        [
          "Wei",
          "Li"
        ],
        [
          "Yanzhang",
          "He"
        ]
      ],
      "title": "Tied &amp; Reduced RNN-T Decoder",
      "original": "0212",
      "page_count": 5,
      "order": 934,
      "p1": "4563",
      "pn": "4567",
      "abstract": [
        "Previous works on the Recurrent Neural Network-Transducer (RNN-T) models\nhave shown that, under some conditions, it is possible to simplify\nits prediction network with little or no loss in recognition accuracy\n[1, 2, 3]. This is done by limiting the context size of previous labels\nand/or using a simpler architecture for its layers instead of LSTMs.\nThe benefits of such changes include reduction in model size, faster\ninference and power savings, which are all useful for on-device applications.<br\nstyle=\"mso-data-placement:same-cell;\" /><br style=\"mso-data-placement:same-cell;\"\n/>\n  In this work, we study ways to make the RNN-T decoder (prediction\nnetwork + joint network) smaller and faster without degradation in\nrecognition performance. Our prediction network performs a simple weighted\naveraging of the input embeddings, and shares its embedding matrix\nweights with the joint network&#8217;s output layer (a.k.a. weight\ntying, commonly used in language modeling [4]). This simple design,\nwhen used in conjunction with additional Edit-based Minimum Bayes Risk\n(EMBR) training, reduces the RNN-T Decoder from 23M parameters to just\n2M, without affecting word-error rate (WER).\n"
      ],
      "doi": "10.21437/Interspeech.2021-212"
    },
    "kim21m_interspeech": {
      "authors": [
        [
          "Jangho",
          "Kim"
        ],
        [
          "Simyung",
          "Chang"
        ],
        [
          "Nojun",
          "Kwak"
        ]
      ],
      "title": "PQK: Model Compression via Pruning, Quantization, and Knowledge Distillation",
      "original": "0248",
      "page_count": 5,
      "order": 935,
      "p1": "4568",
      "pn": "4572",
      "abstract": [
        "As edge devices become prevalent, deploying Deep Neural Networks (DNN)\non edge devices has become a critical issue. However, DNN requires\na high computational resource which is rarely available for edge devices.\nTo handle this, we propose a novel model compression method for the\ndevices with limited computational resources, called <i>PQK</i> consisting\nof pruning, quantization, and knowledge distillation (KD) processes.\nUnlike traditional pruning and KD, PQK makes use of unimportant weights\npruned in the pruning process to make a teacher network for training\na better student network without pre-training the teacher model. PQK\nhas two phases. Phase 1 exploits iterative pruning and quantization-aware\ntraining to make a lightweight and power-efficient model. In phase\n2, we make a teacher network by adding unimportant weights unused in\nphase 1 to a pruned network. By using this teacher network, we train\nthe pruned network as a student network. In doing so, we do not need\na pre-trained teacher network for the KD framework because the teacher\nand the student networks coexist within the same network (See Fig.\n1). We apply our method to the recognition model and verify the effectiveness\nof PQK on keyword spotting (KWS) and image recognition.\n"
      ],
      "doi": "10.21437/Interspeech.2021-248",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "nagaraja21_interspeech": {
      "authors": [
        [
          "Varun",
          "Nagaraja"
        ],
        [
          "Yangyang",
          "Shi"
        ],
        [
          "Ganesh",
          "Venkatesh"
        ],
        [
          "Ozlem",
          "Kalinli"
        ],
        [
          "Michael L.",
          "Seltzer"
        ],
        [
          "Vikas",
          "Chandra"
        ]
      ],
      "title": "Collaborative Training of Acoustic Encoders for Speech Recognition",
      "original": "0354",
      "page_count": 5,
      "order": 936,
      "p1": "4573",
      "pn": "4577",
      "abstract": [
        "On-device speech recognition requires training models of different\nsizes for deploying on devices with various computational budgets.\nWhen building such different models, we can benefit from training them\njointly to take advantage of the knowledge shared between them. Joint\ntraining is also efficient since it reduces the redundancy in the training\nprocedure&#8217;s data handling operations. We propose a method for\ncollaboratively training acoustic encoders of different sizes for speech\nrecognition. We use a sequence transducer setup where different acoustic\nencoders share a common predictor and joiner modules. The acoustic\nencoders are also trained using co-distillation through an auxiliary\ntask for frame level chenone prediction, along with the transducer\nloss. We perform experiments using the LibriSpeech corpus and demonstrate\nthat the collaboratively trained acoustic encoders can provide up to\na 11% relative improvement in the word error rate on both the test\npartitions.\n"
      ],
      "doi": "10.21437/Interspeech.2021-354",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "wang21ha_interspeech": {
      "authors": [
        [
          "Xiong",
          "Wang"
        ],
        [
          "Sining",
          "Sun"
        ],
        [
          "Lei",
          "Xie"
        ],
        [
          "Long",
          "Ma"
        ]
      ],
      "title": "Efficient Conformer with Prob-Sparse Attention Mechanism for End-to-End Speech Recognition",
      "original": "0415",
      "page_count": 5,
      "order": 937,
      "p1": "4578",
      "pn": "4582",
      "abstract": [
        "End-to-end models are favored in automatic speech recognition (ASR)\nbecause of their simplified system structure and superior performance.\nAmong these models, Transformer and Conformer have achieved state-of-the-art\nrecognition accuracy in which self-attention plays a vital role in\ncapturing important global information. However, the time and memory\ncomplexity of self-attention increases squarely with the length of\nthe sentence. In this paper, a prob-sparse self-attention mechanism\nis introduced into Conformer to sparse the computing process of self-attention\nin order to accelerate inference speed and reduce space consumption.\nSpecifically, we adopt a Kullback-Leibler divergence based sparsity\nmeasurement for each query to decide whether we compute the attention\nfunction on this query. By using the prob-sparse attention mechanism,\nwe achieve impressively 8% to 45% inference speed-up and 15% to 45%\nmemory usage reduction of the self-attention module of Conformer Transducer\nwhile maintaining the same level of error rate.\n"
      ],
      "doi": "10.21437/Interspeech.2021-415",
      "author_area_id": "9",
      "author_area_label": "Speech Recognition: Architecture, Search, and Linguistic Components"
    },
    "parcollet21_interspeech": {
      "authors": [
        [
          "Titouan",
          "Parcollet"
        ],
        [
          "Mirco",
          "Ravanelli"
        ]
      ],
      "title": "The Energy and Carbon Footprint of Training End-to-End Speech Recognizers",
      "original": "0456",
      "page_count": 5,
      "order": 938,
      "p1": "4583",
      "pn": "4587",
      "abstract": [
        "Deep learning contributes to reaching higher levels of artificial intelligence.\nDue to its pervasive adoption, however, growing concerns on the environmental\nimpact of this technology have been raised. In particular, the energy\nconsumed at training and inference time by modern neural networks is\nfar from being negligible and will increase even further due to the\ndeployment of ever larger models.<br style=\"mso-data-placement:same-cell;\"\n/><br style=\"mso-data-placement:same-cell;\" />\n  This work investigates\nfor the first time the carbon cost of end-to-end automatic speech recognition\n(ASR). First, it quantifies the amount of CO<SUB>2</SUB> emitted while\ntraining state-of-the-art (SOTA) ASR systems on a university-scale\ncluster. Then, it shows that a tiny performance improvement comes at\nan extremely high carbon cost. For instance, the conducted experiments\nreveal that a SOTA Transformer emits 50% of its total training released\nCO<SUB>2</SUB> solely to achieve a final decrease of 0.3 of the word\nerror rate. With this study, we hope to raise awareness on this crucial\ntopic and we provide guidelines, insights, and estimates enabling researchers\nto better assess the environmental impact of training speech technologies.\n"
      ],
      "doi": "10.21437/Interspeech.2021-456",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation"
    },
    "chen21v_interspeech": {
      "authors": [
        [
          "Long",
          "Chen"
        ],
        [
          "Venkatesh",
          "Ravichandran"
        ],
        [
          "Andreas",
          "Stolcke"
        ]
      ],
      "title": "Graph-Based Label Propagation for Semi-Supervised Speaker Identification",
      "original": "1209",
      "page_count": 5,
      "order": 939,
      "p1": "4588",
      "pn": "4592",
      "abstract": [
        "Speaker identification in the household scenario (e.g., for smart speakers)\nis typically based on only a few enrollment utterances but a much larger\nset of unlabeled data, suggesting semi-supervised learning to improve\nspeaker profiles. We propose a graph-based semi-supervised learning\napproach for speaker identification in the household scenario, to leverage\nthe unlabeled speech samples. In contrast to most of the works in speaker\nrecognition that focus on speaker-discriminative embeddings, this work\nfocuses on speaker label inference (scoring). Given a pre-trained embedding\nextractor, graph-based learning allows us to integrate information\nabout both labeled and unlabeled utterances. Considering each utterance\nas a graph node, we represent pairwise utterance similarity scores\nas edge weights. Graphs are constructed per household, and speaker\nidentities are propagated to unlabeled nodes to optimize a global consistency\ncriterion. We show in experiments on the VoxCeleb dataset that this\napproach makes effective use of unlabeled data and improves speaker\nidentification accuracy compared to two state-of-the-art scoring methods\nas well as their semi-supervised variants based on pseudo-labels.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1209",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "li21q_interspeech": {
      "authors": [
        [
          "Ruirui",
          "Li"
        ],
        [
          "Chelsea J.-T.",
          "Ju"
        ],
        [
          "Zeya",
          "Chen"
        ],
        [
          "Hongda",
          "Mao"
        ],
        [
          "Oguz",
          "Elibol"
        ],
        [
          "Andreas",
          "Stolcke"
        ]
      ],
      "title": "Fusion of Embeddings Networks for Robust Combination of Text Dependent and Independent Speaker Recognition",
      "original": "0003",
      "page_count": 5,
      "order": 940,
      "p1": "4593",
      "pn": "4597",
      "abstract": [
        "By implicitly recognizing a user based on his/her speech input, speaker\nidentification enables many downstream applications, such as personalized\nsystem behavior and expedited shopping checkouts. Based on whether\nthe speech content is constrained or not, both text-dependent (TD)\nand text-independent (TI) speaker recognition models may be used. We\nwish to combine the advantages of both types of models through an ensemble\nsystem to make more reliable predictions. However, any such combined\napproach has to be robust to incomplete inputs, i.e., when either TD\nor TI input is missing. As a solution we propose a fusion of embeddings\nnetwork ( foenet) architecture, combining joint learning with neural\nattention. We compare  foenet with four competitive baseline methods\non a dataset of voice assistant inputs, and show that it achieves higher\naccuracy than the baseline and score fusion methods, especially in\nthe presence of incomplete inputs.\n"
      ],
      "doi": "10.21437/Interspeech.2021-3",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "cumani21_interspeech": {
      "authors": [
        [
          "Sandro",
          "Cumani"
        ],
        [
          "Salvatore",
          "Sarni"
        ]
      ],
      "title": "A Generative Model for Duration-Dependent Score Calibration",
      "original": "0114",
      "page_count": 5,
      "order": 941,
      "p1": "4598",
      "pn": "4602",
      "abstract": [
        "In this work we introduce a generative score calibration model for\nspeaker verification systems able to explicitly account for utterance-dependent\nmiscalibration sources, with a focus on segment duration. The model\nis theoretically motivated by an analysis of the effects of distribution\nmismatch on the scores produced by Probabilistic Linear Discriminant\nAnalysis (PLDA), and extends our previous investigation on the distribution\nof well-calibrated PLDA log-likelihood ratios. We characterize target\nand non-target scores by means of Variance-Gamma densities, whose parameters\nrepresent effective between and within-class variabilities. Experimental\nresults on SRE 2019 show that the proposed method improves both calibration\nand verification accuracy with respect to duration-agnostic models\nand to duration-aware discriminative methods.\n"
      ],
      "doi": "10.21437/Interspeech.2021-114",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "pelecanos21_interspeech": {
      "authors": [
        [
          "Jason",
          "Pelecanos"
        ],
        [
          "Quan",
          "Wang"
        ],
        [
          "Ignacio Lopez",
          "Moreno"
        ]
      ],
      "title": "Dr-Vectors: Decision Residual Networks and an Improved Loss for Speaker Recognition",
      "original": "0641",
      "page_count": 5,
      "order": 942,
      "p1": "4603",
      "pn": "4607",
      "abstract": [
        "Many neural network speaker recognition systems model each speaker\nusing a fixed-dimensional embedding vector. These embeddings are generally\ncompared using either linear or 2nd-order scoring and, until recently,\ndo not handle utterance-specific uncertainty. In this work we propose\nscoring these representations in a way that can capture uncertainty,\nenroll/test asymmetry and additional non-linear information. This is\nachieved by incorporating a 2nd-stage neural network (known as a decision\nnetwork) as part of an end-to-end training regimen. In particular,\nwe propose the concept of decision residual networks which involves\nthe use of a compact decision network to leverage cosine scores and\nto model the residual signal that&#8217;s needed. Additionally, we\npresent a modification to the generalized end-to-end softmax loss function\nto target the separation of same/different speaker scores. We observed\nsignificant performance gains for the two techniques.\n"
      ],
      "doi": "10.21437/Interspeech.2021-641",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "kataria21b_interspeech": {
      "authors": [
        [
          "Saurabh",
          "Kataria"
        ],
        [
          "Shi-Xiong",
          "Zhang"
        ],
        [
          "Dong",
          "Yu"
        ]
      ],
      "title": "Multi-Channel Speaker Verification for Single and Multi-Talker Speech",
      "original": "0681",
      "page_count": 5,
      "order": 943,
      "p1": "4608",
      "pn": "4612",
      "abstract": [
        "To improve speaker verification in real scenarios with interference\nspeakers, noise, and reverberation, we propose to bring together advancements\nmade in multi-channel speech features. Specifically, we combine <i>spectral</i>,\n<i>spatial</i>, and <i>directional</i> features, which includes inter-channel\nphase difference, multichannel <i>sinc</i> convolutions, directional\npower ratio features, and angle features. To maximally leverage supervised\nlearning, our framework is also equipped with multi-channel speech\nenhancement and voice activity detection. On all simulated, replayed,\nand real recordings, we observe large and consistent improvements at\nvarious degradation levels. On real recordings of multi-talker speech,\nwe achieve a 36% relative reduction in equal error rate w.r.t. single-channel\nbaseline. We find the improvements from speaker-dependent <i>directional</i>\nfeatures more consistent in multi-talker conditions than clean. Lastly,\nwe investigate if the learned multi-channel speaker embedding space\ncan be made more discriminative through a contrastive loss-based fine-tuning.\nWith a simple choice of Triplet loss, we observe a further 8.3% relative\nreduction in EER.\n"
      ],
      "doi": "10.21437/Interspeech.2021-681",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "padfield21_interspeech": {
      "authors": [
        [
          "Dirk",
          "Padfield"
        ],
        [
          "Daniel J.",
          "Liebling"
        ]
      ],
      "title": "Chronological Self-Training for Real-Time Speaker Diarization",
      "original": "0822",
      "page_count": 5,
      "order": 944,
      "p1": "4613",
      "pn": "4617",
      "abstract": [
        "Diarization partitions an audio stream into segments based on the voices\nof the speakers. Real-time diarization systems that include an enrollment\nstep should limit enrollment training samples to reduce user interaction\ntime. Although training on a small number of samples yields poor performance,\nwe show that the accuracy can be improved dramatically using a chronological\nself-training approach. We studied the tradeoff between training time\nand classification performance and found that 1 second is sufficient\nto reach over 95% accuracy. We evaluated on 700 audio conversation\nfiles of about 10 minutes each from 6 different languages and demonstrated\naverage diarization error rates as low as 10%.\n"
      ],
      "doi": "10.21437/Interspeech.2021-822",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "xiao21b_interspeech": {
      "authors": [
        [
          "Runqiu",
          "Xiao"
        ],
        [
          "Xiaoxiao",
          "Miao"
        ],
        [
          "Wenchao",
          "Wang"
        ],
        [
          "Pengyuan",
          "Zhang"
        ],
        [
          "Bin",
          "Cai"
        ],
        [
          "Liuping",
          "Luo"
        ]
      ],
      "title": "Adaptive Margin Circle Loss for Speaker Verification",
      "original": "1043",
      "page_count": 5,
      "order": 945,
      "p1": "4618",
      "pn": "4622",
      "abstract": [
        "Deep-Neural-Network (DNN) based speaker verification systems use the\nangular softmax loss with margin penalties to enhance the intra-class\ncompactness of speaker embeddings, which achieved remarkable performance.\nIn this paper, we propose a novel angular loss function called adaptive\nmargin circle loss for speaker verification. The stage-based margin\nand chunk-based margin are applied to improve the angular discrimination\nof circle loss on the training set. The analysis on gradients shows\nthat, compared with the previous angular loss like Additive Margin\nSoftmax(Am-Softmax), circle loss has flexible optimization and definite\nconvergence status. Experiments are carried out on the Voxceleb and\nSITW. By applying adaptive margin circle loss, our best system achieves\n1.31%EER on Voxceleb1 and 2.13% on SITW core-core.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1043",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "obrien21b_interspeech": {
      "authors": [
        [
          "Benjamin",
          "O\u2019Brien"
        ],
        [
          "Christine",
          "Meunier"
        ],
        [
          "Alain",
          "Ghio"
        ]
      ],
      "title": "Presentation Matters: Evaluating Speaker Identification Tasks",
      "original": "1211",
      "page_count": 5,
      "order": 946,
      "p1": "4623",
      "pn": "4627",
      "abstract": [
        "This paper details our evaluations and comparisons of speaker identification\n(SID) performance by listeners across different tasks. Experiment 1\nparticipants completed traditional target-lineup (1-out-of-N speakers\nor out-of-set speaker) and binary (speaker verification) tasks. Experiment\n2 participants completed trials online by using a <i>clustering</i>\nmethod by grouping speech recordings into speaker-specific clusters.\nBoth studies employed similar speech recordings from the PTSVOX corpus.\nOur results showed participants who completed the binary and clustering\ntasks had higher accuracy than those who completed the target-lineup\ntask. We also observed that independent of the tasks participants found\nsome speakers significantly more difficult to identify relative to\ntheir foils. Pearson correlation procedures showed significant negative\ncorrelations between accuracy and task-dependent temporal-based metrics\nacross tasks, where an increase in time required to make determinations\nyielded a decrease in perceptual SID performance. These findings underscored\nthe important role of SID task design and the process of selecting\nspeech recordings. Future work aims to examine the relationship between\ndifferent perceptual SID task performances and scores generated by\nautomatic speaker verification systems.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1211",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "tong21_interspeech": {
      "authors": [
        [
          "Fuchuan",
          "Tong"
        ],
        [
          "Yan",
          "Liu"
        ],
        [
          "Song",
          "Li"
        ],
        [
          "Jie",
          "Wang"
        ],
        [
          "Lin",
          "Li"
        ],
        [
          "Qingyang",
          "Hong"
        ]
      ],
      "title": "Automatic Error Correction for Speaker Embedding Learning with Noisy Labels",
      "original": "2021",
      "page_count": 5,
      "order": 947,
      "p1": "4628",
      "pn": "4632",
      "abstract": [
        "Despite the superior performance deep neural networks have achieved\nin speaker verification tasks, much of their success benefits from\nthe availability of large-scale and carefully labeled datasets. However,\nnoisy labels often occur during data collection. In this paper, we\npropose an automatic error correction method for deep speaker embedding\nlearning with noisy labels. Specifically, a label noise correction\nloss is proposed that leverages a model&#8217;s generalization capability\nto correct noisy labels during training. In addition, we improve the\nvanilla AM-Softmax to estimate a more robust speaker posterior by introducing\nsub-centers. When applied on the VoxCeleb dataset, the proposed method\nperforms gracefully when noisy labels are introduced. Moreover, when\ncombining with the Bayesian estimation of PLDA with noisy training\nlabels at the back-end, the whole system performs better under conditions\nin which noisy labels are present.\n"
      ],
      "doi": "10.21437/Interspeech.2021-2021",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "liao21_interspeech": {
      "authors": [
        [
          "Dexin",
          "Liao"
        ],
        [
          "Jing",
          "Li"
        ],
        [
          "Yiming",
          "Zhi"
        ],
        [
          "Song",
          "Li"
        ],
        [
          "Qingyang",
          "Hong"
        ],
        [
          "Lin",
          "Li"
        ]
      ],
      "title": "An Integrated Framework for Two-Pass Personalized Voice Trigger",
      "original": "2161",
      "page_count": 5,
      "order": 948,
      "p1": "4633",
      "pn": "4637",
      "abstract": [
        "In this paper, we present the XMUSPEECH system for Task 1 of 2020 Personalized\nVoice Trigger Challenge (PVTC2020). Task 1 is a joint wake-up word\ndetection with speaker verification on close talking data. The whole\nsystem consists of a keyword spotting (KWS) sub-system and a speaker\nverification (SV) sub-system. For the KWS system, we applied a Temporal\nDepthwise Separable Convolution Residual Network (TDSC-ResNet) to improve\nthe system&#8217;s performance. For the SV system, we proposed a multi-task\nlearning network, where phonetic branch is trained with the character\nlabel of the utterance, and speaker branch is trained with the label\nof the speaker. Phonetic branch is optimized with connectionist temporal\nclassification (CTC) loss, which is treated as an auxiliary module\nfor speaker branch. Experiments show that our system gets significant\nimprovements compared with baseline system.\n"
      ],
      "doi": "10.21437/Interspeech.2021-2161",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "lian21_interspeech": {
      "authors": [
        [
          "Jiachen",
          "Lian"
        ],
        [
          "Aiswarya Vinod",
          "Kumar"
        ],
        [
          "Hira",
          "Dhamyal"
        ],
        [
          "Bhiksha",
          "Raj"
        ],
        [
          "Rita",
          "Singh"
        ]
      ],
      "title": "Masked Proxy Loss for Text-Independent Speaker Verification",
      "original": "2190",
      "page_count": 5,
      "order": 949,
      "p1": "4638",
      "pn": "4642",
      "abstract": [
        "Open-set speaker recognition can be regarded as a metric learning problem,\nwhich is to maximize inter-class variance and minimize intra-class\nvariance. Supervised metric learning can be categorized into pair-based\nlearning and proxy-based learning [1]. Most of the existing metric\nlearning objectives belong to the former division, the performance\nof which is either highly dependent on sample mining strategy or restricted\nby insufficient label information in the mini-batch. Proxy-based losses\nmitigate both shortcomings, however, fine-grained connections among\nentities are either not or indirectly leveraged. This paper proposes\na Masked Proxy (MP) loss which directly incorporates both proxy-based\nrelationship and pair-based relationship. We further propose Multinomial\nMasked Proxy (MMP) loss to leverage the hardness of speaker pairs.\nThese methods have been applied to evaluate on VoxCeleb test set and\nreach state-of-the-art Equal Error Rate (EER).\n"
      ],
      "doi": "10.21437/Interspeech.2021-2190",
      "author_area_id": "4",
      "author_area_label": "Speaker and Language Identification"
    },
    "lee21h_interspeech": {
      "authors": [
        [
          "Keon",
          "Lee"
        ],
        [
          "Kyumin",
          "Park"
        ],
        [
          "Daeyoung",
          "Kim"
        ]
      ],
      "title": "STYLER: Style Factor Modeling with Rapidity and Robustness via Speech Decomposition for Expressive and Controllable Neural Text to Speech",
      "original": "0838",
      "page_count": 5,
      "order": 950,
      "p1": "4643",
      "pn": "4647",
      "abstract": [
        "Previous works on neural text-to-speech (TTS) have been addressed on\nlimited speed in training and inference time, robustness for difficult\nsynthesis conditions, expressiveness, and controllability. Although\nseveral approaches resolve some limitations, there has been no attempt\nto solve all weaknesses at once. In this paper, we propose STYLER,\nan expressive and controllable TTS framework with high-speed and robust\nsynthesis. Our novel audio-text aligning method called Mel Calibrator\nand excluding autoregressive decoding enable rapid training and inference\nand robust synthesis on unseen data. Also, disentangled style factor\nmodeling under supervision enlarges the controllability in synthesizing\nprocess leading to expressive TTS. On top of it, a novel noise modeling\npipeline using domain adversarial training and Residual Decoding empowers\nnoise-robust style transfer, decomposing the noise without any additional\nlabel. Various experiments demonstrate that STYLER is more effective\nin speed and robustness than expressive TTS with autoregressive decoding\nand more expressive and controllable than reading style non-autoregressive\nTTS. Synthesis samples and experiment results are provided via our\ndemo page, and code is available publicly.\n"
      ],
      "doi": "10.21437/Interspeech.2021-838",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "liu21p_interspeech": {
      "authors": [
        [
          "Rui",
          "Liu"
        ],
        [
          "Berrak",
          "Sisman"
        ],
        [
          "Haizhou",
          "Li"
        ]
      ],
      "title": "Reinforcement Learning for Emotional Text-to-Speech Synthesis with Improved Emotion Discriminability",
      "original": "1236",
      "page_count": 5,
      "order": 951,
      "p1": "4648",
      "pn": "4652",
      "abstract": [
        "Emotional text-to-speech synthesis (ETTS) has seen much progress in\nrecent years. However, the generated voice is often not perceptually\nidentifiable by its intended emotion category. To address this problem,\nwe propose a new interactive training paradigm for ETTS, denoted as\n<i>i-ETTS</i>, which seeks to directly improve the emotion discriminability\nby interacting with a speech emotion recognition (SER) model. Moreover,\nwe formulate an iterative training strategy with reinforcement learning\nto ensure the quality of <i>i-ETTS</i> optimization. Experimental results\ndemonstrate that the proposed <i>i-ETTS</i> outperforms the state-of-the-art\nbaselines by rendering speech with more accurate emotion style. To\nour best knowledge, this is the first study of reinforcement learning\nin emotional text-to-speech synthesis.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1236",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "sivaprasad21_interspeech": {
      "authors": [
        [
          "Sarath",
          "Sivaprasad"
        ],
        [
          "Saiteja",
          "Kosgi"
        ],
        [
          "Vineet",
          "Gandhi"
        ]
      ],
      "title": "Emotional Prosody Control for Speech Generation",
      "original": "0307",
      "page_count": 5,
      "order": 952,
      "p1": "4653",
      "pn": "4657",
      "abstract": [
        "Machine-generated speech is characterized by its limited or unnatural\nemotional variation. Current text to speech systems generates speech\nwith either a flat emotion, emotion selected from a predefined set,\naverage variation learned from prosody sequences in training data or\ntransferred from a source style. We propose a text to speech (TTS)\nsystem, where a user can choose the emotion of generated speech from\na continuous and meaningful emotion space (Arousal-Valence space).\nThe proposed TTS system can generate speech from the text in any speaker&#8217;s\nstyle, with fine control of emotion. We show that the system works\non emotion unseen during training and can scale to previously unseen\nspeakers given his/her speech sample. Our work expands the horizon\nof the state-of-the-art FastSpeech2 backbone to a multi-speaker setting\nand gives it much-coveted continuous (and interpretable) affective\ncontrol, without any observable degradation in the quality of the synthesized\nspeech. Audio samples are publicly available.\n"
      ],
      "doi": "10.21437/Interspeech.2021-307",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "cong21b_interspeech": {
      "authors": [
        [
          "Jian",
          "Cong"
        ],
        [
          "Shan",
          "Yang"
        ],
        [
          "Na",
          "Hu"
        ],
        [
          "Guangzhi",
          "Li"
        ],
        [
          "Lei",
          "Xie"
        ],
        [
          "Dan",
          "Su"
        ]
      ],
      "title": "Controllable Context-Aware Conversational Speech Synthesis",
      "original": "0412",
      "page_count": 5,
      "order": 953,
      "p1": "4658",
      "pn": "4662",
      "abstract": [
        "In spoken conversations, spontaneous behaviors like filled pause and\nprolongations always happen. Conversational partner tends to align\nfeatures of their speech with their interlocutor which is known as\nentrainment. To produce human-like conversations, we propose a unified\ncontrollable spontaneous conversational speech synthesis framework\nto model the above two phenomena. Specifically, we use explicit labels\nto represent two typical spontaneous behaviors <i>filled-pause</i>\nand <i>prolongation</i> in the acoustic model and develop a neural\nnetwork based predictor to predict the occurrences of the two behaviors\nfrom text. We subsequently develop an algorithm based on the predictor\nto control the occurrence frequency of the behaviors, making the synthesized\nspeech vary from less disfluent to more disfluent. To model the speech\nentrainment at acoustic level, we utilize a context acoustic encoder\nto extract a global style embedding from the previous speech conditioning\non the synthesizing of current speech. Furthermore, since the current\nand previous utterances belong to the different speakers in a conversation,\nwe add a domain adversarial training module to eliminate the speaker-related\ninformation in the acoustic encoder while maintaining the style-related\ninformation. Experiments show that our proposed approach can synthesize\nrealistic conversations and control the occurrences of the spontaneous\nbehaviors naturally.\n"
      ],
      "doi": "10.21437/Interspeech.2021-412",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "kim21n_interspeech": {
      "authors": [
        [
          "Minchan",
          "Kim"
        ],
        [
          "Sung Jun",
          "Cheon"
        ],
        [
          "Byoung Jin",
          "Choi"
        ],
        [
          "Jong Jin",
          "Kim"
        ],
        [
          "Nam Soo",
          "Kim"
        ]
      ],
      "title": "Expressive Text-to-Speech Using Style Tag",
      "original": "0465",
      "page_count": 5,
      "order": 954,
      "p1": "4663",
      "pn": "4667",
      "abstract": [
        "As recent text-to-speech (TTS) systems have been rapidly improved in\nspeech quality and generation speed, many researchers now focus on\na more challenging issue: expressive TTS. To control speaking styles,\nexisting expressive TTS models use categorical style index or reference\nspeech as style input. In this work, we propose StyleTagging-TTS (ST-TTS),\na novel expressive TTS model that utilizes a style tag written in natural\nlanguage. Using a style-tagged TTS dataset and a pre-trained language\nmodel, we modeled the relationship between linguistic embedding and\nspeaking style domain, which enables our model to work even with style\ntags unseen during training. As style tag is written in natural language,\nit can control speaking style in a more intuitive, interpretable, and\nscalable way compared with style index or reference speech. In addition,\nin terms of model architecture, we propose an efficient non-autoregressive\n(NAR) TTS architecture with single-stage training. The experimental\nresult shows that ST-TTS outperforms the existing expressive TTS model,\nTacotron2-GST in speech quality and expressiveness.\n"
      ],
      "doi": "10.21437/Interspeech.2021-465",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "yan21d_interspeech": {
      "authors": [
        [
          "Yuzi",
          "Yan"
        ],
        [
          "Xu",
          "Tan"
        ],
        [
          "Bohan",
          "Li"
        ],
        [
          "Guangyan",
          "Zhang"
        ],
        [
          "Tao",
          "Qin"
        ],
        [
          "Sheng",
          "Zhao"
        ],
        [
          "Yuan",
          "Shen"
        ],
        [
          "Wei-Qiang",
          "Zhang"
        ],
        [
          "Tie-Yan",
          "Liu"
        ]
      ],
      "title": "Adaptive Text to Speech for Spontaneous Style",
      "original": "0584",
      "page_count": 5,
      "order": 955,
      "p1": "4668",
      "pn": "4672",
      "abstract": [
        "While recent text to speech (TTS) models perform very well in synthesizing\nreading-style (e.g., audiobook) speech, it is still challenging to\nsynthesize spontaneous-style speech (e.g., podcast or conversation),\nmainly because of two reasons: 1) the lack of training data for spontaneous\nspeech; 2) the difficulty in modeling the filled pauses (<i>um</i>\nand <i>uh</i>) and diverse rhythms in spontaneous speech. In this paper,\nwe develop AdaSpeech 3, an adaptive TTS system that fine-tunes a well-trained\nreading-style TTS model for spontaneous-style speech. Specifically,\n1) to insert filled pauses (FP) in the text sequence appropriately,\nwe introduce an FP predictor to the TTS model; 2) to model the varying\nrhythms, we introduce a duration predictor based on mixture of experts\n(MoE), which contains three experts responsible for the generation\nof fast, medium and slow speech respectively, and fine-tune it as well\nas the pitch predictor for rhythm adaptation; 3) to adapt to other\nspeaker timbre, we fine-tune some parameters in the decoder with few\nspeech data. To address the challenge of lack of training data, we\nmine a spontaneous speech dataset to support our research this work\nand facilitate future research on spontaneous TTS. Experiments show\nthat AdaSpeech 3 synthesizes speech with natural FP and rhythms in\nspontaneous styles, and achieves much better MOS and SMOS scores than\nprevious adaptive TTS systems.\n"
      ],
      "doi": "10.21437/Interspeech.2021-584"
    },
    "li21r_interspeech": {
      "authors": [
        [
          "Xiang",
          "Li"
        ],
        [
          "Changhe",
          "Song"
        ],
        [
          "Jingbei",
          "Li"
        ],
        [
          "Zhiyong",
          "Wu"
        ],
        [
          "Jia",
          "Jia"
        ],
        [
          "Helen",
          "Meng"
        ]
      ],
      "title": "Towards Multi-Scale Style Control for Expressive Speech Synthesis",
      "original": "0947",
      "page_count": 5,
      "order": 956,
      "p1": "4673",
      "pn": "4677",
      "abstract": [
        "This paper introduces a multi-scale speech style modeling method for\nend-to-end expressive speech synthesis. The proposed method employs\na multi-scale reference encoder to extract both the global-scale utterance-level\nand the local-scale quasi-phoneme-level style features of the target\nspeech, which are then fed into the speech synthesis model as an extension\nto the input phoneme sequence. During training time, the multi-scale\nstyle model could be jointly trained with the speech synthesis model\nin an end-to-end fashion. By applying the proposed method to style\ntransfer task, experimental results indicate that the controllability\nof the multi-scale speech style model and the expressiveness of the\nsynthesized speech are greatly improved. Moreover, by assigning different\nreference speeches to extraction of style on each scale, the flexibility\nof the proposed method is further revealed.\n"
      ],
      "doi": "10.21437/Interspeech.2021-947",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "pan21d_interspeech": {
      "authors": [
        [
          "Shifeng",
          "Pan"
        ],
        [
          "Lei",
          "He"
        ]
      ],
      "title": "Cross-Speaker Style Transfer with Prosody Bottleneck in Neural Speech Synthesis",
      "original": "0979",
      "page_count": 5,
      "order": 957,
      "p1": "4678",
      "pn": "4682",
      "abstract": [
        "Cross-speaker style transfer is crucial to the applications of multi-style\nand expressive speech synthesis at scale. It does not require the target\nspeakers to be experts in expressing all styles and to collect corresponding\nrecordings for model training. However, the performances of existing\nstyle transfer methods are still far behind real application needs.\nThe root causes are mainly twofold. Firstly, the style embedding extracted\nfrom single reference speech can hardly provide fine-grained and appropriate\nprosody information for arbitrary text to synthesize. Secondly, in\nthese models the content/text, prosody, and speaker timbre are usually\nhighly entangled, it&#8217;s therefore not realistic to expect a satisfied\nresult when freely combining these components, such as to transfer\nspeaking style between speakers. In this paper, we propose a cross-speaker\nstyle transfer text-to-speech (TTS) model with explicit prosody bottleneck.\nThe prosody bottleneck builds up the kernels accounting for speaking\nstyle robustly, and disentangles the prosody from content and speaker\ntimbre, therefore guarantees high quality cross-speaker style transfer.\nEvaluation result shows the proposed method even achieves on-par performance\nwith source speaker&#8217;s speaker-dependent (SD) model in objective\nmeasurement of prosody, and significantly outperforms the cycle consistency\nand GMVAE-based baselines in objective and subjective evaluations.\n"
      ],
      "doi": "10.21437/Interspeech.2021-979",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "tan21_interspeech": {
      "authors": [
        [
          "Daxin",
          "Tan"
        ],
        [
          "Tan",
          "Lee"
        ]
      ],
      "title": "Fine-Grained Style Modeling, Transfer and Prediction in Text-to-Speech Synthesis via Phone-Level Content-Style Disentanglement",
      "original": "1129",
      "page_count": 5,
      "order": 958,
      "p1": "4683",
      "pn": "4687",
      "abstract": [
        "This paper presents a novel design of neural network system for fine-grained\nstyle modeling, transfer and prediction in expressive text-to-speech\n(TTS) synthesis. Fine-grained modeling is realized by extracting style\nembeddings from the mel-spectrograms of phone-level speech segments.\nCollaborative learning and adversarial learning strategies are applied\nin order to achieve effective disentanglement of content and style\nfactors in speech and alleviate the &#8220;content leakage&#8221; problem\nin style modeling. The proposed system can be used for varying-content\nspeech style transfer in the single-speaker scenario. The results of\nobjective and subjective evaluation show that our system performs better\nthan other fine-grained speech style transfer models, especially in\nthe aspect of content preservation. By incorporating a style predictor,\nthe proposed system can also be used for text-to-speech synthesis.\nAudio samples are provided for system demonstration.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1129",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "an21b_interspeech": {
      "authors": [
        [
          "Xiaochun",
          "An"
        ],
        [
          "Frank K.",
          "Soong"
        ],
        [
          "Lei",
          "Xie"
        ]
      ],
      "title": "Improving Performance of Seen and Unseen Speech Style Transfer in End-to-End Neural TTS",
      "original": "1407",
      "page_count": 5,
      "order": 959,
      "p1": "4688",
      "pn": "4692",
      "abstract": [
        "End-to-end neural TTS training has shown improved performance in speech\nstyle transfer. However, the improvement is still limited by the training\ndata in both target styles and speakers. Inadequate style transfer\nperformance occurs when the trained TTS tries to transfer the speech\nto a target style from a new speaker with an unknown, arbitrary style.\nIn this paper, we propose a new approach to style transfer for both\nseen and unseen styles, with disjoint, multi-style datasets, i.e.,\ndatasets of different styles are recorded, each individual style is\nby one speaker with multiple utterances. To encode the style information,\nwe adopt an inverse autoregressive flow (IAF) structure to improve\nthe variational inference. The whole system is optimized to minimize\na weighed sum of four different loss functions: 1) a reconstruction\nloss to measure the distortions in both source and target reconstructions;\n2) an adversarial loss to &#8220;fool&#8221; a well-trained discriminator;\n3) a style distortion loss to measure the expected style loss after\nthe transfer; 4) a cycle consistency loss to preserve the speaker identity\nof the source after the transfer. Experiments demonstrate, both objectively\nand subjectively, the effectiveness of the proposed approach for seen\nand unseen style transfer tasks. The performance of the new approach\nis better and more robust than those of four baseline systems of the\nprior art.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1407",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "shechtman21_interspeech": {
      "authors": [
        [
          "Slava",
          "Shechtman"
        ],
        [
          "Raul",
          "Fernandez"
        ],
        [
          "Alexander",
          "Sorin"
        ],
        [
          "David",
          "Haws"
        ]
      ],
      "title": "Synthesis of Expressive Speaking Styles with Limited Training Data in a Multi-Speaker, Prosody-Controllable Sequence-to-Sequence Architecture",
      "original": "1446",
      "page_count": 5,
      "order": 960,
      "p1": "4693",
      "pn": "4697",
      "abstract": [
        "Although Sequence-to-Sequence (S2S) architectures have become state-of-the-art\nin speech synthesis, the best models benefit from access to moderate-to-large\namounts of training data, posing a resource bottleneck when we are\ninterested in generating speech in a variety of expressive styles.\nIn this work we explore a S2S architecture variant that is capable\nof generating a variety of stylistic expressive variations observed\nin a limited amount of training data, and of transplanting that style\nto a neutral target speaker for whom no labeled expressive resources\nexist. The architecture is furthermore controllable, allowing the user\nto select an operating point that conveys a desired level of expressiveness.\nWe evaluate this proposal against a classically supervised baseline\nvia perceptual listening tests, and demonstrate that i) it is able\nto outperform the baseline in terms of its generalizability to neutral\nspeakers, ii) it is strongly preferred in terms of its ability to convey\nexpressiveness, and iii) it provides a reasonable trade-off between\nexpressiveness and naturalness, allowing the user to tune it to the\nparticular demands of a given application.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1446",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation"
    },
    "dao21_interspeech": {
      "authors": [
        [
          "Mai Hoang",
          "Dao"
        ],
        [
          "Thinh Hung",
          "Truong"
        ],
        [
          "Dat Quoc",
          "Nguyen"
        ]
      ],
      "title": "Intent Detection and Slot Filling for Vietnamese",
      "original": "0618",
      "page_count": 5,
      "order": 961,
      "p1": "4698",
      "pn": "4702",
      "abstract": [
        "Intent detection and slot filling are important tasks in spoken and\nnatural language understanding. However, Vietnamese is a low-resource\nlanguage in these research topics. In this paper, we present the <i>first</i>\npublic intent detection and slot filling dataset for Vietnamese. In\naddition, we also propose a joint model for intent detection and slot\nfilling, that extends the recent state-of-the-art JointBERT+CRF model\n[1] with an intent-slot attention layer to explicitly incorporate intent\ncontext information into slot filling via &#8220;soft&#8221; intent\nlabel embedding. Experimental results on our Vietnamese dataset show\nthat our proposed model significantly outperforms JointBERT+CRF. We\npublicly release our dataset and the implementation of our model.\n"
      ],
      "doi": "10.21437/Interspeech.2021-618",
      "author_area_id": "12",
      "author_area_label": "Spoken Language Processing: Translation, Information Retrieval, Summarization, Resources and Evaluation"
    },
    "lin21k_interspeech": {
      "authors": [
        [
          "Haitao",
          "Lin"
        ],
        [
          "Lu",
          "Xiang"
        ],
        [
          "Yu",
          "Zhou"
        ],
        [
          "Jiajun",
          "Zhang"
        ],
        [
          "Chengqing",
          "Zong"
        ]
      ],
      "title": "Augmenting Slot Values and Contexts for Spoken Language Understanding with Pretrained Models",
      "original": "0055",
      "page_count": 5,
      "order": 962,
      "p1": "4703",
      "pn": "4707",
      "abstract": [
        "Spoken Language Understanding (SLU) is one essential step in building\na dialogue system. Due to the expensive cost of obtaining the labeled\ndata, SLU suffers from the data scarcity problem. Therefore, in this\npaper, we focus on data augmentation for slot filling task in SLU.\nTo achieve that, we aim at generating more diverse data based on existing\ndata. Specifically, we try to exploit the latent language knowledge\nfrom pretrained language models by finetuning them. We propose two\nstrategies for finetuning process: value-based and context-based augmentation.\nExperimental results on two public SLU datasets have shown that compared\nwith existing data augmentation methods, our proposed method can generate\nmore diverse sentences and significantly improve the performance on\nSLU.\n"
      ],
      "doi": "10.21437/Interspeech.2021-55",
      "author_area_id": "11",
      "author_area_label": "Spoken Dialog Systems and Conversational Analysis"
    },
    "gaspers21_interspeech": {
      "authors": [
        [
          "Judith",
          "Gaspers"
        ],
        [
          "Quynh",
          "Do"
        ],
        [
          "Daniil",
          "Sorokin"
        ],
        [
          "Patrick",
          "Lehnen"
        ]
      ],
      "title": "The Impact of Intent Distribution Mismatch on Semi-Supervised Spoken Language Understanding",
      "original": "0335",
      "page_count": 5,
      "order": 963,
      "p1": "4708",
      "pn": "4712",
      "abstract": [
        "With the expanding role of voice-controlled devices, bootstrapping\nspoken language understanding models from little labeled data becomes\nessential. Semi-supervised learning is a common technique to improve\nmodel performance when labeled data is scarce. In a real-world production\nsystem, the labeled data and the online test data often may come from\ndifferent distributions. In this work, we use semi-supervised learning\nbased on pseudo-labeling with an auxiliary task on incoming unlabeled\nnoisy data, which is closer to the test distribution. We demonstrate\nempirically that our approach can mitigate negative effects arising\nfrom training with non-representative labeled data as well as the negative\nimpacts of noises in the data, which are introduced by pseudo-labeling\nand automatic speech recognition.\n"
      ],
      "doi": "10.21437/Interspeech.2021-335",
      "author_area_id": "11",
      "author_area_label": "Spoken Dialog Systems and Conversational Analysis"
    },
    "jiang21c_interspeech": {
      "authors": [
        [
          "Yidi",
          "Jiang"
        ],
        [
          "Bidisha",
          "Sharma"
        ],
        [
          "Maulik",
          "Madhavi"
        ],
        [
          "Haizhou",
          "Li"
        ]
      ],
      "title": "Knowledge Distillation from BERT Transformer to Speech Transformer for Intent Classification",
      "original": "0402",
      "page_count": 5,
      "order": 964,
      "p1": "4713",
      "pn": "4717",
      "abstract": [
        "End-to-end intent classification using speech has numerous advantages\ncompared to the conventional pipeline approach using automatic speech\nrecognition (ASR), followed by natural language processing modules.\nIt attempts to predict intent from speech without using an intermediate\nASR module. However, such end-to-end framework suffers from the unavailability\nof large speech resources with higher acoustic variation in spoken\nlanguage understanding. In this work, we exploit the scope of the transformer\ndistillation method that is specifically designed for knowledge distillation\nfrom a transformer based language model to a transformer based speech\nmodel. In this regard, we leverage the reliable and widely used bidirectional\nencoder representations from transformers (BERT) model as a language\nmodel and transfer the knowledge to build an acoustic model for intent\nclassification using the speech. In particular, a multilevel transformer\nbased teacher-student model is designed, and knowledge distillation\nis performed across attention and hidden sub-layers of different transformer\nlayers of the student and teacher models. We achieve an intent classification\naccuracy of 99.10% and 88.79% for Fluent speech corpus and ATIS database,\nrespectively. Further, the proposed method demonstrates better performance\nand robustness in acoustically degraded condition compared to the baseline\nmethod.\n"
      ],
      "doi": "10.21437/Interspeech.2021-402",
      "author_area_id": "11",
      "author_area_label": "Spoken Dialog Systems and Conversational Analysis"
    },
    "wang21ia_interspeech": {
      "authors": [
        [
          "Nick J.C.",
          "Wang"
        ],
        [
          "Lu",
          "Wang"
        ],
        [
          "Yandan",
          "Sun"
        ],
        [
          "Haimei",
          "Kang"
        ],
        [
          "Dejun",
          "Zhang"
        ]
      ],
      "title": "Three-Module Modeling For End-to-End Spoken Language Understanding Using Pre-Trained DNN-HMM-Based Acoustic-Phonetic Model",
      "original": "0501",
      "page_count": 5,
      "order": 965,
      "p1": "4718",
      "pn": "4722",
      "abstract": [
        "In spoken language understanding (SLU), what the user says is converted\nto his/her intent. Recent work on end-to-end SLU has shown that accuracy\ncan be improved via pre-training approaches. We revisit ideas presented\nby Lugosch et al. using speech pre-training and three-module modeling;\nhowever, to ease construction of the end-to-end SLU model, we use as\nour phoneme module an open-source acoustic-phonetic model from a DNN-HMM\nhybrid automatic speech recognition (ASR) system instead of training\none from scratch. Hence we fine-tune on speech only for the word module,\nand we apply multi-target learning (MTL) on the word and intent modules\nto jointly optimize SLU performance. MTL yields a relative reduction\nof 40% in intent-classification error rates (from 1.0% to 0.6%). Note\nthat our three-module model is a streaming method. The final outcome\nof the proposed three-module modeling approach yields an intent accuracy\nof 99.4% on FluentSpeech, an intent error rate reduction of 50% compared\nto that of Lugosch et al. Although we focus on real-time streaming\nmethods, we also list non-streaming methods for comparison.\n"
      ],
      "doi": "10.21437/Interspeech.2021-501",
      "author_area_id": "11",
      "author_area_label": "Spoken Dialog Systems and Conversational Analysis"
    },
    "cha21_interspeech": {
      "authors": [
        [
          "Sujeong",
          "Cha"
        ],
        [
          "Wangrui",
          "Hou"
        ],
        [
          "Hyun",
          "Jung"
        ],
        [
          "My",
          "Phung"
        ],
        [
          "Michael",
          "Picheny"
        ],
        [
          "Hong-Kwang J.",
          "Kuo"
        ],
        [
          "Samuel",
          "Thomas"
        ],
        [
          "Edmilson",
          "Morais"
        ]
      ],
      "title": "Speak or Chat with Me: End-to-End Spoken Language Understanding System with Flexible Inputs",
      "original": "0788",
      "page_count": 5,
      "order": 966,
      "p1": "4723",
      "pn": "4727",
      "abstract": [
        "A major focus of recent research in spoken language understanding (SLU)\nhas been on the end-to-end approach where a single model can predict\nintents directly from speech inputs without intermediate transcripts.\nHowever, this approach presents some challenges. First, since speech\ncan be considered as personally identifiable information, in some cases\nonly automatic speech recognition (ASR) transcripts are accessible.\nSecond, intent-labeled speech data is scarce. To address the first\nchallenge, we propose a novel system that can predict intents from\nflexible types of inputs: speech, ASR transcripts, or both. We demonstrate\nstrong performance for either modality separately, and when both speech\nand ASR transcripts are available, through system combination, we achieve\nbetter results than using a single input modality. To address the second\nchallenge, we leverage a semantically robust pre-trained BERT model\nand adopt a cross-modal system that co-trains text embeddings and acoustic\nembeddings in a shared latent space. We further enhance this system\nby utilizing an acoustic module pre-trained on LibriSpeech and domain-adapting\nthe text module on our target datasets. Our experiments show significant\nadvantages for these pre-training and fine-tuning strategies, resulting\nin a system that achieves competitive intent-classification performance\non Snips SLU and Fluent Speech Commands datasets.\n"
      ],
      "doi": "10.21437/Interspeech.2021-788",
      "author_area_id": "11",
      "author_area_label": "Spoken Dialog Systems and Conversational Analysis"
    },
    "zhang21ha_interspeech": {
      "authors": [
        [
          "Xianwei",
          "Zhang"
        ],
        [
          "Liang",
          "He"
        ]
      ],
      "title": "End-to-End Cross-Lingual Spoken Language Understanding Model with Multilingual Pretraining",
      "original": "0818",
      "page_count": 5,
      "order": 967,
      "p1": "4728",
      "pn": "4732",
      "abstract": [
        "The spoken language understanding (SLU) plays an essential role in\nthe field of human-computer interaction. Most of the current SLU systems\nare cascade systems of automatic speech recognition (ASR) and natural\nlanguage understanding (NLU). Error propagation and scarcity of annotated\nspeech data are two common difficulties for resource-poor languages.\nTo solve them, we propose a simple but effective end-to-end cross-lingual\nspoken language understanding model based on XLSR-53, which is a pretrained\nmodel in 53 languages by the Facebook research team. The end-to-end\napproach avoids error propagation and the multilingual pretraining\nreduces data annotation requirements. Our proposed method achieves\n99.71% on the Fluent Speech Commands (FSC) English database and 79.89%\non the CATSLU-MAP Chinese database, in intent classification accuracy.\nTo the best of our knowledge, the former is the reported best result\non the FSC database.\n"
      ],
      "doi": "10.21437/Interspeech.2021-818",
      "author_area_id": "11",
      "author_area_label": "Spoken Dialog Systems and Conversational Analysis"
    },
    "saghir21_interspeech": {
      "authors": [
        [
          "Hamidreza",
          "Saghir"
        ],
        [
          "Samridhi",
          "Choudhary"
        ],
        [
          "Sepehr",
          "Eghbali"
        ],
        [
          "Clement",
          "Chung"
        ]
      ],
      "title": "Factorization-Aware Training of Transformers for Natural Language Understanding on the Edge",
      "original": "1816",
      "page_count": 5,
      "order": 968,
      "p1": "4733",
      "pn": "4737",
      "abstract": [
        "Fine-tuning transformer-based models have shown to outperform other\nmethods for many Natural Language Understanding (NLU) tasks. Recent\nstudies to reduce the size of transformer models have achieved reductions\nof &#62; 80%, making on-device inference on powerful devices possible.\nHowever, other resource-constrained devices, like those enabling voice\nassistants (VAs), require much further reductions. In this work, we\npropose factorization-aware training (FAT), wherein we factorize the\nlinear mappings of an already compressed transformer model (DistilBERT)\nand train jointly on NLU tasks. We test this method on three different\nNLU datasets and show our method outperforms naive application of factorization\nafter training by 10%&#8211;440% across various compression rates.\nAdditionally, We introduce a new metric called factorization gap and\nuse it to analyze the need for FAT across various model components.\nWe also present results for training subsets of factorized components\nto enable faster training, re-usability and maintainability for multiple\non-device models. We further demonstrate the trade-off between memory,\ninference speed and performance at a given compression-rate for a on-device\nimplementation of a factorized model. Our best performing factorized\nmodel, achieves a relative size reduction of 84% with &#8776;10% relative\ndegradation in NLU error rate compared to a non-factorized model on\nour internal dataset.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1816",
      "author_area_id": "11",
      "author_area_label": "Spoken Dialog Systems and Conversational Analysis"
    },
    "saxon21_interspeech": {
      "authors": [
        [
          "Michael",
          "Saxon"
        ],
        [
          "Samridhi",
          "Choudhary"
        ],
        [
          "Joseph P.",
          "McKenna"
        ],
        [
          "Athanasios",
          "Mouchtaris"
        ]
      ],
      "title": "End-to-End Spoken Language Understanding for Generalized Voice Assistants",
      "original": "1826",
      "page_count": 5,
      "order": 969,
      "p1": "4738",
      "pn": "4742",
      "abstract": [
        "End-to-end (E2E) spoken language understanding (SLU) systems predict\nutterance semantics directly from speech using a single model. Previous\nwork in this area has focused on targeted tasks in fixed domains, where\nthe output semantic structure is assumed a priori and the input speech\nis of limited complexity. In this work we present our approach to developing\nan E2E model for generalized SLU in commercial voice assistants (VAs).\nWe propose a fully differentiable, transformer-based, hierarchical\nsystem that can be pretrained at both the ASR and NLU levels. This\nis then fine-tuned on both transcription and semantic classification\nlosses to handle a diverse set of intent and argument combinations.\nThis leads to an SLU system that achieves significant improvements\nover baselines on a complex internal generalized VA dataset with a\n43% improvement in accuracy, while still meeting the 99% accuracy benchmark\non the popular Fluent Speech Commands dataset. We further evaluate\nour model on a hard test set, exclusively containing slot arguments\nunseen in training, and demonstrate a nearly 20% improvement, showing\nthe efficacy of our approach in truly demanding VA scenarios.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1826",
      "author_area_id": "11",
      "author_area_label": "Spoken Dialog Systems and Conversational Analysis"
    },
    "han21f_interspeech": {
      "authors": [
        [
          "Soyeon Caren",
          "Han"
        ],
        [
          "Siqu",
          "Long"
        ],
        [
          "Huichun",
          "Li"
        ],
        [
          "Henry",
          "Weld"
        ],
        [
          "Josiah",
          "Poon"
        ]
      ],
      "title": "Bi-Directional Joint Neural Networks for Intent Classification and Slot Filling",
      "original": "2044",
      "page_count": 5,
      "order": 970,
      "p1": "4743",
      "pn": "4747",
      "abstract": [
        "Intent classification and slot filling are two critical tasks for natural\nlanguage understanding. Traditionally the two tasks proceeded independently.\nHowever, more recently joint models for intent classification and slot\nfilling have achieved state-of-the-art performance, and have proved\nthat there exists a strong relationship between the two tasks. In this\npaper, we propose a bi-directional joint model for intent classification\nand slot filling, which includes a multi-stage hierarchical process\nvia BERT and bi-directional joint natural language understanding mechanisms,\nincluding intent2slot and slot2intent, to obtain mutual performance\nenhancement between intent classification and slot filling. The evaluations\nshow that our model achieves state-of-the-art results on intent classification\naccuracy, slot filling F1, and significantly improves sentence-level\nsemantic frame accuracy when applied to publicly available benchmark\ndatasets, ATIS (88.6%) and SNIPS (92.8%).\n"
      ],
      "doi": "10.21437/Interspeech.2021-2044",
      "author_area_id": "11",
      "author_area_label": "Spoken Dialog Systems and Conversational Analysis"
    },
    "cutler21_interspeech": {
      "authors": [
        [
          "Ross",
          "Cutler"
        ],
        [
          "Ando",
          "Saabas"
        ],
        [
          "Tanel",
          "Parnamaa"
        ],
        [
          "Markus",
          "Loide"
        ],
        [
          "Sten",
          "Sootla"
        ],
        [
          "Marju",
          "Purin"
        ],
        [
          "Hannes",
          "Gamper"
        ],
        [
          "Sebastian",
          "Braun"
        ],
        [
          "Karsten",
          "Sorensen"
        ],
        [
          "Robert",
          "Aichner"
        ],
        [
          "Sriram",
          "Srinivasan"
        ]
      ],
      "title": "INTERSPEECH 2021 Acoustic Echo Cancellation Challenge",
      "original": "1870",
      "page_count": 5,
      "order": 971,
      "p1": "4748",
      "pn": "4752",
      "abstract": [
        "The INTERSPEECH 2021 Acoustic Echo Cancellation Challenge is intended\nto stimulate research in the area of acoustic echo cancellation (AEC),\nwhich is an important part of speech enhancement and still a top issue\nin audio communication. Many recent AEC studies report good performance\non synthetic datasets where the training and testing data may come\nfrom the same underlying distribution. However, AEC performance often\ndegrades significantly on real recordings. Also, most of the conventional\nobjective metrics such as echo return loss enhancement and perceptual\nevaluation of speech quality do not correlate well with subjective\nspeech quality tests in the presence of background noise and reverberation\nfound in realistic environments. In this challenge, we open source\ntwo large datasets to train AEC models under both single talk and double\ntalk scenarios. These datasets consist of recordings from more than\n5,000 real audio devices and human speakers in real environments, as\nwell as a synthetic dataset. We also open source an online subjective\ntest framework and provide an online objective metric service for researchers\nto quickly test their results. The winners of this challenge are selected\nbased on the average Mean Opinion Score achieved across all different\nsingle talk and double talk scenarios.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1870",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement:14 Special Sessions"
    },
    "pfeifenberger21_interspeech": {
      "authors": [
        [
          "Lukas",
          "Pfeifenberger"
        ],
        [
          "Matthias",
          "Zoehrer"
        ],
        [
          "Franz",
          "Pernkopf"
        ]
      ],
      "title": "Acoustic Echo Cancellation with Cross-Domain Learning",
      "original": "0085",
      "page_count": 5,
      "order": 972,
      "p1": "4753",
      "pn": "4757",
      "abstract": [
        "This paper proposes the Cross-Domain Echo-Controller (CDEC), submitted\nto the Interspeech 2021 AEC-Challenge. The algorithm consists of three\nbuilding blocks: (i) a Time-Delay Compensation (TDC) module, (ii) a\nfrequency-domain block-based Acoustic Echo Canceler (AEC), and (iii)\na Time-Domain Neural-Network (TD-NN) used as a post-processor. Our\nsystem achieves an overall MOS score of 3.80, while only using 2.1\nmillion parameters at a system latency of 32ms.\n"
      ],
      "doi": "10.21437/Interspeech.2021-85",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement:14 Special Sessions"
    },
    "zhang21ia_interspeech": {
      "authors": [
        [
          "Shimin",
          "Zhang"
        ],
        [
          "Yuxiang",
          "Kong"
        ],
        [
          "Shubo",
          "Lv"
        ],
        [
          "Yanxin",
          "Hu"
        ],
        [
          "Lei",
          "Xie"
        ]
      ],
      "title": "F-T-LSTM Based Complex Network for Joint Acoustic Echo Cancellation and Speech Enhancement",
      "original": "1359",
      "page_count": 5,
      "order": 973,
      "p1": "4758",
      "pn": "4762",
      "abstract": [
        "With the increasing demand for audio communication and online conference,\nensuring the robustness of Acoustic Echo Cancellation (AEC) under the\ncomplicated acoustic scenario including noise, reverberation and nonlinear\ndistortion has become a top issue. Although there have been some traditional\nmethods that consider nonlinear distortion, they are still inefficient\nfor echo suppression and the performance will be attenuated when noise\nis present. In this paper, we present a real-time AEC approach using\ncomplex neural network to better modeling the important phase information\nand frequency-time-LSTMs (F-T-LSTM), which scan both frequency and\ntime axis, for better temporal modeling. Moreover, we utilize modified\nSI-SNR as cost function to make the model to have better echo cancellation\nand noise suppression (NS) performance. With only 1.4M parameters,\nthe proposed approach outperforms the AEC-challenge baseline by 0.27\nin terms of Mean Opinion Score (MOS).\n"
      ],
      "doi": "10.21437/Interspeech.2021-1359",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement:14 Special Sessions"
    },
    "seidel21_interspeech": {
      "authors": [
        [
          "Ernst",
          "Seidel"
        ],
        [
          "Jan",
          "Franzen"
        ],
        [
          "Maximilian",
          "Strake"
        ],
        [
          "Tim",
          "Fingscheidt"
        ]
      ],
      "title": "Y<SUP>2</SUP>-Net FCRN for Acoustic Echo and Noise Suppression",
      "original": "1590",
      "page_count": 5,
      "order": 974,
      "p1": "4763",
      "pn": "4767",
      "abstract": [
        "In recent years, deep neural networks (DNNs) were studied as an alternative\nto traditional acoustic echo cancellation (AEC) algorithms. The proposed\nmodels achieved remarkable performance for the separate tasks of AEC\nand residual echo suppression (RES). A promising network topology is\na fully convolutional recurrent network (FCRN) structure, which has\nalready proven its performance on both noise suppression and AEC tasks,\nindividually. However, the combination of AEC, postfiltering, and noise\nsuppression to a single network typically leads to a noticeable decline\nin the quality of the near-end speech component due to the lack of\na separate loss for echo estimation. In this paper, we propose a two-stage\nmodel (Y<SUP>2</SUP>-Net) which consists of two FCRNs, each with two\ninputs and one output (Y-Net). The first stage (AEC) yields an echo\nestimate, which &#8212; as a novelty for a DNN AEC model &#8212; is\nfurther used by the second stage to perform RES and noise suppression.\nWhile the subjective listening test of the Interspeech 2021 AEC Challenge\nmostly yielded results close to the baseline, the proposed method scored\nan average improvement of 0.46 points over the baseline on the blind\ntestset in double-talk on the instrumental metric DECMOS, provided\nby the challenge organizers.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1590"
    },
    "peng21f_interspeech": {
      "authors": [
        [
          "Renhua",
          "Peng"
        ],
        [
          "Linjuan",
          "Cheng"
        ],
        [
          "Chengshi",
          "Zheng"
        ],
        [
          "Xiaodong",
          "Li"
        ]
      ],
      "title": "Acoustic Echo Cancellation Using Deep Complex Neural Network with Nonlinear Magnitude Compression and Phase Information",
      "original": "2022",
      "page_count": 5,
      "order": 975,
      "p1": "4768",
      "pn": "4772",
      "abstract": [
        "This paper describes a two-stage acoustic echo cancellation (AEC) and\nsuppression framework for the INTERSPEECH2021 AEC Challenge. In the\nfirst stage, four parallel partitioned block frequency domain adaptive\nfilters are used to cancel the linear echo components, where the far-end\nsignal is delayed 0ms, 320ms, 640ms and 960ms for these four adaptive\nfilters, respectively, thus a maximum 1280 ms time delay can be well\nhandled in the blind test dataset. The error signal with minimum energy\nand its corresponding reference signal are chosen as the input for\nthe second stage, where a gate complex convolutional recurrent neural\nnetwork (GCCRN) is trained to further suppress the residual echo, late\nreverberation and environmental noise simultaneously. To improve the\nperformance of GCCRN, we compress both the magnitude of the error signal\nand that of the far-end reference signal, and then the two compressed\nmagnitudes are combined with the phase of the error signal to regenerate\nthe complex spectra as the input features of GCCRN. Numerous experimental\nresults show that the proposed framework is robust to the blind test\ndataset, and achieves a promising result with the P.808 evaluation.\n"
      ],
      "doi": "10.21437/Interspeech.2021-2022",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "ivry21_interspeech": {
      "authors": [
        [
          "Amir",
          "Ivry"
        ],
        [
          "Israel",
          "Cohen"
        ],
        [
          "Baruch",
          "Berdugo"
        ]
      ],
      "title": "Nonlinear Acoustic Echo Cancellation with Deep Learning",
      "original": "0722",
      "page_count": 5,
      "order": 976,
      "p1": "4773",
      "pn": "4777",
      "abstract": [
        "We propose a nonlinear acoustic echo cancellation system, which aims\nto model the echo path from the far-end signal to the near-end microphone\nin two parts. Inspired by the physical behavior of modern hands-free\ndevices, we first introduce a novel neural network architecture that\nis specifically designed to model the nonlinear distortions these devices\ninduce between receiving and playing the far-end signal. To account\nfor variations between devices, we construct this network with trainable\nmemory length and nonlinear activation functions that are not parameterized\nin advance, but are rather optimized during the training stage using\nthe training data. Second, the network is succeeded by a standard adaptive\nlinear filter that constantly tracks the echo path between the loudspeaker\noutput and the microphone. During training, the network and filter\nare jointly optimized to learn the network parameters. This system\nrequires 17 thousand parameters that consume 500 Million floating-point\noperations per second and 40 Kilo-bytes of memory. It also satisfies\nhands-free communication timing requirements on a standard neural processor,\nwhich renders it adequate for embedding on hands-free communication\ndevices. Using 280 hours of real and synthetic data, experiments show\nadvantageous performance compared to competing methods.\n"
      ],
      "doi": "10.21437/Interspeech.2021-722",
      "author_area_id": "6",
      "author_area_label": "Speech Coding and Enhancement"
    },
    "green21_interspeech": {
      "authors": [
        [
          "Jordan R.",
          "Green"
        ],
        [
          "Robert L.",
          "MacDonald"
        ],
        [
          "Pan-Pan",
          "Jiang"
        ],
        [
          "Julie",
          "Cattiau"
        ],
        [
          "Rus",
          "Heywood"
        ],
        [
          "Richard",
          "Cave"
        ],
        [
          "Katie",
          "Seaver"
        ],
        [
          "Marilyn A.",
          "Ladewig"
        ],
        [
          "Jimmy",
          "Tobin"
        ],
        [
          "Michael P.",
          "Brenner"
        ],
        [
          "Philip C.",
          "Nelson"
        ],
        [
          "Katrin",
          "Tomanek"
        ]
      ],
      "title": "Automatic Speech Recognition of Disordered Speech: Personalized Models Outperforming Human Listeners on Short Phrases",
      "original": "1384",
      "page_count": 5,
      "order": 977,
      "p1": "4778",
      "pn": "4782",
      "abstract": [
        "This study evaluated the accuracy of personalized automatic speech\nrecognition (ASR) for recognizing disordered speech from a large cohort\nof individuals with a wide range of underlying etiologies using an\nopen vocabulary. The performance of these models was benchmarked relative\nto that of expert human transcribers and two different speaker-independent\nASR models trained on typical speech. 432 individuals with self-reported\ndisordered speech recorded at least 300 short phrases using a web-based\napplication. Word error rates (WERs) were estimated for three different\nASR models and for human transcribers. Metadata were collected to evaluate\nthe potential impact of participants, atypical speech characteristics,\nand technical factors on recognition accuracy. Personalized models\noutperformed human transcribers with median and max recognition accuracy\ngains of 9% and 80%, respectively. The accuracies of personalized models\nwere high (median WER: 4.6%) and better than those of speaker-independent\nmodels (median WER: 31%). The most significant improvements were for\nthe most severely affected speakers. Low signal-to-noise ratio and\nfewer training utterances were associated with poor word recognition,\neven for speakers with mild speech impairments. Our results demonstrate\nthe efficacy of personalized ASR models in recognizing a wide range\nof speech impairments and severities and using an open vocabulary.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1384",
      "author_area_id": "13",
      "author_area_label": " Speech, Voice, and Hearing Disorders:14 Special Sessions"
    },
    "neumann21b_interspeech": {
      "authors": [
        [
          "Michael",
          "Neumann"
        ],
        [
          "Oliver",
          "Roesler"
        ],
        [
          "Jackson",
          "Liscombe"
        ],
        [
          "Hardik",
          "Kothare"
        ],
        [
          "David",
          "Suendermann-Oeft"
        ],
        [
          "David",
          "Pautler"
        ],
        [
          "Indu",
          "Navar"
        ],
        [
          "Aria",
          "Anvar"
        ],
        [
          "Jochen",
          "Kumm"
        ],
        [
          "Raquel",
          "Norel"
        ],
        [
          "Ernest",
          "Fraenkel"
        ],
        [
          "Alexander V.",
          "Sherman"
        ],
        [
          "James D.",
          "Berry"
        ],
        [
          "Gary L.",
          "Pattee"
        ],
        [
          "Jun",
          "Wang"
        ],
        [
          "Jordan R.",
          "Green"
        ],
        [
          "Vikram",
          "Ramanarayanan"
        ]
      ],
      "title": "Investigating the Utility of Multimodal Conversational Technology and Audiovisual Analytic Measures for the Assessment and Monitoring of Amyotrophic Lateral Sclerosis at Scale",
      "original": "1801",
      "page_count": 5,
      "order": 978,
      "p1": "4783",
      "pn": "4787",
      "abstract": [
        "We propose a cloud-based multimodal dialog platform for the remote\nassessment and monitoring of Amyotrophic Lateral Sclerosis (ALS) at\nscale. This paper presents our vision, technology setup, and an initial\ninvestigation of the efficacy of the various acoustic and visual speech\nmetrics automatically extracted by the platform. 82 healthy controls\nand 54 people with ALS (pALS) were instructed to interact with the\nplatform and completed a battery of speaking tasks designed to probe\nthe acoustic, articulatory, phonatory, and respiratory aspects of their\nspeech. We find that multiple acoustic (rate, duration, voicing) and\nvisual (higher order statistics of the jaw and lip) speech metrics\nshow statistically significant differences between controls, bulbar\nsymptomatic and bulbar pre-symptomatic patients. We report on the sensitivity\nand specificity of these metrics using five-fold cross-validation.\nWe further conducted a LASSO-LARS regression analysis to uncover the\nrelative contributions of various acoustic and visual features in predicting\nthe severity of patients&#8217; ALS (as measured by their self-reported\nALSFRSR scores). Our results provide encouraging evidence of the utility\nof automatically extracted audiovisual analytics for scalable remote\npatient assessment and monitoring in ALS.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1801",
      "author_area_id": "13",
      "author_area_label": " Speech, Voice, and Hearing Disorders"
    },
    "hermann21_interspeech": {
      "authors": [
        [
          "Enno",
          "Hermann"
        ],
        [
          "Mathew",
          "Magimai-Doss"
        ]
      ],
      "title": "Handling Acoustic Variation in Dysarthric Speech Recognition Systems Through Model Combination",
      "original": "2212",
      "page_count": 5,
      "order": 979,
      "p1": "4788",
      "pn": "4792",
      "abstract": [
        "Developing automatic speech recognition (ASR) systems that recognise\ndysarthric speech as well as control speech from unimpaired speakers\nremains challenging. Including more highly variable dysarthric speech\nduring training can also negatively affect the performance on control\nspeakers, which is not desirable when developing speech recognisers\nfor a wider audience. In this work, we analyse how the acoustic variability\nof dysarthric speech affects ASR systems and propose the combination\nof multiple acoustic models trained on different subsets of speakers\nto mitigate this effect. This approach shows improvements for both\ndysarthric and control speakers on the Torgo and UA-Speech corpora.\n"
      ],
      "doi": "10.21437/Interspeech.2021-2212",
      "author_area_id": "13",
      "author_area_label": " Speech, Voice, and Hearing Disorders"
    },
    "geng21b_interspeech": {
      "authors": [
        [
          "Mengzhe",
          "Geng"
        ],
        [
          "Shansong",
          "Liu"
        ],
        [
          "Jianwei",
          "Yu"
        ],
        [
          "Xurong",
          "Xie"
        ],
        [
          "Shoukang",
          "Hu"
        ],
        [
          "Zi",
          "Ye"
        ],
        [
          "Zengrui",
          "Jin"
        ],
        [
          "Xunying",
          "Liu"
        ],
        [
          "Helen",
          "Meng"
        ]
      ],
      "title": "Spectro-Temporal Deep Features for Disordered Speech Assessment and Recognition",
      "original": "0060",
      "page_count": 5,
      "order": 980,
      "p1": "4793",
      "pn": "4797",
      "abstract": [
        "Automatic recognition of disordered speech remains a highly challenging\ntask to date. Sources of variability commonly found in normal speech\nincluding accent, age or gender, when further compounded with the underlying\ncauses of speech impairment and varying severity levels, create large\ndiversity among speakers. To this end, speaker adaptation techniques\nplay a vital role in current speech recognition systems. Motivated\nby the spectro-temporal level differences between disordered and normal\nspeech that systematically manifest in articulatory imprecision, decreased\nvolume and clarity, slower speaking rates and increased dysfluencies,\nnovel spectro-temporal subspace basis embedding deep features derived\nby SVD decomposition of speech spectrum are proposed to facilitate\nboth accurate speech intelligibility assessment and auxiliary feature\nbased speaker adaptation of state-of-the-art hybrid DNN and end-to-end\ndisordered speech recognition systems. Experiments conducted on the\nUASpeech corpus suggest the proposed spectro-temporal deep feature\nadapted systems consistently outperformed baseline i-Vector adaptation\nby up to 2.63% absolute (8.6% relative) reduction in word error rate\n(WER) with or without data augmentation. Learning hidden unit contribution\n(LHUC) based speaker adaptation was further applied. The final speaker\nadapted system using the proposed spectral basis embedding features\ngave an overall WER of 25.6% on the UASpeech test set of 16 dysarthric\nspeakers.\n"
      ],
      "doi": "10.21437/Interspeech.2021-60",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation:14 Special Sessions"
    },
    "gutz21_interspeech": {
      "authors": [
        [
          "Sarah E.",
          "Gutz"
        ],
        [
          "Hannah P.",
          "Rowe"
        ],
        [
          "Jordan R.",
          "Green"
        ]
      ],
      "title": "Speaking with a KN95 Face Mask: ASR Performance and Speaker Compensation",
      "original": "0099",
      "page_count": 5,
      "order": 981,
      "p1": "4798",
      "pn": "4802",
      "abstract": [
        "The increasing prevalence of face masks in the United States due to\nthe COVID-19 pandemic necessitates serious consideration of the functional\nimpact of wearing a mask on speech. This study considers how the presence\nof a KN95 mask affects the performance of a commercial ASR system,\nGoogle Cloud Speech. We present evidence that wearing a mask does not\nimpact ASR performance at the sentence level. Moreover, speakers may\nbe naturally adapting to the mask by increasing their vowel space area.\nHowever, when speakers intentionally altered their speech by speaking\nclearly or loudly (though not slowly), ASR performance improved. These\nfindings suggest that ASR users can employ speech strategies to achieve\nbetter ASR results when wearing a mask. Beyond healthy speakers, our\nstudy has implications for mask-wearing ASR users with otherwise reduced\nspeech intelligibility.\n"
      ],
      "doi": "10.21437/Interspeech.2021-99",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation:14 Special Sessions"
    },
    "jin21_interspeech": {
      "authors": [
        [
          "Zengrui",
          "Jin"
        ],
        [
          "Mengzhe",
          "Geng"
        ],
        [
          "Xurong",
          "Xie"
        ],
        [
          "Jianwei",
          "Yu"
        ],
        [
          "Shansong",
          "Liu"
        ],
        [
          "Xunying",
          "Liu"
        ],
        [
          "Helen",
          "Meng"
        ]
      ],
      "title": "Adversarial Data Augmentation for Disordered Speech Recognition",
      "original": "0168",
      "page_count": 5,
      "order": 982,
      "p1": "4803",
      "pn": "4807",
      "abstract": [
        "Automatic recognition of disordered speech remains a highly challenging\ntask to date. The underlying neuro-motor conditions, often compounded\nwith co-occurring physical disabilities, lead to the difficulty in\ncollecting large quantities of impaired speech required for ASR system\ndevelopment. To this end, data augmentation techniques play a vital\nrole in current disordered speech recognition systems. In contrast\nto existing data augmentation techniques only modifying the speaking\nrate or overall shape of spectral contour, fine-grained spectro-temporal\ndifferences between disordered and normal speech are modelled using\ndeep convolutional generative adversarial networks (DCGAN) during data\naugmentation to modify normal speech spectra into those closer to disordered\nspeech. Experiments conducted on the UASpeech corpus suggest the proposed\nadversarial data augmentation approach consistently outperformed the\nbaseline augmentation methods using tempo or speed perturbation on\na state-of-the-art hybrid DNN system. An overall word error rate (WER)\nreduction up to 3.05% (9.7% relative) was obtained over the baseline\nsystem using no data augmentation. The final learning hidden unit contribution\n(LHUC) speaker adapted system using the best adversarial augmentation\napproach gives an overall WER of 25.89% on the UASpeech test set of\n16 dysarthric speakers.\n"
      ],
      "doi": "10.21437/Interspeech.2021-168",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation:14 Special Sessions"
    },
    "xie21b_interspeech": {
      "authors": [
        [
          "Xurong",
          "Xie"
        ],
        [
          "Rukiye",
          "Ruzi"
        ],
        [
          "Xunying",
          "Liu"
        ],
        [
          "Lan",
          "Wang"
        ]
      ],
      "title": "Variational Auto-Encoder Based Variability Encoding for Dysarthric Speech Recognition",
      "original": "0173",
      "page_count": 5,
      "order": 983,
      "p1": "4808",
      "pn": "4812",
      "abstract": [
        "Dysarthric speech recognition is a challenging task due to acoustic\nvariability and limited amount of available data. Diverse conditions\nof dysarthric speakers account for the acoustic variability, which\nmake the variability difficult to be modeled precisely. This paper\npresents a variational auto-encoder based variability encoder (VAEVE)\nto explicitly encode such variability for dysarthric speech. The VAEVE\nmakes use of both phoneme information and low-dimensional latent variable\nto reconstruct the input acoustic features, thereby the latent variable\nis forced to encode the phoneme-independent variability. Stochastic\ngradient variational Bayes algorithm is applied to model the distribution\nfor generating variability encodings, which are further used as auxiliary\nfeatures for DNN acoustic modeling. Experiment results conducted on\nthe UASpeech corpus show that the VAEVE based variability encodings\nhave complementary effect to the learning hidden unit contributions\n(LHUC) speaker adaptation. The systems using variability encodings\nconsistently outperform the comparable baseline systems without using\nthem, and obtain absolute word error rate (WER) reduction by up to\n2.2% on dysarthric speech with &#8220;Very low&#8221; intelligibility\nlevel, and up to 2% on the &#8220;Mixed&#8221; type of dysarthric speech\nwith diverse or uncertain conditions.\n"
      ],
      "doi": "10.21437/Interspeech.2021-173",
      "author_area_id": "13",
      "author_area_label": " Speech, Voice, and Hearing Disorders:14 Special Sessions"
    },
    "wang21ja_interspeech": {
      "authors": [
        [
          "Disong",
          "Wang"
        ],
        [
          "Songxiang",
          "Liu"
        ],
        [
          "Lifa",
          "Sun"
        ],
        [
          "Xixin",
          "Wu"
        ],
        [
          "Xunying",
          "Liu"
        ],
        [
          "Helen",
          "Meng"
        ]
      ],
      "title": "Learning Explicit Prosody Models and Deep Speaker Embeddings for Atypical Voice Conversion",
      "original": "0285",
      "page_count": 5,
      "order": 984,
      "p1": "4813",
      "pn": "4817",
      "abstract": [
        "Though significant progress has been made for the voice conversion\n(VC) of typical speech, VC for atypical speech, e.g., dysarthric and\nsecond-language (L2) speech, remains a challenge, since it involves\ncorrecting for atypical prosody while maintaining speaker identity.\nTo address this issue, we propose a VC system with explicit prosodic\nmodelling and deep speaker embedding (DSE) learning. First, a speech-encoder\nstrives to extract robust phoneme embeddings from atypical speech.\nSecond, a prosody corrector takes in phoneme embeddings to infer typical\nphoneme duration and pitch values. Third, a conversion model takes\nphoneme embeddings and typical prosody features as inputs to generate\nthe converted speech, conditioned on the target DSE that is learned\nvia speaker encoder or speaker adaptation. Extensive experiments demonstrate\nthat speaker adaptation can achieve higher speaker similarity, and\nthe speaker encoder based conversion model can greatly reduce dysarthric\nand non-native pronunciation patterns with improved speech intelligibility.\nA comparison of speech recognition results between the original dysarthric\nspeech and converted speech show that absolute reduction of 47.6% character\nerror rate (CER) and 29.3% word error rate (WER) can be achieved.\n"
      ],
      "doi": "10.21437/Interspeech.2021-285",
      "author_area_id": "7",
      "author_area_label": "Speech Synthesis and Spoken Language Generation:14 Special Sessions"
    },
    "deng21d_interspeech": {
      "authors": [
        [
          "Jiajun",
          "Deng"
        ],
        [
          "Fabian Ritter",
          "Gutierrez"
        ],
        [
          "Shoukang",
          "Hu"
        ],
        [
          "Mengzhe",
          "Geng"
        ],
        [
          "Xurong",
          "Xie"
        ],
        [
          "Zi",
          "Ye"
        ],
        [
          "Shansong",
          "Liu"
        ],
        [
          "Jianwei",
          "Yu"
        ],
        [
          "Xunying",
          "Liu"
        ],
        [
          "Helen",
          "Meng"
        ]
      ],
      "title": "Bayesian Parametric and Architectural Domain Adaptation of LF-MMI Trained TDNNs for Elderly and Dysarthric Speech Recognition",
      "original": "0289",
      "page_count": 5,
      "order": 985,
      "p1": "4818",
      "pn": "4822",
      "abstract": [
        "Automatic recognition of elderly and disordered speech remains a highly\nchallenging task to date. Such data is not only difficult to collect\nin large quantities, but also exhibits a significant mismatch against\nnormal speech trained ASR systems. To this end, conventional deep neural\nnetwork model adaptation approaches only consider parameter fine-tuning\non limited target domain data. In this paper, a novel Bayesian parametric\nand neural architectural domain adaptation approach is proposed. Both\nthe standard model parameters and architectural hyper-parameters (hidden\nlayer L/R context offsets) of two lattice-free MMI (LF-MMI) factored\nTDNN systems separately trained using large quantities of normal speech\nfrom the English LibriSpeech and Cantonese SpeechOcean corpora were\ndomain adapted to two tasks: a) 16-hour DementiaBank elderly speech\ncorpus; and b) 14-hour CUDYS dysarthric speech database. A Bayesian\ndifferentiable architectural search (DARTS) super-network was designed\nto allow both efficient search over up to 7<SUP>28</SUP> different\nTDNN structures during domain adaptation, and robust modelling of parameter\nuncertainty given limited target domain data. Absolute recognition\nerror rate reductions of 1.82% and 2.93% (13.2% and 8.3% relative)\nwere obtained over the baseline systems performing model parameter\nfine-tuning only. Consistent performance improvements were retained\nafter data augmentation and learning hidden unit contribution (LHUC)\nbased speaker adaptation was performed.\n"
      ],
      "doi": "10.21437/Interspeech.2021-289",
      "author_area_id": "8",
      "author_area_label": "Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation:14 Special Sessions"
    },
    "cai21c_interspeech": {
      "authors": [
        [
          "Shanqing",
          "Cai"
        ],
        [
          "Lisie",
          "Lillianfeld"
        ],
        [
          "Katie",
          "Seaver"
        ],
        [
          "Jordan R.",
          "Green"
        ],
        [
          "Michael P.",
          "Brenner"
        ],
        [
          "Philip C.",
          "Nelson"
        ],
        [
          "D.",
          "Sculley"
        ]
      ],
      "title": "A Voice-Activated Switch for Persons with Motor and Speech Impairments: Isolated-Vowel Spotting Using Neural Networks",
      "original": "0330",
      "page_count": 5,
      "order": 986,
      "p1": "4823",
      "pn": "4827",
      "abstract": [
        "Severe speech impairments limit the precision and range of producible\nspeech sounds. As a result, generic automatic speech recognition (ASR)\nand keyword spotting (KWS) systems fail to accurately recognize the\nutterances produced by individuals with severe speech impairments.\nThis paper describes an approach in a simple speech sound, namely isolated\nopen vowel (/a/), is used in lieu of more motorically-demanding utterances.\nA neural network (NN) is trained to detect the isolated open vowel\nuttered by impaired speakers. The NN is trained with a two-phase approach.\nThe pre-training phase uses samples from unimpaired speakers along\nwith samples of background noises and unrelated speech; then the fine-tuning\nphase uses samples of vowel samples collected from individuals with\nspeech impairments. This model can be built into an experimental mobile\napp to act as a switch that allows users to activate preconfigured\nactions such as alerting caregivers. Preliminary user testing indicates\nthe vowel spotter has the potential to be a useful and flexible emergency\ncommunication channel for motor- and speech-impaired individuals.\n"
      ],
      "doi": "10.21437/Interspeech.2021-330",
      "author_area_id": "5",
      "author_area_label": "Analysis of Speech and Audio Signals:14 Special Sessions"
    },
    "chen21w_interspeech": {
      "authors": [
        [
          "Zhehuai",
          "Chen"
        ],
        [
          "Bhuvana",
          "Ramabhadran"
        ],
        [
          "Fadi",
          "Biadsy"
        ],
        [
          "Xia",
          "Zhang"
        ],
        [
          "Youzheng",
          "Chen"
        ],
        [
          "Liyang",
          "Jiang"
        ],
        [
          "Fang",
          "Chu"
        ],
        [
          "Rohan",
          "Doshi"
        ],
        [
          "Pedro J.",
          "Moreno"
        ]
      ],
      "title": "Conformer Parrotron: A Faster and Stronger End-to-End Speech Conversion and Recognition Model for Atypical Speech",
      "original": "0676",
      "page_count": 5,
      "order": 987,
      "p1": "4828",
      "pn": "4832",
      "abstract": [
        "Parrotron is an end-to-end personalizable model that enables many-to-one\nvoice conversion (VC) and automated speech recognition (ASR) simultaneously\nfor atypical speech. In this work, we present the next-generation Parrotron\nmodel with improvements in overall accuracy, training and inference\nspeeds. The proposed architecture builds on the recent Conformer encoder\ncomprising of convolution and attention layer based blocks used in\nASR. We introduce architectural modifications that subsamples encoder\nactivations to achieve speed-ups in training and inference. In order\nto jointly improve ASR and voice conversion quality, we show that this\nrequires a corresponding upsampling after the Conformer encoder blocks.\nWe provide an in-depth analysis on how the proposed approach can maximize\nthe efficiency of a speech-to-speech conversion model in the context\nof atypical speech. Experiments on both many-to-one and one-to-one\ndysarthric speech conversion tasks show that we can achieve up to 7&#215;\nspeedup and 35% relative reduction in WER over the previous best Transformer\nParrotron.\n"
      ],
      "doi": "10.21437/Interspeech.2021-676",
      "author_area_id": "13",
      "author_area_label": " Speech, Voice, and Hearing Disorders:14 Special Sessions"
    },
    "macdonald21_interspeech": {
      "authors": [
        [
          "Robert L.",
          "MacDonald"
        ],
        [
          "Pan-Pan",
          "Jiang"
        ],
        [
          "Julie",
          "Cattiau"
        ],
        [
          "Rus",
          "Heywood"
        ],
        [
          "Richard",
          "Cave"
        ],
        [
          "Katie",
          "Seaver"
        ],
        [
          "Marilyn A.",
          "Ladewig"
        ],
        [
          "Jimmy",
          "Tobin"
        ],
        [
          "Michael P.",
          "Brenner"
        ],
        [
          "Philip C.",
          "Nelson"
        ],
        [
          "Jordan R.",
          "Green"
        ],
        [
          "Katrin",
          "Tomanek"
        ]
      ],
      "title": "Disordered Speech Data Collection: Lessons Learned at 1 Million Utterances from Project Euphonia",
      "original": "0697",
      "page_count": 5,
      "order": 988,
      "p1": "4833",
      "pn": "4837",
      "abstract": [
        "Speech samples from over 1000 individuals with impaired speech have\nbeen submitted for Project Euphonia, aimed at improving automated speech\nrecognition systems for disordered speech. We provide an overview of\nthe corpus, which recently passed 1 million utterances (&#62;1300 hours),\nand review key lessons learned from this project. The reasoning behind\ndecisions such as phrase set composition, prompted vs extemporaneous\nspeech, metadata and data quality efforts are explained based on findings\nfrom both technical and user-facing research.\n"
      ],
      "doi": "10.21437/Interspeech.2021-697",
      "author_area_id": "13",
      "author_area_label": " Speech, Voice, and Hearing Disorders:14 Special Sessions"
    },
    "yeo21_interspeech": {
      "authors": [
        [
          "Eun Jung",
          "Yeo"
        ],
        [
          "Sunhee",
          "Kim"
        ],
        [
          "Minhwa",
          "Chung"
        ]
      ],
      "title": "Automatic Severity Classification of Korean Dysarthric Speech Using Phoneme-Level Pronunciation Features",
      "original": "1353",
      "page_count": 5,
      "order": 989,
      "p1": "4838",
      "pn": "4842",
      "abstract": [
        "This paper proposes an automatic severity classification method for\nKorean dysarthric speech by using two types of phoneme-level pronunciation\nfeatures. The first type is the percentage of correct phonemes, which\nconsists of percentage of correct consonants, percentage of correct\nvowels, and percentage of total correct phonemes. The second type is\nrelated to the degree of vowel distortion, such as vowel space area,\nformant centralized ratio, vowel articulatory index, and F2-ratio.\nThe baseline experiments use features from our previous study, consisting\nof MFCCs, voice quality features, and prosody features. Compared to\nthe baseline, experiments including phoneme-level pronunciation features\nachieve a relative percentage increase of 32.55% and 33.84% in F1-score\nfor support vector machine and feed-forward neural network classifiers,\nrespectively. Our best performance reaches an F1-score of 77.38%, which\nis a relative percentage increase of 10.39% compared to the best previous\nresults conducted on the Korean dysarthric QoLT corpus. Furthermore,\nwith feature selection applied, all seven phoneme-level pronunciation\nfeatures are chosen, accounting for the highest percentage of the selected\nfeature set by both recursive feature elimination and extra trees classifier\nfeature selection algorithms. Results indicate that phoneme-level pronunciation\nfeatures are useful in enhancing the performance for automatic severity\nclassification of dysarthric speech.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1353",
      "author_area_id": "13",
      "author_area_label": " Speech, Voice, and Hearing Disorders:14 Special Sessions"
    },
    "venugopalan21_interspeech": {
      "authors": [
        [
          "Subhashini",
          "Venugopalan"
        ],
        [
          "Joel",
          "Shor"
        ],
        [
          "Manoj",
          "Plakal"
        ],
        [
          "Jimmy",
          "Tobin"
        ],
        [
          "Katrin",
          "Tomanek"
        ],
        [
          "Jordan R.",
          "Green"
        ],
        [
          "Michael P.",
          "Brenner"
        ]
      ],
      "title": "Comparing Supervised Models and Learned Speech Representations for Classifying Intelligibility of Disordered Speech on Selected Phrases",
      "original": "1913",
      "page_count": 5,
      "order": 990,
      "p1": "4843",
      "pn": "4847",
      "abstract": [
        "Automatic classification of disordered speech can provide an objective\ntool for identifying the presence and severity of a speech impairment.\nClassification approaches can also help identify hard-to-recognize\nspeech samples to teach ASR systems about the variable manifestations\nof impaired speech. Here, we develop and compare different deep learning\ntechniques to classify the intelligibility of disordered speech on\nselected phrases. We collected samples from a diverse set of 661 speakers\nwith a variety of self-reported disorders speaking 29 words or phrases,\nwhich were rated by speech-language pathologists for their overall\nintelligibility using a five-point Likert scale. We then evaluated\nclassifiers developed using 3 approaches: (1) a convolutional neural\nnetwork (CNN) trained for the task, (2) classifiers trained on non-semantic\nspeech representations from CNNs that used an unsupervised objective\n[1], and (3) classifiers trained on the acoustic (encoder) embeddings\nfrom an ASR system trained on typical speech [2]. We found that the\nASR encoder&#8217;s embeddings considerably outperform the other two\non detecting and classifying disordered speech. Further analysis shows\nthat the ASR embeddings cluster speech by the spoken phrase, while\nthe non-semantic embeddings cluster speech by speaker. Also, longer\nphrases are more indicative of intelligibility deficits than single\nwords.\n"
      ],
      "doi": "10.21437/Interspeech.2021-1913",
      "author_area_id": "13",
      "author_area_label": " Speech, Voice, and Hearing Disorders:14 Special Sessions"
    },
    "mitra21_interspeech": {
      "authors": [
        [
          "Vikramjit",
          "Mitra"
        ],
        [
          "Zifang",
          "Huang"
        ],
        [
          "Colin",
          "Lea"
        ],
        [
          "Lauren",
          "Tooley"
        ],
        [
          "Sarah",
          "Wu"
        ],
        [
          "Darren",
          "Botten"
        ],
        [
          "Ashwini",
          "Palekar"
        ],
        [
          "Shrinath",
          "Thelapurath"
        ],
        [
          "Panayiotis",
          "Georgiou"
        ],
        [
          "Sachin",
          "Kajarekar"
        ],
        [
          "Jefferey",
          "Bigham"
        ]
      ],
      "title": "Analysis and Tuning of a Voice Assistant System for Dysfluent Speech",
      "original": "2006",
      "page_count": 5,
      "order": 991,
      "p1": "4848",
      "pn": "4852",
      "abstract": [
        "Dysfluencies and variations in speech pronunciation can severely degrade\nspeech recognition performance, and for many individuals with moderate-to-severe\nspeech disorders, voice operated systems do not work. Current speech\nrecognition systems are trained primarily with data from fluent speakers\nand as a consequence do not generalize well to speech with dysfluencies\nsuch as sound or word repetitions, sound prolongations, or audible\nblocks. The focus of this work is on quantitative analysis of a consumer\nspeech recognition system on individuals who stutter and production-oriented\napproaches for improving performance for common voice assistant tasks\n(i.e., &#8220;what is the weather?&#8221;). At baseline, this system\nintroduces a significant number of insertion and substitution errors\nresulting in intended speech Word Error Rates (isWER) that are 13.64%\nworse (absolute) for individuals with fluency disorders. We show that\nby simply tuning the decoding parameters in an existing hybrid speech\nrecognition system one can improve isWER by 24% (relative) for individuals\nwith fluency disorders. Tuning these parameters translates to 3.6%\nbetter domain recognition and 1.7% better intent recognition relative\nto the default setup for the 18 study participants across all stuttering\nseverities.\n"
      ],
      "doi": "10.21437/Interspeech.2021-2006",
      "author_area_id": "13",
      "author_area_label": " Speech, Voice, and Hearing Disorders:14 Special Sessions"
    },
    "kawahara21b_interspeech": {
      "authors": [
        [
          "Hideki",
          "Kawahara"
        ],
        [
          "Kohei",
          "Yatabe"
        ],
        [
          "Ken-Ichi",
          "Sakakibara"
        ],
        [
          "Mitsunori",
          "Mizumachi"
        ],
        [
          "Masanori",
          "Morise"
        ],
        [
          "Hideki",
          "Banno"
        ],
        [
          "Toshio",
          "Irino"
        ]
      ],
      "title": "Interactive and Real-Time Acoustic Measurement Tools for Speech Data Acquisition and Presentation: Application of an Extended Member of Time Stretched Pulses",
      "original": "8021",
      "page_count": 2,
      "order": 992,
      "p1": "4853",
      "pn": "4854",
      "abstract": [
        "Objective measurements of speech data acquisition and presentation\nprocesses are crucial for assuring reproducibility and reusability\nof experimental results and acquired materials. We introduce setting\nand measurement examples of those conditions using an interactive and\nreal-time acoustic measurement tool based on an extended time-stretched\npulse. We also introduce supporting tools.\n"
      ]
    },
    "tihelka21_interspeech": {
      "authors": [
        [
          "Daniel",
          "Tihelka"
        ],
        [
          "Mark\u00e9ta",
          "\u0158ez\u00e1\u010dkov\u00e1"
        ],
        [
          "Martin",
          "Gr\u016fber"
        ],
        [
          "Zden\u011bk",
          "Hanzl\u00ed\u010dek"
        ],
        [
          "Jakub",
          "V\u00edt"
        ],
        [
          "Jind\u0159ich",
          "Matou\u0161ek"
        ]
      ],
      "title": "Save Your Voice: Voice Banking and TTS for Anyone",
      "original": "8023",
      "page_count": 2,
      "order": 993,
      "p1": "4855",
      "pn": "4856",
      "abstract": [
        "The paper describes the process of automatic building of a personalized\nTTS system. The system was primarily developed for people facing the\nthreat of voice loss; however, it can be used by anyone who wants to\nsave his/her voice for any reason. Regarding the target group of users,\nthe whole system is designed to be as simple to use as possible while\nstill being fully autonomous.\n"
      ]
    },
    "zhang21ja_interspeech": {
      "authors": [
        [
          "Yang",
          "Zhang"
        ],
        [
          "Evelina",
          "Bakhturina"
        ],
        [
          "Boris",
          "Ginsburg"
        ]
      ],
      "title": "NeMo (Inverse) Text Normalization: From Development to Production",
      "original": "8024",
      "page_count": 3,
      "order": 994,
      "p1": "4857",
      "pn": "4859",
      "no_doi": true,
      "abstract": [
        "We introduce the NeMo Text Processing (NTP) toolkit &#8212; an open-source\nPython library for text normalization (TN) and inverse text normalization\n(ITN) based on weighted finite-state transducers (WFSTs). The English\ngrammars provided within NTP can be seamlessly deployed to the C++\nSparrowhawk framework for production.\n"
      ],
      "author_area_id": "10",
      "author_area_label": "Speech Recognition: Technologies and Systems for New Applications"
    },
    "hembise21_interspeech": {
      "authors": [
        [
          "Corentin",
          "Hembise"
        ],
        [
          "Lucile",
          "Gelin"
        ],
        [
          "Morgane",
          "Daniel"
        ]
      ],
      "title": "Lalilo: A Reading Assistant for Children Featuring Speech Recognition-Based Reading Mistake Detection",
      "original": "8025",
      "page_count": 2,
      "order": 995,
      "p1": "4860",
      "pn": "4861",
      "abstract": [
        "Lalilo is a reading assistant intended to help kindergarten to second\ngrade students to master their reading skills. Students progress at\ntheir own pace thanks to an adaptive learning system that differentiates\ninstructions. Teachers can access data on their students&#8217; progression.\nAmong other exercises, a read-aloud exercise is provided for students\nto practice their reading. This exercise uses a reading mistake detection\nsystem based on speech recognition to offer automatic feedback on the\nchild&#8217;s reading. Since speech recognition on children learning\nto read is highly challenging, we overcome potential inaccurate thus\ndamageable feedback with an uncertainty estimation leading to a neutral\nfeedback.\n"
      ]
    },
    "nguyen21f_interspeech": {
      "authors": [
        [
          "Manh Hung",
          "Nguyen"
        ],
        [
          "Vu",
          "Hoang"
        ],
        [
          "Tu Anh",
          "Nguyen"
        ],
        [
          "Trung H.",
          "Bui"
        ]
      ],
      "title": "Automatic Radiology Report Editing Through Voice",
      "original": "8026",
      "page_count": 2,
      "order": 996,
      "p1": "4862",
      "pn": "4863",
      "abstract": [
        "We present a system that allows radiologists to edit the radiology\nreport through their voices. This is a function in our bigger system\nat VinBrain LLC that uses AI algorithms to assist radiologists with\nchest x-ray diagnosis, the system can suggest the abnormalities, then\nbases on the radiologist&#8217;s confirmations or conclusions to automatically\ngenerate the report using predefined templates. We then allow the radiologist\nto freely edit the report using voice. The system combines two components,\nthe first is the Speech Recognition System (SRS), and the second is\nthe Natural Language Understanding System (NLUS) that executes the\nuser&#8217;s command. The user can delete, modify or add an arbitrary\nwhole sentence. In addition, we successfully developed an SRS for such\na non-mainstream language as Vietnamese and adapted it for the radiology\ndomain.\n"
      ]
    },
    "shi21e_interspeech": {
      "authors": [
        [
          "Ke",
          "Shi"
        ],
        [
          "Kye Min",
          "Tan"
        ],
        [
          "Huayun",
          "Zhang"
        ],
        [
          "Siti Umairah Md.",
          "Salleh"
        ],
        [
          "Shikang",
          "Ni"
        ],
        [
          "Nancy F.",
          "Chen"
        ]
      ],
      "title": "WittyKiddy: Multilingual Spoken Language Learning for Kids",
      "original": "8028",
      "page_count": 2,
      "order": 997,
      "p1": "4864",
      "pn": "4865",
      "abstract": [
        "We present <i>WittyKiddy</i>, a spoken language learning system for\nchildren, developed at the Institute for Infocomm Research (I2R), A*STAR,\nSingapore. Our system automatically evaluates a student&#8217;s oral\nproficiency by scoring pronunciation, fluency and intonation of a spoken\nutterance. We demonstrate the technical capabilities of the system\nvia reading aloud exercises and oral cloze tests in English and Malay.\nBoth quantitative and qualitative feedback are given to the student.\nOur work helps support multilingual education for children.\n"
      ]
    },
    "jin21b_interspeech": {
      "authors": [
        [
          "Chunxiang",
          "Jin"
        ],
        [
          "Minghui",
          "Yang"
        ],
        [
          "Zujie",
          "Wen"
        ]
      ],
      "title": "Duplex Conversation in Outbound Agent System",
      "original": "8032",
      "page_count": 2,
      "order": 998,
      "p1": "4866",
      "pn": "4867",
      "abstract": [
        "Intelligent outbound is a popular way to contact customers. The traditional\noutbound agents communicate with users in a simplex way. The user and\nthe agent cannot speak at the same time, and the user cannot actively\ninterrupt the conversation while the agent is playing audio generated\nby TTS. The traditional solution is based on the output of the VAD\nmodule, once the user voice is detected, the agent will immediately\nstop talking. However, the user sometimes expresses the short answer\nat will, not to interrupt the agent, and it will cause the agent to\nbe frequently interrupted. In addition, when users say named entity\nnouns(numbers, locations, company names, etc), their speech speed is\nslow and the pause time between words is longer, and they may be interrupted\nby the agent unreasonably. We propose a method to identify user&#8217;s\ninterruption requests and discontinuous expressions by analyzing the\nsemantic information of the user&#8217;s utterance. As a result, fluency\nof the dialogue is improved.\n"
      ]
    },
    "udupa21b_interspeech": {
      "authors": [
        [
          "Sathvik",
          "Udupa"
        ],
        [
          "Anwesha",
          "Roy"
        ],
        [
          "Abhayjeet",
          "Singh"
        ],
        [
          "Aravind",
          "Illa"
        ],
        [
          "Prasanta Kumar",
          "Ghosh"
        ]
      ],
      "title": "Web Interface for Estimating Articulatory Movements in Speech Production from Acoustics and Text",
      "original": "8033",
      "page_count": 2,
      "order": 999,
      "p1": "4868",
      "pn": "4869",
      "abstract": [
        "We release a web interface to visualise estimated articulatory movements\nin speech production from different modalities &#8212; acoustics and\ntext. We allow the use of various trained models for this purpose.\nThis tool also serves the purpose of comparing the predicted articulatory\nmovements from different modalities and visually understanding the\neffect of noise in speech.\n"
      ]
    }
  },
  "sessions": [
    {
      "title": "Speech Synthesis: Other Topics",
      "papers": [
        "pucher21_interspeech",
        "rezackova21_interspeech",
        "perrotin21_interspeech",
        "do21_interspeech"
      ]
    },
    {
      "title": "Disordered Speech",
      "papers": [
        "talkar21_interspeech",
        "vasquezcorrea21_interspeech",
        "daoudi21_interspeech",
        "wang21_interspeech",
        "turrisi21_interspeech"
      ]
    },
    {
      "title": "Speech Signal Analysis and Representation II",
      "papers": [
        "bie21_interspeech",
        "yurt21_interspeech",
        "prasad21_interspeech",
        "teytaut21_interspeech"
      ]
    },
    {
      "title": "Feature, Embedding and Neural Architecture for Speaker Recognition",
      "papers": [
        "kim21_interspeech",
        "qi21_interspeech",
        "zhang21_interspeech",
        "wu21_interspeech",
        "zhu21_interspeech",
        "tu21_interspeech",
        "zhu21b_interspeech",
        "liu21_interspeech",
        "zhu21c_interspeech"
      ]
    },
    {
      "title": "Speech Synthesis: Toward End-to-End Synthesis II",
      "papers": [
        "gong21_interspeech",
        "bak21_interspeech",
        "nakamura21_interspeech",
        "kakegawa21_interspeech",
        "dai21_interspeech",
        "dou21_interspeech",
        "elias21_interspeech",
        "wu21b_interspeech",
        "jia21_interspeech",
        "ge21_interspeech"
      ]
    },
    {
      "title": "Speech Enhancement and Intelligibility",
      "papers": [
        "sun21_interspeech",
        "zhang21b_interspeech",
        "pan21_interspeech",
        "biswas21_interspeech",
        "yamamoto21_interspeech",
        "liu21b_interspeech",
        "kong21_interspeech",
        "hsieh21_interspeech",
        "fu21_interspeech",
        "edraki21_interspeech",
        "qiu21_interspeech",
        "nayem21_interspeech",
        "zhang21c_interspeech"
      ]
    },
    {
      "title": "Spoken Dialogue Systems I",
      "papers": [
        "nguyen21_interspeech",
        "chen21_interspeech",
        "su21_interspeech",
        "chiba21_interspeech",
        "yamazaki21_interspeech",
        "xu21_interspeech",
        "tang21_interspeech",
        "wang21b_interspeech"
      ]
    },
    {
      "title": "Topics in ASR: Robustness, Feature Extraction, and Far-Field ASR",
      "papers": [
        "zhang21d_interspeech",
        "siminyu21_interspeech",
        "loweimi21_interspeech",
        "fujimoto21_interspeech",
        "ratnarajah21_interspeech",
        "chen21b_interspeech",
        "chang21_interspeech",
        "tsunoo21_interspeech",
        "ma21_interspeech",
        "likhomanenko21_interspeech",
        "lam21_interspeech"
      ]
    },
    {
      "title": "Voice Activity Detection and Keyword Spotting",
      "papers": [
        "hou21_interspeech",
        "kim21b_interspeech",
        "park21_interspeech",
        "ichikawa21_interspeech",
        "zhou21_interspeech",
        "makishima21_interspeech",
        "nonaka21_interspeech",
        "kwon21_interspeech",
        "wei21_interspeech",
        "bhati21_interspeech",
        "xu21b_interspeech"
      ]
    },
    {
      "title": "Voice and Voicing",
      "papers": [
        "chlebowski21_interspeech",
        "wang21c_interspeech",
        "bonneau21_interspeech",
        "chakraborty21_interspeech",
        "urooj21_interspeech",
        "tamim21_interspeech",
        "coy21_interspeech",
        "jessen21_interspeech",
        "lo21_interspeech",
        "soo21_interspeech",
        "lalhminghlui21_interspeech"
      ]
    },
    {
      "title": "The INTERSPEECH 2021 Computational Paralinguistics Challenge (ComParE) &#8212; COVID-19 Cough, COVID-19 Speech, Escalation &amp; Primates",
      "papers": [
        "schuller21_interspeech",
        "soleraurena21_interspeech",
        "klumpp21_interspeech",
        "casanova21_interspeech",
        "illium21_interspeech",
        "pellegrini21_interspeech",
        "muller21_interspeech",
        "zwerts21_interspeech",
        "rizos21_interspeech",
        "egaslopez21_interspeech",
        "verkholyak21_interspeech",
        "schiller21_interspeech"
      ]
    },
    {
      "title": "Survey Talk 1: Heidi Christensen",
      "papers": [
        "christensen21_interspeech"
      ]
    },
    {
      "title": "Embedding and Network Architecture for Speaker Recognition",
      "papers": [
        "luu21_interspeech",
        "rybicka21_interspeech",
        "stafylakis21_interspeech",
        "he21_interspeech",
        "peng21_interspeech"
      ]
    },
    {
      "title": "Speech Perception I",
      "papers": [
        "xiao21_interspeech",
        "block21_interspeech",
        "monesi21_interspeech",
        "bosch21_interspeech",
        "bosch21b_interspeech",
        "brand21_interspeech"
      ]
    },
    {
      "title": "Acoustic Event Detection and Acoustic Scene Classification",
      "papers": [
        "kim21c_interspeech",
        "wang21d_interspeech",
        "zheng21_interspeech",
        "nandi21_interspeech",
        "sundar21_interspeech",
        "gong21b_interspeech",
        "seo21_interspeech",
        "bear21_interspeech",
        "hori21_interspeech",
        "si21_interspeech",
        "deshmukh21_interspeech",
        "komatsu21_interspeech"
      ]
    },
    {
      "title": "Diverse Modes of Speech Acquisition and Processing",
      "papers": [
        "tseng21_interspeech",
        "wang21e_interspeech",
        "sharma21_interspeech",
        "abraham21_interspeech",
        "wang21f_interspeech",
        "dineley21_interspeech",
        "li21_interspeech",
        "ribeiro21_interspeech",
        "ferreira21_interspeech",
        "cao21_interspeech"
      ]
    },
    {
      "title": "Multi-Channel Speech Enhancement and Hearing Aids",
      "papers": [
        "schroter21_interspeech",
        "fontaine21_interspeech",
        "zhang21e_interspeech",
        "song21_interspeech",
        "zarazaga21_interspeech",
        "zhang21f_interspeech",
        "graetzer21_interspeech",
        "tu21b_interspeech",
        "sivasankaran21_interspeech",
        "huang21_interspeech"
      ]
    },
    {
      "title": "Self-Supervision and Semi-Supervision for Neural ASR Training",
      "papers": [
        "cao21b_interspeech",
        "sadhu21_interspeech",
        "wallington21_interspeech",
        "hsu21_interspeech",
        "higuchi21_interspeech",
        "misra21_interspeech",
        "chen21c_interspeech",
        "likhomanenko21b_interspeech",
        "yue21_interspeech",
        "deng21_interspeech"
      ]
    },
    {
      "title": "Spoken Language Processing I",
      "papers": [
        "seyfarth21_interspeech",
        "mcgowan21_interspeech",
        "rocholl21_interspeech",
        "chen21d_interspeech",
        "ihori21_interspeech",
        "lin21_interspeech",
        "wintrode21_interspeech",
        "palaskar21_interspeech",
        "lee21_interspeech",
        "wodarczak21_interspeech"
      ]
    },
    {
      "title": "Voice Conversion and Adaptation II",
      "papers": [
        "broughton21_interspeech",
        "zhou21b_interspeech",
        "ding21_interspeech",
        "he21b_interspeech",
        "chen21e_interspeech",
        "wang21g_interspeech",
        "lin21b_interspeech",
        "liberatore21_interspeech",
        "wang21h_interspeech",
        "luong21_interspeech"
      ]
    },
    {
      "title": "Privacy-Preserving Machine Learning for Audio &amp; Speech Processing",
      "papers": [
        "chouchane21_interspeech",
        "aloufi21_interspeech",
        "novotney21_interspeech",
        "ro21_interspeech",
        "koppelmann21_interspeech",
        "yang21_interspeech",
        "ma21b_interspeech",
        "shah21_interspeech",
        "fazel21_interspeech"
      ]
    },
    {
      "title": "The First DiCOVA Challenge: Diagnosis of COVID-19 Using Acoustics",
      "papers": [
        "muguli21_interspeech",
        "kamble21_interspeech",
        "karas21_interspeech",
        "sodergren21_interspeech",
        "das21_interspeech",
        "harvill21_interspeech",
        "deshpande21_interspeech",
        "ritwik21_interspeech",
        "mallolragolta21_interspeech",
        "bhosale21_interspeech",
        "avila21_interspeech"
      ]
    },
    {
      "title": "Show and Tell 1",
      "papers": [
        "kiss21_interspeech",
        "weingartova21_interspeech",
        "arai21_interspeech",
        "fabien21_interspeech",
        "flucha21_interspeech",
        "oh21_interspeech",
        "cmejla21_interspeech"
      ]
    },
    {
      "title": "Keynote 1: Hermann Ney",
      "papers": [
        "ney21_interspeech"
      ]
    },
    {
      "title": "ASR Technologies and Systems",
      "papers": [
        "chorowski21_interspeech",
        "chorowski21b_interspeech",
        "suter21_interspeech",
        "joglekar21_interspeech"
      ]
    },
    {
      "title": "Phonation and Voicing",
      "papers": [
        "leykum21_interspeech",
        "hutin21_interspeech",
        "kraljevski21_interspeech",
        "ludusan21_interspeech",
        "rodriguez21_interspeech",
        "chanclu21_interspeech"
      ]
    },
    {
      "title": "Health and Affect I",
      "papers": [
        "son21_interspeech",
        "steinert21_interspeech",
        "hecker21_interspeech",
        "nguyen21b_interspeech"
      ]
    },
    {
      "title": "Robust Speaker Recognition",
      "papers": [
        "borgstrom21_interspeech",
        "wang21i_interspeech",
        "chen21f_interspeech",
        "wang21j_interspeech",
        "brummer21_interspeech",
        "chojnacka21_interspeech",
        "wang21k_interspeech",
        "li21b_interspeech",
        "kataria21_interspeech",
        "pu21_interspeech",
        "wu21c_interspeech",
        "zhang21g_interspeech",
        "patino21_interspeech"
      ]
    },
    {
      "title": "Source Separation, Dereverberation and Echo Cancellation",
      "papers": [
        "luo21_interspeech",
        "wang21l_interspeech",
        "gu21_interspeech",
        "li21c_interspeech",
        "giri21_interspeech",
        "yemini21_interspeech",
        "tanaka21_interspeech",
        "zhang21h_interspeech",
        "na21_interspeech",
        "sato21_interspeech"
      ]
    },
    {
      "title": "Speech Signal Analysis and Representation I",
      "papers": [
        "udupa21_interspeech",
        "yang21b_interspeech",
        "jaramillo21_interspeech",
        "luo21b_interspeech",
        "yarra21_interspeech",
        "huang21b_interspeech",
        "shahrebabaki21_interspeech",
        "lilley21_interspeech",
        "yang21c_interspeech",
        "zhang21i_interspeech",
        "peplinski21_interspeech",
        "mori21_interspeech",
        "kumar21_interspeech"
      ]
    },
    {
      "title": "Spoken Language Understanding I",
      "papers": [
        "peng21b_interspeech",
        "radfar21_interspeech",
        "cao21c_interspeech",
        "muralidharan21_interspeech",
        "wu21d_interspeech",
        "chen21g_interspeech",
        "do21b_interspeech",
        "ganhotra21_interspeech",
        "han21_interspeech",
        "arora21_interspeech"
      ]
    },
    {
      "title": "Topics in ASR: Adaptation, Transfer Learning, Children&#8217;s Speech, and Low-Resource Settings",
      "papers": [
        "sun21b_interspeech",
        "gong21c_interspeech",
        "wang21m_interspeech",
        "sim21_interspeech",
        "kumar21b_interspeech",
        "xu21c_interspeech",
        "lam21b_interspeech",
        "gao21_interspeech",
        "huang21c_interspeech",
        "das21b_interspeech",
        "chu21_interspeech"
      ]
    },
    {
      "title": "Voice Conversion and Adaptation I",
      "papers": [
        "li21d_interspeech",
        "huang21d_interspeech",
        "eskimez21_interspeech",
        "koshizuka21_interspeech",
        "wang21n_interspeech",
        "li21e_interspeech",
        "kumar21c_interspeech",
        "sakamoto21_interspeech",
        "xu21d_interspeech",
        "liu21c_interspeech",
        "zhou21c_interspeech",
        "du21_interspeech"
      ]
    },
    {
      "title": "Voice Quality Characterization for Clinical Voice Assessment: Voice Production, Acoustics, and Auditory Perception",
      "papers": [
        "white21_interspeech",
        "penney21_interspeech",
        "sfakianaki21_interspeech",
        "huckvale21_interspeech",
        "lulich21_interspeech",
        "perez21_interspeech",
        "ferrer21_interspeech"
      ]
    },
    {
      "title": "Miscellanous Topics in ASR",
      "papers": [
        "karpov21_interspeech",
        "sadhu21b_interspeech",
        "alghezi21_interspeech",
        "oneill21_interspeech",
        "evain21_interspeech"
      ]
    },
    {
      "title": "Phonetics I",
      "papers": [
        "sturm21_interspeech",
        "riverincoutlee21_interspeech",
        "zellers21_interspeech",
        "bodur21_interspeech"
      ]
    },
    {
      "title": "Target Speaker Detection, Localization and Separation",
      "papers": [
        "zmolikova21_interspeech",
        "borsdorf21_interspeech",
        "mateju21_interspeech",
        "salvati21_interspeech",
        "yousefi21_interspeech"
      ]
    },
    {
      "title": "Language and Accent Recognition",
      "papers": [
        "liu21d_interspeech",
        "duroselle21_interspeech",
        "wang21o_interspeech",
        "deng21b_interspeech",
        "fan21_interspeech",
        "ramesh21_interspeech",
        "zhang21j_interspeech",
        "tzudir21_interspeech"
      ]
    },
    {
      "title": "Low-Resource Speech Recognition",
      "papers": [
        "khare21_interspeech",
        "feng21_interspeech",
        "kamper21_interspeech",
        "jiang21_interspeech",
        "jacobs21_interspeech",
        "niekerk21_interspeech",
        "takahashi21_interspeech",
        "maekaku21_interspeech",
        "cui21_interspeech",
        "dunbar21_interspeech",
        "gudur21_interspeech",
        "rouditchenko21_interspeech"
      ]
    },
    {
      "title": "Speech Synthesis: Singing, Multimodal, Crosslingual Synthesis",
      "papers": [
        "lee21b_interspeech",
        "maniati21_interspeech",
        "zhan21_interspeech",
        "yang21d_interspeech",
        "liu21e_interspeech",
        "xin21_interspeech",
        "shang21_interspeech",
        "kesim21_interspeech",
        "si21b_interspeech"
      ]
    },
    {
      "title": "Speech Coding and Privacy",
      "papers": [
        "lee21c_interspeech",
        "lin21c_interspeech",
        "wen21_interspeech",
        "zhang21k_interspeech",
        "yi21_interspeech",
        "chettri21_interspeech",
        "cheon21_interspeech",
        "drude21_interspeech",
        "siegert21_interspeech",
        "gabrys21_interspeech",
        "prajapati21_interspeech",
        "lin21d_interspeech",
        "byun21_interspeech",
        "stoidis21_interspeech"
      ]
    },
    {
      "title": "Speech Perception II",
      "papers": [
        "aldholmi21_interspeech",
        "kishiyama21_interspeech",
        "chingacham21_interspeech",
        "simantiraki21_interspeech",
        "saito21_interspeech",
        "xu21e_interspeech",
        "zhang21l_interspeech",
        "terblanche21_interspeech",
        "einfeldt21_interspeech",
        "kumar21d_interspeech",
        "zeng21_interspeech",
        "ashihara21_interspeech"
      ]
    },
    {
      "title": "Streaming for ASR/RNN Transducers",
      "papers": [
        "nguyen21c_interspeech",
        "joshi21_interspeech",
        "le21_interspeech",
        "sainath21_interspeech",
        "lu21_interspeech",
        "moriya21_interspeech",
        "schwarz21_interspeech",
        "huang21e_interspeech",
        "cui21b_interspeech",
        "doutre21_interspeech",
        "audhkhasi21_interspeech",
        "inaguma21_interspeech",
        "moritz21_interspeech",
        "kim21d_interspeech"
      ]
    },
    {
      "title": "ConferencingSpeech 2021 Challenge: Far-Field Multi-Channel Speech Enhancement for Video Conferencing",
      "papers": [
        "ren21_interspeech",
        "zhu21d_interspeech",
        "wang21p_interspeech",
        "han21b_interspeech",
        "liu21f_interspeech",
        "raj21_interspeech",
        "xue21_interspeech"
      ]
    },
    {
      "title": "Survey Talk 2: Sriram Ganapathy",
      "papers": [
        "ganapathy21_interspeech"
      ]
    },
    {
      "title": "Keynote 2: Pascale Fung",
      "papers": [
        "fung21_interspeech"
      ]
    },
    {
      "title": "Language Modeling and Text-Based Innovations for ASR",
      "papers": [
        "fohr21_interspeech",
        "benes21_interspeech",
        "gao21b_interspeech",
        "pylkkonen21_interspeech"
      ]
    },
    {
      "title": "Speaker, Language, and Privacy",
      "papers": [
        "cieri21_interspeech",
        "fenu21_interspeech",
        "zhang21m_interspeech",
        "noe21_interspeech"
      ]
    },
    {
      "title": "Assessment of Pathological Speech and Language I",
      "papers": [
        "romana21_interspeech",
        "vaysse21_interspeech",
        "qi21b_interspeech",
        "mathad21_interspeech",
        "villatorotello21_interspeech",
        "shandiz21_interspeech"
      ]
    },
    {
      "title": "Communication and Interaction, Multimodality",
      "papers": [
        "lamba21_interspeech",
        "cook21_interspeech",
        "santoso21_interspeech",
        "silpachai21_interspeech",
        "menshikova21_interspeech",
        "nasreen21_interspeech",
        "kothare21_interspeech",
        "ishi21_interspeech"
      ]
    },
    {
      "title": "Language and Lexical Modeling for ASR",
      "papers": [
        "kim21e_interspeech",
        "wang21q_interspeech",
        "shi21_interspeech",
        "papadourakis21_interspeech",
        "mansfield21_interspeech",
        "huang21f_interspeech",
        "andresferrer21_interspeech",
        "huang21g_interspeech",
        "zhao21_interspeech",
        "dai21b_interspeech",
        "kurata21_interspeech",
        "saebi21_interspeech",
        "namazifar21_interspeech"
      ]
    },
    {
      "title": "Novel Neural Network Architectures for ASR",
      "papers": [
        "shi21b_interspeech",
        "zhang21n_interspeech",
        "zeyer21_interspeech",
        "mavandadi21_interspeech",
        "tuske21_interspeech",
        "an21_interspeech",
        "you21_interspeech",
        "leong21_interspeech",
        "lin21e_interspeech",
        "karita21_interspeech",
        "hori21b_interspeech",
        "haidar21_interspeech",
        "mahadeokar21_interspeech"
      ]
    },
    {
      "title": "Speech Localization, Enhancement, and Quality Assessment",
      "papers": [
        "falkowskigilski21_interspeech",
        "schymura21_interspeech",
        "togami21_interspeech",
        "mittag21_interspeech",
        "naderi21_interspeech",
        "geng21_interspeech",
        "yu21_interspeech",
        "toma21_interspeech",
        "itoyama21_interspeech",
        "liu21g_interspeech",
        "lin21f_interspeech",
        "narayanaswamy21_interspeech",
        "chen21h_interspeech"
      ]
    },
    {
      "title": "Speech Synthesis: Neural Waveform Generation",
      "papers": [
        "you21b_interspeech",
        "cong21_interspeech",
        "yoneyama21_interspeech",
        "mizuta21_interspeech",
        "kim21f_interspeech",
        "yang21e_interspeech",
        "jang21_interspeech",
        "alradhi21_interspeech",
        "tobing21_interspeech",
        "liu21h_interspeech",
        "hwang21_interspeech"
      ]
    },
    {
      "title": "Spoken Machine Translation",
      "papers": [
        "chen21i_interspeech",
        "cherry21_interspeech",
        "wang21r_interspeech",
        "wang21s_interspeech",
        "cheng21_interspeech",
        "effendi21_interspeech",
        "tokuyama21_interspeech",
        "ye21_interspeech",
        "ko21_interspeech",
        "perezgonzalezdemartos21_interspeech",
        "martucci21_interspeech",
        "vyas21_interspeech",
        "ananthanarayana21_interspeech"
      ]
    },
    {
      "title": "SdSV Challenge 2021: Analysis and Exploration of New Ideas on Short-Duration Speaker Verification",
      "papers": [
        "alenin21_interspeech",
        "thienpondt21_interspeech",
        "gusev21_interspeech",
        "kang21_interspeech",
        "qin21_interspeech",
        "zhang21o_interspeech",
        "yan21_interspeech",
        "han21c_interspeech"
      ]
    },
    {
      "title": "Show and Tell 2",
      "papers": [
        "cho21_interspeech",
        "prazak21_interspeech",
        "fragner21_interspeech",
        "beskow21_interspeech",
        "dominguez21_interspeech",
        "guruju21_interspeech",
        "gogineni21_interspeech"
      ]
    },
    {
      "title": "Graph and End-to-End Learning for Speaker Recognition",
      "papers": [
        "raj21b_interspeech",
        "tak21_interspeech",
        "mingote21_interspeech",
        "peng21c_interspeech"
      ]
    },
    {
      "title": "Spoken Language Processing II",
      "papers": [
        "nguyen21d_interspeech",
        "machacek21_interspeech",
        "pouthier21_interspeech",
        "wallbridge21_interspeech"
      ]
    },
    {
      "title": "Speech and Audio Analysis",
      "papers": [
        "michael21_interspeech",
        "bergler21_interspeech",
        "boes21_interspeech",
        "nessler21_interspeech",
        "oncescu21_interspeech"
      ]
    },
    {
      "title": "Cross/Multi-Lingual and Code-Switched ASR",
      "papers": [
        "giollo21_interspeech",
        "pham21_interspeech",
        "conneau21_interspeech",
        "hayakawa21_interspeech",
        "n21_interspeech",
        "kumar21e_interspeech",
        "diwan21_interspeech",
        "winata21_interspeech",
        "sailor21_interspeech",
        "li21f_interspeech",
        "chowdhury21_interspeech",
        "yan21b_interspeech"
      ]
    },
    {
      "title": "Health and Affect II",
      "papers": [
        "martin21_interspeech",
        "gillick21_interspeech",
        "nagano21_interspeech",
        "alsofyani21_interspeech",
        "aloshban21_interspeech",
        "tammewar21_interspeech",
        "condron21_interspeech",
        "cai21_interspeech",
        "botelho21_interspeech",
        "maruri21_interspeech",
        "seneviratne21_interspeech"
      ]
    },
    {
      "title": "Neural Network Training Methods for ASR",
      "papers": [
        "kim21g_interspeech",
        "macoskey21_interspeech",
        "zhang21p_interspeech",
        "xue21b_interspeech",
        "chang21b_interspeech",
        "leal21_interspeech",
        "xu21f_interspeech",
        "wang21t_interspeech",
        "chen21j_interspeech",
        "droppo21_interspeech",
        "billa21_interspeech",
        "fasoli21_interspeech",
        "masumura21_interspeech",
        "meng21_interspeech",
        "jiang21b_interspeech"
      ]
    },
    {
      "title": "Prosodic Features and Structure",
      "papers": [
        "kaland21_interspeech",
        "jespersen21_interspeech",
        "meli21_interspeech",
        "gerazov21_interspeech",
        "tran21_interspeech",
        "liu21i_interspeech",
        "gosy21_interspeech",
        "pan21b_interspeech",
        "jakob21_interspeech",
        "gobl21_interspeech",
        "wagner21_interspeech",
        "mumtaz21_interspeech",
        "stefansdottir21_interspeech",
        "johnson21_interspeech"
      ]
    },
    {
      "title": "Single-Channel Speech Enhancement",
      "papers": [
        "sivaraman21_interspeech",
        "saddler21_interspeech",
        "eskimez21b_interspeech",
        "xu21g_interspeech",
        "chang21c_interspeech",
        "zhang21q_interspeech",
        "agrawal21_interspeech",
        "lee21d_interspeech",
        "kashyap21_interspeech",
        "dang21_interspeech",
        "zhang21r_interspeech",
        "bu21_interspeech",
        "kim21h_interspeech"
      ]
    },
    {
      "title": "Speech Synthesis: Tools, Data, Evaluation",
      "papers": [
        "kongthaworn21_interspeech",
        "valentinibotinhao21_interspeech",
        "zandie21_interspeech",
        "shi21c_interspeech",
        "eng21_interspeech",
        "cui21c_interspeech",
        "rallabandi21_interspeech",
        "bakhturina21_interspeech",
        "tseng21b_interspeech",
        "mussakhojayeva21_interspeech",
        "taylor21_interspeech"
      ]
    },
    {
      "title": "INTERSPEECH 2021 Deep Noise Suppression Challenge",
      "papers": [
        "reddy21_interspeech",
        "li21g_interspeech",
        "xu21h_interspeech",
        "le21b_interspeech",
        "lv21_interspeech",
        "zhang21s_interspeech",
        "zhang21t_interspeech",
        "oostermeijer21_interspeech"
      ]
    },
    {
      "title": "Neural Network Training Methods and Architectures for ASR",
      "papers": [
        "ristea21_interspeech",
        "kojima21_interspeech",
        "lohrenz21_interspeech",
        "zaiem21_interspeech",
        "zeineldeen21_interspeech",
        "vyas21b_interspeech"
      ]
    },
    {
      "title": "Emotion and Sentiment Analysis I",
      "papers": [
        "moine21_interspeech",
        "leem21_interspeech",
        "georgiou21_interspeech"
      ]
    },
    {
      "title": "Linguistic Components in End-to-End ASR",
      "papers": [
        "klejch21_interspeech",
        "zhou21d_interspeech",
        "zhou21e_interspeech",
        "khosravani21_interspeech",
        "egorova21_interspeech",
        "wiesner21_interspeech"
      ]
    },
    {
      "title": "Assessment of Pathological Speech and Language II",
      "papers": [
        "xue21c_interspeech",
        "kim21i_interspeech",
        "jesko21_interspeech",
        "fivela21_interspeech",
        "ng21_interspeech",
        "hair21_interspeech",
        "mirheidari21_interspeech",
        "yue21b_interspeech",
        "xia21_interspeech",
        "wang21u_interspeech",
        "bhattacharjee21_interspeech",
        "haulcy21_interspeech"
      ]
    },
    {
      "title": "Multimodal Systems",
      "papers": [
        "nortje21_interspeech",
        "sanabria21_interspeech",
        "zhao21b_interspeech",
        "wang21v_interspeech",
        "olaleye21_interspeech",
        "khorrami21_interspeech",
        "chen21k_interspeech",
        "rouditchenko21b_interspeech",
        "ma21c_interspeech",
        "rose21_interspeech",
        "wu21e_interspeech"
      ]
    },
    {
      "title": "Source Separation I",
      "papers": [
        "chen21l_interspeech",
        "ali21_interspeech",
        "han21d_interspeech",
        "yuan21_interspeech",
        "wang21w_interspeech",
        "nakagome21_interspeech",
        "huang21h_interspeech",
        "wang21x_interspeech",
        "wu21f_interspeech",
        "luo21c_interspeech",
        "xu21i_interspeech"
      ]
    },
    {
      "title": "Speaker Diarization I",
      "papers": [
        "liu21j_interspeech",
        "jung21_interspeech",
        "wan21_interspeech",
        "takashima21_interspeech",
        "kwon21b_interspeech",
        "wang21y_interspeech",
        "bredin21_interspeech",
        "xue21d_interspeech",
        "anidjar21_interspeech"
      ]
    },
    {
      "title": "Speech Synthesis: Prosody Modeling I",
      "papers": [
        "futamata21_interspeech",
        "vallesperez21_interspeech",
        "du21b_interspeech",
        "fujita21_interspeech",
        "zou21_interspeech",
        "sharma21b_interspeech",
        "zhang21u_interspeech",
        "baird21_interspeech"
      ]
    },
    {
      "title": "Speech Production II",
      "papers": [
        "yoshinaga21_interspeech",
        "arai21b_interspeech",
        "tanji21_interspeech",
        "inaam21_interspeech",
        "werner21_interspeech",
        "xu21j_interspeech",
        "elmers21_interspeech",
        "chen21m_interspeech",
        "kawahara21_interspeech"
      ]
    },
    {
      "title": "Spoken Dialogue Systems II",
      "papers": [
        "you21c_interspeech",
        "duan21_interspeech",
        "rohmatillah21_interspeech",
        "fujie21_interspeech",
        "chen21n_interspeech",
        "sundararaman21_interspeech",
        "luo21d_interspeech",
        "shenoy21_interspeech"
      ]
    },
    {
      "title": "Oriental Language Recognition",
      "papers": [
        "li21h_interspeech",
        "duroselle21b_interspeech",
        "kong21b_interspeech",
        "wang21z_interspeech",
        "yu21b_interspeech",
        "li21i_interspeech"
      ]
    },
    {
      "title": "Automatic Speech Recognition in Air Traffic Management",
      "papers": [
        "jahchan21_interspeech",
        "szoke21_interspeech",
        "ohneiser21_interspeech",
        "zuluagagomez21_interspeech",
        "kocour21_interspeech",
        "elie21_interspeech"
      ]
    },
    {
      "title": "Show and Tell 3",
      "papers": [
        "milde21_interspeech",
        "wilbrandt21_interspeech",
        "codinafilba21_interspeech",
        "rownicka21_interspeech",
        "geislinger21_interspeech",
        "nicmanis21_interspeech",
        "kachare21_interspeech"
      ]
    },
    {
      "title": "Survey Talk 3: Karen Livescu",
      "papers": [
        "livescu21_interspeech"
      ]
    },
    {
      "title": "Keynote 3: Mounya Elhilali",
      "papers": [
        "elhilali21_interspeech"
      ]
    },
    {
      "title": "Speech Production I",
      "papers": [
        "ribeiro21b_interspeech",
        "blandin21_interspeech",
        "wagner21b_interspeech",
        "medina21_interspeech",
        "georges21_interspeech",
        "weston21_interspeech"
      ]
    },
    {
      "title": "Speech Enhancement and Coding",
      "papers": [
        "vali21_interspeech",
        "nareddula21_interspeech",
        "marcinek21_interspeech"
      ]
    },
    {
      "title": "Emotion and Sentiment Analysis II",
      "papers": [
        "xia21b_interspeech",
        "li21j_interspeech",
        "vaaras21_interspeech",
        "qian21_interspeech",
        "t21_interspeech",
        "li21k_interspeech",
        "pepino21_interspeech",
        "liu21k_interspeech",
        "kumawat21_interspeech",
        "keesing21_interspeech",
        "shon21_interspeech"
      ]
    },
    {
      "title": "Multi- and Cross-Lingual ASR, Other Topics in ASR",
      "papers": [
        "hou21b_interspeech",
        "kanda21_interspeech",
        "lu21b_interspeech",
        "kim21j_interspeech",
        "diwan21b_interspeech",
        "fukuda21_interspeech",
        "ray21_interspeech",
        "lu21c_interspeech",
        "delrio21_interspeech",
        "sun21c_interspeech",
        "ali21b_interspeech"
      ]
    },
    {
      "title": "Source Separation II",
      "papers": [
        "eisenberg21_interspeech",
        "luo21e_interspeech",
        "neumann21_interspeech",
        "zhang21v_interspeech",
        "delcroix21_interspeech",
        "han21e_interspeech",
        "hu21_interspeech",
        "sarkar21_interspeech",
        "maciejewski21_interspeech",
        "lan21_interspeech",
        "deng21c_interspeech",
        "wang21aa_interspeech",
        "rigal21_interspeech"
      ]
    },
    {
      "title": "Speaker Diarization II",
      "papers": [
        "singh21_interspeech",
        "zhang21w_interspeech",
        "he21c_interspeech",
        "dawalatabad21_interspeech",
        "kinoshita21_interspeech",
        "ryant21_interspeech",
        "leung21_interspeech",
        "obrien21_interspeech",
        "karra21_interspeech"
      ]
    },
    {
      "title": "Speech Synthesis: Toward End-to-End Synthesis I",
      "papers": [
        "hong21_interspeech",
        "nguyen21e_interspeech",
        "tang21b_interspeech",
        "jeong21_interspeech",
        "bae21_interspeech",
        "polyak21_interspeech",
        "karanasou21_interspeech",
        "paul21_interspeech",
        "wu21g_interspeech",
        "chung21_interspeech",
        "lin21g_interspeech",
        "casanova21b_interspeech"
      ]
    },
    {
      "title": "Tools, Corpora and Resources",
      "papers": [
        "palmer21_interspeech",
        "salesky21_interspeech",
        "mortensen21_interspeech",
        "fu21b_interspeech",
        "chen21o_interspeech",
        "kim21k_interspeech",
        "ahmed21_interspeech",
        "fallgren21_interspeech",
        "ryumina21_interspeech",
        "garcesdiazmunio21_interspeech",
        "kapoor21_interspeech",
        "cho21b_interspeech",
        "zhang21x_interspeech"
      ]
    },
    {
      "title": "Non-Autoregressive Sequential Modeling for Speech Processing",
      "papers": [
        "fan21b_interspeech",
        "guo21_interspeech",
        "ng21b_interspeech",
        "liu21l_interspeech",
        "nozaki21_interspeech",
        "fujita21b_interspeech",
        "lee21e_interspeech",
        "li21l_interspeech",
        "wang21ba_interspeech",
        "beliaev21_interspeech",
        "chen21p_interspeech",
        "chen21q_interspeech",
        "lu21d_interspeech"
      ]
    },
    {
      "title": "The ADReSSo Challenge: Detecting Cognitive Decline Using Speech Only",
      "papers": [
        "luz21_interspeech",
        "pereztoro21_interspeech",
        "zhu21e_interspeech",
        "gauder21_interspeech",
        "balagopalan21_interspeech",
        "qiao21_interspeech",
        "pan21c_interspeech",
        "syed21_interspeech",
        "rohanian21_interspeech",
        "pappagari21_interspeech",
        "chen21r_interspeech",
        "wang21ca_interspeech"
      ]
    },
    {
      "title": "Robust and Far-Field ASR",
      "papers": [
        "gong21d_interspeech",
        "gretter21_interspeech",
        "rumberg21_interspeech",
        "cornell21_interspeech",
        "gelin21_interspeech"
      ]
    },
    {
      "title": "Speech Synthesis: Prosody Modeling II",
      "papers": [
        "stephenson21_interspeech",
        "rijn21_interspeech",
        "mohan21_interspeech",
        "torresquintero21_interspeech",
        "trang21_interspeech"
      ]
    },
    {
      "title": "Source Separation III",
      "papers": [
        "dovrat21_interspeech",
        "fras21_interspeech",
        "strauss21_interspeech",
        "borsdorf21b_interspeech"
      ]
    },
    {
      "title": "Non-Native Speech",
      "papers": [
        "tsukada21_interspeech",
        "korzekwa21_interspeech",
        "braun21_interspeech",
        "zhang21y_interspeech",
        "mirzaei21_interspeech",
        "ding21b_interspeech",
        "lin21h_interspeech",
        "kudera21_interspeech",
        "zhang21z_interspeech",
        "wu21h_interspeech",
        "graham21_interspeech"
      ]
    },
    {
      "title": "Phonetics II",
      "papers": [
        "oh21b_interspeech",
        "liu21m_interspeech",
        "hejna21_interspeech",
        "muhlack21_interspeech",
        "ge21b_interspeech",
        "huang21i_interspeech",
        "gibson21_interspeech",
        "gully21_interspeech",
        "guevararukoz21_interspeech",
        "guo21b_interspeech",
        "meister21_interspeech",
        "dapolito21_interspeech",
        "heeringa21_interspeech",
        "billington21_interspeech"
      ]
    },
    {
      "title": "Search/Decoding Techniques and Confidence Measures for ASR",
      "papers": [
        "tian21_interspeech",
        "mitrofanov21_interspeech",
        "allauzen21_interspeech",
        "inaguma21b_interspeech",
        "yao21_interspeech",
        "tanaka21b_interspeech",
        "lee21f_interspeech",
        "li21m_interspeech",
        "qiu21b_interspeech",
        "ollerenshaw21_interspeech",
        "afshan21_interspeech"
      ]
    },
    {
      "title": "Speech Synthesis: Linguistic Processing, Paradigms and Other Topics",
      "papers": [
        "tjandra21_interspeech",
        "choi21_interspeech",
        "zhang21aa_interspeech",
        "li21n_interspeech",
        "shi21d_interspeech",
        "chen21s_interspeech",
        "zhou21f_interspeech",
        "novitasari21_interspeech",
        "zhang21ba_interspeech"
      ]
    },
    {
      "title": "Speech Type Classification and Diagnosis",
      "papers": [
        "mansbach21_interspeech",
        "fakhry21_interspeech",
        "ma21d_interspeech",
        "dhamyal21_interspeech",
        "yan21c_interspeech",
        "paul21b_interspeech",
        "takeda21_interspeech",
        "teh21_interspeech",
        "fukumori21_interspeech",
        "baghel21_interspeech",
        "gao21c_interspeech",
        "chen21t_interspeech"
      ]
    },
    {
      "title": "Spoken Term Detection &amp; Voice Search",
      "papers": [
        "abdullah21_interspeech",
        "gao21d_interspeech",
        "rikhye21_interspeech",
        "garg21_interspeech",
        "mazumder21_interspeech",
        "wang21da_interspeech",
        "chen21u_interspeech",
        "lee21g_interspeech",
        "zhou21g_interspeech",
        "jia21b_interspeech",
        "wang21ea_interspeech",
        "berg21_interspeech",
        "awasthi21_interspeech"
      ]
    },
    {
      "title": "Voice Anti-Spoofing and Countermeasure",
      "papers": [
        "wang21fa_interspeech",
        "zhang21ca_interspeech",
        "xie21_interspeech",
        "cheng21b_interspeech",
        "zhang21da_interspeech",
        "peng21d_interspeech",
        "ling21_interspeech",
        "wu21i_interspeech",
        "kinnunen21_interspeech",
        "villalba21_interspeech",
        "zhang21ea_interspeech",
        "li21o_interspeech",
        "ge21c_interspeech"
      ]
    },
    {
      "title": "OpenASR20 and Low Resource ASR Development",
      "papers": [
        "peterson21_interspeech",
        "madikeri21_interspeech",
        "zhu21f_interspeech",
        "lin21i_interspeech",
        "zhao21c_interspeech",
        "alumae21_interspeech",
        "morris21_interspeech"
      ]
    },
    {
      "title": "Survey Talk 4: Alejandrina Cristia",
      "papers": [
        "cristia21_interspeech"
      ]
    },
    {
      "title": "Keynote 4: Tom&#225;&#353; Mikolov",
      "papers": [
        "mikolov21_interspeech"
      ]
    },
    {
      "title": "Voice Activity Detection",
      "papers": [
        "gimeno21_interspeech",
        "vuong21_interspeech",
        "sarfjoo21_interspeech",
        "luckenbaugh21_interspeech",
        "ghahabi21_interspeech"
      ]
    },
    {
      "title": "Keyword Search and Spoken Language Processing",
      "papers": [
        "opatka21_interspeech",
        "yusuf21_interspeech",
        "merkx21_interspeech",
        "svec21_interspeech",
        "buet21_interspeech"
      ]
    },
    {
      "title": "Applications in Transcription, Education and Learning",
      "papers": [
        "korzekwa21b_interspeech",
        "kanda21b_interspeech",
        "soltau21_interspeech",
        "vidal21_interspeech",
        "xu21k_interspeech",
        "ando21_interspeech",
        "lin21j_interspeech",
        "zhang21fa_interspeech",
        "peng21e_interspeech",
        "qiao21b_interspeech",
        "tanaka21c_interspeech",
        "cumbal21_interspeech",
        "zhang21ga_interspeech",
        "naijo21_interspeech"
      ]
    },
    {
      "title": "Emotion and Sentiment Analysis III",
      "papers": [
        "haider21_interspeech",
        "liu21n_interspeech",
        "li21p_interspeech",
        "ito21_interspeech",
        "bose21_interspeech",
        "gao21e_interspeech",
        "cai21b_interspeech",
        "seneviratne21b_interspeech",
        "wang21ga_interspeech",
        "liu21o_interspeech"
      ]
    },
    {
      "title": "Resource-Constrained ASR",
      "papers": [
        "mordido21_interspeech",
        "cheng21c_interspeech",
        "kim21l_interspeech",
        "swaminathan21_interspeech",
        "gao21f_interspeech",
        "shangguan21_interspeech",
        "macoskey21b_interspeech",
        "botros21_interspeech",
        "kim21m_interspeech",
        "nagaraja21_interspeech",
        "wang21ha_interspeech",
        "parcollet21_interspeech"
      ]
    },
    {
      "title": "Speaker Recognition: Applications",
      "papers": [
        "chen21v_interspeech",
        "li21q_interspeech",
        "cumani21_interspeech",
        "pelecanos21_interspeech",
        "kataria21b_interspeech",
        "padfield21_interspeech",
        "xiao21b_interspeech",
        "obrien21b_interspeech",
        "tong21_interspeech",
        "liao21_interspeech",
        "lian21_interspeech"
      ]
    },
    {
      "title": "Speech Synthesis: Speaking Style and Emotion",
      "papers": [
        "lee21h_interspeech",
        "liu21p_interspeech",
        "sivaprasad21_interspeech",
        "cong21b_interspeech",
        "kim21n_interspeech",
        "yan21d_interspeech",
        "li21r_interspeech",
        "pan21d_interspeech",
        "tan21_interspeech",
        "an21b_interspeech",
        "shechtman21_interspeech"
      ]
    },
    {
      "title": "Spoken Language Understanding II",
      "papers": [
        "dao21_interspeech",
        "lin21k_interspeech",
        "gaspers21_interspeech",
        "jiang21c_interspeech",
        "wang21ia_interspeech",
        "cha21_interspeech",
        "zhang21ha_interspeech",
        "saghir21_interspeech",
        "saxon21_interspeech",
        "han21f_interspeech"
      ]
    },
    {
      "title": "INTERSPEECH 2021 Acoustic Echo Cancellation Challenge",
      "papers": [
        "cutler21_interspeech",
        "pfeifenberger21_interspeech",
        "zhang21ia_interspeech",
        "seidel21_interspeech",
        "peng21f_interspeech",
        "ivry21_interspeech"
      ]
    },
    {
      "title": "Speech Recognition of Atypical Speech",
      "papers": [
        "green21_interspeech",
        "neumann21b_interspeech",
        "hermann21_interspeech",
        "geng21b_interspeech",
        "gutz21_interspeech",
        "jin21_interspeech",
        "xie21b_interspeech",
        "wang21ja_interspeech",
        "deng21d_interspeech",
        "cai21c_interspeech",
        "chen21w_interspeech",
        "macdonald21_interspeech",
        "yeo21_interspeech",
        "venugopalan21_interspeech",
        "mitra21_interspeech"
      ]
    },
    {
      "title": "Show and Tell 4",
      "papers": [
        "kawahara21b_interspeech",
        "tihelka21_interspeech",
        "zhang21ja_interspeech",
        "hembise21_interspeech",
        "nguyen21f_interspeech",
        "shi21e_interspeech",
        "jin21b_interspeech",
        "udupa21b_interspeech"
      ]
    }
  ],
  "doi": "10.21437/Interspeech.2021"
}