{
 "title": "2nd Workshop on Child Computer Interaction (WOCCI 2009)",
 "location": "Cambridge, MA, USA",
 "startDate": "5/11/2009",
 "endDate": "5/11/2009",
 "conf": "WOCCI",
 "year": "2009",
 "name": "wocci_2009",
 "series": "WOCCI",
 "SIG": "CHILD",
 "title1": "2nd Workshop on Child Computer Interaction",
 "title2": "(WOCCI 2009)",
 "date": "5 November 2009",
 "booklet": "wocci_2009.pdf",
 "papers": {
  "ward09_wocci": {
   "authors": [
    [
     "Wayne H.",
     "Ward"
    ],
    [
     "Ronald A.",
     "Cole"
    ]
   ],
   "title": "Conversations with a virtual science tutor in multimedia learning environments",
   "original": "wc09_121",
   "page_count": 2,
   "order": 1,
   "p1": "121",
   "pn": "122",
   "abstract": [
    "During the past two years, a team of researchers, developers and teachers at Boulder Language Technologies has been collaborating with colleagues at the University of Colorado and the University of Pittsburgh to develop My Science Tutor (MyST), an intelligent tutoring system that is designed to improve science achievement of third, fourth and fifth grade students through conversational spoken dialogs with a virtual science tutor. The virtual tutor is a lifelike computer character that produces accurate visual speech (synchronized with recorded or synthesized speech) and emotions. We are developing spoken dialogs that use open ended questions such as “So, what did you learn about today?” “Ah, you built a circuit, tell me more about that.” The dialogs are guided by principles of Questioning the Author, an approach designed to manage classroom conversations to improve comprehension of stories, which we have modified for one on one tutoring in collaboration with Margaret Mckeown, one of the co-developers of the program. In our project, children leave the classroom to interact with My Science Tutor following classroom science investigations that are part of the FOSS science program, used in over 100,000 classrooms by over 2 million children in the United States. The conversational dialogs in MyST are designed to help children arrive at accurate explanations of the science observations, data and concepts encountered in their recent hands-on classroom science investigations. In addition to open ended questions, MyST presents illustrations and flash animations at appropriate times to focus the dialog on specific phenomena and concepts.   In our talk, we will describe both the system architecture (the spoken dialog system and component modules) and the process of developing and integrating the speech recognition, natural language understanding, dialog modeling and character animation technologies used in the system. The project also involves the development of a corpus of annotated dialogs collected during the tutoring sessions. A Wizard of Oz procedure is used for data collection. In this procedure, students are interacting with the virtual tutor, while a human tutor at a second computer is mediating the interaction to ensure a fluent dialog. All interaction data for the sessions are logged by the system. The student speech is transcribed and the logs are then annotated for semantic content. To date we have recorded and transcribed data for over 1000 individual tutoring sessions; over 100 hours of children’s speech. We will demonstrate the system in both Wizard of Oz and standalone modes, and present initial results comparing students’ knowledge of science concepts on standardized tests for students in the same classroom who either did or did not use the system.\n",
    ""
   ]
  },
  "bocklet09_wocci": {
   "authors": [
    [
     "Tobias",
     "Bocklet"
    ],
    [
     "Cordula",
     "Winterholler"
    ],
    [
     "Andreas",
     "Maier"
    ],
    [
     "Maria",
     "Schuster"
    ],
    [
     "Elmar",
     "Nöth"
    ]
   ],
   "title": "An automatic screening test for preschool children: theory and data collection",
   "original": "wc09_001",
   "page_count": 4,
   "order": 2,
   "p1": "1",
   "pn": "4",
   "abstract": [
    "In this paper the first version of a recording and evaluation system for an ongoing research project is presented: an automatic school enrollment screening test for children in preschool age. The screening test focuses on different aspects of the development of children’s speech and language and has a duration of about 15 minutes. It is based on parts of already standardized tests. For a first version these tests are digitized, and adapted to an existing computer system for speech assessment so that recordings can be performed in local preschools. The recordings take place while a speech therapist is assisting the child. The described version focuses on the data collection and an subjective speech and language assessment of the attendant speech therapist. Children that are awaiting school enrollment in fall, have been selected for this project. It is planed to collect a total amount of more than 500 children within the next 2 years. It is planed to record the children continuously over a period of 3-4 years at an interval of 12 months in order to measure their development automatically.\n",
    ""
   ]
  },
  "kasuriya09_wocci": {
   "authors": [
    [
     "Sawit",
     "Kasuriya"
    ],
    [
     "Alistair D. N.",
     "Edwards"
    ]
   ],
   "title": "Pilot experiments on children’s voice recording",
   "original": "wc09_005",
   "page_count": 5,
   "order": 3,
   "p1": "5",
   "pn": "9",
   "abstract": [
    "Automatic speech recognition is being used increasingly in a variety of applications. There is great potential for its use in educational applications for children. However, the accuracy of recognition of child speech is very low. There are probably a number of reasons for this, but one is the difficulty in collecting high-quality recordings of children to be used in the building of speech models. If a better interface can be provided between the child and the recording equipment then it may be possible to collect better samples. Interfaces have been designed to be tested to that end, using alternative interface paradigms: push-to-talk and a limited time recording with and without a progress bar. These alternatives will be compared by collecting speech samples and measuring their quality.\n",
    ""
   ]
  },
  "ibanezmartinez09_wocci": {
   "authors": [
    [
     "Jesús",
     "Ibáñez-Martínez"
    ],
    [
     "Carlos",
     "Delgado-Mata"
    ]
   ],
   "title": "From competitive to social two-player videogames",
   "original": "wc09_011",
   "page_count": 5,
   "order": 4,
   "p1": "11",
   "pn": "15",
   "abstract": [
    "In this paper we present a strategy to design social videogames (from classic competitive ones) which allow parents to play with their children and have fun in spite of their different levels. A first tennis videogame (based on the classic Pong) which implements this strategy has already been developed. In the paper we first motivate this work and briefly survey related work. Then we describe the general strategy and its application to the case of the tennis videogame.\n",
    ""
   ]
  },
  "bosch09_wocci": {
   "authors": [
    [
     "Louis ten",
     "Bosch"
    ],
    [
     "Lou",
     "Boves"
    ],
    [
     "Okko",
     "Räsänen"
    ]
   ],
   "title": "Learning meaningful units from multimodal input – the effect of interaction strategies",
   "original": "wc09_017",
   "page_count": 5,
   "order": 5,
   "p1": "17",
   "pn": "21",
   "abstract": [
    "This paper describes a computational model of language acquisition based on meaningful interaction between an infant and its caregivers. Learning takes place in an interactive loop between (virtual) caregiver and (virtual) learner who only uses general and cognitively plausible learning strategies and who does not rely on unrealistic prior knowledge about linguistic categories. In this work, the model is used to study the effects of different attentional factors in learning of word-object paring during learner-caregiver interaction.\n",
    ""
   ]
  },
  "bocklet09b_wocci": {
   "authors": [
    [
     "Tobias",
     "Bocklet"
    ],
    [
     "Andreas",
     "Maier"
    ],
    [
     "Korbinian",
     "Riedhammer"
    ],
    [
     "Elmar",
     "Nöth"
    ]
   ],
   "title": "Towards a language-independent intelligibility assessment of children with cleft lip and palate",
   "original": "wc09_023",
   "page_count": 4,
   "order": 6,
   "p1": "23",
   "pn": "26",
   "abstract": [
    "We describe a novel evaluation system for the intelligibility assessment of children with CLP on standardized tests. The system is solely based on standard cepstral features in form of MFCCs. No other information like word alignments is used. So the system can be easily adapted to other languages. For each child one GMM is created by adaptation of a UBM to the speaker-specific MFCCs. The components of this GMM are concatenated in order to create a so-called GMM supervector. These GMM supervectors are then used as meta features for an SVR. We evaluated our languageindependent system on two different datasets of children suffering from CLP. One dataset contains recordings of 35 German children, where the children named different pictograms. The other dataset contains recordings of 14 Italian speaking children, who repeated standardized sentences. On both datasets we achieved high correlations: up to 0.81 for the German dataset and 0.83 for the Italian dataset.\n",
    ""
   ]
  },
  "maier09_wocci": {
   "authors": [
    [
     "Andreas",
     "Maier"
    ],
    [
     "Stefanie",
     "Horndasch"
    ],
    [
     "Elmar",
     "Nöth"
    ]
   ],
   "title": "Automatic classification of reading disorders in a single word reading test",
   "original": "wc09_027",
   "page_count": 4,
   "order": 7,
   "p1": "27",
   "pn": "30",
   "abstract": [
    "In clinical practice, reading disorders are still evaluated perceptually. In order to alleviate this problem, we propose to use automatic speech processing techniques to classify reading disorders. Therefore, we recorded 38 children who were suspected to have a reading disorder. The recordings were performed using a German standard test for reading disorders. Each child was perceptually assessed and the number of reading errors per child was recorded. Furthermore, the reading duration was stored for each child. If either of both values exceeded an age-dependent limit, the child was diagnosed having a reading disorder. In 30 of the 38 children the reading disorder was confirmed. In this paper, we present the results on the automatic evaluation concerning a single word reading test. We achieve up to 78.9% recognition rate in the detection of the exceedance of the reading error limit and 97.4% recognition rate in the classification for reading disorder.\n",
    ""
   ]
  },
  "saz09_wocci": {
   "authors": [
    [
     "Oscar",
     "Saz"
    ],
    [
     "Eduardo",
     "Lleida"
    ],
    [
     "W.-Ricardo",
     "Rodríguez"
    ]
   ],
   "title": "Avoiding speaker variability in pronunciation verification of children’ disordered speech",
   "original": "wc09_031",
   "page_count": 5,
   "order": 8,
   "p1": "31",
   "pn": "35",
   "abstract": [
    "This paper deals with the problematic of speaker variability in a task of pronunciation verification for the speech therapy of children and young adults in Computer-Aided Pronunciation Training (CAPT) tools. The baseline system evaluates two different score normalization techniques: Traditional Test normalization (T-norm), and a novel N-best based normalization that outperforms the first by normalizing to the log-likelihood score of the first alternative phoneme in an unconstrained N-best list. When performing speaker adaptation, the use of all the adaptation data from the speaker improves the performance measured in Equal Error Rate (EER) of these systems compared to the speaker independent systems; but this can be outperformed by more precise models that only adapt to the correctly pronounced phonetic units as labeled by a set of human experts. The best EER obtained in all experiments is 15.63% when using both elements: Score normalization and speaker adaptation. The possibility of automatizing a more precise adaptation without the human intervention is finally proposed and discussed.\n",
    ""
   ]
  },
  "kannetis09_wocci": {
   "authors": [
    [
     "Theofanis",
     "Kannetis"
    ],
    [
     "Alexandros",
     "Potamianos"
    ],
    [
     "Georgios N.",
     "Yannakakis"
    ]
   ],
   "title": "Fantasy, curiosity and challenge as adaptation indicators in multimodal dialogue systems for preschoolers",
   "original": "wc09_037",
   "page_count": 6,
   "order": 9,
   "p1": "37",
   "pn": "42",
   "abstract": [
    "In this paper, we investigate how fantasy, curiosity and challenge contribute to the user experience in multimodal dialogue computer games for preschool children. For this purpose, an on-line multimodal platform has been designed, implemented and used as a starting point to develop five task oriented games suitable for preschoolers, with varying levels of fantasy and curiosity elements, as well as, variable difficulty levels. Nine preschool children were asked to play these games in different configurations and choose the application setup that they enjoyed most. Results show that fantasy and curiosity are correlated with children's entertainment, while the level of difficulty seems to depend on each child's individual preferences and capabilities. In addition, a variety of objective metrics (task completion, interaction time, wrong answers), audio features and emotional state have been investigated as potential features that can predict optimal levels of fantasy, curiosity and difficulty for each child. Emotional state recognition results are also reported.\n",
    ""
   ]
  },
  "brooks09_wocci": {
   "authors": [
    [
     "Douglas",
     "Brooks"
    ],
    [
     "Ayanna M.",
     "Howard"
    ]
   ],
   "title": "Upper limb rehabilitation and evaluation of children using a humanoid robot",
   "original": "wc09_043",
   "page_count": 5,
   "order": 10,
   "p1": "43",
   "pn": "47",
   "abstract": [
    "This paper discusses a preliminary approach to matching child movements with robotic movements for the purpose of evaluating child upper limb rehabilitation exercises. Utilizing existing algorithms termed Motion History Imaging and Dynamic Time Warping for determining areas of movement and video frame mapping respectively, we are able to determine whether or not a patient is consistently performing accurate rehabilitation exercises. The overall goal of this research is to fuse play and rehabilitation techniques using a robotic design to induce child-robot interaction that will be entertaining as well as effective for the child.\n",
    ""
   ]
  },
  "black09_wocci": {
   "authors": [
    [
     "Matthew",
     "Black"
    ],
    [
     "Jeannette",
     "Chang"
    ],
    [
     "Jonathan",
     "Chang"
    ],
    [
     "Shrikanth",
     "Narayanan"
    ]
   ],
   "title": "Comparison of child-human and child-computer interactions based on manual annotations",
   "original": "wc09_049",
   "page_count": 6,
   "order": 11,
   "p1": "49",
   "pn": "54",
   "abstract": [
    "Technological advancements in recent years have been accompanied by a notable increase in research related to conversational child-machine interfaces. The technology has many applications from entertainment to education. In order to integrate this technology successfully we, however, need to understand the key differences (if any exist) in how children interact with machines versus how they interact with humans. Such knowledge could inform the design of more childappropriate interfaces as well as highlight any distinct characteristics of child-computer interactions that may be crucial for specific applications. In this paper, we analyze a subset of the Little CHIMP corpus, in which preschool aged children have a series of conversations with a human moderator and a Wizard-of-Oz controlled computer character. We first manually transcribed and annotated the data using an objective audio-visual behavior coding scheme. We next extracted features exemplifying language and social communication from these transcriptions and annotations and performed statistical hypothesis tests comparing the child-human and child-computer interactions. Finally, we discuss the differences between these two dyadic conversations.\n",
    ""
   ]
  },
  "maarouf09_wocci": {
   "authors": [
    [
     "Ismaïl El",
     "Maarouf"
    ],
    [
     "Jeanne",
     "Villaneau"
    ],
    [
     "Farida",
     "Saïd"
    ],
    [
     "Dominique",
     "Duhaut"
    ]
   ],
   "title": "Comparing child and adult language: exploring semantic constraints",
   "original": "wc09_055",
   "page_count": 5,
   "order": 12,
   "p1": "55",
   "pn": "59",
   "abstract": [
    "Actual research on child-machine interaction indicate that children are specific with respect to various acoustic, linguistic, psychological, cultural and social factors. We wish to address the linguistic factor, focusing on the semantic knowledge which needs to be mastered by a computer system designed to interact with children. Our work is intentionally usage-based and application-driven.   The research was conducted in the frame of the EmotiRob project, which aims at building a companion robot for children experiencing emotional difficulties. The robot is supposed to understand the emotional state of the child and respond (albeit non linguistically) adequately. The interactional capacities are heavily dependent on the results of the comprehension module. The comprehension model incorporates semantic knowledge such as children-based ontologies and specific semantic associative rules.   Our study is based on a corpus of Fairy Tales, which will later be compared to an oral corpus when the latter is completed. We argue that lexical knowledge and semantic associations discovered in this corpus will not differ greatly between writing and speech. Fairy Tales constitute privileged material for teachers and psychologists who argue that they play a crucial role in child socialization and structuration of concepts.   To spot child language specificities, we provide a contrastive analysis of semantic preferences according to production (child VS adult authored text) and to reception (child VS adult destined text). We use a shallow ontology to compare verb constraints on specific syntactic positions in child VS adult texts. Preliminary results show, as expected, a significant difference in terms of reception, though questioning the idea that adult language is much more constraining, while differences in terms of production are less obvious and call for a detailed qualitative study.\n",
    ""
   ]
  },
  "yildirim09_wocci": {
   "authors": [
    [
     "Serdar",
     "Yildirim"
    ],
    [
     "Shrikanth",
     "Narayanan"
    ]
   ],
   "title": "Recognizing child’s emotional state in problem-solving child-machine interactions",
   "original": "wc09_061",
   "page_count": 4,
   "order": 13,
   "p1": "61",
   "pn": "64",
   "abstract": [
    "The need for automatic recognition of a speaker's emotion within a spoken dialog system framework has received increased attention with demand for computer interfaces that provide natural and user-adaptive spoken interaction. This paper addresses the problem of automatically recognizing a child's emotional state using information obtained from audio and video signals. The study is based on a multimodal data corpus consisting of spontaneous conversations between a child and a computer agent. Four different techniques [k-nearest neighborhood (k-NN) classifier, decision tree, linear discriminant classifier (LDC), and support vector machine classifier (SVC)] were employed for classifying utterances into 2 emotion classes, negative and non-negative, for both acoustic and visual information. Experimental results show that, overall, combining visual information with acoustic information leads to performance improvements in emotion recognition. We obtained the best results when information sources were combined at feature level. Specifically, results showed that the addition of visual information to acoustic information yields relative improvements in emotion recognition of 3.8% with both LDC and SVC classifiers for information fusion at the feature level over that of using only acoustic information.\n",
    ""
   ]
  },
  "ruuska09_wocci": {
   "authors": [
    [
     "Heikki",
     "Ruuska"
    ],
    [
     "Shinya",
     "Kiriyama"
    ],
    [
     "Yoichi",
     "Takebayashi"
    ]
   ],
   "title": "Child selection of learning methods: a corpus based on real-world data",
   "original": "wc09_065",
   "page_count": 4,
   "order": 14,
   "p1": "65",
   "pn": "68",
   "abstract": [
    "We are running a parent-child learning project for constructing a multimodal child behavioral corpus. It is based on a learning environment where children and their parents regularly attend both guided lessons and have chances for free play. Among other purposes, we use the gathered data for analyzing how children construct evaluation schemas of learning processes. One important kind of learning is learning which learning methods are useful and which harmful in different situations. We have examined how learning methods, which are different from skills, develop from simple to complex physical and social manipulation. Our research shows hints on how negative knowledge on unsuccessful learning methods is used to encourage cognitive constructs more advanced than binary repetition, resulting in developing complex methods such as learning to reason by analogies and use them to construct new complex hypotheses about the world, and using adults and other children as sources of knowledge. We are also researching how right kind of support and example from adults close to children are essential in endowing and encouraging children with using these abilities.\n",
    ""
   ]
  },
  "tatsumi09_wocci": {
   "authors": [
    [
     "Takeo",
     "Tatsumi"
    ],
    [
     "Yoshiaki",
     "Nakano"
    ],
    [
     "Kiyoshi",
     "Tajitsu"
    ],
    [
     "Haruhiko",
     "Okumura"
    ],
    [
     "Yasunari",
     "Harada"
    ]
   ],
   "title": "Incorporating music into the study of algorithms and computer programming",
   "original": "wc09_069",
   "page_count": 8,
   "order": 15,
   "p1": "69",
   "pn": "76",
   "abstract": [
    "According to the national syllabus set forth by the Ministry of Education, Culture, Sports, Science and Technology, or hence force MEXT for short, and implemented in senior high schools since the school year of 2003-2004 in Japan, every student is expected to study and acquire necessary credits for the newly established subject Information Study. Learning about algorithms, with some exposure to and experience with programming languages, is part and parcel of the ‘Scientific Aspect,’ one of the three major goals set forth by MEXT, of this new subject. When learning a computer programming language, students must get accustomed to many new concepts, terms, and definitions along with the syntax and semantics of this language. There are too many elements to comprehend and master at the same time and this is part of the reasons why the number of students who study algorithms in K-12 is not increasing despite the introduction of Information Study in Japan. Noticing some apparent similarities between learning musical scores and learning computer programming languages, the authors proposed that incorporating study of music is a reasonable and interesting new way in learning programming languages and algorithms for senior high school students. In this paper, we describe our field trial based on this idea and suggest reasons why we believe this musical approach might be effective in learning the ‘Scientific Aspect’ of Information Study.\n",
    ""
   ]
  },
  "gonzalez09_wocci": {
   "authors": [
    [
     "Berto",
     "Gonzalez"
    ],
    [
     "John",
     "Borland"
    ],
    [
     "Kathleen",
     "Geraghty"
    ]
   ],
   "title": "Whole body interaction for child-centered multimodal language learning",
   "original": "wc09_077",
   "page_count": 5,
   "order": 16,
   "p1": "77",
   "pn": "81",
   "abstract": [
    "Children engage with the world with their whole bodies, and we suggest here that during dialect learning, as during other learning activities, technology be capable of responding in whole body ways. As the child becomes more engaged in a shared-reality environment, the coordination of the whole-body behaviors between the VP and child should increase, thereby enhancing the experience. In this paper, we present our work on developing a virtual agent that embodies whole-body behaviors and a sharedreality environment that encourages children to use whole-body expression in the context of learning dialect, and science talk.\n",
    ""
   ]
  },
  "robinson09_wocci": {
   "authors": [
    [
     "Ashley",
     "Robinson"
    ],
    [
     "Chao",
     "Peng"
    ],
    [
     "Francis",
     "Quek"
    ],
    [
     "Yong",
     "Cao"
    ]
   ],
   "title": "Interacting with stories",
   "original": "wc09_083",
   "page_count": 6,
   "order": 17,
   "p1": "83",
   "pn": "88",
   "abstract": [
    "In today's media-saturated world, students are consuming media both actively and passively. To facilitate active interaction with media, we address a specific kind of audio-visual media interaction in which we call a hyper-drama. We address hyper-drama interaction preferences across two age groups: grades one to five and grades 6 to 9. These hyper-drama interactions include a token on a horizontal display versus mouse on a desktop display for story navigation, desktop display versus tablet display for scene viewing, and virtual buttons versus speech for character interaction and decision making within the hyper-drama. We conducted a within-subjects pilot study to evaluate these interaction techniques.\n",
    ""
   ]
  },
  "gerosa09_wocci": {
   "authors": [
    [
     "Matteo",
     "Gerosa"
    ],
    [
     "Diego",
     "Giuliani"
    ],
    [
     "Shrikanth",
     "Narayanan"
    ],
    [
     "Alexandros",
     "Potamianos"
    ]
   ],
   "title": "A review of ASR technologies for children’s speech",
   "original": "wc09_089",
   "page_count": 8,
   "order": 18,
   "p1": "89",
   "pn": "96",
   "abstract": [
    "In this paper, we review: (1) the acoustic and linguistic properties of children’s speech for both read and spontaneous speech, and (2) the developments in automatic speech recognition for children with application to spoken dialogue and multimodal dialogue system design. First, the effect of developmental changes on the absolute values and variability of acoustic correlates is presented for read speech for children ages 6 and up. Then, verbal child-machine spontaneous interaction is reviewed and results from recent studies are presented. Age trends of acoustic, linguistic and interaction parameters are discussed, such as sentence duration, filled pauses, politeness and frustration markers, and modality usage. Some differences between child-machine and humanhuman interaction are pointed out. The implications for acoustic modeling, linguistic modeling and spoken dialogue system design for children are presented. We conclude with a review of relevant applications of spoken dialogue technologies for children.\n",
    ""
   ]
  },
  "xu09_wocci": {
   "authors": [
    [
     "Dongxin",
     "Xu"
    ],
    [
     "Jeffrey A.",
     "Richards"
    ],
    [
     "Jill",
     "Gilkerson"
    ],
    [
     "Umit",
     "Yapanel"
    ],
    [
     "Sharmistha",
     "Gray"
    ],
    [
     "John H. L.",
     "Hansen"
    ]
   ],
   "title": "Automatic childhood autism detection by vocalization decomposition with phone-like units",
   "original": "wc09_097",
   "page_count": 7,
   "order": 19,
   "p1": "97",
   "pn": "103",
   "abstract": [
    "Autism is a major child development disorder with a prevalence of 1/150 in the US. Although early identification is crucial to early intervention, there currently are few efficient screening tools in clinical use. This study reports a fully automatic mechanism for child autism detection/screening using the LENATM (Language ENvironment Analysis) System, which utilizes speech signal processing technology to analyze and monitor a child’s natural language environment and the vocalizations/speech of the child. We previously reported preliminary results using child vocalization composition information generated automatically by the LENA System employing an adult phone model. In this paper, some extensions have been made, including enlargement of the dataset, introduction of a new child vocalization decomposition with the k-means clusters derived directly from the child vocalizations, and its combination with the previous decomposition. The experiment and comparison consistently shows that the child vocalization composition contains rich discriminant information for autism detection. It also shows that the child vocalization composition features generated with the adult phone-model and the child clusters perform similarly when individually used, and complement each other when combined. The combined feature set significantly reduces the error rate. The relative error reduction is 21.7% at the recording-level and 16.8% at the child-level, achieving detection accuracies of 87.4% for recordings and 90.6% for children at the equal-error-rate points.\n",
    ""
   ]
  },
  "bolanos09_wocci": {
   "authors": [
    [
     "Daniel",
     "Bolaños"
    ],
    [
     "Wayne H.",
     "Ward"
    ],
    [
     "Ronald A.",
     "Cole"
    ]
   ],
   "title": "A reference verification framework and its application to a children’s speech reading tracker",
   "original": "wc09_105",
   "page_count": 2,
   "order": 20,
   "p1": "105",
   "pn": "106",
   "abstract": [
    "In this article we present a novel approach to reference verification, the problem of determining if a speakers' utterance matches a specificied reference (text) string, and discuss its application to a reading tracker system for children's speech.   Unlike other reading tracker systems proposed in the literature that are built over conventional speech recognizers with ad-hoc language models, the reading tracker described here is designed specifically for the task of estimating whether a child has read an expected sequence of words out loud; the tracker is designed to deal in a natural and flexible way with disfluencies that frequently appear in children's speech while reading out loud, (e.g., partial-words, repetitions, self-corrections, etc), and to overcome problems caused by using language models within the reference verification task. Two mechanisms have been introduced for this purpose, the utilization of filler models and the inclusion of backward inter-word transitions in the decoding network.   While this article focuses on the approach used to overcome errors observed in previous systems, the performance of this system will be evaluated on a corpus of children's speech while reading out loud and compared to the performance of a \"traditional\" reading tracker system that are built on top of a speech recognition system. The results of this comparison will be presented at WOCCI 2009.\n",
    ""
   ]
  },
  "yapanel09_wocci": {
   "authors": [
    [
     "Umit",
     "Yapanel"
    ],
    [
     "Dongxin",
     "Xu"
    ],
    [
     "John H. L.",
     "Hansen"
    ],
    [
     "Sharmistha",
     "Gray"
    ],
    [
     "Jill",
     "Gilkerson"
    ],
    [
     "Jeffrey A.",
     "Richards"
    ]
   ],
   "title": "Preliminary study of stress/neutral detection on recordings of children in the natural home environment",
   "original": "wc09_107",
   "page_count": 5,
   "order": 21,
   "p1": "107",
   "pn": "111",
   "abstract": [
    "Emotion and stress/neutral detection based on an input audio stream has been a topic of interest in the literature with various applications. This paper reports on a preliminary study of stress/neutral detection based on naturalistic home environment recordings of children. One major motivation of the work is to add stress/neutral detection functionality into the LENATM System [10]. The study started with an acted emotion database, and tested the acoustic feature of Mel-frequency cepstral coefficients and the Gaussian Mixture Model (GMM) for stress/neutral detection on this relatively simple database. The method was then applied to the adult speech segments automatically extracted from home recordings of children with the LENA System, achieving 72% accuracy for adult stress/neutral detection. The application of this new functionality to a large number of naturalistic home environment recordings of children reveals interesting and meaningful statistical differences among the families of typically developing children, language-delayed children, and children with Autism Spectrum Disorders (ASD). The result suggests the potential for stress/neutral detection, along with the LENA System, as an integrated solution for (i) quality assessment of the child language environment, (ii) monitoring language interventions for disordered children, or (iii) general psychological and behavioral research.\n",
    ""
   ]
  },
  "jokisch09_wocci": {
   "authors": [
    [
     "Oliver",
     "Jokisch"
    ],
    [
     "Horst-Udo",
     "Hain"
    ],
    [
     "Rico",
     "Petrick"
    ],
    [
     "Rüdiger",
     "Hoffmann"
    ]
   ],
   "title": "Robustness optimization of a speech interface for child-directed embedded language tutoring",
   "original": "wc09_113",
   "page_count": 4,
   "order": 22,
   "p1": "113",
   "pn": "116",
   "abstract": [
    "This contribution describes the robustness evaluation and optimization steps for a speech interface which is suitable for embedded language tutoring with special focus on children’s speech. The baseline algorithms are derived from the pronunciation tutoring system AzAR directed to adult learners of German. The first prototype LiSA (2008) - directed to young children starting at 3 years - is currently evaluated and optimized, mainly addressing following issues: (a) the challenge of ASR-based pronunciation assessment for children’s speech, (b) the handling of noise and reverberation in an embedded application scenario, and (c) the extraction of additional information such as age or gender. The article summarizes evaluation results of the speech recognizer in laboratory and real-world room environment.\n",
    ""
   ]
  },
  "patil09_wocci": {
   "authors": [
    [
     "Sanjay A.",
     "Patil"
    ],
    [
     "John H. L.",
     "Hansen"
    ],
    [
     "Jill",
     "Gilkerson"
    ],
    [
     "Sharmishta",
     "Gray"
    ],
    [
     "Dongxin",
     "Xu"
    ]
   ],
   "title": "Assessing the stress/neutral speech environment in adult/child interactions for applications in child language development",
   "original": "wc09_117",
   "page_count": 4,
   "order": 23,
   "p1": "117",
   "pn": "120",
   "abstract": [
    "It is known that for effective child language development, the number of adult words heard and adult-child exchanges in the early phase (8-20 months) is important. Language development can be represented in terms of adult word count (AWC) and conversational turns (CT) between the adult and child. The focus of this study is to investigate if perceived “stress” in the adult speech side of these exchanges impacts AWC or CTs, thus potentially impacting a child’s language acquisition skills. We propose to develop a scheme to detect the presence of stress in the adult side of child-adult audio streams and relate this with metrics available for assessing language development. The proposed approach represents the first attempt to assess child-adult interactions from a stress/neutral assessment approach where recordings are monitored continuously for 12-hour periods of time. Here, a proposed speaking rate measure based on the utterance length (UL /AWC) shows a statistical correlation with stress levels, with male adults showing more significance as compared to female adults. Thus, adults increase speaking rate when under stress which impacts their ability to convey articulation details, and therefore a potential negatively impacting child language acquisition.\n",
    ""
   ]
  }
 },
 "sessions": [
  {
   "title": "Invited Paper",
   "papers": [
    "ward09_wocci"
   ]
  },
  {
   "title": "Contributed Papers",
   "papers": [
    "bocklet09_wocci",
    "kasuriya09_wocci",
    "ibanezmartinez09_wocci",
    "bosch09_wocci",
    "bocklet09b_wocci",
    "maier09_wocci",
    "saz09_wocci",
    "kannetis09_wocci",
    "brooks09_wocci",
    "black09_wocci",
    "maarouf09_wocci",
    "yildirim09_wocci",
    "ruuska09_wocci",
    "tatsumi09_wocci",
    "gonzalez09_wocci",
    "robinson09_wocci",
    "gerosa09_wocci",
    "xu09_wocci",
    "bolanos09_wocci",
    "yapanel09_wocci",
    "jokisch09_wocci",
    "patil09_wocci"
   ]
  }
 ]
}